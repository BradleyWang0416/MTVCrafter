The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-10-08 10:34:25,808 
python train_vqvae_new.py --batch_size 48 --config vqvae_experiment_configs/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/config.yaml --data_mode joint3d --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl --not_find_unused_parameters --nb_code 8192 --codebook_dim 2048 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level [0,1,2,3] --vision_guidance_ratio 0.5 --downsample_time [1,2] --frame_upsample_rate [2.0,1.0] --fix_weights --resume_pth vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_40000 --vision_guidance_where enc --vision_guidance_fuse ada_sample
2025-10-08 10:34:25,808 
PID: 3767329
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
2025-10-08 10:35:59,750 Data loaded with 97196 samples
vision backbone weights are fixed
vision backbone weights are fixed
2025-10-08 10:36:00,913 Trainable parameters: 179,398,051
2025-10-08 10:36:00,913 Non-trainable parameters: 28,535,552
2025-10-08 10:36:02,266 Loading checkpoint from vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_40000
2025-10-08 10:36:02,266 Resuming from epoch 27 and iteration 40000
Missing keys: []Missing keys: []

Unexpected keys: []Unexpected keys: []

Trainning Epoch:   5%|▌         | 27/494 [00:00<?, ?it/s]2025-10-08 10:36:02,525 Number of trainable parameters: 179.398051 M
2025-10-08 10:36:02,525 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'joint3d', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [['image']], 'normalize': 'anisotropic', 'filter_invalid_images': True, 'processed_image_shape': [192, 256], 'backbone': 'hrnet_32', 'get_item_list': ['factor_2_5d', 'video_rgb', 'joint3d_image_affined', 'joint3d_image_affined_normed', 'joint3d_image_affined_scale', 'joint3d_image_affined_transl', 'affine_trans', 'affine_trans_inv', 'joint_2_5d_image'], 'config': 'vqvae_experiment_configs/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/config.yaml', 'resume_pth': 'vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_40000', 'batch_size': 48, 'commit_ratio': 0.5, 'nb_code': 8192, 'codebook_dim': 2048, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_image_affined_normed', 'hrnet_output_level': [0, 1, 2, 3], 'fix_weights': True, 'fix_weights_except': 'PLACEHOLDERPLACEHOLDERPLACEHOLDER', 'vision_guidance_ratio': 0.5, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0], 'vision_guidance_where': 'enc', 'vision_guidance_fuse': 'ada_sample'}
Trainning Epoch:   5%|▌         | 27/494 [00:00<?, ?it/s]2025-10-08 10:40:01,378 Stage: Train 0.5 | Epoch: 0 | Iter: 40200 | Total Loss: 0.006041 | Recon Loss: 0.005623 | Commit Loss: 0.000835 | Perplexity: 2980.660786
2025-10-08 10:43:56,387 Stage: Train 0.5 | Epoch: 0 | Iter: 40400 | Total Loss: 0.004937 | Recon Loss: 0.004557 | Commit Loss: 0.000759 | Perplexity: 2965.833641
2025-10-08 10:47:52,500 Stage: Train 0.5 | Epoch: 0 | Iter: 40600 | Total Loss: 0.004680 | Recon Loss: 0.004292 | Commit Loss: 0.000778 | Perplexity: 2958.923154
2025-10-08 10:51:47,658 Stage: Train 0.5 | Epoch: 0 | Iter: 40800 | Total Loss: 0.004754 | Recon Loss: 0.004373 | Commit Loss: 0.000762 | Perplexity: 2974.494070
2025-10-08 10:55:37,577 Stage: Train 0.5 | Epoch: 0 | Iter: 41000 | Total Loss: 0.004916 | Recon Loss: 0.004537 | Commit Loss: 0.000757 | Perplexity: 2975.061976
Trainning Epoch:   6%|▌         | 28/494 [19:49<154:01:52, 1189.94s/it]Trainning Epoch:   6%|▌         | 28/494 [19:49<154:01:52, 1189.94s/it]2025-10-08 10:59:32,840 Stage: Train 0.5 | Epoch: 1 | Iter: 41200 | Total Loss: 0.004927 | Recon Loss: 0.004557 | Commit Loss: 0.000741 | Perplexity: 2959.860160
2025-10-08 11:03:24,579 Stage: Train 0.5 | Epoch: 1 | Iter: 41400 | Total Loss: 0.004729 | Recon Loss: 0.004346 | Commit Loss: 0.000764 | Perplexity: 2963.739226
2025-10-08 11:07:15,954 Stage: Train 0.5 | Epoch: 1 | Iter: 41600 | Total Loss: 0.004767 | Recon Loss: 0.004387 | Commit Loss: 0.000759 | Perplexity: 2969.946034
2025-10-08 11:11:07,428 Stage: Train 0.5 | Epoch: 1 | Iter: 41800 | Total Loss: 0.004769 | Recon Loss: 0.004395 | Commit Loss: 0.000747 | Perplexity: 2965.482944
2025-10-08 11:14:58,839 Stage: Train 0.5 | Epoch: 1 | Iter: 42000 | Total Loss: 0.004797 | Recon Loss: 0.004422 | Commit Loss: 0.000749 | Perplexity: 2972.480214
Trainning Epoch:   6%|▌         | 29/494 [39:26<152:40:08, 1181.95s/it]Trainning Epoch:   6%|▌         | 29/494 [39:26<152:40:09, 1181.96s/it]2025-10-08 11:18:55,115 Stage: Train 0.5 | Epoch: 2 | Iter: 42200 | Total Loss: 0.004803 | Recon Loss: 0.004431 | Commit Loss: 0.000743 | Perplexity: 2963.635735
2025-10-08 11:22:48,931 Stage: Train 0.5 | Epoch: 2 | Iter: 42400 | Total Loss: 0.004736 | Recon Loss: 0.004351 | Commit Loss: 0.000771 | Perplexity: 2968.613794
2025-10-08 11:26:40,864 Stage: Train 0.5 | Epoch: 2 | Iter: 42600 | Total Loss: 0.004857 | Recon Loss: 0.004478 | Commit Loss: 0.000757 | Perplexity: 2964.698190
2025-10-08 11:30:32,527 Stage: Train 0.5 | Epoch: 2 | Iter: 42800 | Total Loss: 0.004718 | Recon Loss: 0.004347 | Commit Loss: 0.000742 | Perplexity: 2961.710294
2025-10-08 11:34:23,722 Stage: Train 0.5 | Epoch: 2 | Iter: 43000 | Total Loss: 0.004780 | Recon Loss: 0.004404 | Commit Loss: 0.000751 | Perplexity: 2972.648917
Trainning Epoch:   6%|▌         | 30/494 [59:06<152:13:21, 1181.04s/it]Trainning Epoch:   6%|▌         | 30/494 [59:06<152:13:21, 1181.04s/it]2025-10-08 11:38:21,822 Stage: Train 0.5 | Epoch: 3 | Iter: 43200 | Total Loss: 0.004708 | Recon Loss: 0.004327 | Commit Loss: 0.000763 | Perplexity: 2972.446591
2025-10-08 11:42:15,034 Stage: Train 0.5 | Epoch: 3 | Iter: 43400 | Total Loss: 0.004726 | Recon Loss: 0.004340 | Commit Loss: 0.000772 | Perplexity: 2966.490155
2025-10-08 11:46:06,979 Stage: Train 0.5 | Epoch: 3 | Iter: 43600 | Total Loss: 0.004705 | Recon Loss: 0.004330 | Commit Loss: 0.000750 | Perplexity: 2957.965469
2025-10-08 11:49:57,508 Stage: Train 0.5 | Epoch: 3 | Iter: 43800 | Total Loss: 0.004687 | Recon Loss: 0.004309 | Commit Loss: 0.000755 | Perplexity: 2976.979834
2025-10-08 11:53:49,237 Stage: Train 0.5 | Epoch: 3 | Iter: 44000 | Total Loss: 0.004767 | Recon Loss: 0.004394 | Commit Loss: 0.000745 | Perplexity: 2967.164177
Trainning Epoch:   6%|▋         | 31/494 [1:18:47<151:53:02, 1180.96s/it]Trainning Epoch:   6%|▋         | 31/494 [1:18:47<151:53:02, 1180.95s/it]2025-10-08 11:57:43,851 Stage: Train 0.5 | Epoch: 4 | Iter: 44200 | Total Loss: 0.004696 | Recon Loss: 0.004318 | Commit Loss: 0.000755 | Perplexity: 2971.296973
2025-10-08 12:01:33,434 Stage: Train 0.5 | Epoch: 4 | Iter: 44400 | Total Loss: 0.004715 | Recon Loss: 0.004342 | Commit Loss: 0.000746 | Perplexity: 2962.308959
2025-10-08 12:05:22,689 Stage: Train 0.5 | Epoch: 4 | Iter: 44600 | Total Loss: 0.004658 | Recon Loss: 0.004275 | Commit Loss: 0.000767 | Perplexity: 2975.805861
2025-10-08 12:09:12,253 Stage: Train 0.5 | Epoch: 4 | Iter: 44800 | Total Loss: 0.004644 | Recon Loss: 0.004263 | Commit Loss: 0.000763 | Perplexity: 2961.041573
2025-10-08 12:13:02,554 Stage: Train 0.5 | Epoch: 4 | Iter: 45000 | Total Loss: 0.004548 | Recon Loss: 0.004162 | Commit Loss: 0.000772 | Perplexity: 2967.183065
Trainning Epoch:   6%|▋         | 32/494 [1:38:14<150:55:21, 1176.02s/it]Trainning Epoch:   6%|▋         | 32/494 [1:38:14<150:55:21, 1176.02s/it]2025-10-08 12:16:58,867 Stage: Train 0.5 | Epoch: 5 | Iter: 45200 | Total Loss: 0.004608 | Recon Loss: 0.004232 | Commit Loss: 0.000753 | Perplexity: 2967.493113
2025-10-08 12:20:51,411 Stage: Train 0.5 | Epoch: 5 | Iter: 45400 | Total Loss: 0.004619 | Recon Loss: 0.004238 | Commit Loss: 0.000761 | Perplexity: 2969.950371
2025-10-08 12:24:43,778 Stage: Train 0.5 | Epoch: 5 | Iter: 45600 | Total Loss: 0.004605 | Recon Loss: 0.004223 | Commit Loss: 0.000765 | Perplexity: 2961.549843
2025-10-08 12:28:35,063 Stage: Train 0.5 | Epoch: 5 | Iter: 45800 | Total Loss: 0.004590 | Recon Loss: 0.004206 | Commit Loss: 0.000769 | Perplexity: 2963.157034
2025-10-08 12:32:25,965 Stage: Train 0.5 | Epoch: 5 | Iter: 46000 | Total Loss: 0.004597 | Recon Loss: 0.004211 | Commit Loss: 0.000771 | Perplexity: 2965.529907
Trainning Epoch:   7%|▋         | 33/494 [1:57:53<150:45:00, 1177.22s/it]Trainning Epoch:   7%|▋         | 33/494 [1:57:53<150:45:00, 1177.22s/it]2025-10-08 12:36:20,209 Stage: Train 0.5 | Epoch: 6 | Iter: 46200 | Total Loss: 0.004549 | Recon Loss: 0.004171 | Commit Loss: 0.000757 | Perplexity: 2969.008744
2025-10-08 12:40:09,151 Stage: Train 0.5 | Epoch: 6 | Iter: 46400 | Total Loss: 0.004541 | Recon Loss: 0.004157 | Commit Loss: 0.000768 | Perplexity: 2962.533225
2025-10-08 12:43:58,111 Stage: Train 0.5 | Epoch: 6 | Iter: 46600 | Total Loss: 0.004521 | Recon Loss: 0.004129 | Commit Loss: 0.000784 | Perplexity: 2963.728540
2025-10-08 12:47:46,836 Stage: Train 0.5 | Epoch: 6 | Iter: 46800 | Total Loss: 0.004590 | Recon Loss: 0.004204 | Commit Loss: 0.000772 | Perplexity: 2965.132938
2025-10-08 12:51:36,334 Stage: Train 0.5 | Epoch: 6 | Iter: 47000 | Total Loss: 0.004508 | Recon Loss: 0.004127 | Commit Loss: 0.000762 | Perplexity: 2966.733530
Trainning Epoch:   7%|▋         | 34/494 [2:17:17<149:52:16, 1172.90s/it]Trainning Epoch:   7%|▋         | 34/494 [2:17:17<149:52:16, 1172.91s/it]2025-10-08 12:55:28,241 Stage: Train 0.5 | Epoch: 7 | Iter: 47200 | Total Loss: 0.004561 | Recon Loss: 0.004179 | Commit Loss: 0.000765 | Perplexity: 2968.751884
2025-10-08 12:59:16,962 Stage: Train 0.5 | Epoch: 7 | Iter: 47400 | Total Loss: 0.004545 | Recon Loss: 0.004164 | Commit Loss: 0.000763 | Perplexity: 2962.472227
2025-10-08 13:03:06,612 Stage: Train 0.5 | Epoch: 7 | Iter: 47600 | Total Loss: 0.004435 | Recon Loss: 0.004044 | Commit Loss: 0.000783 | Perplexity: 2969.750553
2025-10-08 13:06:55,601 Stage: Train 0.5 | Epoch: 7 | Iter: 47800 | Total Loss: 0.004533 | Recon Loss: 0.004152 | Commit Loss: 0.000762 | Perplexity: 2962.510421
2025-10-08 13:10:45,028 Stage: Train 0.5 | Epoch: 7 | Iter: 48000 | Total Loss: 0.004494 | Recon Loss: 0.004105 | Commit Loss: 0.000779 | Perplexity: 2965.003304
Trainning Epoch:   7%|▋         | 35/494 [2:36:41<149:08:55, 1169.79s/it]Trainning Epoch:   7%|▋         | 35/494 [2:36:41<149:08:55, 1169.79s/it]2025-10-08 13:14:38,433 Stage: Train 0.5 | Epoch: 8 | Iter: 48200 | Total Loss: 0.004530 | Recon Loss: 0.004147 | Commit Loss: 0.000767 | Perplexity: 2962.215626
2025-10-08 13:18:29,647 Stage: Train 0.5 | Epoch: 8 | Iter: 48400 | Total Loss: 0.004506 | Recon Loss: 0.004124 | Commit Loss: 0.000764 | Perplexity: 2970.507257
2025-10-08 13:22:19,756 Stage: Train 0.5 | Epoch: 8 | Iter: 48600 | Total Loss: 0.004430 | Recon Loss: 0.004039 | Commit Loss: 0.000781 | Perplexity: 2966.995818
2025-10-08 13:26:10,044 Stage: Train 0.5 | Epoch: 8 | Iter: 48800 | Total Loss: 0.004531 | Recon Loss: 0.004151 | Commit Loss: 0.000760 | Perplexity: 2963.797477
2025-10-08 13:30:00,571 Stage: Train 0.5 | Epoch: 8 | Iter: 49000 | Total Loss: 0.004451 | Recon Loss: 0.004063 | Commit Loss: 0.000775 | Perplexity: 2970.322299
Trainning Epoch:   7%|▋         | 36/494 [2:56:11<148:52:01, 1170.13s/it]Trainning Epoch:   7%|▋         | 36/494 [2:56:11<148:52:07, 1170.15s/it]2025-10-08 13:33:55,062 Stage: Train 0.5 | Epoch: 9 | Iter: 49200 | Total Loss: 0.004533 | Recon Loss: 0.004145 | Commit Loss: 0.000778 | Perplexity: 2966.348862
2025-10-08 13:37:48,521 Stage: Train 0.5 | Epoch: 9 | Iter: 49400 | Total Loss: 0.004448 | Recon Loss: 0.004060 | Commit Loss: 0.000778 | Perplexity: 2964.046340
2025-10-08 13:41:41,418 Stage: Train 0.5 | Epoch: 9 | Iter: 49600 | Total Loss: 0.004434 | Recon Loss: 0.004046 | Commit Loss: 0.000775 | Perplexity: 2962.077335
2025-10-08 13:45:34,400 Stage: Train 0.5 | Epoch: 9 | Iter: 49800 | Total Loss: 0.004470 | Recon Loss: 0.004083 | Commit Loss: 0.000774 | Perplexity: 2971.051588
2025-10-08 13:49:27,668 Stage: Train 0.5 | Epoch: 9 | Iter: 50000 | Total Loss: 0.004405 | Recon Loss: 0.004013 | Commit Loss: 0.000785 | Perplexity: 2973.789054
Trainning Epoch:   7%|▋         | 37/494 [3:15:56<149:05:58, 1174.53s/it]Trainning Epoch:   7%|▋         | 37/494 [3:15:56<149:06:03, 1174.54s/it]2025-10-08 13:53:23,070 Stage: Train 0.5 | Epoch: 10 | Iter: 50200 | Total Loss: 0.004517 | Recon Loss: 0.004130 | Commit Loss: 0.000775 | Perplexity: 2964.408367
2025-10-08 13:57:13,477 Stage: Train 0.5 | Epoch: 10 | Iter: 50400 | Total Loss: 0.004303 | Recon Loss: 0.003910 | Commit Loss: 0.000786 | Perplexity: 2956.110542
2025-10-08 14:01:02,641 Stage: Train 0.5 | Epoch: 10 | Iter: 50600 | Total Loss: 0.004402 | Recon Loss: 0.004017 | Commit Loss: 0.000770 | Perplexity: 2968.306404
2025-10-08 14:04:52,445 Stage: Train 0.5 | Epoch: 10 | Iter: 50800 | Total Loss: 0.004494 | Recon Loss: 0.004110 | Commit Loss: 0.000767 | Perplexity: 2974.809485
2025-10-08 14:08:43,262 Stage: Train 0.5 | Epoch: 10 | Iter: 51000 | Total Loss: 0.004482 | Recon Loss: 0.004091 | Commit Loss: 0.000782 | Perplexity: 2964.557601
Trainning Epoch:   8%|▊         | 38/494 [3:35:25<148:32:51, 1172.74s/it]Trainning Epoch:   8%|▊         | 38/494 [3:35:25<148:32:51, 1172.74s/it]2025-10-08 14:12:37,815 Stage: Train 0.5 | Epoch: 11 | Iter: 51200 | Total Loss: 0.004425 | Recon Loss: 0.004038 | Commit Loss: 0.000774 | Perplexity: 2969.071119
2025-10-08 14:16:30,297 Stage: Train 0.5 | Epoch: 11 | Iter: 51400 | Total Loss: 0.004425 | Recon Loss: 0.004039 | Commit Loss: 0.000771 | Perplexity: 2977.805394
2025-10-08 14:20:21,451 Stage: Train 0.5 | Epoch: 11 | Iter: 51600 | Total Loss: 0.004454 | Recon Loss: 0.004067 | Commit Loss: 0.000774 | Perplexity: 2972.576294
2025-10-08 14:24:13,235 Stage: Train 0.5 | Epoch: 11 | Iter: 51800 | Total Loss: 0.004426 | Recon Loss: 0.004039 | Commit Loss: 0.000775 | Perplexity: 2963.173617
2025-10-08 14:28:07,363 Stage: Train 0.5 | Epoch: 11 | Iter: 52000 | Total Loss: 0.004411 | Recon Loss: 0.004018 | Commit Loss: 0.000786 | Perplexity: 2981.918937
Trainning Epoch:   8%|▊         | 39/494 [3:55:05<148:31:25, 1175.13s/it]Trainning Epoch:   8%|▊         | 39/494 [3:55:05<148:31:24, 1175.13s/it]2025-10-08 14:32:02,662 Stage: Train 0.5 | Epoch: 12 | Iter: 52200 | Total Loss: 0.004264 | Recon Loss: 0.003868 | Commit Loss: 0.000792 | Perplexity: 2976.371440
2025-10-08 14:35:51,224 Stage: Train 0.5 | Epoch: 12 | Iter: 52400 | Total Loss: 0.004328 | Recon Loss: 0.003938 | Commit Loss: 0.000780 | Perplexity: 2964.642742
2025-10-08 14:39:39,778 Stage: Train 0.5 | Epoch: 12 | Iter: 52600 | Total Loss: 0.004351 | Recon Loss: 0.003960 | Commit Loss: 0.000783 | Perplexity: 2969.900477
2025-10-08 14:43:28,990 Stage: Train 0.5 | Epoch: 12 | Iter: 52800 | Total Loss: 0.004349 | Recon Loss: 0.003964 | Commit Loss: 0.000771 | Perplexity: 2960.492961
2025-10-08 14:47:17,613 Stage: Train 0.5 | Epoch: 12 | Iter: 53000 | Total Loss: 0.004415 | Recon Loss: 0.004024 | Commit Loss: 0.000781 | Perplexity: 2985.372112
Trainning Epoch:   8%|▊         | 40/494 [4:14:27<147:42:20, 1171.23s/it]Trainning Epoch:   8%|▊         | 40/494 [4:14:27<147:42:20, 1171.23s/it]2025-10-08 14:51:10,248 Stage: Train 0.5 | Epoch: 13 | Iter: 53200 | Total Loss: 0.004317 | Recon Loss: 0.003919 | Commit Loss: 0.000795 | Perplexity: 2972.995254
2025-10-08 14:55:01,159 Stage: Train 0.5 | Epoch: 13 | Iter: 53400 | Total Loss: 0.004494 | Recon Loss: 0.004108 | Commit Loss: 0.000772 | Perplexity: 2972.037944
2025-10-08 14:58:52,173 Stage: Train 0.5 | Epoch: 13 | Iter: 53600 | Total Loss: 0.004271 | Recon Loss: 0.003881 | Commit Loss: 0.000779 | Perplexity: 2964.806344
2025-10-08 15:02:42,669 Stage: Train 0.5 | Epoch: 13 | Iter: 53800 | Total Loss: 0.004337 | Recon Loss: 0.003942 | Commit Loss: 0.000790 | Perplexity: 2973.563358
2025-10-08 15:06:33,351 Stage: Train 0.5 | Epoch: 13 | Iter: 54000 | Total Loss: 0.004387 | Recon Loss: 0.003991 | Commit Loss: 0.000791 | Perplexity: 2977.020289
Trainning Epoch:   8%|▊         | 41/494 [4:33:59<147:23:28, 1171.32s/it]Trainning Epoch:   8%|▊         | 41/494 [4:33:59<147:23:29, 1171.32s/it]2025-10-08 15:10:26,578 Stage: Train 0.5 | Epoch: 14 | Iter: 54200 | Total Loss: 0.004360 | Recon Loss: 0.003976 | Commit Loss: 0.000769 | Perplexity: 2972.782115
2025-10-08 15:14:15,722 Stage: Train 0.5 | Epoch: 14 | Iter: 54400 | Total Loss: 0.004267 | Recon Loss: 0.003872 | Commit Loss: 0.000790 | Perplexity: 2973.112185
2025-10-08 15:18:04,212 Stage: Train 0.5 | Epoch: 14 | Iter: 54600 | Total Loss: 0.004485 | Recon Loss: 0.004100 | Commit Loss: 0.000769 | Perplexity: 2972.486547
2025-10-08 15:21:54,028 Stage: Train 0.5 | Epoch: 14 | Iter: 54800 | Total Loss: 0.004239 | Recon Loss: 0.003853 | Commit Loss: 0.000773 | Perplexity: 2982.264479
2025-10-08 15:25:43,173 Stage: Train 0.5 | Epoch: 14 | Iter: 55000 | Total Loss: 0.004419 | Recon Loss: 0.004023 | Commit Loss: 0.000794 | Perplexity: 2981.194663
Trainning Epoch:   9%|▊         | 42/494 [4:53:23<146:47:44, 1169.17s/it]Trainning Epoch:   9%|▊         | 42/494 [4:53:23<146:47:44, 1169.17s/it]2025-10-08 15:29:36,267 Stage: Train 0.5 | Epoch: 15 | Iter: 55200 | Total Loss: 0.004249 | Recon Loss: 0.003861 | Commit Loss: 0.000776 | Perplexity: 2979.200099
2025-10-08 15:33:29,653 Stage: Train 0.5 | Epoch: 15 | Iter: 55400 | Total Loss: 0.004296 | Recon Loss: 0.003907 | Commit Loss: 0.000778 | Perplexity: 2980.416123
2025-10-08 15:37:23,093 Stage: Train 0.5 | Epoch: 15 | Iter: 55600 | Total Loss: 0.004267 | Recon Loss: 0.003883 | Commit Loss: 0.000769 | Perplexity: 2972.923381
2025-10-08 15:41:14,440 Stage: Train 0.5 | Epoch: 15 | Iter: 55800 | Total Loss: 0.004370 | Recon Loss: 0.003980 | Commit Loss: 0.000780 | Perplexity: 2981.524667
2025-10-08 15:45:05,298 Stage: Train 0.5 | Epoch: 15 | Iter: 56000 | Total Loss: 0.004318 | Recon Loss: 0.003934 | Commit Loss: 0.000768 | Perplexity: 2979.565232
2025-10-08 15:48:55,177 Stage: Train 0.5 | Epoch: 15 | Iter: 56200 | Total Loss: 0.004281 | Recon Loss: 0.003888 | Commit Loss: 0.000787 | Perplexity: 2972.251302
Trainning Epoch:   9%|▊         | 43/494 [5:13:02<146:49:11, 1171.95s/it]Trainning Epoch:   9%|▊         | 43/494 [5:13:02<146:49:11, 1171.95s/it]2025-10-08 15:52:50,672 Stage: Train 0.5 | Epoch: 16 | Iter: 56400 | Total Loss: 0.004263 | Recon Loss: 0.003876 | Commit Loss: 0.000774 | Perplexity: 2984.748284
2025-10-08 15:56:41,992 Stage: Train 0.5 | Epoch: 16 | Iter: 56600 | Total Loss: 0.004359 | Recon Loss: 0.003964 | Commit Loss: 0.000791 | Perplexity: 2984.237686
2025-10-08 16:00:31,418 Stage: Train 0.5 | Epoch: 16 | Iter: 56800 | Total Loss: 0.004276 | Recon Loss: 0.003884 | Commit Loss: 0.000784 | Perplexity: 2975.273192
2025-10-08 16:04:19,632 Stage: Train 0.5 | Epoch: 16 | Iter: 57000 | Total Loss: 0.004196 | Recon Loss: 0.003804 | Commit Loss: 0.000785 | Perplexity: 2985.795122
2025-10-08 16:08:08,243 Stage: Train 0.5 | Epoch: 16 | Iter: 57200 | Total Loss: 0.004263 | Recon Loss: 0.003867 | Commit Loss: 0.000793 | Perplexity: 2984.359878
Trainning Epoch:   9%|▉         | 44/494 [5:32:29<146:19:55, 1170.66s/it]Trainning Epoch:   9%|▉         | 44/494 [5:32:29<146:19:54, 1170.66s/it]2025-10-08 16:12:04,747 Stage: Train 0.5 | Epoch: 17 | Iter: 57400 | Total Loss: 0.004191 | Recon Loss: 0.003798 | Commit Loss: 0.000786 | Perplexity: 2991.050348
2025-10-08 16:15:56,919 Stage: Train 0.5 | Epoch: 17 | Iter: 57600 | Total Loss: 0.004279 | Recon Loss: 0.003893 | Commit Loss: 0.000772 | Perplexity: 2967.380872
2025-10-08 16:19:47,240 Stage: Train 0.5 | Epoch: 17 | Iter: 57800 | Total Loss: 0.004361 | Recon Loss: 0.003968 | Commit Loss: 0.000786 | Perplexity: 2985.991880
2025-10-08 16:23:38,017 Stage: Train 0.5 | Epoch: 17 | Iter: 58000 | Total Loss: 0.004168 | Recon Loss: 0.003778 | Commit Loss: 0.000780 | Perplexity: 2986.941174
2025-10-08 16:27:28,712 Stage: Train 0.5 | Epoch: 17 | Iter: 58200 | Total Loss: 0.004217 | Recon Loss: 0.003816 | Commit Loss: 0.000800 | Perplexity: 2976.248137
Trainning Epoch:   9%|▉         | 45/494 [5:52:05<146:11:22, 1172.12s/it]Trainning Epoch:   9%|▉         | 45/494 [5:52:05<146:11:22, 1172.12s/it]2025-10-08 16:31:21,948 Stage: Train 0.5 | Epoch: 18 | Iter: 58400 | Total Loss: 0.004281 | Recon Loss: 0.003892 | Commit Loss: 0.000777 | Perplexity: 2989.617560
2025-10-08 16:35:11,425 Stage: Train 0.5 | Epoch: 18 | Iter: 58600 | Total Loss: 0.004171 | Recon Loss: 0.003776 | Commit Loss: 0.000790 | Perplexity: 2992.660687
2025-10-08 16:39:00,358 Stage: Train 0.5 | Epoch: 18 | Iter: 58800 | Total Loss: 0.004186 | Recon Loss: 0.003794 | Commit Loss: 0.000783 | Perplexity: 2986.753336
2025-10-08 16:42:48,680 Stage: Train 0.5 | Epoch: 18 | Iter: 59000 | Total Loss: 0.004165 | Recon Loss: 0.003767 | Commit Loss: 0.000797 | Perplexity: 2992.376357
2025-10-08 16:46:36,762 Stage: Train 0.5 | Epoch: 18 | Iter: 59200 | Total Loss: 0.004300 | Recon Loss: 0.003908 | Commit Loss: 0.000784 | Perplexity: 2987.734426
Trainning Epoch:   9%|▉         | 46/494 [6:11:27<145:30:43, 1169.29s/it]Trainning Epoch:   9%|▉         | 46/494 [6:11:27<145:30:43, 1169.29s/it]2025-10-08 16:50:30,116 Stage: Train 0.5 | Epoch: 19 | Iter: 59400 | Total Loss: 0.004138 | Recon Loss: 0.003743 | Commit Loss: 0.000790 | Perplexity: 2992.423138
2025-10-08 16:54:19,654 Stage: Train 0.5 | Epoch: 19 | Iter: 59600 | Total Loss: 0.004267 | Recon Loss: 0.003875 | Commit Loss: 0.000783 | Perplexity: 2989.611606
2025-10-08 16:58:08,503 Stage: Train 0.5 | Epoch: 19 | Iter: 59800 | Total Loss: 0.004200 | Recon Loss: 0.003809 | Commit Loss: 0.000781 | Perplexity: 2981.338068
2025-10-08 17:01:57,745 Stage: Train 0.5 | Epoch: 19 | Iter: 60000 | Total Loss: 0.004200 | Recon Loss: 0.003802 | Commit Loss: 0.000796 | Perplexity: 2993.329633
2025-10-08 17:01:57,746 Saving model at iteration 60000
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
2025-10-08 17:01:58,233 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000
2025-10-08 17:01:59,721 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/model.safetensors
2025-10-08 17:02:01,486 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/optimizer.bin
2025-10-08 17:02:01,487 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/scheduler.bin
2025-10-08 17:02:01,487 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/sampler.bin
2025-10-08 17:02:01,488 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/random_states_0.pkl
2025-10-08 17:05:49,872 Stage: Train 0.5 | Epoch: 19 | Iter: 60200 | Total Loss: 0.004035 | Recon Loss: 0.003637 | Commit Loss: 0.000797 | Perplexity: 2990.828420
Trainning Epoch:  10%|▉         | 47/494 [6:30:55<145:08:09, 1168.88s/it]Trainning Epoch:  10%|▉         | 47/494 [6:30:55<145:08:08, 1168.88s/it]2025-10-08 17:09:46,997 Stage: Train 0.5 | Epoch: 20 | Iter: 60400 | Total Loss: 0.004142 | Recon Loss: 0.003748 | Commit Loss: 0.000787 | Perplexity: 2985.499940
2025-10-08 17:13:39,748 Stage: Train 0.5 | Epoch: 20 | Iter: 60600 | Total Loss: 0.004117 | Recon Loss: 0.003721 | Commit Loss: 0.000791 | Perplexity: 2993.678135
2025-10-08 17:17:31,438 Stage: Train 0.5 | Epoch: 20 | Iter: 60800 | Total Loss: 0.004146 | Recon Loss: 0.003748 | Commit Loss: 0.000797 | Perplexity: 2988.706085
2025-10-08 17:21:23,391 Stage: Train 0.5 | Epoch: 20 | Iter: 61000 | Total Loss: 0.004156 | Recon Loss: 0.003769 | Commit Loss: 0.000775 | Perplexity: 2988.704697
2025-10-08 17:25:15,119 Stage: Train 0.5 | Epoch: 20 | Iter: 61200 | Total Loss: 0.004084 | Recon Loss: 0.003695 | Commit Loss: 0.000777 | Perplexity: 2999.961442
Trainning Epoch:  10%|▉         | 48/494 [6:50:37<145:16:35, 1172.64s/it]Trainning Epoch:  10%|▉         | 48/494 [6:50:37<145:16:38, 1172.64s/it]2025-10-08 17:29:10,128 Stage: Train 0.5 | Epoch: 21 | Iter: 61400 | Total Loss: 0.004148 | Recon Loss: 0.003759 | Commit Loss: 0.000779 | Perplexity: 2990.975804
2025-10-08 17:32:59,227 Stage: Train 0.5 | Epoch: 21 | Iter: 61600 | Total Loss: 0.004148 | Recon Loss: 0.003752 | Commit Loss: 0.000790 | Perplexity: 2987.901266
2025-10-08 17:36:47,929 Stage: Train 0.5 | Epoch: 21 | Iter: 61800 | Total Loss: 0.004133 | Recon Loss: 0.003739 | Commit Loss: 0.000788 | Perplexity: 2987.246914
2025-10-08 17:40:36,747 Stage: Train 0.5 | Epoch: 21 | Iter: 62000 | Total Loss: 0.004044 | Recon Loss: 0.003642 | Commit Loss: 0.000805 | Perplexity: 2993.149281
2025-10-08 17:44:26,007 Stage: Train 0.5 | Epoch: 21 | Iter: 62200 | Total Loss: 0.004089 | Recon Loss: 0.003701 | Commit Loss: 0.000776 | Perplexity: 2988.690776
Trainning Epoch:  10%|▉         | 49/494 [7:10:01<144:39:24, 1170.26s/it]Trainning Epoch:  10%|▉         | 49/494 [7:10:01<144:39:23, 1170.26s/it]2025-10-08 17:48:21,246 Stage: Train 0.5 | Epoch: 22 | Iter: 62400 | Total Loss: 0.004065 | Recon Loss: 0.003671 | Commit Loss: 0.000787 | Perplexity: 2992.827146
2025-10-08 17:52:11,160 Stage: Train 0.5 | Epoch: 22 | Iter: 62600 | Total Loss: 0.004169 | Recon Loss: 0.003784 | Commit Loss: 0.000771 | Perplexity: 2999.406455
2025-10-08 17:56:00,019 Stage: Train 0.5 | Epoch: 22 | Iter: 62800 | Total Loss: 0.004091 | Recon Loss: 0.003697 | Commit Loss: 0.000789 | Perplexity: 2995.063917
2025-10-08 17:59:49,276 Stage: Train 0.5 | Epoch: 22 | Iter: 63000 | Total Loss: 0.004122 | Recon Loss: 0.003724 | Commit Loss: 0.000796 | Perplexity: 2979.064135
2025-10-08 18:03:39,111 Stage: Train 0.5 | Epoch: 22 | Iter: 63200 | Total Loss: 0.004070 | Recon Loss: 0.003678 | Commit Loss: 0.000784 | Perplexity: 3006.038885
Trainning Epoch:  10%|█         | 50/494 [7:29:30<144:16:58, 1169.86s/it]Trainning Epoch:  10%|█         | 50/494 [7:29:30<144:16:59, 1169.86s/it]2025-10-08 18:07:35,093 Stage: Train 0.5 | Epoch: 23 | Iter: 63400 | Total Loss: 0.003993 | Recon Loss: 0.003597 | Commit Loss: 0.000794 | Perplexity: 3001.998080
2025-10-08 18:11:27,390 Stage: Train 0.5 | Epoch: 23 | Iter: 63600 | Total Loss: 0.004071 | Recon Loss: 0.003677 | Commit Loss: 0.000788 | Perplexity: 2985.766884
2025-10-08 18:15:19,381 Stage: Train 0.5 | Epoch: 23 | Iter: 63800 | Total Loss: 0.004100 | Recon Loss: 0.003708 | Commit Loss: 0.000786 | Perplexity: 2995.154170
2025-10-08 18:19:10,715 Stage: Train 0.5 | Epoch: 23 | Iter: 64000 | Total Loss: 0.004179 | Recon Loss: 0.003788 | Commit Loss: 0.000782 | Perplexity: 2983.005404
2025-10-08 18:23:01,629 Stage: Train 0.5 | Epoch: 23 | Iter: 64200 | Total Loss: 0.004133 | Recon Loss: 0.003744 | Commit Loss: 0.000778 | Perplexity: 2993.509020
Trainning Epoch:  10%|█         | 51/494 [7:49:08<144:14:56, 1172.23s/it]Trainning Epoch:  10%|█         | 51/494 [7:49:08<144:14:57, 1172.23s/it]2025-10-08 18:26:56,153 Stage: Train 0.5 | Epoch: 24 | Iter: 64400 | Total Loss: 0.003979 | Recon Loss: 0.003586 | Commit Loss: 0.000786 | Perplexity: 2997.248422
2025-10-08 18:30:45,865 Stage: Train 0.5 | Epoch: 24 | Iter: 64600 | Total Loss: 0.004097 | Recon Loss: 0.003700 | Commit Loss: 0.000795 | Perplexity: 3003.243229
2025-10-08 18:34:35,888 Stage: Train 0.5 | Epoch: 24 | Iter: 64800 | Total Loss: 0.004032 | Recon Loss: 0.003640 | Commit Loss: 0.000784 | Perplexity: 2994.309088
2025-10-08 18:38:25,883 Stage: Train 0.5 | Epoch: 24 | Iter: 65000 | Total Loss: 0.004030 | Recon Loss: 0.003634 | Commit Loss: 0.000792 | Perplexity: 3001.690281
2025-10-08 18:42:20,287 Stage: Train 0.5 | Epoch: 24 | Iter: 65200 | Total Loss: 0.004007 | Recon Loss: 0.003614 | Commit Loss: 0.000786 | Perplexity: 3000.044539
Trainning Epoch:  11%|█         | 52/494 [8:08:40<143:55:45, 1172.27s/it]Trainning Epoch:  11%|█         | 52/494 [8:08:40<143:55:45, 1172.27s/it]2025-10-08 18:46:13,921 Stage: Train 0.5 | Epoch: 25 | Iter: 65400 | Total Loss: 0.004018 | Recon Loss: 0.003631 | Commit Loss: 0.000773 | Perplexity: 2994.701108
2025-10-08 18:50:05,160 Stage: Train 0.5 | Epoch: 25 | Iter: 65600 | Total Loss: 0.004028 | Recon Loss: 0.003635 | Commit Loss: 0.000785 | Perplexity: 2990.276115
2025-10-08 18:53:55,577 Stage: Train 0.5 | Epoch: 25 | Iter: 65800 | Total Loss: 0.003971 | Recon Loss: 0.003576 | Commit Loss: 0.000789 | Perplexity: 2985.687194
2025-10-08 18:57:45,457 Stage: Train 0.5 | Epoch: 25 | Iter: 66000 | Total Loss: 0.004048 | Recon Loss: 0.003659 | Commit Loss: 0.000779 | Perplexity: 2996.872767
2025-10-08 19:01:36,162 Stage: Train 0.5 | Epoch: 25 | Iter: 66200 | Total Loss: 0.004055 | Recon Loss: 0.003657 | Commit Loss: 0.000796 | Perplexity: 2996.087317
Trainning Epoch:  11%|█         | 53/494 [8:28:12<143:33:42, 1171.93s/it]Trainning Epoch:  11%|█         | 53/494 [8:28:12<143:33:41, 1171.93s/it]2025-10-08 19:05:30,298 Stage: Train 0.5 | Epoch: 26 | Iter: 66400 | Total Loss: 0.004011 | Recon Loss: 0.003619 | Commit Loss: 0.000783 | Perplexity: 2989.694689
2025-10-08 19:09:20,599 Stage: Train 0.5 | Epoch: 26 | Iter: 66600 | Total Loss: 0.003975 | Recon Loss: 0.003583 | Commit Loss: 0.000785 | Perplexity: 2987.194946
2025-10-08 19:13:10,236 Stage: Train 0.5 | Epoch: 26 | Iter: 66800 | Total Loss: 0.003953 | Recon Loss: 0.003558 | Commit Loss: 0.000790 | Perplexity: 2995.704115
2025-10-08 19:17:00,014 Stage: Train 0.5 | Epoch: 26 | Iter: 67000 | Total Loss: 0.003989 | Recon Loss: 0.003596 | Commit Loss: 0.000786 | Perplexity: 2999.301521
2025-10-08 19:20:50,507 Stage: Train 0.5 | Epoch: 26 | Iter: 67200 | Total Loss: 0.004087 | Recon Loss: 0.003693 | Commit Loss: 0.000787 | Perplexity: 2983.517731
Trainning Epoch:  11%|█         | 54/494 [8:47:41<143:08:32, 1171.17s/it]Trainning Epoch:  11%|█         | 54/494 [8:47:41<143:08:33, 1171.17s/it]2025-10-08 19:24:44,224 Stage: Train 0.5 | Epoch: 27 | Iter: 67400 | Total Loss: 0.003974 | Recon Loss: 0.003576 | Commit Loss: 0.000795 | Perplexity: 2994.159237
2025-10-08 19:28:33,830 Stage: Train 0.5 | Epoch: 27 | Iter: 67600 | Total Loss: 0.003985 | Recon Loss: 0.003593 | Commit Loss: 0.000783 | Perplexity: 2994.906783
2025-10-08 19:32:22,342 Stage: Train 0.5 | Epoch: 27 | Iter: 67800 | Total Loss: 0.004042 | Recon Loss: 0.003651 | Commit Loss: 0.000782 | Perplexity: 3005.595826
2025-10-08 19:36:11,016 Stage: Train 0.5 | Epoch: 27 | Iter: 68000 | Total Loss: 0.003927 | Recon Loss: 0.003532 | Commit Loss: 0.000790 | Perplexity: 2993.888645
2025-10-08 19:40:00,953 Stage: Train 0.5 | Epoch: 27 | Iter: 68200 | Total Loss: 0.004036 | Recon Loss: 0.003650 | Commit Loss: 0.000772 | Perplexity: 2992.893965
Trainning Epoch:  11%|█         | 55/494 [9:07:06<142:35:39, 1169.34s/it]Trainning Epoch:  11%|█         | 55/494 [9:07:06<142:35:39, 1169.34s/it]2025-10-08 19:43:54,313 Stage: Train 0.5 | Epoch: 28 | Iter: 68400 | Total Loss: 0.003898 | Recon Loss: 0.003511 | Commit Loss: 0.000775 | Perplexity: 2995.078903
2025-10-08 19:47:43,145 Stage: Train 0.5 | Epoch: 28 | Iter: 68600 | Total Loss: 0.003978 | Recon Loss: 0.003591 | Commit Loss: 0.000774 | Perplexity: 2996.191853
2025-10-08 19:51:32,027 Stage: Train 0.5 | Epoch: 28 | Iter: 68800 | Total Loss: 0.003937 | Recon Loss: 0.003538 | Commit Loss: 0.000798 | Perplexity: 3014.237842
2025-10-08 19:55:20,950 Stage: Train 0.5 | Epoch: 28 | Iter: 69000 | Total Loss: 0.003955 | Recon Loss: 0.003561 | Commit Loss: 0.000788 | Perplexity: 2999.536071
2025-10-08 19:59:11,080 Stage: Train 0.5 | Epoch: 28 | Iter: 69200 | Total Loss: 0.003996 | Recon Loss: 0.003610 | Commit Loss: 0.000771 | Perplexity: 2989.936328
Trainning Epoch:  11%|█▏        | 56/494 [9:26:30<142:04:49, 1167.78s/it]Trainning Epoch:  11%|█▏        | 56/494 [9:26:30<142:04:50, 1167.79s/it]2025-10-08 20:03:03,384 Stage: Train 0.5 | Epoch: 29 | Iter: 69400 | Total Loss: 0.003960 | Recon Loss: 0.003565 | Commit Loss: 0.000789 | Perplexity: 2992.826669
2025-10-08 20:06:54,682 Stage: Train 0.5 | Epoch: 29 | Iter: 69600 | Total Loss: 0.003905 | Recon Loss: 0.003514 | Commit Loss: 0.000781 | Perplexity: 3005.687441
2025-10-08 20:10:44,513 Stage: Train 0.5 | Epoch: 29 | Iter: 69800 | Total Loss: 0.003963 | Recon Loss: 0.003574 | Commit Loss: 0.000778 | Perplexity: 3003.364613
2025-10-08 20:14:35,023 Stage: Train 0.5 | Epoch: 29 | Iter: 70000 | Total Loss: 0.004005 | Recon Loss: 0.003613 | Commit Loss: 0.000785 | Perplexity: 3003.806504
2025-10-08 20:18:24,961 Stage: Train 0.5 | Epoch: 29 | Iter: 70200 | Total Loss: 0.003961 | Recon Loss: 0.003566 | Commit Loss: 0.000790 | Perplexity: 3001.806085
Trainning Epoch:  12%|█▏        | 57/494 [9:45:59<141:48:07, 1168.16s/it]Trainning Epoch:  12%|█▏        | 57/494 [9:45:59<141:48:07, 1168.16s/it]2025-10-08 20:22:17,616 Stage: Train 0.5 | Epoch: 30 | Iter: 70400 | Total Loss: 0.003878 | Recon Loss: 0.003488 | Commit Loss: 0.000780 | Perplexity: 3007.742358
2025-10-08 20:26:08,123 Stage: Train 0.5 | Epoch: 30 | Iter: 70600 | Total Loss: 0.003857 | Recon Loss: 0.003466 | Commit Loss: 0.000782 | Perplexity: 2991.537246
2025-10-08 20:29:57,622 Stage: Train 0.5 | Epoch: 30 | Iter: 70800 | Total Loss: 0.003911 | Recon Loss: 0.003512 | Commit Loss: 0.000799 | Perplexity: 2988.108595
2025-10-08 20:33:48,497 Stage: Train 0.5 | Epoch: 30 | Iter: 71000 | Total Loss: 0.003964 | Recon Loss: 0.003576 | Commit Loss: 0.000776 | Perplexity: 3005.064532
2025-10-08 20:37:38,306 Stage: Train 0.5 | Epoch: 30 | Iter: 71200 | Total Loss: 0.003974 | Recon Loss: 0.003586 | Commit Loss: 0.000775 | Perplexity: 2996.205913
2025-10-08 20:41:28,027 Stage: Train 0.5 | Epoch: 30 | Iter: 71400 | Total Loss: 0.003917 | Recon Loss: 0.003520 | Commit Loss: 0.000793 | Perplexity: 3009.263866
Trainning Epoch:  12%|█▏        | 58/494 [10:05:29<141:31:12, 1168.51s/it]Trainning Epoch:  12%|█▏        | 58/494 [10:05:29<141:31:12, 1168.51s/it]2025-10-08 20:45:22,228 Stage: Train 0.5 | Epoch: 31 | Iter: 71600 | Total Loss: 0.003885 | Recon Loss: 0.003491 | Commit Loss: 0.000789 | Perplexity: 2994.021519
2025-10-08 20:49:12,173 Stage: Train 0.5 | Epoch: 31 | Iter: 71800 | Total Loss: 0.003864 | Recon Loss: 0.003466 | Commit Loss: 0.000797 | Perplexity: 3000.686493
2025-10-08 20:53:02,913 Stage: Train 0.5 | Epoch: 31 | Iter: 72000 | Total Loss: 0.003877 | Recon Loss: 0.003485 | Commit Loss: 0.000785 | Perplexity: 3006.422867
2025-10-08 20:56:53,248 Stage: Train 0.5 | Epoch: 31 | Iter: 72200 | Total Loss: 0.003890 | Recon Loss: 0.003502 | Commit Loss: 0.000776 | Perplexity: 3009.638345
2025-10-08 21:00:42,957 Stage: Train 0.5 | Epoch: 31 | Iter: 72400 | Total Loss: 0.003899 | Recon Loss: 0.003504 | Commit Loss: 0.000789 | Perplexity: 2993.509081
Trainning Epoch:  12%|█▏        | 59/494 [10:24:58<141:14:15, 1168.86s/it]Trainning Epoch:  12%|█▏        | 59/494 [10:24:58<141:14:15, 1168.86s/it]2025-10-08 21:04:36,981 Stage: Train 0.5 | Epoch: 32 | Iter: 72600 | Total Loss: 0.003890 | Recon Loss: 0.003503 | Commit Loss: 0.000775 | Perplexity: 3000.375753
2025-10-08 21:08:26,534 Stage: Train 0.5 | Epoch: 32 | Iter: 72800 | Total Loss: 0.003920 | Recon Loss: 0.003529 | Commit Loss: 0.000783 | Perplexity: 3000.550841
2025-10-08 21:12:16,659 Stage: Train 0.5 | Epoch: 32 | Iter: 73000 | Total Loss: 0.003871 | Recon Loss: 0.003478 | Commit Loss: 0.000786 | Perplexity: 3018.444452
2025-10-08 21:16:05,927 Stage: Train 0.5 | Epoch: 32 | Iter: 73200 | Total Loss: 0.003917 | Recon Loss: 0.003525 | Commit Loss: 0.000782 | Perplexity: 3007.374581
2025-10-08 21:19:54,891 Stage: Train 0.5 | Epoch: 32 | Iter: 73400 | Total Loss: 0.003915 | Recon Loss: 0.003517 | Commit Loss: 0.000797 | Perplexity: 3009.711627
Trainning Epoch:  12%|█▏        | 60/494 [10:44:25<140:50:25, 1168.26s/it]Trainning Epoch:  12%|█▏        | 60/494 [10:44:25<140:50:25, 1168.26s/it]2025-10-08 21:23:50,149 Stage: Train 0.5 | Epoch: 33 | Iter: 73600 | Total Loss: 0.003809 | Recon Loss: 0.003412 | Commit Loss: 0.000794 | Perplexity: 3012.539990
2025-10-08 21:27:41,381 Stage: Train 0.5 | Epoch: 33 | Iter: 73800 | Total Loss: 0.003874 | Recon Loss: 0.003483 | Commit Loss: 0.000782 | Perplexity: 3008.279242
2025-10-08 21:31:32,459 Stage: Train 0.5 | Epoch: 33 | Iter: 74000 | Total Loss: 0.003901 | Recon Loss: 0.003512 | Commit Loss: 0.000778 | Perplexity: 3009.151073
2025-10-08 21:35:23,317 Stage: Train 0.5 | Epoch: 33 | Iter: 74200 | Total Loss: 0.003836 | Recon Loss: 0.003445 | Commit Loss: 0.000783 | Perplexity: 3003.440814
2025-10-08 21:39:14,395 Stage: Train 0.5 | Epoch: 33 | Iter: 74400 | Total Loss: 0.003923 | Recon Loss: 0.003538 | Commit Loss: 0.000771 | Perplexity: 3008.433320
Trainning Epoch:  12%|█▏        | 61/494 [11:04:00<140:45:30, 1170.28s/it]Trainning Epoch:  12%|█▏        | 61/494 [11:04:00<140:45:30, 1170.28s/it]2025-10-08 21:43:08,406 Stage: Train 0.5 | Epoch: 34 | Iter: 74600 | Total Loss: 0.003840 | Recon Loss: 0.003444 | Commit Loss: 0.000793 | Perplexity: 2998.919539
2025-10-08 21:46:58,636 Stage: Train 0.5 | Epoch: 34 | Iter: 74800 | Total Loss: 0.003777 | Recon Loss: 0.003377 | Commit Loss: 0.000799 | Perplexity: 3005.980884
2025-10-08 21:50:47,481 Stage: Train 0.5 | Epoch: 34 | Iter: 75000 | Total Loss: 0.003828 | Recon Loss: 0.003434 | Commit Loss: 0.000788 | Perplexity: 3008.175775
2025-10-08 21:54:37,079 Stage: Train 0.5 | Epoch: 34 | Iter: 75200 | Total Loss: 0.003829 | Recon Loss: 0.003439 | Commit Loss: 0.000778 | Perplexity: 3007.586339
2025-10-08 21:58:26,441 Stage: Train 0.5 | Epoch: 34 | Iter: 75400 | Total Loss: 0.003898 | Recon Loss: 0.003512 | Commit Loss: 0.000773 | Perplexity: 3004.211263
Trainning Epoch:  13%|█▎        | 62/494 [11:23:26<140:17:04, 1169.04s/it]Trainning Epoch:  13%|█▎        | 62/494 [11:23:26<140:17:04, 1169.04s/it]2025-10-08 22:02:23,787 Stage: Train 0.5 | Epoch: 35 | Iter: 75600 | Total Loss: 0.003791 | Recon Loss: 0.003406 | Commit Loss: 0.000769 | Perplexity: 3005.536490
2025-10-08 22:06:19,218 Stage: Train 0.5 | Epoch: 35 | Iter: 75800 | Total Loss: 0.003875 | Recon Loss: 0.003484 | Commit Loss: 0.000783 | Perplexity: 3011.661661
2025-10-08 22:10:12,876 Stage: Train 0.5 | Epoch: 35 | Iter: 76000 | Total Loss: 0.003853 | Recon Loss: 0.003456 | Commit Loss: 0.000793 | Perplexity: 3006.153983
2025-10-08 22:14:06,482 Stage: Train 0.5 | Epoch: 35 | Iter: 76200 | Total Loss: 0.003826 | Recon Loss: 0.003440 | Commit Loss: 0.000772 | Perplexity: 3003.350751
2025-10-08 22:17:59,224 Stage: Train 0.5 | Epoch: 35 | Iter: 76400 | Total Loss: 0.003809 | Recon Loss: 0.003410 | Commit Loss: 0.000797 | Perplexity: 3008.831638
Trainning Epoch:  13%|█▎        | 63/494 [11:43:15<140:39:58, 1174.94s/it]Trainning Epoch:  13%|█▎        | 63/494 [11:43:15<140:39:58, 1174.94s/it]2025-10-08 22:21:52,998 Stage: Train 0.5 | Epoch: 36 | Iter: 76600 | Total Loss: 0.003845 | Recon Loss: 0.003462 | Commit Loss: 0.000766 | Perplexity: 3015.865254
2025-10-08 22:25:44,017 Stage: Train 0.5 | Epoch: 36 | Iter: 76800 | Total Loss: 0.003865 | Recon Loss: 0.003469 | Commit Loss: 0.000792 | Perplexity: 3000.659357
2025-10-08 22:29:33,446 Stage: Train 0.5 | Epoch: 36 | Iter: 77000 | Total Loss: 0.003855 | Recon Loss: 0.003465 | Commit Loss: 0.000779 | Perplexity: 3010.340537
2025-10-08 22:33:22,691 Stage: Train 0.5 | Epoch: 36 | Iter: 77200 | Total Loss: 0.003791 | Recon Loss: 0.003397 | Commit Loss: 0.000789 | Perplexity: 3006.708688
2025-10-08 22:37:11,652 Stage: Train 0.5 | Epoch: 36 | Iter: 77400 | Total Loss: 0.003849 | Recon Loss: 0.003459 | Commit Loss: 0.000782 | Perplexity: 3016.975330
Trainning Epoch:  13%|█▎        | 64/494 [12:02:41<140:01:30, 1172.30s/it]Trainning Epoch:  13%|█▎        | 64/494 [12:02:41<140:01:30, 1172.30s/it]2025-10-08 22:41:04,643 Stage: Train 0.5 | Epoch: 37 | Iter: 77600 | Total Loss: 0.003787 | Recon Loss: 0.003405 | Commit Loss: 0.000764 | Perplexity: 2998.042448
2025-10-08 22:44:54,496 Stage: Train 0.5 | Epoch: 37 | Iter: 77800 | Total Loss: 0.003732 | Recon Loss: 0.003339 | Commit Loss: 0.000785 | Perplexity: 3017.114740
2025-10-08 22:48:42,845 Stage: Train 0.5 | Epoch: 37 | Iter: 78000 | Total Loss: 0.003751 | Recon Loss: 0.003363 | Commit Loss: 0.000777 | Perplexity: 3010.149594
2025-10-08 22:52:31,354 Stage: Train 0.5 | Epoch: 37 | Iter: 78200 | Total Loss: 0.003754 | Recon Loss: 0.003370 | Commit Loss: 0.000768 | Perplexity: 3012.375924
2025-10-08 22:56:20,228 Stage: Train 0.5 | Epoch: 37 | Iter: 78400 | Total Loss: 0.003775 | Recon Loss: 0.003381 | Commit Loss: 0.000788 | Perplexity: 3019.600553
Trainning Epoch:  13%|█▎        | 65/494 [12:22:05<139:23:26, 1169.71s/it]Trainning Epoch:  13%|█▎        | 65/494 [12:22:05<139:23:26, 1169.71s/it]2025-10-08 23:00:15,761 Stage: Train 0.5 | Epoch: 38 | Iter: 78600 | Total Loss: 0.003730 | Recon Loss: 0.003338 | Commit Loss: 0.000784 | Perplexity: 3017.005585
2025-10-08 23:04:08,192 Stage: Train 0.5 | Epoch: 38 | Iter: 78800 | Total Loss: 0.003798 | Recon Loss: 0.003411 | Commit Loss: 0.000775 | Perplexity: 3018.583726
2025-10-08 23:07:59,907 Stage: Train 0.5 | Epoch: 38 | Iter: 79000 | Total Loss: 0.003791 | Recon Loss: 0.003404 | Commit Loss: 0.000774 | Perplexity: 3006.935796
2025-10-08 23:11:51,372 Stage: Train 0.5 | Epoch: 38 | Iter: 79200 | Total Loss: 0.003740 | Recon Loss: 0.003348 | Commit Loss: 0.000784 | Perplexity: 3014.812241
2025-10-08 23:15:41,647 Stage: Train 0.5 | Epoch: 38 | Iter: 79400 | Total Loss: 0.003700 | Recon Loss: 0.003312 | Commit Loss: 0.000777 | Perplexity: 3014.738472
Trainning Epoch:  13%|█▎        | 66/494 [12:41:43<139:21:34, 1172.18s/it]Trainning Epoch:  13%|█▎        | 66/494 [12:41:43<139:21:34, 1172.18s/it]2025-10-08 23:19:37,068 Stage: Train 0.5 | Epoch: 39 | Iter: 79600 | Total Loss: 0.003837 | Recon Loss: 0.003450 | Commit Loss: 0.000772 | Perplexity: 3017.857206
2025-10-08 23:23:25,931 Stage: Train 0.5 | Epoch: 39 | Iter: 79800 | Total Loss: 0.003735 | Recon Loss: 0.003345 | Commit Loss: 0.000779 | Perplexity: 3022.657545
2025-10-08 23:27:15,126 Stage: Train 0.5 | Epoch: 39 | Iter: 80000 | Total Loss: 0.003765 | Recon Loss: 0.003375 | Commit Loss: 0.000781 | Perplexity: 3021.271335
2025-10-08 23:27:15,127 Saving model at iteration 80000
2025-10-08 23:27:15,309 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000
2025-10-08 23:27:16,756 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000/model.safetensors
2025-10-08 23:27:18,468 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000/optimizer.bin
2025-10-08 23:27:18,468 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000/scheduler.bin
2025-10-08 23:27:18,469 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000/sampler.bin
2025-10-08 23:27:18,470 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_80000/random_states_0.pkl
2025-10-08 23:31:07,190 Stage: Train 0.5 | Epoch: 39 | Iter: 80200 | Total Loss: 0.003794 | Recon Loss: 0.003405 | Commit Loss: 0.000778 | Perplexity: 3000.859493
2025-10-08 23:34:57,012 Stage: Train 0.5 | Epoch: 39 | Iter: 80400 | Total Loss: 0.003795 | Recon Loss: 0.003408 | Commit Loss: 0.000774 | Perplexity: 3021.350609
Trainning Epoch:  14%|█▎        | 67/494 [13:01:12<138:55:56, 1171.33s/it]Trainning Epoch:  14%|█▎        | 67/494 [13:01:12<138:55:58, 1171.33s/it]2025-10-08 23:38:52,328 Stage: Train 0.5 | Epoch: 40 | Iter: 80600 | Total Loss: 0.003719 | Recon Loss: 0.003328 | Commit Loss: 0.000782 | Perplexity: 3028.706967
2025-10-08 23:42:44,943 Stage: Train 0.5 | Epoch: 40 | Iter: 80800 | Total Loss: 0.003673 | Recon Loss: 0.003281 | Commit Loss: 0.000784 | Perplexity: 3025.215775
2025-10-08 23:46:37,647 Stage: Train 0.5 | Epoch: 40 | Iter: 81000 | Total Loss: 0.003771 | Recon Loss: 0.003379 | Commit Loss: 0.000786 | Perplexity: 3024.549729
2025-10-08 23:50:30,966 Stage: Train 0.5 | Epoch: 40 | Iter: 81200 | Total Loss: 0.003695 | Recon Loss: 0.003306 | Commit Loss: 0.000779 | Perplexity: 3014.801144
2025-10-08 23:54:25,658 Stage: Train 0.5 | Epoch: 40 | Iter: 81400 | Total Loss: 0.003693 | Recon Loss: 0.003293 | Commit Loss: 0.000800 | Perplexity: 3015.664480
Trainning Epoch:  14%|█▍        | 68/494 [13:20:58<139:06:53, 1175.62s/it]Trainning Epoch:  14%|█▍        | 68/494 [13:20:58<139:06:54, 1175.62s/it]2025-10-08 23:58:22,434 Stage: Train 0.5 | Epoch: 41 | Iter: 81600 | Total Loss: 0.003722 | Recon Loss: 0.003327 | Commit Loss: 0.000791 | Perplexity: 3014.700410
2025-10-09 00:02:16,077 Stage: Train 0.5 | Epoch: 41 | Iter: 81800 | Total Loss: 0.003734 | Recon Loss: 0.003343 | Commit Loss: 0.000783 | Perplexity: 3015.645065
2025-10-09 00:06:07,430 Stage: Train 0.5 | Epoch: 41 | Iter: 82000 | Total Loss: 0.003719 | Recon Loss: 0.003337 | Commit Loss: 0.000765 | Perplexity: 3014.705696
2025-10-09 00:09:59,863 Stage: Train 0.5 | Epoch: 41 | Iter: 82200 | Total Loss: 0.003679 | Recon Loss: 0.003291 | Commit Loss: 0.000777 | Perplexity: 3024.524468
2025-10-09 00:13:52,917 Stage: Train 0.5 | Epoch: 41 | Iter: 82400 | Total Loss: 0.003668 | Recon Loss: 0.003278 | Commit Loss: 0.000780 | Perplexity: 3014.318004
Trainning Epoch:  14%|█▍        | 69/494 [13:40:40<139:02:25, 1177.75s/it]Trainning Epoch:  14%|█▍        | 69/494 [13:40:40<139:02:27, 1177.76s/it]2025-10-09 00:17:50,433 Stage: Train 0.5 | Epoch: 42 | Iter: 82600 | Total Loss: 0.003720 | Recon Loss: 0.003336 | Commit Loss: 0.000769 | Perplexity: 3025.409431
2025-10-09 00:21:43,969 Stage: Train 0.5 | Epoch: 42 | Iter: 82800 | Total Loss: 0.003769 | Recon Loss: 0.003375 | Commit Loss: 0.000789 | Perplexity: 3026.313921
2025-10-09 00:25:37,264 Stage: Train 0.5 | Epoch: 42 | Iter: 83000 | Total Loss: 0.003661 | Recon Loss: 0.003272 | Commit Loss: 0.000779 | Perplexity: 3025.366179
2025-10-09 00:29:30,380 Stage: Train 0.5 | Epoch: 42 | Iter: 83200 | Total Loss: 0.003762 | Recon Loss: 0.003369 | Commit Loss: 0.000784 | Perplexity: 3022.638909
2025-10-09 00:33:23,496 Stage: Train 0.5 | Epoch: 42 | Iter: 83400 | Total Loss: 0.003740 | Recon Loss: 0.003361 | Commit Loss: 0.000758 | Perplexity: 3025.330554
Trainning Epoch:  14%|█▍        | 70/494 [14:00:24<138:55:31, 1179.55s/it]Trainning Epoch:  14%|█▍        | 70/494 [14:00:24<138:55:34, 1179.56s/it]2025-10-09 00:37:19,206 Stage: Train 0.5 | Epoch: 43 | Iter: 83600 | Total Loss: 0.003652 | Recon Loss: 0.003265 | Commit Loss: 0.000774 | Perplexity: 3024.950956
2025-10-09 00:41:11,703 Stage: Train 0.5 | Epoch: 43 | Iter: 83800 | Total Loss: 0.003690 | Recon Loss: 0.003309 | Commit Loss: 0.000764 | Perplexity: 3020.019799
2025-10-09 00:45:01,926 Stage: Train 0.5 | Epoch: 43 | Iter: 84000 | Total Loss: 0.003681 | Recon Loss: 0.003286 | Commit Loss: 0.000790 | Perplexity: 3029.691825
2025-10-09 00:48:52,232 Stage: Train 0.5 | Epoch: 43 | Iter: 84200 | Total Loss: 0.003751 | Recon Loss: 0.003361 | Commit Loss: 0.000780 | Perplexity: 3029.264617
2025-10-09 00:52:43,404 Stage: Train 0.5 | Epoch: 43 | Iter: 84400 | Total Loss: 0.003730 | Recon Loss: 0.003347 | Commit Loss: 0.000766 | Perplexity: 3028.922190
Trainning Epoch:  14%|█▍        | 71/494 [14:19:59<138:26:28, 1178.22s/it]Trainning Epoch:  14%|█▍        | 71/494 [14:19:59<138:26:34, 1178.24s/it]2025-10-09 00:56:38,918 Stage: Train 0.5 | Epoch: 44 | Iter: 84600 | Total Loss: 0.003713 | Recon Loss: 0.003321 | Commit Loss: 0.000785 | Perplexity: 3024.381637
2025-10-09 01:00:29,220 Stage: Train 0.5 | Epoch: 44 | Iter: 84800 | Total Loss: 0.003659 | Recon Loss: 0.003270 | Commit Loss: 0.000778 | Perplexity: 3029.940333
2025-10-09 01:04:19,120 Stage: Train 0.5 | Epoch: 44 | Iter: 85000 | Total Loss: 0.003682 | Recon Loss: 0.003296 | Commit Loss: 0.000771 | Perplexity: 3023.375927
2025-10-09 01:08:08,841 Stage: Train 0.5 | Epoch: 44 | Iter: 85200 | Total Loss: 0.003624 | Recon Loss: 0.003233 | Commit Loss: 0.000781 | Perplexity: 3024.672974
2025-10-09 01:11:58,581 Stage: Train 0.5 | Epoch: 44 | Iter: 85400 | Total Loss: 0.003735 | Recon Loss: 0.003349 | Commit Loss: 0.000771 | Perplexity: 3017.116829
Trainning Epoch:  15%|█▍        | 72/494 [14:39:28<137:46:14, 1175.29s/it]Trainning Epoch:  15%|█▍        | 72/494 [14:39:28<137:46:13, 1175.29s/it]2025-10-09 01:15:52,528 Stage: Train 0.5 | Epoch: 45 | Iter: 85600 | Total Loss: 0.003677 | Recon Loss: 0.003282 | Commit Loss: 0.000790 | Perplexity: 3021.993002
2025-10-09 01:19:42,543 Stage: Train 0.5 | Epoch: 45 | Iter: 85800 | Total Loss: 0.003648 | Recon Loss: 0.003262 | Commit Loss: 0.000772 | Perplexity: 3027.041367
2025-10-09 01:23:31,529 Stage: Train 0.5 | Epoch: 45 | Iter: 86000 | Total Loss: 0.003700 | Recon Loss: 0.003305 | Commit Loss: 0.000790 | Perplexity: 3029.534619
2025-10-09 01:27:20,368 Stage: Train 0.5 | Epoch: 45 | Iter: 86200 | Total Loss: 0.003748 | Recon Loss: 0.003361 | Commit Loss: 0.000775 | Perplexity: 3030.856300
2025-10-09 01:31:10,157 Stage: Train 0.5 | Epoch: 45 | Iter: 86400 | Total Loss: 0.003653 | Recon Loss: 0.003265 | Commit Loss: 0.000777 | Perplexity: 3028.321689
Trainning Epoch:  15%|█▍        | 73/494 [14:58:55<137:08:52, 1172.76s/it]Trainning Epoch:  15%|█▍        | 73/494 [14:58:55<137:08:53, 1172.76s/it]2025-10-09 01:35:04,333 Stage: Train 0.5 | Epoch: 46 | Iter: 86600 | Total Loss: 0.003645 | Recon Loss: 0.003256 | Commit Loss: 0.000779 | Perplexity: 3018.647792
2025-10-09 01:38:54,624 Stage: Train 0.5 | Epoch: 46 | Iter: 86800 | Total Loss: 0.003708 | Recon Loss: 0.003321 | Commit Loss: 0.000773 | Perplexity: 3026.425789
2025-10-09 01:42:43,471 Stage: Train 0.5 | Epoch: 46 | Iter: 87000 | Total Loss: 0.003601 | Recon Loss: 0.003214 | Commit Loss: 0.000775 | Perplexity: 3035.641776
2025-10-09 01:46:33,324 Stage: Train 0.5 | Epoch: 46 | Iter: 87200 | Total Loss: 0.003690 | Recon Loss: 0.003308 | Commit Loss: 0.000764 | Perplexity: 3012.405193
2025-10-09 01:50:21,992 Stage: Train 0.5 | Epoch: 46 | Iter: 87400 | Total Loss: 0.003601 | Recon Loss: 0.003214 | Commit Loss: 0.000775 | Perplexity: 3027.158092
2025-10-09 01:54:10,918 Stage: Train 0.5 | Epoch: 46 | Iter: 87600 | Total Loss: 0.003642 | Recon Loss: 0.003258 | Commit Loss: 0.000768 | Perplexity: 3030.505410
Trainning Epoch:  15%|█▍        | 74/494 [15:18:20<136:34:45, 1170.68s/it]Trainning Epoch:  15%|█▍        | 74/494 [15:18:20<136:34:45, 1170.68s/it]2025-10-09 01:58:04,835 Stage: Train 0.5 | Epoch: 47 | Iter: 87800 | Total Loss: 0.003673 | Recon Loss: 0.003282 | Commit Loss: 0.000781 | Perplexity: 3027.191453
2025-10-09 02:01:53,610 Stage: Train 0.5 | Epoch: 47 | Iter: 88000 | Total Loss: 0.003631 | Recon Loss: 0.003240 | Commit Loss: 0.000782 | Perplexity: 3022.583160
2025-10-09 02:05:44,105 Stage: Train 0.5 | Epoch: 47 | Iter: 88200 | Total Loss: 0.003646 | Recon Loss: 0.003259 | Commit Loss: 0.000775 | Perplexity: 3037.856631
2025-10-09 02:09:33,549 Stage: Train 0.5 | Epoch: 47 | Iter: 88400 | Total Loss: 0.003584 | Recon Loss: 0.003200 | Commit Loss: 0.000768 | Perplexity: 3041.102155
2025-10-09 02:13:22,815 Stage: Train 0.5 | Epoch: 47 | Iter: 88600 | Total Loss: 0.003683 | Recon Loss: 0.003296 | Commit Loss: 0.000773 | Perplexity: 3030.561616
Trainning Epoch:  15%|█▌        | 75/494 [15:37:47<136:07:15, 1169.53s/it]Trainning Epoch:  15%|█▌        | 75/494 [15:37:47<136:07:15, 1169.54s/it]2025-10-09 02:17:18,251 Stage: Train 0.5 | Epoch: 48 | Iter: 88800 | Total Loss: 0.003606 | Recon Loss: 0.003225 | Commit Loss: 0.000763 | Perplexity: 3028.479830
2025-10-09 02:21:07,624 Stage: Train 0.5 | Epoch: 48 | Iter: 89000 | Total Loss: 0.003653 | Recon Loss: 0.003265 | Commit Loss: 0.000775 | Perplexity: 3032.154198
2025-10-09 02:24:58,527 Stage: Train 0.5 | Epoch: 48 | Iter: 89200 | Total Loss: 0.003545 | Recon Loss: 0.003153 | Commit Loss: 0.000784 | Perplexity: 3023.552491
2025-10-09 02:28:48,676 Stage: Train 0.5 | Epoch: 48 | Iter: 89400 | Total Loss: 0.003596 | Recon Loss: 0.003208 | Commit Loss: 0.000777 | Perplexity: 3042.454351
2025-10-09 02:32:39,740 Stage: Train 0.5 | Epoch: 48 | Iter: 89600 | Total Loss: 0.003581 | Recon Loss: 0.003193 | Commit Loss: 0.000777 | Perplexity: 3029.512330
Trainning Epoch:  15%|█▌        | 76/494 [15:57:19<135:52:25, 1170.20s/it]Trainning Epoch:  15%|█▌        | 76/494 [15:57:19<135:52:25, 1170.20s/it]2025-10-09 02:36:33,876 Stage: Train 0.5 | Epoch: 49 | Iter: 89800 | Total Loss: 0.003651 | Recon Loss: 0.003266 | Commit Loss: 0.000769 | Perplexity: 3036.284888
2025-10-09 02:40:24,661 Stage: Train 0.5 | Epoch: 49 | Iter: 90000 | Total Loss: 0.003557 | Recon Loss: 0.003165 | Commit Loss: 0.000785 | Perplexity: 3026.587659
2025-10-09 02:44:15,315 Stage: Train 0.5 | Epoch: 49 | Iter: 90200 | Total Loss: 0.003527 | Recon Loss: 0.003132 | Commit Loss: 0.000789 | Perplexity: 3030.305052
2025-10-09 02:48:04,262 Stage: Train 0.5 | Epoch: 49 | Iter: 90400 | Total Loss: 0.003626 | Recon Loss: 0.003237 | Commit Loss: 0.000778 | Perplexity: 3040.700736
2025-10-09 02:51:53,713 Stage: Train 0.5 | Epoch: 49 | Iter: 90600 | Total Loss: 0.003644 | Recon Loss: 0.003253 | Commit Loss: 0.000782 | Perplexity: 3039.861678
Trainning Epoch:  16%|█▌        | 77/494 [16:16:48<135:29:58, 1169.78s/it]Trainning Epoch:  16%|█▌        | 77/494 [16:16:48<135:29:58, 1169.78s/it]2025-10-09 02:55:50,250 Stage: Train 0.5 | Epoch: 50 | Iter: 90800 | Total Loss: 0.003563 | Recon Loss: 0.003166 | Commit Loss: 0.000796 | Perplexity: 3027.845692
2025-10-09 02:59:43,756 Stage: Train 0.5 | Epoch: 50 | Iter: 91000 | Total Loss: 0.003538 | Recon Loss: 0.003153 | Commit Loss: 0.000770 | Perplexity: 3031.571533
2025-10-09 03:03:35,029 Stage: Train 0.5 | Epoch: 50 | Iter: 91200 | Total Loss: 0.003585 | Recon Loss: 0.003194 | Commit Loss: 0.000782 | Perplexity: 3034.001721
2025-10-09 03:07:25,929 Stage: Train 0.5 | Epoch: 50 | Iter: 91400 | Total Loss: 0.003710 | Recon Loss: 0.003325 | Commit Loss: 0.000769 | Perplexity: 3033.355483
2025-10-09 03:11:16,522 Stage: Train 0.5 | Epoch: 50 | Iter: 91600 | Total Loss: 0.003556 | Recon Loss: 0.003163 | Commit Loss: 0.000786 | Perplexity: 3033.503234
Trainning Epoch:  16%|█▌        | 78/494 [16:36:27<135:29:41, 1172.55s/it]Trainning Epoch:  16%|█▌        | 78/494 [16:36:27<135:29:41, 1172.55s/it]2025-10-09 03:15:11,999 Stage: Train 0.5 | Epoch: 51 | Iter: 91800 | Total Loss: 0.003526 | Recon Loss: 0.003135 | Commit Loss: 0.000782 | Perplexity: 3042.223171
2025-10-09 03:19:02,548 Stage: Train 0.5 | Epoch: 51 | Iter: 92000 | Total Loss: 0.003618 | Recon Loss: 0.003231 | Commit Loss: 0.000776 | Perplexity: 3026.746216
2025-10-09 03:22:51,700 Stage: Train 0.5 | Epoch: 51 | Iter: 92200 | Total Loss: 0.003572 | Recon Loss: 0.003183 | Commit Loss: 0.000777 | Perplexity: 3032.638669
2025-10-09 03:26:40,532 Stage: Train 0.5 | Epoch: 51 | Iter: 92400 | Total Loss: 0.003572 | Recon Loss: 0.003183 | Commit Loss: 0.000778 | Perplexity: 3029.146390
2025-10-09 03:30:29,886 Stage: Train 0.5 | Epoch: 51 | Iter: 92600 | Total Loss: 0.003623 | Recon Loss: 0.003234 | Commit Loss: 0.000778 | Perplexity: 3041.636058
Trainning Epoch:  16%|█▌        | 79/494 [16:55:54<134:58:28, 1170.86s/it]Trainning Epoch:  16%|█▌        | 79/494 [16:55:54<134:58:29, 1170.87s/it]2025-10-09 03:34:25,224 Stage: Train 0.5 | Epoch: 52 | Iter: 92800 | Total Loss: 0.003519 | Recon Loss: 0.003134 | Commit Loss: 0.000770 | Perplexity: 3039.236563
2025-10-09 03:38:18,029 Stage: Train 0.5 | Epoch: 52 | Iter: 93000 | Total Loss: 0.003565 | Recon Loss: 0.003175 | Commit Loss: 0.000781 | Perplexity: 3036.432677
2025-10-09 03:42:09,551 Stage: Train 0.5 | Epoch: 52 | Iter: 93200 | Total Loss: 0.003556 | Recon Loss: 0.003170 | Commit Loss: 0.000772 | Perplexity: 3034.583116
2025-10-09 03:46:01,219 Stage: Train 0.5 | Epoch: 52 | Iter: 93400 | Total Loss: 0.003609 | Recon Loss: 0.003223 | Commit Loss: 0.000772 | Perplexity: 3034.615529
2025-10-09 03:49:52,961 Stage: Train 0.5 | Epoch: 52 | Iter: 93600 | Total Loss: 0.003513 | Recon Loss: 0.003122 | Commit Loss: 0.000782 | Perplexity: 3055.888966
Trainning Epoch:  16%|█▌        | 80/494 [17:15:32<134:55:03, 1173.20s/it]Trainning Epoch:  16%|█▌        | 80/494 [17:15:32<134:55:03, 1173.20s/it]2025-10-09 03:53:48,820 Stage: Train 0.5 | Epoch: 53 | Iter: 93800 | Total Loss: 0.003505 | Recon Loss: 0.003115 | Commit Loss: 0.000780 | Perplexity: 3041.156986
2025-10-09 03:57:42,157 Stage: Train 0.5 | Epoch: 53 | Iter: 94000 | Total Loss: 0.003525 | Recon Loss: 0.003136 | Commit Loss: 0.000778 | Perplexity: 3044.369601
2025-10-09 04:01:33,388 Stage: Train 0.5 | Epoch: 53 | Iter: 94200 | Total Loss: 0.003485 | Recon Loss: 0.003099 | Commit Loss: 0.000771 | Perplexity: 3042.100051
2025-10-09 04:05:24,253 Stage: Train 0.5 | Epoch: 53 | Iter: 94400 | Total Loss: 0.003541 | Recon Loss: 0.003153 | Commit Loss: 0.000776 | Perplexity: 3038.732958
2025-10-09 04:09:14,489 Stage: Train 0.5 | Epoch: 53 | Iter: 94600 | Total Loss: 0.003510 | Recon Loss: 0.003120 | Commit Loss: 0.000779 | Perplexity: 3046.033624
Trainning Epoch:  16%|█▋        | 81/494 [17:35:09<134:42:13, 1174.17s/it]Trainning Epoch:  16%|█▋        | 81/494 [17:35:09<134:42:12, 1174.17s/it]2025-10-09 04:13:09,333 Stage: Train 0.5 | Epoch: 54 | Iter: 94800 | Total Loss: 0.003561 | Recon Loss: 0.003174 | Commit Loss: 0.000773 | Perplexity: 3038.830118
2025-10-09 04:17:02,749 Stage: Train 0.5 | Epoch: 54 | Iter: 95000 | Total Loss: 0.003487 | Recon Loss: 0.003098 | Commit Loss: 0.000779 | Perplexity: 3051.458044
2025-10-09 04:20:54,342 Stage: Train 0.5 | Epoch: 54 | Iter: 95200 | Total Loss: 0.003536 | Recon Loss: 0.003148 | Commit Loss: 0.000776 | Perplexity: 3039.236906
2025-10-09 04:24:45,854 Stage: Train 0.5 | Epoch: 54 | Iter: 95400 | Total Loss: 0.003481 | Recon Loss: 0.003090 | Commit Loss: 0.000781 | Perplexity: 3036.997087
2025-10-09 04:28:37,103 Stage: Train 0.5 | Epoch: 54 | Iter: 95600 | Total Loss: 0.003530 | Recon Loss: 0.003137 | Commit Loss: 0.000787 | Perplexity: 3040.279509
Trainning Epoch:  17%|█▋        | 82/494 [17:54:47<134:30:24, 1175.30s/it]Trainning Epoch:  17%|█▋        | 82/494 [17:54:47<134:30:24, 1175.30s/it]2025-10-09 04:32:32,501 Stage: Train 0.5 | Epoch: 55 | Iter: 95800 | Total Loss: 0.003559 | Recon Loss: 0.003173 | Commit Loss: 0.000771 | Perplexity: 3042.449338
2025-10-09 04:36:23,485 Stage: Train 0.5 | Epoch: 55 | Iter: 96000 | Total Loss: 0.003500 | Recon Loss: 0.003115 | Commit Loss: 0.000769 | Perplexity: 3048.702164
2025-10-09 04:40:14,524 Stage: Train 0.5 | Epoch: 55 | Iter: 96200 | Total Loss: 0.003561 | Recon Loss: 0.003176 | Commit Loss: 0.000771 | Perplexity: 3045.276057
2025-10-09 04:44:05,164 Stage: Train 0.5 | Epoch: 55 | Iter: 96400 | Total Loss: 0.003494 | Recon Loss: 0.003108 | Commit Loss: 0.000772 | Perplexity: 3044.696479
2025-10-09 04:47:56,211 Stage: Train 0.5 | Epoch: 55 | Iter: 96600 | Total Loss: 0.003515 | Recon Loss: 0.003125 | Commit Loss: 0.000781 | Perplexity: 3047.698419
Trainning Epoch:  17%|█▋        | 83/494 [18:14:22<134:09:34, 1175.12s/it]Trainning Epoch:  17%|█▋        | 83/494 [18:14:22<134:09:34, 1175.12s/it]2025-10-09 04:51:53,016 Stage: Train 0.5 | Epoch: 56 | Iter: 96800 | Total Loss: 0.003602 | Recon Loss: 0.003218 | Commit Loss: 0.000768 | Perplexity: 3051.873229
2025-10-09 04:55:45,733 Stage: Train 0.5 | Epoch: 56 | Iter: 97000 | Total Loss: 0.003485 | Recon Loss: 0.003090 | Commit Loss: 0.000791 | Perplexity: 3043.117072
2025-10-09 04:59:37,309 Stage: Train 0.5 | Epoch: 56 | Iter: 97200 | Total Loss: 0.003513 | Recon Loss: 0.003125 | Commit Loss: 0.000776 | Perplexity: 3041.108352
2025-10-09 05:03:28,075 Stage: Train 0.5 | Epoch: 56 | Iter: 97400 | Total Loss: 0.003473 | Recon Loss: 0.003082 | Commit Loss: 0.000781 | Perplexity: 3054.182976
2025-10-09 05:07:19,293 Stage: Train 0.5 | Epoch: 56 | Iter: 97600 | Total Loss: 0.003562 | Recon Loss: 0.003182 | Commit Loss: 0.000761 | Perplexity: 3052.144264
Trainning Epoch:  17%|█▋        | 84/494 [18:34:00<133:57:28, 1176.22s/it]Trainning Epoch:  17%|█▋        | 84/494 [18:34:00<133:57:28, 1176.22s/it]2025-10-09 05:11:14,915 Stage: Train 0.5 | Epoch: 57 | Iter: 97800 | Total Loss: 0.003505 | Recon Loss: 0.003125 | Commit Loss: 0.000758 | Perplexity: 3044.983442
2025-10-09 05:15:05,370 Stage: Train 0.5 | Epoch: 57 | Iter: 98000 | Total Loss: 0.003456 | Recon Loss: 0.003071 | Commit Loss: 0.000770 | Perplexity: 3049.705864
2025-10-09 05:18:55,243 Stage: Train 0.5 | Epoch: 57 | Iter: 98200 | Total Loss: 0.003485 | Recon Loss: 0.003101 | Commit Loss: 0.000768 | Perplexity: 3060.793114
2025-10-09 05:22:45,175 Stage: Train 0.5 | Epoch: 57 | Iter: 98400 | Total Loss: 0.003420 | Recon Loss: 0.003030 | Commit Loss: 0.000779 | Perplexity: 3047.159822
2025-10-09 05:26:35,658 Stage: Train 0.5 | Epoch: 57 | Iter: 98600 | Total Loss: 0.003518 | Recon Loss: 0.003129 | Commit Loss: 0.000778 | Perplexity: 3042.581804
Trainning Epoch:  17%|█▋        | 85/494 [18:53:31<133:26:08, 1174.50s/it]Trainning Epoch:  17%|█▋        | 85/494 [18:53:31<133:26:08, 1174.50s/it]2025-10-09 05:30:31,027 Stage: Train 0.5 | Epoch: 58 | Iter: 98800 | Total Loss: 0.003595 | Recon Loss: 0.003211 | Commit Loss: 0.000768 | Perplexity: 3034.767770
2025-10-09 05:34:20,106 Stage: Train 0.5 | Epoch: 58 | Iter: 99000 | Total Loss: 0.003502 | Recon Loss: 0.003118 | Commit Loss: 0.000768 | Perplexity: 3045.033176
2025-10-09 05:38:08,944 Stage: Train 0.5 | Epoch: 58 | Iter: 99200 | Total Loss: 0.003475 | Recon Loss: 0.003089 | Commit Loss: 0.000773 | Perplexity: 3050.383901
2025-10-09 05:41:58,360 Stage: Train 0.5 | Epoch: 58 | Iter: 99400 | Total Loss: 0.003537 | Recon Loss: 0.003155 | Commit Loss: 0.000764 | Perplexity: 3037.551204
2025-10-09 05:45:47,993 Stage: Train 0.5 | Epoch: 58 | Iter: 99600 | Total Loss: 0.003471 | Recon Loss: 0.003086 | Commit Loss: 0.000769 | Perplexity: 3050.614847
Trainning Epoch:  17%|█▋        | 86/494 [19:12:56<132:48:31, 1171.84s/it]Trainning Epoch:  17%|█▋        | 86/494 [19:12:56<132:48:31, 1171.84s/it]2025-10-09 05:49:41,621 Stage: Train 0.5 | Epoch: 59 | Iter: 99800 | Total Loss: 0.003479 | Recon Loss: 0.003100 | Commit Loss: 0.000757 | Perplexity: 3051.829979
2025-10-09 05:53:34,638 Stage: Train 0.5 | Epoch: 59 | Iter: 100000 | Total Loss: 0.003504 | Recon Loss: 0.003124 | Commit Loss: 0.000759 | Perplexity: 3059.890449
2025-10-09 05:53:34,638 Saving model at iteration 100000
2025-10-09 05:53:34,838 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000
2025-10-09 05:53:36,262 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000/model.safetensors
2025-10-09 05:53:37,997 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000/optimizer.bin
2025-10-09 05:53:37,998 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000/scheduler.bin
2025-10-09 05:53:37,998 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000/sampler.bin
2025-10-09 05:53:37,999 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_100000/random_states_0.pkl
2025-10-09 05:57:29,928 Stage: Train 0.5 | Epoch: 59 | Iter: 100200 | Total Loss: 0.003458 | Recon Loss: 0.003074 | Commit Loss: 0.000767 | Perplexity: 3058.068489
2025-10-09 06:01:20,977 Stage: Train 0.5 | Epoch: 59 | Iter: 100400 | Total Loss: 0.003430 | Recon Loss: 0.003049 | Commit Loss: 0.000761 | Perplexity: 3039.541332
2025-10-09 06:05:12,450 Stage: Train 0.5 | Epoch: 59 | Iter: 100600 | Total Loss: 0.003466 | Recon Loss: 0.003076 | Commit Loss: 0.000779 | Perplexity: 3057.803676
Trainning Epoch:  18%|█▊        | 87/494 [19:32:38<132:48:34, 1174.73s/it]Trainning Epoch:  18%|█▊        | 87/494 [19:32:38<132:48:35, 1174.73s/it]2025-10-09 06:09:07,039 Stage: Train 0.5 | Epoch: 60 | Iter: 100800 | Total Loss: 0.003473 | Recon Loss: 0.003087 | Commit Loss: 0.000770 | Perplexity: 3050.235603
2025-10-09 06:12:57,730 Stage: Train 0.5 | Epoch: 60 | Iter: 101000 | Total Loss: 0.003411 | Recon Loss: 0.003029 | Commit Loss: 0.000763 | Perplexity: 3050.559366
2025-10-09 06:16:49,237 Stage: Train 0.5 | Epoch: 60 | Iter: 101200 | Total Loss: 0.003489 | Recon Loss: 0.003107 | Commit Loss: 0.000765 | Perplexity: 3051.615299
2025-10-09 06:20:41,735 Stage: Train 0.5 | Epoch: 60 | Iter: 101400 | Total Loss: 0.003476 | Recon Loss: 0.003091 | Commit Loss: 0.000771 | Perplexity: 3059.338895
2025-10-09 06:24:33,574 Stage: Train 0.5 | Epoch: 60 | Iter: 101600 | Total Loss: 0.003496 | Recon Loss: 0.003111 | Commit Loss: 0.000771 | Perplexity: 3050.627236
Trainning Epoch:  18%|█▊        | 88/494 [19:52:14<132:32:19, 1175.22s/it]Trainning Epoch:  18%|█▊        | 88/494 [19:52:14<132:32:21, 1175.23s/it]2025-10-09 06:28:29,809 Stage: Train 0.5 | Epoch: 61 | Iter: 101800 | Total Loss: 0.003438 | Recon Loss: 0.003046 | Commit Loss: 0.000783 | Perplexity: 3065.671699
2025-10-09 06:32:22,515 Stage: Train 0.5 | Epoch: 61 | Iter: 102000 | Total Loss: 0.003424 | Recon Loss: 0.003044 | Commit Loss: 0.000760 | Perplexity: 3059.198677
2025-10-09 06:36:15,571 Stage: Train 0.5 | Epoch: 61 | Iter: 102200 | Total Loss: 0.003496 | Recon Loss: 0.003113 | Commit Loss: 0.000765 | Perplexity: 3047.872942
2025-10-09 06:40:08,784 Stage: Train 0.5 | Epoch: 61 | Iter: 102400 | Total Loss: 0.003491 | Recon Loss: 0.003107 | Commit Loss: 0.000769 | Perplexity: 3044.869139
2025-10-09 06:44:01,948 Stage: Train 0.5 | Epoch: 61 | Iter: 102600 | Total Loss: 0.003429 | Recon Loss: 0.003040 | Commit Loss: 0.000778 | Perplexity: 3072.992878
2025-10-09 06:47:54,982 Stage: Train 0.5 | Epoch: 61 | Iter: 102800 | Total Loss: 0.003473 | Recon Loss: 0.003080 | Commit Loss: 0.000785 | Perplexity: 3050.710122
Trainning Epoch:  18%|█▊        | 89/494 [20:11:59<132:32:05, 1178.09s/it]Trainning Epoch:  18%|█▊        | 89/494 [20:11:59<132:32:07, 1178.09s/it]2025-10-09 06:51:51,489 Stage: Train 0.5 | Epoch: 62 | Iter: 103000 | Total Loss: 0.003478 | Recon Loss: 0.003096 | Commit Loss: 0.000764 | Perplexity: 3048.776403
2025-10-09 06:55:44,381 Stage: Train 0.5 | Epoch: 62 | Iter: 103200 | Total Loss: 0.003449 | Recon Loss: 0.003055 | Commit Loss: 0.000789 | Perplexity: 3059.246632
2025-10-09 06:59:37,653 Stage: Train 0.5 | Epoch: 62 | Iter: 103400 | Total Loss: 0.003447 | Recon Loss: 0.003065 | Commit Loss: 0.000763 | Perplexity: 3060.542598
2025-10-09 07:03:31,061 Stage: Train 0.5 | Epoch: 62 | Iter: 103600 | Total Loss: 0.003446 | Recon Loss: 0.003064 | Commit Loss: 0.000763 | Perplexity: 3053.614679
2025-10-09 07:07:24,258 Stage: Train 0.5 | Epoch: 62 | Iter: 103800 | Total Loss: 0.003432 | Recon Loss: 0.003049 | Commit Loss: 0.000765 | Perplexity: 3064.202352
Trainning Epoch:  18%|█▊        | 90/494 [20:31:43<132:24:56, 1179.94s/it]Trainning Epoch:  18%|█▊        | 90/494 [20:31:43<132:24:56, 1179.94s/it]2025-10-09 07:11:21,155 Stage: Train 0.5 | Epoch: 63 | Iter: 104000 | Total Loss: 0.003497 | Recon Loss: 0.003106 | Commit Loss: 0.000782 | Perplexity: 3065.870509
2025-10-09 07:15:15,234 Stage: Train 0.5 | Epoch: 63 | Iter: 104200 | Total Loss: 0.003472 | Recon Loss: 0.003090 | Commit Loss: 0.000763 | Perplexity: 3054.744586
2025-10-09 07:19:09,166 Stage: Train 0.5 | Epoch: 63 | Iter: 104400 | Total Loss: 0.003474 | Recon Loss: 0.003099 | Commit Loss: 0.000748 | Perplexity: 3045.645094
2025-10-09 07:23:03,377 Stage: Train 0.5 | Epoch: 63 | Iter: 104600 | Total Loss: 0.003416 | Recon Loss: 0.003030 | Commit Loss: 0.000772 | Perplexity: 3047.598555
2025-10-09 07:26:57,402 Stage: Train 0.5 | Epoch: 63 | Iter: 104800 | Total Loss: 0.003413 | Recon Loss: 0.003022 | Commit Loss: 0.000782 | Perplexity: 3060.651049
Trainning Epoch:  18%|█▊        | 91/494 [20:51:32<132:22:49, 1182.55s/it]Trainning Epoch:  18%|█▊        | 91/494 [20:51:32<132:22:49, 1182.56s/it]2025-10-09 07:30:55,639 Stage: Train 0.5 | Epoch: 64 | Iter: 105000 | Total Loss: 0.003390 | Recon Loss: 0.003007 | Commit Loss: 0.000766 | Perplexity: 3060.012279
2025-10-09 07:34:49,441 Stage: Train 0.5 | Epoch: 64 | Iter: 105200 | Total Loss: 0.003414 | Recon Loss: 0.003032 | Commit Loss: 0.000764 | Perplexity: 3053.394298
2025-10-09 07:38:42,981 Stage: Train 0.5 | Epoch: 64 | Iter: 105400 | Total Loss: 0.003391 | Recon Loss: 0.003009 | Commit Loss: 0.000764 | Perplexity: 3062.706072
2025-10-09 07:42:36,097 Stage: Train 0.5 | Epoch: 64 | Iter: 105600 | Total Loss: 0.003425 | Recon Loss: 0.003037 | Commit Loss: 0.000775 | Perplexity: 3049.752034
2025-10-09 07:46:30,004 Stage: Train 0.5 | Epoch: 64 | Iter: 105800 | Total Loss: 0.003363 | Recon Loss: 0.002981 | Commit Loss: 0.000762 | Perplexity: 3063.367351
Trainning Epoch:  19%|█▊        | 92/494 [21:11:20<132:13:14, 1184.07s/it]Trainning Epoch:  19%|█▊        | 92/494 [21:11:20<132:13:14, 1184.07s/it]2025-10-09 07:50:25,229 Stage: Train 0.5 | Epoch: 65 | Iter: 106000 | Total Loss: 0.003417 | Recon Loss: 0.003038 | Commit Loss: 0.000757 | Perplexity: 3057.875265
2025-10-09 07:54:17,717 Stage: Train 0.5 | Epoch: 65 | Iter: 106200 | Total Loss: 0.003351 | Recon Loss: 0.002962 | Commit Loss: 0.000777 | Perplexity: 3066.608771
2025-10-09 07:58:11,430 Stage: Train 0.5 | Epoch: 65 | Iter: 106400 | Total Loss: 0.003453 | Recon Loss: 0.003062 | Commit Loss: 0.000783 | Perplexity: 3065.817029
2025-10-09 08:02:04,957 Stage: Train 0.5 | Epoch: 65 | Iter: 106600 | Total Loss: 0.003417 | Recon Loss: 0.003027 | Commit Loss: 0.000779 | Perplexity: 3065.575690
2025-10-09 08:05:58,834 Stage: Train 0.5 | Epoch: 65 | Iter: 106800 | Total Loss: 0.003400 | Recon Loss: 0.003011 | Commit Loss: 0.000779 | Perplexity: 3065.773114
Trainning Epoch:  19%|█▉        | 93/494 [21:31:04<131:55:08, 1184.31s/it]Trainning Epoch:  19%|█▉        | 93/494 [21:31:04<131:55:08, 1184.31s/it]2025-10-09 08:09:55,945 Stage: Train 0.5 | Epoch: 66 | Iter: 107000 | Total Loss: 0.003386 | Recon Loss: 0.003002 | Commit Loss: 0.000767 | Perplexity: 3052.099343
2025-10-09 08:13:50,066 Stage: Train 0.5 | Epoch: 66 | Iter: 107200 | Total Loss: 0.003427 | Recon Loss: 0.003043 | Commit Loss: 0.000768 | Perplexity: 3064.034595
2025-10-09 08:17:44,287 Stage: Train 0.5 | Epoch: 66 | Iter: 107400 | Total Loss: 0.003422 | Recon Loss: 0.003041 | Commit Loss: 0.000763 | Perplexity: 3069.926500
2025-10-09 08:21:37,267 Stage: Train 0.5 | Epoch: 66 | Iter: 107600 | Total Loss: 0.003434 | Recon Loss: 0.003048 | Commit Loss: 0.000772 | Perplexity: 3064.988120
2025-10-09 08:25:30,377 Stage: Train 0.5 | Epoch: 66 | Iter: 107800 | Total Loss: 0.003374 | Recon Loss: 0.002983 | Commit Loss: 0.000782 | Perplexity: 3078.371230
Trainning Epoch:  19%|█▉        | 94/494 [21:50:50<131:38:07, 1184.72s/it]Trainning Epoch:  19%|█▉        | 94/494 [21:50:50<131:38:07, 1184.72s/it]2025-10-09 08:29:26,109 Stage: Train 0.5 | Epoch: 67 | Iter: 108000 | Total Loss: 0.003356 | Recon Loss: 0.002965 | Commit Loss: 0.000781 | Perplexity: 3064.211047
2025-10-09 08:33:18,970 Stage: Train 0.5 | Epoch: 67 | Iter: 108200 | Total Loss: 0.003357 | Recon Loss: 0.002972 | Commit Loss: 0.000771 | Perplexity: 3057.092086
2025-10-09 08:37:11,961 Stage: Train 0.5 | Epoch: 67 | Iter: 108400 | Total Loss: 0.003384 | Recon Loss: 0.003002 | Commit Loss: 0.000765 | Perplexity: 3069.808519
2025-10-09 08:41:06,151 Stage: Train 0.5 | Epoch: 67 | Iter: 108600 | Total Loss: 0.003374 | Recon Loss: 0.002995 | Commit Loss: 0.000758 | Perplexity: 3061.927815
2025-10-09 08:45:01,833 Stage: Train 0.5 | Epoch: 67 | Iter: 108800 | Total Loss: 0.003411 | Recon Loss: 0.003026 | Commit Loss: 0.000770 | Perplexity: 3060.133103
Trainning Epoch:  19%|█▉        | 95/494 [22:10:37<131:22:31, 1185.34s/it]Trainning Epoch:  19%|█▉        | 95/494 [22:10:37<131:22:31, 1185.34s/it]2025-10-09 08:48:57,948 Stage: Train 0.5 | Epoch: 68 | Iter: 109000 | Total Loss: 0.003362 | Recon Loss: 0.002979 | Commit Loss: 0.000766 | Perplexity: 3066.228759
2025-10-09 08:52:51,764 Stage: Train 0.5 | Epoch: 68 | Iter: 109200 | Total Loss: 0.003346 | Recon Loss: 0.002958 | Commit Loss: 0.000776 | Perplexity: 3067.326859
2025-10-09 08:56:45,051 Stage: Train 0.5 | Epoch: 68 | Iter: 109400 | Total Loss: 0.003391 | Recon Loss: 0.003007 | Commit Loss: 0.000768 | Perplexity: 3066.438684
2025-10-09 09:00:38,483 Stage: Train 0.5 | Epoch: 68 | Iter: 109600 | Total Loss: 0.003388 | Recon Loss: 0.003002 | Commit Loss: 0.000772 | Perplexity: 3056.889113
2025-10-09 09:04:33,079 Stage: Train 0.5 | Epoch: 68 | Iter: 109800 | Total Loss: 0.003360 | Recon Loss: 0.002979 | Commit Loss: 0.000762 | Perplexity: 3061.735785
Trainning Epoch:  19%|█▉        | 96/494 [22:30:23<131:04:03, 1185.54s/it]Trainning Epoch:  19%|█▉        | 96/494 [22:30:23<131:04:03, 1185.54s/it]2025-10-09 09:08:29,498 Stage: Train 0.5 | Epoch: 69 | Iter: 110000 | Total Loss: 0.003357 | Recon Loss: 0.002977 | Commit Loss: 0.000761 | Perplexity: 3065.657418
2025-10-09 09:12:21,614 Stage: Train 0.5 | Epoch: 69 | Iter: 110200 | Total Loss: 0.003413 | Recon Loss: 0.003024 | Commit Loss: 0.000778 | Perplexity: 3072.251610
2025-10-09 09:16:13,487 Stage: Train 0.5 | Epoch: 69 | Iter: 110400 | Total Loss: 0.003440 | Recon Loss: 0.003060 | Commit Loss: 0.000761 | Perplexity: 3062.871063
2025-10-09 09:20:06,197 Stage: Train 0.5 | Epoch: 69 | Iter: 110600 | Total Loss: 0.003452 | Recon Loss: 0.003068 | Commit Loss: 0.000769 | Perplexity: 3064.496367
2025-10-09 09:24:04,882 Stage: Train 0.5 | Epoch: 69 | Iter: 110800 | Total Loss: 0.003315 | Recon Loss: 0.002932 | Commit Loss: 0.000766 | Perplexity: 3057.996868
Trainning Epoch:  20%|█▉        | 97/494 [22:50:10<130:46:43, 1185.90s/it]Trainning Epoch:  20%|█▉        | 97/494 [22:50:10<130:46:44, 1185.90s/it]2025-10-09 09:27:59,704 Stage: Train 0.5 | Epoch: 70 | Iter: 111000 | Total Loss: 0.003358 | Recon Loss: 0.002974 | Commit Loss: 0.000767 | Perplexity: 3074.462382
2025-10-09 09:31:52,635 Stage: Train 0.5 | Epoch: 70 | Iter: 111200 | Total Loss: 0.003364 | Recon Loss: 0.002985 | Commit Loss: 0.000760 | Perplexity: 3057.652877
2025-10-09 09:35:53,464 Stage: Train 0.5 | Epoch: 70 | Iter: 111400 | Total Loss: 0.003337 | Recon Loss: 0.002947 | Commit Loss: 0.000780 | Perplexity: 3068.014530
2025-10-09 09:39:44,236 Stage: Train 0.5 | Epoch: 70 | Iter: 111600 | Total Loss: 0.003410 | Recon Loss: 0.003027 | Commit Loss: 0.000767 | Perplexity: 3059.806027
2025-10-09 09:43:34,918 Stage: Train 0.5 | Epoch: 70 | Iter: 111800 | Total Loss: 0.003349 | Recon Loss: 0.002956 | Commit Loss: 0.000786 | Perplexity: 3065.846152
Trainning Epoch:  20%|█▉        | 98/494 [23:09:54<130:23:47, 1185.42s/it]Trainning Epoch:  20%|█▉        | 98/494 [23:09:54<130:23:47, 1185.42s/it]2025-10-09 09:47:27,931 Stage: Train 0.5 | Epoch: 71 | Iter: 112000 | Total Loss: 0.003312 | Recon Loss: 0.002924 | Commit Loss: 0.000775 | Perplexity: 3064.956991
2025-10-09 09:51:15,094 Stage: Train 0.5 | Epoch: 71 | Iter: 112200 | Total Loss: 0.003343 | Recon Loss: 0.002954 | Commit Loss: 0.000779 | Perplexity: 3067.035581
2025-10-09 09:55:03,271 Stage: Train 0.5 | Epoch: 71 | Iter: 112400 | Total Loss: 0.003415 | Recon Loss: 0.003027 | Commit Loss: 0.000777 | Perplexity: 3076.802074
2025-10-09 09:58:50,659 Stage: Train 0.5 | Epoch: 71 | Iter: 112600 | Total Loss: 0.003353 | Recon Loss: 0.002967 | Commit Loss: 0.000773 | Perplexity: 3063.993760
2025-10-09 10:02:38,308 Stage: Train 0.5 | Epoch: 71 | Iter: 112800 | Total Loss: 0.003376 | Recon Loss: 0.002989 | Commit Loss: 0.000773 | Perplexity: 3064.791343
Trainning Epoch:  20%|██        | 99/494 [23:29:11<129:08:02, 1176.92s/it]Trainning Epoch:  20%|██        | 99/494 [23:29:11<129:08:03, 1176.92s/it]2025-10-09 10:06:30,064 Stage: Train 0.5 | Epoch: 72 | Iter: 113000 | Total Loss: 0.003282 | Recon Loss: 0.002895 | Commit Loss: 0.000775 | Perplexity: 3062.402961
2025-10-09 10:10:22,245 Stage: Train 0.5 | Epoch: 72 | Iter: 113200 | Total Loss: 0.003318 | Recon Loss: 0.002938 | Commit Loss: 0.000762 | Perplexity: 3067.345822
2025-10-09 10:14:15,235 Stage: Train 0.5 | Epoch: 72 | Iter: 113400 | Total Loss: 0.003307 | Recon Loss: 0.002924 | Commit Loss: 0.000766 | Perplexity: 3077.263579
2025-10-09 10:18:11,205 Stage: Train 0.5 | Epoch: 72 | Iter: 113600 | Total Loss: 0.003336 | Recon Loss: 0.002951 | Commit Loss: 0.000771 | Perplexity: 3067.399968
2025-10-09 10:22:07,775 Stage: Train 0.5 | Epoch: 72 | Iter: 113800 | Total Loss: 0.003310 | Recon Loss: 0.002925 | Commit Loss: 0.000769 | Perplexity: 3063.448639
Trainning Epoch:  20%|██        | 100/494 [23:49:00<129:12:31, 1180.59s/it]Trainning Epoch:  20%|██        | 100/494 [23:49:00<129:12:34, 1180.59s/it]2025-10-09 10:26:05,895 Stage: Train 0.5 | Epoch: 73 | Iter: 114000 | Total Loss: 0.003361 | Recon Loss: 0.002976 | Commit Loss: 0.000769 | Perplexity: 3057.535376
2025-10-09 10:29:59,097 Stage: Train 0.5 | Epoch: 73 | Iter: 114200 | Total Loss: 0.003321 | Recon Loss: 0.002937 | Commit Loss: 0.000767 | Perplexity: 3073.226163
2025-10-09 10:33:52,667 Stage: Train 0.5 | Epoch: 73 | Iter: 114400 | Total Loss: 0.003337 | Recon Loss: 0.002948 | Commit Loss: 0.000778 | Perplexity: 3074.174884
2025-10-09 10:37:45,906 Stage: Train 0.5 | Epoch: 73 | Iter: 114600 | Total Loss: 0.003301 | Recon Loss: 0.002912 | Commit Loss: 0.000778 | Perplexity: 3069.028208
2025-10-09 10:41:39,184 Stage: Train 0.5 | Epoch: 73 | Iter: 114800 | Total Loss: 0.003312 | Recon Loss: 0.002930 | Commit Loss: 0.000764 | Perplexity: 3072.084861
Trainning Epoch:  20%|██        | 101/494 [24:08:45<129:01:22, 1181.89s/it]Trainning Epoch:  20%|██        | 101/494 [24:08:45<129:01:22, 1181.89s/it]2025-10-09 10:45:35,873 Stage: Train 0.5 | Epoch: 74 | Iter: 115000 | Total Loss: 0.003324 | Recon Loss: 0.002939 | Commit Loss: 0.000769 | Perplexity: 3067.905194
2025-10-09 10:49:29,699 Stage: Train 0.5 | Epoch: 74 | Iter: 115200 | Total Loss: 0.003352 | Recon Loss: 0.002968 | Commit Loss: 0.000768 | Perplexity: 3079.395016
2025-10-09 10:53:23,353 Stage: Train 0.5 | Epoch: 74 | Iter: 115400 | Total Loss: 0.003305 | Recon Loss: 0.002918 | Commit Loss: 0.000774 | Perplexity: 3067.167240
2025-10-09 10:57:17,247 Stage: Train 0.5 | Epoch: 74 | Iter: 115600 | Total Loss: 0.003360 | Recon Loss: 0.002963 | Commit Loss: 0.000795 | Perplexity: 3062.602927
2025-10-09 11:01:11,020 Stage: Train 0.5 | Epoch: 74 | Iter: 115800 | Total Loss: 0.003386 | Recon Loss: 0.003009 | Commit Loss: 0.000755 | Perplexity: 3075.004923
Trainning Epoch:  21%|██        | 102/494 [24:28:33<128:53:09, 1183.65s/it]Trainning Epoch:  21%|██        | 102/494 [24:28:33<128:53:09, 1183.65s/it]2025-10-09 11:05:07,386 Stage: Train 0.5 | Epoch: 75 | Iter: 116000 | Total Loss: 0.003319 | Recon Loss: 0.002937 | Commit Loss: 0.000766 | Perplexity: 3068.738102
2025-10-09 11:08:58,042 Stage: Train 0.5 | Epoch: 75 | Iter: 116200 | Total Loss: 0.003307 | Recon Loss: 0.002916 | Commit Loss: 0.000781 | Perplexity: 3063.328464
2025-10-09 11:12:49,266 Stage: Train 0.5 | Epoch: 75 | Iter: 116400 | Total Loss: 0.003329 | Recon Loss: 0.002948 | Commit Loss: 0.000761 | Perplexity: 3071.822297
2025-10-09 11:16:40,403 Stage: Train 0.5 | Epoch: 75 | Iter: 116600 | Total Loss: 0.003298 | Recon Loss: 0.002913 | Commit Loss: 0.000770 | Perplexity: 3073.181245
2025-10-09 11:20:31,382 Stage: Train 0.5 | Epoch: 75 | Iter: 116800 | Total Loss: 0.003325 | Recon Loss: 0.002931 | Commit Loss: 0.000788 | Perplexity: 3076.066338
Trainning Epoch:  21%|██        | 103/494 [24:48:06<128:12:33, 1180.44s/it]Trainning Epoch:  21%|██        | 103/494 [24:48:06<128:12:33, 1180.44s/it]2025-10-09 11:24:25,754 Stage: Train 0.5 | Epoch: 76 | Iter: 117000 | Total Loss: 0.003259 | Recon Loss: 0.002876 | Commit Loss: 0.000765 | Perplexity: 3074.009260
2025-10-09 11:28:16,872 Stage: Train 0.5 | Epoch: 76 | Iter: 117200 | Total Loss: 0.003304 | Recon Loss: 0.002921 | Commit Loss: 0.000766 | Perplexity: 3076.466490
2025-10-09 11:32:08,493 Stage: Train 0.5 | Epoch: 76 | Iter: 117400 | Total Loss: 0.003303 | Recon Loss: 0.002919 | Commit Loss: 0.000766 | Perplexity: 3070.761127
2025-10-09 11:35:59,909 Stage: Train 0.5 | Epoch: 76 | Iter: 117600 | Total Loss: 0.003273 | Recon Loss: 0.002887 | Commit Loss: 0.000774 | Perplexity: 3079.307867
2025-10-09 11:39:51,536 Stage: Train 0.5 | Epoch: 76 | Iter: 117800 | Total Loss: 0.003414 | Recon Loss: 0.003027 | Commit Loss: 0.000774 | Perplexity: 3068.624424
2025-10-09 11:43:42,937 Stage: Train 0.5 | Epoch: 76 | Iter: 118000 | Total Loss: 0.003294 | Recon Loss: 0.002912 | Commit Loss: 0.000766 | Perplexity: 3075.316765
Trainning Epoch:  21%|██        | 104/494 [25:07:41<127:42:55, 1178.91s/it]Trainning Epoch:  21%|██        | 104/494 [25:07:41<127:42:56, 1178.91s/it]2025-10-09 11:47:39,105 Stage: Train 0.5 | Epoch: 77 | Iter: 118200 | Total Loss: 0.003277 | Recon Loss: 0.002894 | Commit Loss: 0.000765 | Perplexity: 3079.507046
2025-10-09 11:51:32,352 Stage: Train 0.5 | Epoch: 77 | Iter: 118400 | Total Loss: 0.003268 | Recon Loss: 0.002876 | Commit Loss: 0.000783 | Perplexity: 3069.720197
2025-10-09 11:55:25,351 Stage: Train 0.5 | Epoch: 77 | Iter: 118600 | Total Loss: 0.003307 | Recon Loss: 0.002923 | Commit Loss: 0.000768 | Perplexity: 3067.689907
2025-10-09 11:59:18,790 Stage: Train 0.5 | Epoch: 77 | Iter: 118800 | Total Loss: 0.003301 | Recon Loss: 0.002920 | Commit Loss: 0.000763 | Perplexity: 3072.902479
2025-10-09 12:03:11,956 Stage: Train 0.5 | Epoch: 77 | Iter: 119000 | Total Loss: 0.003276 | Recon Loss: 0.002886 | Commit Loss: 0.000780 | Perplexity: 3082.895466
Trainning Epoch:  21%|██▏       | 105/494 [25:27:25<127:33:22, 1180.47s/it]Trainning Epoch:  21%|██▏       | 105/494 [25:27:25<127:33:22, 1180.47s/it]2025-10-09 12:07:08,833 Stage: Train 0.5 | Epoch: 78 | Iter: 119200 | Total Loss: 0.003233 | Recon Loss: 0.002849 | Commit Loss: 0.000769 | Perplexity: 3070.408441
2025-10-09 12:11:02,656 Stage: Train 0.5 | Epoch: 78 | Iter: 119400 | Total Loss: 0.003298 | Recon Loss: 0.002912 | Commit Loss: 0.000772 | Perplexity: 3077.308359
2025-10-09 12:14:56,181 Stage: Train 0.5 | Epoch: 78 | Iter: 119600 | Total Loss: 0.003301 | Recon Loss: 0.002917 | Commit Loss: 0.000768 | Perplexity: 3074.053625
2025-10-09 12:18:49,953 Stage: Train 0.5 | Epoch: 78 | Iter: 119800 | Total Loss: 0.003320 | Recon Loss: 0.002939 | Commit Loss: 0.000761 | Perplexity: 3079.516483
2025-10-09 12:22:43,934 Stage: Train 0.5 | Epoch: 78 | Iter: 120000 | Total Loss: 0.003295 | Recon Loss: 0.002913 | Commit Loss: 0.000764 | Perplexity: 3075.803667
2025-10-09 12:22:43,934 Saving model at iteration 120000
2025-10-09 12:22:44,084 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000
2025-10-09 12:22:45,377 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000/model.safetensors
2025-10-09 12:22:47,064 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000/optimizer.bin
2025-10-09 12:22:47,064 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000/scheduler.bin
2025-10-09 12:22:47,064 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000/sampler.bin
2025-10-09 12:22:47,065 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_120000/random_states_0.pkl
Trainning Epoch:  21%|██▏       | 106/494 [25:47:15<127:32:33, 1183.38s/it]Trainning Epoch:  21%|██▏       | 106/494 [25:47:15<127:32:34, 1183.39s/it]2025-10-09 12:26:42,406 Stage: Train 0.5 | Epoch: 79 | Iter: 120200 | Total Loss: 0.003227 | Recon Loss: 0.002841 | Commit Loss: 0.000771 | Perplexity: 3072.929679
2025-10-09 12:30:34,358 Stage: Train 0.5 | Epoch: 79 | Iter: 120400 | Total Loss: 0.003235 | Recon Loss: 0.002852 | Commit Loss: 0.000765 | Perplexity: 3073.162488
2025-10-09 12:34:26,476 Stage: Train 0.5 | Epoch: 79 | Iter: 120600 | Total Loss: 0.003304 | Recon Loss: 0.002916 | Commit Loss: 0.000776 | Perplexity: 3074.891395
2025-10-09 12:38:18,854 Stage: Train 0.5 | Epoch: 79 | Iter: 120800 | Total Loss: 0.003254 | Recon Loss: 0.002868 | Commit Loss: 0.000773 | Perplexity: 3078.639531
2025-10-09 12:42:11,059 Stage: Train 0.5 | Epoch: 79 | Iter: 121000 | Total Loss: 0.003258 | Recon Loss: 0.002874 | Commit Loss: 0.000769 | Perplexity: 3077.815898
Trainning Epoch:  22%|██▏       | 107/494 [26:06:54<127:04:13, 1182.05s/it]Trainning Epoch:  22%|██▏       | 107/494 [26:06:54<127:04:13, 1182.05s/it]2025-10-09 12:46:02,226 Stage: Train 0.5 | Epoch: 80 | Iter: 121200 | Total Loss: 0.003274 | Recon Loss: 0.002893 | Commit Loss: 0.000761 | Perplexity: 3068.475326
2025-10-09 12:49:49,279 Stage: Train 0.5 | Epoch: 80 | Iter: 121400 | Total Loss: 0.003303 | Recon Loss: 0.002915 | Commit Loss: 0.000777 | Perplexity: 3078.295004
2025-10-09 12:53:36,754 Stage: Train 0.5 | Epoch: 80 | Iter: 121600 | Total Loss: 0.003232 | Recon Loss: 0.002848 | Commit Loss: 0.000769 | Perplexity: 3078.249417
2025-10-09 12:57:23,538 Stage: Train 0.5 | Epoch: 80 | Iter: 121800 | Total Loss: 0.003256 | Recon Loss: 0.002870 | Commit Loss: 0.000772 | Perplexity: 3079.518870
2025-10-09 13:01:10,739 Stage: Train 0.5 | Epoch: 80 | Iter: 122000 | Total Loss: 0.003239 | Recon Loss: 0.002854 | Commit Loss: 0.000770 | Perplexity: 3085.484309
Trainning Epoch:  22%|██▏       | 108/494 [26:26:08<125:49:48, 1173.55s/it]Trainning Epoch:  22%|██▏       | 108/494 [26:26:08<125:49:48, 1173.55s/it]2025-10-09 13:05:06,054 Stage: Train 0.5 | Epoch: 81 | Iter: 122200 | Total Loss: 0.003305 | Recon Loss: 0.002926 | Commit Loss: 0.000760 | Perplexity: 3082.422693
2025-10-09 13:08:59,884 Stage: Train 0.5 | Epoch: 81 | Iter: 122400 | Total Loss: 0.003254 | Recon Loss: 0.002868 | Commit Loss: 0.000770 | Perplexity: 3075.546768
2025-10-09 13:12:52,971 Stage: Train 0.5 | Epoch: 81 | Iter: 122600 | Total Loss: 0.003205 | Recon Loss: 0.002817 | Commit Loss: 0.000775 | Perplexity: 3083.241744
2025-10-09 13:16:46,036 Stage: Train 0.5 | Epoch: 81 | Iter: 122800 | Total Loss: 0.003254 | Recon Loss: 0.002868 | Commit Loss: 0.000771 | Perplexity: 3085.149098
2025-10-09 13:20:39,314 Stage: Train 0.5 | Epoch: 81 | Iter: 123000 | Total Loss: 0.003248 | Recon Loss: 0.002864 | Commit Loss: 0.000767 | Perplexity: 3087.935688
Trainning Epoch:  22%|██▏       | 109/494 [26:45:53<125:52:31, 1177.02s/it]Trainning Epoch:  22%|██▏       | 109/494 [26:45:53<125:52:31, 1177.02s/it]2025-10-09 13:24:33,475 Stage: Train 0.5 | Epoch: 82 | Iter: 123200 | Total Loss: 0.003214 | Recon Loss: 0.002835 | Commit Loss: 0.000759 | Perplexity: 3080.352825
2025-10-09 13:28:24,673 Stage: Train 0.5 | Epoch: 82 | Iter: 123400 | Total Loss: 0.003233 | Recon Loss: 0.002853 | Commit Loss: 0.000761 | Perplexity: 3084.853712
2025-10-09 13:32:15,774 Stage: Train 0.5 | Epoch: 82 | Iter: 123600 | Total Loss: 0.003236 | Recon Loss: 0.002846 | Commit Loss: 0.000781 | Perplexity: 3086.250627
2025-10-09 13:36:07,061 Stage: Train 0.5 | Epoch: 82 | Iter: 123800 | Total Loss: 0.003247 | Recon Loss: 0.002865 | Commit Loss: 0.000764 | Perplexity: 3086.911874
2025-10-09 13:39:58,166 Stage: Train 0.5 | Epoch: 82 | Iter: 124000 | Total Loss: 0.003243 | Recon Loss: 0.002855 | Commit Loss: 0.000776 | Perplexity: 3077.472755
Trainning Epoch:  22%|██▏       | 110/494 [27:05:27<125:25:48, 1175.91s/it]Trainning Epoch:  22%|██▏       | 110/494 [27:05:27<125:25:49, 1175.91s/it]2025-10-09 13:43:53,024 Stage: Train 0.5 | Epoch: 83 | Iter: 124200 | Total Loss: 0.003230 | Recon Loss: 0.002850 | Commit Loss: 0.000762 | Perplexity: 3080.033062
2025-10-09 13:47:45,643 Stage: Train 0.5 | Epoch: 83 | Iter: 124400 | Total Loss: 0.003237 | Recon Loss: 0.002852 | Commit Loss: 0.000769 | Perplexity: 3077.156327
2025-10-09 13:51:38,535 Stage: Train 0.5 | Epoch: 83 | Iter: 124600 | Total Loss: 0.003253 | Recon Loss: 0.002867 | Commit Loss: 0.000770 | Perplexity: 3085.416204
2025-10-09 13:55:32,061 Stage: Train 0.5 | Epoch: 83 | Iter: 124800 | Total Loss: 0.003248 | Recon Loss: 0.002860 | Commit Loss: 0.000776 | Perplexity: 3087.469891
2025-10-09 13:59:25,034 Stage: Train 0.5 | Epoch: 83 | Iter: 125000 | Total Loss: 0.003176 | Recon Loss: 0.002795 | Commit Loss: 0.000763 | Perplexity: 3079.668076
Trainning Epoch:  22%|██▏       | 111/494 [27:25:09<125:18:41, 1177.86s/it]Trainning Epoch:  22%|██▏       | 111/494 [27:25:09<125:18:41, 1177.86s/it]2025-10-09 14:03:19,619 Stage: Train 0.5 | Epoch: 84 | Iter: 125200 | Total Loss: 0.003236 | Recon Loss: 0.002844 | Commit Loss: 0.000783 | Perplexity: 3087.476317
2025-10-09 14:07:11,077 Stage: Train 0.5 | Epoch: 84 | Iter: 125400 | Total Loss: 0.003228 | Recon Loss: 0.002850 | Commit Loss: 0.000755 | Perplexity: 3083.380042
2025-10-09 14:11:02,655 Stage: Train 0.5 | Epoch: 84 | Iter: 125600 | Total Loss: 0.003238 | Recon Loss: 0.002855 | Commit Loss: 0.000766 | Perplexity: 3088.236940
2025-10-09 14:14:54,460 Stage: Train 0.5 | Epoch: 84 | Iter: 125800 | Total Loss: 0.003197 | Recon Loss: 0.002813 | Commit Loss: 0.000767 | Perplexity: 3090.618519
2025-10-09 14:18:46,061 Stage: Train 0.5 | Epoch: 84 | Iter: 126000 | Total Loss: 0.003237 | Recon Loss: 0.002849 | Commit Loss: 0.000777 | Perplexity: 3081.945172
Trainning Epoch:  23%|██▎       | 112/494 [27:44:45<124:54:42, 1177.18s/it]Trainning Epoch:  23%|██▎       | 112/494 [27:44:45<124:54:43, 1177.18s/it]2025-10-09 14:22:37,794 Stage: Train 0.5 | Epoch: 85 | Iter: 126200 | Total Loss: 0.003256 | Recon Loss: 0.002867 | Commit Loss: 0.000779 | Perplexity: 3082.766982
2025-10-09 14:26:27,605 Stage: Train 0.5 | Epoch: 85 | Iter: 126400 | Total Loss: 0.003212 | Recon Loss: 0.002831 | Commit Loss: 0.000762 | Perplexity: 3090.125645
2025-10-09 14:30:17,939 Stage: Train 0.5 | Epoch: 85 | Iter: 126600 | Total Loss: 0.003251 | Recon Loss: 0.002871 | Commit Loss: 0.000761 | Perplexity: 3089.719442
2025-10-09 14:34:08,904 Stage: Train 0.5 | Epoch: 85 | Iter: 126800 | Total Loss: 0.003258 | Recon Loss: 0.002877 | Commit Loss: 0.000762 | Perplexity: 3086.012606
2025-10-09 14:37:59,403 Stage: Train 0.5 | Epoch: 85 | Iter: 127000 | Total Loss: 0.003261 | Recon Loss: 0.002879 | Commit Loss: 0.000763 | Perplexity: 3077.121682
Trainning Epoch:  23%|██▎       | 113/494 [28:04:13<124:18:06, 1174.51s/it]Trainning Epoch:  23%|██▎       | 113/494 [28:04:13<124:18:06, 1174.51s/it]2025-10-09 14:41:53,583 Stage: Train 0.5 | Epoch: 86 | Iter: 127200 | Total Loss: 0.003170 | Recon Loss: 0.002792 | Commit Loss: 0.000757 | Perplexity: 3084.589249
2025-10-09 14:45:45,632 Stage: Train 0.5 | Epoch: 86 | Iter: 127400 | Total Loss: 0.003214 | Recon Loss: 0.002828 | Commit Loss: 0.000772 | Perplexity: 3082.069263
2025-10-09 14:49:37,503 Stage: Train 0.5 | Epoch: 86 | Iter: 127600 | Total Loss: 0.003188 | Recon Loss: 0.002801 | Commit Loss: 0.000774 | Perplexity: 3089.227485
2025-10-09 14:53:29,508 Stage: Train 0.5 | Epoch: 86 | Iter: 127800 | Total Loss: 0.003231 | Recon Loss: 0.002850 | Commit Loss: 0.000761 | Perplexity: 3089.570255
2025-10-09 14:57:21,996 Stage: Train 0.5 | Epoch: 86 | Iter: 128000 | Total Loss: 0.003212 | Recon Loss: 0.002829 | Commit Loss: 0.000768 | Perplexity: 3098.489659
Trainning Epoch:  23%|██▎       | 114/494 [28:23:51<124:06:07, 1175.70s/it]Trainning Epoch:  23%|██▎       | 114/494 [28:23:51<124:06:08, 1175.71s/it]2025-10-09 15:01:17,453 Stage: Train 0.5 | Epoch: 87 | Iter: 128200 | Total Loss: 0.003158 | Recon Loss: 0.002777 | Commit Loss: 0.000762 | Perplexity: 3099.640186
2025-10-09 15:05:10,274 Stage: Train 0.5 | Epoch: 87 | Iter: 128400 | Total Loss: 0.003232 | Recon Loss: 0.002845 | Commit Loss: 0.000774 | Perplexity: 3091.916143
2025-10-09 15:09:02,750 Stage: Train 0.5 | Epoch: 87 | Iter: 128600 | Total Loss: 0.003179 | Recon Loss: 0.002792 | Commit Loss: 0.000775 | Perplexity: 3086.487281
2025-10-09 15:12:55,291 Stage: Train 0.5 | Epoch: 87 | Iter: 128800 | Total Loss: 0.003237 | Recon Loss: 0.002855 | Commit Loss: 0.000765 | Perplexity: 3083.986786
2025-10-09 15:16:47,652 Stage: Train 0.5 | Epoch: 87 | Iter: 129000 | Total Loss: 0.003130 | Recon Loss: 0.002747 | Commit Loss: 0.000767 | Perplexity: 3088.327020
Trainning Epoch:  23%|██▎       | 115/494 [28:43:32<123:55:56, 1177.19s/it]Trainning Epoch:  23%|██▎       | 115/494 [28:43:32<123:55:56, 1177.19s/it]2025-10-09 15:20:43,086 Stage: Train 0.5 | Epoch: 88 | Iter: 129200 | Total Loss: 0.003197 | Recon Loss: 0.002811 | Commit Loss: 0.000772 | Perplexity: 3079.467324
2025-10-09 15:24:34,397 Stage: Train 0.5 | Epoch: 88 | Iter: 129400 | Total Loss: 0.003200 | Recon Loss: 0.002810 | Commit Loss: 0.000779 | Perplexity: 3099.550159
2025-10-09 15:28:25,746 Stage: Train 0.5 | Epoch: 88 | Iter: 129600 | Total Loss: 0.003229 | Recon Loss: 0.002851 | Commit Loss: 0.000757 | Perplexity: 3088.538992
2025-10-09 15:32:17,083 Stage: Train 0.5 | Epoch: 88 | Iter: 129800 | Total Loss: 0.003191 | Recon Loss: 0.002804 | Commit Loss: 0.000776 | Perplexity: 3089.468273
2025-10-09 15:36:08,859 Stage: Train 0.5 | Epoch: 88 | Iter: 130000 | Total Loss: 0.003204 | Recon Loss: 0.002821 | Commit Loss: 0.000767 | Perplexity: 3098.026296
Trainning Epoch:  23%|██▎       | 116/494 [29:03:07<123:33:05, 1176.68s/it]Trainning Epoch:  23%|██▎       | 116/494 [29:03:07<123:33:05, 1176.68s/it]2025-10-09 15:40:02,841 Stage: Train 0.5 | Epoch: 89 | Iter: 130200 | Total Loss: 0.003233 | Recon Loss: 0.002848 | Commit Loss: 0.000771 | Perplexity: 3093.263158
2025-10-09 15:43:53,080 Stage: Train 0.5 | Epoch: 89 | Iter: 130400 | Total Loss: 0.003169 | Recon Loss: 0.002782 | Commit Loss: 0.000773 | Perplexity: 3099.812758
2025-10-09 15:47:44,270 Stage: Train 0.5 | Epoch: 89 | Iter: 130600 | Total Loss: 0.003201 | Recon Loss: 0.002818 | Commit Loss: 0.000766 | Perplexity: 3086.531487
2025-10-09 15:51:35,455 Stage: Train 0.5 | Epoch: 89 | Iter: 130800 | Total Loss: 0.003193 | Recon Loss: 0.002811 | Commit Loss: 0.000763 | Perplexity: 3094.774766
2025-10-09 15:55:27,009 Stage: Train 0.5 | Epoch: 89 | Iter: 131000 | Total Loss: 0.003247 | Recon Loss: 0.002863 | Commit Loss: 0.000769 | Perplexity: 3102.999875
Trainning Epoch:  24%|██▎       | 117/494 [29:22:41<123:07:09, 1175.68s/it]Trainning Epoch:  24%|██▎       | 117/494 [29:22:41<123:07:10, 1175.68s/it]2025-10-09 15:59:22,193 Stage: Train 0.5 | Epoch: 90 | Iter: 131200 | Total Loss: 0.003167 | Recon Loss: 0.002782 | Commit Loss: 0.000768 | Perplexity: 3099.282753
2025-10-09 16:03:14,156 Stage: Train 0.5 | Epoch: 90 | Iter: 131400 | Total Loss: 0.003194 | Recon Loss: 0.002809 | Commit Loss: 0.000769 | Perplexity: 3098.999485
2025-10-09 16:07:05,994 Stage: Train 0.5 | Epoch: 90 | Iter: 131600 | Total Loss: 0.003198 | Recon Loss: 0.002815 | Commit Loss: 0.000767 | Perplexity: 3098.478772
2025-10-09 16:10:58,123 Stage: Train 0.5 | Epoch: 90 | Iter: 131800 | Total Loss: 0.003154 | Recon Loss: 0.002768 | Commit Loss: 0.000773 | Perplexity: 3095.463396
2025-10-09 16:14:50,317 Stage: Train 0.5 | Epoch: 90 | Iter: 132000 | Total Loss: 0.003203 | Recon Loss: 0.002819 | Commit Loss: 0.000769 | Perplexity: 3094.817029
Trainning Epoch:  24%|██▍       | 118/494 [29:42:19<122:53:03, 1176.55s/it]Trainning Epoch:  24%|██▍       | 118/494 [29:42:19<122:53:03, 1176.55s/it]2025-10-09 16:18:45,287 Stage: Train 0.5 | Epoch: 91 | Iter: 132200 | Total Loss: 0.003136 | Recon Loss: 0.002748 | Commit Loss: 0.000775 | Perplexity: 3088.389692
2025-10-09 16:22:35,509 Stage: Train 0.5 | Epoch: 91 | Iter: 132400 | Total Loss: 0.003198 | Recon Loss: 0.002819 | Commit Loss: 0.000760 | Perplexity: 3091.556595
2025-10-09 16:26:26,334 Stage: Train 0.5 | Epoch: 91 | Iter: 132600 | Total Loss: 0.003159 | Recon Loss: 0.002773 | Commit Loss: 0.000773 | Perplexity: 3096.918575
2025-10-09 16:30:17,422 Stage: Train 0.5 | Epoch: 91 | Iter: 132800 | Total Loss: 0.003163 | Recon Loss: 0.002777 | Commit Loss: 0.000772 | Perplexity: 3088.185288
2025-10-09 16:34:08,492 Stage: Train 0.5 | Epoch: 91 | Iter: 133000 | Total Loss: 0.003147 | Recon Loss: 0.002763 | Commit Loss: 0.000767 | Perplexity: 3096.803453
Trainning Epoch:  24%|██▍       | 119/494 [30:01:53<122:27:05, 1175.53s/it]Trainning Epoch:  24%|██▍       | 119/494 [30:01:53<122:27:05, 1175.54s/it]2025-10-09 16:38:03,946 Stage: Train 0.5 | Epoch: 92 | Iter: 133200 | Total Loss: 0.003202 | Recon Loss: 0.002816 | Commit Loss: 0.000773 | Perplexity: 3090.610734
2025-10-09 16:41:57,677 Stage: Train 0.5 | Epoch: 92 | Iter: 133400 | Total Loss: 0.003150 | Recon Loss: 0.002775 | Commit Loss: 0.000750 | Perplexity: 3085.706888
2025-10-09 16:45:51,532 Stage: Train 0.5 | Epoch: 92 | Iter: 133600 | Total Loss: 0.003148 | Recon Loss: 0.002763 | Commit Loss: 0.000770 | Perplexity: 3098.928198
2025-10-09 16:49:45,094 Stage: Train 0.5 | Epoch: 92 | Iter: 133800 | Total Loss: 0.003219 | Recon Loss: 0.002836 | Commit Loss: 0.000765 | Perplexity: 3104.616946
2025-10-09 16:53:38,632 Stage: Train 0.5 | Epoch: 92 | Iter: 134000 | Total Loss: 0.003211 | Recon Loss: 0.002818 | Commit Loss: 0.000785 | Perplexity: 3092.429670
2025-10-09 16:57:32,251 Stage: Train 0.5 | Epoch: 92 | Iter: 134200 | Total Loss: 0.003214 | Recon Loss: 0.002827 | Commit Loss: 0.000775 | Perplexity: 3084.831626
Trainning Epoch:  24%|██▍       | 120/494 [30:21:40<122:29:28, 1179.06s/it]Trainning Epoch:  24%|██▍       | 120/494 [30:21:40<122:29:28, 1179.06s/it]2025-10-09 17:01:28,234 Stage: Train 0.5 | Epoch: 93 | Iter: 134400 | Total Loss: 0.003189 | Recon Loss: 0.002809 | Commit Loss: 0.000759 | Perplexity: 3093.278652
2025-10-09 17:05:21,727 Stage: Train 0.5 | Epoch: 93 | Iter: 134600 | Total Loss: 0.003136 | Recon Loss: 0.002746 | Commit Loss: 0.000779 | Perplexity: 3094.334940
2025-10-09 17:09:14,860 Stage: Train 0.5 | Epoch: 93 | Iter: 134800 | Total Loss: 0.003159 | Recon Loss: 0.002772 | Commit Loss: 0.000772 | Perplexity: 3086.808973
2025-10-09 17:13:08,180 Stage: Train 0.5 | Epoch: 93 | Iter: 135000 | Total Loss: 0.003171 | Recon Loss: 0.002787 | Commit Loss: 0.000769 | Perplexity: 3096.146465
2025-10-09 17:17:00,848 Stage: Train 0.5 | Epoch: 93 | Iter: 135200 | Total Loss: 0.003130 | Recon Loss: 0.002742 | Commit Loss: 0.000776 | Perplexity: 3088.028549
Trainning Epoch:  24%|██▍       | 121/494 [30:41:23<122:18:01, 1180.38s/it]Trainning Epoch:  24%|██▍       | 121/494 [30:41:23<122:18:01, 1180.38s/it]2025-10-09 17:20:54,286 Stage: Train 0.5 | Epoch: 94 | Iter: 135400 | Total Loss: 0.003136 | Recon Loss: 0.002757 | Commit Loss: 0.000758 | Perplexity: 3095.121149
2025-10-09 17:24:44,350 Stage: Train 0.5 | Epoch: 94 | Iter: 135600 | Total Loss: 0.003256 | Recon Loss: 0.002878 | Commit Loss: 0.000756 | Perplexity: 3093.753143
2025-10-09 17:28:34,995 Stage: Train 0.5 | Epoch: 94 | Iter: 135800 | Total Loss: 0.003137 | Recon Loss: 0.002750 | Commit Loss: 0.000775 | Perplexity: 3092.755887
2025-10-09 17:32:26,138 Stage: Train 0.5 | Epoch: 94 | Iter: 136000 | Total Loss: 0.003167 | Recon Loss: 0.002784 | Commit Loss: 0.000765 | Perplexity: 3102.714166
2025-10-09 17:36:17,501 Stage: Train 0.5 | Epoch: 94 | Iter: 136200 | Total Loss: 0.003140 | Recon Loss: 0.002753 | Commit Loss: 0.000774 | Perplexity: 3099.811040
Trainning Epoch:  25%|██▍       | 122/494 [31:00:55<121:42:18, 1177.79s/it]Trainning Epoch:  25%|██▍       | 122/494 [31:00:55<121:42:19, 1177.79s/it]2025-10-09 17:40:12,183 Stage: Train 0.5 | Epoch: 95 | Iter: 136400 | Total Loss: 0.003146 | Recon Loss: 0.002763 | Commit Loss: 0.000767 | Perplexity: 3093.880151
2025-10-09 17:44:03,292 Stage: Train 0.5 | Epoch: 95 | Iter: 136600 | Total Loss: 0.003151 | Recon Loss: 0.002765 | Commit Loss: 0.000771 | Perplexity: 3099.495553
2025-10-09 17:47:54,971 Stage: Train 0.5 | Epoch: 95 | Iter: 136800 | Total Loss: 0.003146 | Recon Loss: 0.002770 | Commit Loss: 0.000752 | Perplexity: 3090.030951
2025-10-09 17:51:46,722 Stage: Train 0.5 | Epoch: 95 | Iter: 137000 | Total Loss: 0.003156 | Recon Loss: 0.002771 | Commit Loss: 0.000770 | Perplexity: 3095.606906
2025-10-09 17:55:38,529 Stage: Train 0.5 | Epoch: 95 | Iter: 137200 | Total Loss: 0.003136 | Recon Loss: 0.002748 | Commit Loss: 0.000776 | Perplexity: 3100.802306
Trainning Epoch:  25%|██▍       | 123/494 [31:20:31<121:19:32, 1177.28s/it]Trainning Epoch:  25%|██▍       | 123/494 [31:20:31<121:19:32, 1177.28s/it]2025-10-09 17:59:34,909 Stage: Train 0.5 | Epoch: 96 | Iter: 137400 | Total Loss: 0.003164 | Recon Loss: 0.002776 | Commit Loss: 0.000778 | Perplexity: 3106.199900
2025-10-09 18:03:28,462 Stage: Train 0.5 | Epoch: 96 | Iter: 137600 | Total Loss: 0.003154 | Recon Loss: 0.002770 | Commit Loss: 0.000767 | Perplexity: 3101.077482
2025-10-09 18:07:21,864 Stage: Train 0.5 | Epoch: 96 | Iter: 137800 | Total Loss: 0.003142 | Recon Loss: 0.002759 | Commit Loss: 0.000766 | Perplexity: 3095.836595
2025-10-09 18:11:15,472 Stage: Train 0.5 | Epoch: 96 | Iter: 138000 | Total Loss: 0.003115 | Recon Loss: 0.002728 | Commit Loss: 0.000775 | Perplexity: 3097.123358
2025-10-09 18:15:10,077 Stage: Train 0.5 | Epoch: 96 | Iter: 138200 | Total Loss: 0.003164 | Recon Loss: 0.002779 | Commit Loss: 0.000770 | Perplexity: 3094.074082
Trainning Epoch:  25%|██▌       | 124/494 [31:40:18<121:17:49, 1180.19s/it]Trainning Epoch:  25%|██▌       | 124/494 [31:40:18<121:17:50, 1180.19s/it]2025-10-09 18:19:06,986 Stage: Train 0.5 | Epoch: 97 | Iter: 138400 | Total Loss: 0.003118 | Recon Loss: 0.002738 | Commit Loss: 0.000760 | Perplexity: 3103.364028
2025-10-09 18:22:59,930 Stage: Train 0.5 | Epoch: 97 | Iter: 138600 | Total Loss: 0.003176 | Recon Loss: 0.002784 | Commit Loss: 0.000784 | Perplexity: 3100.460966
2025-10-09 18:26:53,467 Stage: Train 0.5 | Epoch: 97 | Iter: 138800 | Total Loss: 0.003151 | Recon Loss: 0.002768 | Commit Loss: 0.000767 | Perplexity: 3089.084746
2025-10-09 18:30:47,085 Stage: Train 0.5 | Epoch: 97 | Iter: 139000 | Total Loss: 0.003191 | Recon Loss: 0.002807 | Commit Loss: 0.000768 | Perplexity: 3105.548132
2025-10-09 18:34:40,638 Stage: Train 0.5 | Epoch: 97 | Iter: 139200 | Total Loss: 0.003131 | Recon Loss: 0.002741 | Commit Loss: 0.000779 | Perplexity: 3092.611960
Trainning Epoch:  25%|██▌       | 125/494 [32:00:04<121:08:30, 1181.87s/it]Trainning Epoch:  25%|██▌       | 125/494 [32:00:04<121:08:31, 1181.87s/it]2025-10-09 18:38:35,515 Stage: Train 0.5 | Epoch: 98 | Iter: 139400 | Total Loss: 0.003127 | Recon Loss: 0.002734 | Commit Loss: 0.000787 | Perplexity: 3102.645490
2025-10-09 18:42:26,880 Stage: Train 0.5 | Epoch: 98 | Iter: 139600 | Total Loss: 0.003121 | Recon Loss: 0.002740 | Commit Loss: 0.000764 | Perplexity: 3099.670243
2025-10-09 18:46:18,371 Stage: Train 0.5 | Epoch: 98 | Iter: 139800 | Total Loss: 0.003060 | Recon Loss: 0.002683 | Commit Loss: 0.000754 | Perplexity: 3088.087524
2025-10-09 18:50:09,703 Stage: Train 0.5 | Epoch: 98 | Iter: 140000 | Total Loss: 0.003142 | Recon Loss: 0.002752 | Commit Loss: 0.000780 | Perplexity: 3094.617927
2025-10-09 18:50:09,703 Saving model at iteration 140000
2025-10-09 18:50:09,978 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000
2025-10-09 18:50:11,349 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000/model.safetensors
2025-10-09 18:50:12,954 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000/optimizer.bin
2025-10-09 18:50:12,954 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000/scheduler.bin
2025-10-09 18:50:12,954 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000/sampler.bin
2025-10-09 18:50:12,955 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_140000/random_states_0.pkl
2025-10-09 18:54:04,303 Stage: Train 0.5 | Epoch: 98 | Iter: 140200 | Total Loss: 0.003177 | Recon Loss: 0.002794 | Commit Loss: 0.000766 | Perplexity: 3096.650999
Trainning Epoch:  26%|██▌       | 126/494 [32:19:42<120:41:50, 1180.73s/it]Trainning Epoch:  26%|██▌       | 126/494 [32:19:42<120:41:51, 1180.74s/it]2025-10-09 18:57:59,668 Stage: Train 0.5 | Epoch: 99 | Iter: 140400 | Total Loss: 0.003103 | Recon Loss: 0.002720 | Commit Loss: 0.000765 | Perplexity: 3107.031366
2025-10-09 19:01:52,158 Stage: Train 0.5 | Epoch: 99 | Iter: 140600 | Total Loss: 0.003119 | Recon Loss: 0.002739 | Commit Loss: 0.000758 | Perplexity: 3090.240013
2025-10-09 19:05:44,836 Stage: Train 0.5 | Epoch: 99 | Iter: 140800 | Total Loss: 0.003158 | Recon Loss: 0.002769 | Commit Loss: 0.000779 | Perplexity: 3099.973741
2025-10-09 19:09:37,586 Stage: Train 0.5 | Epoch: 99 | Iter: 141000 | Total Loss: 0.003148 | Recon Loss: 0.002762 | Commit Loss: 0.000771 | Perplexity: 3093.235299
2025-10-09 19:13:30,493 Stage: Train 0.5 | Epoch: 99 | Iter: 141200 | Total Loss: 0.003106 | Recon Loss: 0.002721 | Commit Loss: 0.000770 | Perplexity: 3098.593118
Trainning Epoch:  26%|██▌       | 127/494 [32:39:24<120:24:32, 1181.12s/it]Trainning Epoch:  26%|██▌       | 127/494 [32:39:24<120:24:32, 1181.12s/it]2025-10-09 19:17:26,034 Stage: Train 0.5 | Epoch: 100 | Iter: 141400 | Total Loss: 0.003123 | Recon Loss: 0.002743 | Commit Loss: 0.000759 | Perplexity: 3092.337860
2025-10-09 19:21:18,190 Stage: Train 0.5 | Epoch: 100 | Iter: 141600 | Total Loss: 0.003133 | Recon Loss: 0.002745 | Commit Loss: 0.000776 | Perplexity: 3109.408162
2025-10-09 19:25:10,619 Stage: Train 0.5 | Epoch: 100 | Iter: 141800 | Total Loss: 0.003121 | Recon Loss: 0.002740 | Commit Loss: 0.000761 | Perplexity: 3104.937980
2025-10-09 19:29:02,973 Stage: Train 0.5 | Epoch: 100 | Iter: 142000 | Total Loss: 0.003111 | Recon Loss: 0.002723 | Commit Loss: 0.000776 | Perplexity: 3099.218230
2025-10-09 19:32:55,297 Stage: Train 0.5 | Epoch: 100 | Iter: 142200 | Total Loss: 0.003127 | Recon Loss: 0.002735 | Commit Loss: 0.000785 | Perplexity: 3095.982942
Trainning Epoch:  26%|██▌       | 128/494 [32:59:04<120:01:54, 1180.64s/it]Trainning Epoch:  26%|██▌       | 128/494 [32:59:04<120:01:54, 1180.64s/it]2025-10-09 19:36:51,362 Stage: Train 0.5 | Epoch: 101 | Iter: 142400 | Total Loss: 0.003112 | Recon Loss: 0.002732 | Commit Loss: 0.000760 | Perplexity: 3093.475378
2025-10-09 19:40:43,508 Stage: Train 0.5 | Epoch: 101 | Iter: 142600 | Total Loss: 0.003098 | Recon Loss: 0.002716 | Commit Loss: 0.000765 | Perplexity: 3114.844630
2025-10-09 19:44:35,898 Stage: Train 0.5 | Epoch: 101 | Iter: 142800 | Total Loss: 0.003148 | Recon Loss: 0.002764 | Commit Loss: 0.000768 | Perplexity: 3105.703884
2025-10-09 19:48:28,138 Stage: Train 0.5 | Epoch: 101 | Iter: 143000 | Total Loss: 0.003050 | Recon Loss: 0.002663 | Commit Loss: 0.000774 | Perplexity: 3108.071672
2025-10-09 19:52:20,248 Stage: Train 0.5 | Epoch: 101 | Iter: 143200 | Total Loss: 0.003145 | Recon Loss: 0.002762 | Commit Loss: 0.000766 | Perplexity: 3091.000057
Trainning Epoch:  26%|██▌       | 129/494 [33:18:43<119:40:51, 1180.41s/it]Trainning Epoch:  26%|██▌       | 129/494 [33:18:43<119:40:51, 1180.42s/it]2025-10-09 19:56:14,779 Stage: Train 0.5 | Epoch: 102 | Iter: 143400 | Total Loss: 0.003108 | Recon Loss: 0.002718 | Commit Loss: 0.000780 | Perplexity: 3104.020040
2025-10-09 20:00:06,705 Stage: Train 0.5 | Epoch: 102 | Iter: 143600 | Total Loss: 0.003082 | Recon Loss: 0.002706 | Commit Loss: 0.000750 | Perplexity: 3104.871587
2025-10-09 20:03:58,314 Stage: Train 0.5 | Epoch: 102 | Iter: 143800 | Total Loss: 0.003088 | Recon Loss: 0.002701 | Commit Loss: 0.000773 | Perplexity: 3106.415120
2025-10-09 20:07:49,800 Stage: Train 0.5 | Epoch: 102 | Iter: 144000 | Total Loss: 0.003096 | Recon Loss: 0.002713 | Commit Loss: 0.000766 | Perplexity: 3104.359070
2025-10-09 20:11:42,019 Stage: Train 0.5 | Epoch: 102 | Iter: 144200 | Total Loss: 0.003113 | Recon Loss: 0.002732 | Commit Loss: 0.000762 | Perplexity: 3100.532521
Trainning Epoch:  26%|██▋       | 130/494 [33:38:21<119:15:25, 1179.47s/it]Trainning Epoch:  26%|██▋       | 130/494 [33:38:21<119:15:25, 1179.47s/it]2025-10-09 20:15:38,167 Stage: Train 0.5 | Epoch: 103 | Iter: 144400 | Total Loss: 0.003081 | Recon Loss: 0.002697 | Commit Loss: 0.000768 | Perplexity: 3103.904127
2025-10-09 20:19:31,803 Stage: Train 0.5 | Epoch: 103 | Iter: 144600 | Total Loss: 0.003164 | Recon Loss: 0.002788 | Commit Loss: 0.000752 | Perplexity: 3112.357238
2025-10-09 20:23:25,541 Stage: Train 0.5 | Epoch: 103 | Iter: 144800 | Total Loss: 0.003045 | Recon Loss: 0.002664 | Commit Loss: 0.000762 | Perplexity: 3104.913295
2025-10-09 20:27:19,199 Stage: Train 0.5 | Epoch: 103 | Iter: 145000 | Total Loss: 0.003104 | Recon Loss: 0.002724 | Commit Loss: 0.000761 | Perplexity: 3096.338843
2025-10-09 20:31:12,609 Stage: Train 0.5 | Epoch: 103 | Iter: 145200 | Total Loss: 0.003078 | Recon Loss: 0.002691 | Commit Loss: 0.000774 | Perplexity: 3099.879930
Trainning Epoch:  27%|██▋       | 131/494 [33:58:07<119:08:05, 1181.50s/it]Trainning Epoch:  27%|██▋       | 131/494 [33:58:07<119:08:06, 1181.50s/it]2025-10-09 20:35:09,504 Stage: Train 0.5 | Epoch: 104 | Iter: 145400 | Total Loss: 0.003114 | Recon Loss: 0.002728 | Commit Loss: 0.000772 | Perplexity: 3102.529854
2025-10-09 20:39:03,217 Stage: Train 0.5 | Epoch: 104 | Iter: 145600 | Total Loss: 0.003037 | Recon Loss: 0.002651 | Commit Loss: 0.000772 | Perplexity: 3101.839623
2025-10-09 20:42:56,574 Stage: Train 0.5 | Epoch: 104 | Iter: 145800 | Total Loss: 0.003166 | Recon Loss: 0.002787 | Commit Loss: 0.000758 | Perplexity: 3101.766677
2025-10-09 20:46:49,921 Stage: Train 0.5 | Epoch: 104 | Iter: 146000 | Total Loss: 0.003047 | Recon Loss: 0.002662 | Commit Loss: 0.000768 | Perplexity: 3097.843323
2025-10-09 20:50:43,498 Stage: Train 0.5 | Epoch: 104 | Iter: 146200 | Total Loss: 0.003078 | Recon Loss: 0.002695 | Commit Loss: 0.000767 | Perplexity: 3108.636860
Trainning Epoch:  27%|██▋       | 132/494 [34:17:53<118:56:30, 1182.85s/it]Trainning Epoch:  27%|██▋       | 132/494 [34:17:53<118:56:30, 1182.85s/it]2025-10-09 20:54:38,930 Stage: Train 0.5 | Epoch: 105 | Iter: 146400 | Total Loss: 0.003089 | Recon Loss: 0.002701 | Commit Loss: 0.000778 | Perplexity: 3114.924211
2025-10-09 20:58:29,002 Stage: Train 0.5 | Epoch: 105 | Iter: 146600 | Total Loss: 0.003058 | Recon Loss: 0.002673 | Commit Loss: 0.000770 | Perplexity: 3102.392723
2025-10-09 21:02:19,439 Stage: Train 0.5 | Epoch: 105 | Iter: 146800 | Total Loss: 0.003094 | Recon Loss: 0.002713 | Commit Loss: 0.000764 | Perplexity: 3100.129471
2025-10-09 21:06:10,352 Stage: Train 0.5 | Epoch: 105 | Iter: 147000 | Total Loss: 0.003068 | Recon Loss: 0.002687 | Commit Loss: 0.000762 | Perplexity: 3099.964801
2025-10-09 21:10:01,149 Stage: Train 0.5 | Epoch: 105 | Iter: 147200 | Total Loss: 0.003096 | Recon Loss: 0.002713 | Commit Loss: 0.000766 | Perplexity: 3099.670420
Trainning Epoch:  27%|██▋       | 133/494 [34:37:24<118:15:03, 1179.23s/it]Trainning Epoch:  27%|██▋       | 133/494 [34:37:24<118:15:04, 1179.24s/it]2025-10-09 21:13:56,047 Stage: Train 0.5 | Epoch: 106 | Iter: 147400 | Total Loss: 0.003032 | Recon Loss: 0.002649 | Commit Loss: 0.000766 | Perplexity: 3097.698561
2025-10-09 21:17:49,256 Stage: Train 0.5 | Epoch: 106 | Iter: 147600 | Total Loss: 0.003079 | Recon Loss: 0.002698 | Commit Loss: 0.000763 | Perplexity: 3102.777422
2025-10-09 21:21:41,868 Stage: Train 0.5 | Epoch: 106 | Iter: 147800 | Total Loss: 0.003130 | Recon Loss: 0.002750 | Commit Loss: 0.000759 | Perplexity: 3105.129574
2025-10-09 21:25:34,882 Stage: Train 0.5 | Epoch: 106 | Iter: 148000 | Total Loss: 0.003055 | Recon Loss: 0.002670 | Commit Loss: 0.000770 | Perplexity: 3110.284427
2025-10-09 21:29:27,749 Stage: Train 0.5 | Epoch: 106 | Iter: 148200 | Total Loss: 0.003036 | Recon Loss: 0.002652 | Commit Loss: 0.000769 | Perplexity: 3099.751317
Trainning Epoch:  27%|██▋       | 134/494 [34:57:07<118:02:50, 1180.47s/it]Trainning Epoch:  27%|██▋       | 134/494 [34:57:07<118:02:50, 1180.47s/it]2025-10-09 21:33:23,670 Stage: Train 0.5 | Epoch: 107 | Iter: 148400 | Total Loss: 0.003095 | Recon Loss: 0.002712 | Commit Loss: 0.000766 | Perplexity: 3101.039547
2025-10-09 21:37:16,445 Stage: Train 0.5 | Epoch: 107 | Iter: 148600 | Total Loss: 0.003059 | Recon Loss: 0.002675 | Commit Loss: 0.000767 | Perplexity: 3112.446051
2025-10-09 21:41:09,270 Stage: Train 0.5 | Epoch: 107 | Iter: 148800 | Total Loss: 0.003100 | Recon Loss: 0.002717 | Commit Loss: 0.000767 | Perplexity: 3103.529878
2025-10-09 21:45:02,034 Stage: Train 0.5 | Epoch: 107 | Iter: 149000 | Total Loss: 0.003048 | Recon Loss: 0.002661 | Commit Loss: 0.000774 | Perplexity: 3109.333829
2025-10-09 21:48:54,758 Stage: Train 0.5 | Epoch: 107 | Iter: 149200 | Total Loss: 0.003075 | Recon Loss: 0.002691 | Commit Loss: 0.000767 | Perplexity: 3112.873386
2025-10-09 21:52:47,473 Stage: Train 0.5 | Epoch: 107 | Iter: 149400 | Total Loss: 0.003079 | Recon Loss: 0.002698 | Commit Loss: 0.000763 | Perplexity: 3121.006437
Trainning Epoch:  27%|██▋       | 135/494 [35:16:49<117:46:03, 1180.96s/it]Trainning Epoch:  27%|██▋       | 135/494 [35:16:49<117:46:03, 1180.96s/it]2025-10-09 21:56:43,308 Stage: Train 0.5 | Epoch: 108 | Iter: 149600 | Total Loss: 0.003020 | Recon Loss: 0.002637 | Commit Loss: 0.000766 | Perplexity: 3106.175575
2025-10-09 22:00:35,845 Stage: Train 0.5 | Epoch: 108 | Iter: 149800 | Total Loss: 0.003084 | Recon Loss: 0.002700 | Commit Loss: 0.000768 | Perplexity: 3100.781279
2025-10-09 22:04:29,069 Stage: Train 0.5 | Epoch: 108 | Iter: 150000 | Total Loss: 0.003100 | Recon Loss: 0.002712 | Commit Loss: 0.000775 | Perplexity: 3109.955398
2025-10-09 22:08:22,032 Stage: Train 0.5 | Epoch: 108 | Iter: 150200 | Total Loss: 0.003107 | Recon Loss: 0.002729 | Commit Loss: 0.000757 | Perplexity: 3104.862700
2025-10-09 22:12:15,383 Stage: Train 0.5 | Epoch: 108 | Iter: 150400 | Total Loss: 0.003084 | Recon Loss: 0.002699 | Commit Loss: 0.000770 | Perplexity: 3107.371078
Trainning Epoch:  28%|██▊       | 136/494 [35:36:32<117:30:12, 1181.60s/it]Trainning Epoch:  28%|██▊       | 136/494 [35:36:32<117:30:12, 1181.60s/it]2025-10-09 22:16:09,174 Stage: Train 0.5 | Epoch: 109 | Iter: 150600 | Total Loss: 0.003063 | Recon Loss: 0.002682 | Commit Loss: 0.000761 | Perplexity: 3108.623226
2025-10-09 22:20:00,357 Stage: Train 0.5 | Epoch: 109 | Iter: 150800 | Total Loss: 0.003076 | Recon Loss: 0.002687 | Commit Loss: 0.000779 | Perplexity: 3111.529863
2025-10-09 22:23:51,205 Stage: Train 0.5 | Epoch: 109 | Iter: 151000 | Total Loss: 0.003061 | Recon Loss: 0.002677 | Commit Loss: 0.000767 | Perplexity: 3106.580396
2025-10-09 22:27:42,113 Stage: Train 0.5 | Epoch: 109 | Iter: 151200 | Total Loss: 0.003065 | Recon Loss: 0.002685 | Commit Loss: 0.000760 | Perplexity: 3108.160891
2025-10-09 22:31:32,847 Stage: Train 0.5 | Epoch: 109 | Iter: 151400 | Total Loss: 0.003011 | Recon Loss: 0.002638 | Commit Loss: 0.000747 | Perplexity: 3101.354082
Trainning Epoch:  28%|██▊       | 137/494 [35:56:05<116:53:55, 1178.81s/it]Trainning Epoch:  28%|██▊       | 137/494 [35:56:05<116:53:55, 1178.81s/it]2025-10-09 22:35:27,386 Stage: Train 0.5 | Epoch: 110 | Iter: 151600 | Total Loss: 0.003026 | Recon Loss: 0.002645 | Commit Loss: 0.000762 | Perplexity: 3106.323417
2025-10-09 22:39:18,532 Stage: Train 0.5 | Epoch: 110 | Iter: 151800 | Total Loss: 0.003031 | Recon Loss: 0.002650 | Commit Loss: 0.000763 | Perplexity: 3113.542736
2025-10-09 22:43:09,876 Stage: Train 0.5 | Epoch: 110 | Iter: 152000 | Total Loss: 0.003070 | Recon Loss: 0.002686 | Commit Loss: 0.000767 | Perplexity: 3113.924343
2025-10-09 22:47:01,112 Stage: Train 0.5 | Epoch: 110 | Iter: 152200 | Total Loss: 0.003034 | Recon Loss: 0.002655 | Commit Loss: 0.000758 | Perplexity: 3110.912219
2025-10-09 22:50:52,930 Stage: Train 0.5 | Epoch: 110 | Iter: 152400 | Total Loss: 0.003049 | Recon Loss: 0.002668 | Commit Loss: 0.000763 | Perplexity: 3113.832559
Trainning Epoch:  28%|██▊       | 138/494 [36:15:40<116:27:50, 1177.73s/it]Trainning Epoch:  28%|██▊       | 138/494 [36:15:40<116:27:50, 1177.73s/it]2025-10-09 22:54:48,152 Stage: Train 0.5 | Epoch: 111 | Iter: 152600 | Total Loss: 0.003045 | Recon Loss: 0.002664 | Commit Loss: 0.000763 | Perplexity: 3111.422816
2025-10-09 22:58:40,357 Stage: Train 0.5 | Epoch: 111 | Iter: 152800 | Total Loss: 0.003040 | Recon Loss: 0.002658 | Commit Loss: 0.000763 | Perplexity: 3104.736022
2025-10-09 23:02:32,582 Stage: Train 0.5 | Epoch: 111 | Iter: 153000 | Total Loss: 0.002977 | Recon Loss: 0.002599 | Commit Loss: 0.000756 | Perplexity: 3109.399702
2025-10-09 23:06:24,631 Stage: Train 0.5 | Epoch: 111 | Iter: 153200 | Total Loss: 0.003105 | Recon Loss: 0.002717 | Commit Loss: 0.000776 | Perplexity: 3107.531655
2025-10-09 23:10:16,737 Stage: Train 0.5 | Epoch: 111 | Iter: 153400 | Total Loss: 0.003023 | Recon Loss: 0.002643 | Commit Loss: 0.000761 | Perplexity: 3105.747415
Trainning Epoch:  28%|██▊       | 139/494 [36:35:18<116:09:45, 1177.99s/it]Trainning Epoch:  28%|██▊       | 139/494 [36:35:18<116:09:45, 1177.99s/it]2025-10-09 23:14:11,241 Stage: Train 0.5 | Epoch: 112 | Iter: 153600 | Total Loss: 0.003061 | Recon Loss: 0.002677 | Commit Loss: 0.000769 | Perplexity: 3108.101339
2025-10-09 23:18:03,657 Stage: Train 0.5 | Epoch: 112 | Iter: 153800 | Total Loss: 0.003054 | Recon Loss: 0.002670 | Commit Loss: 0.000766 | Perplexity: 3107.157644
2025-10-09 23:21:56,008 Stage: Train 0.5 | Epoch: 112 | Iter: 154000 | Total Loss: 0.003041 | Recon Loss: 0.002658 | Commit Loss: 0.000765 | Perplexity: 3112.662457
2025-10-09 23:25:48,421 Stage: Train 0.5 | Epoch: 112 | Iter: 154200 | Total Loss: 0.003059 | Recon Loss: 0.002684 | Commit Loss: 0.000749 | Perplexity: 3111.202074
2025-10-09 23:29:40,914 Stage: Train 0.5 | Epoch: 112 | Iter: 154400 | Total Loss: 0.003037 | Recon Loss: 0.002657 | Commit Loss: 0.000760 | Perplexity: 3103.565309
Trainning Epoch:  28%|██▊       | 140/494 [36:54:58<115:52:57, 1178.47s/it]Trainning Epoch:  28%|██▊       | 140/494 [36:54:58<115:52:58, 1178.47s/it]2025-10-09 23:33:34,992 Stage: Train 0.5 | Epoch: 113 | Iter: 154600 | Total Loss: 0.003039 | Recon Loss: 0.002663 | Commit Loss: 0.000753 | Perplexity: 3104.330836
2025-10-09 23:37:25,585 Stage: Train 0.5 | Epoch: 113 | Iter: 154800 | Total Loss: 0.003040 | Recon Loss: 0.002656 | Commit Loss: 0.000768 | Perplexity: 3115.517549
2025-10-09 23:41:16,873 Stage: Train 0.5 | Epoch: 113 | Iter: 155000 | Total Loss: 0.003008 | Recon Loss: 0.002622 | Commit Loss: 0.000772 | Perplexity: 3109.212325
2025-10-09 23:45:08,138 Stage: Train 0.5 | Epoch: 113 | Iter: 155200 | Total Loss: 0.003074 | Recon Loss: 0.002693 | Commit Loss: 0.000762 | Perplexity: 3114.251501
2025-10-09 23:48:59,232 Stage: Train 0.5 | Epoch: 113 | Iter: 155400 | Total Loss: 0.003026 | Recon Loss: 0.002643 | Commit Loss: 0.000766 | Perplexity: 3118.175048
Trainning Epoch:  29%|██▊       | 141/494 [37:14:31<115:23:10, 1176.74s/it]Trainning Epoch:  29%|██▊       | 141/494 [37:14:31<115:23:12, 1176.75s/it]2025-10-09 23:52:53,776 Stage: Train 0.5 | Epoch: 114 | Iter: 155600 | Total Loss: 0.003050 | Recon Loss: 0.002672 | Commit Loss: 0.000756 | Perplexity: 3117.915620
2025-10-09 23:56:45,377 Stage: Train 0.5 | Epoch: 114 | Iter: 155800 | Total Loss: 0.003009 | Recon Loss: 0.002631 | Commit Loss: 0.000757 | Perplexity: 3097.463507
2025-10-10 00:00:37,143 Stage: Train 0.5 | Epoch: 114 | Iter: 156000 | Total Loss: 0.003059 | Recon Loss: 0.002680 | Commit Loss: 0.000759 | Perplexity: 3113.018654
2025-10-10 00:04:28,992 Stage: Train 0.5 | Epoch: 114 | Iter: 156200 | Total Loss: 0.003005 | Recon Loss: 0.002623 | Commit Loss: 0.000764 | Perplexity: 3119.952998
2025-10-10 00:08:20,927 Stage: Train 0.5 | Epoch: 114 | Iter: 156400 | Total Loss: 0.003075 | Recon Loss: 0.002697 | Commit Loss: 0.000757 | Perplexity: 3111.889845
Trainning Epoch:  29%|██▊       | 142/494 [37:34:08<115:04:37, 1176.92s/it]Trainning Epoch:  29%|██▊       | 142/494 [37:34:08<115:04:37, 1176.92s/it]2025-10-10 00:12:16,351 Stage: Train 0.5 | Epoch: 115 | Iter: 156600 | Total Loss: 0.003058 | Recon Loss: 0.002678 | Commit Loss: 0.000760 | Perplexity: 3112.853069
2025-10-10 00:16:09,466 Stage: Train 0.5 | Epoch: 115 | Iter: 156800 | Total Loss: 0.003031 | Recon Loss: 0.002655 | Commit Loss: 0.000753 | Perplexity: 3113.282300
2025-10-10 00:20:02,407 Stage: Train 0.5 | Epoch: 115 | Iter: 157000 | Total Loss: 0.002991 | Recon Loss: 0.002611 | Commit Loss: 0.000761 | Perplexity: 3110.315139
2025-10-10 00:23:55,668 Stage: Train 0.5 | Epoch: 115 | Iter: 157200 | Total Loss: 0.003015 | Recon Loss: 0.002636 | Commit Loss: 0.000759 | Perplexity: 3115.753511
2025-10-10 00:27:48,998 Stage: Train 0.5 | Epoch: 115 | Iter: 157400 | Total Loss: 0.003080 | Recon Loss: 0.002699 | Commit Loss: 0.000761 | Perplexity: 3115.554363
Trainning Epoch:  29%|██▉       | 143/494 [37:53:52<114:57:02, 1178.98s/it]Trainning Epoch:  29%|██▉       | 143/494 [37:53:52<114:57:02, 1178.98s/it]2025-10-10 00:31:45,014 Stage: Train 0.5 | Epoch: 116 | Iter: 157600 | Total Loss: 0.003008 | Recon Loss: 0.002630 | Commit Loss: 0.000756 | Perplexity: 3112.685785
2025-10-10 00:35:37,890 Stage: Train 0.5 | Epoch: 116 | Iter: 157800 | Total Loss: 0.003012 | Recon Loss: 0.002632 | Commit Loss: 0.000761 | Perplexity: 3114.320887
2025-10-10 00:39:30,925 Stage: Train 0.5 | Epoch: 116 | Iter: 158000 | Total Loss: 0.002987 | Recon Loss: 0.002609 | Commit Loss: 0.000757 | Perplexity: 3115.202938
2025-10-10 00:43:23,837 Stage: Train 0.5 | Epoch: 116 | Iter: 158200 | Total Loss: 0.003029 | Recon Loss: 0.002650 | Commit Loss: 0.000757 | Perplexity: 3110.519504
2025-10-10 00:47:16,860 Stage: Train 0.5 | Epoch: 116 | Iter: 158400 | Total Loss: 0.003021 | Recon Loss: 0.002637 | Commit Loss: 0.000767 | Perplexity: 3131.259539
Trainning Epoch:  29%|██▉       | 144/494 [38:13:35<114:44:13, 1180.15s/it]Trainning Epoch:  29%|██▉       | 144/494 [38:13:35<114:44:14, 1180.16s/it]2025-10-10 00:51:11,616 Stage: Train 0.5 | Epoch: 117 | Iter: 158600 | Total Loss: 0.003019 | Recon Loss: 0.002633 | Commit Loss: 0.000771 | Perplexity: 3108.393055
2025-10-10 00:55:00,021 Stage: Train 0.5 | Epoch: 117 | Iter: 158800 | Total Loss: 0.003008 | Recon Loss: 0.002625 | Commit Loss: 0.000766 | Perplexity: 3111.703862
2025-10-10 00:58:48,738 Stage: Train 0.5 | Epoch: 117 | Iter: 159000 | Total Loss: 0.003006 | Recon Loss: 0.002631 | Commit Loss: 0.000750 | Perplexity: 3110.517188
2025-10-10 01:02:37,547 Stage: Train 0.5 | Epoch: 117 | Iter: 159200 | Total Loss: 0.002999 | Recon Loss: 0.002616 | Commit Loss: 0.000765 | Perplexity: 3125.287068
2025-10-10 01:06:26,836 Stage: Train 0.5 | Epoch: 117 | Iter: 159400 | Total Loss: 0.003005 | Recon Loss: 0.002626 | Commit Loss: 0.000758 | Perplexity: 3116.649780
Trainning Epoch:  29%|██▉       | 145/494 [38:32:58<113:54:54, 1175.06s/it]Trainning Epoch:  29%|██▉       | 145/494 [38:32:58<113:54:54, 1175.06s/it]2025-10-10 01:10:20,524 Stage: Train 0.5 | Epoch: 118 | Iter: 159600 | Total Loss: 0.002991 | Recon Loss: 0.002609 | Commit Loss: 0.000764 | Perplexity: 3119.043759
2025-10-10 01:14:12,790 Stage: Train 0.5 | Epoch: 118 | Iter: 159800 | Total Loss: 0.003033 | Recon Loss: 0.002651 | Commit Loss: 0.000764 | Perplexity: 3119.396287
2025-10-10 01:18:05,584 Stage: Train 0.5 | Epoch: 118 | Iter: 160000 | Total Loss: 0.003028 | Recon Loss: 0.002650 | Commit Loss: 0.000758 | Perplexity: 3114.269115
2025-10-10 01:18:05,584 Saving model at iteration 160000
2025-10-10 01:18:05,861 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000
2025-10-10 01:18:07,360 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000/model.safetensors
2025-10-10 01:18:09,038 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000/optimizer.bin
2025-10-10 01:18:09,038 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000/scheduler.bin
2025-10-10 01:18:09,038 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000/sampler.bin
2025-10-10 01:18:09,039 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_160000/random_states_0.pkl
2025-10-10 01:22:01,362 Stage: Train 0.5 | Epoch: 118 | Iter: 160200 | Total Loss: 0.002965 | Recon Loss: 0.002582 | Commit Loss: 0.000765 | Perplexity: 3116.944884
2025-10-10 01:25:53,657 Stage: Train 0.5 | Epoch: 118 | Iter: 160400 | Total Loss: 0.003010 | Recon Loss: 0.002629 | Commit Loss: 0.000763 | Perplexity: 3117.741785
Trainning Epoch:  30%|██▉       | 146/494 [38:52:41<113:49:42, 1177.54s/it]Trainning Epoch:  30%|██▉       | 146/494 [38:52:41<113:49:42, 1177.54s/it]2025-10-10 01:29:48,602 Stage: Train 0.5 | Epoch: 119 | Iter: 160600 | Total Loss: 0.002981 | Recon Loss: 0.002603 | Commit Loss: 0.000757 | Perplexity: 3118.502179
2025-10-10 01:33:39,806 Stage: Train 0.5 | Epoch: 119 | Iter: 160800 | Total Loss: 0.002983 | Recon Loss: 0.002609 | Commit Loss: 0.000749 | Perplexity: 3115.749364
2025-10-10 01:37:31,441 Stage: Train 0.5 | Epoch: 119 | Iter: 161000 | Total Loss: 0.003038 | Recon Loss: 0.002653 | Commit Loss: 0.000769 | Perplexity: 3117.154958
2025-10-10 01:41:22,566 Stage: Train 0.5 | Epoch: 119 | Iter: 161200 | Total Loss: 0.003023 | Recon Loss: 0.002643 | Commit Loss: 0.000759 | Perplexity: 3112.185300
2025-10-10 01:45:14,381 Stage: Train 0.5 | Epoch: 119 | Iter: 161400 | Total Loss: 0.002963 | Recon Loss: 0.002579 | Commit Loss: 0.000767 | Perplexity: 3111.481324
Trainning Epoch:  30%|██▉       | 147/494 [39:12:16<113:26:02, 1176.84s/it]Trainning Epoch:  30%|██▉       | 147/494 [39:12:16<113:26:02, 1176.84s/it]2025-10-10 01:49:08,786 Stage: Train 0.5 | Epoch: 120 | Iter: 161600 | Total Loss: 0.003018 | Recon Loss: 0.002633 | Commit Loss: 0.000771 | Perplexity: 3112.213029
2025-10-10 01:52:59,594 Stage: Train 0.5 | Epoch: 120 | Iter: 161800 | Total Loss: 0.003004 | Recon Loss: 0.002624 | Commit Loss: 0.000761 | Perplexity: 3113.880647
2025-10-10 01:56:50,401 Stage: Train 0.5 | Epoch: 120 | Iter: 162000 | Total Loss: 0.002990 | Recon Loss: 0.002613 | Commit Loss: 0.000755 | Perplexity: 3113.170790
2025-10-10 02:00:41,787 Stage: Train 0.5 | Epoch: 120 | Iter: 162200 | Total Loss: 0.003009 | Recon Loss: 0.002631 | Commit Loss: 0.000757 | Perplexity: 3127.747063
2025-10-10 02:04:33,118 Stage: Train 0.5 | Epoch: 120 | Iter: 162400 | Total Loss: 0.002949 | Recon Loss: 0.002568 | Commit Loss: 0.000762 | Perplexity: 3114.830039
Trainning Epoch:  30%|██▉       | 148/494 [39:31:50<113:00:50, 1175.87s/it]Trainning Epoch:  30%|██▉       | 148/494 [39:31:50<113:00:50, 1175.87s/it]2025-10-10 02:08:26,919 Stage: Train 0.5 | Epoch: 121 | Iter: 162600 | Total Loss: 0.002981 | Recon Loss: 0.002603 | Commit Loss: 0.000757 | Perplexity: 3120.044773
2025-10-10 02:12:16,933 Stage: Train 0.5 | Epoch: 121 | Iter: 162800 | Total Loss: 0.002971 | Recon Loss: 0.002590 | Commit Loss: 0.000761 | Perplexity: 3119.027642
2025-10-10 02:16:07,478 Stage: Train 0.5 | Epoch: 121 | Iter: 163000 | Total Loss: 0.003036 | Recon Loss: 0.002655 | Commit Loss: 0.000762 | Perplexity: 3105.771310
2025-10-10 02:19:57,951 Stage: Train 0.5 | Epoch: 121 | Iter: 163200 | Total Loss: 0.002945 | Recon Loss: 0.002568 | Commit Loss: 0.000753 | Perplexity: 3118.053270
2025-10-10 02:23:48,066 Stage: Train 0.5 | Epoch: 121 | Iter: 163400 | Total Loss: 0.002980 | Recon Loss: 0.002595 | Commit Loss: 0.000770 | Perplexity: 3124.624415
Trainning Epoch:  30%|███       | 149/494 [39:51:19<112:29:34, 1173.84s/it]Trainning Epoch:  30%|███       | 149/494 [39:51:19<112:29:34, 1173.84s/it]2025-10-10 02:27:41,617 Stage: Train 0.5 | Epoch: 122 | Iter: 163600 | Total Loss: 0.003020 | Recon Loss: 0.002629 | Commit Loss: 0.000781 | Perplexity: 3120.031626
2025-10-10 02:31:31,767 Stage: Train 0.5 | Epoch: 122 | Iter: 163800 | Total Loss: 0.003021 | Recon Loss: 0.002646 | Commit Loss: 0.000752 | Perplexity: 3122.145269
2025-10-10 02:35:21,907 Stage: Train 0.5 | Epoch: 122 | Iter: 164000 | Total Loss: 0.002984 | Recon Loss: 0.002603 | Commit Loss: 0.000762 | Perplexity: 3125.466996
2025-10-10 02:39:12,526 Stage: Train 0.5 | Epoch: 122 | Iter: 164200 | Total Loss: 0.003039 | Recon Loss: 0.002656 | Commit Loss: 0.000767 | Perplexity: 3111.914570
2025-10-10 02:43:02,817 Stage: Train 0.5 | Epoch: 122 | Iter: 164400 | Total Loss: 0.002971 | Recon Loss: 0.002590 | Commit Loss: 0.000761 | Perplexity: 3111.606558
Trainning Epoch:  30%|███       | 150/494 [40:10:50<112:04:11, 1172.83s/it]Trainning Epoch:  30%|███       | 150/494 [40:10:50<112:04:12, 1172.83s/it]2025-10-10 02:46:57,169 Stage: Train 0.5 | Epoch: 123 | Iter: 164600 | Total Loss: 0.002941 | Recon Loss: 0.002565 | Commit Loss: 0.000750 | Perplexity: 3109.469567
2025-10-10 02:50:49,662 Stage: Train 0.5 | Epoch: 123 | Iter: 164800 | Total Loss: 0.002980 | Recon Loss: 0.002605 | Commit Loss: 0.000751 | Perplexity: 3119.942341
2025-10-10 02:54:43,426 Stage: Train 0.5 | Epoch: 123 | Iter: 165000 | Total Loss: 0.003006 | Recon Loss: 0.002627 | Commit Loss: 0.000758 | Perplexity: 3116.287687
2025-10-10 02:58:37,282 Stage: Train 0.5 | Epoch: 123 | Iter: 165200 | Total Loss: 0.002969 | Recon Loss: 0.002593 | Commit Loss: 0.000753 | Perplexity: 3126.957686
2025-10-10 03:02:31,107 Stage: Train 0.5 | Epoch: 123 | Iter: 165400 | Total Loss: 0.002947 | Recon Loss: 0.002563 | Commit Loss: 0.000767 | Perplexity: 3127.266680
2025-10-10 03:06:24,725 Stage: Train 0.5 | Epoch: 123 | Iter: 165600 | Total Loss: 0.003023 | Recon Loss: 0.002645 | Commit Loss: 0.000755 | Perplexity: 3118.359341
Trainning Epoch:  31%|███       | 151/494 [40:30:36<112:07:21, 1176.80s/it]Trainning Epoch:  31%|███       | 151/494 [40:30:36<112:07:21, 1176.80s/it]2025-10-10 03:10:18,483 Stage: Train 0.5 | Epoch: 124 | Iter: 165800 | Total Loss: 0.002977 | Recon Loss: 0.002592 | Commit Loss: 0.000771 | Perplexity: 3126.091218
2025-10-10 03:14:09,640 Stage: Train 0.5 | Epoch: 124 | Iter: 166000 | Total Loss: 0.002964 | Recon Loss: 0.002585 | Commit Loss: 0.000758 | Perplexity: 3118.321885
2025-10-10 03:18:00,379 Stage: Train 0.5 | Epoch: 124 | Iter: 166200 | Total Loss: 0.002947 | Recon Loss: 0.002565 | Commit Loss: 0.000764 | Perplexity: 3116.658597
2025-10-10 03:21:51,766 Stage: Train 0.5 | Epoch: 124 | Iter: 166400 | Total Loss: 0.003018 | Recon Loss: 0.002638 | Commit Loss: 0.000759 | Perplexity: 3111.236227
2025-10-10 03:25:43,250 Stage: Train 0.5 | Epoch: 124 | Iter: 166600 | Total Loss: 0.002899 | Recon Loss: 0.002521 | Commit Loss: 0.000756 | Perplexity: 3122.707040
Trainning Epoch:  31%|███       | 152/494 [40:50:09<111:42:01, 1175.79s/it]Trainning Epoch:  31%|███       | 152/494 [40:50:09<111:42:02, 1175.80s/it]2025-10-10 03:29:38,926 Stage: Train 0.5 | Epoch: 125 | Iter: 166800 | Total Loss: 0.003012 | Recon Loss: 0.002638 | Commit Loss: 0.000747 | Perplexity: 3118.478187
2025-10-10 03:33:31,921 Stage: Train 0.5 | Epoch: 125 | Iter: 167000 | Total Loss: 0.002909 | Recon Loss: 0.002528 | Commit Loss: 0.000761 | Perplexity: 3128.662118
2025-10-10 03:37:24,170 Stage: Train 0.5 | Epoch: 125 | Iter: 167200 | Total Loss: 0.002977 | Recon Loss: 0.002597 | Commit Loss: 0.000760 | Perplexity: 3127.735214
2025-10-10 03:41:17,012 Stage: Train 0.5 | Epoch: 125 | Iter: 167400 | Total Loss: 0.002983 | Recon Loss: 0.002600 | Commit Loss: 0.000767 | Perplexity: 3128.082837
2025-10-10 03:45:09,152 Stage: Train 0.5 | Epoch: 125 | Iter: 167600 | Total Loss: 0.002973 | Recon Loss: 0.002599 | Commit Loss: 0.000749 | Perplexity: 3113.010873
Trainning Epoch:  31%|███       | 153/494 [41:09:50<111:31:42, 1177.43s/it]Trainning Epoch:  31%|███       | 153/494 [41:09:50<111:31:43, 1177.43s/it]2025-10-10 03:49:04,146 Stage: Train 0.5 | Epoch: 126 | Iter: 167800 | Total Loss: 0.002959 | Recon Loss: 0.002584 | Commit Loss: 0.000751 | Perplexity: 3124.895934
2025-10-10 03:52:55,991 Stage: Train 0.5 | Epoch: 126 | Iter: 168000 | Total Loss: 0.002926 | Recon Loss: 0.002545 | Commit Loss: 0.000761 | Perplexity: 3121.201223
2025-10-10 03:56:47,395 Stage: Train 0.5 | Epoch: 126 | Iter: 168200 | Total Loss: 0.002976 | Recon Loss: 0.002594 | Commit Loss: 0.000764 | Perplexity: 3129.904176
2025-10-10 04:00:39,102 Stage: Train 0.5 | Epoch: 126 | Iter: 168400 | Total Loss: 0.002938 | Recon Loss: 0.002559 | Commit Loss: 0.000757 | Perplexity: 3117.810509
2025-10-10 04:04:31,430 Stage: Train 0.5 | Epoch: 126 | Iter: 168600 | Total Loss: 0.002969 | Recon Loss: 0.002591 | Commit Loss: 0.000755 | Perplexity: 3119.239077
Trainning Epoch:  31%|███       | 154/494 [41:29:28<111:11:42, 1177.36s/it]Trainning Epoch:  31%|███       | 154/494 [41:29:28<111:11:42, 1177.36s/it]2025-10-10 04:08:28,144 Stage: Train 0.5 | Epoch: 127 | Iter: 168800 | Total Loss: 0.002925 | Recon Loss: 0.002544 | Commit Loss: 0.000762 | Perplexity: 3124.360253
2025-10-10 04:12:20,618 Stage: Train 0.5 | Epoch: 127 | Iter: 169000 | Total Loss: 0.002954 | Recon Loss: 0.002575 | Commit Loss: 0.000760 | Perplexity: 3121.787278
2025-10-10 04:16:13,724 Stage: Train 0.5 | Epoch: 127 | Iter: 169200 | Total Loss: 0.002965 | Recon Loss: 0.002586 | Commit Loss: 0.000759 | Perplexity: 3127.688254
2025-10-10 04:20:06,296 Stage: Train 0.5 | Epoch: 127 | Iter: 169400 | Total Loss: 0.003004 | Recon Loss: 0.002627 | Commit Loss: 0.000754 | Perplexity: 3114.779990
2025-10-10 04:23:58,639 Stage: Train 0.5 | Epoch: 127 | Iter: 169600 | Total Loss: 0.002971 | Recon Loss: 0.002595 | Commit Loss: 0.000753 | Perplexity: 3119.290410
Trainning Epoch:  31%|███▏      | 155/494 [41:49:10<111:00:16, 1178.81s/it]Trainning Epoch:  31%|███▏      | 155/494 [41:49:10<111:00:17, 1178.81s/it]2025-10-10 04:27:50,750 Stage: Train 0.5 | Epoch: 128 | Iter: 169800 | Total Loss: 0.002923 | Recon Loss: 0.002542 | Commit Loss: 0.000762 | Perplexity: 3120.527017
2025-10-10 04:31:39,015 Stage: Train 0.5 | Epoch: 128 | Iter: 170000 | Total Loss: 0.002952 | Recon Loss: 0.002574 | Commit Loss: 0.000757 | Perplexity: 3126.035825
2025-10-10 04:35:27,109 Stage: Train 0.5 | Epoch: 128 | Iter: 170200 | Total Loss: 0.002931 | Recon Loss: 0.002552 | Commit Loss: 0.000758 | Perplexity: 3127.919456
2025-10-10 04:39:16,033 Stage: Train 0.5 | Epoch: 128 | Iter: 170400 | Total Loss: 0.002950 | Recon Loss: 0.002568 | Commit Loss: 0.000765 | Perplexity: 3129.681852
2025-10-10 04:43:05,232 Stage: Train 0.5 | Epoch: 128 | Iter: 170600 | Total Loss: 0.002942 | Recon Loss: 0.002558 | Commit Loss: 0.000767 | Perplexity: 3122.187434
Trainning Epoch:  32%|███▏      | 156/494 [42:08:30<110:09:53, 1173.35s/it]Trainning Epoch:  32%|███▏      | 156/494 [42:08:30<110:09:56, 1173.36s/it]2025-10-10 04:46:58,340 Stage: Train 0.5 | Epoch: 129 | Iter: 170800 | Total Loss: 0.002910 | Recon Loss: 0.002532 | Commit Loss: 0.000757 | Perplexity: 3122.007461
2025-10-10 04:50:47,782 Stage: Train 0.5 | Epoch: 129 | Iter: 171000 | Total Loss: 0.002944 | Recon Loss: 0.002566 | Commit Loss: 0.000756 | Perplexity: 3115.348337
2025-10-10 04:54:38,302 Stage: Train 0.5 | Epoch: 129 | Iter: 171200 | Total Loss: 0.002926 | Recon Loss: 0.002552 | Commit Loss: 0.000747 | Perplexity: 3126.307673
2025-10-10 04:58:28,574 Stage: Train 0.5 | Epoch: 129 | Iter: 171400 | Total Loss: 0.002921 | Recon Loss: 0.002542 | Commit Loss: 0.000756 | Perplexity: 3118.012604
2025-10-10 05:02:18,605 Stage: Train 0.5 | Epoch: 129 | Iter: 171600 | Total Loss: 0.002953 | Recon Loss: 0.002570 | Commit Loss: 0.000766 | Perplexity: 3125.339292
Trainning Epoch:  32%|███▏      | 157/494 [42:27:59<109:42:57, 1172.04s/it]Trainning Epoch:  32%|███▏      | 157/494 [42:27:59<109:42:59, 1172.04s/it]2025-10-10 05:06:12,302 Stage: Train 0.5 | Epoch: 130 | Iter: 171800 | Total Loss: 0.002951 | Recon Loss: 0.002572 | Commit Loss: 0.000758 | Perplexity: 3132.501523
2025-10-10 05:10:02,973 Stage: Train 0.5 | Epoch: 130 | Iter: 172000 | Total Loss: 0.002945 | Recon Loss: 0.002569 | Commit Loss: 0.000753 | Perplexity: 3115.625385
2025-10-10 05:13:54,044 Stage: Train 0.5 | Epoch: 130 | Iter: 172200 | Total Loss: 0.002943 | Recon Loss: 0.002560 | Commit Loss: 0.000767 | Perplexity: 3123.088591
2025-10-10 05:17:45,682 Stage: Train 0.5 | Epoch: 130 | Iter: 172400 | Total Loss: 0.002976 | Recon Loss: 0.002594 | Commit Loss: 0.000764 | Perplexity: 3125.702450
2025-10-10 05:21:36,838 Stage: Train 0.5 | Epoch: 130 | Iter: 172600 | Total Loss: 0.002944 | Recon Loss: 0.002559 | Commit Loss: 0.000769 | Perplexity: 3122.980236
Trainning Epoch:  32%|███▏      | 158/494 [42:47:33<109:26:04, 1172.51s/it]Trainning Epoch:  32%|███▏      | 158/494 [42:47:33<109:26:04, 1172.51s/it]2025-10-10 05:25:31,884 Stage: Train 0.5 | Epoch: 131 | Iter: 172800 | Total Loss: 0.002965 | Recon Loss: 0.002581 | Commit Loss: 0.000766 | Perplexity: 3123.221252
2025-10-10 05:29:23,151 Stage: Train 0.5 | Epoch: 131 | Iter: 173000 | Total Loss: 0.002894 | Recon Loss: 0.002519 | Commit Loss: 0.000751 | Perplexity: 3138.233492
2025-10-10 05:33:14,828 Stage: Train 0.5 | Epoch: 131 | Iter: 173200 | Total Loss: 0.003005 | Recon Loss: 0.002621 | Commit Loss: 0.000768 | Perplexity: 3132.242393
2025-10-10 05:37:06,192 Stage: Train 0.5 | Epoch: 131 | Iter: 173400 | Total Loss: 0.002905 | Recon Loss: 0.002525 | Commit Loss: 0.000760 | Perplexity: 3124.234072
2025-10-10 05:40:57,535 Stage: Train 0.5 | Epoch: 131 | Iter: 173600 | Total Loss: 0.002942 | Recon Loss: 0.002561 | Commit Loss: 0.000762 | Perplexity: 3123.780865
Trainning Epoch:  32%|███▏      | 159/494 [43:07:09<109:11:54, 1173.48s/it]Trainning Epoch:  32%|███▏      | 159/494 [43:07:09<109:11:55, 1173.48s/it]2025-10-10 05:44:52,641 Stage: Train 0.5 | Epoch: 132 | Iter: 173800 | Total Loss: 0.002911 | Recon Loss: 0.002538 | Commit Loss: 0.000746 | Perplexity: 3118.154016
2025-10-10 05:48:43,140 Stage: Train 0.5 | Epoch: 132 | Iter: 174000 | Total Loss: 0.002926 | Recon Loss: 0.002542 | Commit Loss: 0.000768 | Perplexity: 3123.183756
2025-10-10 05:52:34,417 Stage: Train 0.5 | Epoch: 132 | Iter: 174200 | Total Loss: 0.002971 | Recon Loss: 0.002593 | Commit Loss: 0.000758 | Perplexity: 3123.577644
2025-10-10 05:56:26,025 Stage: Train 0.5 | Epoch: 132 | Iter: 174400 | Total Loss: 0.002922 | Recon Loss: 0.002541 | Commit Loss: 0.000762 | Perplexity: 3123.521168
2025-10-10 06:00:18,521 Stage: Train 0.5 | Epoch: 132 | Iter: 174600 | Total Loss: 0.002929 | Recon Loss: 0.002545 | Commit Loss: 0.000767 | Perplexity: 3126.232296
Trainning Epoch:  32%|███▏      | 160/494 [43:26:45<108:57:48, 1174.46s/it]Trainning Epoch:  32%|███▏      | 160/494 [43:26:45<108:57:49, 1174.46s/it]2025-10-10 06:04:12,295 Stage: Train 0.5 | Epoch: 133 | Iter: 174800 | Total Loss: 0.002889 | Recon Loss: 0.002513 | Commit Loss: 0.000753 | Perplexity: 3121.859342
2025-10-10 06:08:00,963 Stage: Train 0.5 | Epoch: 133 | Iter: 175000 | Total Loss: 0.002975 | Recon Loss: 0.002596 | Commit Loss: 0.000759 | Perplexity: 3112.841901
2025-10-10 06:11:51,005 Stage: Train 0.5 | Epoch: 133 | Iter: 175200 | Total Loss: 0.002929 | Recon Loss: 0.002551 | Commit Loss: 0.000757 | Perplexity: 3129.085527
2025-10-10 06:15:41,246 Stage: Train 0.5 | Epoch: 133 | Iter: 175400 | Total Loss: 0.002923 | Recon Loss: 0.002548 | Commit Loss: 0.000749 | Perplexity: 3123.411650
2025-10-10 06:19:32,389 Stage: Train 0.5 | Epoch: 133 | Iter: 175600 | Total Loss: 0.002925 | Recon Loss: 0.002548 | Commit Loss: 0.000754 | Perplexity: 3127.001173
Trainning Epoch:  33%|███▎      | 161/494 [43:46:14<108:28:33, 1172.71s/it]Trainning Epoch:  33%|███▎      | 161/494 [43:46:14<108:28:33, 1172.71s/it]2025-10-10 06:23:26,202 Stage: Train 0.5 | Epoch: 134 | Iter: 175800 | Total Loss: 0.002919 | Recon Loss: 0.002540 | Commit Loss: 0.000757 | Perplexity: 3127.512070
2025-10-10 06:27:16,232 Stage: Train 0.5 | Epoch: 134 | Iter: 176000 | Total Loss: 0.002922 | Recon Loss: 0.002545 | Commit Loss: 0.000755 | Perplexity: 3128.794070
2025-10-10 06:31:06,449 Stage: Train 0.5 | Epoch: 134 | Iter: 176200 | Total Loss: 0.002896 | Recon Loss: 0.002517 | Commit Loss: 0.000757 | Perplexity: 3134.422722
2025-10-10 06:34:56,971 Stage: Train 0.5 | Epoch: 134 | Iter: 176400 | Total Loss: 0.002917 | Recon Loss: 0.002541 | Commit Loss: 0.000751 | Perplexity: 3122.095095
2025-10-10 06:38:47,927 Stage: Train 0.5 | Epoch: 134 | Iter: 176600 | Total Loss: 0.002923 | Recon Loss: 0.002537 | Commit Loss: 0.000771 | Perplexity: 3122.517388
Trainning Epoch:  33%|███▎      | 162/494 [44:05:44<108:04:10, 1171.84s/it]Trainning Epoch:  33%|███▎      | 162/494 [44:05:44<108:04:10, 1171.84s/it]2025-10-10 06:42:43,009 Stage: Train 0.5 | Epoch: 135 | Iter: 176800 | Total Loss: 0.002878 | Recon Loss: 0.002500 | Commit Loss: 0.000757 | Perplexity: 3134.554834
2025-10-10 06:46:35,481 Stage: Train 0.5 | Epoch: 135 | Iter: 177000 | Total Loss: 0.002946 | Recon Loss: 0.002569 | Commit Loss: 0.000753 | Perplexity: 3129.107631
2025-10-10 06:50:27,968 Stage: Train 0.5 | Epoch: 135 | Iter: 177200 | Total Loss: 0.002914 | Recon Loss: 0.002527 | Commit Loss: 0.000773 | Perplexity: 3121.598861
2025-10-10 06:54:20,968 Stage: Train 0.5 | Epoch: 135 | Iter: 177400 | Total Loss: 0.002943 | Recon Loss: 0.002563 | Commit Loss: 0.000761 | Perplexity: 3131.382904
2025-10-10 06:58:13,810 Stage: Train 0.5 | Epoch: 135 | Iter: 177600 | Total Loss: 0.002886 | Recon Loss: 0.002508 | Commit Loss: 0.000755 | Perplexity: 3124.775387
Trainning Epoch:  33%|███▎      | 163/494 [44:25:26<108:02:22, 1175.05s/it]Trainning Epoch:  33%|███▎      | 163/494 [44:25:26<108:02:22, 1175.05s/it]2025-10-10 07:02:08,949 Stage: Train 0.5 | Epoch: 136 | Iter: 177800 | Total Loss: 0.002889 | Recon Loss: 0.002503 | Commit Loss: 0.000772 | Perplexity: 3132.337719
2025-10-10 07:05:58,643 Stage: Train 0.5 | Epoch: 136 | Iter: 178000 | Total Loss: 0.002902 | Recon Loss: 0.002523 | Commit Loss: 0.000757 | Perplexity: 3130.240718
2025-10-10 07:09:49,121 Stage: Train 0.5 | Epoch: 136 | Iter: 178200 | Total Loss: 0.002891 | Recon Loss: 0.002515 | Commit Loss: 0.000752 | Perplexity: 3133.937421
2025-10-10 07:13:39,369 Stage: Train 0.5 | Epoch: 136 | Iter: 178400 | Total Loss: 0.002908 | Recon Loss: 0.002529 | Commit Loss: 0.000758 | Perplexity: 3134.870747
2025-10-10 07:17:29,953 Stage: Train 0.5 | Epoch: 136 | Iter: 178600 | Total Loss: 0.002927 | Recon Loss: 0.002549 | Commit Loss: 0.000757 | Perplexity: 3132.521819
Trainning Epoch:  33%|███▎      | 164/494 [44:44:56<107:33:08, 1173.30s/it]Trainning Epoch:  33%|███▎      | 164/494 [44:44:56<107:33:08, 1173.30s/it]2025-10-10 07:21:23,705 Stage: Train 0.5 | Epoch: 137 | Iter: 178800 | Total Loss: 0.002941 | Recon Loss: 0.002561 | Commit Loss: 0.000759 | Perplexity: 3128.677081
2025-10-10 07:25:13,981 Stage: Train 0.5 | Epoch: 137 | Iter: 179000 | Total Loss: 0.002898 | Recon Loss: 0.002520 | Commit Loss: 0.000756 | Perplexity: 3126.696132
2025-10-10 07:29:04,525 Stage: Train 0.5 | Epoch: 137 | Iter: 179200 | Total Loss: 0.002927 | Recon Loss: 0.002544 | Commit Loss: 0.000766 | Perplexity: 3141.853799
2025-10-10 07:32:55,267 Stage: Train 0.5 | Epoch: 137 | Iter: 179400 | Total Loss: 0.002890 | Recon Loss: 0.002508 | Commit Loss: 0.000763 | Perplexity: 3132.304924
2025-10-10 07:36:46,160 Stage: Train 0.5 | Epoch: 137 | Iter: 179600 | Total Loss: 0.002875 | Recon Loss: 0.002497 | Commit Loss: 0.000755 | Perplexity: 3135.334609
Trainning Epoch:  33%|███▎      | 165/494 [45:04:27<107:10:34, 1172.75s/it]Trainning Epoch:  33%|███▎      | 165/494 [45:04:27<107:10:34, 1172.75s/it]2025-10-10 07:40:40,410 Stage: Train 0.5 | Epoch: 138 | Iter: 179800 | Total Loss: 0.002899 | Recon Loss: 0.002521 | Commit Loss: 0.000756 | Perplexity: 3133.840093
2025-10-10 07:44:30,660 Stage: Train 0.5 | Epoch: 138 | Iter: 180000 | Total Loss: 0.002932 | Recon Loss: 0.002565 | Commit Loss: 0.000734 | Perplexity: 3130.140452
2025-10-10 07:44:30,661 Saving model at iteration 180000
2025-10-10 07:44:31,169 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000
2025-10-10 07:44:32,428 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000/model.safetensors
2025-10-10 07:44:34,155 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000/optimizer.bin
2025-10-10 07:44:34,156 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000/scheduler.bin
2025-10-10 07:44:34,156 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000/sampler.bin
2025-10-10 07:44:34,158 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_180000/random_states_0.pkl
2025-10-10 07:48:23,660 Stage: Train 0.5 | Epoch: 138 | Iter: 180200 | Total Loss: 0.002903 | Recon Loss: 0.002524 | Commit Loss: 0.000757 | Perplexity: 3130.320232
2025-10-10 07:52:13,455 Stage: Train 0.5 | Epoch: 138 | Iter: 180400 | Total Loss: 0.002906 | Recon Loss: 0.002528 | Commit Loss: 0.000754 | Perplexity: 3141.669567
2025-10-10 07:56:02,397 Stage: Train 0.5 | Epoch: 138 | Iter: 180600 | Total Loss: 0.002881 | Recon Loss: 0.002497 | Commit Loss: 0.000768 | Perplexity: 3149.188278
2025-10-10 07:59:49,462 Stage: Train 0.5 | Epoch: 138 | Iter: 180800 | Total Loss: 0.002922 | Recon Loss: 0.002546 | Commit Loss: 0.000752 | Perplexity: 3133.460898
Trainning Epoch:  34%|███▎      | 166/494 [45:23:55<106:42:16, 1171.15s/it]Trainning Epoch:  34%|███▎      | 166/494 [45:23:55<106:42:18, 1171.15s/it]2025-10-10 08:03:40,902 Stage: Train 0.5 | Epoch: 139 | Iter: 181000 | Total Loss: 0.002922 | Recon Loss: 0.002543 | Commit Loss: 0.000758 | Perplexity: 3142.185096
2025-10-10 08:07:30,909 Stage: Train 0.5 | Epoch: 139 | Iter: 181200 | Total Loss: 0.002895 | Recon Loss: 0.002524 | Commit Loss: 0.000744 | Perplexity: 3141.882955
2025-10-10 08:11:21,265 Stage: Train 0.5 | Epoch: 139 | Iter: 181400 | Total Loss: 0.002880 | Recon Loss: 0.002500 | Commit Loss: 0.000759 | Perplexity: 3141.369498
2025-10-10 08:15:11,449 Stage: Train 0.5 | Epoch: 139 | Iter: 181600 | Total Loss: 0.002906 | Recon Loss: 0.002529 | Commit Loss: 0.000752 | Perplexity: 3131.949205
2025-10-10 08:19:00,952 Stage: Train 0.5 | Epoch: 139 | Iter: 181800 | Total Loss: 0.002913 | Recon Loss: 0.002533 | Commit Loss: 0.000761 | Perplexity: 3130.143456
Trainning Epoch:  34%|███▍      | 167/494 [45:43:21<106:15:08, 1169.75s/it]Trainning Epoch:  34%|███▍      | 167/494 [45:43:21<106:15:08, 1169.75s/it]2025-10-10 08:22:48,561 Stage: Train 0.5 | Epoch: 140 | Iter: 182000 | Total Loss: 0.002862 | Recon Loss: 0.002481 | Commit Loss: 0.000762 | Perplexity: 3137.051793
2025-10-10 08:26:33,551 Stage: Train 0.5 | Epoch: 140 | Iter: 182200 | Total Loss: 0.002923 | Recon Loss: 0.002552 | Commit Loss: 0.000742 | Perplexity: 3125.752531
2025-10-10 08:30:18,234 Stage: Train 0.5 | Epoch: 140 | Iter: 182400 | Total Loss: 0.002866 | Recon Loss: 0.002488 | Commit Loss: 0.000757 | Perplexity: 3131.953540
2025-10-10 08:34:02,495 Stage: Train 0.5 | Epoch: 140 | Iter: 182600 | Total Loss: 0.002915 | Recon Loss: 0.002535 | Commit Loss: 0.000760 | Perplexity: 3132.881643
2025-10-10 08:37:46,738 Stage: Train 0.5 | Epoch: 140 | Iter: 182800 | Total Loss: 0.002885 | Recon Loss: 0.002507 | Commit Loss: 0.000755 | Perplexity: 3140.966724
Trainning Epoch:  34%|███▍      | 168/494 [46:02:21<105:06:41, 1160.74s/it]Trainning Epoch:  34%|███▍      | 168/494 [46:02:21<105:06:42, 1160.74s/it]2025-10-10 08:41:39,667 Stage: Train 0.5 | Epoch: 141 | Iter: 183000 | Total Loss: 0.002870 | Recon Loss: 0.002486 | Commit Loss: 0.000768 | Perplexity: 3138.376646
2025-10-10 08:45:30,456 Stage: Train 0.5 | Epoch: 141 | Iter: 183200 | Total Loss: 0.002878 | Recon Loss: 0.002501 | Commit Loss: 0.000756 | Perplexity: 3129.694158
2025-10-10 08:49:21,541 Stage: Train 0.5 | Epoch: 141 | Iter: 183400 | Total Loss: 0.002940 | Recon Loss: 0.002563 | Commit Loss: 0.000753 | Perplexity: 3126.111000
2025-10-10 08:53:12,854 Stage: Train 0.5 | Epoch: 141 | Iter: 183600 | Total Loss: 0.002906 | Recon Loss: 0.002532 | Commit Loss: 0.000748 | Perplexity: 3138.549044
2025-10-10 08:57:04,001 Stage: Train 0.5 | Epoch: 141 | Iter: 183800 | Total Loss: 0.002873 | Recon Loss: 0.002495 | Commit Loss: 0.000756 | Perplexity: 3148.264893
Trainning Epoch:  34%|███▍      | 169/494 [46:21:54<105:07:23, 1164.44s/it]Trainning Epoch:  34%|███▍      | 169/494 [46:21:54<105:07:23, 1164.44s/it]2025-10-10 09:00:57,373 Stage: Train 0.5 | Epoch: 142 | Iter: 184000 | Total Loss: 0.002901 | Recon Loss: 0.002524 | Commit Loss: 0.000755 | Perplexity: 3146.430652
2025-10-10 09:04:47,911 Stage: Train 0.5 | Epoch: 142 | Iter: 184200 | Total Loss: 0.002906 | Recon Loss: 0.002526 | Commit Loss: 0.000759 | Perplexity: 3141.644672
2025-10-10 09:08:38,833 Stage: Train 0.5 | Epoch: 142 | Iter: 184400 | Total Loss: 0.002864 | Recon Loss: 0.002486 | Commit Loss: 0.000757 | Perplexity: 3133.190551
2025-10-10 09:12:29,756 Stage: Train 0.5 | Epoch: 142 | Iter: 184600 | Total Loss: 0.002858 | Recon Loss: 0.002481 | Commit Loss: 0.000753 | Perplexity: 3132.591677
2025-10-10 09:16:20,987 Stage: Train 0.5 | Epoch: 142 | Iter: 184800 | Total Loss: 0.002942 | Recon Loss: 0.002564 | Commit Loss: 0.000756 | Perplexity: 3139.979058
Trainning Epoch:  34%|███▍      | 170/494 [46:41:26<105:00:37, 1166.78s/it]Trainning Epoch:  34%|███▍      | 170/494 [46:41:26<105:00:37, 1166.78s/it]2025-10-10 09:20:11,100 Stage: Train 0.5 | Epoch: 143 | Iter: 185000 | Total Loss: 0.002843 | Recon Loss: 0.002470 | Commit Loss: 0.000748 | Perplexity: 3139.868472
2025-10-10 09:23:57,018 Stage: Train 0.5 | Epoch: 143 | Iter: 185200 | Total Loss: 0.002882 | Recon Loss: 0.002499 | Commit Loss: 0.000766 | Perplexity: 3132.577727
2025-10-10 09:27:44,037 Stage: Train 0.5 | Epoch: 143 | Iter: 185400 | Total Loss: 0.002891 | Recon Loss: 0.002515 | Commit Loss: 0.000752 | Perplexity: 3134.147130
2025-10-10 09:31:30,818 Stage: Train 0.5 | Epoch: 143 | Iter: 185600 | Total Loss: 0.002886 | Recon Loss: 0.002511 | Commit Loss: 0.000751 | Perplexity: 3129.766622
2025-10-10 09:35:17,603 Stage: Train 0.5 | Epoch: 143 | Iter: 185800 | Total Loss: 0.002865 | Recon Loss: 0.002487 | Commit Loss: 0.000755 | Perplexity: 3138.537715
Trainning Epoch:  35%|███▍      | 171/494 [47:00:37<104:15:07, 1161.94s/it]Trainning Epoch:  35%|███▍      | 171/494 [47:00:37<104:15:07, 1161.94s/it]2025-10-10 09:39:08,147 Stage: Train 0.5 | Epoch: 144 | Iter: 186000 | Total Loss: 0.002901 | Recon Loss: 0.002519 | Commit Loss: 0.000764 | Perplexity: 3144.361997
2025-10-10 09:42:57,201 Stage: Train 0.5 | Epoch: 144 | Iter: 186200 | Total Loss: 0.002880 | Recon Loss: 0.002499 | Commit Loss: 0.000762 | Perplexity: 3137.977150
2025-10-10 09:46:46,337 Stage: Train 0.5 | Epoch: 144 | Iter: 186400 | Total Loss: 0.002900 | Recon Loss: 0.002523 | Commit Loss: 0.000755 | Perplexity: 3136.366249
2025-10-10 09:50:34,812 Stage: Train 0.5 | Epoch: 144 | Iter: 186600 | Total Loss: 0.002922 | Recon Loss: 0.002541 | Commit Loss: 0.000761 | Perplexity: 3135.622202
2025-10-10 09:54:23,441 Stage: Train 0.5 | Epoch: 144 | Iter: 186800 | Total Loss: 0.002860 | Recon Loss: 0.002485 | Commit Loss: 0.000750 | Perplexity: 3136.105321
Trainning Epoch:  35%|███▍      | 172/494 [47:19:58<103:54:15, 1161.66s/it]Trainning Epoch:  35%|███▍      | 172/494 [47:19:58<103:54:17, 1161.67s/it]2025-10-10 09:58:12,717 Stage: Train 0.5 | Epoch: 145 | Iter: 187000 | Total Loss: 0.002853 | Recon Loss: 0.002476 | Commit Loss: 0.000753 | Perplexity: 3127.366387
2025-10-10 10:01:58,374 Stage: Train 0.5 | Epoch: 145 | Iter: 187200 | Total Loss: 0.002844 | Recon Loss: 0.002467 | Commit Loss: 0.000753 | Perplexity: 3131.933881
2025-10-10 10:05:43,954 Stage: Train 0.5 | Epoch: 145 | Iter: 187400 | Total Loss: 0.002859 | Recon Loss: 0.002483 | Commit Loss: 0.000751 | Perplexity: 3139.322771
2025-10-10 10:09:30,001 Stage: Train 0.5 | Epoch: 145 | Iter: 187600 | Total Loss: 0.002858 | Recon Loss: 0.002478 | Commit Loss: 0.000759 | Perplexity: 3136.112294
2025-10-10 10:13:15,630 Stage: Train 0.5 | Epoch: 145 | Iter: 187800 | Total Loss: 0.002867 | Recon Loss: 0.002492 | Commit Loss: 0.000749 | Perplexity: 3130.088817
Trainning Epoch:  35%|███▌      | 173/494 [47:39:03<103:09:10, 1156.86s/it]Trainning Epoch:  35%|███▌      | 173/494 [47:39:03<103:09:10, 1156.86s/it]2025-10-10 10:17:04,723 Stage: Train 0.5 | Epoch: 146 | Iter: 188000 | Total Loss: 0.002889 | Recon Loss: 0.002509 | Commit Loss: 0.000761 | Perplexity: 3139.335782
2025-10-10 10:20:52,308 Stage: Train 0.5 | Epoch: 146 | Iter: 188200 | Total Loss: 0.002823 | Recon Loss: 0.002450 | Commit Loss: 0.000746 | Perplexity: 3144.414242
2025-10-10 10:24:40,062 Stage: Train 0.5 | Epoch: 146 | Iter: 188400 | Total Loss: 0.002836 | Recon Loss: 0.002462 | Commit Loss: 0.000748 | Perplexity: 3123.589956
2025-10-10 10:28:27,716 Stage: Train 0.5 | Epoch: 146 | Iter: 188600 | Total Loss: 0.002863 | Recon Loss: 0.002485 | Commit Loss: 0.000756 | Perplexity: 3137.962135
2025-10-10 10:32:15,472 Stage: Train 0.5 | Epoch: 146 | Iter: 188800 | Total Loss: 0.002890 | Recon Loss: 0.002515 | Commit Loss: 0.000750 | Perplexity: 3136.182915
Trainning Epoch:  35%|███▌      | 174/494 [47:58:19<102:47:58, 1156.50s/it]Trainning Epoch:  35%|███▌      | 174/494 [47:58:19<102:47:59, 1156.50s/it]2025-10-10 10:36:07,200 Stage: Train 0.5 | Epoch: 147 | Iter: 189000 | Total Loss: 0.002875 | Recon Loss: 0.002490 | Commit Loss: 0.000771 | Perplexity: 3143.745952
2025-10-10 10:39:57,986 Stage: Train 0.5 | Epoch: 147 | Iter: 189200 | Total Loss: 0.002851 | Recon Loss: 0.002469 | Commit Loss: 0.000764 | Perplexity: 3146.168669
2025-10-10 10:43:48,905 Stage: Train 0.5 | Epoch: 147 | Iter: 189400 | Total Loss: 0.002844 | Recon Loss: 0.002468 | Commit Loss: 0.000752 | Perplexity: 3133.572860
2025-10-10 10:47:39,659 Stage: Train 0.5 | Epoch: 147 | Iter: 189600 | Total Loss: 0.002853 | Recon Loss: 0.002471 | Commit Loss: 0.000765 | Perplexity: 3140.597531
2025-10-10 10:51:30,681 Stage: Train 0.5 | Epoch: 147 | Iter: 189800 | Total Loss: 0.002852 | Recon Loss: 0.002479 | Commit Loss: 0.000746 | Perplexity: 3131.369799
Trainning Epoch:  35%|███▌      | 175/494 [48:17:51<102:53:02, 1161.07s/it]Trainning Epoch:  35%|███▌      | 175/494 [48:17:51<102:53:02, 1161.07s/it]2025-10-10 10:55:24,527 Stage: Train 0.5 | Epoch: 148 | Iter: 190000 | Total Loss: 0.002860 | Recon Loss: 0.002481 | Commit Loss: 0.000758 | Perplexity: 3143.091273
2025-10-10 10:59:15,028 Stage: Train 0.5 | Epoch: 148 | Iter: 190200 | Total Loss: 0.002886 | Recon Loss: 0.002512 | Commit Loss: 0.000749 | Perplexity: 3139.217893
2025-10-10 11:03:06,487 Stage: Train 0.5 | Epoch: 148 | Iter: 190400 | Total Loss: 0.002836 | Recon Loss: 0.002450 | Commit Loss: 0.000771 | Perplexity: 3142.740083
2025-10-10 11:06:57,800 Stage: Train 0.5 | Epoch: 148 | Iter: 190600 | Total Loss: 0.002871 | Recon Loss: 0.002496 | Commit Loss: 0.000752 | Perplexity: 3136.291129
2025-10-10 11:10:48,739 Stage: Train 0.5 | Epoch: 148 | Iter: 190800 | Total Loss: 0.002823 | Recon Loss: 0.002444 | Commit Loss: 0.000757 | Perplexity: 3142.348358
Trainning Epoch:  36%|███▌      | 176/494 [48:37:23<102:52:10, 1164.56s/it]Trainning Epoch:  36%|███▌      | 176/494 [48:37:23<102:52:11, 1164.57s/it]2025-10-10 11:14:41,658 Stage: Train 0.5 | Epoch: 149 | Iter: 191000 | Total Loss: 0.002892 | Recon Loss: 0.002517 | Commit Loss: 0.000750 | Perplexity: 3148.096550
2025-10-10 11:18:31,697 Stage: Train 0.5 | Epoch: 149 | Iter: 191200 | Total Loss: 0.002834 | Recon Loss: 0.002449 | Commit Loss: 0.000768 | Perplexity: 3155.359626
2025-10-10 11:22:21,903 Stage: Train 0.5 | Epoch: 149 | Iter: 191400 | Total Loss: 0.002855 | Recon Loss: 0.002475 | Commit Loss: 0.000760 | Perplexity: 3129.118947
2025-10-10 11:26:12,530 Stage: Train 0.5 | Epoch: 149 | Iter: 191600 | Total Loss: 0.002850 | Recon Loss: 0.002474 | Commit Loss: 0.000752 | Perplexity: 3143.053192
2025-10-10 11:30:03,526 Stage: Train 0.5 | Epoch: 149 | Iter: 191800 | Total Loss: 0.002846 | Recon Loss: 0.002467 | Commit Loss: 0.000759 | Perplexity: 3145.947572
Trainning Epoch:  36%|███▌      | 177/494 [48:56:53<102:41:05, 1166.14s/it]Trainning Epoch:  36%|███▌      | 177/494 [48:56:53<102:41:04, 1166.14s/it]2025-10-10 11:33:55,454 Stage: Train 0.5 | Epoch: 150 | Iter: 192000 | Total Loss: 0.002856 | Recon Loss: 0.002483 | Commit Loss: 0.000746 | Perplexity: 3129.977648
2025-10-10 11:37:44,198 Stage: Train 0.5 | Epoch: 150 | Iter: 192200 | Total Loss: 0.002846 | Recon Loss: 0.002469 | Commit Loss: 0.000753 | Perplexity: 3129.970651
2025-10-10 11:41:33,418 Stage: Train 0.5 | Epoch: 150 | Iter: 192400 | Total Loss: 0.002803 | Recon Loss: 0.002427 | Commit Loss: 0.000751 | Perplexity: 3136.653925
2025-10-10 11:45:23,191 Stage: Train 0.5 | Epoch: 150 | Iter: 192600 | Total Loss: 0.002868 | Recon Loss: 0.002488 | Commit Loss: 0.000759 | Perplexity: 3135.406213
2025-10-10 11:49:12,598 Stage: Train 0.5 | Epoch: 150 | Iter: 192800 | Total Loss: 0.002860 | Recon Loss: 0.002485 | Commit Loss: 0.000750 | Perplexity: 3146.962773
Trainning Epoch:  36%|███▌      | 178/494 [49:16:17<102:17:36, 1165.37s/it]Trainning Epoch:  36%|███▌      | 178/494 [49:16:17<102:17:36, 1165.37s/it]2025-10-10 11:53:05,127 Stage: Train 0.5 | Epoch: 151 | Iter: 193000 | Total Loss: 0.002882 | Recon Loss: 0.002507 | Commit Loss: 0.000750 | Perplexity: 3123.208883
2025-10-10 11:56:54,049 Stage: Train 0.5 | Epoch: 151 | Iter: 193200 | Total Loss: 0.002835 | Recon Loss: 0.002458 | Commit Loss: 0.000755 | Perplexity: 3131.496787
2025-10-10 12:00:43,006 Stage: Train 0.5 | Epoch: 151 | Iter: 193400 | Total Loss: 0.002820 | Recon Loss: 0.002444 | Commit Loss: 0.000752 | Perplexity: 3138.006326
2025-10-10 12:04:31,847 Stage: Train 0.5 | Epoch: 151 | Iter: 193600 | Total Loss: 0.002825 | Recon Loss: 0.002445 | Commit Loss: 0.000761 | Perplexity: 3139.279052
2025-10-10 12:08:20,714 Stage: Train 0.5 | Epoch: 151 | Iter: 193800 | Total Loss: 0.002888 | Recon Loss: 0.002509 | Commit Loss: 0.000758 | Perplexity: 3142.082072
Trainning Epoch:  36%|███▌      | 179/494 [49:35:39<101:53:27, 1164.47s/it]Trainning Epoch:  36%|███▌      | 179/494 [49:35:39<101:53:27, 1164.47s/it]2025-10-10 12:12:12,105 Stage: Train 0.5 | Epoch: 152 | Iter: 194000 | Total Loss: 0.002812 | Recon Loss: 0.002434 | Commit Loss: 0.000756 | Perplexity: 3134.325831
2025-10-10 12:16:00,014 Stage: Train 0.5 | Epoch: 152 | Iter: 194200 | Total Loss: 0.002875 | Recon Loss: 0.002497 | Commit Loss: 0.000755 | Perplexity: 3144.844150
2025-10-10 12:19:48,632 Stage: Train 0.5 | Epoch: 152 | Iter: 194400 | Total Loss: 0.002835 | Recon Loss: 0.002461 | Commit Loss: 0.000747 | Perplexity: 3134.602732
2025-10-10 12:23:37,023 Stage: Train 0.5 | Epoch: 152 | Iter: 194600 | Total Loss: 0.002868 | Recon Loss: 0.002484 | Commit Loss: 0.000768 | Perplexity: 3145.519470
2025-10-10 12:27:24,804 Stage: Train 0.5 | Epoch: 152 | Iter: 194800 | Total Loss: 0.002882 | Recon Loss: 0.002500 | Commit Loss: 0.000764 | Perplexity: 3144.052072
Trainning Epoch:  36%|███▋      | 180/494 [49:54:57<101:23:52, 1162.52s/it]Trainning Epoch:  36%|███▋      | 180/494 [49:54:57<101:23:52, 1162.53s/it]2025-10-10 12:31:15,496 Stage: Train 0.5 | Epoch: 153 | Iter: 195000 | Total Loss: 0.002834 | Recon Loss: 0.002455 | Commit Loss: 0.000758 | Perplexity: 3133.772157
2025-10-10 12:35:03,883 Stage: Train 0.5 | Epoch: 153 | Iter: 195200 | Total Loss: 0.002789 | Recon Loss: 0.002412 | Commit Loss: 0.000754 | Perplexity: 3136.888962
2025-10-10 12:38:53,388 Stage: Train 0.5 | Epoch: 153 | Iter: 195400 | Total Loss: 0.002842 | Recon Loss: 0.002465 | Commit Loss: 0.000754 | Perplexity: 3139.064591
2025-10-10 12:42:42,531 Stage: Train 0.5 | Epoch: 153 | Iter: 195600 | Total Loss: 0.002814 | Recon Loss: 0.002437 | Commit Loss: 0.000754 | Perplexity: 3146.578489
2025-10-10 12:46:31,854 Stage: Train 0.5 | Epoch: 153 | Iter: 195800 | Total Loss: 0.002886 | Recon Loss: 0.002508 | Commit Loss: 0.000756 | Perplexity: 3138.721818
2025-10-10 12:50:21,312 Stage: Train 0.5 | Epoch: 153 | Iter: 196000 | Total Loss: 0.002848 | Recon Loss: 0.002472 | Commit Loss: 0.000752 | Perplexity: 3146.995991
Trainning Epoch:  37%|███▋      | 181/494 [50:14:21<101:05:57, 1162.80s/it]Trainning Epoch:  37%|███▋      | 181/494 [50:14:21<101:05:58, 1162.81s/it]2025-10-10 12:54:14,683 Stage: Train 0.5 | Epoch: 154 | Iter: 196200 | Total Loss: 0.002813 | Recon Loss: 0.002442 | Commit Loss: 0.000742 | Perplexity: 3136.721945
2025-10-10 12:58:05,098 Stage: Train 0.5 | Epoch: 154 | Iter: 196400 | Total Loss: 0.002821 | Recon Loss: 0.002444 | Commit Loss: 0.000754 | Perplexity: 3151.202677
2025-10-10 13:01:55,221 Stage: Train 0.5 | Epoch: 154 | Iter: 196600 | Total Loss: 0.002820 | Recon Loss: 0.002440 | Commit Loss: 0.000762 | Perplexity: 3140.223231
2025-10-10 13:05:45,806 Stage: Train 0.5 | Epoch: 154 | Iter: 196800 | Total Loss: 0.002834 | Recon Loss: 0.002451 | Commit Loss: 0.000764 | Perplexity: 3149.465879
2025-10-10 13:09:36,468 Stage: Train 0.5 | Epoch: 154 | Iter: 197000 | Total Loss: 0.002865 | Recon Loss: 0.002485 | Commit Loss: 0.000761 | Perplexity: 3144.476301
Trainning Epoch:  37%|███▋      | 182/494 [50:33:51<100:57:40, 1164.94s/it]Trainning Epoch:  37%|███▋      | 182/494 [50:33:51<100:57:40, 1164.94s/it]2025-10-10 13:13:25,513 Stage: Train 0.5 | Epoch: 155 | Iter: 197200 | Total Loss: 0.002868 | Recon Loss: 0.002488 | Commit Loss: 0.000760 | Perplexity: 3138.659020
2025-10-10 13:17:12,248 Stage: Train 0.5 | Epoch: 155 | Iter: 197400 | Total Loss: 0.002829 | Recon Loss: 0.002447 | Commit Loss: 0.000764 | Perplexity: 3148.252316
2025-10-10 13:20:59,051 Stage: Train 0.5 | Epoch: 155 | Iter: 197600 | Total Loss: 0.002790 | Recon Loss: 0.002414 | Commit Loss: 0.000753 | Perplexity: 3140.161584
2025-10-10 13:24:45,380 Stage: Train 0.5 | Epoch: 155 | Iter: 197800 | Total Loss: 0.002808 | Recon Loss: 0.002431 | Commit Loss: 0.000755 | Perplexity: 3146.440291
2025-10-10 13:28:31,815 Stage: Train 0.5 | Epoch: 155 | Iter: 198000 | Total Loss: 0.002842 | Recon Loss: 0.002464 | Commit Loss: 0.000754 | Perplexity: 3134.082131
Trainning Epoch:  37%|███▋      | 183/494 [50:53:01<100:15:11, 1160.49s/it]Trainning Epoch:  37%|███▋      | 183/494 [50:53:01<100:15:12, 1160.49s/it]2025-10-10 13:32:22,154 Stage: Train 0.5 | Epoch: 156 | Iter: 198200 | Total Loss: 0.002804 | Recon Loss: 0.002433 | Commit Loss: 0.000741 | Perplexity: 3140.090709
2025-10-10 13:36:09,866 Stage: Train 0.5 | Epoch: 156 | Iter: 198400 | Total Loss: 0.002868 | Recon Loss: 0.002491 | Commit Loss: 0.000754 | Perplexity: 3145.961088
2025-10-10 13:39:57,535 Stage: Train 0.5 | Epoch: 156 | Iter: 198600 | Total Loss: 0.002808 | Recon Loss: 0.002428 | Commit Loss: 0.000760 | Perplexity: 3137.439279
2025-10-10 13:43:45,217 Stage: Train 0.5 | Epoch: 156 | Iter: 198800 | Total Loss: 0.002809 | Recon Loss: 0.002440 | Commit Loss: 0.000738 | Perplexity: 3145.986571
2025-10-10 13:47:33,476 Stage: Train 0.5 | Epoch: 156 | Iter: 199000 | Total Loss: 0.002820 | Recon Loss: 0.002446 | Commit Loss: 0.000748 | Perplexity: 3133.236643
Trainning Epoch:  37%|███▋      | 184/494 [51:12:17<99:49:47, 1159.31s/it] Trainning Epoch:  37%|███▋      | 184/494 [51:12:17<99:49:47, 1159.32s/it] 2025-10-10 13:51:24,287 Stage: Train 0.5 | Epoch: 157 | Iter: 199200 | Total Loss: 0.002813 | Recon Loss: 0.002428 | Commit Loss: 0.000771 | Perplexity: 3154.378700
2025-10-10 13:55:13,753 Stage: Train 0.5 | Epoch: 157 | Iter: 199400 | Total Loss: 0.002815 | Recon Loss: 0.002438 | Commit Loss: 0.000754 | Perplexity: 3135.294482
2025-10-10 13:59:03,175 Stage: Train 0.5 | Epoch: 157 | Iter: 199600 | Total Loss: 0.002803 | Recon Loss: 0.002424 | Commit Loss: 0.000757 | Perplexity: 3134.601718
2025-10-10 14:02:52,394 Stage: Train 0.5 | Epoch: 157 | Iter: 199800 | Total Loss: 0.002831 | Recon Loss: 0.002459 | Commit Loss: 0.000744 | Perplexity: 3146.767230
2025-10-10 14:06:41,712 Stage: Train 0.5 | Epoch: 157 | Iter: 200000 | Total Loss: 0.002812 | Recon Loss: 0.002432 | Commit Loss: 0.000761 | Perplexity: 3147.032909
2025-10-10 14:06:41,713 Saving model at iteration 200000
2025-10-10 14:06:42,159 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000
2025-10-10 14:06:43,414 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000/model.safetensors
2025-10-10 14:06:44,992 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000/optimizer.bin
2025-10-10 14:06:44,992 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000/scheduler.bin
2025-10-10 14:06:44,993 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000/sampler.bin
2025-10-10 14:06:44,994 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_200000/random_states_0.pkl
Trainning Epoch:  37%|███▋      | 185/494 [51:31:44<99:41:55, 1161.54s/it]Trainning Epoch:  37%|███▋      | 185/494 [51:31:44<99:41:56, 1161.54s/it]2025-10-10 14:10:38,369 Stage: Train 0.5 | Epoch: 158 | Iter: 200200 | Total Loss: 0.002791 | Recon Loss: 0.002420 | Commit Loss: 0.000742 | Perplexity: 3145.863737
2025-10-10 14:14:28,954 Stage: Train 0.5 | Epoch: 158 | Iter: 200400 | Total Loss: 0.002780 | Recon Loss: 0.002412 | Commit Loss: 0.000736 | Perplexity: 3138.154650
2025-10-10 14:18:19,737 Stage: Train 0.5 | Epoch: 158 | Iter: 200600 | Total Loss: 0.002795 | Recon Loss: 0.002419 | Commit Loss: 0.000751 | Perplexity: 3141.384747
2025-10-10 14:22:10,466 Stage: Train 0.5 | Epoch: 158 | Iter: 200800 | Total Loss: 0.002845 | Recon Loss: 0.002469 | Commit Loss: 0.000752 | Perplexity: 3151.865693
2025-10-10 14:26:01,377 Stage: Train 0.5 | Epoch: 158 | Iter: 201000 | Total Loss: 0.002797 | Recon Loss: 0.002421 | Commit Loss: 0.000752 | Perplexity: 3146.816219
Trainning Epoch:  38%|███▊      | 186/494 [51:51:16<99:38:30, 1164.65s/it]Trainning Epoch:  38%|███▊      | 186/494 [51:51:16<99:38:30, 1164.65s/it]2025-10-10 14:29:54,285 Stage: Train 0.5 | Epoch: 159 | Iter: 201200 | Total Loss: 0.002823 | Recon Loss: 0.002450 | Commit Loss: 0.000746 | Perplexity: 3155.170643
2025-10-10 14:33:41,765 Stage: Train 0.5 | Epoch: 159 | Iter: 201400 | Total Loss: 0.002795 | Recon Loss: 0.002417 | Commit Loss: 0.000757 | Perplexity: 3148.168331
2025-10-10 14:37:29,534 Stage: Train 0.5 | Epoch: 159 | Iter: 201600 | Total Loss: 0.002804 | Recon Loss: 0.002428 | Commit Loss: 0.000752 | Perplexity: 3139.980425
2025-10-10 14:41:17,112 Stage: Train 0.5 | Epoch: 159 | Iter: 201800 | Total Loss: 0.002807 | Recon Loss: 0.002430 | Commit Loss: 0.000754 | Perplexity: 3150.014178
2025-10-10 14:45:04,313 Stage: Train 0.5 | Epoch: 159 | Iter: 202000 | Total Loss: 0.002808 | Recon Loss: 0.002432 | Commit Loss: 0.000752 | Perplexity: 3149.607686
Trainning Epoch:  38%|███▊      | 187/494 [52:10:32<99:06:21, 1162.16s/it]Trainning Epoch:  38%|███▊      | 187/494 [52:10:32<99:06:22, 1162.16s/it]2025-10-10 14:48:56,934 Stage: Train 0.5 | Epoch: 160 | Iter: 202200 | Total Loss: 0.002830 | Recon Loss: 0.002454 | Commit Loss: 0.000753 | Perplexity: 3154.213218
2025-10-10 14:52:47,965 Stage: Train 0.5 | Epoch: 160 | Iter: 202400 | Total Loss: 0.002847 | Recon Loss: 0.002473 | Commit Loss: 0.000748 | Perplexity: 3136.956985
2025-10-10 14:56:38,830 Stage: Train 0.5 | Epoch: 160 | Iter: 202600 | Total Loss: 0.002793 | Recon Loss: 0.002415 | Commit Loss: 0.000755 | Perplexity: 3137.612438
2025-10-10 15:00:29,764 Stage: Train 0.5 | Epoch: 160 | Iter: 202800 | Total Loss: 0.002837 | Recon Loss: 0.002459 | Commit Loss: 0.000756 | Perplexity: 3135.125288
2025-10-10 15:04:20,627 Stage: Train 0.5 | Epoch: 160 | Iter: 203000 | Total Loss: 0.002762 | Recon Loss: 0.002388 | Commit Loss: 0.000748 | Perplexity: 3143.728995
Trainning Epoch:  38%|███▊      | 188/494 [52:30:05<99:02:32, 1165.21s/it]Trainning Epoch:  38%|███▊      | 188/494 [52:30:05<99:02:32, 1165.20s/it]2025-10-10 15:08:12,361 Stage: Train 0.5 | Epoch: 161 | Iter: 203200 | Total Loss: 0.002782 | Recon Loss: 0.002412 | Commit Loss: 0.000740 | Perplexity: 3151.303716
2025-10-10 15:12:01,133 Stage: Train 0.5 | Epoch: 161 | Iter: 203400 | Total Loss: 0.002783 | Recon Loss: 0.002407 | Commit Loss: 0.000751 | Perplexity: 3147.971052
2025-10-10 15:15:50,978 Stage: Train 0.5 | Epoch: 161 | Iter: 203600 | Total Loss: 0.002784 | Recon Loss: 0.002403 | Commit Loss: 0.000763 | Perplexity: 3154.101595
2025-10-10 15:19:42,068 Stage: Train 0.5 | Epoch: 161 | Iter: 203800 | Total Loss: 0.002803 | Recon Loss: 0.002426 | Commit Loss: 0.000755 | Perplexity: 3149.498362
2025-10-10 15:23:32,107 Stage: Train 0.5 | Epoch: 161 | Iter: 204000 | Total Loss: 0.002819 | Recon Loss: 0.002444 | Commit Loss: 0.000750 | Perplexity: 3142.711929
Trainning Epoch:  38%|███▊      | 189/494 [52:49:31<98:45:24, 1165.66s/it]Trainning Epoch:  38%|███▊      | 189/494 [52:49:31<98:45:25, 1165.66s/it]2025-10-10 15:27:25,949 Stage: Train 0.5 | Epoch: 162 | Iter: 204200 | Total Loss: 0.002801 | Recon Loss: 0.002427 | Commit Loss: 0.000749 | Perplexity: 3151.750822
2025-10-10 15:31:16,378 Stage: Train 0.5 | Epoch: 162 | Iter: 204400 | Total Loss: 0.002822 | Recon Loss: 0.002447 | Commit Loss: 0.000751 | Perplexity: 3140.528029
2025-10-10 15:35:06,869 Stage: Train 0.5 | Epoch: 162 | Iter: 204600 | Total Loss: 0.002792 | Recon Loss: 0.002419 | Commit Loss: 0.000745 | Perplexity: 3139.340973
2025-10-10 15:38:58,437 Stage: Train 0.5 | Epoch: 162 | Iter: 204800 | Total Loss: 0.002775 | Recon Loss: 0.002402 | Commit Loss: 0.000747 | Perplexity: 3150.473672
2025-10-10 15:42:49,601 Stage: Train 0.5 | Epoch: 162 | Iter: 205000 | Total Loss: 0.002795 | Recon Loss: 0.002421 | Commit Loss: 0.000748 | Perplexity: 3148.563547
Trainning Epoch:  38%|███▊      | 190/494 [53:09:04<98:36:35, 1167.75s/it]Trainning Epoch:  38%|███▊      | 190/494 [53:09:04<98:36:36, 1167.75s/it]2025-10-10 15:46:41,145 Stage: Train 0.5 | Epoch: 163 | Iter: 205200 | Total Loss: 0.002814 | Recon Loss: 0.002442 | Commit Loss: 0.000745 | Perplexity: 3140.570426
2025-10-10 15:50:28,206 Stage: Train 0.5 | Epoch: 163 | Iter: 205400 | Total Loss: 0.002740 | Recon Loss: 0.002368 | Commit Loss: 0.000744 | Perplexity: 3140.685406
2025-10-10 15:54:15,422 Stage: Train 0.5 | Epoch: 163 | Iter: 205600 | Total Loss: 0.002798 | Recon Loss: 0.002421 | Commit Loss: 0.000755 | Perplexity: 3153.870341
2025-10-10 15:58:04,354 Stage: Train 0.5 | Epoch: 163 | Iter: 205800 | Total Loss: 0.002771 | Recon Loss: 0.002402 | Commit Loss: 0.000738 | Perplexity: 3145.670632
2025-10-10 16:01:54,042 Stage: Train 0.5 | Epoch: 163 | Iter: 206000 | Total Loss: 0.002791 | Recon Loss: 0.002414 | Commit Loss: 0.000754 | Perplexity: 3143.275420
Trainning Epoch:  39%|███▊      | 191/494 [53:28:23<98:03:26, 1165.04s/it]Trainning Epoch:  39%|███▊      | 191/494 [53:28:23<98:03:26, 1165.04s/it]2025-10-10 16:05:47,290 Stage: Train 0.5 | Epoch: 164 | Iter: 206200 | Total Loss: 0.002836 | Recon Loss: 0.002461 | Commit Loss: 0.000750 | Perplexity: 3147.329457
2025-10-10 16:09:36,707 Stage: Train 0.5 | Epoch: 164 | Iter: 206400 | Total Loss: 0.002758 | Recon Loss: 0.002382 | Commit Loss: 0.000751 | Perplexity: 3148.233741
2025-10-10 16:13:27,419 Stage: Train 0.5 | Epoch: 164 | Iter: 206600 | Total Loss: 0.002776 | Recon Loss: 0.002400 | Commit Loss: 0.000751 | Perplexity: 3139.375862
2025-10-10 16:17:19,032 Stage: Train 0.5 | Epoch: 164 | Iter: 206800 | Total Loss: 0.002776 | Recon Loss: 0.002396 | Commit Loss: 0.000760 | Perplexity: 3150.195100
2025-10-10 16:21:10,189 Stage: Train 0.5 | Epoch: 164 | Iter: 207000 | Total Loss: 0.002809 | Recon Loss: 0.002429 | Commit Loss: 0.000759 | Perplexity: 3158.556781
Trainning Epoch:  39%|███▉      | 192/494 [53:47:54<97:53:56, 1167.01s/it]Trainning Epoch:  39%|███▉      | 192/494 [53:47:54<97:53:56, 1167.01s/it]2025-10-10 16:25:03,946 Stage: Train 0.5 | Epoch: 165 | Iter: 207200 | Total Loss: 0.002798 | Recon Loss: 0.002421 | Commit Loss: 0.000754 | Perplexity: 3155.595054
2025-10-10 16:28:53,645 Stage: Train 0.5 | Epoch: 165 | Iter: 207400 | Total Loss: 0.002785 | Recon Loss: 0.002409 | Commit Loss: 0.000752 | Perplexity: 3150.654200
2025-10-10 16:32:43,084 Stage: Train 0.5 | Epoch: 165 | Iter: 207600 | Total Loss: 0.002785 | Recon Loss: 0.002410 | Commit Loss: 0.000750 | Perplexity: 3152.378655
2025-10-10 16:36:32,603 Stage: Train 0.5 | Epoch: 165 | Iter: 207800 | Total Loss: 0.002772 | Recon Loss: 0.002398 | Commit Loss: 0.000747 | Perplexity: 3144.279519
2025-10-10 16:40:22,115 Stage: Train 0.5 | Epoch: 165 | Iter: 208000 | Total Loss: 0.002821 | Recon Loss: 0.002448 | Commit Loss: 0.000745 | Perplexity: 3153.557828
Trainning Epoch:  39%|███▉      | 193/494 [54:07:20<97:33:06, 1166.73s/it]Trainning Epoch:  39%|███▉      | 193/494 [54:07:20<97:33:06, 1166.73s/it]2025-10-10 16:44:14,849 Stage: Train 0.5 | Epoch: 166 | Iter: 208200 | Total Loss: 0.002751 | Recon Loss: 0.002376 | Commit Loss: 0.000750 | Perplexity: 3155.912489
2025-10-10 16:48:05,288 Stage: Train 0.5 | Epoch: 166 | Iter: 208400 | Total Loss: 0.002778 | Recon Loss: 0.002404 | Commit Loss: 0.000748 | Perplexity: 3153.585857
2025-10-10 16:51:56,013 Stage: Train 0.5 | Epoch: 166 | Iter: 208600 | Total Loss: 0.002788 | Recon Loss: 0.002412 | Commit Loss: 0.000752 | Perplexity: 3151.734635
2025-10-10 16:55:46,269 Stage: Train 0.5 | Epoch: 166 | Iter: 208800 | Total Loss: 0.002771 | Recon Loss: 0.002398 | Commit Loss: 0.000746 | Perplexity: 3144.602389
2025-10-10 16:59:36,726 Stage: Train 0.5 | Epoch: 166 | Iter: 209000 | Total Loss: 0.002776 | Recon Loss: 0.002404 | Commit Loss: 0.000744 | Perplexity: 3149.863606
Trainning Epoch:  39%|███▉      | 194/494 [54:26:51<97:19:07, 1167.82s/it]Trainning Epoch:  39%|███▉      | 194/494 [54:26:51<97:19:06, 1167.82s/it]2025-10-10 17:03:30,063 Stage: Train 0.5 | Epoch: 167 | Iter: 209200 | Total Loss: 0.002763 | Recon Loss: 0.002390 | Commit Loss: 0.000746 | Perplexity: 3152.375433
2025-10-10 17:07:21,042 Stage: Train 0.5 | Epoch: 167 | Iter: 209400 | Total Loss: 0.002793 | Recon Loss: 0.002420 | Commit Loss: 0.000747 | Perplexity: 3155.490129
2025-10-10 17:11:11,903 Stage: Train 0.5 | Epoch: 167 | Iter: 209600 | Total Loss: 0.002775 | Recon Loss: 0.002403 | Commit Loss: 0.000745 | Perplexity: 3156.092415
2025-10-10 17:15:02,836 Stage: Train 0.5 | Epoch: 167 | Iter: 209800 | Total Loss: 0.002814 | Recon Loss: 0.002439 | Commit Loss: 0.000751 | Perplexity: 3155.766346
2025-10-10 17:18:53,859 Stage: Train 0.5 | Epoch: 167 | Iter: 210000 | Total Loss: 0.002744 | Recon Loss: 0.002366 | Commit Loss: 0.000756 | Perplexity: 3148.848898
Trainning Epoch:  39%|███▉      | 195/494 [54:46:23<97:06:33, 1169.21s/it]Trainning Epoch:  39%|███▉      | 195/494 [54:46:23<97:06:33, 1169.21s/it]2025-10-10 17:22:47,633 Stage: Train 0.5 | Epoch: 168 | Iter: 210200 | Total Loss: 0.002754 | Recon Loss: 0.002381 | Commit Loss: 0.000746 | Perplexity: 3138.591328
2025-10-10 17:26:35,254 Stage: Train 0.5 | Epoch: 168 | Iter: 210400 | Total Loss: 0.002773 | Recon Loss: 0.002401 | Commit Loss: 0.000744 | Perplexity: 3156.923604
2025-10-10 17:30:23,843 Stage: Train 0.5 | Epoch: 168 | Iter: 210600 | Total Loss: 0.002764 | Recon Loss: 0.002387 | Commit Loss: 0.000754 | Perplexity: 3149.456721
2025-10-10 17:34:13,121 Stage: Train 0.5 | Epoch: 168 | Iter: 210800 | Total Loss: 0.002787 | Recon Loss: 0.002416 | Commit Loss: 0.000743 | Perplexity: 3159.449635
2025-10-10 17:38:01,993 Stage: Train 0.5 | Epoch: 168 | Iter: 211000 | Total Loss: 0.002808 | Recon Loss: 0.002433 | Commit Loss: 0.000751 | Perplexity: 3161.086503
Trainning Epoch:  40%|███▉      | 196/494 [55:05:45<96:35:55, 1166.97s/it]Trainning Epoch:  40%|███▉      | 196/494 [55:05:45<96:35:55, 1166.97s/it]2025-10-10 17:41:54,329 Stage: Train 0.5 | Epoch: 169 | Iter: 211200 | Total Loss: 0.002755 | Recon Loss: 0.002381 | Commit Loss: 0.000749 | Perplexity: 3161.041378
2025-10-10 17:45:45,305 Stage: Train 0.5 | Epoch: 169 | Iter: 211400 | Total Loss: 0.002760 | Recon Loss: 0.002383 | Commit Loss: 0.000754 | Perplexity: 3162.502231
2025-10-10 17:49:36,522 Stage: Train 0.5 | Epoch: 169 | Iter: 211600 | Total Loss: 0.002762 | Recon Loss: 0.002390 | Commit Loss: 0.000743 | Perplexity: 3154.145242
2025-10-10 17:53:27,511 Stage: Train 0.5 | Epoch: 169 | Iter: 211800 | Total Loss: 0.002765 | Recon Loss: 0.002389 | Commit Loss: 0.000753 | Perplexity: 3152.470104
2025-10-10 17:57:18,597 Stage: Train 0.5 | Epoch: 169 | Iter: 212000 | Total Loss: 0.002747 | Recon Loss: 0.002368 | Commit Loss: 0.000757 | Perplexity: 3163.680094
2025-10-10 18:01:09,650 Stage: Train 0.5 | Epoch: 169 | Iter: 212200 | Total Loss: 0.002754 | Recon Loss: 0.002379 | Commit Loss: 0.000748 | Perplexity: 3150.268669
Trainning Epoch:  40%|███▉      | 197/494 [55:25:18<96:26:01, 1168.89s/it]Trainning Epoch:  40%|███▉      | 197/494 [55:25:18<96:26:01, 1168.90s/it]2025-10-10 18:05:00,354 Stage: Train 0.5 | Epoch: 170 | Iter: 212400 | Total Loss: 0.002764 | Recon Loss: 0.002391 | Commit Loss: 0.000746 | Perplexity: 3152.299740
2025-10-10 18:08:48,476 Stage: Train 0.5 | Epoch: 170 | Iter: 212600 | Total Loss: 0.002740 | Recon Loss: 0.002373 | Commit Loss: 0.000734 | Perplexity: 3142.454556
2025-10-10 18:12:36,094 Stage: Train 0.5 | Epoch: 170 | Iter: 212800 | Total Loss: 0.002747 | Recon Loss: 0.002371 | Commit Loss: 0.000751 | Perplexity: 3155.472609
2025-10-10 18:16:24,694 Stage: Train 0.5 | Epoch: 170 | Iter: 213000 | Total Loss: 0.002767 | Recon Loss: 0.002395 | Commit Loss: 0.000744 | Perplexity: 3153.070579
2025-10-10 18:20:13,221 Stage: Train 0.5 | Epoch: 170 | Iter: 213200 | Total Loss: 0.002748 | Recon Loss: 0.002379 | Commit Loss: 0.000736 | Perplexity: 3148.178048
Trainning Epoch:  40%|████      | 198/494 [55:44:37<95:50:51, 1165.71s/it]Trainning Epoch:  40%|████      | 198/494 [55:44:37<95:50:51, 1165.71s/it]2025-10-10 18:24:07,058 Stage: Train 0.5 | Epoch: 171 | Iter: 213400 | Total Loss: 0.002750 | Recon Loss: 0.002374 | Commit Loss: 0.000752 | Perplexity: 3153.535652
2025-10-10 18:27:58,395 Stage: Train 0.5 | Epoch: 171 | Iter: 213600 | Total Loss: 0.002736 | Recon Loss: 0.002361 | Commit Loss: 0.000751 | Perplexity: 3151.084379
2025-10-10 18:31:49,530 Stage: Train 0.5 | Epoch: 171 | Iter: 213800 | Total Loss: 0.002736 | Recon Loss: 0.002363 | Commit Loss: 0.000745 | Perplexity: 3153.547325
2025-10-10 18:35:40,355 Stage: Train 0.5 | Epoch: 171 | Iter: 214000 | Total Loss: 0.002763 | Recon Loss: 0.002393 | Commit Loss: 0.000741 | Perplexity: 3161.197731
2025-10-10 18:39:31,517 Stage: Train 0.5 | Epoch: 171 | Iter: 214200 | Total Loss: 0.002731 | Recon Loss: 0.002352 | Commit Loss: 0.000758 | Perplexity: 3148.119009
Trainning Epoch:  40%|████      | 199/494 [56:04:10<95:43:15, 1168.12s/it]Trainning Epoch:  40%|████      | 199/494 [56:04:10<95:43:14, 1168.12s/it]2025-10-10 18:43:25,240 Stage: Train 0.5 | Epoch: 172 | Iter: 214400 | Total Loss: 0.002749 | Recon Loss: 0.002377 | Commit Loss: 0.000745 | Perplexity: 3145.146752
2025-10-10 18:47:16,469 Stage: Train 0.5 | Epoch: 172 | Iter: 214600 | Total Loss: 0.002770 | Recon Loss: 0.002396 | Commit Loss: 0.000749 | Perplexity: 3148.395526
2025-10-10 18:51:08,724 Stage: Train 0.5 | Epoch: 172 | Iter: 214800 | Total Loss: 0.002760 | Recon Loss: 0.002394 | Commit Loss: 0.000731 | Perplexity: 3144.441664
2025-10-10 18:55:00,703 Stage: Train 0.5 | Epoch: 172 | Iter: 215000 | Total Loss: 0.002747 | Recon Loss: 0.002372 | Commit Loss: 0.000751 | Perplexity: 3146.565311
2025-10-10 18:58:49,802 Stage: Train 0.5 | Epoch: 172 | Iter: 215200 | Total Loss: 0.002739 | Recon Loss: 0.002362 | Commit Loss: 0.000753 | Perplexity: 3152.151340
Trainning Epoch:  40%|████      | 200/494 [56:23:43<95:30:12, 1169.43s/it]Trainning Epoch:  40%|████      | 200/494 [56:23:43<95:30:13, 1169.43s/it]2025-10-10 19:02:42,978 Stage: Train 0.5 | Epoch: 173 | Iter: 215400 | Total Loss: 0.002727 | Recon Loss: 0.002354 | Commit Loss: 0.000747 | Perplexity: 3153.300110
2025-10-10 19:06:33,872 Stage: Train 0.5 | Epoch: 173 | Iter: 215600 | Total Loss: 0.002750 | Recon Loss: 0.002381 | Commit Loss: 0.000738 | Perplexity: 3161.156338
2025-10-10 19:10:25,813 Stage: Train 0.5 | Epoch: 173 | Iter: 215800 | Total Loss: 0.002744 | Recon Loss: 0.002371 | Commit Loss: 0.000746 | Perplexity: 3164.078936
2025-10-10 19:14:16,670 Stage: Train 0.5 | Epoch: 173 | Iter: 216000 | Total Loss: 0.002761 | Recon Loss: 0.002390 | Commit Loss: 0.000742 | Perplexity: 3156.042578
2025-10-10 19:18:07,336 Stage: Train 0.5 | Epoch: 173 | Iter: 216200 | Total Loss: 0.002748 | Recon Loss: 0.002373 | Commit Loss: 0.000750 | Perplexity: 3146.478667
Trainning Epoch:  41%|████      | 201/494 [56:43:16<95:16:05, 1170.53s/it]Trainning Epoch:  41%|████      | 201/494 [56:43:16<95:16:05, 1170.53s/it]2025-10-10 19:21:57,577 Stage: Train 0.5 | Epoch: 174 | Iter: 216400 | Total Loss: 0.002755 | Recon Loss: 0.002382 | Commit Loss: 0.000747 | Perplexity: 3151.146451
2025-10-10 19:25:43,955 Stage: Train 0.5 | Epoch: 174 | Iter: 216600 | Total Loss: 0.002746 | Recon Loss: 0.002372 | Commit Loss: 0.000748 | Perplexity: 3146.218531
2025-10-10 19:29:31,086 Stage: Train 0.5 | Epoch: 174 | Iter: 216800 | Total Loss: 0.002739 | Recon Loss: 0.002369 | Commit Loss: 0.000742 | Perplexity: 3163.518057
2025-10-10 19:33:18,326 Stage: Train 0.5 | Epoch: 174 | Iter: 217000 | Total Loss: 0.002776 | Recon Loss: 0.002405 | Commit Loss: 0.000744 | Perplexity: 3154.841713
2025-10-10 19:37:05,113 Stage: Train 0.5 | Epoch: 174 | Iter: 217200 | Total Loss: 0.002725 | Recon Loss: 0.002354 | Commit Loss: 0.000742 | Perplexity: 3159.536987
Trainning Epoch:  41%|████      | 202/494 [57:02:27<94:28:23, 1164.74s/it]Trainning Epoch:  41%|████      | 202/494 [57:02:27<94:28:22, 1164.74s/it]2025-10-10 19:40:57,223 Stage: Train 0.5 | Epoch: 175 | Iter: 217400 | Total Loss: 0.002758 | Recon Loss: 0.002384 | Commit Loss: 0.000748 | Perplexity: 3160.269607
2025-10-10 19:44:48,446 Stage: Train 0.5 | Epoch: 175 | Iter: 217600 | Total Loss: 0.002737 | Recon Loss: 0.002363 | Commit Loss: 0.000747 | Perplexity: 3161.037253
2025-10-10 19:48:39,327 Stage: Train 0.5 | Epoch: 175 | Iter: 217800 | Total Loss: 0.002715 | Recon Loss: 0.002344 | Commit Loss: 0.000743 | Perplexity: 3154.957723
2025-10-10 19:52:30,386 Stage: Train 0.5 | Epoch: 175 | Iter: 218000 | Total Loss: 0.002793 | Recon Loss: 0.002422 | Commit Loss: 0.000741 | Perplexity: 3139.506698
2025-10-10 19:56:21,353 Stage: Train 0.5 | Epoch: 175 | Iter: 218200 | Total Loss: 0.002716 | Recon Loss: 0.002344 | Commit Loss: 0.000743 | Perplexity: 3160.573109
Trainning Epoch:  41%|████      | 203/494 [57:22:00<94:21:05, 1167.23s/it]Trainning Epoch:  41%|████      | 203/494 [57:22:00<94:21:05, 1167.23s/it]2025-10-10 20:00:15,095 Stage: Train 0.5 | Epoch: 176 | Iter: 218400 | Total Loss: 0.002745 | Recon Loss: 0.002365 | Commit Loss: 0.000760 | Perplexity: 3152.794031
2025-10-10 20:04:05,262 Stage: Train 0.5 | Epoch: 176 | Iter: 218600 | Total Loss: 0.002753 | Recon Loss: 0.002375 | Commit Loss: 0.000756 | Perplexity: 3152.818602
2025-10-10 20:07:55,575 Stage: Train 0.5 | Epoch: 176 | Iter: 218800 | Total Loss: 0.002705 | Recon Loss: 0.002334 | Commit Loss: 0.000743 | Perplexity: 3156.554946
2025-10-10 20:11:45,954 Stage: Train 0.5 | Epoch: 176 | Iter: 219000 | Total Loss: 0.002748 | Recon Loss: 0.002372 | Commit Loss: 0.000751 | Perplexity: 3165.983260
2025-10-10 20:15:37,046 Stage: Train 0.5 | Epoch: 176 | Iter: 219200 | Total Loss: 0.002737 | Recon Loss: 0.002366 | Commit Loss: 0.000742 | Perplexity: 3145.749656
Trainning Epoch:  41%|████▏     | 204/494 [57:41:31<94:06:11, 1168.18s/it]Trainning Epoch:  41%|████▏     | 204/494 [57:41:31<94:06:11, 1168.18s/it]2025-10-10 20:19:28,390 Stage: Train 0.5 | Epoch: 177 | Iter: 219400 | Total Loss: 0.002717 | Recon Loss: 0.002349 | Commit Loss: 0.000736 | Perplexity: 3168.080659
2025-10-10 20:23:15,443 Stage: Train 0.5 | Epoch: 177 | Iter: 219600 | Total Loss: 0.002728 | Recon Loss: 0.002354 | Commit Loss: 0.000748 | Perplexity: 3171.815632
2025-10-10 20:27:03,973 Stage: Train 0.5 | Epoch: 177 | Iter: 219800 | Total Loss: 0.002716 | Recon Loss: 0.002338 | Commit Loss: 0.000757 | Perplexity: 3158.113616
2025-10-10 20:30:53,218 Stage: Train 0.5 | Epoch: 177 | Iter: 220000 | Total Loss: 0.002720 | Recon Loss: 0.002348 | Commit Loss: 0.000743 | Perplexity: 3149.700756
2025-10-10 20:30:53,218 Saving model at iteration 220000
2025-10-10 20:30:53,707 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000
2025-10-10 20:30:54,970 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000/model.safetensors
2025-10-10 20:30:56,516 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000/optimizer.bin
2025-10-10 20:30:56,516 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000/scheduler.bin
2025-10-10 20:30:56,517 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000/sampler.bin
2025-10-10 20:30:56,518 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_178_step_220000/random_states_0.pkl
2025-10-10 20:34:45,455 Stage: Train 0.5 | Epoch: 177 | Iter: 220200 | Total Loss: 0.002746 | Recon Loss: 0.002376 | Commit Loss: 0.000741 | Perplexity: 3146.183884
Trainning Epoch:  41%|████▏     | 205/494 [58:00:53<93:38:24, 1166.45s/it]Trainning Epoch:  41%|████▏     | 205/494 [58:00:53<93:38:24, 1166.45s/it]2025-10-10 20:38:35,857 Stage: Train 0.5 | Epoch: 178 | Iter: 220400 | Total Loss: 0.002745 | Recon Loss: 0.002376 | Commit Loss: 0.000738 | Perplexity: 3158.665598
2025-10-10 20:42:22,618 Stage: Train 0.5 | Epoch: 178 | Iter: 220600 | Total Loss: 0.002727 | Recon Loss: 0.002360 | Commit Loss: 0.000736 | Perplexity: 3160.178890
2025-10-10 20:46:10,015 Stage: Train 0.5 | Epoch: 178 | Iter: 220800 | Total Loss: 0.002727 | Recon Loss: 0.002355 | Commit Loss: 0.000743 | Perplexity: 3156.435555
2025-10-10 20:49:57,495 Stage: Train 0.5 | Epoch: 178 | Iter: 221000 | Total Loss: 0.002716 | Recon Loss: 0.002342 | Commit Loss: 0.000749 | Perplexity: 3165.637317
2025-10-10 20:53:45,039 Stage: Train 0.5 | Epoch: 178 | Iter: 221200 | Total Loss: 0.002708 | Recon Loss: 0.002336 | Commit Loss: 0.000743 | Perplexity: 3168.505593
Trainning Epoch:  42%|████▏     | 206/494 [58:20:06<93:00:22, 1162.58s/it]Trainning Epoch:  42%|████▏     | 206/494 [58:20:07<93:00:23, 1162.58s/it]2025-10-10 20:57:35,954 Stage: Train 0.5 | Epoch: 179 | Iter: 221400 | Total Loss: 0.002725 | Recon Loss: 0.002358 | Commit Loss: 0.000734 | Perplexity: 3153.151730
2025-10-10 21:01:24,926 Stage: Train 0.5 | Epoch: 179 | Iter: 221600 | Total Loss: 0.002700 | Recon Loss: 0.002332 | Commit Loss: 0.000737 | Perplexity: 3157.000665
2025-10-10 21:05:13,938 Stage: Train 0.5 | Epoch: 179 | Iter: 221800 | Total Loss: 0.002688 | Recon Loss: 0.002316 | Commit Loss: 0.000743 | Perplexity: 3164.612815
2025-10-10 21:09:03,014 Stage: Train 0.5 | Epoch: 179 | Iter: 222000 | Total Loss: 0.002738 | Recon Loss: 0.002363 | Commit Loss: 0.000751 | Perplexity: 3165.174967
2025-10-10 21:12:52,048 Stage: Train 0.5 | Epoch: 179 | Iter: 222200 | Total Loss: 0.002719 | Recon Loss: 0.002337 | Commit Loss: 0.000763 | Perplexity: 3160.931630
Trainning Epoch:  42%|████▏     | 207/494 [58:39:30<92:41:50, 1162.75s/it]Trainning Epoch:  42%|████▏     | 207/494 [58:39:30<92:41:50, 1162.76s/it]2025-10-10 21:16:43,773 Stage: Train 0.5 | Epoch: 180 | Iter: 222400 | Total Loss: 0.002736 | Recon Loss: 0.002357 | Commit Loss: 0.000758 | Perplexity: 3165.488627
2025-10-10 21:20:32,330 Stage: Train 0.5 | Epoch: 180 | Iter: 222600 | Total Loss: 0.002740 | Recon Loss: 0.002372 | Commit Loss: 0.000736 | Perplexity: 3175.731238
2025-10-10 21:24:21,608 Stage: Train 0.5 | Epoch: 180 | Iter: 222800 | Total Loss: 0.002721 | Recon Loss: 0.002348 | Commit Loss: 0.000747 | Perplexity: 3158.914316
2025-10-10 21:28:10,507 Stage: Train 0.5 | Epoch: 180 | Iter: 223000 | Total Loss: 0.002700 | Recon Loss: 0.002331 | Commit Loss: 0.000739 | Perplexity: 3156.030981
2025-10-10 21:31:59,550 Stage: Train 0.5 | Epoch: 180 | Iter: 223200 | Total Loss: 0.002735 | Recon Loss: 0.002361 | Commit Loss: 0.000748 | Perplexity: 3151.657719
Trainning Epoch:  42%|████▏     | 208/494 [58:58:52<92:21:32, 1162.56s/it]Trainning Epoch:  42%|████▏     | 208/494 [58:58:52<92:21:33, 1162.56s/it]2025-10-10 21:35:52,056 Stage: Train 0.5 | Epoch: 181 | Iter: 223400 | Total Loss: 0.002716 | Recon Loss: 0.002347 | Commit Loss: 0.000739 | Perplexity: 3167.478361
2025-10-10 21:39:43,249 Stage: Train 0.5 | Epoch: 181 | Iter: 223600 | Total Loss: 0.002730 | Recon Loss: 0.002359 | Commit Loss: 0.000743 | Perplexity: 3170.556478
2025-10-10 21:43:35,127 Stage: Train 0.5 | Epoch: 181 | Iter: 223800 | Total Loss: 0.002717 | Recon Loss: 0.002348 | Commit Loss: 0.000740 | Perplexity: 3157.822599
2025-10-10 21:47:26,498 Stage: Train 0.5 | Epoch: 181 | Iter: 224000 | Total Loss: 0.002711 | Recon Loss: 0.002339 | Commit Loss: 0.000745 | Perplexity: 3155.399259
2025-10-10 21:51:17,863 Stage: Train 0.5 | Epoch: 181 | Iter: 224200 | Total Loss: 0.002689 | Recon Loss: 0.002319 | Commit Loss: 0.000740 | Perplexity: 3161.118525
Trainning Epoch:  42%|████▏     | 209/494 [59:18:27<92:19:38, 1166.24s/it]Trainning Epoch:  42%|████▏     | 209/494 [59:18:27<92:19:38, 1166.24s/it]2025-10-10 21:55:11,304 Stage: Train 0.5 | Epoch: 182 | Iter: 224400 | Total Loss: 0.002733 | Recon Loss: 0.002361 | Commit Loss: 0.000744 | Perplexity: 3153.439227
2025-10-10 21:58:59,919 Stage: Train 0.5 | Epoch: 182 | Iter: 224600 | Total Loss: 0.002703 | Recon Loss: 0.002331 | Commit Loss: 0.000743 | Perplexity: 3150.093497
2025-10-10 22:02:49,699 Stage: Train 0.5 | Epoch: 182 | Iter: 224800 | Total Loss: 0.002679 | Recon Loss: 0.002309 | Commit Loss: 0.000740 | Perplexity: 3162.477258
2025-10-10 22:06:40,003 Stage: Train 0.5 | Epoch: 182 | Iter: 225000 | Total Loss: 0.002751 | Recon Loss: 0.002381 | Commit Loss: 0.000740 | Perplexity: 3162.541241
2025-10-10 22:10:30,034 Stage: Train 0.5 | Epoch: 182 | Iter: 225200 | Total Loss: 0.002692 | Recon Loss: 0.002325 | Commit Loss: 0.000735 | Perplexity: 3154.871685
Trainning Epoch:  43%|████▎     | 210/494 [59:37:53<92:00:25, 1166.29s/it]Trainning Epoch:  43%|████▎     | 210/494 [59:37:53<92:00:25, 1166.29s/it]2025-10-10 22:14:23,150 Stage: Train 0.5 | Epoch: 183 | Iter: 225400 | Total Loss: 0.002779 | Recon Loss: 0.002408 | Commit Loss: 0.000743 | Perplexity: 3157.911869
2025-10-10 22:18:10,732 Stage: Train 0.5 | Epoch: 183 | Iter: 225600 | Total Loss: 0.002662 | Recon Loss: 0.002292 | Commit Loss: 0.000740 | Perplexity: 3158.726077
2025-10-10 22:21:59,269 Stage: Train 0.5 | Epoch: 183 | Iter: 225800 | Total Loss: 0.002767 | Recon Loss: 0.002396 | Commit Loss: 0.000743 | Perplexity: 3159.960551
2025-10-10 22:25:47,473 Stage: Train 0.5 | Epoch: 183 | Iter: 226000 | Total Loss: 0.002695 | Recon Loss: 0.002324 | Commit Loss: 0.000743 | Perplexity: 3159.290677
2025-10-10 22:29:35,915 Stage: Train 0.5 | Epoch: 183 | Iter: 226200 | Total Loss: 0.002699 | Recon Loss: 0.002328 | Commit Loss: 0.000743 | Perplexity: 3165.488118
Trainning Epoch:  43%|████▎     | 211/494 [59:57:13<91:32:13, 1164.43s/it]Trainning Epoch:  43%|████▎     | 211/494 [59:57:13<91:32:12, 1164.43s/it]2025-10-10 22:33:28,255 Stage: Train 0.5 | Epoch: 184 | Iter: 226400 | Total Loss: 0.002710 | Recon Loss: 0.002341 | Commit Loss: 0.000738 | Perplexity: 3149.724596
2025-10-10 22:37:17,312 Stage: Train 0.5 | Epoch: 184 | Iter: 226600 | Total Loss: 0.002699 | Recon Loss: 0.002331 | Commit Loss: 0.000736 | Perplexity: 3162.339829
2025-10-10 22:41:06,345 Stage: Train 0.5 | Epoch: 184 | Iter: 226800 | Total Loss: 0.002712 | Recon Loss: 0.002341 | Commit Loss: 0.000742 | Perplexity: 3161.670175
2025-10-10 22:44:55,405 Stage: Train 0.5 | Epoch: 184 | Iter: 227000 | Total Loss: 0.002720 | Recon Loss: 0.002347 | Commit Loss: 0.000747 | Perplexity: 3161.983820
2025-10-10 22:48:44,541 Stage: Train 0.5 | Epoch: 184 | Iter: 227200 | Total Loss: 0.002697 | Recon Loss: 0.002325 | Commit Loss: 0.000743 | Perplexity: 3151.683690
2025-10-10 22:52:33,756 Stage: Train 0.5 | Epoch: 184 | Iter: 227400 | Total Loss: 0.002713 | Recon Loss: 0.002344 | Commit Loss: 0.000738 | Perplexity: 3161.113361
Trainning Epoch:  43%|████▎     | 212/494 [60:16:37<91:11:26, 1164.14s/it]Trainning Epoch:  43%|████▎     | 212/494 [60:16:37<91:11:26, 1164.14s/it]2025-10-10 22:56:23,807 Stage: Train 0.5 | Epoch: 185 | Iter: 227600 | Total Loss: 0.002675 | Recon Loss: 0.002307 | Commit Loss: 0.000737 | Perplexity: 3159.672175
2025-10-10 23:00:12,369 Stage: Train 0.5 | Epoch: 185 | Iter: 227800 | Total Loss: 0.002682 | Recon Loss: 0.002317 | Commit Loss: 0.000730 | Perplexity: 3159.891731
2025-10-10 23:04:00,767 Stage: Train 0.5 | Epoch: 185 | Iter: 228000 | Total Loss: 0.002708 | Recon Loss: 0.002336 | Commit Loss: 0.000744 | Perplexity: 3165.683772
2025-10-10 23:07:49,110 Stage: Train 0.5 | Epoch: 185 | Iter: 228200 | Total Loss: 0.002728 | Recon Loss: 0.002358 | Commit Loss: 0.000741 | Perplexity: 3159.366130
2025-10-10 23:11:37,464 Stage: Train 0.5 | Epoch: 185 | Iter: 228400 | Total Loss: 0.002702 | Recon Loss: 0.002328 | Commit Loss: 0.000747 | Perplexity: 3160.740013
Trainning Epoch:  43%|████▎     | 213/494 [60:35:55<90:44:02, 1162.43s/it]Trainning Epoch:  43%|████▎     | 213/494 [60:35:55<90:44:02, 1162.43s/it]2025-10-10 23:15:26,947 Stage: Train 0.5 | Epoch: 186 | Iter: 228600 | Total Loss: 0.002669 | Recon Loss: 0.002303 | Commit Loss: 0.000732 | Perplexity: 3158.405128
2025-10-10 23:19:14,790 Stage: Train 0.5 | Epoch: 186 | Iter: 228800 | Total Loss: 0.002693 | Recon Loss: 0.002325 | Commit Loss: 0.000737 | Perplexity: 3167.596650
2025-10-10 23:23:02,574 Stage: Train 0.5 | Epoch: 186 | Iter: 229000 | Total Loss: 0.002659 | Recon Loss: 0.002287 | Commit Loss: 0.000743 | Perplexity: 3161.739950
2025-10-10 23:26:50,205 Stage: Train 0.5 | Epoch: 186 | Iter: 229200 | Total Loss: 0.002700 | Recon Loss: 0.002333 | Commit Loss: 0.000735 | Perplexity: 3161.411630
2025-10-10 23:30:38,070 Stage: Train 0.5 | Epoch: 186 | Iter: 229400 | Total Loss: 0.002690 | Recon Loss: 0.002322 | Commit Loss: 0.000737 | Perplexity: 3155.900745
Trainning Epoch:  43%|████▎     | 214/494 [60:55:11<90:15:04, 1160.37s/it]Trainning Epoch:  43%|████▎     | 214/494 [60:55:11<90:15:03, 1160.37s/it]2025-10-10 23:34:32,010 Stage: Train 0.5 | Epoch: 187 | Iter: 229600 | Total Loss: 0.002706 | Recon Loss: 0.002335 | Commit Loss: 0.000742 | Perplexity: 3167.659680
2025-10-10 23:38:23,346 Stage: Train 0.5 | Epoch: 187 | Iter: 229800 | Total Loss: 0.002709 | Recon Loss: 0.002337 | Commit Loss: 0.000745 | Perplexity: 3157.282396
2025-10-10 23:42:14,629 Stage: Train 0.5 | Epoch: 187 | Iter: 230000 | Total Loss: 0.002705 | Recon Loss: 0.002336 | Commit Loss: 0.000738 | Perplexity: 3163.337992
2025-10-10 23:46:06,284 Stage: Train 0.5 | Epoch: 187 | Iter: 230200 | Total Loss: 0.002684 | Recon Loss: 0.002318 | Commit Loss: 0.000731 | Perplexity: 3164.769454
2025-10-10 23:49:57,846 Stage: Train 0.5 | Epoch: 187 | Iter: 230400 | Total Loss: 0.002670 | Recon Loss: 0.002301 | Commit Loss: 0.000738 | Perplexity: 3160.775861
Trainning Epoch:  44%|████▎     | 215/494 [61:14:46<90:16:22, 1164.81s/it]Trainning Epoch:  44%|████▎     | 215/494 [61:14:46<90:16:24, 1164.82s/it]2025-10-10 23:53:51,561 Stage: Train 0.5 | Epoch: 188 | Iter: 230600 | Total Loss: 0.002689 | Recon Loss: 0.002318 | Commit Loss: 0.000743 | Perplexity: 3165.727195
2025-10-10 23:57:42,058 Stage: Train 0.5 | Epoch: 188 | Iter: 230800 | Total Loss: 0.002684 | Recon Loss: 0.002314 | Commit Loss: 0.000739 | Perplexity: 3163.356332
2025-10-11 00:01:32,542 Stage: Train 0.5 | Epoch: 188 | Iter: 231000 | Total Loss: 0.002695 | Recon Loss: 0.002327 | Commit Loss: 0.000736 | Perplexity: 3153.918855
2025-10-11 00:05:23,050 Stage: Train 0.5 | Epoch: 188 | Iter: 231200 | Total Loss: 0.002685 | Recon Loss: 0.002319 | Commit Loss: 0.000731 | Perplexity: 3166.174192
2025-10-11 00:09:13,626 Stage: Train 0.5 | Epoch: 188 | Iter: 231400 | Total Loss: 0.002692 | Recon Loss: 0.002323 | Commit Loss: 0.000738 | Perplexity: 3166.782686
Trainning Epoch:  44%|████▎     | 216/494 [61:34:16<90:04:27, 1166.43s/it]Trainning Epoch:  44%|████▎     | 216/494 [61:34:16<90:04:27, 1166.43s/it]2025-10-11 00:13:05,373 Stage: Train 0.5 | Epoch: 189 | Iter: 231600 | Total Loss: 0.002713 | Recon Loss: 0.002343 | Commit Loss: 0.000739 | Perplexity: 3169.833557
2025-10-11 00:16:54,419 Stage: Train 0.5 | Epoch: 189 | Iter: 231800 | Total Loss: 0.002706 | Recon Loss: 0.002338 | Commit Loss: 0.000736 | Perplexity: 3164.430084
2025-10-11 00:20:43,269 Stage: Train 0.5 | Epoch: 189 | Iter: 232000 | Total Loss: 0.002683 | Recon Loss: 0.002311 | Commit Loss: 0.000743 | Perplexity: 3156.983910
2025-10-11 00:24:32,319 Stage: Train 0.5 | Epoch: 189 | Iter: 232200 | Total Loss: 0.002738 | Recon Loss: 0.002372 | Commit Loss: 0.000733 | Perplexity: 3160.711708
2025-10-11 00:28:20,950 Stage: Train 0.5 | Epoch: 189 | Iter: 232400 | Total Loss: 0.002704 | Recon Loss: 0.002336 | Commit Loss: 0.000737 | Perplexity: 3153.743387
Trainning Epoch:  44%|████▍     | 217/494 [61:53:38<89:39:13, 1165.18s/it]Trainning Epoch:  44%|████▍     | 217/494 [61:53:38<89:39:14, 1165.18s/it]2025-10-11 00:32:11,046 Stage: Train 0.5 | Epoch: 190 | Iter: 232600 | Total Loss: 0.002743 | Recon Loss: 0.002367 | Commit Loss: 0.000752 | Perplexity: 3156.270348
2025-10-11 00:35:59,247 Stage: Train 0.5 | Epoch: 190 | Iter: 232800 | Total Loss: 0.002634 | Recon Loss: 0.002262 | Commit Loss: 0.000745 | Perplexity: 3167.678116
2025-10-11 00:39:47,253 Stage: Train 0.5 | Epoch: 190 | Iter: 233000 | Total Loss: 0.002672 | Recon Loss: 0.002302 | Commit Loss: 0.000740 | Perplexity: 3169.373024
2025-10-11 00:43:34,881 Stage: Train 0.5 | Epoch: 190 | Iter: 233200 | Total Loss: 0.002696 | Recon Loss: 0.002330 | Commit Loss: 0.000733 | Perplexity: 3161.404780
2025-10-11 00:47:22,174 Stage: Train 0.5 | Epoch: 190 | Iter: 233400 | Total Loss: 0.002678 | Recon Loss: 0.002316 | Commit Loss: 0.000725 | Perplexity: 3165.982799
Trainning Epoch:  44%|████▍     | 218/494 [62:12:54<89:06:38, 1162.31s/it]Trainning Epoch:  44%|████▍     | 218/494 [62:12:54<89:06:38, 1162.31s/it]2025-10-11 00:51:14,250 Stage: Train 0.5 | Epoch: 191 | Iter: 233600 | Total Loss: 0.002655 | Recon Loss: 0.002287 | Commit Loss: 0.000736 | Perplexity: 3168.962426
2025-10-11 00:55:02,986 Stage: Train 0.5 | Epoch: 191 | Iter: 233800 | Total Loss: 0.002663 | Recon Loss: 0.002298 | Commit Loss: 0.000730 | Perplexity: 3165.481860
2025-10-11 00:58:51,389 Stage: Train 0.5 | Epoch: 191 | Iter: 234000 | Total Loss: 0.002660 | Recon Loss: 0.002292 | Commit Loss: 0.000735 | Perplexity: 3149.897225
2025-10-11 01:02:40,060 Stage: Train 0.5 | Epoch: 191 | Iter: 234200 | Total Loss: 0.002685 | Recon Loss: 0.002309 | Commit Loss: 0.000753 | Perplexity: 3172.325874
2025-10-11 01:06:28,584 Stage: Train 0.5 | Epoch: 191 | Iter: 234400 | Total Loss: 0.002667 | Recon Loss: 0.002297 | Commit Loss: 0.000740 | Perplexity: 3163.804023
Trainning Epoch:  44%|████▍     | 219/494 [62:32:15<88:45:50, 1162.00s/it]Trainning Epoch:  44%|████▍     | 219/494 [62:32:15<88:45:51, 1162.01s/it]2025-10-11 01:10:21,525 Stage: Train 0.5 | Epoch: 192 | Iter: 234600 | Total Loss: 0.002659 | Recon Loss: 0.002295 | Commit Loss: 0.000729 | Perplexity: 3164.333136
2025-10-11 01:14:13,026 Stage: Train 0.5 | Epoch: 192 | Iter: 234800 | Total Loss: 0.002675 | Recon Loss: 0.002304 | Commit Loss: 0.000743 | Perplexity: 3166.097097
2025-10-11 01:18:04,276 Stage: Train 0.5 | Epoch: 192 | Iter: 235000 | Total Loss: 0.002690 | Recon Loss: 0.002322 | Commit Loss: 0.000736 | Perplexity: 3175.803654
2025-10-11 01:21:55,813 Stage: Train 0.5 | Epoch: 192 | Iter: 235200 | Total Loss: 0.002710 | Recon Loss: 0.002344 | Commit Loss: 0.000733 | Perplexity: 3155.557559
2025-10-11 01:25:47,504 Stage: Train 0.5 | Epoch: 192 | Iter: 235400 | Total Loss: 0.002675 | Recon Loss: 0.002303 | Commit Loss: 0.000744 | Perplexity: 3161.033571
Trainning Epoch:  45%|████▍     | 220/494 [62:51:50<88:44:22, 1165.92s/it]Trainning Epoch:  45%|████▍     | 220/494 [62:51:50<88:44:23, 1165.92s/it]2025-10-11 01:29:40,703 Stage: Train 0.5 | Epoch: 193 | Iter: 235600 | Total Loss: 0.002718 | Recon Loss: 0.002348 | Commit Loss: 0.000740 | Perplexity: 3172.905895
2025-10-11 01:33:30,601 Stage: Train 0.5 | Epoch: 193 | Iter: 235800 | Total Loss: 0.002659 | Recon Loss: 0.002288 | Commit Loss: 0.000742 | Perplexity: 3170.317526
2025-10-11 01:37:20,868 Stage: Train 0.5 | Epoch: 193 | Iter: 236000 | Total Loss: 0.002667 | Recon Loss: 0.002296 | Commit Loss: 0.000741 | Perplexity: 3168.876115
2025-10-11 01:41:11,556 Stage: Train 0.5 | Epoch: 193 | Iter: 236200 | Total Loss: 0.002661 | Recon Loss: 0.002293 | Commit Loss: 0.000736 | Perplexity: 3164.148783
2025-10-11 01:45:02,073 Stage: Train 0.5 | Epoch: 193 | Iter: 236400 | Total Loss: 0.002690 | Recon Loss: 0.002318 | Commit Loss: 0.000745 | Perplexity: 3163.154121
Trainning Epoch:  45%|████▍     | 221/494 [63:11:20<88:30:12, 1167.08s/it]Trainning Epoch:  45%|████▍     | 221/494 [63:11:20<88:30:12, 1167.08s/it]2025-10-11 01:48:54,420 Stage: Train 0.5 | Epoch: 194 | Iter: 236600 | Total Loss: 0.002663 | Recon Loss: 0.002298 | Commit Loss: 0.000731 | Perplexity: 3167.096174
2025-10-11 01:52:42,943 Stage: Train 0.5 | Epoch: 194 | Iter: 236800 | Total Loss: 0.002653 | Recon Loss: 0.002283 | Commit Loss: 0.000741 | Perplexity: 3164.459214
2025-10-11 01:56:32,150 Stage: Train 0.5 | Epoch: 194 | Iter: 237000 | Total Loss: 0.002657 | Recon Loss: 0.002287 | Commit Loss: 0.000740 | Perplexity: 3170.283236
2025-10-11 02:00:21,184 Stage: Train 0.5 | Epoch: 194 | Iter: 237200 | Total Loss: 0.002674 | Recon Loss: 0.002307 | Commit Loss: 0.000733 | Perplexity: 3163.677427
2025-10-11 02:04:12,272 Stage: Train 0.5 | Epoch: 194 | Iter: 237400 | Total Loss: 0.002714 | Recon Loss: 0.002347 | Commit Loss: 0.000734 | Perplexity: 3164.579843
Trainning Epoch:  45%|████▍     | 222/494 [63:30:45<88:08:06, 1166.49s/it]Trainning Epoch:  45%|████▍     | 222/494 [63:30:45<88:08:06, 1166.49s/it]2025-10-11 02:08:05,948 Stage: Train 0.5 | Epoch: 195 | Iter: 237600 | Total Loss: 0.002652 | Recon Loss: 0.002278 | Commit Loss: 0.000747 | Perplexity: 3166.067201
2025-10-11 02:11:57,373 Stage: Train 0.5 | Epoch: 195 | Iter: 237800 | Total Loss: 0.002648 | Recon Loss: 0.002276 | Commit Loss: 0.000743 | Perplexity: 3168.090017
2025-10-11 02:15:49,502 Stage: Train 0.5 | Epoch: 195 | Iter: 238000 | Total Loss: 0.002661 | Recon Loss: 0.002288 | Commit Loss: 0.000745 | Perplexity: 3160.929467
2025-10-11 02:19:41,315 Stage: Train 0.5 | Epoch: 195 | Iter: 238200 | Total Loss: 0.002669 | Recon Loss: 0.002304 | Commit Loss: 0.000729 | Perplexity: 3172.461034
2025-10-11 02:23:33,363 Stage: Train 0.5 | Epoch: 195 | Iter: 238400 | Total Loss: 0.002707 | Recon Loss: 0.002334 | Commit Loss: 0.000746 | Perplexity: 3168.767372
Trainning Epoch:  45%|████▌     | 223/494 [63:50:22<88:02:34, 1169.57s/it]Trainning Epoch:  45%|████▌     | 223/494 [63:50:22<88:02:34, 1169.57s/it]2025-10-11 02:27:28,149 Stage: Train 0.5 | Epoch: 196 | Iter: 238600 | Total Loss: 0.002686 | Recon Loss: 0.002317 | Commit Loss: 0.000740 | Perplexity: 3159.280376
2025-10-11 02:31:18,398 Stage: Train 0.5 | Epoch: 196 | Iter: 238800 | Total Loss: 0.002683 | Recon Loss: 0.002316 | Commit Loss: 0.000735 | Perplexity: 3166.230316
2025-10-11 02:35:09,290 Stage: Train 0.5 | Epoch: 196 | Iter: 239000 | Total Loss: 0.002660 | Recon Loss: 0.002291 | Commit Loss: 0.000738 | Perplexity: 3170.922885
2025-10-11 02:39:00,442 Stage: Train 0.5 | Epoch: 196 | Iter: 239200 | Total Loss: 0.002679 | Recon Loss: 0.002307 | Commit Loss: 0.000744 | Perplexity: 3166.046346
2025-10-11 02:42:51,815 Stage: Train 0.5 | Epoch: 196 | Iter: 239400 | Total Loss: 0.002640 | Recon Loss: 0.002270 | Commit Loss: 0.000741 | Perplexity: 3173.046331
Trainning Epoch:  45%|████▌     | 224/494 [64:09:55<87:47:25, 1170.54s/it]Trainning Epoch:  45%|████▌     | 224/494 [64:09:55<87:47:25, 1170.54s/it]2025-10-11 02:46:44,544 Stage: Train 0.5 | Epoch: 197 | Iter: 239600 | Total Loss: 0.002676 | Recon Loss: 0.002306 | Commit Loss: 0.000739 | Perplexity: 3164.156592
2025-10-11 02:50:32,614 Stage: Train 0.5 | Epoch: 197 | Iter: 239800 | Total Loss: 0.002675 | Recon Loss: 0.002308 | Commit Loss: 0.000734 | Perplexity: 3165.983889
2025-10-11 02:54:22,280 Stage: Train 0.5 | Epoch: 197 | Iter: 240000 | Total Loss: 0.002666 | Recon Loss: 0.002296 | Commit Loss: 0.000741 | Perplexity: 3169.627501
2025-10-11 02:54:22,280 Saving model at iteration 240000
2025-10-11 02:54:22,457 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000
2025-10-11 02:54:23,738 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000/model.safetensors
2025-10-11 02:54:25,293 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000/optimizer.bin
2025-10-11 02:54:25,294 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000/scheduler.bin
2025-10-11 02:54:25,294 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000/sampler.bin
2025-10-11 02:54:25,295 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_198_step_240000/random_states_0.pkl
2025-10-11 02:58:14,141 Stage: Train 0.5 | Epoch: 197 | Iter: 240200 | Total Loss: 0.002687 | Recon Loss: 0.002317 | Commit Loss: 0.000740 | Perplexity: 3172.268706
2025-10-11 03:02:03,578 Stage: Train 0.5 | Epoch: 197 | Iter: 240400 | Total Loss: 0.002679 | Recon Loss: 0.002309 | Commit Loss: 0.000740 | Perplexity: 3176.218336
Trainning Epoch:  46%|████▌     | 225/494 [64:29:20<87:20:55, 1168.98s/it]Trainning Epoch:  46%|████▌     | 225/494 [64:29:20<87:20:55, 1168.98s/it]2025-10-11 03:05:55,354 Stage: Train 0.5 | Epoch: 198 | Iter: 240600 | Total Loss: 0.002669 | Recon Loss: 0.002301 | Commit Loss: 0.000735 | Perplexity: 3170.425883
2025-10-11 03:09:41,629 Stage: Train 0.5 | Epoch: 198 | Iter: 240800 | Total Loss: 0.002617 | Recon Loss: 0.002249 | Commit Loss: 0.000735 | Perplexity: 3168.154407
2025-10-11 03:13:28,310 Stage: Train 0.5 | Epoch: 198 | Iter: 241000 | Total Loss: 0.002703 | Recon Loss: 0.002336 | Commit Loss: 0.000734 | Perplexity: 3164.561608
2025-10-11 03:17:15,166 Stage: Train 0.5 | Epoch: 198 | Iter: 241200 | Total Loss: 0.002670 | Recon Loss: 0.002301 | Commit Loss: 0.000736 | Perplexity: 3171.510278
2025-10-11 03:21:02,645 Stage: Train 0.5 | Epoch: 198 | Iter: 241400 | Total Loss: 0.002676 | Recon Loss: 0.002302 | Commit Loss: 0.000749 | Perplexity: 3172.388702
Trainning Epoch:  46%|████▌     | 226/494 [64:48:32<86:38:08, 1163.76s/it]Trainning Epoch:  46%|████▌     | 226/494 [64:48:32<86:38:08, 1163.76s/it]2025-10-11 03:24:52,358 Stage: Train 0.5 | Epoch: 199 | Iter: 241600 | Total Loss: 0.002645 | Recon Loss: 0.002276 | Commit Loss: 0.000736 | Perplexity: 3175.962976
2025-10-11 03:28:41,079 Stage: Train 0.5 | Epoch: 199 | Iter: 241800 | Total Loss: 0.002655 | Recon Loss: 0.002289 | Commit Loss: 0.000732 | Perplexity: 3175.010988
2025-10-11 03:32:31,568 Stage: Train 0.5 | Epoch: 199 | Iter: 242000 | Total Loss: 0.002649 | Recon Loss: 0.002281 | Commit Loss: 0.000737 | Perplexity: 3175.650708
2025-10-11 03:36:22,397 Stage: Train 0.5 | Epoch: 199 | Iter: 242200 | Total Loss: 0.002667 | Recon Loss: 0.002298 | Commit Loss: 0.000737 | Perplexity: 3163.466068
2025-10-11 03:40:12,969 Stage: Train 0.5 | Epoch: 199 | Iter: 242400 | Total Loss: 0.002668 | Recon Loss: 0.002298 | Commit Loss: 0.000742 | Perplexity: 3169.483007
2025-10-11 03:44:03,692 Stage: Train 0.5 | Epoch: 199 | Iter: 242600 | Total Loss: 0.002672 | Recon Loss: 0.002306 | Commit Loss: 0.000732 | Perplexity: 3178.025873
Trainning Epoch:  46%|████▌     | 227/494 [65:08:01<86:26:00, 1165.39s/it]Trainning Epoch:  46%|████▌     | 227/494 [65:08:01<86:25:59, 1165.39s/it]2025-10-11 03:47:54,973 Stage: Train 0.5 | Epoch: 200 | Iter: 242800 | Total Loss: 0.002663 | Recon Loss: 0.002294 | Commit Loss: 0.000739 | Perplexity: 3160.739404
2025-10-11 03:51:44,342 Stage: Train 0.5 | Epoch: 200 | Iter: 243000 | Total Loss: 0.002609 | Recon Loss: 0.002244 | Commit Loss: 0.000731 | Perplexity: 3163.000173
2025-10-11 03:55:34,352 Stage: Train 0.5 | Epoch: 200 | Iter: 243200 | Total Loss: 0.002685 | Recon Loss: 0.002315 | Commit Loss: 0.000740 | Perplexity: 3161.800610
2025-10-11 03:59:23,662 Stage: Train 0.5 | Epoch: 200 | Iter: 243400 | Total Loss: 0.002654 | Recon Loss: 0.002285 | Commit Loss: 0.000736 | Perplexity: 3171.955311
2025-10-11 04:03:12,995 Stage: Train 0.5 | Epoch: 200 | Iter: 243600 | Total Loss: 0.002665 | Recon Loss: 0.002295 | Commit Loss: 0.000740 | Perplexity: 3176.052712
Trainning Epoch:  46%|████▌     | 228/494 [65:27:25<86:04:47, 1164.99s/it]Trainning Epoch:  46%|████▌     | 228/494 [65:27:25<86:04:47, 1164.99s/it]2025-10-11 04:07:04,841 Stage: Train 0.5 | Epoch: 201 | Iter: 243800 | Total Loss: 0.002707 | Recon Loss: 0.002341 | Commit Loss: 0.000733 | Perplexity: 3174.592587
2025-10-11 04:10:54,512 Stage: Train 0.5 | Epoch: 201 | Iter: 244000 | Total Loss: 0.002623 | Recon Loss: 0.002251 | Commit Loss: 0.000744 | Perplexity: 3173.665336
2025-10-11 04:14:44,347 Stage: Train 0.5 | Epoch: 201 | Iter: 244200 | Total Loss: 0.002663 | Recon Loss: 0.002295 | Commit Loss: 0.000735 | Perplexity: 3171.384746
2025-10-11 04:18:32,113 Stage: Train 0.5 | Epoch: 201 | Iter: 244400 | Total Loss: 0.002652 | Recon Loss: 0.002282 | Commit Loss: 0.000739 | Perplexity: 3168.891801
2025-10-11 04:22:18,738 Stage: Train 0.5 | Epoch: 201 | Iter: 244600 | Total Loss: 0.002660 | Recon Loss: 0.002297 | Commit Loss: 0.000726 | Perplexity: 3169.702384
Trainning Epoch:  46%|████▋     | 229/494 [65:46:45<85:39:24, 1163.64s/it]Trainning Epoch:  46%|████▋     | 229/494 [65:46:45<85:39:25, 1163.64s/it]2025-10-11 04:26:11,237 Stage: Train 0.5 | Epoch: 202 | Iter: 244800 | Total Loss: 0.002657 | Recon Loss: 0.002290 | Commit Loss: 0.000732 | Perplexity: 3183.664752
2025-10-11 04:30:01,619 Stage: Train 0.5 | Epoch: 202 | Iter: 245000 | Total Loss: 0.002618 | Recon Loss: 0.002249 | Commit Loss: 0.000738 | Perplexity: 3172.484248
2025-10-11 04:33:52,007 Stage: Train 0.5 | Epoch: 202 | Iter: 245200 | Total Loss: 0.002653 | Recon Loss: 0.002283 | Commit Loss: 0.000739 | Perplexity: 3177.703792
2025-10-11 04:37:42,082 Stage: Train 0.5 | Epoch: 202 | Iter: 245400 | Total Loss: 0.002640 | Recon Loss: 0.002271 | Commit Loss: 0.000738 | Perplexity: 3171.019963
2025-10-11 04:41:32,434 Stage: Train 0.5 | Epoch: 202 | Iter: 245600 | Total Loss: 0.002653 | Recon Loss: 0.002289 | Commit Loss: 0.000730 | Perplexity: 3162.103260
Trainning Epoch:  47%|████▋     | 230/494 [66:06:14<85:27:12, 1165.27s/it]Trainning Epoch:  47%|████▋     | 230/494 [66:06:14<85:27:12, 1165.27s/it]2025-10-11 04:45:26,126 Stage: Train 0.5 | Epoch: 203 | Iter: 245800 | Total Loss: 0.002653 | Recon Loss: 0.002282 | Commit Loss: 0.000742 | Perplexity: 3173.869716
2025-10-11 04:49:16,651 Stage: Train 0.5 | Epoch: 203 | Iter: 246000 | Total Loss: 0.002661 | Recon Loss: 0.002295 | Commit Loss: 0.000731 | Perplexity: 3162.134413
2025-10-11 04:53:07,709 Stage: Train 0.5 | Epoch: 203 | Iter: 246200 | Total Loss: 0.002639 | Recon Loss: 0.002276 | Commit Loss: 0.000725 | Perplexity: 3173.006421
2025-10-11 04:56:58,822 Stage: Train 0.5 | Epoch: 203 | Iter: 246400 | Total Loss: 0.002664 | Recon Loss: 0.002291 | Commit Loss: 0.000744 | Perplexity: 3164.503251
2025-10-11 05:00:50,042 Stage: Train 0.5 | Epoch: 203 | Iter: 246600 | Total Loss: 0.002641 | Recon Loss: 0.002277 | Commit Loss: 0.000728 | Perplexity: 3179.707501
Trainning Epoch:  47%|████▋     | 231/494 [66:25:47<85:16:54, 1167.36s/it]Trainning Epoch:  47%|████▋     | 231/494 [66:25:47<85:16:55, 1167.36s/it]2025-10-11 05:04:43,700 Stage: Train 0.5 | Epoch: 204 | Iter: 246800 | Total Loss: 0.002630 | Recon Loss: 0.002264 | Commit Loss: 0.000732 | Perplexity: 3177.638281
2025-10-11 05:08:34,267 Stage: Train 0.5 | Epoch: 204 | Iter: 247000 | Total Loss: 0.002642 | Recon Loss: 0.002274 | Commit Loss: 0.000736 | Perplexity: 3167.331104
2025-10-11 05:12:25,324 Stage: Train 0.5 | Epoch: 204 | Iter: 247200 | Total Loss: 0.002653 | Recon Loss: 0.002284 | Commit Loss: 0.000737 | Perplexity: 3157.379033
2025-10-11 05:16:16,478 Stage: Train 0.5 | Epoch: 204 | Iter: 247400 | Total Loss: 0.002663 | Recon Loss: 0.002293 | Commit Loss: 0.000740 | Perplexity: 3170.638497
2025-10-11 05:20:08,018 Stage: Train 0.5 | Epoch: 204 | Iter: 247600 | Total Loss: 0.002627 | Recon Loss: 0.002256 | Commit Loss: 0.000740 | Perplexity: 3174.164208
Trainning Epoch:  47%|████▋     | 232/494 [66:45:20<85:05:42, 1169.25s/it]Trainning Epoch:  47%|████▋     | 232/494 [66:45:20<85:05:42, 1169.25s/it]2025-10-11 05:24:01,898 Stage: Train 0.5 | Epoch: 205 | Iter: 247800 | Total Loss: 0.002632 | Recon Loss: 0.002265 | Commit Loss: 0.000734 | Perplexity: 3173.159967
2025-10-11 05:27:52,640 Stage: Train 0.5 | Epoch: 205 | Iter: 248000 | Total Loss: 0.002607 | Recon Loss: 0.002242 | Commit Loss: 0.000730 | Perplexity: 3178.632395
2025-10-11 05:31:43,093 Stage: Train 0.5 | Epoch: 205 | Iter: 248200 | Total Loss: 0.002676 | Recon Loss: 0.002307 | Commit Loss: 0.000739 | Perplexity: 3175.073192
2025-10-11 05:35:33,737 Stage: Train 0.5 | Epoch: 205 | Iter: 248400 | Total Loss: 0.002617 | Recon Loss: 0.002249 | Commit Loss: 0.000734 | Perplexity: 3169.410076
2025-10-11 05:39:24,538 Stage: Train 0.5 | Epoch: 205 | Iter: 248600 | Total Loss: 0.002683 | Recon Loss: 0.002326 | Commit Loss: 0.000715 | Perplexity: 3175.297123
Trainning Epoch:  47%|████▋     | 233/494 [67:04:51<84:48:49, 1169.85s/it]Trainning Epoch:  47%|████▋     | 233/494 [67:04:51<84:48:49, 1169.85s/it]2025-10-11 05:43:17,139 Stage: Train 0.5 | Epoch: 206 | Iter: 248800 | Total Loss: 0.002683 | Recon Loss: 0.002314 | Commit Loss: 0.000737 | Perplexity: 3163.998379
2025-10-11 05:47:05,299 Stage: Train 0.5 | Epoch: 206 | Iter: 249000 | Total Loss: 0.002606 | Recon Loss: 0.002239 | Commit Loss: 0.000733 | Perplexity: 3177.417086
2025-10-11 05:50:54,488 Stage: Train 0.5 | Epoch: 206 | Iter: 249200 | Total Loss: 0.002629 | Recon Loss: 0.002260 | Commit Loss: 0.000737 | Perplexity: 3182.645247
2025-10-11 05:54:44,151 Stage: Train 0.5 | Epoch: 206 | Iter: 249400 | Total Loss: 0.002648 | Recon Loss: 0.002282 | Commit Loss: 0.000733 | Perplexity: 3172.183080
2025-10-11 05:58:34,246 Stage: Train 0.5 | Epoch: 206 | Iter: 249600 | Total Loss: 0.002637 | Recon Loss: 0.002267 | Commit Loss: 0.000740 | Perplexity: 3163.125084
Trainning Epoch:  47%|████▋     | 234/494 [67:24:16<84:22:27, 1168.26s/it]Trainning Epoch:  47%|████▋     | 234/494 [67:24:16<84:22:27, 1168.26s/it]2025-10-11 06:02:27,534 Stage: Train 0.5 | Epoch: 207 | Iter: 249800 | Total Loss: 0.002615 | Recon Loss: 0.002249 | Commit Loss: 0.000732 | Perplexity: 3171.830133
2025-10-11 06:06:17,754 Stage: Train 0.5 | Epoch: 207 | Iter: 250000 | Total Loss: 0.002628 | Recon Loss: 0.002263 | Commit Loss: 0.000729 | Perplexity: 3170.726444
2025-10-11 06:10:08,790 Stage: Train 0.5 | Epoch: 207 | Iter: 250200 | Total Loss: 0.002636 | Recon Loss: 0.002270 | Commit Loss: 0.000732 | Perplexity: 3175.497075
2025-10-11 06:13:59,792 Stage: Train 0.5 | Epoch: 207 | Iter: 250400 | Total Loss: 0.002620 | Recon Loss: 0.002255 | Commit Loss: 0.000728 | Perplexity: 3169.168075
2025-10-11 06:17:50,862 Stage: Train 0.5 | Epoch: 207 | Iter: 250600 | Total Loss: 0.002670 | Recon Loss: 0.002295 | Commit Loss: 0.000751 | Perplexity: 3156.299200
Trainning Epoch:  48%|████▊     | 235/494 [67:43:48<84:08:02, 1169.43s/it]Trainning Epoch:  48%|████▊     | 235/494 [67:43:48<84:08:02, 1169.43s/it]2025-10-11 06:21:46,243 Stage: Train 0.5 | Epoch: 208 | Iter: 250800 | Total Loss: 0.002644 | Recon Loss: 0.002280 | Commit Loss: 0.000728 | Perplexity: 3179.184009
2025-10-11 06:25:37,196 Stage: Train 0.5 | Epoch: 208 | Iter: 251000 | Total Loss: 0.002617 | Recon Loss: 0.002251 | Commit Loss: 0.000732 | Perplexity: 3175.340541
2025-10-11 06:29:26,940 Stage: Train 0.5 | Epoch: 208 | Iter: 251200 | Total Loss: 0.002629 | Recon Loss: 0.002258 | Commit Loss: 0.000741 | Perplexity: 3175.575519
2025-10-11 06:33:14,238 Stage: Train 0.5 | Epoch: 208 | Iter: 251400 | Total Loss: 0.002643 | Recon Loss: 0.002271 | Commit Loss: 0.000743 | Perplexity: 3170.289332
2025-10-11 06:37:00,468 Stage: Train 0.5 | Epoch: 208 | Iter: 251600 | Total Loss: 0.002633 | Recon Loss: 0.002269 | Commit Loss: 0.000729 | Perplexity: 3169.706128
Trainning Epoch:  48%|████▊     | 236/494 [68:03:12<83:40:42, 1167.61s/it]Trainning Epoch:  48%|████▊     | 236/494 [68:03:12<83:40:42, 1167.61s/it]2025-10-11 06:40:53,958 Stage: Train 0.5 | Epoch: 209 | Iter: 251800 | Total Loss: 0.002647 | Recon Loss: 0.002282 | Commit Loss: 0.000730 | Perplexity: 3161.853394
2025-10-11 06:44:44,622 Stage: Train 0.5 | Epoch: 209 | Iter: 252000 | Total Loss: 0.002610 | Recon Loss: 0.002240 | Commit Loss: 0.000741 | Perplexity: 3174.795601
2025-10-11 06:48:35,686 Stage: Train 0.5 | Epoch: 209 | Iter: 252200 | Total Loss: 0.002627 | Recon Loss: 0.002261 | Commit Loss: 0.000732 | Perplexity: 3176.798208
2025-10-11 06:52:26,621 Stage: Train 0.5 | Epoch: 209 | Iter: 252400 | Total Loss: 0.002627 | Recon Loss: 0.002257 | Commit Loss: 0.000740 | Perplexity: 3184.821907
2025-10-11 06:56:17,944 Stage: Train 0.5 | Epoch: 209 | Iter: 252600 | Total Loss: 0.002648 | Recon Loss: 0.002280 | Commit Loss: 0.000737 | Perplexity: 3160.403295
Trainning Epoch:  48%|████▊     | 237/494 [68:22:45<83:28:58, 1169.41s/it]Trainning Epoch:  48%|████▊     | 237/494 [68:22:45<83:28:58, 1169.41s/it]2025-10-11 07:00:12,021 Stage: Train 0.5 | Epoch: 210 | Iter: 252800 | Total Loss: 0.002613 | Recon Loss: 0.002251 | Commit Loss: 0.000724 | Perplexity: 3175.685439
2025-10-11 07:04:03,110 Stage: Train 0.5 | Epoch: 210 | Iter: 253000 | Total Loss: 0.002603 | Recon Loss: 0.002238 | Commit Loss: 0.000730 | Perplexity: 3173.257776
2025-10-11 07:07:54,159 Stage: Train 0.5 | Epoch: 210 | Iter: 253200 | Total Loss: 0.002608 | Recon Loss: 0.002241 | Commit Loss: 0.000734 | Perplexity: 3160.564955
2025-10-11 07:11:44,890 Stage: Train 0.5 | Epoch: 210 | Iter: 253400 | Total Loss: 0.002607 | Recon Loss: 0.002242 | Commit Loss: 0.000730 | Perplexity: 3172.979762
2025-10-11 07:15:35,828 Stage: Train 0.5 | Epoch: 210 | Iter: 253600 | Total Loss: 0.002586 | Recon Loss: 0.002219 | Commit Loss: 0.000733 | Perplexity: 3169.634771
Trainning Epoch:  48%|████▊     | 238/494 [68:42:18<83:13:38, 1170.39s/it]Trainning Epoch:  48%|████▊     | 238/494 [68:42:18<83:13:39, 1170.39s/it]2025-10-11 07:19:29,551 Stage: Train 0.5 | Epoch: 211 | Iter: 253800 | Total Loss: 0.002625 | Recon Loss: 0.002257 | Commit Loss: 0.000736 | Perplexity: 3179.016926
2025-10-11 07:23:20,997 Stage: Train 0.5 | Epoch: 211 | Iter: 254000 | Total Loss: 0.002620 | Recon Loss: 0.002251 | Commit Loss: 0.000738 | Perplexity: 3176.093016
2025-10-11 07:27:12,862 Stage: Train 0.5 | Epoch: 211 | Iter: 254200 | Total Loss: 0.002649 | Recon Loss: 0.002282 | Commit Loss: 0.000734 | Perplexity: 3177.315027
2025-10-11 07:31:05,520 Stage: Train 0.5 | Epoch: 211 | Iter: 254400 | Total Loss: 0.002624 | Recon Loss: 0.002257 | Commit Loss: 0.000734 | Perplexity: 3175.019338
2025-10-11 07:34:58,448 Stage: Train 0.5 | Epoch: 211 | Iter: 254600 | Total Loss: 0.002638 | Recon Loss: 0.002272 | Commit Loss: 0.000731 | Perplexity: 3182.985815
Trainning Epoch:  48%|████▊     | 239/494 [69:01:57<83:05:18, 1173.01s/it]Trainning Epoch:  48%|████▊     | 239/494 [69:01:57<83:05:19, 1173.02s/it]2025-10-11 07:38:54,470 Stage: Train 0.5 | Epoch: 212 | Iter: 254800 | Total Loss: 0.002584 | Recon Loss: 0.002216 | Commit Loss: 0.000737 | Perplexity: 3165.469553
2025-10-11 07:42:45,517 Stage: Train 0.5 | Epoch: 212 | Iter: 255000 | Total Loss: 0.002599 | Recon Loss: 0.002237 | Commit Loss: 0.000725 | Perplexity: 3168.360089
2025-10-11 07:46:36,859 Stage: Train 0.5 | Epoch: 212 | Iter: 255200 | Total Loss: 0.002642 | Recon Loss: 0.002274 | Commit Loss: 0.000735 | Perplexity: 3166.863988
2025-10-11 07:50:28,570 Stage: Train 0.5 | Epoch: 212 | Iter: 255400 | Total Loss: 0.002604 | Recon Loss: 0.002238 | Commit Loss: 0.000731 | Perplexity: 3180.530205
2025-10-11 07:54:20,130 Stage: Train 0.5 | Epoch: 212 | Iter: 255600 | Total Loss: 0.002610 | Recon Loss: 0.002241 | Commit Loss: 0.000738 | Perplexity: 3180.550653
Trainning Epoch:  49%|████▊     | 240/494 [69:21:33<82:49:30, 1173.90s/it]Trainning Epoch:  49%|████▊     | 240/494 [69:21:33<82:49:30, 1173.90s/it]2025-10-11 07:58:14,760 Stage: Train 0.5 | Epoch: 213 | Iter: 255800 | Total Loss: 0.002621 | Recon Loss: 0.002247 | Commit Loss: 0.000747 | Perplexity: 3183.988827
2025-10-11 08:02:03,228 Stage: Train 0.5 | Epoch: 213 | Iter: 256000 | Total Loss: 0.002587 | Recon Loss: 0.002215 | Commit Loss: 0.000745 | Perplexity: 3180.731549
2025-10-11 08:05:52,287 Stage: Train 0.5 | Epoch: 213 | Iter: 256200 | Total Loss: 0.002629 | Recon Loss: 0.002261 | Commit Loss: 0.000736 | Perplexity: 3168.463615
2025-10-11 08:09:41,718 Stage: Train 0.5 | Epoch: 213 | Iter: 256400 | Total Loss: 0.002661 | Recon Loss: 0.002292 | Commit Loss: 0.000738 | Perplexity: 3167.452369
2025-10-11 08:13:31,199 Stage: Train 0.5 | Epoch: 213 | Iter: 256600 | Total Loss: 0.002589 | Recon Loss: 0.002224 | Commit Loss: 0.000729 | Perplexity: 3181.343959
Trainning Epoch:  49%|████▉     | 241/494 [69:40:57<82:16:55, 1170.81s/it]Trainning Epoch:  49%|████▉     | 241/494 [69:40:57<82:16:56, 1170.82s/it]2025-10-11 08:17:23,617 Stage: Train 0.5 | Epoch: 214 | Iter: 256800 | Total Loss: 0.002623 | Recon Loss: 0.002258 | Commit Loss: 0.000731 | Perplexity: 3174.807906
2025-10-11 08:21:11,965 Stage: Train 0.5 | Epoch: 214 | Iter: 257000 | Total Loss: 0.002652 | Recon Loss: 0.002277 | Commit Loss: 0.000750 | Perplexity: 3188.286870
2025-10-11 08:25:00,674 Stage: Train 0.5 | Epoch: 214 | Iter: 257200 | Total Loss: 0.002606 | Recon Loss: 0.002244 | Commit Loss: 0.000725 | Perplexity: 3174.987410
2025-10-11 08:28:49,518 Stage: Train 0.5 | Epoch: 214 | Iter: 257400 | Total Loss: 0.002592 | Recon Loss: 0.002224 | Commit Loss: 0.000736 | Perplexity: 3173.468290
2025-10-11 08:32:38,440 Stage: Train 0.5 | Epoch: 214 | Iter: 257600 | Total Loss: 0.002644 | Recon Loss: 0.002268 | Commit Loss: 0.000751 | Perplexity: 3188.648281
Trainning Epoch:  49%|████▉     | 242/494 [70:00:18<81:45:33, 1167.99s/it]Trainning Epoch:  49%|████▉     | 242/494 [70:00:18<81:45:33, 1167.99s/it]2025-10-11 08:36:29,525 Stage: Train 0.5 | Epoch: 215 | Iter: 257800 | Total Loss: 0.002568 | Recon Loss: 0.002203 | Commit Loss: 0.000728 | Perplexity: 3172.529060
2025-10-11 08:40:17,464 Stage: Train 0.5 | Epoch: 215 | Iter: 258000 | Total Loss: 0.002636 | Recon Loss: 0.002272 | Commit Loss: 0.000728 | Perplexity: 3176.345460
2025-10-11 08:44:06,306 Stage: Train 0.5 | Epoch: 215 | Iter: 258200 | Total Loss: 0.002595 | Recon Loss: 0.002231 | Commit Loss: 0.000729 | Perplexity: 3164.608307
2025-10-11 08:47:54,808 Stage: Train 0.5 | Epoch: 215 | Iter: 258400 | Total Loss: 0.002655 | Recon Loss: 0.002288 | Commit Loss: 0.000734 | Perplexity: 3168.548022
2025-10-11 08:51:43,873 Stage: Train 0.5 | Epoch: 215 | Iter: 258600 | Total Loss: 0.002608 | Recon Loss: 0.002237 | Commit Loss: 0.000742 | Perplexity: 3181.576908
2025-10-11 08:55:33,210 Stage: Train 0.5 | Epoch: 215 | Iter: 258800 | Total Loss: 0.002601 | Recon Loss: 0.002235 | Commit Loss: 0.000730 | Perplexity: 3166.436578
Trainning Epoch:  49%|████▉     | 243/494 [70:19:39<81:17:54, 1166.03s/it]Trainning Epoch:  49%|████▉     | 243/494 [70:19:39<81:17:55, 1166.04s/it]2025-10-11 08:59:26,877 Stage: Train 0.5 | Epoch: 216 | Iter: 259000 | Total Loss: 0.002582 | Recon Loss: 0.002216 | Commit Loss: 0.000731 | Perplexity: 3177.446646
2025-10-11 09:03:16,952 Stage: Train 0.5 | Epoch: 216 | Iter: 259200 | Total Loss: 0.002631 | Recon Loss: 0.002266 | Commit Loss: 0.000729 | Perplexity: 3172.572053
2025-10-11 09:07:07,355 Stage: Train 0.5 | Epoch: 216 | Iter: 259400 | Total Loss: 0.002595 | Recon Loss: 0.002226 | Commit Loss: 0.000738 | Perplexity: 3166.416449
2025-10-11 09:10:57,993 Stage: Train 0.5 | Epoch: 216 | Iter: 259600 | Total Loss: 0.002644 | Recon Loss: 0.002279 | Commit Loss: 0.000731 | Perplexity: 3173.209246
2025-10-11 09:14:48,403 Stage: Train 0.5 | Epoch: 216 | Iter: 259800 | Total Loss: 0.002595 | Recon Loss: 0.002231 | Commit Loss: 0.000727 | Perplexity: 3173.706743
Trainning Epoch:  49%|████▉     | 244/494 [70:39:10<81:03:42, 1167.29s/it]Trainning Epoch:  49%|████▉     | 244/494 [70:39:10<81:03:42, 1167.29s/it]2025-10-11 09:18:41,811 Stage: Train 0.5 | Epoch: 217 | Iter: 260000 | Total Loss: 0.002586 | Recon Loss: 0.002220 | Commit Loss: 0.000732 | Perplexity: 3181.403342
2025-10-11 09:18:41,811 Saving model at iteration 260000
2025-10-11 09:18:42,012 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000
2025-10-11 09:18:43,516 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000/model.safetensors
2025-10-11 09:18:45,164 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000/optimizer.bin
2025-10-11 09:18:45,164 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000/scheduler.bin
2025-10-11 09:18:45,165 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000/sampler.bin
2025-10-11 09:18:45,165 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_218_step_260000/random_states_0.pkl
2025-10-11 09:22:35,375 Stage: Train 0.5 | Epoch: 217 | Iter: 260200 | Total Loss: 0.002600 | Recon Loss: 0.002233 | Commit Loss: 0.000734 | Perplexity: 3185.187706
2025-10-11 09:26:25,534 Stage: Train 0.5 | Epoch: 217 | Iter: 260400 | Total Loss: 0.002614 | Recon Loss: 0.002248 | Commit Loss: 0.000731 | Perplexity: 3168.453098
2025-10-11 09:30:15,643 Stage: Train 0.5 | Epoch: 217 | Iter: 260600 | Total Loss: 0.002637 | Recon Loss: 0.002272 | Commit Loss: 0.000731 | Perplexity: 3172.523212
2025-10-11 09:34:06,181 Stage: Train 0.5 | Epoch: 217 | Iter: 260800 | Total Loss: 0.002616 | Recon Loss: 0.002246 | Commit Loss: 0.000741 | Perplexity: 3175.061057
Trainning Epoch:  50%|████▉     | 245/494 [70:58:43<80:51:11, 1168.96s/it]Trainning Epoch:  50%|████▉     | 245/494 [70:58:43<80:51:11, 1168.96s/it]2025-10-11 09:37:59,958 Stage: Train 0.5 | Epoch: 218 | Iter: 261000 | Total Loss: 0.002575 | Recon Loss: 0.002213 | Commit Loss: 0.000724 | Perplexity: 3176.670156
2025-10-11 09:41:50,394 Stage: Train 0.5 | Epoch: 218 | Iter: 261200 | Total Loss: 0.002610 | Recon Loss: 0.002242 | Commit Loss: 0.000736 | Perplexity: 3173.991154
2025-10-11 09:45:40,984 Stage: Train 0.5 | Epoch: 218 | Iter: 261400 | Total Loss: 0.002599 | Recon Loss: 0.002231 | Commit Loss: 0.000737 | Perplexity: 3169.562935
2025-10-11 09:49:31,650 Stage: Train 0.5 | Epoch: 218 | Iter: 261600 | Total Loss: 0.002671 | Recon Loss: 0.002302 | Commit Loss: 0.000738 | Perplexity: 3171.392196
2025-10-11 09:53:22,220 Stage: Train 0.5 | Epoch: 218 | Iter: 261800 | Total Loss: 0.002579 | Recon Loss: 0.002214 | Commit Loss: 0.000730 | Perplexity: 3182.836415
Trainning Epoch:  50%|████▉     | 246/494 [71:18:13<80:34:09, 1169.55s/it]Trainning Epoch:  50%|████▉     | 246/494 [71:18:13<80:34:09, 1169.56s/it]2025-10-11 09:57:17,128 Stage: Train 0.5 | Epoch: 219 | Iter: 262000 | Total Loss: 0.002571 | Recon Loss: 0.002206 | Commit Loss: 0.000729 | Perplexity: 3189.788218
2025-10-11 10:01:08,451 Stage: Train 0.5 | Epoch: 219 | Iter: 262200 | Total Loss: 0.002594 | Recon Loss: 0.002228 | Commit Loss: 0.000733 | Perplexity: 3177.655442
2025-10-11 10:04:59,678 Stage: Train 0.5 | Epoch: 219 | Iter: 262400 | Total Loss: 0.002578 | Recon Loss: 0.002213 | Commit Loss: 0.000730 | Perplexity: 3175.508240
2025-10-11 10:08:51,635 Stage: Train 0.5 | Epoch: 219 | Iter: 262600 | Total Loss: 0.002618 | Recon Loss: 0.002250 | Commit Loss: 0.000736 | Perplexity: 3188.677864
2025-10-11 10:12:42,868 Stage: Train 0.5 | Epoch: 219 | Iter: 262800 | Total Loss: 0.002627 | Recon Loss: 0.002258 | Commit Loss: 0.000738 | Perplexity: 3186.459834
Trainning Epoch:  50%|█████     | 247/494 [71:37:50<80:22:44, 1171.52s/it]Trainning Epoch:  50%|█████     | 247/494 [71:37:50<80:22:44, 1171.52s/it]2025-10-11 10:16:37,264 Stage: Train 0.5 | Epoch: 220 | Iter: 263000 | Total Loss: 0.002597 | Recon Loss: 0.002231 | Commit Loss: 0.000733 | Perplexity: 3179.935042
2025-10-11 10:20:29,233 Stage: Train 0.5 | Epoch: 220 | Iter: 263200 | Total Loss: 0.002580 | Recon Loss: 0.002217 | Commit Loss: 0.000726 | Perplexity: 3187.604740
2025-10-11 10:24:21,005 Stage: Train 0.5 | Epoch: 220 | Iter: 263400 | Total Loss: 0.002629 | Recon Loss: 0.002265 | Commit Loss: 0.000729 | Perplexity: 3170.395680
2025-10-11 10:28:11,481 Stage: Train 0.5 | Epoch: 220 | Iter: 263600 | Total Loss: 0.002630 | Recon Loss: 0.002260 | Commit Loss: 0.000740 | Perplexity: 3179.727545
2025-10-11 10:32:01,823 Stage: Train 0.5 | Epoch: 220 | Iter: 263800 | Total Loss: 0.002593 | Recon Loss: 0.002233 | Commit Loss: 0.000720 | Perplexity: 3177.875782
Trainning Epoch:  50%|█████     | 248/494 [71:57:23<80:05:21, 1172.04s/it]Trainning Epoch:  50%|█████     | 248/494 [71:57:23<80:05:21, 1172.04s/it]2025-10-11 10:35:55,501 Stage: Train 0.5 | Epoch: 221 | Iter: 264000 | Total Loss: 0.002582 | Recon Loss: 0.002213 | Commit Loss: 0.000737 | Perplexity: 3171.927283
2025-10-11 10:39:44,847 Stage: Train 0.5 | Epoch: 221 | Iter: 264200 | Total Loss: 0.002594 | Recon Loss: 0.002222 | Commit Loss: 0.000744 | Perplexity: 3185.116738
2025-10-11 10:43:34,554 Stage: Train 0.5 | Epoch: 221 | Iter: 264400 | Total Loss: 0.002579 | Recon Loss: 0.002212 | Commit Loss: 0.000734 | Perplexity: 3190.359142
2025-10-11 10:47:24,023 Stage: Train 0.5 | Epoch: 221 | Iter: 264600 | Total Loss: 0.002646 | Recon Loss: 0.002284 | Commit Loss: 0.000724 | Perplexity: 3169.889081
2025-10-11 10:51:13,738 Stage: Train 0.5 | Epoch: 221 | Iter: 264800 | Total Loss: 0.002609 | Recon Loss: 0.002241 | Commit Loss: 0.000736 | Perplexity: 3184.978561
Trainning Epoch:  50%|█████     | 249/494 [72:16:49<79:39:07, 1170.40s/it]Trainning Epoch:  50%|█████     | 249/494 [72:16:49<79:39:08, 1170.40s/it]2025-10-11 10:55:04,362 Stage: Train 0.5 | Epoch: 222 | Iter: 265000 | Total Loss: 0.002561 | Recon Loss: 0.002192 | Commit Loss: 0.000738 | Perplexity: 3180.882460
2025-10-11 10:58:53,513 Stage: Train 0.5 | Epoch: 222 | Iter: 265200 | Total Loss: 0.002597 | Recon Loss: 0.002234 | Commit Loss: 0.000728 | Perplexity: 3177.411516
2025-10-11 11:02:43,815 Stage: Train 0.5 | Epoch: 222 | Iter: 265400 | Total Loss: 0.002581 | Recon Loss: 0.002208 | Commit Loss: 0.000747 | Perplexity: 3176.321486
2025-10-11 11:06:32,469 Stage: Train 0.5 | Epoch: 222 | Iter: 265600 | Total Loss: 0.002570 | Recon Loss: 0.002204 | Commit Loss: 0.000733 | Perplexity: 3190.195159
2025-10-11 11:10:21,145 Stage: Train 0.5 | Epoch: 222 | Iter: 265800 | Total Loss: 0.002600 | Recon Loss: 0.002233 | Commit Loss: 0.000734 | Perplexity: 3177.476117
Trainning Epoch:  51%|█████     | 250/494 [72:36:11<79:09:28, 1167.90s/it]Trainning Epoch:  51%|█████     | 250/494 [72:36:11<79:09:28, 1167.90s/it]2025-10-11 11:14:12,024 Stage: Train 0.5 | Epoch: 223 | Iter: 266000 | Total Loss: 0.002579 | Recon Loss: 0.002212 | Commit Loss: 0.000735 | Perplexity: 3183.917378
2025-10-11 11:18:00,986 Stage: Train 0.5 | Epoch: 223 | Iter: 266200 | Total Loss: 0.002562 | Recon Loss: 0.002197 | Commit Loss: 0.000729 | Perplexity: 3183.129545
2025-10-11 11:21:50,029 Stage: Train 0.5 | Epoch: 223 | Iter: 266400 | Total Loss: 0.002595 | Recon Loss: 0.002231 | Commit Loss: 0.000727 | Perplexity: 3187.075022
2025-10-11 11:25:38,015 Stage: Train 0.5 | Epoch: 223 | Iter: 266600 | Total Loss: 0.002569 | Recon Loss: 0.002201 | Commit Loss: 0.000735 | Perplexity: 3180.611625
2025-10-11 11:29:25,482 Stage: Train 0.5 | Epoch: 223 | Iter: 266800 | Total Loss: 0.002612 | Recon Loss: 0.002245 | Commit Loss: 0.000734 | Perplexity: 3188.777456
Trainning Epoch:  51%|█████     | 251/494 [72:55:30<78:38:00, 1164.94s/it]Trainning Epoch:  51%|█████     | 251/494 [72:55:30<78:38:01, 1164.94s/it]2025-10-11 11:33:16,486 Stage: Train 0.5 | Epoch: 224 | Iter: 267000 | Total Loss: 0.002581 | Recon Loss: 0.002212 | Commit Loss: 0.000739 | Perplexity: 3181.662983
2025-10-11 11:37:05,451 Stage: Train 0.5 | Epoch: 224 | Iter: 267200 | Total Loss: 0.002578 | Recon Loss: 0.002208 | Commit Loss: 0.000739 | Perplexity: 3182.679301
2025-10-11 11:40:54,428 Stage: Train 0.5 | Epoch: 224 | Iter: 267400 | Total Loss: 0.002579 | Recon Loss: 0.002213 | Commit Loss: 0.000732 | Perplexity: 3176.734346
2025-10-11 11:44:43,333 Stage: Train 0.5 | Epoch: 224 | Iter: 267600 | Total Loss: 0.002563 | Recon Loss: 0.002195 | Commit Loss: 0.000735 | Perplexity: 3170.993610
2025-10-11 11:48:32,703 Stage: Train 0.5 | Epoch: 224 | Iter: 267800 | Total Loss: 0.002634 | Recon Loss: 0.002262 | Commit Loss: 0.000746 | Perplexity: 3174.164299
Trainning Epoch:  51%|█████     | 252/494 [73:14:53<78:16:36, 1164.45s/it]Trainning Epoch:  51%|█████     | 252/494 [73:14:53<78:16:37, 1164.45s/it]2025-10-11 11:52:24,339 Stage: Train 0.5 | Epoch: 225 | Iter: 268000 | Total Loss: 0.002583 | Recon Loss: 0.002213 | Commit Loss: 0.000740 | Perplexity: 3192.130042
2025-10-11 11:56:11,609 Stage: Train 0.5 | Epoch: 225 | Iter: 268200 | Total Loss: 0.002581 | Recon Loss: 0.002215 | Commit Loss: 0.000732 | Perplexity: 3184.277478
2025-10-11 11:59:59,073 Stage: Train 0.5 | Epoch: 225 | Iter: 268400 | Total Loss: 0.002568 | Recon Loss: 0.002207 | Commit Loss: 0.000723 | Perplexity: 3172.022979
2025-10-11 12:03:46,621 Stage: Train 0.5 | Epoch: 225 | Iter: 268600 | Total Loss: 0.002545 | Recon Loss: 0.002180 | Commit Loss: 0.000730 | Perplexity: 3178.187843
2025-10-11 12:07:34,623 Stage: Train 0.5 | Epoch: 225 | Iter: 268800 | Total Loss: 0.002572 | Recon Loss: 0.002203 | Commit Loss: 0.000738 | Perplexity: 3187.354296
Trainning Epoch:  51%|█████     | 253/494 [73:34:09<77:47:27, 1162.02s/it]Trainning Epoch:  51%|█████     | 253/494 [73:34:09<77:47:27, 1162.02s/it]2025-10-11 12:11:25,790 Stage: Train 0.5 | Epoch: 226 | Iter: 269000 | Total Loss: 0.002623 | Recon Loss: 0.002259 | Commit Loss: 0.000728 | Perplexity: 3183.144750
2025-10-11 12:15:14,263 Stage: Train 0.5 | Epoch: 226 | Iter: 269200 | Total Loss: 0.002553 | Recon Loss: 0.002188 | Commit Loss: 0.000730 | Perplexity: 3188.698661
2025-10-11 12:19:02,805 Stage: Train 0.5 | Epoch: 226 | Iter: 269400 | Total Loss: 0.002561 | Recon Loss: 0.002193 | Commit Loss: 0.000738 | Perplexity: 3190.240701
2025-10-11 12:22:51,699 Stage: Train 0.5 | Epoch: 226 | Iter: 269600 | Total Loss: 0.002588 | Recon Loss: 0.002220 | Commit Loss: 0.000736 | Perplexity: 3187.478239
2025-10-11 12:26:40,509 Stage: Train 0.5 | Epoch: 226 | Iter: 269800 | Total Loss: 0.002566 | Recon Loss: 0.002199 | Commit Loss: 0.000735 | Perplexity: 3167.569784
Trainning Epoch:  51%|█████▏    | 254/494 [73:53:31<77:27:35, 1161.90s/it]Trainning Epoch:  51%|█████▏    | 254/494 [73:53:31<77:27:35, 1161.90s/it]2025-10-11 12:30:33,144 Stage: Train 0.5 | Epoch: 227 | Iter: 270000 | Total Loss: 0.002592 | Recon Loss: 0.002228 | Commit Loss: 0.000728 | Perplexity: 3179.917728
2025-10-11 12:34:21,994 Stage: Train 0.5 | Epoch: 227 | Iter: 270200 | Total Loss: 0.002562 | Recon Loss: 0.002193 | Commit Loss: 0.000738 | Perplexity: 3176.918086
2025-10-11 12:38:11,311 Stage: Train 0.5 | Epoch: 227 | Iter: 270400 | Total Loss: 0.002588 | Recon Loss: 0.002219 | Commit Loss: 0.000738 | Perplexity: 3186.845551
2025-10-11 12:42:00,357 Stage: Train 0.5 | Epoch: 227 | Iter: 270600 | Total Loss: 0.002615 | Recon Loss: 0.002249 | Commit Loss: 0.000732 | Perplexity: 3180.410554
2025-10-11 12:45:49,705 Stage: Train 0.5 | Epoch: 227 | Iter: 270800 | Total Loss: 0.002610 | Recon Loss: 0.002246 | Commit Loss: 0.000728 | Perplexity: 3185.894160
Trainning Epoch:  52%|█████▏    | 255/494 [74:12:55<77:10:43, 1162.53s/it]Trainning Epoch:  52%|█████▏    | 255/494 [74:12:55<77:10:43, 1162.53s/it]2025-10-11 12:49:42,361 Stage: Train 0.5 | Epoch: 228 | Iter: 271000 | Total Loss: 0.002570 | Recon Loss: 0.002202 | Commit Loss: 0.000735 | Perplexity: 3177.635640
2025-10-11 12:53:30,869 Stage: Train 0.5 | Epoch: 228 | Iter: 271200 | Total Loss: 0.002563 | Recon Loss: 0.002195 | Commit Loss: 0.000736 | Perplexity: 3196.471207
2025-10-11 12:57:20,131 Stage: Train 0.5 | Epoch: 228 | Iter: 271400 | Total Loss: 0.002561 | Recon Loss: 0.002196 | Commit Loss: 0.000730 | Perplexity: 3189.532849
2025-10-11 13:01:09,212 Stage: Train 0.5 | Epoch: 228 | Iter: 271600 | Total Loss: 0.002580 | Recon Loss: 0.002214 | Commit Loss: 0.000733 | Perplexity: 3196.463776
2025-10-11 13:04:58,282 Stage: Train 0.5 | Epoch: 228 | Iter: 271800 | Total Loss: 0.002566 | Recon Loss: 0.002199 | Commit Loss: 0.000734 | Perplexity: 3182.652825
Trainning Epoch:  52%|█████▏    | 256/494 [74:32:20<76:54:44, 1163.38s/it]Trainning Epoch:  52%|█████▏    | 256/494 [74:32:20<76:54:45, 1163.38s/it]2025-10-11 13:08:53,201 Stage: Train 0.5 | Epoch: 229 | Iter: 272000 | Total Loss: 0.002553 | Recon Loss: 0.002187 | Commit Loss: 0.000731 | Perplexity: 3185.066650
2025-10-11 13:12:45,949 Stage: Train 0.5 | Epoch: 229 | Iter: 272200 | Total Loss: 0.002568 | Recon Loss: 0.002206 | Commit Loss: 0.000724 | Perplexity: 3190.540697
2025-10-11 13:16:38,454 Stage: Train 0.5 | Epoch: 229 | Iter: 272400 | Total Loss: 0.002548 | Recon Loss: 0.002183 | Commit Loss: 0.000731 | Perplexity: 3189.082648
2025-10-11 13:20:30,440 Stage: Train 0.5 | Epoch: 229 | Iter: 272600 | Total Loss: 0.002588 | Recon Loss: 0.002214 | Commit Loss: 0.000747 | Perplexity: 3182.515747
2025-10-11 13:24:22,100 Stage: Train 0.5 | Epoch: 229 | Iter: 272800 | Total Loss: 0.002582 | Recon Loss: 0.002212 | Commit Loss: 0.000740 | Perplexity: 3183.495598
Trainning Epoch:  52%|█████▏    | 257/494 [74:51:59<76:53:46, 1168.04s/it]Trainning Epoch:  52%|█████▏    | 257/494 [74:51:59<76:53:46, 1168.04s/it]2025-10-11 13:28:17,121 Stage: Train 0.5 | Epoch: 230 | Iter: 273000 | Total Loss: 0.002570 | Recon Loss: 0.002205 | Commit Loss: 0.000731 | Perplexity: 3183.920780
2025-10-11 13:32:07,346 Stage: Train 0.5 | Epoch: 230 | Iter: 273200 | Total Loss: 0.002590 | Recon Loss: 0.002223 | Commit Loss: 0.000733 | Perplexity: 3183.792964
2025-10-11 13:35:57,533 Stage: Train 0.5 | Epoch: 230 | Iter: 273400 | Total Loss: 0.002572 | Recon Loss: 0.002206 | Commit Loss: 0.000733 | Perplexity: 3189.898518
2025-10-11 13:39:47,579 Stage: Train 0.5 | Epoch: 230 | Iter: 273600 | Total Loss: 0.002552 | Recon Loss: 0.002187 | Commit Loss: 0.000730 | Perplexity: 3182.656759
2025-10-11 13:43:37,810 Stage: Train 0.5 | Epoch: 230 | Iter: 273800 | Total Loss: 0.002587 | Recon Loss: 0.002220 | Commit Loss: 0.000732 | Perplexity: 3174.480850
2025-10-11 13:47:28,184 Stage: Train 0.5 | Epoch: 230 | Iter: 274000 | Total Loss: 0.002625 | Recon Loss: 0.002259 | Commit Loss: 0.000732 | Perplexity: 3178.720183
Trainning Epoch:  52%|█████▏    | 258/494 [75:11:29<76:36:10, 1168.52s/it]Trainning Epoch:  52%|█████▏    | 258/494 [75:11:29<76:36:11, 1168.52s/it]2025-10-11 13:51:21,158 Stage: Train 0.5 | Epoch: 231 | Iter: 274200 | Total Loss: 0.002567 | Recon Loss: 0.002201 | Commit Loss: 0.000731 | Perplexity: 3183.824336
2025-10-11 13:55:11,203 Stage: Train 0.5 | Epoch: 231 | Iter: 274400 | Total Loss: 0.002573 | Recon Loss: 0.002204 | Commit Loss: 0.000736 | Perplexity: 3176.439540
2025-10-11 13:59:01,337 Stage: Train 0.5 | Epoch: 231 | Iter: 274600 | Total Loss: 0.002569 | Recon Loss: 0.002203 | Commit Loss: 0.000732 | Perplexity: 3176.554698
2025-10-11 14:02:51,129 Stage: Train 0.5 | Epoch: 231 | Iter: 274800 | Total Loss: 0.002555 | Recon Loss: 0.002189 | Commit Loss: 0.000731 | Perplexity: 3176.464755
2025-10-11 14:06:41,049 Stage: Train 0.5 | Epoch: 231 | Iter: 275000 | Total Loss: 0.002579 | Recon Loss: 0.002214 | Commit Loss: 0.000729 | Perplexity: 3189.222755
Trainning Epoch:  52%|█████▏    | 259/494 [75:30:56<76:15:42, 1168.27s/it]Trainning Epoch:  52%|█████▏    | 259/494 [75:30:56<76:15:43, 1168.27s/it]2025-10-11 14:10:33,827 Stage: Train 0.5 | Epoch: 232 | Iter: 275200 | Total Loss: 0.002579 | Recon Loss: 0.002213 | Commit Loss: 0.000733 | Perplexity: 3185.009698
2025-10-11 14:14:23,555 Stage: Train 0.5 | Epoch: 232 | Iter: 275400 | Total Loss: 0.002599 | Recon Loss: 0.002234 | Commit Loss: 0.000730 | Perplexity: 3173.818101
2025-10-11 14:18:13,055 Stage: Train 0.5 | Epoch: 232 | Iter: 275600 | Total Loss: 0.002585 | Recon Loss: 0.002223 | Commit Loss: 0.000725 | Perplexity: 3179.031569
2025-10-11 14:22:02,509 Stage: Train 0.5 | Epoch: 232 | Iter: 275800 | Total Loss: 0.002549 | Recon Loss: 0.002180 | Commit Loss: 0.000738 | Perplexity: 3181.769435
2025-10-11 14:25:51,901 Stage: Train 0.5 | Epoch: 232 | Iter: 276000 | Total Loss: 0.002556 | Recon Loss: 0.002193 | Commit Loss: 0.000727 | Perplexity: 3178.652755
Trainning Epoch:  53%|█████▎    | 260/494 [75:50:22<75:53:25, 1167.55s/it]Trainning Epoch:  53%|█████▎    | 260/494 [75:50:22<75:53:26, 1167.55s/it]2025-10-11 14:29:44,658 Stage: Train 0.5 | Epoch: 233 | Iter: 276200 | Total Loss: 0.002551 | Recon Loss: 0.002187 | Commit Loss: 0.000727 | Perplexity: 3178.823927
2025-10-11 14:33:35,102 Stage: Train 0.5 | Epoch: 233 | Iter: 276400 | Total Loss: 0.002560 | Recon Loss: 0.002195 | Commit Loss: 0.000731 | Perplexity: 3171.468060
2025-10-11 14:37:25,951 Stage: Train 0.5 | Epoch: 233 | Iter: 276600 | Total Loss: 0.002571 | Recon Loss: 0.002207 | Commit Loss: 0.000730 | Perplexity: 3176.519672
2025-10-11 14:41:15,444 Stage: Train 0.5 | Epoch: 233 | Iter: 276800 | Total Loss: 0.002548 | Recon Loss: 0.002185 | Commit Loss: 0.000727 | Perplexity: 3178.075503
2025-10-11 14:45:04,091 Stage: Train 0.5 | Epoch: 233 | Iter: 277000 | Total Loss: 0.002559 | Recon Loss: 0.002194 | Commit Loss: 0.000730 | Perplexity: 3184.763662
Trainning Epoch:  53%|█████▎    | 261/494 [76:09:49<75:33:21, 1167.39s/it]Trainning Epoch:  53%|█████▎    | 261/494 [76:09:49<75:33:21, 1167.39s/it]2025-10-11 14:48:55,092 Stage: Train 0.5 | Epoch: 234 | Iter: 277200 | Total Loss: 0.002558 | Recon Loss: 0.002195 | Commit Loss: 0.000726 | Perplexity: 3183.699065
2025-10-11 14:52:44,474 Stage: Train 0.5 | Epoch: 234 | Iter: 277400 | Total Loss: 0.002600 | Recon Loss: 0.002233 | Commit Loss: 0.000735 | Perplexity: 3185.904235
2025-10-11 14:56:33,748 Stage: Train 0.5 | Epoch: 234 | Iter: 277600 | Total Loss: 0.002563 | Recon Loss: 0.002193 | Commit Loss: 0.000739 | Perplexity: 3179.174391
2025-10-11 15:00:23,908 Stage: Train 0.5 | Epoch: 234 | Iter: 277800 | Total Loss: 0.002567 | Recon Loss: 0.002200 | Commit Loss: 0.000734 | Perplexity: 3191.123221
2025-10-11 15:04:13,879 Stage: Train 0.5 | Epoch: 234 | Iter: 278000 | Total Loss: 0.002564 | Recon Loss: 0.002204 | Commit Loss: 0.000720 | Perplexity: 3187.549010
Trainning Epoch:  53%|█████▎    | 262/494 [76:29:14<75:11:05, 1166.66s/it]Trainning Epoch:  53%|█████▎    | 262/494 [76:29:14<75:11:05, 1166.66s/it]2025-10-11 15:08:07,270 Stage: Train 0.5 | Epoch: 235 | Iter: 278200 | Total Loss: 0.002546 | Recon Loss: 0.002184 | Commit Loss: 0.000726 | Perplexity: 3189.282654
2025-10-11 15:11:57,198 Stage: Train 0.5 | Epoch: 235 | Iter: 278400 | Total Loss: 0.002564 | Recon Loss: 0.002199 | Commit Loss: 0.000730 | Perplexity: 3181.498191
2025-10-11 15:15:46,573 Stage: Train 0.5 | Epoch: 235 | Iter: 278600 | Total Loss: 0.002555 | Recon Loss: 0.002187 | Commit Loss: 0.000736 | Perplexity: 3180.123392
2025-10-11 15:19:36,032 Stage: Train 0.5 | Epoch: 235 | Iter: 278800 | Total Loss: 0.002624 | Recon Loss: 0.002262 | Commit Loss: 0.000724 | Perplexity: 3180.790964
2025-10-11 15:23:26,995 Stage: Train 0.5 | Epoch: 235 | Iter: 279000 | Total Loss: 0.002538 | Recon Loss: 0.002168 | Commit Loss: 0.000740 | Perplexity: 3182.650956
Trainning Epoch:  53%|█████▎    | 263/494 [76:48:43<74:53:31, 1167.15s/it]Trainning Epoch:  53%|█████▎    | 263/494 [76:48:43<74:53:32, 1167.15s/it]2025-10-11 15:27:22,286 Stage: Train 0.5 | Epoch: 236 | Iter: 279200 | Total Loss: 0.002591 | Recon Loss: 0.002223 | Commit Loss: 0.000736 | Perplexity: 3190.176206
2025-10-11 15:31:14,555 Stage: Train 0.5 | Epoch: 236 | Iter: 279400 | Total Loss: 0.002562 | Recon Loss: 0.002199 | Commit Loss: 0.000726 | Perplexity: 3194.789530
2025-10-11 15:35:06,647 Stage: Train 0.5 | Epoch: 236 | Iter: 279600 | Total Loss: 0.002563 | Recon Loss: 0.002204 | Commit Loss: 0.000717 | Perplexity: 3188.076670
2025-10-11 15:38:58,734 Stage: Train 0.5 | Epoch: 236 | Iter: 279800 | Total Loss: 0.002542 | Recon Loss: 0.002175 | Commit Loss: 0.000736 | Perplexity: 3186.273552
2025-10-11 15:42:50,424 Stage: Train 0.5 | Epoch: 236 | Iter: 280000 | Total Loss: 0.002549 | Recon Loss: 0.002180 | Commit Loss: 0.000739 | Perplexity: 3183.379376
2025-10-11 15:42:50,424 Saving model at iteration 280000
2025-10-11 15:42:50,930 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000
2025-10-11 15:42:52,298 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000/model.safetensors
2025-10-11 15:42:53,941 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000/optimizer.bin
2025-10-11 15:42:53,941 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000/scheduler.bin
2025-10-11 15:42:53,941 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000/sampler.bin
2025-10-11 15:42:53,942 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_237_step_280000/random_states_0.pkl
Trainning Epoch:  53%|█████▎    | 264/494 [77:08:25<74:51:38, 1171.73s/it]Trainning Epoch:  53%|█████▎    | 264/494 [77:08:25<74:51:38, 1171.73s/it]2025-10-11 15:46:50,510 Stage: Train 0.5 | Epoch: 237 | Iter: 280200 | Total Loss: 0.002609 | Recon Loss: 0.002247 | Commit Loss: 0.000724 | Perplexity: 3181.086680
2025-10-11 15:50:43,860 Stage: Train 0.5 | Epoch: 237 | Iter: 280400 | Total Loss: 0.002527 | Recon Loss: 0.002163 | Commit Loss: 0.000728 | Perplexity: 3178.132367
2025-10-11 15:54:37,444 Stage: Train 0.5 | Epoch: 237 | Iter: 280600 | Total Loss: 0.002561 | Recon Loss: 0.002191 | Commit Loss: 0.000739 | Perplexity: 3189.349829
2025-10-11 15:58:31,813 Stage: Train 0.5 | Epoch: 237 | Iter: 280800 | Total Loss: 0.002562 | Recon Loss: 0.002200 | Commit Loss: 0.000723 | Perplexity: 3188.159689
2025-10-11 16:02:26,011 Stage: Train 0.5 | Epoch: 237 | Iter: 281000 | Total Loss: 0.002563 | Recon Loss: 0.002194 | Commit Loss: 0.000737 | Perplexity: 3183.700415
Trainning Epoch:  54%|█████▎    | 265/494 [77:28:13<74:50:36, 1176.58s/it]Trainning Epoch:  54%|█████▎    | 265/494 [77:28:13<74:50:36, 1176.58s/it]2025-10-11 16:06:21,585 Stage: Train 0.5 | Epoch: 238 | Iter: 281200 | Total Loss: 0.002551 | Recon Loss: 0.002186 | Commit Loss: 0.000730 | Perplexity: 3177.420930
2025-10-11 16:10:12,153 Stage: Train 0.5 | Epoch: 238 | Iter: 281400 | Total Loss: 0.002534 | Recon Loss: 0.002169 | Commit Loss: 0.000730 | Perplexity: 3186.276219
2025-10-11 16:14:02,335 Stage: Train 0.5 | Epoch: 238 | Iter: 281600 | Total Loss: 0.002580 | Recon Loss: 0.002213 | Commit Loss: 0.000734 | Perplexity: 3187.448542
2025-10-11 16:17:52,722 Stage: Train 0.5 | Epoch: 238 | Iter: 281800 | Total Loss: 0.002562 | Recon Loss: 0.002196 | Commit Loss: 0.000732 | Perplexity: 3183.034701
2025-10-11 16:21:43,262 Stage: Train 0.5 | Epoch: 238 | Iter: 282000 | Total Loss: 0.002545 | Recon Loss: 0.002183 | Commit Loss: 0.000724 | Perplexity: 3180.081541
Trainning Epoch:  54%|█████▍    | 266/494 [77:47:44<74:24:33, 1174.88s/it]Trainning Epoch:  54%|█████▍    | 266/494 [77:47:44<74:24:33, 1174.89s/it]2025-10-11 16:25:38,012 Stage: Train 0.5 | Epoch: 239 | Iter: 282200 | Total Loss: 0.002543 | Recon Loss: 0.002177 | Commit Loss: 0.000732 | Perplexity: 3188.188666
2025-10-11 16:29:30,527 Stage: Train 0.5 | Epoch: 239 | Iter: 282400 | Total Loss: 0.002568 | Recon Loss: 0.002203 | Commit Loss: 0.000729 | Perplexity: 3190.446122
2025-10-11 16:33:22,735 Stage: Train 0.5 | Epoch: 239 | Iter: 282600 | Total Loss: 0.002539 | Recon Loss: 0.002175 | Commit Loss: 0.000729 | Perplexity: 3179.141335
2025-10-11 16:37:15,030 Stage: Train 0.5 | Epoch: 239 | Iter: 282800 | Total Loss: 0.002536 | Recon Loss: 0.002170 | Commit Loss: 0.000734 | Perplexity: 3190.551411
2025-10-11 16:41:06,817 Stage: Train 0.5 | Epoch: 239 | Iter: 283000 | Total Loss: 0.002553 | Recon Loss: 0.002187 | Commit Loss: 0.000732 | Perplexity: 3188.319236
Trainning Epoch:  54%|█████▍    | 267/494 [78:07:23<74:09:55, 1176.19s/it]Trainning Epoch:  54%|█████▍    | 267/494 [78:07:23<74:09:55, 1176.19s/it]2025-10-11 16:45:00,111 Stage: Train 0.5 | Epoch: 240 | Iter: 283200 | Total Loss: 0.002589 | Recon Loss: 0.002220 | Commit Loss: 0.000737 | Perplexity: 3190.111293
2025-10-11 16:48:49,268 Stage: Train 0.5 | Epoch: 240 | Iter: 283400 | Total Loss: 0.002558 | Recon Loss: 0.002196 | Commit Loss: 0.000724 | Perplexity: 3175.659304
2025-10-11 16:52:38,905 Stage: Train 0.5 | Epoch: 240 | Iter: 283600 | Total Loss: 0.002532 | Recon Loss: 0.002168 | Commit Loss: 0.000729 | Perplexity: 3184.969421
2025-10-11 16:56:28,995 Stage: Train 0.5 | Epoch: 240 | Iter: 283800 | Total Loss: 0.002562 | Recon Loss: 0.002202 | Commit Loss: 0.000720 | Perplexity: 3178.369565
2025-10-11 17:00:18,962 Stage: Train 0.5 | Epoch: 240 | Iter: 284000 | Total Loss: 0.002539 | Recon Loss: 0.002172 | Commit Loss: 0.000734 | Perplexity: 3200.686445
Trainning Epoch:  54%|█████▍    | 268/494 [78:26:49<73:38:33, 1173.07s/it]Trainning Epoch:  54%|█████▍    | 268/494 [78:26:49<73:38:33, 1173.07s/it]2025-10-11 17:04:12,871 Stage: Train 0.5 | Epoch: 241 | Iter: 284200 | Total Loss: 0.002541 | Recon Loss: 0.002179 | Commit Loss: 0.000724 | Perplexity: 3174.186997
2025-10-11 17:08:05,852 Stage: Train 0.5 | Epoch: 241 | Iter: 284400 | Total Loss: 0.002547 | Recon Loss: 0.002188 | Commit Loss: 0.000719 | Perplexity: 3178.556632
2025-10-11 17:11:59,278 Stage: Train 0.5 | Epoch: 241 | Iter: 284600 | Total Loss: 0.002535 | Recon Loss: 0.002165 | Commit Loss: 0.000738 | Perplexity: 3179.603307
2025-10-11 17:15:52,883 Stage: Train 0.5 | Epoch: 241 | Iter: 284800 | Total Loss: 0.002552 | Recon Loss: 0.002193 | Commit Loss: 0.000719 | Perplexity: 3184.017505
2025-10-11 17:19:46,683 Stage: Train 0.5 | Epoch: 241 | Iter: 285000 | Total Loss: 0.002537 | Recon Loss: 0.002174 | Commit Loss: 0.000725 | Perplexity: 3186.191633
Trainning Epoch:  54%|█████▍    | 269/494 [78:46:34<73:32:35, 1176.69s/it]Trainning Epoch:  54%|█████▍    | 269/494 [78:46:34<73:32:36, 1176.69s/it]2025-10-11 17:23:43,164 Stage: Train 0.5 | Epoch: 242 | Iter: 285200 | Total Loss: 0.002545 | Recon Loss: 0.002184 | Commit Loss: 0.000722 | Perplexity: 3174.629314
2025-10-11 17:27:36,638 Stage: Train 0.5 | Epoch: 242 | Iter: 285400 | Total Loss: 0.002533 | Recon Loss: 0.002167 | Commit Loss: 0.000731 | Perplexity: 3183.725972
2025-10-11 17:31:29,983 Stage: Train 0.5 | Epoch: 242 | Iter: 285600 | Total Loss: 0.002584 | Recon Loss: 0.002220 | Commit Loss: 0.000727 | Perplexity: 3173.866069
2025-10-11 17:35:23,556 Stage: Train 0.5 | Epoch: 242 | Iter: 285800 | Total Loss: 0.002503 | Recon Loss: 0.002138 | Commit Loss: 0.000728 | Perplexity: 3186.971971
2025-10-11 17:39:17,490 Stage: Train 0.5 | Epoch: 242 | Iter: 286000 | Total Loss: 0.002545 | Recon Loss: 0.002179 | Commit Loss: 0.000731 | Perplexity: 3185.147783
Trainning Epoch:  55%|█████▍    | 270/494 [79:06:20<73:23:37, 1179.54s/it]Trainning Epoch:  55%|█████▍    | 270/494 [79:06:20<73:23:37, 1179.54s/it]2025-10-11 17:43:13,873 Stage: Train 0.5 | Epoch: 243 | Iter: 286200 | Total Loss: 0.002559 | Recon Loss: 0.002193 | Commit Loss: 0.000734 | Perplexity: 3191.515149
2025-10-11 17:47:06,820 Stage: Train 0.5 | Epoch: 243 | Iter: 286400 | Total Loss: 0.002534 | Recon Loss: 0.002170 | Commit Loss: 0.000728 | Perplexity: 3194.101259
2025-10-11 17:51:00,185 Stage: Train 0.5 | Epoch: 243 | Iter: 286600 | Total Loss: 0.002527 | Recon Loss: 0.002169 | Commit Loss: 0.000717 | Perplexity: 3177.220613
2025-10-11 17:54:53,575 Stage: Train 0.5 | Epoch: 243 | Iter: 286800 | Total Loss: 0.002574 | Recon Loss: 0.002204 | Commit Loss: 0.000740 | Perplexity: 3187.283873
2025-10-11 17:58:46,927 Stage: Train 0.5 | Epoch: 243 | Iter: 287000 | Total Loss: 0.002539 | Recon Loss: 0.002174 | Commit Loss: 0.000729 | Perplexity: 3188.126707
Trainning Epoch:  55%|█████▍    | 271/494 [79:26:04<73:09:03, 1180.91s/it]Trainning Epoch:  55%|█████▍    | 271/494 [79:26:04<73:09:03, 1180.91s/it]2025-10-11 18:02:42,374 Stage: Train 0.5 | Epoch: 244 | Iter: 287200 | Total Loss: 0.002551 | Recon Loss: 0.002186 | Commit Loss: 0.000729 | Perplexity: 3179.493903
2025-10-11 18:06:31,659 Stage: Train 0.5 | Epoch: 244 | Iter: 287400 | Total Loss: 0.002545 | Recon Loss: 0.002186 | Commit Loss: 0.000719 | Perplexity: 3182.604196
2025-10-11 18:10:23,120 Stage: Train 0.5 | Epoch: 244 | Iter: 287600 | Total Loss: 0.002533 | Recon Loss: 0.002173 | Commit Loss: 0.000721 | Perplexity: 3180.259484
2025-10-11 18:14:15,645 Stage: Train 0.5 | Epoch: 244 | Iter: 287800 | Total Loss: 0.002577 | Recon Loss: 0.002215 | Commit Loss: 0.000723 | Perplexity: 3173.520414
2025-10-11 18:18:08,820 Stage: Train 0.5 | Epoch: 244 | Iter: 288000 | Total Loss: 0.002544 | Recon Loss: 0.002182 | Commit Loss: 0.000725 | Perplexity: 3194.323792
Trainning Epoch:  55%|█████▌    | 272/494 [79:45:41<72:45:17, 1179.81s/it]Trainning Epoch:  55%|█████▌    | 272/494 [79:45:41<72:45:17, 1179.81s/it]2025-10-11 18:22:04,656 Stage: Train 0.5 | Epoch: 245 | Iter: 288200 | Total Loss: 0.002528 | Recon Loss: 0.002164 | Commit Loss: 0.000729 | Perplexity: 3194.266359
2025-10-11 18:25:52,627 Stage: Train 0.5 | Epoch: 245 | Iter: 288400 | Total Loss: 0.002547 | Recon Loss: 0.002186 | Commit Loss: 0.000723 | Perplexity: 3187.848317
2025-10-11 18:29:41,499 Stage: Train 0.5 | Epoch: 245 | Iter: 288600 | Total Loss: 0.002546 | Recon Loss: 0.002182 | Commit Loss: 0.000728 | Perplexity: 3196.547002
2025-10-11 18:33:30,348 Stage: Train 0.5 | Epoch: 245 | Iter: 288800 | Total Loss: 0.002527 | Recon Loss: 0.002165 | Commit Loss: 0.000725 | Perplexity: 3186.890088
2025-10-11 18:37:19,485 Stage: Train 0.5 | Epoch: 245 | Iter: 289000 | Total Loss: 0.002505 | Recon Loss: 0.002141 | Commit Loss: 0.000729 | Perplexity: 3180.224243
Trainning Epoch:  55%|█████▌    | 273/494 [80:05:04<72:06:48, 1174.70s/it]Trainning Epoch:  55%|█████▌    | 273/494 [80:05:04<72:06:47, 1174.70s/it]2025-10-11 18:41:12,459 Stage: Train 0.5 | Epoch: 246 | Iter: 289200 | Total Loss: 0.002564 | Recon Loss: 0.002198 | Commit Loss: 0.000731 | Perplexity: 3189.582893
2025-10-11 18:45:00,607 Stage: Train 0.5 | Epoch: 246 | Iter: 289400 | Total Loss: 0.002519 | Recon Loss: 0.002155 | Commit Loss: 0.000728 | Perplexity: 3185.646113
2025-10-11 18:48:49,348 Stage: Train 0.5 | Epoch: 246 | Iter: 289600 | Total Loss: 0.002540 | Recon Loss: 0.002175 | Commit Loss: 0.000730 | Perplexity: 3186.465309
2025-10-11 18:52:38,966 Stage: Train 0.5 | Epoch: 246 | Iter: 289800 | Total Loss: 0.002514 | Recon Loss: 0.002149 | Commit Loss: 0.000728 | Perplexity: 3190.143221
2025-10-11 18:56:29,603 Stage: Train 0.5 | Epoch: 246 | Iter: 290000 | Total Loss: 0.002543 | Recon Loss: 0.002168 | Commit Loss: 0.000749 | Perplexity: 3181.899275
2025-10-11 19:00:21,085 Stage: Train 0.5 | Epoch: 246 | Iter: 290200 | Total Loss: 0.002576 | Recon Loss: 0.002207 | Commit Loss: 0.000737 | Perplexity: 3192.096534
Trainning Epoch:  55%|█████▌    | 274/494 [80:24:31<71:38:29, 1172.32s/it]Trainning Epoch:  55%|█████▌    | 274/494 [80:24:31<71:38:29, 1172.32s/it]2025-10-11 19:04:16,531 Stage: Train 0.5 | Epoch: 247 | Iter: 290400 | Total Loss: 0.002515 | Recon Loss: 0.002148 | Commit Loss: 0.000733 | Perplexity: 3194.672485
2025-10-11 19:08:08,145 Stage: Train 0.5 | Epoch: 247 | Iter: 290600 | Total Loss: 0.002552 | Recon Loss: 0.002184 | Commit Loss: 0.000738 | Perplexity: 3196.599642
2025-10-11 19:11:58,951 Stage: Train 0.5 | Epoch: 247 | Iter: 290800 | Total Loss: 0.002537 | Recon Loss: 0.002170 | Commit Loss: 0.000734 | Perplexity: 3184.971276
2025-10-11 19:15:49,398 Stage: Train 0.5 | Epoch: 247 | Iter: 291000 | Total Loss: 0.002543 | Recon Loss: 0.002176 | Commit Loss: 0.000735 | Perplexity: 3182.761827
2025-10-11 19:19:39,143 Stage: Train 0.5 | Epoch: 247 | Iter: 291200 | Total Loss: 0.002521 | Recon Loss: 0.002156 | Commit Loss: 0.000730 | Perplexity: 3185.690021
Trainning Epoch:  56%|█████▌    | 275/494 [80:44:04<71:19:31, 1172.47s/it]Trainning Epoch:  56%|█████▌    | 275/494 [80:44:04<71:19:31, 1172.47s/it]2025-10-11 19:23:33,515 Stage: Train 0.5 | Epoch: 248 | Iter: 291400 | Total Loss: 0.002537 | Recon Loss: 0.002175 | Commit Loss: 0.000726 | Perplexity: 3192.415426
2025-10-11 19:27:23,905 Stage: Train 0.5 | Epoch: 248 | Iter: 291600 | Total Loss: 0.002528 | Recon Loss: 0.002165 | Commit Loss: 0.000725 | Perplexity: 3189.480256
2025-10-11 19:31:14,093 Stage: Train 0.5 | Epoch: 248 | Iter: 291800 | Total Loss: 0.002507 | Recon Loss: 0.002142 | Commit Loss: 0.000730 | Perplexity: 3194.280133
2025-10-11 19:35:04,071 Stage: Train 0.5 | Epoch: 248 | Iter: 292000 | Total Loss: 0.002540 | Recon Loss: 0.002172 | Commit Loss: 0.000736 | Perplexity: 3190.774980
2025-10-11 19:38:53,852 Stage: Train 0.5 | Epoch: 248 | Iter: 292200 | Total Loss: 0.002516 | Recon Loss: 0.002156 | Commit Loss: 0.000721 | Perplexity: 3183.113005
Trainning Epoch:  56%|█████▌    | 276/494 [81:03:33<70:56:47, 1171.59s/it]Trainning Epoch:  56%|█████▌    | 276/494 [81:03:33<70:56:47, 1171.59s/it]2025-10-11 19:42:46,780 Stage: Train 0.5 | Epoch: 249 | Iter: 292400 | Total Loss: 0.002533 | Recon Loss: 0.002169 | Commit Loss: 0.000727 | Perplexity: 3190.187786
2025-10-11 19:46:35,966 Stage: Train 0.5 | Epoch: 249 | Iter: 292600 | Total Loss: 0.002514 | Recon Loss: 0.002148 | Commit Loss: 0.000732 | Perplexity: 3186.682557
2025-10-11 19:50:25,301 Stage: Train 0.5 | Epoch: 249 | Iter: 292800 | Total Loss: 0.002523 | Recon Loss: 0.002158 | Commit Loss: 0.000730 | Perplexity: 3178.230256
2025-10-11 19:54:14,769 Stage: Train 0.5 | Epoch: 249 | Iter: 293000 | Total Loss: 0.002532 | Recon Loss: 0.002168 | Commit Loss: 0.000728 | Perplexity: 3185.317614
2025-10-11 19:58:04,379 Stage: Train 0.5 | Epoch: 249 | Iter: 293200 | Total Loss: 0.002540 | Recon Loss: 0.002171 | Commit Loss: 0.000737 | Perplexity: 3186.253772
Trainning Epoch:  56%|█████▌    | 277/494 [81:22:59<70:30:38, 1169.76s/it]Trainning Epoch:  56%|█████▌    | 277/494 [81:22:59<70:30:39, 1169.77s/it]2025-10-11 20:01:57,969 Stage: Train 0.5 | Epoch: 250 | Iter: 293400 | Total Loss: 0.002537 | Recon Loss: 0.002178 | Commit Loss: 0.000719 | Perplexity: 3187.224197
2025-10-11 20:05:50,375 Stage: Train 0.5 | Epoch: 250 | Iter: 293600 | Total Loss: 0.002544 | Recon Loss: 0.002183 | Commit Loss: 0.000722 | Perplexity: 3181.121869
2025-10-11 20:09:43,104 Stage: Train 0.5 | Epoch: 250 | Iter: 293800 | Total Loss: 0.002507 | Recon Loss: 0.002146 | Commit Loss: 0.000722 | Perplexity: 3179.980271
2025-10-11 20:13:35,638 Stage: Train 0.5 | Epoch: 250 | Iter: 294000 | Total Loss: 0.002511 | Recon Loss: 0.002146 | Commit Loss: 0.000730 | Perplexity: 3185.727244
2025-10-11 20:17:28,530 Stage: Train 0.5 | Epoch: 250 | Iter: 294200 | Total Loss: 0.002524 | Recon Loss: 0.002157 | Commit Loss: 0.000734 | Perplexity: 3187.167335
Trainning Epoch:  56%|█████▋    | 278/494 [81:42:39<70:22:25, 1172.90s/it]Trainning Epoch:  56%|█████▋    | 278/494 [81:42:39<70:22:25, 1172.90s/it]2025-10-11 20:21:24,880 Stage: Train 0.5 | Epoch: 251 | Iter: 294400 | Total Loss: 0.002537 | Recon Loss: 0.002174 | Commit Loss: 0.000727 | Perplexity: 3186.884266
2025-10-11 20:25:18,380 Stage: Train 0.5 | Epoch: 251 | Iter: 294600 | Total Loss: 0.002471 | Recon Loss: 0.002108 | Commit Loss: 0.000727 | Perplexity: 3184.403936
2025-10-11 20:29:11,901 Stage: Train 0.5 | Epoch: 251 | Iter: 294800 | Total Loss: 0.002514 | Recon Loss: 0.002144 | Commit Loss: 0.000740 | Perplexity: 3186.161130
2025-10-11 20:33:05,301 Stage: Train 0.5 | Epoch: 251 | Iter: 295000 | Total Loss: 0.002550 | Recon Loss: 0.002184 | Commit Loss: 0.000732 | Perplexity: 3198.545110
2025-10-11 20:36:58,652 Stage: Train 0.5 | Epoch: 251 | Iter: 295200 | Total Loss: 0.002497 | Recon Loss: 0.002130 | Commit Loss: 0.000733 | Perplexity: 3191.030997
Trainning Epoch:  56%|█████▋    | 279/494 [82:02:24<70:16:18, 1176.65s/it]Trainning Epoch:  56%|█████▋    | 279/494 [82:02:24<70:16:19, 1176.65s/it]2025-10-11 20:40:55,697 Stage: Train 0.5 | Epoch: 252 | Iter: 295400 | Total Loss: 0.002524 | Recon Loss: 0.002159 | Commit Loss: 0.000729 | Perplexity: 3191.195398
2025-10-11 20:44:49,745 Stage: Train 0.5 | Epoch: 252 | Iter: 295600 | Total Loss: 0.002515 | Recon Loss: 0.002156 | Commit Loss: 0.000717 | Perplexity: 3188.490293
2025-10-11 20:48:43,450 Stage: Train 0.5 | Epoch: 252 | Iter: 295800 | Total Loss: 0.002529 | Recon Loss: 0.002164 | Commit Loss: 0.000729 | Perplexity: 3192.798896
2025-10-11 20:52:37,816 Stage: Train 0.5 | Epoch: 252 | Iter: 296000 | Total Loss: 0.002548 | Recon Loss: 0.002181 | Commit Loss: 0.000734 | Perplexity: 3186.423730
2025-10-11 20:56:32,531 Stage: Train 0.5 | Epoch: 252 | Iter: 296200 | Total Loss: 0.002520 | Recon Loss: 0.002156 | Commit Loss: 0.000727 | Perplexity: 3184.374730
Trainning Epoch:  57%|█████▋    | 280/494 [82:22:14<70:10:00, 1180.37s/it]Trainning Epoch:  57%|█████▋    | 280/494 [82:22:14<70:10:00, 1180.38s/it]2025-10-11 21:00:27,549 Stage: Train 0.5 | Epoch: 253 | Iter: 296400 | Total Loss: 0.002507 | Recon Loss: 0.002146 | Commit Loss: 0.000722 | Perplexity: 3184.777991
2025-10-11 21:04:19,069 Stage: Train 0.5 | Epoch: 253 | Iter: 296600 | Total Loss: 0.002515 | Recon Loss: 0.002150 | Commit Loss: 0.000729 | Perplexity: 3183.119379
2025-10-11 21:08:10,451 Stage: Train 0.5 | Epoch: 253 | Iter: 296800 | Total Loss: 0.002513 | Recon Loss: 0.002149 | Commit Loss: 0.000728 | Perplexity: 3192.068467
2025-10-11 21:12:02,113 Stage: Train 0.5 | Epoch: 253 | Iter: 297000 | Total Loss: 0.002496 | Recon Loss: 0.002130 | Commit Loss: 0.000732 | Perplexity: 3192.453601
2025-10-11 21:15:53,830 Stage: Train 0.5 | Epoch: 253 | Iter: 297200 | Total Loss: 0.002529 | Recon Loss: 0.002159 | Commit Loss: 0.000740 | Perplexity: 3196.687968
Trainning Epoch:  57%|█████▋    | 281/494 [82:41:49<69:45:13, 1178.94s/it]Trainning Epoch:  57%|█████▋    | 281/494 [82:41:49<69:45:14, 1178.94s/it]2025-10-11 21:19:48,256 Stage: Train 0.5 | Epoch: 254 | Iter: 297400 | Total Loss: 0.002485 | Recon Loss: 0.002123 | Commit Loss: 0.000724 | Perplexity: 3183.749037
2025-10-11 21:23:37,847 Stage: Train 0.5 | Epoch: 254 | Iter: 297600 | Total Loss: 0.002525 | Recon Loss: 0.002146 | Commit Loss: 0.000756 | Perplexity: 3185.502695
2025-10-11 21:27:27,643 Stage: Train 0.5 | Epoch: 254 | Iter: 297800 | Total Loss: 0.002538 | Recon Loss: 0.002173 | Commit Loss: 0.000730 | Perplexity: 3185.414495
2025-10-11 21:31:17,369 Stage: Train 0.5 | Epoch: 254 | Iter: 298000 | Total Loss: 0.002488 | Recon Loss: 0.002124 | Commit Loss: 0.000729 | Perplexity: 3184.947937
2025-10-11 21:35:07,158 Stage: Train 0.5 | Epoch: 254 | Iter: 298200 | Total Loss: 0.002541 | Recon Loss: 0.002175 | Commit Loss: 0.000732 | Perplexity: 3191.239930
Trainning Epoch:  57%|█████▋    | 282/494 [83:01:16<69:12:52, 1175.34s/it]Trainning Epoch:  57%|█████▋    | 282/494 [83:01:16<69:12:52, 1175.34s/it]2025-10-11 21:39:01,729 Stage: Train 0.5 | Epoch: 255 | Iter: 298400 | Total Loss: 0.002522 | Recon Loss: 0.002157 | Commit Loss: 0.000732 | Perplexity: 3195.485509
2025-10-11 21:42:56,315 Stage: Train 0.5 | Epoch: 255 | Iter: 298600 | Total Loss: 0.002484 | Recon Loss: 0.002121 | Commit Loss: 0.000728 | Perplexity: 3199.714397
2025-10-11 21:46:50,803 Stage: Train 0.5 | Epoch: 255 | Iter: 298800 | Total Loss: 0.002489 | Recon Loss: 0.002128 | Commit Loss: 0.000723 | Perplexity: 3188.107249
2025-10-11 21:50:44,861 Stage: Train 0.5 | Epoch: 255 | Iter: 299000 | Total Loss: 0.002495 | Recon Loss: 0.002130 | Commit Loss: 0.000730 | Perplexity: 3187.664431
2025-10-11 21:54:39,341 Stage: Train 0.5 | Epoch: 255 | Iter: 299200 | Total Loss: 0.002514 | Recon Loss: 0.002150 | Commit Loss: 0.000728 | Perplexity: 3181.560544
Trainning Epoch:  57%|█████▋    | 283/494 [83:21:06<69:08:59, 1179.81s/it]Trainning Epoch:  57%|█████▋    | 283/494 [83:21:06<69:08:59, 1179.81s/it]2025-10-11 21:58:34,454 Stage: Train 0.5 | Epoch: 256 | Iter: 299400 | Total Loss: 0.002505 | Recon Loss: 0.002144 | Commit Loss: 0.000721 | Perplexity: 3180.175029
2025-10-11 22:02:23,719 Stage: Train 0.5 | Epoch: 256 | Iter: 299600 | Total Loss: 0.002520 | Recon Loss: 0.002159 | Commit Loss: 0.000722 | Perplexity: 3198.044792
2025-10-11 22:06:12,927 Stage: Train 0.5 | Epoch: 256 | Iter: 299800 | Total Loss: 0.002547 | Recon Loss: 0.002183 | Commit Loss: 0.000728 | Perplexity: 3183.226604
2025-10-11 22:10:01,969 Stage: Train 0.5 | Epoch: 256 | Iter: 300000 | Total Loss: 0.002536 | Recon Loss: 0.002170 | Commit Loss: 0.000731 | Perplexity: 3200.198328
2025-10-11 22:10:01,969 Saving model at iteration 300000
2025-10-11 22:10:02,488 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000
2025-10-11 22:10:03,847 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000/model.safetensors
2025-10-11 22:10:05,506 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000/optimizer.bin
2025-10-11 22:10:05,507 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000/scheduler.bin
2025-10-11 22:10:05,507 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000/sampler.bin
2025-10-11 22:10:05,508 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_257_step_300000/random_states_0.pkl
2025-10-11 22:13:54,610 Stage: Train 0.5 | Epoch: 256 | Iter: 300200 | Total Loss: 0.002512 | Recon Loss: 0.002151 | Commit Loss: 0.000723 | Perplexity: 3180.124089
Trainning Epoch:  57%|█████▋    | 284/494 [83:40:33<68:36:03, 1176.02s/it]Trainning Epoch:  57%|█████▋    | 284/494 [83:40:33<68:36:04, 1176.02s/it]2025-10-11 22:17:48,661 Stage: Train 0.5 | Epoch: 257 | Iter: 300400 | Total Loss: 0.002545 | Recon Loss: 0.002181 | Commit Loss: 0.000729 | Perplexity: 3182.440017
2025-10-11 22:21:41,876 Stage: Train 0.5 | Epoch: 257 | Iter: 300600 | Total Loss: 0.002546 | Recon Loss: 0.002181 | Commit Loss: 0.000731 | Perplexity: 3197.057104
2025-10-11 22:25:34,997 Stage: Train 0.5 | Epoch: 257 | Iter: 300800 | Total Loss: 0.002510 | Recon Loss: 0.002148 | Commit Loss: 0.000726 | Perplexity: 3186.112958
2025-10-11 22:29:28,406 Stage: Train 0.5 | Epoch: 257 | Iter: 301000 | Total Loss: 0.002501 | Recon Loss: 0.002134 | Commit Loss: 0.000734 | Perplexity: 3187.388142
2025-10-11 22:33:21,798 Stage: Train 0.5 | Epoch: 257 | Iter: 301200 | Total Loss: 0.002545 | Recon Loss: 0.002182 | Commit Loss: 0.000727 | Perplexity: 3192.755839
Trainning Epoch:  58%|█████▊    | 285/494 [84:00:19<68:26:12, 1178.82s/it]Trainning Epoch:  58%|█████▊    | 285/494 [84:00:19<68:26:13, 1178.82s/it]2025-10-11 22:37:18,893 Stage: Train 0.5 | Epoch: 258 | Iter: 301400 | Total Loss: 0.002514 | Recon Loss: 0.002152 | Commit Loss: 0.000724 | Perplexity: 3184.773622
2025-10-11 22:41:11,860 Stage: Train 0.5 | Epoch: 258 | Iter: 301600 | Total Loss: 0.002524 | Recon Loss: 0.002157 | Commit Loss: 0.000733 | Perplexity: 3200.796201
2025-10-11 22:45:05,277 Stage: Train 0.5 | Epoch: 258 | Iter: 301800 | Total Loss: 0.002527 | Recon Loss: 0.002168 | Commit Loss: 0.000717 | Perplexity: 3190.255902
2025-10-11 22:48:58,410 Stage: Train 0.5 | Epoch: 258 | Iter: 302000 | Total Loss: 0.002478 | Recon Loss: 0.002114 | Commit Loss: 0.000729 | Perplexity: 3187.504242
2025-10-11 22:52:51,823 Stage: Train 0.5 | Epoch: 258 | Iter: 302200 | Total Loss: 0.002525 | Recon Loss: 0.002164 | Commit Loss: 0.000722 | Perplexity: 3188.053313
Trainning Epoch:  58%|█████▊    | 286/494 [84:20:03<68:12:25, 1180.51s/it]Trainning Epoch:  58%|█████▊    | 286/494 [84:20:03<68:12:26, 1180.51s/it]2025-10-11 22:56:47,163 Stage: Train 0.5 | Epoch: 259 | Iter: 302400 | Total Loss: 0.002521 | Recon Loss: 0.002156 | Commit Loss: 0.000730 | Perplexity: 3186.915055
2025-10-11 23:00:39,194 Stage: Train 0.5 | Epoch: 259 | Iter: 302600 | Total Loss: 0.002493 | Recon Loss: 0.002130 | Commit Loss: 0.000726 | Perplexity: 3197.616312
2025-10-11 23:04:31,607 Stage: Train 0.5 | Epoch: 259 | Iter: 302800 | Total Loss: 0.002486 | Recon Loss: 0.002123 | Commit Loss: 0.000726 | Perplexity: 3191.935365
2025-10-11 23:08:23,929 Stage: Train 0.5 | Epoch: 259 | Iter: 303000 | Total Loss: 0.002513 | Recon Loss: 0.002146 | Commit Loss: 0.000733 | Perplexity: 3196.124957
2025-10-11 23:12:16,086 Stage: Train 0.5 | Epoch: 259 | Iter: 303200 | Total Loss: 0.002507 | Recon Loss: 0.002142 | Commit Loss: 0.000730 | Perplexity: 3193.030885
Trainning Epoch:  58%|█████▊    | 287/494 [84:39:42<67:51:09, 1180.05s/it]Trainning Epoch:  58%|█████▊    | 287/494 [84:39:42<67:51:09, 1180.05s/it]2025-10-11 23:16:10,969 Stage: Train 0.5 | Epoch: 260 | Iter: 303400 | Total Loss: 0.002500 | Recon Loss: 0.002134 | Commit Loss: 0.000732 | Perplexity: 3190.642025
2025-10-11 23:20:00,043 Stage: Train 0.5 | Epoch: 260 | Iter: 303600 | Total Loss: 0.002497 | Recon Loss: 0.002127 | Commit Loss: 0.000740 | Perplexity: 3192.080627
2025-10-11 23:23:50,699 Stage: Train 0.5 | Epoch: 260 | Iter: 303800 | Total Loss: 0.002502 | Recon Loss: 0.002138 | Commit Loss: 0.000728 | Perplexity: 3181.468879
2025-10-11 23:27:41,110 Stage: Train 0.5 | Epoch: 260 | Iter: 304000 | Total Loss: 0.002492 | Recon Loss: 0.002129 | Commit Loss: 0.000727 | Perplexity: 3192.515896
2025-10-11 23:31:31,344 Stage: Train 0.5 | Epoch: 260 | Iter: 304200 | Total Loss: 0.002531 | Recon Loss: 0.002165 | Commit Loss: 0.000732 | Perplexity: 3181.082637
Trainning Epoch:  58%|█████▊    | 288/494 [84:59:11<67:19:25, 1176.53s/it]Trainning Epoch:  58%|█████▊    | 288/494 [84:59:11<67:19:25, 1176.53s/it]2025-10-11 23:35:25,037 Stage: Train 0.5 | Epoch: 261 | Iter: 304400 | Total Loss: 0.002510 | Recon Loss: 0.002148 | Commit Loss: 0.000724 | Perplexity: 3190.163724
2025-10-11 23:39:18,845 Stage: Train 0.5 | Epoch: 261 | Iter: 304600 | Total Loss: 0.002510 | Recon Loss: 0.002149 | Commit Loss: 0.000722 | Perplexity: 3193.227373
2025-10-11 23:43:12,464 Stage: Train 0.5 | Epoch: 261 | Iter: 304800 | Total Loss: 0.002532 | Recon Loss: 0.002167 | Commit Loss: 0.000731 | Perplexity: 3184.426035
2025-10-11 23:47:06,168 Stage: Train 0.5 | Epoch: 261 | Iter: 305000 | Total Loss: 0.002504 | Recon Loss: 0.002140 | Commit Loss: 0.000729 | Perplexity: 3184.001273
2025-10-11 23:51:00,198 Stage: Train 0.5 | Epoch: 261 | Iter: 305200 | Total Loss: 0.002508 | Recon Loss: 0.002146 | Commit Loss: 0.000724 | Perplexity: 3187.381954
2025-10-11 23:54:53,665 Stage: Train 0.5 | Epoch: 261 | Iter: 305400 | Total Loss: 0.002513 | Recon Loss: 0.002147 | Commit Loss: 0.000732 | Perplexity: 3192.257410
Trainning Epoch:  59%|█████▊    | 289/494 [85:18:58<67:10:39, 1179.71s/it]Trainning Epoch:  59%|█████▊    | 289/494 [85:18:58<67:10:41, 1179.71s/it]2025-10-11 23:58:45,817 Stage: Train 0.5 | Epoch: 262 | Iter: 305600 | Total Loss: 0.002484 | Recon Loss: 0.002122 | Commit Loss: 0.000725 | Perplexity: 3200.535624
2025-10-12 00:02:35,969 Stage: Train 0.5 | Epoch: 262 | Iter: 305800 | Total Loss: 0.002516 | Recon Loss: 0.002150 | Commit Loss: 0.000731 | Perplexity: 3182.069302
2025-10-12 00:06:26,466 Stage: Train 0.5 | Epoch: 262 | Iter: 306000 | Total Loss: 0.002488 | Recon Loss: 0.002118 | Commit Loss: 0.000740 | Perplexity: 3193.415031
2025-10-12 00:10:17,050 Stage: Train 0.5 | Epoch: 262 | Iter: 306200 | Total Loss: 0.002526 | Recon Loss: 0.002162 | Commit Loss: 0.000728 | Perplexity: 3193.946409
2025-10-12 00:14:07,750 Stage: Train 0.5 | Epoch: 262 | Iter: 306400 | Total Loss: 0.002516 | Recon Loss: 0.002152 | Commit Loss: 0.000727 | Perplexity: 3192.248105
Trainning Epoch:  59%|█████▊    | 290/494 [85:38:27<66:39:57, 1176.46s/it]Trainning Epoch:  59%|█████▊    | 290/494 [85:38:27<66:39:57, 1176.46s/it]2025-10-12 00:18:01,922 Stage: Train 0.5 | Epoch: 263 | Iter: 306600 | Total Loss: 0.002516 | Recon Loss: 0.002153 | Commit Loss: 0.000725 | Perplexity: 3179.497261
2025-10-12 00:21:53,937 Stage: Train 0.5 | Epoch: 263 | Iter: 306800 | Total Loss: 0.002480 | Recon Loss: 0.002117 | Commit Loss: 0.000726 | Perplexity: 3184.150566
2025-10-12 00:25:45,734 Stage: Train 0.5 | Epoch: 263 | Iter: 307000 | Total Loss: 0.002483 | Recon Loss: 0.002119 | Commit Loss: 0.000728 | Perplexity: 3204.961658
2025-10-12 00:29:37,411 Stage: Train 0.5 | Epoch: 263 | Iter: 307200 | Total Loss: 0.002489 | Recon Loss: 0.002124 | Commit Loss: 0.000731 | Perplexity: 3189.668652
2025-10-12 00:33:29,489 Stage: Train 0.5 | Epoch: 263 | Iter: 307400 | Total Loss: 0.002513 | Recon Loss: 0.002152 | Commit Loss: 0.000722 | Perplexity: 3183.959839
Trainning Epoch:  59%|█████▉    | 291/494 [85:58:04<66:20:51, 1176.61s/it]Trainning Epoch:  59%|█████▉    | 291/494 [85:58:04<66:20:52, 1176.61s/it]2025-10-12 00:37:22,681 Stage: Train 0.5 | Epoch: 264 | Iter: 307600 | Total Loss: 0.002501 | Recon Loss: 0.002136 | Commit Loss: 0.000729 | Perplexity: 3194.727285
2025-10-12 00:41:13,191 Stage: Train 0.5 | Epoch: 264 | Iter: 307800 | Total Loss: 0.002512 | Recon Loss: 0.002148 | Commit Loss: 0.000728 | Perplexity: 3197.803214
2025-10-12 00:45:03,648 Stage: Train 0.5 | Epoch: 264 | Iter: 308000 | Total Loss: 0.002481 | Recon Loss: 0.002119 | Commit Loss: 0.000724 | Perplexity: 3187.145610
2025-10-12 00:48:54,237 Stage: Train 0.5 | Epoch: 264 | Iter: 308200 | Total Loss: 0.002486 | Recon Loss: 0.002119 | Commit Loss: 0.000735 | Perplexity: 3189.608716
2025-10-12 00:52:44,964 Stage: Train 0.5 | Epoch: 264 | Iter: 308400 | Total Loss: 0.002535 | Recon Loss: 0.002171 | Commit Loss: 0.000729 | Perplexity: 3191.126755
Trainning Epoch:  59%|█████▉    | 292/494 [86:17:34<65:54:55, 1174.73s/it]Trainning Epoch:  59%|█████▉    | 292/494 [86:17:34<65:54:55, 1174.73s/it]2025-10-12 00:56:39,005 Stage: Train 0.5 | Epoch: 265 | Iter: 308600 | Total Loss: 0.002443 | Recon Loss: 0.002082 | Commit Loss: 0.000723 | Perplexity: 3186.634895
2025-10-12 01:00:30,458 Stage: Train 0.5 | Epoch: 265 | Iter: 308800 | Total Loss: 0.002498 | Recon Loss: 0.002129 | Commit Loss: 0.000736 | Perplexity: 3193.568184
2025-10-12 01:04:22,251 Stage: Train 0.5 | Epoch: 265 | Iter: 309000 | Total Loss: 0.002514 | Recon Loss: 0.002148 | Commit Loss: 0.000732 | Perplexity: 3188.623376
2025-10-12 01:08:13,597 Stage: Train 0.5 | Epoch: 265 | Iter: 309200 | Total Loss: 0.002505 | Recon Loss: 0.002141 | Commit Loss: 0.000728 | Perplexity: 3199.036069
2025-10-12 01:12:04,806 Stage: Train 0.5 | Epoch: 265 | Iter: 309400 | Total Loss: 0.002490 | Recon Loss: 0.002128 | Commit Loss: 0.000725 | Perplexity: 3187.569321
Trainning Epoch:  59%|█████▉    | 293/494 [86:37:09<65:35:33, 1174.79s/it]Trainning Epoch:  59%|█████▉    | 293/494 [86:37:09<65:35:33, 1174.79s/it]2025-10-12 01:16:01,265 Stage: Train 0.5 | Epoch: 266 | Iter: 309600 | Total Loss: 0.002497 | Recon Loss: 0.002132 | Commit Loss: 0.000732 | Perplexity: 3199.768137
2025-10-12 01:19:55,922 Stage: Train 0.5 | Epoch: 266 | Iter: 309800 | Total Loss: 0.002525 | Recon Loss: 0.002166 | Commit Loss: 0.000719 | Perplexity: 3172.422878
2025-10-12 01:23:50,255 Stage: Train 0.5 | Epoch: 266 | Iter: 310000 | Total Loss: 0.002487 | Recon Loss: 0.002126 | Commit Loss: 0.000722 | Perplexity: 3190.251354
2025-10-12 01:27:44,809 Stage: Train 0.5 | Epoch: 266 | Iter: 310200 | Total Loss: 0.002508 | Recon Loss: 0.002144 | Commit Loss: 0.000729 | Perplexity: 3204.510175
2025-10-12 01:31:38,973 Stage: Train 0.5 | Epoch: 266 | Iter: 310400 | Total Loss: 0.002478 | Recon Loss: 0.002112 | Commit Loss: 0.000731 | Perplexity: 3187.787003
Trainning Epoch:  60%|█████▉    | 294/494 [86:56:59<65:31:20, 1179.40s/it]Trainning Epoch:  60%|█████▉    | 294/494 [86:56:59<65:31:20, 1179.40s/it]2025-10-12 01:35:35,945 Stage: Train 0.5 | Epoch: 267 | Iter: 310600 | Total Loss: 0.002499 | Recon Loss: 0.002140 | Commit Loss: 0.000718 | Perplexity: 3188.215723
2025-10-12 01:39:30,168 Stage: Train 0.5 | Epoch: 267 | Iter: 310800 | Total Loss: 0.002461 | Recon Loss: 0.002098 | Commit Loss: 0.000726 | Perplexity: 3193.348525
2025-10-12 01:43:24,417 Stage: Train 0.5 | Epoch: 267 | Iter: 311000 | Total Loss: 0.002472 | Recon Loss: 0.002110 | Commit Loss: 0.000725 | Perplexity: 3184.578334
2025-10-12 01:47:19,022 Stage: Train 0.5 | Epoch: 267 | Iter: 311200 | Total Loss: 0.002486 | Recon Loss: 0.002120 | Commit Loss: 0.000731 | Perplexity: 3196.510084
2025-10-12 01:51:13,519 Stage: Train 0.5 | Epoch: 267 | Iter: 311400 | Total Loss: 0.002519 | Recon Loss: 0.002159 | Commit Loss: 0.000722 | Perplexity: 3189.757341
Trainning Epoch:  60%|█████▉    | 295/494 [87:16:49<65:21:46, 1182.44s/it]Trainning Epoch:  60%|█████▉    | 295/494 [87:16:49<65:21:46, 1182.44s/it]2025-10-12 01:55:07,394 Stage: Train 0.5 | Epoch: 268 | Iter: 311600 | Total Loss: 0.002464 | Recon Loss: 0.002103 | Commit Loss: 0.000721 | Perplexity: 3184.029133
2025-10-12 01:58:57,018 Stage: Train 0.5 | Epoch: 268 | Iter: 311800 | Total Loss: 0.002486 | Recon Loss: 0.002120 | Commit Loss: 0.000732 | Perplexity: 3198.682821
2025-10-12 02:02:46,986 Stage: Train 0.5 | Epoch: 268 | Iter: 312000 | Total Loss: 0.002492 | Recon Loss: 0.002132 | Commit Loss: 0.000719 | Perplexity: 3187.176697
2025-10-12 02:06:36,997 Stage: Train 0.5 | Epoch: 268 | Iter: 312200 | Total Loss: 0.002515 | Recon Loss: 0.002150 | Commit Loss: 0.000731 | Perplexity: 3188.723588
2025-10-12 02:10:26,860 Stage: Train 0.5 | Epoch: 268 | Iter: 312400 | Total Loss: 0.002461 | Recon Loss: 0.002100 | Commit Loss: 0.000722 | Perplexity: 3194.685167
Trainning Epoch:  60%|█████▉    | 296/494 [87:36:16<64:46:47, 1177.82s/it]Trainning Epoch:  60%|█████▉    | 296/494 [87:36:16<64:46:48, 1177.82s/it]2025-10-12 02:14:22,994 Stage: Train 0.5 | Epoch: 269 | Iter: 312600 | Total Loss: 0.002473 | Recon Loss: 0.002104 | Commit Loss: 0.000738 | Perplexity: 3202.591060
2025-10-12 02:18:16,250 Stage: Train 0.5 | Epoch: 269 | Iter: 312800 | Total Loss: 0.002468 | Recon Loss: 0.002106 | Commit Loss: 0.000725 | Perplexity: 3195.012878
2025-10-12 02:22:09,703 Stage: Train 0.5 | Epoch: 269 | Iter: 313000 | Total Loss: 0.002489 | Recon Loss: 0.002124 | Commit Loss: 0.000730 | Perplexity: 3193.525789
2025-10-12 02:26:04,226 Stage: Train 0.5 | Epoch: 269 | Iter: 313200 | Total Loss: 0.002478 | Recon Loss: 0.002114 | Commit Loss: 0.000727 | Perplexity: 3195.254542
2025-10-12 02:29:58,683 Stage: Train 0.5 | Epoch: 269 | Iter: 313400 | Total Loss: 0.002487 | Recon Loss: 0.002119 | Commit Loss: 0.000736 | Perplexity: 3196.045015
Trainning Epoch:  60%|██████    | 297/494 [87:56:05<64:38:14, 1181.19s/it]Trainning Epoch:  60%|██████    | 297/494 [87:56:05<64:38:14, 1181.19s/it]2025-10-12 02:33:56,634 Stage: Train 0.5 | Epoch: 270 | Iter: 313600 | Total Loss: 0.002509 | Recon Loss: 0.002142 | Commit Loss: 0.000734 | Perplexity: 3194.215089
2025-10-12 02:37:52,028 Stage: Train 0.5 | Epoch: 270 | Iter: 313800 | Total Loss: 0.002489 | Recon Loss: 0.002127 | Commit Loss: 0.000724 | Perplexity: 3188.003823
2025-10-12 02:41:47,038 Stage: Train 0.5 | Epoch: 270 | Iter: 314000 | Total Loss: 0.002487 | Recon Loss: 0.002124 | Commit Loss: 0.000725 | Perplexity: 3185.270979
2025-10-12 02:45:42,229 Stage: Train 0.5 | Epoch: 270 | Iter: 314200 | Total Loss: 0.002464 | Recon Loss: 0.002103 | Commit Loss: 0.000721 | Perplexity: 3193.289100
2025-10-12 02:49:37,170 Stage: Train 0.5 | Epoch: 270 | Iter: 314400 | Total Loss: 0.002476 | Recon Loss: 0.002109 | Commit Loss: 0.000734 | Perplexity: 3194.204056
Trainning Epoch:  60%|██████    | 298/494 [88:15:59<64:31:19, 1185.10s/it]Trainning Epoch:  60%|██████    | 298/494 [88:15:59<64:31:20, 1185.11s/it]2025-10-12 02:53:35,111 Stage: Train 0.5 | Epoch: 271 | Iter: 314600 | Total Loss: 0.002501 | Recon Loss: 0.002136 | Commit Loss: 0.000731 | Perplexity: 3207.835862
2025-10-12 02:57:28,991 Stage: Train 0.5 | Epoch: 271 | Iter: 314800 | Total Loss: 0.002459 | Recon Loss: 0.002097 | Commit Loss: 0.000725 | Perplexity: 3197.575292
2025-10-12 03:01:23,260 Stage: Train 0.5 | Epoch: 271 | Iter: 315000 | Total Loss: 0.002476 | Recon Loss: 0.002115 | Commit Loss: 0.000721 | Perplexity: 3186.028364
2025-10-12 03:05:17,904 Stage: Train 0.5 | Epoch: 271 | Iter: 315200 | Total Loss: 0.002484 | Recon Loss: 0.002118 | Commit Loss: 0.000732 | Perplexity: 3178.363123
2025-10-12 03:09:12,577 Stage: Train 0.5 | Epoch: 271 | Iter: 315400 | Total Loss: 0.002459 | Recon Loss: 0.002098 | Commit Loss: 0.000723 | Perplexity: 3186.544590
Trainning Epoch:  61%|██████    | 299/494 [88:35:49<64:16:31, 1186.62s/it]Trainning Epoch:  61%|██████    | 299/494 [88:35:49<64:16:32, 1186.63s/it]2025-10-12 03:13:09,265 Stage: Train 0.5 | Epoch: 272 | Iter: 315600 | Total Loss: 0.002452 | Recon Loss: 0.002089 | Commit Loss: 0.000727 | Perplexity: 3196.531401
2025-10-12 03:17:01,922 Stage: Train 0.5 | Epoch: 272 | Iter: 315800 | Total Loss: 0.002475 | Recon Loss: 0.002114 | Commit Loss: 0.000720 | Perplexity: 3183.396241
2025-10-12 03:20:55,117 Stage: Train 0.5 | Epoch: 272 | Iter: 316000 | Total Loss: 0.002491 | Recon Loss: 0.002126 | Commit Loss: 0.000730 | Perplexity: 3186.948662
2025-10-12 03:24:48,700 Stage: Train 0.5 | Epoch: 272 | Iter: 316200 | Total Loss: 0.002463 | Recon Loss: 0.002098 | Commit Loss: 0.000729 | Perplexity: 3194.746896
2025-10-12 03:28:42,625 Stage: Train 0.5 | Epoch: 272 | Iter: 316400 | Total Loss: 0.002490 | Recon Loss: 0.002124 | Commit Loss: 0.000731 | Perplexity: 3192.060996
Trainning Epoch:  61%|██████    | 300/494 [88:55:34<63:54:55, 1186.06s/it]Trainning Epoch:  61%|██████    | 300/494 [88:55:34<63:54:55, 1186.06s/it]2025-10-12 03:32:40,126 Stage: Train 0.5 | Epoch: 273 | Iter: 316600 | Total Loss: 0.002513 | Recon Loss: 0.002146 | Commit Loss: 0.000734 | Perplexity: 3195.360470
2025-10-12 03:36:35,269 Stage: Train 0.5 | Epoch: 273 | Iter: 316800 | Total Loss: 0.002467 | Recon Loss: 0.002106 | Commit Loss: 0.000721 | Perplexity: 3193.295824
2025-10-12 03:40:30,330 Stage: Train 0.5 | Epoch: 273 | Iter: 317000 | Total Loss: 0.002456 | Recon Loss: 0.002095 | Commit Loss: 0.000723 | Perplexity: 3188.194237
2025-10-12 03:44:25,285 Stage: Train 0.5 | Epoch: 273 | Iter: 317200 | Total Loss: 0.002503 | Recon Loss: 0.002142 | Commit Loss: 0.000723 | Perplexity: 3195.049772
2025-10-12 03:48:20,224 Stage: Train 0.5 | Epoch: 273 | Iter: 317400 | Total Loss: 0.002446 | Recon Loss: 0.002079 | Commit Loss: 0.000734 | Perplexity: 3187.263412
Trainning Epoch:  61%|██████    | 301/494 [89:15:28<63:42:50, 1188.45s/it]Trainning Epoch:  61%|██████    | 301/494 [89:15:28<63:42:50, 1188.45s/it]2025-10-12 03:52:18,748 Stage: Train 0.5 | Epoch: 274 | Iter: 317600 | Total Loss: 0.002523 | Recon Loss: 0.002157 | Commit Loss: 0.000732 | Perplexity: 3201.358263
2025-10-12 03:56:14,208 Stage: Train 0.5 | Epoch: 274 | Iter: 317800 | Total Loss: 0.002468 | Recon Loss: 0.002104 | Commit Loss: 0.000727 | Perplexity: 3194.496289
2025-10-12 04:00:09,643 Stage: Train 0.5 | Epoch: 274 | Iter: 318000 | Total Loss: 0.002495 | Recon Loss: 0.002122 | Commit Loss: 0.000745 | Perplexity: 3201.257753
2025-10-12 04:04:05,729 Stage: Train 0.5 | Epoch: 274 | Iter: 318200 | Total Loss: 0.002448 | Recon Loss: 0.002086 | Commit Loss: 0.000725 | Perplexity: 3198.769677
2025-10-12 04:08:01,431 Stage: Train 0.5 | Epoch: 274 | Iter: 318400 | Total Loss: 0.002474 | Recon Loss: 0.002109 | Commit Loss: 0.000731 | Perplexity: 3184.419607
Trainning Epoch:  61%|██████    | 302/494 [89:35:25<63:31:11, 1191.00s/it]Trainning Epoch:  61%|██████    | 302/494 [89:35:25<63:31:12, 1191.00s/it]2025-10-12 04:12:00,253 Stage: Train 0.5 | Epoch: 275 | Iter: 318600 | Total Loss: 0.002450 | Recon Loss: 0.002092 | Commit Loss: 0.000716 | Perplexity: 3193.283943
2025-10-12 04:15:53,495 Stage: Train 0.5 | Epoch: 275 | Iter: 318800 | Total Loss: 0.002447 | Recon Loss: 0.002084 | Commit Loss: 0.000725 | Perplexity: 3195.274919
2025-10-12 04:19:47,072 Stage: Train 0.5 | Epoch: 275 | Iter: 319000 | Total Loss: 0.002474 | Recon Loss: 0.002115 | Commit Loss: 0.000719 | Perplexity: 3191.012959
2025-10-12 04:23:40,329 Stage: Train 0.5 | Epoch: 275 | Iter: 319200 | Total Loss: 0.002471 | Recon Loss: 0.002106 | Commit Loss: 0.000730 | Perplexity: 3193.210693
2025-10-12 04:27:33,931 Stage: Train 0.5 | Epoch: 275 | Iter: 319400 | Total Loss: 0.002501 | Recon Loss: 0.002136 | Commit Loss: 0.000730 | Perplexity: 3200.218813
Trainning Epoch:  61%|██████▏   | 303/494 [89:55:11<63:06:28, 1189.47s/it]Trainning Epoch:  61%|██████▏   | 303/494 [89:55:11<63:06:28, 1189.47s/it]2025-10-12 04:31:31,037 Stage: Train 0.5 | Epoch: 276 | Iter: 319600 | Total Loss: 0.002461 | Recon Loss: 0.002095 | Commit Loss: 0.000732 | Perplexity: 3193.104510
2025-10-12 04:35:26,505 Stage: Train 0.5 | Epoch: 276 | Iter: 319800 | Total Loss: 0.002490 | Recon Loss: 0.002128 | Commit Loss: 0.000724 | Perplexity: 3187.729069
2025-10-12 04:39:22,268 Stage: Train 0.5 | Epoch: 276 | Iter: 320000 | Total Loss: 0.002463 | Recon Loss: 0.002099 | Commit Loss: 0.000729 | Perplexity: 3193.389955
2025-10-12 04:39:22,268 Saving model at iteration 320000
2025-10-12 04:39:22,679 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000
2025-10-12 04:39:24,301 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000/model.safetensors
2025-10-12 04:39:26,037 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000/optimizer.bin
2025-10-12 04:39:26,038 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000/scheduler.bin
2025-10-12 04:39:26,038 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000/sampler.bin
2025-10-12 04:39:26,039 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_277_step_320000/random_states_0.pkl
2025-10-12 04:43:21,580 Stage: Train 0.5 | Epoch: 276 | Iter: 320200 | Total Loss: 0.002502 | Recon Loss: 0.002136 | Commit Loss: 0.000732 | Perplexity: 3192.505405
2025-10-12 04:47:17,408 Stage: Train 0.5 | Epoch: 276 | Iter: 320400 | Total Loss: 0.002456 | Recon Loss: 0.002090 | Commit Loss: 0.000733 | Perplexity: 3197.274795
2025-10-12 04:51:13,039 Stage: Train 0.5 | Epoch: 276 | Iter: 320600 | Total Loss: 0.002465 | Recon Loss: 0.002101 | Commit Loss: 0.000727 | Perplexity: 3192.684690
Trainning Epoch:  62%|██████▏   | 304/494 [90:15:11<62:57:17, 1192.83s/it]Trainning Epoch:  62%|██████▏   | 304/494 [90:15:11<62:57:17, 1192.83s/it]2025-10-12 04:55:07,906 Stage: Train 0.5 | Epoch: 277 | Iter: 320800 | Total Loss: 0.002460 | Recon Loss: 0.002104 | Commit Loss: 0.000712 | Perplexity: 3190.929668
2025-10-12 04:59:00,159 Stage: Train 0.5 | Epoch: 277 | Iter: 321000 | Total Loss: 0.002479 | Recon Loss: 0.002117 | Commit Loss: 0.000723 | Perplexity: 3200.915057
2025-10-12 05:02:52,585 Stage: Train 0.5 | Epoch: 277 | Iter: 321200 | Total Loss: 0.002475 | Recon Loss: 0.002106 | Commit Loss: 0.000737 | Perplexity: 3190.784343
2025-10-12 05:06:44,910 Stage: Train 0.5 | Epoch: 277 | Iter: 321400 | Total Loss: 0.002447 | Recon Loss: 0.002078 | Commit Loss: 0.000738 | Perplexity: 3203.549780
2025-10-12 05:10:37,347 Stage: Train 0.5 | Epoch: 277 | Iter: 321600 | Total Loss: 0.002499 | Recon Loss: 0.002131 | Commit Loss: 0.000736 | Perplexity: 3198.368186
Trainning Epoch:  62%|██████▏   | 305/494 [90:34:51<62:24:38, 1188.77s/it]Trainning Epoch:  62%|██████▏   | 305/494 [90:34:51<62:24:38, 1188.78s/it]2025-10-12 05:14:34,869 Stage: Train 0.5 | Epoch: 278 | Iter: 321800 | Total Loss: 0.002475 | Recon Loss: 0.002112 | Commit Loss: 0.000726 | Perplexity: 3196.917122
2025-10-12 05:18:29,997 Stage: Train 0.5 | Epoch: 278 | Iter: 322000 | Total Loss: 0.002483 | Recon Loss: 0.002117 | Commit Loss: 0.000731 | Perplexity: 3188.694521
2025-10-12 05:22:25,114 Stage: Train 0.5 | Epoch: 278 | Iter: 322200 | Total Loss: 0.002437 | Recon Loss: 0.002073 | Commit Loss: 0.000729 | Perplexity: 3184.495443
2025-10-12 05:26:20,320 Stage: Train 0.5 | Epoch: 278 | Iter: 322400 | Total Loss: 0.002472 | Recon Loss: 0.002112 | Commit Loss: 0.000720 | Perplexity: 3193.545585
2025-10-12 05:30:15,835 Stage: Train 0.5 | Epoch: 278 | Iter: 322600 | Total Loss: 0.002467 | Recon Loss: 0.002100 | Commit Loss: 0.000734 | Perplexity: 3207.787400
Trainning Epoch:  62%|██████▏   | 306/494 [90:54:45<62:09:38, 1190.31s/it]Trainning Epoch:  62%|██████▏   | 306/494 [90:54:45<62:09:38, 1190.31s/it]2025-10-12 05:34:13,808 Stage: Train 0.5 | Epoch: 279 | Iter: 322800 | Total Loss: 0.002458 | Recon Loss: 0.002096 | Commit Loss: 0.000725 | Perplexity: 3194.361431
2025-10-12 05:38:08,522 Stage: Train 0.5 | Epoch: 279 | Iter: 323000 | Total Loss: 0.002427 | Recon Loss: 0.002063 | Commit Loss: 0.000727 | Perplexity: 3203.140579
2025-10-12 05:42:03,490 Stage: Train 0.5 | Epoch: 279 | Iter: 323200 | Total Loss: 0.002459 | Recon Loss: 0.002095 | Commit Loss: 0.000729 | Perplexity: 3193.255742
2025-10-12 05:45:58,253 Stage: Train 0.5 | Epoch: 279 | Iter: 323400 | Total Loss: 0.002430 | Recon Loss: 0.002070 | Commit Loss: 0.000722 | Perplexity: 3195.790627
2025-10-12 05:49:53,241 Stage: Train 0.5 | Epoch: 279 | Iter: 323600 | Total Loss: 0.002490 | Recon Loss: 0.002123 | Commit Loss: 0.000735 | Perplexity: 3204.569315
Trainning Epoch:  62%|██████▏   | 307/494 [91:14:37<61:51:55, 1190.99s/it]Trainning Epoch:  62%|██████▏   | 307/494 [91:14:37<61:51:55, 1190.99s/it]2025-10-12 05:53:49,563 Stage: Train 0.5 | Epoch: 280 | Iter: 323800 | Total Loss: 0.002445 | Recon Loss: 0.002084 | Commit Loss: 0.000723 | Perplexity: 3196.460520
2025-10-12 05:57:43,925 Stage: Train 0.5 | Epoch: 280 | Iter: 324000 | Total Loss: 0.002469 | Recon Loss: 0.002105 | Commit Loss: 0.000730 | Perplexity: 3197.368394
2025-10-12 06:01:39,041 Stage: Train 0.5 | Epoch: 280 | Iter: 324200 | Total Loss: 0.002435 | Recon Loss: 0.002067 | Commit Loss: 0.000735 | Perplexity: 3192.547476
2025-10-12 06:05:33,761 Stage: Train 0.5 | Epoch: 280 | Iter: 324400 | Total Loss: 0.002471 | Recon Loss: 0.002109 | Commit Loss: 0.000725 | Perplexity: 3193.027466
2025-10-12 06:09:28,547 Stage: Train 0.5 | Epoch: 280 | Iter: 324600 | Total Loss: 0.002447 | Recon Loss: 0.002085 | Commit Loss: 0.000724 | Perplexity: 3189.587770
Trainning Epoch:  62%|██████▏   | 308/494 [91:34:27<61:31:28, 1190.80s/it]Trainning Epoch:  62%|██████▏   | 308/494 [91:34:27<61:31:28, 1190.80s/it]2025-10-12 06:13:26,232 Stage: Train 0.5 | Epoch: 281 | Iter: 324800 | Total Loss: 0.002508 | Recon Loss: 0.002143 | Commit Loss: 0.000729 | Perplexity: 3195.776909
2025-10-12 06:17:22,002 Stage: Train 0.5 | Epoch: 281 | Iter: 325000 | Total Loss: 0.002453 | Recon Loss: 0.002090 | Commit Loss: 0.000726 | Perplexity: 3192.892705
2025-10-12 06:21:18,021 Stage: Train 0.5 | Epoch: 281 | Iter: 325200 | Total Loss: 0.002467 | Recon Loss: 0.002106 | Commit Loss: 0.000722 | Perplexity: 3203.325532
2025-10-12 06:25:13,915 Stage: Train 0.5 | Epoch: 281 | Iter: 325400 | Total Loss: 0.002431 | Recon Loss: 0.002070 | Commit Loss: 0.000721 | Perplexity: 3187.860702
2025-10-12 06:29:09,573 Stage: Train 0.5 | Epoch: 281 | Iter: 325600 | Total Loss: 0.002479 | Recon Loss: 0.002112 | Commit Loss: 0.000732 | Perplexity: 3198.599008
Trainning Epoch:  63%|██████▎   | 309/494 [91:54:25<61:17:40, 1192.76s/it]Trainning Epoch:  63%|██████▎   | 309/494 [91:54:25<61:17:40, 1192.76s/it]2025-10-12 06:33:07,686 Stage: Train 0.5 | Epoch: 282 | Iter: 325800 | Total Loss: 0.002454 | Recon Loss: 0.002091 | Commit Loss: 0.000725 | Perplexity: 3188.311162
2025-10-12 06:37:02,071 Stage: Train 0.5 | Epoch: 282 | Iter: 326000 | Total Loss: 0.002500 | Recon Loss: 0.002133 | Commit Loss: 0.000733 | Perplexity: 3186.631683
2025-10-12 06:40:57,246 Stage: Train 0.5 | Epoch: 282 | Iter: 326200 | Total Loss: 0.002437 | Recon Loss: 0.002075 | Commit Loss: 0.000726 | Perplexity: 3189.746523
2025-10-12 06:44:52,379 Stage: Train 0.5 | Epoch: 282 | Iter: 326400 | Total Loss: 0.002442 | Recon Loss: 0.002077 | Commit Loss: 0.000729 | Perplexity: 3194.092733
2025-10-12 06:48:47,633 Stage: Train 0.5 | Epoch: 282 | Iter: 326600 | Total Loss: 0.002462 | Recon Loss: 0.002102 | Commit Loss: 0.000719 | Perplexity: 3203.122268
Trainning Epoch:  63%|██████▎   | 310/494 [92:14:17<60:57:28, 1192.65s/it]Trainning Epoch:  63%|██████▎   | 310/494 [92:14:17<60:57:28, 1192.66s/it]2025-10-12 06:52:42,365 Stage: Train 0.5 | Epoch: 283 | Iter: 326800 | Total Loss: 0.002469 | Recon Loss: 0.002104 | Commit Loss: 0.000730 | Perplexity: 3200.732069
2025-10-12 06:56:34,414 Stage: Train 0.5 | Epoch: 283 | Iter: 327000 | Total Loss: 0.002527 | Recon Loss: 0.002164 | Commit Loss: 0.000725 | Perplexity: 3195.170486
2025-10-12 07:00:27,662 Stage: Train 0.5 | Epoch: 283 | Iter: 327200 | Total Loss: 0.002416 | Recon Loss: 0.002048 | Commit Loss: 0.000736 | Perplexity: 3189.604996
2025-10-12 07:04:20,728 Stage: Train 0.5 | Epoch: 283 | Iter: 327400 | Total Loss: 0.002473 | Recon Loss: 0.002111 | Commit Loss: 0.000724 | Perplexity: 3203.891224
2025-10-12 07:08:13,507 Stage: Train 0.5 | Epoch: 283 | Iter: 327600 | Total Loss: 0.002470 | Recon Loss: 0.002106 | Commit Loss: 0.000727 | Perplexity: 3195.384713
Trainning Epoch:  63%|██████▎   | 311/494 [92:33:58<60:26:20, 1188.97s/it]Trainning Epoch:  63%|██████▎   | 311/494 [92:33:58<60:26:21, 1188.97s/it]2025-10-12 07:12:10,386 Stage: Train 0.5 | Epoch: 284 | Iter: 327800 | Total Loss: 0.002423 | Recon Loss: 0.002059 | Commit Loss: 0.000729 | Perplexity: 3194.367922
2025-10-12 07:16:04,645 Stage: Train 0.5 | Epoch: 284 | Iter: 328000 | Total Loss: 0.002446 | Recon Loss: 0.002084 | Commit Loss: 0.000725 | Perplexity: 3199.306520
2025-10-12 07:19:58,919 Stage: Train 0.5 | Epoch: 284 | Iter: 328200 | Total Loss: 0.002462 | Recon Loss: 0.002096 | Commit Loss: 0.000732 | Perplexity: 3198.180798
2025-10-12 07:23:53,130 Stage: Train 0.5 | Epoch: 284 | Iter: 328400 | Total Loss: 0.002412 | Recon Loss: 0.002049 | Commit Loss: 0.000725 | Perplexity: 3205.696921
2025-10-12 07:27:47,386 Stage: Train 0.5 | Epoch: 284 | Iter: 328600 | Total Loss: 0.002457 | Recon Loss: 0.002096 | Commit Loss: 0.000723 | Perplexity: 3201.740504
Trainning Epoch:  63%|██████▎   | 312/494 [92:53:47<60:07:21, 1189.24s/it]Trainning Epoch:  63%|██████▎   | 312/494 [92:53:47<60:07:21, 1189.24s/it]2025-10-12 07:31:45,417 Stage: Train 0.5 | Epoch: 285 | Iter: 328800 | Total Loss: 0.002460 | Recon Loss: 0.002095 | Commit Loss: 0.000731 | Perplexity: 3203.048945
2025-10-12 07:35:41,063 Stage: Train 0.5 | Epoch: 285 | Iter: 329000 | Total Loss: 0.002425 | Recon Loss: 0.002062 | Commit Loss: 0.000728 | Perplexity: 3198.572158
2025-10-12 07:39:37,115 Stage: Train 0.5 | Epoch: 285 | Iter: 329200 | Total Loss: 0.002465 | Recon Loss: 0.002102 | Commit Loss: 0.000727 | Perplexity: 3202.731718
2025-10-12 07:43:33,120 Stage: Train 0.5 | Epoch: 285 | Iter: 329400 | Total Loss: 0.002448 | Recon Loss: 0.002085 | Commit Loss: 0.000726 | Perplexity: 3186.629065
2025-10-12 07:47:29,478 Stage: Train 0.5 | Epoch: 285 | Iter: 329600 | Total Loss: 0.002454 | Recon Loss: 0.002092 | Commit Loss: 0.000725 | Perplexity: 3192.090397
Trainning Epoch:  63%|██████▎   | 313/494 [93:13:46<59:55:50, 1191.99s/it]Trainning Epoch:  63%|██████▎   | 313/494 [93:13:46<59:55:50, 1191.99s/it]2025-10-12 07:51:28,950 Stage: Train 0.5 | Epoch: 286 | Iter: 329800 | Total Loss: 0.002451 | Recon Loss: 0.002087 | Commit Loss: 0.000729 | Perplexity: 3201.054808
2025-10-12 07:55:24,908 Stage: Train 0.5 | Epoch: 286 | Iter: 330000 | Total Loss: 0.002474 | Recon Loss: 0.002112 | Commit Loss: 0.000725 | Perplexity: 3205.894413
2025-10-12 07:59:20,857 Stage: Train 0.5 | Epoch: 286 | Iter: 330200 | Total Loss: 0.002463 | Recon Loss: 0.002101 | Commit Loss: 0.000724 | Perplexity: 3186.463234
2025-10-12 08:03:17,124 Stage: Train 0.5 | Epoch: 286 | Iter: 330400 | Total Loss: 0.002432 | Recon Loss: 0.002068 | Commit Loss: 0.000728 | Perplexity: 3195.306180
2025-10-12 08:07:13,057 Stage: Train 0.5 | Epoch: 286 | Iter: 330600 | Total Loss: 0.002477 | Recon Loss: 0.002113 | Commit Loss: 0.000728 | Perplexity: 3196.220074
Trainning Epoch:  64%|██████▎   | 314/494 [93:33:45<59:42:11, 1194.07s/it]Trainning Epoch:  64%|██████▎   | 314/494 [93:33:45<59:42:11, 1194.07s/it]2025-10-12 08:11:10,925 Stage: Train 0.5 | Epoch: 287 | Iter: 330800 | Total Loss: 0.002433 | Recon Loss: 0.002073 | Commit Loss: 0.000720 | Perplexity: 3190.585554
2025-10-12 08:15:04,003 Stage: Train 0.5 | Epoch: 287 | Iter: 331000 | Total Loss: 0.002428 | Recon Loss: 0.002064 | Commit Loss: 0.000729 | Perplexity: 3196.662584
2025-10-12 08:18:58,169 Stage: Train 0.5 | Epoch: 287 | Iter: 331200 | Total Loss: 0.002468 | Recon Loss: 0.002101 | Commit Loss: 0.000734 | Perplexity: 3197.556353
2025-10-12 08:22:52,033 Stage: Train 0.5 | Epoch: 287 | Iter: 331400 | Total Loss: 0.002465 | Recon Loss: 0.002100 | Commit Loss: 0.000730 | Perplexity: 3191.859790
2025-10-12 08:26:45,492 Stage: Train 0.5 | Epoch: 287 | Iter: 331600 | Total Loss: 0.002470 | Recon Loss: 0.002108 | Commit Loss: 0.000725 | Perplexity: 3208.705972
Trainning Epoch:  64%|██████▍   | 315/494 [93:53:31<59:15:03, 1191.64s/it]Trainning Epoch:  64%|██████▍   | 315/494 [93:53:31<59:15:04, 1191.64s/it]2025-10-12 08:30:41,710 Stage: Train 0.5 | Epoch: 288 | Iter: 331800 | Total Loss: 0.002472 | Recon Loss: 0.002112 | Commit Loss: 0.000721 | Perplexity: 3194.988481
2025-10-12 08:34:32,759 Stage: Train 0.5 | Epoch: 288 | Iter: 332000 | Total Loss: 0.002411 | Recon Loss: 0.002049 | Commit Loss: 0.000724 | Perplexity: 3189.711255
2025-10-12 08:38:25,159 Stage: Train 0.5 | Epoch: 288 | Iter: 332200 | Total Loss: 0.002449 | Recon Loss: 0.002086 | Commit Loss: 0.000726 | Perplexity: 3202.395509
2025-10-12 08:42:17,458 Stage: Train 0.5 | Epoch: 288 | Iter: 332400 | Total Loss: 0.002453 | Recon Loss: 0.002082 | Commit Loss: 0.000741 | Perplexity: 3201.557704
2025-10-12 08:46:10,719 Stage: Train 0.5 | Epoch: 288 | Iter: 332600 | Total Loss: 0.002469 | Recon Loss: 0.002108 | Commit Loss: 0.000721 | Perplexity: 3194.404865
Trainning Epoch:  64%|██████▍   | 316/494 [94:13:11<58:45:02, 1188.22s/it]Trainning Epoch:  64%|██████▍   | 316/494 [94:13:11<58:45:03, 1188.22s/it]2025-10-12 08:50:07,982 Stage: Train 0.5 | Epoch: 289 | Iter: 332800 | Total Loss: 0.002440 | Recon Loss: 0.002077 | Commit Loss: 0.000725 | Perplexity: 3207.466910
2025-10-12 08:54:03,690 Stage: Train 0.5 | Epoch: 289 | Iter: 333000 | Total Loss: 0.002419 | Recon Loss: 0.002057 | Commit Loss: 0.000724 | Perplexity: 3197.624440
2025-10-12 08:57:59,535 Stage: Train 0.5 | Epoch: 289 | Iter: 333200 | Total Loss: 0.002488 | Recon Loss: 0.002121 | Commit Loss: 0.000735 | Perplexity: 3204.276758
2025-10-12 09:01:55,793 Stage: Train 0.5 | Epoch: 289 | Iter: 333400 | Total Loss: 0.002443 | Recon Loss: 0.002082 | Commit Loss: 0.000723 | Perplexity: 3202.639448
2025-10-12 09:05:51,904 Stage: Train 0.5 | Epoch: 289 | Iter: 333600 | Total Loss: 0.002450 | Recon Loss: 0.002082 | Commit Loss: 0.000736 | Perplexity: 3203.478989
Trainning Epoch:  64%|██████▍   | 317/494 [94:33:09<58:34:16, 1191.28s/it]Trainning Epoch:  64%|██████▍   | 317/494 [94:33:09<58:34:16, 1191.28s/it]2025-10-12 09:09:51,181 Stage: Train 0.5 | Epoch: 290 | Iter: 333800 | Total Loss: 0.002418 | Recon Loss: 0.002053 | Commit Loss: 0.000730 | Perplexity: 3186.307847
2025-10-12 09:13:47,687 Stage: Train 0.5 | Epoch: 290 | Iter: 334000 | Total Loss: 0.002497 | Recon Loss: 0.002133 | Commit Loss: 0.000728 | Perplexity: 3197.070693
2025-10-12 09:17:43,687 Stage: Train 0.5 | Epoch: 290 | Iter: 334200 | Total Loss: 0.002407 | Recon Loss: 0.002045 | Commit Loss: 0.000723 | Perplexity: 3197.456620
2025-10-12 09:21:39,754 Stage: Train 0.5 | Epoch: 290 | Iter: 334400 | Total Loss: 0.002452 | Recon Loss: 0.002087 | Commit Loss: 0.000730 | Perplexity: 3203.941582
2025-10-12 09:25:35,289 Stage: Train 0.5 | Epoch: 290 | Iter: 334600 | Total Loss: 0.002424 | Recon Loss: 0.002059 | Commit Loss: 0.000729 | Perplexity: 3199.910367
Trainning Epoch:  64%|██████▍   | 318/494 [94:53:08<58:20:49, 1193.47s/it]Trainning Epoch:  64%|██████▍   | 318/494 [94:53:08<58:20:49, 1193.47s/it]2025-10-12 09:29:34,287 Stage: Train 0.5 | Epoch: 291 | Iter: 334800 | Total Loss: 0.002447 | Recon Loss: 0.002085 | Commit Loss: 0.000723 | Perplexity: 3202.263274
2025-10-12 09:33:30,256 Stage: Train 0.5 | Epoch: 291 | Iter: 335000 | Total Loss: 0.002413 | Recon Loss: 0.002052 | Commit Loss: 0.000722 | Perplexity: 3204.570972
2025-10-12 09:37:26,158 Stage: Train 0.5 | Epoch: 291 | Iter: 335200 | Total Loss: 0.002477 | Recon Loss: 0.002114 | Commit Loss: 0.000727 | Perplexity: 3202.144930
2025-10-12 09:41:22,515 Stage: Train 0.5 | Epoch: 291 | Iter: 335400 | Total Loss: 0.002409 | Recon Loss: 0.002042 | Commit Loss: 0.000735 | Perplexity: 3191.673688
2025-10-12 09:45:18,984 Stage: Train 0.5 | Epoch: 291 | Iter: 335600 | Total Loss: 0.002419 | Recon Loss: 0.002056 | Commit Loss: 0.000727 | Perplexity: 3210.040758
Trainning Epoch:  65%|██████▍   | 319/494 [95:13:07<58:06:12, 1195.27s/it]Trainning Epoch:  65%|██████▍   | 319/494 [95:13:07<58:06:11, 1195.27s/it]2025-10-12 09:49:18,352 Stage: Train 0.5 | Epoch: 292 | Iter: 335800 | Total Loss: 0.002424 | Recon Loss: 0.002057 | Commit Loss: 0.000734 | Perplexity: 3203.146232
2025-10-12 09:53:13,713 Stage: Train 0.5 | Epoch: 292 | Iter: 336000 | Total Loss: 0.002435 | Recon Loss: 0.002073 | Commit Loss: 0.000722 | Perplexity: 3195.084142
2025-10-12 09:57:09,404 Stage: Train 0.5 | Epoch: 292 | Iter: 336200 | Total Loss: 0.002444 | Recon Loss: 0.002081 | Commit Loss: 0.000725 | Perplexity: 3206.581802
2025-10-12 10:01:05,396 Stage: Train 0.5 | Epoch: 292 | Iter: 336400 | Total Loss: 0.002446 | Recon Loss: 0.002083 | Commit Loss: 0.000726 | Perplexity: 3197.310153
2025-10-12 10:05:01,474 Stage: Train 0.5 | Epoch: 292 | Iter: 336600 | Total Loss: 0.002439 | Recon Loss: 0.002076 | Commit Loss: 0.000726 | Perplexity: 3203.309491
2025-10-12 10:08:57,773 Stage: Train 0.5 | Epoch: 292 | Iter: 336800 | Total Loss: 0.002457 | Recon Loss: 0.002088 | Commit Loss: 0.000739 | Perplexity: 3199.156167
Trainning Epoch:  65%|██████▍   | 320/494 [95:33:05<57:48:35, 1196.06s/it]Trainning Epoch:  65%|██████▍   | 320/494 [95:33:05<57:48:35, 1196.06s/it]2025-10-12 10:12:55,325 Stage: Train 0.5 | Epoch: 293 | Iter: 337000 | Total Loss: 0.002461 | Recon Loss: 0.002100 | Commit Loss: 0.000724 | Perplexity: 3207.799309
2025-10-12 10:16:49,841 Stage: Train 0.5 | Epoch: 293 | Iter: 337200 | Total Loss: 0.002425 | Recon Loss: 0.002058 | Commit Loss: 0.000733 | Perplexity: 3202.838059
2025-10-12 10:20:44,683 Stage: Train 0.5 | Epoch: 293 | Iter: 337400 | Total Loss: 0.002430 | Recon Loss: 0.002068 | Commit Loss: 0.000724 | Perplexity: 3191.303284
2025-10-12 10:24:39,588 Stage: Train 0.5 | Epoch: 293 | Iter: 337600 | Total Loss: 0.002446 | Recon Loss: 0.002084 | Commit Loss: 0.000723 | Perplexity: 3195.249711
2025-10-12 10:28:34,493 Stage: Train 0.5 | Epoch: 293 | Iter: 337800 | Total Loss: 0.002437 | Recon Loss: 0.002075 | Commit Loss: 0.000724 | Perplexity: 3199.519039
Trainning Epoch:  65%|██████▍   | 321/494 [95:52:57<57:24:58, 1194.79s/it]Trainning Epoch:  65%|██████▍   | 321/494 [95:52:57<57:24:58, 1194.79s/it]2025-10-12 10:32:32,760 Stage: Train 0.5 | Epoch: 294 | Iter: 338000 | Total Loss: 0.002395 | Recon Loss: 0.002026 | Commit Loss: 0.000739 | Perplexity: 3199.799434
2025-10-12 10:36:28,231 Stage: Train 0.5 | Epoch: 294 | Iter: 338200 | Total Loss: 0.002444 | Recon Loss: 0.002078 | Commit Loss: 0.000733 | Perplexity: 3206.760964
2025-10-12 10:40:23,177 Stage: Train 0.5 | Epoch: 294 | Iter: 338400 | Total Loss: 0.002410 | Recon Loss: 0.002049 | Commit Loss: 0.000723 | Perplexity: 3205.081813
2025-10-12 10:44:18,659 Stage: Train 0.5 | Epoch: 294 | Iter: 338600 | Total Loss: 0.002454 | Recon Loss: 0.002093 | Commit Loss: 0.000721 | Perplexity: 3195.684760
2025-10-12 10:48:14,631 Stage: Train 0.5 | Epoch: 294 | Iter: 338800 | Total Loss: 0.002446 | Recon Loss: 0.002081 | Commit Loss: 0.000728 | Perplexity: 3202.343809
Trainning Epoch:  65%|██████▌   | 322/494 [96:12:53<57:05:48, 1195.05s/it]Trainning Epoch:  65%|██████▌   | 322/494 [96:12:53<57:05:48, 1195.05s/it]2025-10-12 10:52:11,430 Stage: Train 0.5 | Epoch: 295 | Iter: 339000 | Total Loss: 0.002421 | Recon Loss: 0.002055 | Commit Loss: 0.000731 | Perplexity: 3205.614304
2025-10-12 10:56:05,465 Stage: Train 0.5 | Epoch: 295 | Iter: 339200 | Total Loss: 0.002606 | Recon Loss: 0.002101 | Commit Loss: 0.001010 | Perplexity: 3193.514573
2025-10-12 10:59:59,850 Stage: Train 0.5 | Epoch: 295 | Iter: 339400 | Total Loss: 0.002421 | Recon Loss: 0.002056 | Commit Loss: 0.000730 | Perplexity: 3201.583248
2025-10-12 11:03:53,817 Stage: Train 0.5 | Epoch: 295 | Iter: 339600 | Total Loss: 0.002434 | Recon Loss: 0.002069 | Commit Loss: 0.000729 | Perplexity: 3209.899911
2025-10-12 11:07:48,289 Stage: Train 0.5 | Epoch: 295 | Iter: 339800 | Total Loss: 0.002419 | Recon Loss: 0.002050 | Commit Loss: 0.000738 | Perplexity: 3208.014966
Trainning Epoch:  65%|██████▌   | 323/494 [96:32:41<56:40:19, 1193.09s/it]Trainning Epoch:  65%|██████▌   | 323/494 [96:32:41<56:40:19, 1193.10s/it]2025-10-12 11:11:46,525 Stage: Train 0.5 | Epoch: 296 | Iter: 340000 | Total Loss: 0.002425 | Recon Loss: 0.002060 | Commit Loss: 0.000730 | Perplexity: 3204.640734
2025-10-12 11:11:46,526 Saving model at iteration 340000
2025-10-12 11:11:47,062 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000
2025-10-12 11:11:48,451 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000/model.safetensors
2025-10-12 11:11:50,190 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000/optimizer.bin
2025-10-12 11:11:50,191 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000/scheduler.bin
2025-10-12 11:11:50,191 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000/sampler.bin
2025-10-12 11:11:50,192 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_297_step_340000/random_states_0.pkl
2025-10-12 11:15:45,666 Stage: Train 0.5 | Epoch: 296 | Iter: 340200 | Total Loss: 0.002455 | Recon Loss: 0.002084 | Commit Loss: 0.000742 | Perplexity: 3198.775409
2025-10-12 11:19:40,997 Stage: Train 0.5 | Epoch: 296 | Iter: 340400 | Total Loss: 0.002447 | Recon Loss: 0.002084 | Commit Loss: 0.000726 | Perplexity: 3207.788552
2025-10-12 11:23:36,244 Stage: Train 0.5 | Epoch: 296 | Iter: 340600 | Total Loss: 0.002416 | Recon Loss: 0.002052 | Commit Loss: 0.000729 | Perplexity: 3200.029121
2025-10-12 11:27:31,592 Stage: Train 0.5 | Epoch: 296 | Iter: 340800 | Total Loss: 0.002427 | Recon Loss: 0.002063 | Commit Loss: 0.000728 | Perplexity: 3200.258979
Trainning Epoch:  66%|██████▌   | 324/494 [96:52:41<56:25:37, 1194.92s/it]Trainning Epoch:  66%|██████▌   | 324/494 [96:52:41<56:25:37, 1194.92s/it]2025-10-12 11:31:30,010 Stage: Train 0.5 | Epoch: 297 | Iter: 341000 | Total Loss: 0.002442 | Recon Loss: 0.002083 | Commit Loss: 0.000718 | Perplexity: 3199.790610
2025-10-12 11:35:25,418 Stage: Train 0.5 | Epoch: 297 | Iter: 341200 | Total Loss: 0.002418 | Recon Loss: 0.002057 | Commit Loss: 0.000721 | Perplexity: 3204.680540
2025-10-12 11:39:20,647 Stage: Train 0.5 | Epoch: 297 | Iter: 341400 | Total Loss: 0.002404 | Recon Loss: 0.002041 | Commit Loss: 0.000726 | Perplexity: 3195.034351
2025-10-12 11:43:15,904 Stage: Train 0.5 | Epoch: 297 | Iter: 341600 | Total Loss: 0.002440 | Recon Loss: 0.002074 | Commit Loss: 0.000732 | Perplexity: 3201.169015
2025-10-12 11:47:11,272 Stage: Train 0.5 | Epoch: 297 | Iter: 341800 | Total Loss: 0.002424 | Recon Loss: 0.002058 | Commit Loss: 0.000732 | Perplexity: 3199.297423
Trainning Epoch:  66%|██████▌   | 325/494 [97:12:35<56:05:38, 1194.90s/it]Trainning Epoch:  66%|██████▌   | 325/494 [97:12:35<56:05:38, 1194.90s/it]2025-10-12 11:51:07,446 Stage: Train 0.5 | Epoch: 298 | Iter: 342000 | Total Loss: 0.002437 | Recon Loss: 0.002074 | Commit Loss: 0.000726 | Perplexity: 3192.679255
2025-10-12 11:54:59,816 Stage: Train 0.5 | Epoch: 298 | Iter: 342200 | Total Loss: 0.002440 | Recon Loss: 0.002079 | Commit Loss: 0.000722 | Perplexity: 3202.943723
2025-10-12 11:58:52,491 Stage: Train 0.5 | Epoch: 298 | Iter: 342400 | Total Loss: 0.002425 | Recon Loss: 0.002067 | Commit Loss: 0.000718 | Perplexity: 3199.539301
2025-10-12 12:02:45,679 Stage: Train 0.5 | Epoch: 298 | Iter: 342600 | Total Loss: 0.002445 | Recon Loss: 0.002083 | Commit Loss: 0.000724 | Perplexity: 3193.987117
2025-10-12 12:06:39,016 Stage: Train 0.5 | Epoch: 298 | Iter: 342800 | Total Loss: 0.002427 | Recon Loss: 0.002060 | Commit Loss: 0.000734 | Perplexity: 3195.622413
Trainning Epoch:  66%|██████▌   | 326/494 [97:32:17<55:34:43, 1190.97s/it]Trainning Epoch:  66%|██████▌   | 326/494 [97:32:17<55:34:43, 1190.97s/it]2025-10-12 12:10:36,829 Stage: Train 0.5 | Epoch: 299 | Iter: 343000 | Total Loss: 0.002415 | Recon Loss: 0.002052 | Commit Loss: 0.000724 | Perplexity: 3188.443979
2025-10-12 12:14:32,597 Stage: Train 0.5 | Epoch: 299 | Iter: 343200 | Total Loss: 0.002405 | Recon Loss: 0.002044 | Commit Loss: 0.000722 | Perplexity: 3192.488098
2025-10-12 12:18:28,112 Stage: Train 0.5 | Epoch: 299 | Iter: 343400 | Total Loss: 0.002439 | Recon Loss: 0.002072 | Commit Loss: 0.000733 | Perplexity: 3195.390448
2025-10-12 12:22:23,758 Stage: Train 0.5 | Epoch: 299 | Iter: 343600 | Total Loss: 0.002431 | Recon Loss: 0.002064 | Commit Loss: 0.000734 | Perplexity: 3195.428461
2025-10-12 12:26:19,603 Stage: Train 0.5 | Epoch: 299 | Iter: 343800 | Total Loss: 0.002441 | Recon Loss: 0.002076 | Commit Loss: 0.000729 | Perplexity: 3200.482300
Trainning Epoch:  66%|██████▌   | 327/494 [97:52:14<55:19:50, 1192.76s/it]Trainning Epoch:  66%|██████▌   | 327/494 [97:52:14<55:19:50, 1192.76s/it]2025-10-12 12:30:18,284 Stage: Train 0.5 | Epoch: 300 | Iter: 344000 | Total Loss: 0.002431 | Recon Loss: 0.002066 | Commit Loss: 0.000730 | Perplexity: 3194.797037
2025-10-12 12:34:12,137 Stage: Train 0.5 | Epoch: 300 | Iter: 344200 | Total Loss: 0.002416 | Recon Loss: 0.002053 | Commit Loss: 0.000725 | Perplexity: 3194.628611
2025-10-12 12:38:06,644 Stage: Train 0.5 | Epoch: 300 | Iter: 344400 | Total Loss: 0.002415 | Recon Loss: 0.002048 | Commit Loss: 0.000732 | Perplexity: 3202.512063
2025-10-12 12:42:01,320 Stage: Train 0.5 | Epoch: 300 | Iter: 344600 | Total Loss: 0.002432 | Recon Loss: 0.002066 | Commit Loss: 0.000731 | Perplexity: 3199.666865
2025-10-12 12:45:55,995 Stage: Train 0.5 | Epoch: 300 | Iter: 344800 | Total Loss: 0.002441 | Recon Loss: 0.002076 | Commit Loss: 0.000730 | Perplexity: 3195.490037
Trainning Epoch:  66%|██████▋   | 328/494 [98:12:05<54:58:33, 1192.25s/it]Trainning Epoch:  66%|██████▋   | 328/494 [98:12:05<54:58:33, 1192.25s/it]2025-10-12 12:49:54,256 Stage: Train 0.5 | Epoch: 301 | Iter: 345000 | Total Loss: 0.002427 | Recon Loss: 0.002066 | Commit Loss: 0.000721 | Perplexity: 3204.393007
2025-10-12 12:53:49,884 Stage: Train 0.5 | Epoch: 301 | Iter: 345200 | Total Loss: 0.002417 | Recon Loss: 0.002052 | Commit Loss: 0.000731 | Perplexity: 3195.294998
2025-10-12 12:57:45,425 Stage: Train 0.5 | Epoch: 301 | Iter: 345400 | Total Loss: 0.002438 | Recon Loss: 0.002075 | Commit Loss: 0.000726 | Perplexity: 3196.017780
2025-10-12 13:01:40,875 Stage: Train 0.5 | Epoch: 301 | Iter: 345600 | Total Loss: 0.002427 | Recon Loss: 0.002063 | Commit Loss: 0.000728 | Perplexity: 3210.416738
2025-10-12 13:05:36,437 Stage: Train 0.5 | Epoch: 301 | Iter: 345800 | Total Loss: 0.002440 | Recon Loss: 0.002077 | Commit Loss: 0.000725 | Perplexity: 3208.162111
Trainning Epoch:  67%|██████▋   | 329/494 [98:32:02<54:42:10, 1193.52s/it]Trainning Epoch:  67%|██████▋   | 329/494 [98:32:02<54:42:11, 1193.52s/it]2025-10-12 13:09:34,698 Stage: Train 0.5 | Epoch: 302 | Iter: 346000 | Total Loss: 0.002419 | Recon Loss: 0.002059 | Commit Loss: 0.000720 | Perplexity: 3199.428138
2025-10-12 13:13:30,445 Stage: Train 0.5 | Epoch: 302 | Iter: 346200 | Total Loss: 0.002435 | Recon Loss: 0.002069 | Commit Loss: 0.000734 | Perplexity: 3209.612300
2025-10-12 13:17:26,419 Stage: Train 0.5 | Epoch: 302 | Iter: 346400 | Total Loss: 0.002460 | Recon Loss: 0.002098 | Commit Loss: 0.000723 | Perplexity: 3200.543766
2025-10-12 13:21:22,634 Stage: Train 0.5 | Epoch: 302 | Iter: 346600 | Total Loss: 0.002417 | Recon Loss: 0.002052 | Commit Loss: 0.000728 | Perplexity: 3210.247146
2025-10-12 13:25:18,377 Stage: Train 0.5 | Epoch: 302 | Iter: 346800 | Total Loss: 0.002404 | Recon Loss: 0.002040 | Commit Loss: 0.000727 | Perplexity: 3192.521488
Trainning Epoch:  67%|██████▋   | 330/494 [98:51:59<54:25:29, 1194.69s/it]Trainning Epoch:  67%|██████▋   | 330/494 [98:51:59<54:25:29, 1194.69s/it]2025-10-12 13:29:16,645 Stage: Train 0.5 | Epoch: 303 | Iter: 347000 | Total Loss: 0.002409 | Recon Loss: 0.002045 | Commit Loss: 0.000728 | Perplexity: 3202.637448
2025-10-12 13:33:11,431 Stage: Train 0.5 | Epoch: 303 | Iter: 347200 | Total Loss: 0.002456 | Recon Loss: 0.002088 | Commit Loss: 0.000737 | Perplexity: 3212.182397
2025-10-12 13:37:06,156 Stage: Train 0.5 | Epoch: 303 | Iter: 347400 | Total Loss: 0.002412 | Recon Loss: 0.002054 | Commit Loss: 0.000717 | Perplexity: 3192.713824
2025-10-12 13:41:01,214 Stage: Train 0.5 | Epoch: 303 | Iter: 347600 | Total Loss: 0.002424 | Recon Loss: 0.002061 | Commit Loss: 0.000727 | Perplexity: 3199.859063
2025-10-12 13:44:56,578 Stage: Train 0.5 | Epoch: 303 | Iter: 347800 | Total Loss: 0.002438 | Recon Loss: 0.002070 | Commit Loss: 0.000736 | Perplexity: 3205.065486
Trainning Epoch:  67%|██████▋   | 331/494 [99:11:52<54:04:07, 1194.16s/it]Trainning Epoch:  67%|██████▋   | 331/494 [99:11:52<54:04:07, 1194.16s/it]2025-10-12 13:48:53,874 Stage: Train 0.5 | Epoch: 304 | Iter: 348000 | Total Loss: 0.002423 | Recon Loss: 0.002061 | Commit Loss: 0.000723 | Perplexity: 3203.122065
2025-10-12 13:52:47,405 Stage: Train 0.5 | Epoch: 304 | Iter: 348200 | Total Loss: 0.002441 | Recon Loss: 0.002078 | Commit Loss: 0.000725 | Perplexity: 3199.863857
2025-10-12 13:56:40,894 Stage: Train 0.5 | Epoch: 304 | Iter: 348400 | Total Loss: 0.002410 | Recon Loss: 0.002046 | Commit Loss: 0.000727 | Perplexity: 3205.257478
2025-10-12 14:00:34,314 Stage: Train 0.5 | Epoch: 304 | Iter: 348600 | Total Loss: 0.002405 | Recon Loss: 0.002036 | Commit Loss: 0.000737 | Perplexity: 3206.548242
2025-10-12 14:04:27,875 Stage: Train 0.5 | Epoch: 304 | Iter: 348800 | Total Loss: 0.002429 | Recon Loss: 0.002066 | Commit Loss: 0.000726 | Perplexity: 3199.029537
Trainning Epoch:  67%|██████▋   | 332/494 [99:31:37<53:36:59, 1191.48s/it]Trainning Epoch:  67%|██████▋   | 332/494 [99:31:37<53:37:00, 1191.48s/it]2025-10-12 14:08:25,257 Stage: Train 0.5 | Epoch: 305 | Iter: 349000 | Total Loss: 0.002422 | Recon Loss: 0.002056 | Commit Loss: 0.000732 | Perplexity: 3208.563127
2025-10-12 14:12:20,497 Stage: Train 0.5 | Epoch: 305 | Iter: 349200 | Total Loss: 0.002426 | Recon Loss: 0.002063 | Commit Loss: 0.000727 | Perplexity: 3197.484197
2025-10-12 14:16:15,849 Stage: Train 0.5 | Epoch: 305 | Iter: 349400 | Total Loss: 0.002434 | Recon Loss: 0.002069 | Commit Loss: 0.000729 | Perplexity: 3198.567322
2025-10-12 14:20:11,204 Stage: Train 0.5 | Epoch: 305 | Iter: 349600 | Total Loss: 0.002416 | Recon Loss: 0.002050 | Commit Loss: 0.000732 | Perplexity: 3202.918143
2025-10-12 14:24:07,080 Stage: Train 0.5 | Epoch: 305 | Iter: 349800 | Total Loss: 0.002414 | Recon Loss: 0.002055 | Commit Loss: 0.000719 | Perplexity: 3205.448179
Trainning Epoch:  67%|██████▋   | 333/494 [99:51:34<53:21:22, 1193.06s/it]Trainning Epoch:  67%|██████▋   | 333/494 [99:51:34<53:21:22, 1193.06s/it]2025-10-12 14:28:06,052 Stage: Train 0.5 | Epoch: 306 | Iter: 350000 | Total Loss: 0.002405 | Recon Loss: 0.002045 | Commit Loss: 0.000720 | Perplexity: 3198.163850
2025-10-12 14:32:00,295 Stage: Train 0.5 | Epoch: 306 | Iter: 350200 | Total Loss: 0.002440 | Recon Loss: 0.002078 | Commit Loss: 0.000723 | Perplexity: 3199.581007
2025-10-12 14:35:54,924 Stage: Train 0.5 | Epoch: 306 | Iter: 350400 | Total Loss: 0.002421 | Recon Loss: 0.002059 | Commit Loss: 0.000724 | Perplexity: 3195.011191
2025-10-12 14:39:49,571 Stage: Train 0.5 | Epoch: 306 | Iter: 350600 | Total Loss: 0.002421 | Recon Loss: 0.002058 | Commit Loss: 0.000727 | Perplexity: 3207.625172
2025-10-12 14:43:44,693 Stage: Train 0.5 | Epoch: 306 | Iter: 350800 | Total Loss: 0.002402 | Recon Loss: 0.002040 | Commit Loss: 0.000724 | Perplexity: 3201.422179
Trainning Epoch:  68%|██████▊   | 334/494 [100:11:26<53:00:49, 1192.81s/it]Trainning Epoch:  68%|██████▊   | 334/494 [100:11:26<53:00:49, 1192.81s/it]2025-10-12 14:47:42,863 Stage: Train 0.5 | Epoch: 307 | Iter: 351000 | Total Loss: 0.002435 | Recon Loss: 0.002073 | Commit Loss: 0.000725 | Perplexity: 3206.694841
2025-10-12 14:51:37,082 Stage: Train 0.5 | Epoch: 307 | Iter: 351200 | Total Loss: 0.002429 | Recon Loss: 0.002068 | Commit Loss: 0.000722 | Perplexity: 3199.011471
2025-10-12 14:55:32,249 Stage: Train 0.5 | Epoch: 307 | Iter: 351400 | Total Loss: 0.002427 | Recon Loss: 0.002061 | Commit Loss: 0.000733 | Perplexity: 3200.302026
2025-10-12 14:59:27,137 Stage: Train 0.5 | Epoch: 307 | Iter: 351600 | Total Loss: 0.002412 | Recon Loss: 0.002045 | Commit Loss: 0.000734 | Perplexity: 3212.572660
2025-10-12 15:03:22,570 Stage: Train 0.5 | Epoch: 307 | Iter: 351800 | Total Loss: 0.002388 | Recon Loss: 0.002027 | Commit Loss: 0.000721 | Perplexity: 3205.369395
2025-10-12 15:07:17,604 Stage: Train 0.5 | Epoch: 307 | Iter: 352000 | Total Loss: 0.002419 | Recon Loss: 0.002055 | Commit Loss: 0.000727 | Perplexity: 3197.212144
Trainning Epoch:  68%|██████▊   | 335/494 [100:31:19<52:41:11, 1192.90s/it]Trainning Epoch:  68%|██████▊   | 335/494 [100:31:19<52:41:11, 1192.90s/it]2025-10-12 15:11:14,231 Stage: Train 0.5 | Epoch: 308 | Iter: 352200 | Total Loss: 0.002440 | Recon Loss: 0.002077 | Commit Loss: 0.000726 | Perplexity: 3213.871301
2025-10-12 15:15:09,261 Stage: Train 0.5 | Epoch: 308 | Iter: 352400 | Total Loss: 0.002420 | Recon Loss: 0.002058 | Commit Loss: 0.000724 | Perplexity: 3207.160491
2025-10-12 15:19:04,085 Stage: Train 0.5 | Epoch: 308 | Iter: 352600 | Total Loss: 0.002418 | Recon Loss: 0.002054 | Commit Loss: 0.000728 | Perplexity: 3202.455062
2025-10-12 15:22:58,956 Stage: Train 0.5 | Epoch: 308 | Iter: 352800 | Total Loss: 0.002406 | Recon Loss: 0.002044 | Commit Loss: 0.000724 | Perplexity: 3192.565540
2025-10-12 15:26:54,542 Stage: Train 0.5 | Epoch: 308 | Iter: 353000 | Total Loss: 0.002410 | Recon Loss: 0.002044 | Commit Loss: 0.000731 | Perplexity: 3218.905139
Trainning Epoch:  68%|██████▊   | 336/494 [100:51:11<52:20:34, 1192.62s/it]Trainning Epoch:  68%|██████▊   | 336/494 [100:51:11<52:20:34, 1192.62s/it]2025-10-12 15:30:51,050 Stage: Train 0.5 | Epoch: 309 | Iter: 353200 | Total Loss: 0.002415 | Recon Loss: 0.002050 | Commit Loss: 0.000730 | Perplexity: 3204.247507
2025-10-12 15:34:44,649 Stage: Train 0.5 | Epoch: 309 | Iter: 353400 | Total Loss: 0.002426 | Recon Loss: 0.002064 | Commit Loss: 0.000724 | Perplexity: 3197.930110
2025-10-12 15:38:38,890 Stage: Train 0.5 | Epoch: 309 | Iter: 353600 | Total Loss: 0.002395 | Recon Loss: 0.002035 | Commit Loss: 0.000720 | Perplexity: 3196.823464
2025-10-12 15:42:33,611 Stage: Train 0.5 | Epoch: 309 | Iter: 353800 | Total Loss: 0.002396 | Recon Loss: 0.002029 | Commit Loss: 0.000734 | Perplexity: 3200.944053
2025-10-12 15:46:27,760 Stage: Train 0.5 | Epoch: 309 | Iter: 354000 | Total Loss: 0.002413 | Recon Loss: 0.002048 | Commit Loss: 0.000732 | Perplexity: 3201.297963
Trainning Epoch:  68%|██████▊   | 337/494 [101:11:00<51:57:35, 1191.44s/it]Trainning Epoch:  68%|██████▊   | 337/494 [101:11:00<51:57:35, 1191.44s/it]2025-10-12 15:50:23,967 Stage: Train 0.5 | Epoch: 310 | Iter: 354200 | Total Loss: 0.002391 | Recon Loss: 0.002029 | Commit Loss: 0.000722 | Perplexity: 3207.330249
2025-10-12 15:54:17,255 Stage: Train 0.5 | Epoch: 310 | Iter: 354400 | Total Loss: 0.002408 | Recon Loss: 0.002040 | Commit Loss: 0.000736 | Perplexity: 3214.603932
2025-10-12 15:58:10,638 Stage: Train 0.5 | Epoch: 310 | Iter: 354600 | Total Loss: 0.002408 | Recon Loss: 0.002046 | Commit Loss: 0.000725 | Perplexity: 3210.633815
2025-10-12 16:02:04,138 Stage: Train 0.5 | Epoch: 310 | Iter: 354800 | Total Loss: 0.002424 | Recon Loss: 0.002056 | Commit Loss: 0.000737 | Perplexity: 3197.765488
2025-10-12 16:05:57,032 Stage: Train 0.5 | Epoch: 310 | Iter: 355000 | Total Loss: 0.002411 | Recon Loss: 0.002047 | Commit Loss: 0.000727 | Perplexity: 3205.962657
Trainning Epoch:  68%|██████▊   | 338/494 [101:30:44<51:32:00, 1189.24s/it]Trainning Epoch:  68%|██████▊   | 338/494 [101:30:44<51:32:01, 1189.24s/it]2025-10-12 16:09:54,225 Stage: Train 0.5 | Epoch: 311 | Iter: 355200 | Total Loss: 0.002415 | Recon Loss: 0.002047 | Commit Loss: 0.000736 | Perplexity: 3214.878866
2025-10-12 16:13:48,985 Stage: Train 0.5 | Epoch: 311 | Iter: 355400 | Total Loss: 0.002423 | Recon Loss: 0.002061 | Commit Loss: 0.000725 | Perplexity: 3208.565857
2025-10-12 16:17:43,898 Stage: Train 0.5 | Epoch: 311 | Iter: 355600 | Total Loss: 0.002410 | Recon Loss: 0.002048 | Commit Loss: 0.000723 | Perplexity: 3193.233718
2025-10-12 16:21:38,581 Stage: Train 0.5 | Epoch: 311 | Iter: 355800 | Total Loss: 0.002432 | Recon Loss: 0.002067 | Commit Loss: 0.000731 | Perplexity: 3196.845048
2025-10-12 16:25:34,004 Stage: Train 0.5 | Epoch: 311 | Iter: 356000 | Total Loss: 0.002443 | Recon Loss: 0.002079 | Commit Loss: 0.000728 | Perplexity: 3201.139314
Trainning Epoch:  69%|██████▊   | 339/494 [101:50:37<51:14:39, 1190.19s/it]Trainning Epoch:  69%|██████▊   | 339/494 [101:50:37<51:14:40, 1190.19s/it]2025-10-12 16:29:31,000 Stage: Train 0.5 | Epoch: 312 | Iter: 356200 | Total Loss: 0.002371 | Recon Loss: 0.002010 | Commit Loss: 0.000721 | Perplexity: 3205.507563
2025-10-12 16:33:25,016 Stage: Train 0.5 | Epoch: 312 | Iter: 356400 | Total Loss: 0.002407 | Recon Loss: 0.002045 | Commit Loss: 0.000725 | Perplexity: 3208.845564
2025-10-12 16:37:19,184 Stage: Train 0.5 | Epoch: 312 | Iter: 356600 | Total Loss: 0.002422 | Recon Loss: 0.002058 | Commit Loss: 0.000728 | Perplexity: 3204.024796
2025-10-12 16:41:13,454 Stage: Train 0.5 | Epoch: 312 | Iter: 356800 | Total Loss: 0.002402 | Recon Loss: 0.002035 | Commit Loss: 0.000734 | Perplexity: 3207.283816
2025-10-12 16:45:08,210 Stage: Train 0.5 | Epoch: 312 | Iter: 357000 | Total Loss: 0.002432 | Recon Loss: 0.002068 | Commit Loss: 0.000729 | Perplexity: 3208.764039
Trainning Epoch:  69%|██████▉   | 340/494 [102:10:26<50:54:16, 1189.97s/it]Trainning Epoch:  69%|██████▉   | 340/494 [102:10:26<50:54:16, 1189.98s/it]2025-10-12 16:49:06,222 Stage: Train 0.5 | Epoch: 313 | Iter: 357200 | Total Loss: 0.002375 | Recon Loss: 0.002011 | Commit Loss: 0.000728 | Perplexity: 3196.581243
2025-10-12 16:53:00,509 Stage: Train 0.5 | Epoch: 313 | Iter: 357400 | Total Loss: 0.002395 | Recon Loss: 0.002034 | Commit Loss: 0.000721 | Perplexity: 3204.150479
2025-10-12 16:56:55,063 Stage: Train 0.5 | Epoch: 313 | Iter: 357600 | Total Loss: 0.002396 | Recon Loss: 0.002034 | Commit Loss: 0.000723 | Perplexity: 3206.737919
2025-10-12 17:00:49,556 Stage: Train 0.5 | Epoch: 313 | Iter: 357800 | Total Loss: 0.002398 | Recon Loss: 0.002032 | Commit Loss: 0.000731 | Perplexity: 3198.356768
2025-10-12 17:04:44,101 Stage: Train 0.5 | Epoch: 313 | Iter: 358000 | Total Loss: 0.002417 | Recon Loss: 0.002053 | Commit Loss: 0.000728 | Perplexity: 3206.354601
Trainning Epoch:  69%|██████▉   | 341/494 [102:30:17<50:35:08, 1190.25s/it]Trainning Epoch:  69%|██████▉   | 341/494 [102:30:17<50:35:08, 1190.25s/it]2025-10-12 17:08:40,852 Stage: Train 0.5 | Epoch: 314 | Iter: 358200 | Total Loss: 0.002409 | Recon Loss: 0.002042 | Commit Loss: 0.000733 | Perplexity: 3214.230211
2025-10-12 17:12:33,766 Stage: Train 0.5 | Epoch: 314 | Iter: 358400 | Total Loss: 0.002414 | Recon Loss: 0.002053 | Commit Loss: 0.000724 | Perplexity: 3212.709159
2025-10-12 17:16:27,518 Stage: Train 0.5 | Epoch: 314 | Iter: 358600 | Total Loss: 0.002391 | Recon Loss: 0.002024 | Commit Loss: 0.000733 | Perplexity: 3198.897313
2025-10-12 17:20:20,988 Stage: Train 0.5 | Epoch: 314 | Iter: 358800 | Total Loss: 0.002379 | Recon Loss: 0.002016 | Commit Loss: 0.000726 | Perplexity: 3201.628530
2025-10-12 17:24:15,072 Stage: Train 0.5 | Epoch: 314 | Iter: 359000 | Total Loss: 0.002436 | Recon Loss: 0.002073 | Commit Loss: 0.000725 | Perplexity: 3205.715889
Trainning Epoch:  69%|██████▉   | 342/494 [102:50:03<50:12:10, 1189.01s/it]Trainning Epoch:  69%|██████▉   | 342/494 [102:50:03<50:12:10, 1189.01s/it]2025-10-12 17:28:10,230 Stage: Train 0.5 | Epoch: 315 | Iter: 359200 | Total Loss: 0.002406 | Recon Loss: 0.002040 | Commit Loss: 0.000732 | Perplexity: 3211.577825
2025-10-12 17:32:00,707 Stage: Train 0.5 | Epoch: 315 | Iter: 359400 | Total Loss: 0.002413 | Recon Loss: 0.002048 | Commit Loss: 0.000731 | Perplexity: 3207.549761
2025-10-12 17:35:51,402 Stage: Train 0.5 | Epoch: 315 | Iter: 359600 | Total Loss: 0.002434 | Recon Loss: 0.002075 | Commit Loss: 0.000718 | Perplexity: 3209.917534
2025-10-12 17:39:42,129 Stage: Train 0.5 | Epoch: 315 | Iter: 359800 | Total Loss: 0.002365 | Recon Loss: 0.002000 | Commit Loss: 0.000729 | Perplexity: 3195.466825
2025-10-12 17:43:32,734 Stage: Train 0.5 | Epoch: 315 | Iter: 360000 | Total Loss: 0.002384 | Recon Loss: 0.002019 | Commit Loss: 0.000731 | Perplexity: 3201.794576
2025-10-12 17:43:32,735 Saving model at iteration 360000
2025-10-12 17:43:32,918 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000
2025-10-12 17:43:34,338 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000/model.safetensors
2025-10-12 17:43:36,090 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000/optimizer.bin
2025-10-12 17:43:36,090 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000/scheduler.bin
2025-10-12 17:43:36,090 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000/sampler.bin
2025-10-12 17:43:36,092 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_316_step_360000/random_states_0.pkl
Trainning Epoch:  69%|██████▉   | 343/494 [103:09:38<49:41:38, 1184.76s/it]Trainning Epoch:  69%|██████▉   | 343/494 [103:09:38<49:41:38, 1184.76s/it]2025-10-12 17:47:30,340 Stage: Train 0.5 | Epoch: 316 | Iter: 360200 | Total Loss: 0.002400 | Recon Loss: 0.002036 | Commit Loss: 0.000728 | Perplexity: 3200.341825
2025-10-12 17:51:23,115 Stage: Train 0.5 | Epoch: 316 | Iter: 360400 | Total Loss: 0.002405 | Recon Loss: 0.002043 | Commit Loss: 0.000723 | Perplexity: 3211.087280
2025-10-12 17:55:16,586 Stage: Train 0.5 | Epoch: 316 | Iter: 360600 | Total Loss: 0.002416 | Recon Loss: 0.002053 | Commit Loss: 0.000725 | Perplexity: 3201.079052
2025-10-12 17:59:09,994 Stage: Train 0.5 | Epoch: 316 | Iter: 360800 | Total Loss: 0.002398 | Recon Loss: 0.002036 | Commit Loss: 0.000726 | Perplexity: 3199.575551
2025-10-12 18:03:03,632 Stage: Train 0.5 | Epoch: 316 | Iter: 361000 | Total Loss: 0.002400 | Recon Loss: 0.002036 | Commit Loss: 0.000729 | Perplexity: 3198.416422
Trainning Epoch:  70%|██████▉   | 344/494 [103:29:22<49:21:28, 1184.59s/it]Trainning Epoch:  70%|██████▉   | 344/494 [103:29:22<49:21:28, 1184.59s/it]2025-10-12 18:07:00,627 Stage: Train 0.5 | Epoch: 317 | Iter: 361200 | Total Loss: 0.002414 | Recon Loss: 0.002051 | Commit Loss: 0.000726 | Perplexity: 3202.984943
2025-10-12 18:10:54,107 Stage: Train 0.5 | Epoch: 317 | Iter: 361400 | Total Loss: 0.002375 | Recon Loss: 0.002012 | Commit Loss: 0.000725 | Perplexity: 3203.572321
2025-10-12 18:14:47,662 Stage: Train 0.5 | Epoch: 317 | Iter: 361600 | Total Loss: 0.002392 | Recon Loss: 0.002029 | Commit Loss: 0.000726 | Perplexity: 3209.604150
2025-10-12 18:18:41,218 Stage: Train 0.5 | Epoch: 317 | Iter: 361800 | Total Loss: 0.002400 | Recon Loss: 0.002037 | Commit Loss: 0.000726 | Perplexity: 3205.094186
2025-10-12 18:22:34,546 Stage: Train 0.5 | Epoch: 317 | Iter: 362000 | Total Loss: 0.002411 | Recon Loss: 0.002049 | Commit Loss: 0.000723 | Perplexity: 3216.763494
Trainning Epoch:  70%|██████▉   | 345/494 [103:49:08<49:03:02, 1185.12s/it]Trainning Epoch:  70%|██████▉   | 345/494 [103:49:08<49:03:02, 1185.12s/it]2025-10-12 18:26:31,457 Stage: Train 0.5 | Epoch: 318 | Iter: 362200 | Total Loss: 0.002387 | Recon Loss: 0.002022 | Commit Loss: 0.000730 | Perplexity: 3204.618292
2025-10-12 18:30:24,789 Stage: Train 0.5 | Epoch: 318 | Iter: 362400 | Total Loss: 0.002481 | Recon Loss: 0.002121 | Commit Loss: 0.000720 | Perplexity: 3201.232001
2025-10-12 18:34:19,170 Stage: Train 0.5 | Epoch: 318 | Iter: 362600 | Total Loss: 0.002397 | Recon Loss: 0.002036 | Commit Loss: 0.000723 | Perplexity: 3200.637694
2025-10-12 18:38:13,548 Stage: Train 0.5 | Epoch: 318 | Iter: 362800 | Total Loss: 0.002393 | Recon Loss: 0.002029 | Commit Loss: 0.000729 | Perplexity: 3198.375436
2025-10-12 18:42:08,327 Stage: Train 0.5 | Epoch: 318 | Iter: 363000 | Total Loss: 0.002388 | Recon Loss: 0.002023 | Commit Loss: 0.000730 | Perplexity: 3210.386189
Trainning Epoch:  70%|███████   | 346/494 [104:08:58<48:46:28, 1186.41s/it]Trainning Epoch:  70%|███████   | 346/494 [104:08:58<48:46:28, 1186.41s/it]2025-10-12 18:46:06,580 Stage: Train 0.5 | Epoch: 319 | Iter: 363200 | Total Loss: 0.002397 | Recon Loss: 0.002032 | Commit Loss: 0.000730 | Perplexity: 3199.990931
2025-10-12 18:50:02,506 Stage: Train 0.5 | Epoch: 319 | Iter: 363400 | Total Loss: 0.002380 | Recon Loss: 0.002018 | Commit Loss: 0.000725 | Perplexity: 3203.221975
2025-10-12 18:53:58,137 Stage: Train 0.5 | Epoch: 319 | Iter: 363600 | Total Loss: 0.002387 | Recon Loss: 0.002017 | Commit Loss: 0.000740 | Perplexity: 3206.062819
2025-10-12 18:57:54,109 Stage: Train 0.5 | Epoch: 319 | Iter: 363800 | Total Loss: 0.002410 | Recon Loss: 0.002045 | Commit Loss: 0.000730 | Perplexity: 3201.394628
2025-10-12 19:01:50,639 Stage: Train 0.5 | Epoch: 319 | Iter: 364000 | Total Loss: 0.002380 | Recon Loss: 0.002018 | Commit Loss: 0.000724 | Perplexity: 3200.426309
Trainning Epoch:  70%|███████   | 347/494 [104:28:57<48:35:47, 1190.12s/it]Trainning Epoch:  70%|███████   | 347/494 [104:28:57<48:35:48, 1190.13s/it]2025-10-12 19:05:49,782 Stage: Train 0.5 | Epoch: 320 | Iter: 364200 | Total Loss: 0.002388 | Recon Loss: 0.002022 | Commit Loss: 0.000731 | Perplexity: 3215.912166
2025-10-12 19:09:42,596 Stage: Train 0.5 | Epoch: 320 | Iter: 364400 | Total Loss: 0.002404 | Recon Loss: 0.002040 | Commit Loss: 0.000728 | Perplexity: 3199.302653
2025-10-12 19:13:35,505 Stage: Train 0.5 | Epoch: 320 | Iter: 364600 | Total Loss: 0.002394 | Recon Loss: 0.002032 | Commit Loss: 0.000724 | Perplexity: 3208.458350
2025-10-12 19:17:28,001 Stage: Train 0.5 | Epoch: 320 | Iter: 364800 | Total Loss: 0.002388 | Recon Loss: 0.002024 | Commit Loss: 0.000729 | Perplexity: 3211.229954
2025-10-12 19:21:20,423 Stage: Train 0.5 | Epoch: 320 | Iter: 365000 | Total Loss: 0.002409 | Recon Loss: 0.002043 | Commit Loss: 0.000733 | Perplexity: 3200.330106
Trainning Epoch:  70%|███████   | 348/494 [104:48:39<48:10:08, 1187.73s/it]Trainning Epoch:  70%|███████   | 348/494 [104:48:39<48:10:09, 1187.73s/it]2025-10-12 19:25:16,310 Stage: Train 0.5 | Epoch: 321 | Iter: 365200 | Total Loss: 0.002400 | Recon Loss: 0.002039 | Commit Loss: 0.000723 | Perplexity: 3203.239366
2025-10-12 19:29:10,375 Stage: Train 0.5 | Epoch: 321 | Iter: 365400 | Total Loss: 0.002370 | Recon Loss: 0.002006 | Commit Loss: 0.000728 | Perplexity: 3213.225367
2025-10-12 19:33:04,618 Stage: Train 0.5 | Epoch: 321 | Iter: 365600 | Total Loss: 0.002443 | Recon Loss: 0.002079 | Commit Loss: 0.000727 | Perplexity: 3206.069457
2025-10-12 19:36:58,870 Stage: Train 0.5 | Epoch: 321 | Iter: 365800 | Total Loss: 0.002364 | Recon Loss: 0.001999 | Commit Loss: 0.000729 | Perplexity: 3205.746151
2025-10-12 19:40:53,186 Stage: Train 0.5 | Epoch: 321 | Iter: 366000 | Total Loss: 0.002405 | Recon Loss: 0.002043 | Commit Loss: 0.000722 | Perplexity: 3197.047465
Trainning Epoch:  71%|███████   | 349/494 [105:08:28<47:51:14, 1188.10s/it]Trainning Epoch:  71%|███████   | 349/494 [105:08:28<47:51:14, 1188.10s/it]2025-10-12 19:44:50,179 Stage: Train 0.5 | Epoch: 322 | Iter: 366200 | Total Loss: 0.002388 | Recon Loss: 0.002027 | Commit Loss: 0.000721 | Perplexity: 3200.378236
2025-10-12 19:48:44,275 Stage: Train 0.5 | Epoch: 322 | Iter: 366400 | Total Loss: 0.002370 | Recon Loss: 0.002006 | Commit Loss: 0.000728 | Perplexity: 3196.826909
2025-10-12 19:52:38,676 Stage: Train 0.5 | Epoch: 322 | Iter: 366600 | Total Loss: 0.002370 | Recon Loss: 0.002008 | Commit Loss: 0.000722 | Perplexity: 3203.527075
2025-10-12 19:56:33,002 Stage: Train 0.5 | Epoch: 322 | Iter: 366800 | Total Loss: 0.002415 | Recon Loss: 0.002051 | Commit Loss: 0.000728 | Perplexity: 3200.525913
2025-10-12 20:00:27,548 Stage: Train 0.5 | Epoch: 322 | Iter: 367000 | Total Loss: 0.002384 | Recon Loss: 0.002022 | Commit Loss: 0.000726 | Perplexity: 3212.157825
Trainning Epoch:  71%|███████   | 350/494 [105:28:18<47:33:06, 1188.79s/it]Trainning Epoch:  71%|███████   | 350/494 [105:28:18<47:33:06, 1188.79s/it]2025-10-12 20:04:25,433 Stage: Train 0.5 | Epoch: 323 | Iter: 367200 | Total Loss: 0.002372 | Recon Loss: 0.002006 | Commit Loss: 0.000733 | Perplexity: 3215.810983
2025-10-12 20:08:21,031 Stage: Train 0.5 | Epoch: 323 | Iter: 367400 | Total Loss: 0.002372 | Recon Loss: 0.002011 | Commit Loss: 0.000721 | Perplexity: 3196.003514
2025-10-12 20:12:17,105 Stage: Train 0.5 | Epoch: 323 | Iter: 367600 | Total Loss: 0.002395 | Recon Loss: 0.002038 | Commit Loss: 0.000716 | Perplexity: 3204.869930
2025-10-12 20:16:12,876 Stage: Train 0.5 | Epoch: 323 | Iter: 367800 | Total Loss: 0.002419 | Recon Loss: 0.002050 | Commit Loss: 0.000736 | Perplexity: 3213.998251
2025-10-12 20:20:08,301 Stage: Train 0.5 | Epoch: 323 | Iter: 368000 | Total Loss: 0.002409 | Recon Loss: 0.002048 | Commit Loss: 0.000722 | Perplexity: 3202.860469
2025-10-12 20:24:04,132 Stage: Train 0.5 | Epoch: 323 | Iter: 368200 | Total Loss: 0.002412 | Recon Loss: 0.002046 | Commit Loss: 0.000733 | Perplexity: 3214.209659
Trainning Epoch:  71%|███████   | 351/494 [105:48:15<47:19:10, 1191.27s/it]Trainning Epoch:  71%|███████   | 351/494 [105:48:15<47:19:10, 1191.27s/it]2025-10-12 20:28:01,384 Stage: Train 0.5 | Epoch: 324 | Iter: 368400 | Total Loss: 0.002384 | Recon Loss: 0.002020 | Commit Loss: 0.000728 | Perplexity: 3206.742323
2025-10-12 20:31:55,738 Stage: Train 0.5 | Epoch: 324 | Iter: 368600 | Total Loss: 0.002365 | Recon Loss: 0.002000 | Commit Loss: 0.000729 | Perplexity: 3215.973711
2025-10-12 20:35:50,444 Stage: Train 0.5 | Epoch: 324 | Iter: 368800 | Total Loss: 0.002413 | Recon Loss: 0.002046 | Commit Loss: 0.000735 | Perplexity: 3202.328641
2025-10-12 20:39:45,273 Stage: Train 0.5 | Epoch: 324 | Iter: 369000 | Total Loss: 0.002411 | Recon Loss: 0.002045 | Commit Loss: 0.000732 | Perplexity: 3209.467690
2025-10-12 20:43:40,164 Stage: Train 0.5 | Epoch: 324 | Iter: 369200 | Total Loss: 0.002359 | Recon Loss: 0.001997 | Commit Loss: 0.000724 | Perplexity: 3204.426521
Trainning Epoch:  71%|███████▏  | 352/494 [106:08:06<46:59:11, 1191.21s/it]Trainning Epoch:  71%|███████▏  | 352/494 [106:08:06<46:59:11, 1191.21s/it]2025-10-12 20:47:34,073 Stage: Train 0.5 | Epoch: 325 | Iter: 369400 | Total Loss: 0.002370 | Recon Loss: 0.002005 | Commit Loss: 0.000730 | Perplexity: 3204.640966
2025-10-12 20:51:24,494 Stage: Train 0.5 | Epoch: 325 | Iter: 369600 | Total Loss: 0.002388 | Recon Loss: 0.002027 | Commit Loss: 0.000722 | Perplexity: 3205.667152
2025-10-12 20:55:15,574 Stage: Train 0.5 | Epoch: 325 | Iter: 369800 | Total Loss: 0.002399 | Recon Loss: 0.002034 | Commit Loss: 0.000730 | Perplexity: 3222.638733
2025-10-12 20:59:06,879 Stage: Train 0.5 | Epoch: 325 | Iter: 370000 | Total Loss: 0.002368 | Recon Loss: 0.002007 | Commit Loss: 0.000723 | Perplexity: 3210.503270
2025-10-12 21:02:58,478 Stage: Train 0.5 | Epoch: 325 | Iter: 370200 | Total Loss: 0.002357 | Recon Loss: 0.001992 | Commit Loss: 0.000730 | Perplexity: 3197.015081
Trainning Epoch:  71%|███████▏  | 353/494 [106:27:39<46:26:37, 1185.80s/it]Trainning Epoch:  71%|███████▏  | 353/494 [106:27:39<46:26:38, 1185.80s/it]2025-10-12 21:06:53,110 Stage: Train 0.5 | Epoch: 326 | Iter: 370400 | Total Loss: 0.002389 | Recon Loss: 0.002029 | Commit Loss: 0.000720 | Perplexity: 3203.190944
2025-10-12 21:10:45,325 Stage: Train 0.5 | Epoch: 326 | Iter: 370600 | Total Loss: 0.002426 | Recon Loss: 0.002065 | Commit Loss: 0.000723 | Perplexity: 3216.900465
2025-10-12 21:14:37,572 Stage: Train 0.5 | Epoch: 326 | Iter: 370800 | Total Loss: 0.002365 | Recon Loss: 0.002000 | Commit Loss: 0.000731 | Perplexity: 3210.735118
2025-10-12 21:18:30,783 Stage: Train 0.5 | Epoch: 326 | Iter: 371000 | Total Loss: 0.002405 | Recon Loss: 0.002039 | Commit Loss: 0.000731 | Perplexity: 3201.629636
2025-10-12 21:22:23,764 Stage: Train 0.5 | Epoch: 326 | Iter: 371200 | Total Loss: 0.002372 | Recon Loss: 0.002007 | Commit Loss: 0.000730 | Perplexity: 3213.380107
Trainning Epoch:  72%|███████▏  | 354/494 [106:47:20<46:03:24, 1184.32s/it]Trainning Epoch:  72%|███████▏  | 354/494 [106:47:20<46:03:24, 1184.32s/it]2025-10-12 21:26:20,571 Stage: Train 0.5 | Epoch: 327 | Iter: 371400 | Total Loss: 0.002374 | Recon Loss: 0.002007 | Commit Loss: 0.000733 | Perplexity: 3205.297327
2025-10-12 21:30:14,700 Stage: Train 0.5 | Epoch: 327 | Iter: 371600 | Total Loss: 0.002374 | Recon Loss: 0.002015 | Commit Loss: 0.000719 | Perplexity: 3206.153855
2025-10-12 21:34:08,963 Stage: Train 0.5 | Epoch: 327 | Iter: 371800 | Total Loss: 0.002404 | Recon Loss: 0.002039 | Commit Loss: 0.000730 | Perplexity: 3199.360850
2025-10-12 21:38:03,032 Stage: Train 0.5 | Epoch: 327 | Iter: 372000 | Total Loss: 0.002388 | Recon Loss: 0.002027 | Commit Loss: 0.000722 | Perplexity: 3204.997869
2025-10-12 21:41:57,060 Stage: Train 0.5 | Epoch: 327 | Iter: 372200 | Total Loss: 0.002380 | Recon Loss: 0.002014 | Commit Loss: 0.000732 | Perplexity: 3208.884154
Trainning Epoch:  72%|███████▏  | 355/494 [107:07:09<45:46:37, 1185.59s/it]Trainning Epoch:  72%|███████▏  | 355/494 [107:07:09<45:46:37, 1185.59s/it]2025-10-12 21:45:54,521 Stage: Train 0.5 | Epoch: 328 | Iter: 372400 | Total Loss: 0.002396 | Recon Loss: 0.002032 | Commit Loss: 0.000727 | Perplexity: 3208.500292
2025-10-12 21:49:49,948 Stage: Train 0.5 | Epoch: 328 | Iter: 372600 | Total Loss: 0.002380 | Recon Loss: 0.002015 | Commit Loss: 0.000730 | Perplexity: 3211.665400
2025-10-12 21:53:45,165 Stage: Train 0.5 | Epoch: 328 | Iter: 372800 | Total Loss: 0.002361 | Recon Loss: 0.001998 | Commit Loss: 0.000726 | Perplexity: 3207.050424
2025-10-12 21:57:40,576 Stage: Train 0.5 | Epoch: 328 | Iter: 373000 | Total Loss: 0.002372 | Recon Loss: 0.002011 | Commit Loss: 0.000724 | Perplexity: 3208.812512
2025-10-12 22:01:35,915 Stage: Train 0.5 | Epoch: 328 | Iter: 373200 | Total Loss: 0.002397 | Recon Loss: 0.002029 | Commit Loss: 0.000737 | Perplexity: 3192.791714
Trainning Epoch:  72%|███████▏  | 356/494 [107:27:03<45:33:03, 1188.29s/it]Trainning Epoch:  72%|███████▏  | 356/494 [107:27:03<45:33:03, 1188.29s/it]2025-10-12 22:05:33,232 Stage: Train 0.5 | Epoch: 329 | Iter: 373400 | Total Loss: 0.002399 | Recon Loss: 0.002037 | Commit Loss: 0.000723 | Perplexity: 3204.025032
2025-10-12 22:09:26,773 Stage: Train 0.5 | Epoch: 329 | Iter: 373600 | Total Loss: 0.002355 | Recon Loss: 0.001992 | Commit Loss: 0.000727 | Perplexity: 3202.018735
2025-10-12 22:13:20,579 Stage: Train 0.5 | Epoch: 329 | Iter: 373800 | Total Loss: 0.002371 | Recon Loss: 0.002005 | Commit Loss: 0.000731 | Perplexity: 3216.687002
2025-10-12 22:17:14,466 Stage: Train 0.5 | Epoch: 329 | Iter: 374000 | Total Loss: 0.002380 | Recon Loss: 0.002016 | Commit Loss: 0.000730 | Perplexity: 3207.717761
2025-10-12 22:21:08,087 Stage: Train 0.5 | Epoch: 329 | Iter: 374200 | Total Loss: 0.002389 | Recon Loss: 0.002026 | Commit Loss: 0.000726 | Perplexity: 3200.344844
Trainning Epoch:  72%|███████▏  | 357/494 [107:46:50<45:12:08, 1187.80s/it]Trainning Epoch:  72%|███████▏  | 357/494 [107:46:50<45:12:08, 1187.80s/it]2025-10-12 22:25:03,420 Stage: Train 0.5 | Epoch: 330 | Iter: 374400 | Total Loss: 0.002391 | Recon Loss: 0.002027 | Commit Loss: 0.000728 | Perplexity: 3209.123773
2025-10-12 22:28:55,004 Stage: Train 0.5 | Epoch: 330 | Iter: 374600 | Total Loss: 0.002355 | Recon Loss: 0.001994 | Commit Loss: 0.000722 | Perplexity: 3213.280938
2025-10-12 22:32:46,595 Stage: Train 0.5 | Epoch: 330 | Iter: 374800 | Total Loss: 0.002415 | Recon Loss: 0.002050 | Commit Loss: 0.000729 | Perplexity: 3209.947323
2025-10-12 22:36:38,109 Stage: Train 0.5 | Epoch: 330 | Iter: 375000 | Total Loss: 0.002371 | Recon Loss: 0.002008 | Commit Loss: 0.000725 | Perplexity: 3210.514695
2025-10-12 22:40:29,957 Stage: Train 0.5 | Epoch: 330 | Iter: 375200 | Total Loss: 0.002420 | Recon Loss: 0.002055 | Commit Loss: 0.000730 | Perplexity: 3209.437046
Trainning Epoch:  72%|███████▏  | 358/494 [108:06:26<44:44:29, 1184.34s/it]Trainning Epoch:  72%|███████▏  | 358/494 [108:06:26<44:44:30, 1184.34s/it]2025-10-12 22:44:25,190 Stage: Train 0.5 | Epoch: 331 | Iter: 375400 | Total Loss: 0.002387 | Recon Loss: 0.002024 | Commit Loss: 0.000726 | Perplexity: 3222.827241
2025-10-12 22:48:17,993 Stage: Train 0.5 | Epoch: 331 | Iter: 375600 | Total Loss: 0.002380 | Recon Loss: 0.002019 | Commit Loss: 0.000721 | Perplexity: 3207.071351
2025-10-12 22:52:11,127 Stage: Train 0.5 | Epoch: 331 | Iter: 375800 | Total Loss: 0.002378 | Recon Loss: 0.002016 | Commit Loss: 0.000725 | Perplexity: 3207.299825
2025-10-12 22:56:04,156 Stage: Train 0.5 | Epoch: 331 | Iter: 376000 | Total Loss: 0.002386 | Recon Loss: 0.002021 | Commit Loss: 0.000730 | Perplexity: 3209.550771
2025-10-12 22:59:56,990 Stage: Train 0.5 | Epoch: 331 | Iter: 376200 | Total Loss: 0.002395 | Recon Loss: 0.002028 | Commit Loss: 0.000733 | Perplexity: 3208.527699
Trainning Epoch:  73%|███████▎  | 359/494 [108:26:09<44:23:40, 1183.86s/it]Trainning Epoch:  73%|███████▎  | 359/494 [108:26:09<44:23:41, 1183.86s/it]2025-10-12 23:03:53,875 Stage: Train 0.5 | Epoch: 332 | Iter: 376400 | Total Loss: 0.002346 | Recon Loss: 0.001985 | Commit Loss: 0.000722 | Perplexity: 3208.858632
2025-10-12 23:07:48,673 Stage: Train 0.5 | Epoch: 332 | Iter: 376600 | Total Loss: 0.002396 | Recon Loss: 0.002035 | Commit Loss: 0.000723 | Perplexity: 3208.061886
2025-10-12 23:11:43,654 Stage: Train 0.5 | Epoch: 332 | Iter: 376800 | Total Loss: 0.002373 | Recon Loss: 0.002009 | Commit Loss: 0.000728 | Perplexity: 3211.997667
2025-10-12 23:15:38,533 Stage: Train 0.5 | Epoch: 332 | Iter: 377000 | Total Loss: 0.002374 | Recon Loss: 0.002006 | Commit Loss: 0.000736 | Perplexity: 3214.901113
2025-10-12 23:19:33,119 Stage: Train 0.5 | Epoch: 332 | Iter: 377200 | Total Loss: 0.002371 | Recon Loss: 0.002007 | Commit Loss: 0.000728 | Perplexity: 3214.219178
Trainning Epoch:  73%|███████▎  | 360/494 [108:46:02<44:09:43, 1186.45s/it]Trainning Epoch:  73%|███████▎  | 360/494 [108:46:02<44:09:43, 1186.44s/it]2025-10-12 23:23:31,012 Stage: Train 0.5 | Epoch: 333 | Iter: 377400 | Total Loss: 0.002396 | Recon Loss: 0.002025 | Commit Loss: 0.000742 | Perplexity: 3211.840593
2025-10-12 23:27:26,311 Stage: Train 0.5 | Epoch: 333 | Iter: 377600 | Total Loss: 0.002390 | Recon Loss: 0.002026 | Commit Loss: 0.000730 | Perplexity: 3207.126349
2025-10-12 23:31:21,925 Stage: Train 0.5 | Epoch: 333 | Iter: 377800 | Total Loss: 0.002394 | Recon Loss: 0.002030 | Commit Loss: 0.000727 | Perplexity: 3213.862754
2025-10-12 23:35:17,324 Stage: Train 0.5 | Epoch: 333 | Iter: 378000 | Total Loss: 0.002353 | Recon Loss: 0.001990 | Commit Loss: 0.000727 | Perplexity: 3211.152649
2025-10-12 23:39:12,986 Stage: Train 0.5 | Epoch: 333 | Iter: 378200 | Total Loss: 0.002375 | Recon Loss: 0.002008 | Commit Loss: 0.000735 | Perplexity: 3201.373956
Trainning Epoch:  73%|███████▎  | 361/494 [109:05:57<43:56:00, 1189.18s/it]Trainning Epoch:  73%|███████▎  | 361/494 [109:05:57<43:56:00, 1189.18s/it]2025-10-12 23:43:11,002 Stage: Train 0.5 | Epoch: 334 | Iter: 378400 | Total Loss: 0.002358 | Recon Loss: 0.001993 | Commit Loss: 0.000732 | Perplexity: 3207.542566
2025-10-12 23:47:05,010 Stage: Train 0.5 | Epoch: 334 | Iter: 378600 | Total Loss: 0.002409 | Recon Loss: 0.002045 | Commit Loss: 0.000728 | Perplexity: 3186.434318
2025-10-12 23:50:59,193 Stage: Train 0.5 | Epoch: 334 | Iter: 378800 | Total Loss: 0.002370 | Recon Loss: 0.002004 | Commit Loss: 0.000732 | Perplexity: 3207.187185
2025-10-12 23:54:53,793 Stage: Train 0.5 | Epoch: 334 | Iter: 379000 | Total Loss: 0.002369 | Recon Loss: 0.002008 | Commit Loss: 0.000721 | Perplexity: 3208.079271
2025-10-12 23:58:48,005 Stage: Train 0.5 | Epoch: 334 | Iter: 379200 | Total Loss: 0.002353 | Recon Loss: 0.001993 | Commit Loss: 0.000720 | Perplexity: 3204.800975
Trainning Epoch:  73%|███████▎  | 362/494 [109:25:47<43:36:20, 1189.24s/it]Trainning Epoch:  73%|███████▎  | 362/494 [109:25:47<43:36:19, 1189.24s/it]2025-10-13 00:02:44,918 Stage: Train 0.5 | Epoch: 335 | Iter: 379400 | Total Loss: 0.002377 | Recon Loss: 0.002013 | Commit Loss: 0.000728 | Perplexity: 3200.702939
2025-10-13 00:06:35,050 Stage: Train 0.5 | Epoch: 335 | Iter: 379600 | Total Loss: 0.002382 | Recon Loss: 0.002019 | Commit Loss: 0.000726 | Perplexity: 3212.105547
2025-10-13 00:10:25,967 Stage: Train 0.5 | Epoch: 335 | Iter: 379800 | Total Loss: 0.002408 | Recon Loss: 0.002046 | Commit Loss: 0.000725 | Perplexity: 3203.983225
2025-10-13 00:14:16,843 Stage: Train 0.5 | Epoch: 335 | Iter: 380000 | Total Loss: 0.002387 | Recon Loss: 0.002021 | Commit Loss: 0.000732 | Perplexity: 3214.959675
2025-10-13 00:14:16,843 Saving model at iteration 380000
2025-10-13 00:14:17,163 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000
2025-10-13 00:14:18,563 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000/model.safetensors
2025-10-13 00:14:20,318 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000/optimizer.bin
2025-10-13 00:14:20,319 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000/scheduler.bin
2025-10-13 00:14:20,319 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000/sampler.bin
2025-10-13 00:14:20,320 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_336_step_380000/random_states_0.pkl
2025-10-13 00:18:11,673 Stage: Train 0.5 | Epoch: 335 | Iter: 380200 | Total Loss: 0.002391 | Recon Loss: 0.002025 | Commit Loss: 0.000732 | Perplexity: 3209.783762
Trainning Epoch:  73%|███████▎  | 363/494 [109:45:23<43:08:12, 1185.44s/it]Trainning Epoch:  73%|███████▎  | 363/494 [109:45:23<43:08:11, 1185.43s/it]2025-10-13 00:22:07,077 Stage: Train 0.5 | Epoch: 336 | Iter: 380400 | Total Loss: 0.002383 | Recon Loss: 0.002019 | Commit Loss: 0.000729 | Perplexity: 3200.510299
2025-10-13 00:26:00,772 Stage: Train 0.5 | Epoch: 336 | Iter: 380600 | Total Loss: 0.002357 | Recon Loss: 0.001997 | Commit Loss: 0.000720 | Perplexity: 3206.342177
2025-10-13 00:29:55,012 Stage: Train 0.5 | Epoch: 336 | Iter: 380800 | Total Loss: 0.002386 | Recon Loss: 0.002026 | Commit Loss: 0.000720 | Perplexity: 3208.104271
2025-10-13 00:33:49,191 Stage: Train 0.5 | Epoch: 336 | Iter: 381000 | Total Loss: 0.002355 | Recon Loss: 0.001991 | Commit Loss: 0.000729 | Perplexity: 3216.326725
2025-10-13 00:37:43,414 Stage: Train 0.5 | Epoch: 336 | Iter: 381200 | Total Loss: 0.002374 | Recon Loss: 0.002009 | Commit Loss: 0.000731 | Perplexity: 3209.295436
Trainning Epoch:  74%|███████▎  | 364/494 [110:05:12<42:50:41, 1186.48s/it]Trainning Epoch:  74%|███████▎  | 364/494 [110:05:12<42:50:42, 1186.48s/it]2025-10-13 00:41:40,380 Stage: Train 0.5 | Epoch: 337 | Iter: 381400 | Total Loss: 0.002404 | Recon Loss: 0.002039 | Commit Loss: 0.000728 | Perplexity: 3211.898586
2025-10-13 00:45:35,049 Stage: Train 0.5 | Epoch: 337 | Iter: 381600 | Total Loss: 0.002374 | Recon Loss: 0.002010 | Commit Loss: 0.000728 | Perplexity: 3204.518030
2025-10-13 00:49:29,850 Stage: Train 0.5 | Epoch: 337 | Iter: 381800 | Total Loss: 0.002373 | Recon Loss: 0.002008 | Commit Loss: 0.000731 | Perplexity: 3214.512361
2025-10-13 00:53:24,660 Stage: Train 0.5 | Epoch: 337 | Iter: 382000 | Total Loss: 0.002354 | Recon Loss: 0.001987 | Commit Loss: 0.000735 | Perplexity: 3217.782727
2025-10-13 00:57:19,503 Stage: Train 0.5 | Epoch: 337 | Iter: 382200 | Total Loss: 0.002368 | Recon Loss: 0.002006 | Commit Loss: 0.000724 | Perplexity: 3206.359742
Trainning Epoch:  74%|███████▍  | 365/494 [110:25:04<42:34:42, 1188.24s/it]Trainning Epoch:  74%|███████▍  | 365/494 [110:25:04<42:34:42, 1188.24s/it]2025-10-13 01:01:17,507 Stage: Train 0.5 | Epoch: 338 | Iter: 382400 | Total Loss: 0.002345 | Recon Loss: 0.001979 | Commit Loss: 0.000733 | Perplexity: 3209.037234
2025-10-13 01:05:11,973 Stage: Train 0.5 | Epoch: 338 | Iter: 382600 | Total Loss: 0.002385 | Recon Loss: 0.002026 | Commit Loss: 0.000716 | Perplexity: 3211.747662
2025-10-13 01:09:06,748 Stage: Train 0.5 | Epoch: 338 | Iter: 382800 | Total Loss: 0.002314 | Recon Loss: 0.001956 | Commit Loss: 0.000716 | Perplexity: 3212.512322
2025-10-13 01:13:01,261 Stage: Train 0.5 | Epoch: 338 | Iter: 383000 | Total Loss: 0.002372 | Recon Loss: 0.002008 | Commit Loss: 0.000729 | Perplexity: 3223.553958
2025-10-13 01:16:55,645 Stage: Train 0.5 | Epoch: 338 | Iter: 383200 | Total Loss: 0.002398 | Recon Loss: 0.002034 | Commit Loss: 0.000727 | Perplexity: 3208.331458
2025-10-13 01:20:50,410 Stage: Train 0.5 | Epoch: 338 | Iter: 383400 | Total Loss: 0.002376 | Recon Loss: 0.002011 | Commit Loss: 0.000730 | Perplexity: 3209.519266
Trainning Epoch:  74%|███████▍  | 366/494 [110:44:56<42:16:52, 1189.16s/it]Trainning Epoch:  74%|███████▍  | 366/494 [110:44:56<42:16:52, 1189.16s/it]2025-10-13 01:24:48,677 Stage: Train 0.5 | Epoch: 339 | Iter: 383600 | Total Loss: 0.002371 | Recon Loss: 0.002012 | Commit Loss: 0.000719 | Perplexity: 3210.496974
2025-10-13 01:28:43,322 Stage: Train 0.5 | Epoch: 339 | Iter: 383800 | Total Loss: 0.002350 | Recon Loss: 0.001985 | Commit Loss: 0.000730 | Perplexity: 3218.833229
2025-10-13 01:32:37,959 Stage: Train 0.5 | Epoch: 339 | Iter: 384000 | Total Loss: 0.002365 | Recon Loss: 0.002002 | Commit Loss: 0.000728 | Perplexity: 3203.091866
2025-10-13 01:36:32,588 Stage: Train 0.5 | Epoch: 339 | Iter: 384200 | Total Loss: 0.002339 | Recon Loss: 0.001974 | Commit Loss: 0.000730 | Perplexity: 3218.669369
2025-10-13 01:40:27,447 Stage: Train 0.5 | Epoch: 339 | Iter: 384400 | Total Loss: 0.002413 | Recon Loss: 0.002051 | Commit Loss: 0.000723 | Perplexity: 3216.572242
Trainning Epoch:  74%|███████▍  | 367/494 [111:04:48<41:59:00, 1190.08s/it]Trainning Epoch:  74%|███████▍  | 367/494 [111:04:48<41:59:00, 1190.08s/it]2025-10-13 01:44:23,519 Stage: Train 0.5 | Epoch: 340 | Iter: 384600 | Total Loss: 0.002352 | Recon Loss: 0.001993 | Commit Loss: 0.000718 | Perplexity: 3212.487036
2025-10-13 01:48:17,032 Stage: Train 0.5 | Epoch: 340 | Iter: 384800 | Total Loss: 0.002340 | Recon Loss: 0.001981 | Commit Loss: 0.000718 | Perplexity: 3215.187644
2025-10-13 01:52:11,250 Stage: Train 0.5 | Epoch: 340 | Iter: 385000 | Total Loss: 0.002346 | Recon Loss: 0.001985 | Commit Loss: 0.000722 | Perplexity: 3225.645648
2025-10-13 01:56:05,322 Stage: Train 0.5 | Epoch: 340 | Iter: 385200 | Total Loss: 0.002365 | Recon Loss: 0.002000 | Commit Loss: 0.000730 | Perplexity: 3207.345490
2025-10-13 01:59:59,126 Stage: Train 0.5 | Epoch: 340 | Iter: 385400 | Total Loss: 0.002379 | Recon Loss: 0.002012 | Commit Loss: 0.000734 | Perplexity: 3212.570530
Trainning Epoch:  74%|███████▍  | 368/494 [111:24:35<41:37:10, 1189.13s/it]Trainning Epoch:  74%|███████▍  | 368/494 [111:24:35<41:37:10, 1189.13s/it]2025-10-13 02:03:56,075 Stage: Train 0.5 | Epoch: 341 | Iter: 385600 | Total Loss: 0.002369 | Recon Loss: 0.002007 | Commit Loss: 0.000722 | Perplexity: 3213.204635
2025-10-13 02:07:49,519 Stage: Train 0.5 | Epoch: 341 | Iter: 385800 | Total Loss: 0.002356 | Recon Loss: 0.001994 | Commit Loss: 0.000724 | Perplexity: 3212.502334
2025-10-13 02:11:43,481 Stage: Train 0.5 | Epoch: 341 | Iter: 386000 | Total Loss: 0.002386 | Recon Loss: 0.002024 | Commit Loss: 0.000726 | Perplexity: 3202.697844
2025-10-13 02:15:37,177 Stage: Train 0.5 | Epoch: 341 | Iter: 386200 | Total Loss: 0.002353 | Recon Loss: 0.001989 | Commit Loss: 0.000729 | Perplexity: 3213.788672
2025-10-13 02:19:30,876 Stage: Train 0.5 | Epoch: 341 | Iter: 386400 | Total Loss: 0.002366 | Recon Loss: 0.002001 | Commit Loss: 0.000729 | Perplexity: 3220.244973
Trainning Epoch:  75%|███████▍  | 369/494 [111:44:22<41:15:57, 1188.46s/it]Trainning Epoch:  75%|███████▍  | 369/494 [111:44:22<41:15:57, 1188.46s/it]2025-10-13 02:23:27,825 Stage: Train 0.5 | Epoch: 342 | Iter: 386600 | Total Loss: 0.002353 | Recon Loss: 0.001987 | Commit Loss: 0.000732 | Perplexity: 3204.415200
2025-10-13 02:27:22,090 Stage: Train 0.5 | Epoch: 342 | Iter: 386800 | Total Loss: 0.002350 | Recon Loss: 0.001985 | Commit Loss: 0.000730 | Perplexity: 3211.698690
2025-10-13 02:31:16,385 Stage: Train 0.5 | Epoch: 342 | Iter: 387000 | Total Loss: 0.002338 | Recon Loss: 0.001973 | Commit Loss: 0.000730 | Perplexity: 3221.046462
2025-10-13 02:35:10,549 Stage: Train 0.5 | Epoch: 342 | Iter: 387200 | Total Loss: 0.002367 | Recon Loss: 0.002004 | Commit Loss: 0.000726 | Perplexity: 3208.279288
2025-10-13 02:39:04,891 Stage: Train 0.5 | Epoch: 342 | Iter: 387400 | Total Loss: 0.002360 | Recon Loss: 0.001999 | Commit Loss: 0.000722 | Perplexity: 3215.071312
Trainning Epoch:  75%|███████▍  | 370/494 [112:04:11<40:56:32, 1188.65s/it]Trainning Epoch:  75%|███████▍  | 370/494 [112:04:11<40:56:31, 1188.64s/it]2025-10-13 02:43:02,271 Stage: Train 0.5 | Epoch: 343 | Iter: 387600 | Total Loss: 0.002341 | Recon Loss: 0.001977 | Commit Loss: 0.000728 | Perplexity: 3219.149342
2025-10-13 02:46:56,761 Stage: Train 0.5 | Epoch: 343 | Iter: 387800 | Total Loss: 0.002386 | Recon Loss: 0.002022 | Commit Loss: 0.000726 | Perplexity: 3213.345493
2025-10-13 02:50:51,126 Stage: Train 0.5 | Epoch: 343 | Iter: 388000 | Total Loss: 0.002369 | Recon Loss: 0.002006 | Commit Loss: 0.000727 | Perplexity: 3210.022941
2025-10-13 02:54:45,536 Stage: Train 0.5 | Epoch: 343 | Iter: 388200 | Total Loss: 0.002348 | Recon Loss: 0.001984 | Commit Loss: 0.000728 | Perplexity: 3213.345447
2025-10-13 02:58:40,352 Stage: Train 0.5 | Epoch: 343 | Iter: 388400 | Total Loss: 0.002357 | Recon Loss: 0.001992 | Commit Loss: 0.000730 | Perplexity: 3219.665879
Trainning Epoch:  75%|███████▌  | 371/494 [112:24:02<40:38:16, 1189.40s/it]Trainning Epoch:  75%|███████▌  | 371/494 [112:24:02<40:38:17, 1189.41s/it]2025-10-13 03:02:37,959 Stage: Train 0.5 | Epoch: 344 | Iter: 388600 | Total Loss: 0.002346 | Recon Loss: 0.001984 | Commit Loss: 0.000723 | Perplexity: 3222.155627
2025-10-13 03:06:32,499 Stage: Train 0.5 | Epoch: 344 | Iter: 388800 | Total Loss: 0.002352 | Recon Loss: 0.001987 | Commit Loss: 0.000730 | Perplexity: 3216.258895
2025-10-13 03:10:27,459 Stage: Train 0.5 | Epoch: 344 | Iter: 389000 | Total Loss: 0.002370 | Recon Loss: 0.002001 | Commit Loss: 0.000738 | Perplexity: 3214.664573
2025-10-13 03:14:22,221 Stage: Train 0.5 | Epoch: 344 | Iter: 389200 | Total Loss: 0.002334 | Recon Loss: 0.001969 | Commit Loss: 0.000732 | Perplexity: 3221.667339
2025-10-13 03:18:16,926 Stage: Train 0.5 | Epoch: 344 | Iter: 389400 | Total Loss: 0.002393 | Recon Loss: 0.002024 | Commit Loss: 0.000736 | Perplexity: 3222.209755
Trainning Epoch:  75%|███████▌  | 372/494 [112:43:54<40:19:54, 1190.12s/it]Trainning Epoch:  75%|███████▌  | 372/494 [112:43:54<40:19:54, 1190.12s/it]2025-10-13 03:22:14,647 Stage: Train 0.5 | Epoch: 345 | Iter: 389600 | Total Loss: 0.002384 | Recon Loss: 0.002022 | Commit Loss: 0.000723 | Perplexity: 3209.366201
2025-10-13 03:26:09,357 Stage: Train 0.5 | Epoch: 345 | Iter: 389800 | Total Loss: 0.002348 | Recon Loss: 0.001988 | Commit Loss: 0.000722 | Perplexity: 3214.039664
2025-10-13 03:30:03,889 Stage: Train 0.5 | Epoch: 345 | Iter: 390000 | Total Loss: 0.002358 | Recon Loss: 0.001998 | Commit Loss: 0.000719 | Perplexity: 3205.335040
2025-10-13 03:33:59,101 Stage: Train 0.5 | Epoch: 345 | Iter: 390200 | Total Loss: 0.002359 | Recon Loss: 0.001990 | Commit Loss: 0.000737 | Perplexity: 3215.794087
2025-10-13 03:37:53,830 Stage: Train 0.5 | Epoch: 345 | Iter: 390400 | Total Loss: 0.002387 | Recon Loss: 0.002024 | Commit Loss: 0.000726 | Perplexity: 3208.582874
Trainning Epoch:  76%|███████▌  | 373/494 [113:03:46<40:01:14, 1190.70s/it]Trainning Epoch:  76%|███████▌  | 373/494 [113:03:46<40:01:14, 1190.70s/it]2025-10-13 03:41:52,237 Stage: Train 0.5 | Epoch: 346 | Iter: 390600 | Total Loss: 0.002389 | Recon Loss: 0.002019 | Commit Loss: 0.000738 | Perplexity: 3221.273667
2025-10-13 03:45:47,731 Stage: Train 0.5 | Epoch: 346 | Iter: 390800 | Total Loss: 0.002371 | Recon Loss: 0.002009 | Commit Loss: 0.000723 | Perplexity: 3210.820946
2025-10-13 03:49:42,884 Stage: Train 0.5 | Epoch: 346 | Iter: 391000 | Total Loss: 0.002345 | Recon Loss: 0.001979 | Commit Loss: 0.000732 | Perplexity: 3215.765972
2025-10-13 03:53:38,249 Stage: Train 0.5 | Epoch: 346 | Iter: 391200 | Total Loss: 0.002345 | Recon Loss: 0.001981 | Commit Loss: 0.000728 | Perplexity: 3201.525485
2025-10-13 03:57:33,539 Stage: Train 0.5 | Epoch: 346 | Iter: 391400 | Total Loss: 0.002351 | Recon Loss: 0.001990 | Commit Loss: 0.000722 | Perplexity: 3216.416619
Trainning Epoch:  76%|███████▌  | 374/494 [113:23:41<39:44:05, 1192.05s/it]Trainning Epoch:  76%|███████▌  | 374/494 [113:23:41<39:44:05, 1192.05s/it]2025-10-13 04:01:30,081 Stage: Train 0.5 | Epoch: 347 | Iter: 391600 | Total Loss: 0.002372 | Recon Loss: 0.002009 | Commit Loss: 0.000727 | Perplexity: 3217.046498
2025-10-13 04:05:22,326 Stage: Train 0.5 | Epoch: 347 | Iter: 391800 | Total Loss: 0.002404 | Recon Loss: 0.002044 | Commit Loss: 0.000720 | Perplexity: 3206.323051
2025-10-13 04:09:15,261 Stage: Train 0.5 | Epoch: 347 | Iter: 392000 | Total Loss: 0.002316 | Recon Loss: 0.001951 | Commit Loss: 0.000731 | Perplexity: 3219.772805
2025-10-13 04:13:08,592 Stage: Train 0.5 | Epoch: 347 | Iter: 392200 | Total Loss: 0.002329 | Recon Loss: 0.001964 | Commit Loss: 0.000729 | Perplexity: 3212.029602
2025-10-13 04:17:02,317 Stage: Train 0.5 | Epoch: 347 | Iter: 392400 | Total Loss: 0.002346 | Recon Loss: 0.001980 | Commit Loss: 0.000732 | Perplexity: 3220.070424
Trainning Epoch:  76%|███████▌  | 375/494 [113:43:24<39:18:59, 1189.41s/it]Trainning Epoch:  76%|███████▌  | 375/494 [113:43:24<39:18:59, 1189.41s/it]2025-10-13 04:20:59,883 Stage: Train 0.5 | Epoch: 348 | Iter: 392600 | Total Loss: 0.002370 | Recon Loss: 0.002003 | Commit Loss: 0.000733 | Perplexity: 3215.964628
2025-10-13 04:24:53,786 Stage: Train 0.5 | Epoch: 348 | Iter: 392800 | Total Loss: 0.002351 | Recon Loss: 0.001988 | Commit Loss: 0.000726 | Perplexity: 3226.982496
2025-10-13 04:28:47,547 Stage: Train 0.5 | Epoch: 348 | Iter: 393000 | Total Loss: 0.002370 | Recon Loss: 0.002003 | Commit Loss: 0.000735 | Perplexity: 3212.126117
2025-10-13 04:32:41,442 Stage: Train 0.5 | Epoch: 348 | Iter: 393200 | Total Loss: 0.002356 | Recon Loss: 0.001994 | Commit Loss: 0.000722 | Perplexity: 3216.801000
2025-10-13 04:36:35,226 Stage: Train 0.5 | Epoch: 348 | Iter: 393400 | Total Loss: 0.002366 | Recon Loss: 0.002004 | Commit Loss: 0.000724 | Perplexity: 3207.233416
Trainning Epoch:  76%|███████▌  | 376/494 [114:03:13<38:58:33, 1189.10s/it]Trainning Epoch:  76%|███████▌  | 376/494 [114:03:13<38:58:33, 1189.10s/it]2025-10-13 04:40:32,914 Stage: Train 0.5 | Epoch: 349 | Iter: 393600 | Total Loss: 0.002350 | Recon Loss: 0.001988 | Commit Loss: 0.000725 | Perplexity: 3219.507183
2025-10-13 04:44:28,788 Stage: Train 0.5 | Epoch: 349 | Iter: 393800 | Total Loss: 0.002353 | Recon Loss: 0.001990 | Commit Loss: 0.000726 | Perplexity: 3214.773959
2025-10-13 04:48:24,219 Stage: Train 0.5 | Epoch: 349 | Iter: 394000 | Total Loss: 0.002347 | Recon Loss: 0.001984 | Commit Loss: 0.000725 | Perplexity: 3211.475924
2025-10-13 04:52:19,928 Stage: Train 0.5 | Epoch: 349 | Iter: 394200 | Total Loss: 0.002350 | Recon Loss: 0.001984 | Commit Loss: 0.000732 | Perplexity: 3212.015229
2025-10-13 04:56:15,454 Stage: Train 0.5 | Epoch: 349 | Iter: 394400 | Total Loss: 0.002354 | Recon Loss: 0.001989 | Commit Loss: 0.000730 | Perplexity: 3214.735347
Trainning Epoch:  76%|███████▋  | 377/494 [114:23:09<38:42:57, 1191.26s/it]Trainning Epoch:  76%|███████▋  | 377/494 [114:23:09<38:42:57, 1191.26s/it]2025-10-13 05:00:13,616 Stage: Train 0.5 | Epoch: 350 | Iter: 394600 | Total Loss: 0.002340 | Recon Loss: 0.001978 | Commit Loss: 0.000725 | Perplexity: 3206.390924
2025-10-13 05:04:08,766 Stage: Train 0.5 | Epoch: 350 | Iter: 394800 | Total Loss: 0.002345 | Recon Loss: 0.001981 | Commit Loss: 0.000727 | Perplexity: 3211.772151
2025-10-13 05:08:04,195 Stage: Train 0.5 | Epoch: 350 | Iter: 395000 | Total Loss: 0.002335 | Recon Loss: 0.001972 | Commit Loss: 0.000727 | Perplexity: 3211.202415
2025-10-13 05:11:59,428 Stage: Train 0.5 | Epoch: 350 | Iter: 395200 | Total Loss: 0.002327 | Recon Loss: 0.001965 | Commit Loss: 0.000725 | Perplexity: 3219.383479
2025-10-13 05:15:54,470 Stage: Train 0.5 | Epoch: 350 | Iter: 395400 | Total Loss: 0.002384 | Recon Loss: 0.002006 | Commit Loss: 0.000755 | Perplexity: 3217.596992
Trainning Epoch:  77%|███████▋  | 378/494 [114:43:03<38:24:46, 1192.12s/it]Trainning Epoch:  77%|███████▋  | 378/494 [114:43:03<38:24:46, 1192.12s/it]2025-10-13 05:19:52,747 Stage: Train 0.5 | Epoch: 351 | Iter: 395600 | Total Loss: 0.002377 | Recon Loss: 0.002011 | Commit Loss: 0.000732 | Perplexity: 3214.922990
2025-10-13 05:23:47,297 Stage: Train 0.5 | Epoch: 351 | Iter: 395800 | Total Loss: 0.002333 | Recon Loss: 0.001971 | Commit Loss: 0.000724 | Perplexity: 3217.345900
2025-10-13 05:27:41,977 Stage: Train 0.5 | Epoch: 351 | Iter: 396000 | Total Loss: 0.002337 | Recon Loss: 0.001974 | Commit Loss: 0.000727 | Perplexity: 3217.055439
2025-10-13 05:31:36,862 Stage: Train 0.5 | Epoch: 351 | Iter: 396200 | Total Loss: 0.002357 | Recon Loss: 0.001993 | Commit Loss: 0.000728 | Perplexity: 3220.048569
2025-10-13 05:35:31,686 Stage: Train 0.5 | Epoch: 351 | Iter: 396400 | Total Loss: 0.002342 | Recon Loss: 0.001973 | Commit Loss: 0.000738 | Perplexity: 3217.719951
Trainning Epoch:  77%|███████▋  | 379/494 [115:02:55<38:04:55, 1192.14s/it]Trainning Epoch:  77%|███████▋  | 379/494 [115:02:55<38:04:55, 1192.14s/it]2025-10-13 05:39:29,364 Stage: Train 0.5 | Epoch: 352 | Iter: 396600 | Total Loss: 0.002338 | Recon Loss: 0.001973 | Commit Loss: 0.000730 | Perplexity: 3210.674366
2025-10-13 05:43:21,848 Stage: Train 0.5 | Epoch: 352 | Iter: 396800 | Total Loss: 0.002340 | Recon Loss: 0.001972 | Commit Loss: 0.000736 | Perplexity: 3220.622780
2025-10-13 05:47:14,865 Stage: Train 0.5 | Epoch: 352 | Iter: 397000 | Total Loss: 0.002342 | Recon Loss: 0.001978 | Commit Loss: 0.000728 | Perplexity: 3223.673445
2025-10-13 05:51:08,052 Stage: Train 0.5 | Epoch: 352 | Iter: 397200 | Total Loss: 0.002352 | Recon Loss: 0.001986 | Commit Loss: 0.000731 | Perplexity: 3214.750461
2025-10-13 05:55:00,794 Stage: Train 0.5 | Epoch: 352 | Iter: 397400 | Total Loss: 0.002344 | Recon Loss: 0.001980 | Commit Loss: 0.000727 | Perplexity: 3211.198312
Trainning Epoch:  77%|███████▋  | 380/494 [115:22:38<37:39:53, 1189.42s/it]Trainning Epoch:  77%|███████▋  | 380/494 [115:22:38<37:39:53, 1189.42s/it]2025-10-13 05:58:57,850 Stage: Train 0.5 | Epoch: 353 | Iter: 397600 | Total Loss: 0.002344 | Recon Loss: 0.001984 | Commit Loss: 0.000720 | Perplexity: 3205.933585
2025-10-13 06:02:53,872 Stage: Train 0.5 | Epoch: 353 | Iter: 397800 | Total Loss: 0.002370 | Recon Loss: 0.002006 | Commit Loss: 0.000730 | Perplexity: 3220.032986
2025-10-13 06:06:50,829 Stage: Train 0.5 | Epoch: 353 | Iter: 398000 | Total Loss: 0.002324 | Recon Loss: 0.001961 | Commit Loss: 0.000726 | Perplexity: 3228.374191
2025-10-13 06:10:47,695 Stage: Train 0.5 | Epoch: 353 | Iter: 398200 | Total Loss: 0.002345 | Recon Loss: 0.001982 | Commit Loss: 0.000725 | Perplexity: 3211.722102
2025-10-13 06:14:44,747 Stage: Train 0.5 | Epoch: 353 | Iter: 398400 | Total Loss: 0.002354 | Recon Loss: 0.001991 | Commit Loss: 0.000726 | Perplexity: 3214.829714
2025-10-13 06:18:41,318 Stage: Train 0.5 | Epoch: 353 | Iter: 398600 | Total Loss: 0.002344 | Recon Loss: 0.001980 | Commit Loss: 0.000728 | Perplexity: 3216.786844
Trainning Epoch:  77%|███████▋  | 381/494 [115:42:41<37:27:29, 1193.36s/it]Trainning Epoch:  77%|███████▋  | 381/494 [115:42:41<37:27:30, 1193.36s/it]2025-10-13 06:22:41,650 Stage: Train 0.5 | Epoch: 354 | Iter: 398800 | Total Loss: 0.002353 | Recon Loss: 0.001990 | Commit Loss: 0.000725 | Perplexity: 3218.942285
2025-10-13 06:26:38,264 Stage: Train 0.5 | Epoch: 354 | Iter: 399000 | Total Loss: 0.002312 | Recon Loss: 0.001946 | Commit Loss: 0.000732 | Perplexity: 3220.203281
2025-10-13 06:30:34,751 Stage: Train 0.5 | Epoch: 354 | Iter: 399200 | Total Loss: 0.002340 | Recon Loss: 0.001978 | Commit Loss: 0.000725 | Perplexity: 3218.214813
2025-10-13 06:34:31,217 Stage: Train 0.5 | Epoch: 354 | Iter: 399400 | Total Loss: 0.002353 | Recon Loss: 0.001988 | Commit Loss: 0.000731 | Perplexity: 3219.388104
2025-10-13 06:38:27,701 Stage: Train 0.5 | Epoch: 354 | Iter: 399600 | Total Loss: 0.002372 | Recon Loss: 0.002008 | Commit Loss: 0.000728 | Perplexity: 3222.202878
Trainning Epoch:  77%|███████▋  | 382/494 [116:02:42<37:12:09, 1195.80s/it]Trainning Epoch:  77%|███████▋  | 382/494 [116:02:42<37:12:09, 1195.80s/it]2025-10-13 06:42:26,218 Stage: Train 0.5 | Epoch: 355 | Iter: 399800 | Total Loss: 0.002343 | Recon Loss: 0.001984 | Commit Loss: 0.000719 | Perplexity: 3222.689484
2025-10-13 06:46:21,288 Stage: Train 0.5 | Epoch: 355 | Iter: 400000 | Total Loss: 0.002357 | Recon Loss: 0.001991 | Commit Loss: 0.000732 | Perplexity: 3223.825555
2025-10-13 06:46:21,288 Saving model at iteration 400000
2025-10-13 06:46:21,742 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000
2025-10-13 06:46:23,248 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000/model.safetensors
2025-10-13 06:46:25,022 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000/optimizer.bin
2025-10-13 06:46:25,023 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000/scheduler.bin
2025-10-13 06:46:25,023 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000/sampler.bin
2025-10-13 06:46:25,024 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_356_step_400000/random_states_0.pkl
2025-10-13 06:50:20,128 Stage: Train 0.5 | Epoch: 355 | Iter: 400200 | Total Loss: 0.002349 | Recon Loss: 0.001989 | Commit Loss: 0.000720 | Perplexity: 3221.513496
2025-10-13 06:54:15,336 Stage: Train 0.5 | Epoch: 355 | Iter: 400400 | Total Loss: 0.002329 | Recon Loss: 0.001964 | Commit Loss: 0.000730 | Perplexity: 3221.348571
2025-10-13 06:58:10,508 Stage: Train 0.5 | Epoch: 355 | Iter: 400600 | Total Loss: 0.002364 | Recon Loss: 0.002000 | Commit Loss: 0.000727 | Perplexity: 3208.617970
Trainning Epoch:  78%|███████▊  | 383/494 [116:22:40<36:53:29, 1196.49s/it]Trainning Epoch:  78%|███████▊  | 383/494 [116:22:40<36:53:29, 1196.49s/it]2025-10-13 07:02:07,907 Stage: Train 0.5 | Epoch: 356 | Iter: 400800 | Total Loss: 0.002321 | Recon Loss: 0.001958 | Commit Loss: 0.000728 | Perplexity: 3221.833705
2025-10-13 07:06:01,693 Stage: Train 0.5 | Epoch: 356 | Iter: 401000 | Total Loss: 0.002335 | Recon Loss: 0.001968 | Commit Loss: 0.000733 | Perplexity: 3222.931073
2025-10-13 07:09:55,844 Stage: Train 0.5 | Epoch: 356 | Iter: 401200 | Total Loss: 0.002342 | Recon Loss: 0.001980 | Commit Loss: 0.000724 | Perplexity: 3218.711602
2025-10-13 07:13:50,062 Stage: Train 0.5 | Epoch: 356 | Iter: 401400 | Total Loss: 0.002356 | Recon Loss: 0.001990 | Commit Loss: 0.000731 | Perplexity: 3217.214912
2025-10-13 07:17:44,082 Stage: Train 0.5 | Epoch: 356 | Iter: 401600 | Total Loss: 0.002358 | Recon Loss: 0.001993 | Commit Loss: 0.000730 | Perplexity: 3214.573503
Trainning Epoch:  78%|███████▊  | 384/494 [116:42:29<36:29:11, 1194.10s/it]Trainning Epoch:  78%|███████▊  | 384/494 [116:42:29<36:29:11, 1194.11s/it]2025-10-13 07:21:41,818 Stage: Train 0.5 | Epoch: 357 | Iter: 401800 | Total Loss: 0.002347 | Recon Loss: 0.001981 | Commit Loss: 0.000732 | Perplexity: 3222.215205
2025-10-13 07:25:36,737 Stage: Train 0.5 | Epoch: 357 | Iter: 402000 | Total Loss: 0.002339 | Recon Loss: 0.001976 | Commit Loss: 0.000727 | Perplexity: 3219.681990
2025-10-13 07:29:31,724 Stage: Train 0.5 | Epoch: 357 | Iter: 402200 | Total Loss: 0.002333 | Recon Loss: 0.001965 | Commit Loss: 0.000735 | Perplexity: 3223.242880
2025-10-13 07:33:26,854 Stage: Train 0.5 | Epoch: 357 | Iter: 402400 | Total Loss: 0.002363 | Recon Loss: 0.001996 | Commit Loss: 0.000732 | Perplexity: 3207.821116
2025-10-13 07:37:22,579 Stage: Train 0.5 | Epoch: 357 | Iter: 402600 | Total Loss: 0.002376 | Recon Loss: 0.002013 | Commit Loss: 0.000727 | Perplexity: 3219.174467
Trainning Epoch:  78%|███████▊  | 385/494 [117:02:23<36:09:14, 1194.08s/it]Trainning Epoch:  78%|███████▊  | 385/494 [117:02:23<36:09:14, 1194.08s/it]2025-10-13 07:41:19,152 Stage: Train 0.5 | Epoch: 358 | Iter: 402800 | Total Loss: 0.002286 | Recon Loss: 0.001926 | Commit Loss: 0.000721 | Perplexity: 3213.902262
2025-10-13 07:45:13,262 Stage: Train 0.5 | Epoch: 358 | Iter: 403000 | Total Loss: 0.002372 | Recon Loss: 0.002006 | Commit Loss: 0.000731 | Perplexity: 3216.991544
2025-10-13 07:49:07,239 Stage: Train 0.5 | Epoch: 358 | Iter: 403200 | Total Loss: 0.002354 | Recon Loss: 0.001990 | Commit Loss: 0.000728 | Perplexity: 3225.428088
2025-10-13 07:53:01,996 Stage: Train 0.5 | Epoch: 358 | Iter: 403400 | Total Loss: 0.002373 | Recon Loss: 0.002005 | Commit Loss: 0.000736 | Perplexity: 3219.876692
2025-10-13 07:56:56,543 Stage: Train 0.5 | Epoch: 358 | Iter: 403600 | Total Loss: 0.002332 | Recon Loss: 0.001968 | Commit Loss: 0.000728 | Perplexity: 3208.158593
Trainning Epoch:  78%|███████▊  | 386/494 [117:22:12<35:46:37, 1192.57s/it]Trainning Epoch:  78%|███████▊  | 386/494 [117:22:12<35:46:37, 1192.57s/it]2025-10-13 08:00:53,666 Stage: Train 0.5 | Epoch: 359 | Iter: 403800 | Total Loss: 0.002359 | Recon Loss: 0.001995 | Commit Loss: 0.000727 | Perplexity: 3218.109548
2025-10-13 08:04:47,860 Stage: Train 0.5 | Epoch: 359 | Iter: 404000 | Total Loss: 0.002318 | Recon Loss: 0.001951 | Commit Loss: 0.000733 | Perplexity: 3220.137430
2025-10-13 08:08:41,815 Stage: Train 0.5 | Epoch: 359 | Iter: 404200 | Total Loss: 0.002325 | Recon Loss: 0.001960 | Commit Loss: 0.000730 | Perplexity: 3217.616971
2025-10-13 08:12:35,813 Stage: Train 0.5 | Epoch: 359 | Iter: 404400 | Total Loss: 0.002340 | Recon Loss: 0.001977 | Commit Loss: 0.000727 | Perplexity: 3219.193773
2025-10-13 08:16:29,794 Stage: Train 0.5 | Epoch: 359 | Iter: 404600 | Total Loss: 0.002334 | Recon Loss: 0.001969 | Commit Loss: 0.000729 | Perplexity: 3209.722467
Trainning Epoch:  78%|███████▊  | 387/494 [117:42:00<35:24:18, 1191.21s/it]Trainning Epoch:  78%|███████▊  | 387/494 [117:42:00<35:24:19, 1191.21s/it]2025-10-13 08:20:27,729 Stage: Train 0.5 | Epoch: 360 | Iter: 404800 | Total Loss: 0.002347 | Recon Loss: 0.001981 | Commit Loss: 0.000732 | Perplexity: 3226.060370
2025-10-13 08:24:23,059 Stage: Train 0.5 | Epoch: 360 | Iter: 405000 | Total Loss: 0.002322 | Recon Loss: 0.001957 | Commit Loss: 0.000730 | Perplexity: 3221.784916
2025-10-13 08:28:18,421 Stage: Train 0.5 | Epoch: 360 | Iter: 405200 | Total Loss: 0.002318 | Recon Loss: 0.001956 | Commit Loss: 0.000723 | Perplexity: 3216.973488
2025-10-13 08:32:13,677 Stage: Train 0.5 | Epoch: 360 | Iter: 405400 | Total Loss: 0.002357 | Recon Loss: 0.001992 | Commit Loss: 0.000729 | Perplexity: 3229.796395
2025-10-13 08:36:10,213 Stage: Train 0.5 | Epoch: 360 | Iter: 405600 | Total Loss: 0.002316 | Recon Loss: 0.001952 | Commit Loss: 0.000726 | Perplexity: 3229.921942
Trainning Epoch:  79%|███████▊  | 388/494 [118:01:57<35:07:19, 1192.83s/it]Trainning Epoch:  79%|███████▊  | 388/494 [118:01:57<35:07:20, 1192.83s/it]2025-10-13 08:40:07,824 Stage: Train 0.5 | Epoch: 361 | Iter: 405800 | Total Loss: 0.002348 | Recon Loss: 0.001982 | Commit Loss: 0.000731 | Perplexity: 3217.579872
2025-10-13 08:44:03,750 Stage: Train 0.5 | Epoch: 361 | Iter: 406000 | Total Loss: 0.002362 | Recon Loss: 0.001990 | Commit Loss: 0.000744 | Perplexity: 3237.778776
2025-10-13 08:47:59,538 Stage: Train 0.5 | Epoch: 361 | Iter: 406200 | Total Loss: 0.002321 | Recon Loss: 0.001955 | Commit Loss: 0.000732 | Perplexity: 3228.687681
2025-10-13 08:51:55,204 Stage: Train 0.5 | Epoch: 361 | Iter: 406400 | Total Loss: 0.002327 | Recon Loss: 0.001962 | Commit Loss: 0.000730 | Perplexity: 3227.550975
2025-10-13 08:55:47,262 Stage: Train 0.5 | Epoch: 361 | Iter: 406600 | Total Loss: 0.002315 | Recon Loss: 0.001949 | Commit Loss: 0.000731 | Perplexity: 3214.646593
Trainning Epoch:  79%|███████▊  | 389/494 [118:21:47<34:46:14, 1192.14s/it]Trainning Epoch:  79%|███████▊  | 389/494 [118:21:47<34:46:14, 1192.14s/it]2025-10-13 08:59:43,581 Stage: Train 0.5 | Epoch: 362 | Iter: 406800 | Total Loss: 0.002323 | Recon Loss: 0.001960 | Commit Loss: 0.000725 | Perplexity: 3218.128652
2025-10-13 09:03:38,261 Stage: Train 0.5 | Epoch: 362 | Iter: 407000 | Total Loss: 0.002319 | Recon Loss: 0.001959 | Commit Loss: 0.000720 | Perplexity: 3225.082175
2025-10-13 09:07:32,692 Stage: Train 0.5 | Epoch: 362 | Iter: 407200 | Total Loss: 0.002361 | Recon Loss: 0.001992 | Commit Loss: 0.000738 | Perplexity: 3220.627256
2025-10-13 09:11:27,259 Stage: Train 0.5 | Epoch: 362 | Iter: 407400 | Total Loss: 0.002329 | Recon Loss: 0.001960 | Commit Loss: 0.000738 | Perplexity: 3216.379844
2025-10-13 09:15:22,323 Stage: Train 0.5 | Epoch: 362 | Iter: 407600 | Total Loss: 0.002347 | Recon Loss: 0.001982 | Commit Loss: 0.000730 | Perplexity: 3217.128931
Trainning Epoch:  79%|███████▉  | 390/494 [118:41:39<34:26:08, 1192.00s/it]Trainning Epoch:  79%|███████▉  | 390/494 [118:41:39<34:26:08, 1192.00s/it]2025-10-13 09:19:18,718 Stage: Train 0.5 | Epoch: 363 | Iter: 407800 | Total Loss: 0.002345 | Recon Loss: 0.001981 | Commit Loss: 0.000727 | Perplexity: 3216.538567
2025-10-13 09:23:08,346 Stage: Train 0.5 | Epoch: 363 | Iter: 408000 | Total Loss: 0.002330 | Recon Loss: 0.001967 | Commit Loss: 0.000727 | Perplexity: 3215.932189
2025-10-13 09:26:58,849 Stage: Train 0.5 | Epoch: 363 | Iter: 408200 | Total Loss: 0.002343 | Recon Loss: 0.001975 | Commit Loss: 0.000735 | Perplexity: 3217.822943
2025-10-13 09:30:49,704 Stage: Train 0.5 | Epoch: 363 | Iter: 408400 | Total Loss: 0.002335 | Recon Loss: 0.001974 | Commit Loss: 0.000721 | Perplexity: 3217.773002
2025-10-13 09:34:40,628 Stage: Train 0.5 | Epoch: 363 | Iter: 408600 | Total Loss: 0.002315 | Recon Loss: 0.001952 | Commit Loss: 0.000727 | Perplexity: 3222.259611
Trainning Epoch:  79%|███████▉  | 391/494 [119:01:10<33:55:31, 1185.75s/it]Trainning Epoch:  79%|███████▉  | 391/494 [119:01:10<33:55:32, 1185.75s/it]2025-10-13 09:38:33,431 Stage: Train 0.5 | Epoch: 364 | Iter: 408800 | Total Loss: 0.002321 | Recon Loss: 0.001951 | Commit Loss: 0.000741 | Perplexity: 3223.180123
2025-10-13 09:42:20,697 Stage: Train 0.5 | Epoch: 364 | Iter: 409000 | Total Loss: 0.002333 | Recon Loss: 0.001969 | Commit Loss: 0.000728 | Perplexity: 3218.272388
2025-10-13 09:46:08,718 Stage: Train 0.5 | Epoch: 364 | Iter: 409200 | Total Loss: 0.002316 | Recon Loss: 0.001950 | Commit Loss: 0.000731 | Perplexity: 3219.916431
2025-10-13 09:49:56,322 Stage: Train 0.5 | Epoch: 364 | Iter: 409400 | Total Loss: 0.002336 | Recon Loss: 0.001972 | Commit Loss: 0.000728 | Perplexity: 3226.446415
2025-10-13 09:53:45,030 Stage: Train 0.5 | Epoch: 364 | Iter: 409600 | Total Loss: 0.002340 | Recon Loss: 0.001976 | Commit Loss: 0.000728 | Perplexity: 3216.785337
Trainning Epoch:  79%|███████▉  | 392/494 [119:20:27<33:21:18, 1177.24s/it]Trainning Epoch:  79%|███████▉  | 392/494 [119:20:27<33:21:18, 1177.24s/it]2025-10-13 09:57:37,011 Stage: Train 0.5 | Epoch: 365 | Iter: 409800 | Total Loss: 0.002342 | Recon Loss: 0.001979 | Commit Loss: 0.000725 | Perplexity: 3212.717592
2025-10-13 10:01:27,941 Stage: Train 0.5 | Epoch: 365 | Iter: 410000 | Total Loss: 0.002330 | Recon Loss: 0.001966 | Commit Loss: 0.000729 | Perplexity: 3218.096630
2025-10-13 10:05:19,443 Stage: Train 0.5 | Epoch: 365 | Iter: 410200 | Total Loss: 0.002312 | Recon Loss: 0.001945 | Commit Loss: 0.000733 | Perplexity: 3219.794213
2025-10-13 10:09:10,945 Stage: Train 0.5 | Epoch: 365 | Iter: 410400 | Total Loss: 0.002312 | Recon Loss: 0.001944 | Commit Loss: 0.000735 | Perplexity: 3215.621232
2025-10-13 10:13:02,397 Stage: Train 0.5 | Epoch: 365 | Iter: 410600 | Total Loss: 0.002355 | Recon Loss: 0.001991 | Commit Loss: 0.000729 | Perplexity: 3220.797164
Trainning Epoch:  80%|███████▉  | 393/494 [119:40:02<33:00:23, 1176.47s/it]Trainning Epoch:  80%|███████▉  | 393/494 [119:40:02<33:00:24, 1176.48s/it]2025-10-13 10:16:55,851 Stage: Train 0.5 | Epoch: 366 | Iter: 410800 | Total Loss: 0.002338 | Recon Loss: 0.001976 | Commit Loss: 0.000725 | Perplexity: 3220.764371
2025-10-13 10:20:44,267 Stage: Train 0.5 | Epoch: 366 | Iter: 411000 | Total Loss: 0.002328 | Recon Loss: 0.001963 | Commit Loss: 0.000730 | Perplexity: 3221.780723
2025-10-13 10:24:34,505 Stage: Train 0.5 | Epoch: 366 | Iter: 411200 | Total Loss: 0.002326 | Recon Loss: 0.001962 | Commit Loss: 0.000729 | Perplexity: 3224.584176
2025-10-13 10:28:25,648 Stage: Train 0.5 | Epoch: 366 | Iter: 411400 | Total Loss: 0.002366 | Recon Loss: 0.002003 | Commit Loss: 0.000726 | Perplexity: 3222.723367
2025-10-13 10:32:17,321 Stage: Train 0.5 | Epoch: 366 | Iter: 411600 | Total Loss: 0.002333 | Recon Loss: 0.001969 | Commit Loss: 0.000729 | Perplexity: 3224.938691
Trainning Epoch:  80%|███████▉  | 394/494 [119:59:32<32:37:44, 1174.64s/it]Trainning Epoch:  80%|███████▉  | 394/494 [119:59:32<32:37:44, 1174.65s/it]2025-10-13 10:36:12,537 Stage: Train 0.5 | Epoch: 367 | Iter: 411800 | Total Loss: 0.002305 | Recon Loss: 0.001936 | Commit Loss: 0.000736 | Perplexity: 3206.226617
2025-10-13 10:40:06,282 Stage: Train 0.5 | Epoch: 367 | Iter: 412000 | Total Loss: 0.002303 | Recon Loss: 0.001943 | Commit Loss: 0.000719 | Perplexity: 3211.996799
2025-10-13 10:44:00,283 Stage: Train 0.5 | Epoch: 367 | Iter: 412200 | Total Loss: 0.002361 | Recon Loss: 0.001993 | Commit Loss: 0.000735 | Perplexity: 3220.626256
2025-10-13 10:47:54,191 Stage: Train 0.5 | Epoch: 367 | Iter: 412400 | Total Loss: 0.002320 | Recon Loss: 0.001956 | Commit Loss: 0.000727 | Perplexity: 3225.918767
2025-10-13 10:51:48,031 Stage: Train 0.5 | Epoch: 367 | Iter: 412600 | Total Loss: 0.002325 | Recon Loss: 0.001959 | Commit Loss: 0.000733 | Perplexity: 3224.665765
Trainning Epoch:  80%|███████▉  | 395/494 [120:19:20<32:24:27, 1178.46s/it]Trainning Epoch:  80%|███████▉  | 395/494 [120:19:20<32:24:28, 1178.47s/it]2025-10-13 10:55:44,585 Stage: Train 0.5 | Epoch: 368 | Iter: 412800 | Total Loss: 0.002341 | Recon Loss: 0.001974 | Commit Loss: 0.000736 | Perplexity: 3230.227527
2025-10-13 10:59:35,528 Stage: Train 0.5 | Epoch: 368 | Iter: 413000 | Total Loss: 0.002326 | Recon Loss: 0.001962 | Commit Loss: 0.000727 | Perplexity: 3223.104894
2025-10-13 11:03:27,811 Stage: Train 0.5 | Epoch: 368 | Iter: 413200 | Total Loss: 0.002327 | Recon Loss: 0.001961 | Commit Loss: 0.000733 | Perplexity: 3201.598285
2025-10-13 11:07:20,341 Stage: Train 0.5 | Epoch: 368 | Iter: 413400 | Total Loss: 0.002344 | Recon Loss: 0.001979 | Commit Loss: 0.000731 | Perplexity: 3223.674830
2025-10-13 11:11:12,620 Stage: Train 0.5 | Epoch: 368 | Iter: 413600 | Total Loss: 0.002346 | Recon Loss: 0.001985 | Commit Loss: 0.000722 | Perplexity: 3215.145339
Trainning Epoch:  80%|████████  | 396/494 [120:38:59<32:05:01, 1178.59s/it]Trainning Epoch:  80%|████████  | 396/494 [120:38:59<32:05:01, 1178.59s/it]2025-10-13 11:15:08,803 Stage: Train 0.5 | Epoch: 369 | Iter: 413800 | Total Loss: 0.002333 | Recon Loss: 0.001971 | Commit Loss: 0.000724 | Perplexity: 3228.072430
2025-10-13 11:18:59,107 Stage: Train 0.5 | Epoch: 369 | Iter: 414000 | Total Loss: 0.002297 | Recon Loss: 0.001932 | Commit Loss: 0.000729 | Perplexity: 3223.629216
2025-10-13 11:22:49,829 Stage: Train 0.5 | Epoch: 369 | Iter: 414200 | Total Loss: 0.002335 | Recon Loss: 0.001969 | Commit Loss: 0.000733 | Perplexity: 3209.982922
2025-10-13 11:26:40,558 Stage: Train 0.5 | Epoch: 369 | Iter: 414400 | Total Loss: 0.002367 | Recon Loss: 0.002001 | Commit Loss: 0.000733 | Perplexity: 3220.409207
2025-10-13 11:30:31,690 Stage: Train 0.5 | Epoch: 369 | Iter: 414600 | Total Loss: 0.002339 | Recon Loss: 0.001973 | Commit Loss: 0.000733 | Perplexity: 3217.217725
2025-10-13 11:34:22,782 Stage: Train 0.5 | Epoch: 369 | Iter: 414800 | Total Loss: 0.002313 | Recon Loss: 0.001951 | Commit Loss: 0.000724 | Perplexity: 3222.041517
Trainning Epoch:  80%|████████  | 397/494 [120:58:31<31:42:31, 1176.82s/it]Trainning Epoch:  80%|████████  | 397/494 [120:58:31<31:42:32, 1176.83s/it]2025-10-13 11:38:18,478 Stage: Train 0.5 | Epoch: 370 | Iter: 415000 | Total Loss: 0.002337 | Recon Loss: 0.001971 | Commit Loss: 0.000731 | Perplexity: 3218.715917
2025-10-13 11:42:11,071 Stage: Train 0.5 | Epoch: 370 | Iter: 415200 | Total Loss: 0.002332 | Recon Loss: 0.001967 | Commit Loss: 0.000730 | Perplexity: 3222.957023
2025-10-13 11:46:03,339 Stage: Train 0.5 | Epoch: 370 | Iter: 415400 | Total Loss: 0.002296 | Recon Loss: 0.001932 | Commit Loss: 0.000729 | Perplexity: 3223.841571
2025-10-13 11:49:55,911 Stage: Train 0.5 | Epoch: 370 | Iter: 415600 | Total Loss: 0.002326 | Recon Loss: 0.001959 | Commit Loss: 0.000733 | Perplexity: 3210.031843
2025-10-13 11:53:48,487 Stage: Train 0.5 | Epoch: 370 | Iter: 415800 | Total Loss: 0.002324 | Recon Loss: 0.001960 | Commit Loss: 0.000729 | Perplexity: 3224.774517
Trainning Epoch:  81%|████████  | 398/494 [121:18:12<31:24:47, 1178.00s/it]Trainning Epoch:  81%|████████  | 398/494 [121:18:12<31:24:48, 1178.00s/it]2025-10-13 11:57:40,398 Stage: Train 0.5 | Epoch: 371 | Iter: 416000 | Total Loss: 0.002313 | Recon Loss: 0.001950 | Commit Loss: 0.000727 | Perplexity: 3223.080968
2025-10-13 12:01:29,709 Stage: Train 0.5 | Epoch: 371 | Iter: 416200 | Total Loss: 0.002330 | Recon Loss: 0.001964 | Commit Loss: 0.000731 | Perplexity: 3221.235479
2025-10-13 12:05:19,155 Stage: Train 0.5 | Epoch: 371 | Iter: 416400 | Total Loss: 0.002297 | Recon Loss: 0.001934 | Commit Loss: 0.000727 | Perplexity: 3227.665084
2025-10-13 12:09:08,011 Stage: Train 0.5 | Epoch: 371 | Iter: 416600 | Total Loss: 0.002300 | Recon Loss: 0.001936 | Commit Loss: 0.000728 | Perplexity: 3222.761960
2025-10-13 12:12:57,038 Stage: Train 0.5 | Epoch: 371 | Iter: 416800 | Total Loss: 0.002328 | Recon Loss: 0.001961 | Commit Loss: 0.000734 | Perplexity: 3222.810948
Trainning Epoch:  81%|████████  | 399/494 [121:37:35<30:58:01, 1173.49s/it]Trainning Epoch:  81%|████████  | 399/494 [121:37:35<30:58:02, 1173.50s/it]2025-10-13 12:16:53,031 Stage: Train 0.5 | Epoch: 372 | Iter: 417000 | Total Loss: 0.002325 | Recon Loss: 0.001962 | Commit Loss: 0.000727 | Perplexity: 3210.938856
2025-10-13 12:20:47,256 Stage: Train 0.5 | Epoch: 372 | Iter: 417200 | Total Loss: 0.002283 | Recon Loss: 0.001920 | Commit Loss: 0.000727 | Perplexity: 3219.377639
2025-10-13 12:24:41,039 Stage: Train 0.5 | Epoch: 372 | Iter: 417400 | Total Loss: 0.002317 | Recon Loss: 0.001949 | Commit Loss: 0.000736 | Perplexity: 3224.975922
2025-10-13 12:28:34,834 Stage: Train 0.5 | Epoch: 372 | Iter: 417600 | Total Loss: 0.002311 | Recon Loss: 0.001941 | Commit Loss: 0.000739 | Perplexity: 3229.071111
2025-10-13 12:32:28,986 Stage: Train 0.5 | Epoch: 372 | Iter: 417800 | Total Loss: 0.002346 | Recon Loss: 0.001981 | Commit Loss: 0.000732 | Perplexity: 3223.761370
Trainning Epoch:  81%|████████  | 400/494 [121:57:23<30:45:19, 1177.87s/it]Trainning Epoch:  81%|████████  | 400/494 [121:57:23<30:45:20, 1177.87s/it]2025-10-13 12:36:24,056 Stage: Train 0.5 | Epoch: 373 | Iter: 418000 | Total Loss: 0.002319 | Recon Loss: 0.001953 | Commit Loss: 0.000732 | Perplexity: 3217.621150
2025-10-13 12:40:18,590 Stage: Train 0.5 | Epoch: 373 | Iter: 418200 | Total Loss: 0.002313 | Recon Loss: 0.001946 | Commit Loss: 0.000732 | Perplexity: 3216.330604
2025-10-13 12:44:13,178 Stage: Train 0.5 | Epoch: 373 | Iter: 418400 | Total Loss: 0.002338 | Recon Loss: 0.001975 | Commit Loss: 0.000728 | Perplexity: 3213.773209
2025-10-13 12:48:07,632 Stage: Train 0.5 | Epoch: 373 | Iter: 418600 | Total Loss: 0.002327 | Recon Loss: 0.001963 | Commit Loss: 0.000728 | Perplexity: 3218.716030
2025-10-13 12:52:02,817 Stage: Train 0.5 | Epoch: 373 | Iter: 418800 | Total Loss: 0.002327 | Recon Loss: 0.001963 | Commit Loss: 0.000727 | Perplexity: 3214.239926
Trainning Epoch:  81%|████████  | 401/494 [122:17:13<30:31:03, 1181.33s/it]Trainning Epoch:  81%|████████  | 401/494 [122:17:13<30:31:03, 1181.33s/it]2025-10-13 12:55:58,487 Stage: Train 0.5 | Epoch: 374 | Iter: 419000 | Total Loss: 0.002330 | Recon Loss: 0.001960 | Commit Loss: 0.000740 | Perplexity: 3219.798125
2025-10-13 12:59:50,732 Stage: Train 0.5 | Epoch: 374 | Iter: 419200 | Total Loss: 0.002324 | Recon Loss: 0.001956 | Commit Loss: 0.000735 | Perplexity: 3216.446060
2025-10-13 13:03:43,400 Stage: Train 0.5 | Epoch: 374 | Iter: 419400 | Total Loss: 0.002331 | Recon Loss: 0.001964 | Commit Loss: 0.000734 | Perplexity: 3212.801420
2025-10-13 13:07:36,021 Stage: Train 0.5 | Epoch: 374 | Iter: 419600 | Total Loss: 0.002342 | Recon Loss: 0.001979 | Commit Loss: 0.000726 | Perplexity: 3215.865186
2025-10-13 13:11:29,155 Stage: Train 0.5 | Epoch: 374 | Iter: 419800 | Total Loss: 0.002299 | Recon Loss: 0.001930 | Commit Loss: 0.000739 | Perplexity: 3230.312544
Trainning Epoch:  81%|████████▏ | 402/494 [122:36:54<30:11:11, 1181.21s/it]Trainning Epoch:  81%|████████▏ | 402/494 [122:36:54<30:11:11, 1181.21s/it]2025-10-13 13:15:23,242 Stage: Train 0.5 | Epoch: 375 | Iter: 420000 | Total Loss: 0.002333 | Recon Loss: 0.001965 | Commit Loss: 0.000736 | Perplexity: 3224.093846
2025-10-13 13:15:23,243 Saving model at iteration 420000
2025-10-13 13:15:23,523 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000
2025-10-13 13:15:24,978 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000/model.safetensors
2025-10-13 13:15:26,653 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000/optimizer.bin
2025-10-13 13:15:26,654 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000/scheduler.bin
2025-10-13 13:15:26,654 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000/sampler.bin
2025-10-13 13:15:26,655 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_376_step_420000/random_states_0.pkl
2025-10-13 13:19:17,726 Stage: Train 0.5 | Epoch: 375 | Iter: 420200 | Total Loss: 0.002321 | Recon Loss: 0.001955 | Commit Loss: 0.000731 | Perplexity: 3212.258079
2025-10-13 13:23:08,724 Stage: Train 0.5 | Epoch: 375 | Iter: 420400 | Total Loss: 0.002345 | Recon Loss: 0.001978 | Commit Loss: 0.000732 | Perplexity: 3220.209039
2025-10-13 13:26:59,726 Stage: Train 0.5 | Epoch: 375 | Iter: 420600 | Total Loss: 0.002293 | Recon Loss: 0.001927 | Commit Loss: 0.000733 | Perplexity: 3221.744133
2025-10-13 13:30:50,802 Stage: Train 0.5 | Epoch: 375 | Iter: 420800 | Total Loss: 0.002343 | Recon Loss: 0.001978 | Commit Loss: 0.000730 | Perplexity: 3211.149543
Trainning Epoch:  82%|████████▏ | 403/494 [122:56:30<29:49:07, 1179.65s/it]Trainning Epoch:  82%|████████▏ | 403/494 [122:56:30<29:49:07, 1179.65s/it]2025-10-13 13:34:46,128 Stage: Train 0.5 | Epoch: 376 | Iter: 421000 | Total Loss: 0.002297 | Recon Loss: 0.001930 | Commit Loss: 0.000733 | Perplexity: 3230.144974
2025-10-13 13:38:39,437 Stage: Train 0.5 | Epoch: 376 | Iter: 421200 | Total Loss: 0.002345 | Recon Loss: 0.001980 | Commit Loss: 0.000730 | Perplexity: 3211.044404
2025-10-13 13:42:32,718 Stage: Train 0.5 | Epoch: 376 | Iter: 421400 | Total Loss: 0.002293 | Recon Loss: 0.001925 | Commit Loss: 0.000736 | Perplexity: 3224.357028
2025-10-13 13:46:25,889 Stage: Train 0.5 | Epoch: 376 | Iter: 421600 | Total Loss: 0.002309 | Recon Loss: 0.001943 | Commit Loss: 0.000730 | Perplexity: 3229.854846
2025-10-13 13:50:19,029 Stage: Train 0.5 | Epoch: 376 | Iter: 421800 | Total Loss: 0.002344 | Recon Loss: 0.001978 | Commit Loss: 0.000734 | Perplexity: 3222.366071
Trainning Epoch:  82%|████████▏ | 404/494 [123:16:14<29:31:27, 1180.97s/it]Trainning Epoch:  82%|████████▏ | 404/494 [123:16:14<29:31:27, 1180.97s/it]2025-10-13 13:54:15,511 Stage: Train 0.5 | Epoch: 377 | Iter: 422000 | Total Loss: 0.002334 | Recon Loss: 0.001970 | Commit Loss: 0.000727 | Perplexity: 3227.281871
2025-10-13 13:58:10,158 Stage: Train 0.5 | Epoch: 377 | Iter: 422200 | Total Loss: 0.002340 | Recon Loss: 0.001972 | Commit Loss: 0.000736 | Perplexity: 3222.678069
2025-10-13 14:02:04,990 Stage: Train 0.5 | Epoch: 377 | Iter: 422400 | Total Loss: 0.002305 | Recon Loss: 0.001935 | Commit Loss: 0.000739 | Perplexity: 3223.054398
2025-10-13 14:06:00,286 Stage: Train 0.5 | Epoch: 377 | Iter: 422600 | Total Loss: 0.002321 | Recon Loss: 0.001960 | Commit Loss: 0.000723 | Perplexity: 3226.693982
2025-10-13 14:09:55,193 Stage: Train 0.5 | Epoch: 377 | Iter: 422800 | Total Loss: 0.002325 | Recon Loss: 0.001956 | Commit Loss: 0.000737 | Perplexity: 3217.116699
Trainning Epoch:  82%|████████▏ | 405/494 [123:36:06<29:16:44, 1184.33s/it]Trainning Epoch:  82%|████████▏ | 405/494 [123:36:06<29:16:45, 1184.33s/it]2025-10-13 14:13:52,050 Stage: Train 0.5 | Epoch: 378 | Iter: 423000 | Total Loss: 0.002316 | Recon Loss: 0.001951 | Commit Loss: 0.000729 | Perplexity: 3215.698074
2025-10-13 14:17:45,403 Stage: Train 0.5 | Epoch: 378 | Iter: 423200 | Total Loss: 0.002344 | Recon Loss: 0.001980 | Commit Loss: 0.000728 | Perplexity: 3224.674976
2025-10-13 14:21:38,860 Stage: Train 0.5 | Epoch: 378 | Iter: 423400 | Total Loss: 0.002324 | Recon Loss: 0.001959 | Commit Loss: 0.000731 | Perplexity: 3219.426732
2025-10-13 14:25:32,327 Stage: Train 0.5 | Epoch: 378 | Iter: 423600 | Total Loss: 0.002307 | Recon Loss: 0.001946 | Commit Loss: 0.000722 | Perplexity: 3214.143931
2025-10-13 14:29:25,859 Stage: Train 0.5 | Epoch: 378 | Iter: 423800 | Total Loss: 0.002311 | Recon Loss: 0.001944 | Commit Loss: 0.000734 | Perplexity: 3219.491517
Trainning Epoch:  82%|████████▏ | 406/494 [123:55:51<28:57:23, 1184.59s/it]Trainning Epoch:  82%|████████▏ | 406/494 [123:55:51<28:57:23, 1184.59s/it]2025-10-13 14:33:23,374 Stage: Train 0.5 | Epoch: 379 | Iter: 424000 | Total Loss: 0.002305 | Recon Loss: 0.001937 | Commit Loss: 0.000735 | Perplexity: 3221.829673
2025-10-13 14:37:17,649 Stage: Train 0.5 | Epoch: 379 | Iter: 424200 | Total Loss: 0.002299 | Recon Loss: 0.001934 | Commit Loss: 0.000730 | Perplexity: 3212.873323
2025-10-13 14:41:12,430 Stage: Train 0.5 | Epoch: 379 | Iter: 424400 | Total Loss: 0.002322 | Recon Loss: 0.001956 | Commit Loss: 0.000731 | Perplexity: 3226.980760
2025-10-13 14:45:07,575 Stage: Train 0.5 | Epoch: 379 | Iter: 424600 | Total Loss: 0.002339 | Recon Loss: 0.001972 | Commit Loss: 0.000733 | Perplexity: 3218.560476
2025-10-13 14:49:02,694 Stage: Train 0.5 | Epoch: 379 | Iter: 424800 | Total Loss: 0.002316 | Recon Loss: 0.001952 | Commit Loss: 0.000728 | Perplexity: 3220.103722
Trainning Epoch:  82%|████████▏ | 407/494 [124:15:44<28:41:29, 1187.23s/it]Trainning Epoch:  82%|████████▏ | 407/494 [124:15:44<28:41:29, 1187.23s/it]2025-10-13 14:53:00,788 Stage: Train 0.5 | Epoch: 380 | Iter: 425000 | Total Loss: 0.002308 | Recon Loss: 0.001945 | Commit Loss: 0.000725 | Perplexity: 3214.800544
2025-10-13 14:56:55,598 Stage: Train 0.5 | Epoch: 380 | Iter: 425200 | Total Loss: 0.002310 | Recon Loss: 0.001945 | Commit Loss: 0.000731 | Perplexity: 3233.820205
2025-10-13 15:00:50,887 Stage: Train 0.5 | Epoch: 380 | Iter: 425400 | Total Loss: 0.002313 | Recon Loss: 0.001947 | Commit Loss: 0.000732 | Perplexity: 3224.818994
2025-10-13 15:04:46,202 Stage: Train 0.5 | Epoch: 380 | Iter: 425600 | Total Loss: 0.002328 | Recon Loss: 0.001962 | Commit Loss: 0.000732 | Perplexity: 3227.523718
2025-10-13 15:08:41,470 Stage: Train 0.5 | Epoch: 380 | Iter: 425800 | Total Loss: 0.002342 | Recon Loss: 0.001979 | Commit Loss: 0.000726 | Perplexity: 3217.395676
Trainning Epoch:  83%|████████▎ | 408/494 [124:35:38<28:24:28, 1189.17s/it]Trainning Epoch:  83%|████████▎ | 408/494 [124:35:38<28:24:28, 1189.17s/it]2025-10-13 15:12:38,399 Stage: Train 0.5 | Epoch: 381 | Iter: 426000 | Total Loss: 0.002317 | Recon Loss: 0.001952 | Commit Loss: 0.000730 | Perplexity: 3217.625411
2025-10-13 15:16:31,150 Stage: Train 0.5 | Epoch: 381 | Iter: 426200 | Total Loss: 0.002292 | Recon Loss: 0.001929 | Commit Loss: 0.000726 | Perplexity: 3225.972843
2025-10-13 15:20:24,484 Stage: Train 0.5 | Epoch: 381 | Iter: 426400 | Total Loss: 0.002325 | Recon Loss: 0.001955 | Commit Loss: 0.000741 | Perplexity: 3223.777356
2025-10-13 15:24:18,095 Stage: Train 0.5 | Epoch: 381 | Iter: 426600 | Total Loss: 0.002319 | Recon Loss: 0.001955 | Commit Loss: 0.000729 | Perplexity: 3219.137040
2025-10-13 15:28:12,055 Stage: Train 0.5 | Epoch: 381 | Iter: 426800 | Total Loss: 0.002315 | Recon Loss: 0.001951 | Commit Loss: 0.000726 | Perplexity: 3225.146239
Trainning Epoch:  83%|████████▎ | 409/494 [124:55:23<28:02:58, 1187.98s/it]Trainning Epoch:  83%|████████▎ | 409/494 [124:55:23<28:02:58, 1187.98s/it]2025-10-13 15:32:08,308 Stage: Train 0.5 | Epoch: 382 | Iter: 427000 | Total Loss: 0.002298 | Recon Loss: 0.001934 | Commit Loss: 0.000727 | Perplexity: 3221.685474
2025-10-13 15:35:58,217 Stage: Train 0.5 | Epoch: 382 | Iter: 427200 | Total Loss: 0.002329 | Recon Loss: 0.001958 | Commit Loss: 0.000742 | Perplexity: 3230.980844
2025-10-13 15:39:49,001 Stage: Train 0.5 | Epoch: 382 | Iter: 427400 | Total Loss: 0.002300 | Recon Loss: 0.001931 | Commit Loss: 0.000737 | Perplexity: 3209.761110
2025-10-13 15:43:40,014 Stage: Train 0.5 | Epoch: 382 | Iter: 427600 | Total Loss: 0.002324 | Recon Loss: 0.001957 | Commit Loss: 0.000734 | Perplexity: 3222.240540
2025-10-13 15:47:31,190 Stage: Train 0.5 | Epoch: 382 | Iter: 427800 | Total Loss: 0.002311 | Recon Loss: 0.001945 | Commit Loss: 0.000732 | Perplexity: 3229.932803
Trainning Epoch:  83%|████████▎ | 410/494 [125:14:55<27:36:26, 1183.17s/it]Trainning Epoch:  83%|████████▎ | 410/494 [125:14:55<27:36:26, 1183.17s/it]2025-10-13 15:51:25,912 Stage: Train 0.5 | Epoch: 383 | Iter: 428000 | Total Loss: 0.002322 | Recon Loss: 0.001953 | Commit Loss: 0.000738 | Perplexity: 3234.395269
2025-10-13 15:55:19,350 Stage: Train 0.5 | Epoch: 383 | Iter: 428200 | Total Loss: 0.002297 | Recon Loss: 0.001932 | Commit Loss: 0.000730 | Perplexity: 3227.359810
2025-10-13 15:59:13,582 Stage: Train 0.5 | Epoch: 383 | Iter: 428400 | Total Loss: 0.002340 | Recon Loss: 0.001977 | Commit Loss: 0.000724 | Perplexity: 3217.945338
2025-10-13 16:03:08,314 Stage: Train 0.5 | Epoch: 383 | Iter: 428600 | Total Loss: 0.002281 | Recon Loss: 0.001914 | Commit Loss: 0.000734 | Perplexity: 3232.055725
2025-10-13 16:07:02,887 Stage: Train 0.5 | Epoch: 383 | Iter: 428800 | Total Loss: 0.002303 | Recon Loss: 0.001937 | Commit Loss: 0.000732 | Perplexity: 3227.284561
Trainning Epoch:  83%|████████▎ | 411/494 [125:34:45<27:19:23, 1185.10s/it]Trainning Epoch:  83%|████████▎ | 411/494 [125:34:45<27:19:23, 1185.10s/it]2025-10-13 16:11:01,301 Stage: Train 0.5 | Epoch: 384 | Iter: 429000 | Total Loss: 0.002304 | Recon Loss: 0.001937 | Commit Loss: 0.000733 | Perplexity: 3222.897792
2025-10-13 16:14:55,334 Stage: Train 0.5 | Epoch: 384 | Iter: 429200 | Total Loss: 0.002280 | Recon Loss: 0.001915 | Commit Loss: 0.000729 | Perplexity: 3213.429476
2025-10-13 16:18:49,569 Stage: Train 0.5 | Epoch: 384 | Iter: 429400 | Total Loss: 0.002324 | Recon Loss: 0.001960 | Commit Loss: 0.000729 | Perplexity: 3236.213293
2025-10-13 16:22:43,597 Stage: Train 0.5 | Epoch: 384 | Iter: 429600 | Total Loss: 0.002312 | Recon Loss: 0.001942 | Commit Loss: 0.000740 | Perplexity: 3222.822870
2025-10-13 16:26:37,664 Stage: Train 0.5 | Epoch: 384 | Iter: 429800 | Total Loss: 0.002290 | Recon Loss: 0.001917 | Commit Loss: 0.000745 | Perplexity: 3233.293472
2025-10-13 16:30:31,404 Stage: Train 0.5 | Epoch: 384 | Iter: 430000 | Total Loss: 0.002313 | Recon Loss: 0.001945 | Commit Loss: 0.000737 | Perplexity: 3216.833301
Trainning Epoch:  83%|████████▎ | 412/494 [125:54:34<27:01:25, 1186.41s/it]Trainning Epoch:  83%|████████▎ | 412/494 [125:54:34<27:01:25, 1186.41s/it]2025-10-13 16:34:30,659 Stage: Train 0.5 | Epoch: 385 | Iter: 430200 | Total Loss: 0.002277 | Recon Loss: 0.001916 | Commit Loss: 0.000724 | Perplexity: 3223.981353
2025-10-13 16:38:26,712 Stage: Train 0.5 | Epoch: 385 | Iter: 430400 | Total Loss: 0.002318 | Recon Loss: 0.001954 | Commit Loss: 0.000728 | Perplexity: 3218.640564
2025-10-13 16:42:22,516 Stage: Train 0.5 | Epoch: 385 | Iter: 430600 | Total Loss: 0.002314 | Recon Loss: 0.001945 | Commit Loss: 0.000737 | Perplexity: 3225.694210
2025-10-13 16:46:18,957 Stage: Train 0.5 | Epoch: 385 | Iter: 430800 | Total Loss: 0.002334 | Recon Loss: 0.001967 | Commit Loss: 0.000732 | Perplexity: 3222.596366
2025-10-13 16:50:14,833 Stage: Train 0.5 | Epoch: 385 | Iter: 431000 | Total Loss: 0.002288 | Recon Loss: 0.001921 | Commit Loss: 0.000734 | Perplexity: 3223.624963
Trainning Epoch:  84%|████████▎ | 413/494 [126:14:33<26:46:41, 1190.15s/it]Trainning Epoch:  84%|████████▎ | 413/494 [126:14:33<26:46:41, 1190.15s/it]2025-10-13 16:54:10,587 Stage: Train 0.5 | Epoch: 386 | Iter: 431200 | Total Loss: 0.002336 | Recon Loss: 0.001965 | Commit Loss: 0.000740 | Perplexity: 3218.335402
2025-10-13 16:58:03,211 Stage: Train 0.5 | Epoch: 386 | Iter: 431400 | Total Loss: 0.002292 | Recon Loss: 0.001929 | Commit Loss: 0.000727 | Perplexity: 3227.371947
2025-10-13 17:01:56,137 Stage: Train 0.5 | Epoch: 386 | Iter: 431600 | Total Loss: 0.002310 | Recon Loss: 0.001952 | Commit Loss: 0.000717 | Perplexity: 3217.029554
2025-10-13 17:05:49,176 Stage: Train 0.5 | Epoch: 386 | Iter: 431800 | Total Loss: 0.002304 | Recon Loss: 0.001932 | Commit Loss: 0.000745 | Perplexity: 3229.369060
2025-10-13 17:09:42,418 Stage: Train 0.5 | Epoch: 386 | Iter: 432000 | Total Loss: 0.002305 | Recon Loss: 0.001939 | Commit Loss: 0.000731 | Perplexity: 3237.127091
Trainning Epoch:  84%|████████▍ | 414/494 [126:34:16<26:23:45, 1187.82s/it]Trainning Epoch:  84%|████████▍ | 414/494 [126:34:16<26:23:45, 1187.82s/it]2025-10-13 17:13:39,272 Stage: Train 0.5 | Epoch: 387 | Iter: 432200 | Total Loss: 0.002284 | Recon Loss: 0.001923 | Commit Loss: 0.000722 | Perplexity: 3227.205488
2025-10-13 17:17:33,979 Stage: Train 0.5 | Epoch: 387 | Iter: 432400 | Total Loss: 0.002294 | Recon Loss: 0.001928 | Commit Loss: 0.000733 | Perplexity: 3219.233673
2025-10-13 17:21:28,970 Stage: Train 0.5 | Epoch: 387 | Iter: 432600 | Total Loss: 0.002287 | Recon Loss: 0.001920 | Commit Loss: 0.000734 | Perplexity: 3224.693467
2025-10-13 17:25:24,017 Stage: Train 0.5 | Epoch: 387 | Iter: 432800 | Total Loss: 0.002323 | Recon Loss: 0.001957 | Commit Loss: 0.000732 | Perplexity: 3222.455897
2025-10-13 17:29:18,754 Stage: Train 0.5 | Epoch: 387 | Iter: 433000 | Total Loss: 0.002310 | Recon Loss: 0.001943 | Commit Loss: 0.000734 | Perplexity: 3229.491866
Trainning Epoch:  84%|████████▍ | 415/494 [126:54:07<26:05:32, 1189.02s/it]Trainning Epoch:  84%|████████▍ | 415/494 [126:54:07<26:05:32, 1189.02s/it]2025-10-13 17:33:16,341 Stage: Train 0.5 | Epoch: 388 | Iter: 433200 | Total Loss: 0.002288 | Recon Loss: 0.001924 | Commit Loss: 0.000727 | Perplexity: 3217.529011
2025-10-13 17:37:09,870 Stage: Train 0.5 | Epoch: 388 | Iter: 433400 | Total Loss: 0.002312 | Recon Loss: 0.001948 | Commit Loss: 0.000729 | Perplexity: 3222.487113
2025-10-13 17:41:03,601 Stage: Train 0.5 | Epoch: 388 | Iter: 433600 | Total Loss: 0.002318 | Recon Loss: 0.001949 | Commit Loss: 0.000736 | Perplexity: 3223.837179
2025-10-13 17:44:57,062 Stage: Train 0.5 | Epoch: 388 | Iter: 433800 | Total Loss: 0.002320 | Recon Loss: 0.001953 | Commit Loss: 0.000734 | Perplexity: 3230.066141
2025-10-13 17:48:50,694 Stage: Train 0.5 | Epoch: 388 | Iter: 434000 | Total Loss: 0.002278 | Recon Loss: 0.001914 | Commit Loss: 0.000727 | Perplexity: 3214.812787
Trainning Epoch:  84%|████████▍ | 416/494 [127:13:54<25:44:51, 1188.35s/it]Trainning Epoch:  84%|████████▍ | 416/494 [127:13:54<25:44:51, 1188.35s/it]2025-10-13 17:52:47,401 Stage: Train 0.5 | Epoch: 389 | Iter: 434200 | Total Loss: 0.002303 | Recon Loss: 0.001937 | Commit Loss: 0.000731 | Perplexity: 3218.112599
2025-10-13 17:56:41,327 Stage: Train 0.5 | Epoch: 389 | Iter: 434400 | Total Loss: 0.002318 | Recon Loss: 0.001951 | Commit Loss: 0.000735 | Perplexity: 3222.145764
2025-10-13 18:00:35,516 Stage: Train 0.5 | Epoch: 389 | Iter: 434600 | Total Loss: 0.002306 | Recon Loss: 0.001938 | Commit Loss: 0.000735 | Perplexity: 3223.264813
2025-10-13 18:04:29,412 Stage: Train 0.5 | Epoch: 389 | Iter: 434800 | Total Loss: 0.002325 | Recon Loss: 0.001952 | Commit Loss: 0.000747 | Perplexity: 3225.045823
2025-10-13 18:08:23,998 Stage: Train 0.5 | Epoch: 389 | Iter: 435000 | Total Loss: 0.002285 | Recon Loss: 0.001925 | Commit Loss: 0.000720 | Perplexity: 3223.772562
Trainning Epoch:  84%|████████▍ | 417/494 [127:33:43<25:25:10, 1188.45s/it]Trainning Epoch:  84%|████████▍ | 417/494 [127:33:43<25:25:10, 1188.45s/it]2025-10-13 18:12:20,679 Stage: Train 0.5 | Epoch: 390 | Iter: 435200 | Total Loss: 0.002301 | Recon Loss: 0.001939 | Commit Loss: 0.000724 | Perplexity: 3228.953695
2025-10-13 18:16:14,495 Stage: Train 0.5 | Epoch: 390 | Iter: 435400 | Total Loss: 0.002304 | Recon Loss: 0.001938 | Commit Loss: 0.000731 | Perplexity: 3221.781387
2025-10-13 18:20:08,054 Stage: Train 0.5 | Epoch: 390 | Iter: 435600 | Total Loss: 0.002314 | Recon Loss: 0.001945 | Commit Loss: 0.000739 | Perplexity: 3223.576342
2025-10-13 18:24:01,906 Stage: Train 0.5 | Epoch: 390 | Iter: 435800 | Total Loss: 0.002285 | Recon Loss: 0.001923 | Commit Loss: 0.000726 | Perplexity: 3219.413627
2025-10-13 18:27:56,588 Stage: Train 0.5 | Epoch: 390 | Iter: 436000 | Total Loss: 0.002300 | Recon Loss: 0.001934 | Commit Loss: 0.000732 | Perplexity: 3227.506484
Trainning Epoch:  85%|████████▍ | 418/494 [127:53:31<25:05:06, 1188.24s/it]Trainning Epoch:  85%|████████▍ | 418/494 [127:53:31<25:05:06, 1188.24s/it]2025-10-13 18:31:53,268 Stage: Train 0.5 | Epoch: 391 | Iter: 436200 | Total Loss: 0.002283 | Recon Loss: 0.001921 | Commit Loss: 0.000724 | Perplexity: 3233.683138
2025-10-13 18:35:47,598 Stage: Train 0.5 | Epoch: 391 | Iter: 436400 | Total Loss: 0.002290 | Recon Loss: 0.001924 | Commit Loss: 0.000733 | Perplexity: 3221.762151
2025-10-13 18:39:42,502 Stage: Train 0.5 | Epoch: 391 | Iter: 436600 | Total Loss: 0.002307 | Recon Loss: 0.001941 | Commit Loss: 0.000731 | Perplexity: 3228.650021
2025-10-13 18:43:37,483 Stage: Train 0.5 | Epoch: 391 | Iter: 436800 | Total Loss: 0.002296 | Recon Loss: 0.001931 | Commit Loss: 0.000730 | Perplexity: 3227.987745
2025-10-13 18:47:32,320 Stage: Train 0.5 | Epoch: 391 | Iter: 437000 | Total Loss: 0.002296 | Recon Loss: 0.001928 | Commit Loss: 0.000737 | Perplexity: 3225.556504
Trainning Epoch:  85%|████████▍ | 419/494 [128:13:22<24:46:33, 1189.24s/it]Trainning Epoch:  85%|████████▍ | 419/494 [128:13:22<24:46:33, 1189.24s/it]2025-10-13 18:51:30,134 Stage: Train 0.5 | Epoch: 392 | Iter: 437200 | Total Loss: 0.002310 | Recon Loss: 0.001945 | Commit Loss: 0.000729 | Perplexity: 3228.688109
2025-10-13 18:55:23,781 Stage: Train 0.5 | Epoch: 392 | Iter: 437400 | Total Loss: 0.002308 | Recon Loss: 0.001944 | Commit Loss: 0.000728 | Perplexity: 3217.983276
2025-10-13 18:59:17,451 Stage: Train 0.5 | Epoch: 392 | Iter: 437600 | Total Loss: 0.002260 | Recon Loss: 0.001894 | Commit Loss: 0.000731 | Perplexity: 3226.563848
2025-10-13 19:03:10,811 Stage: Train 0.5 | Epoch: 392 | Iter: 437800 | Total Loss: 0.002328 | Recon Loss: 0.001962 | Commit Loss: 0.000733 | Perplexity: 3235.307448
2025-10-13 19:07:04,188 Stage: Train 0.5 | Epoch: 392 | Iter: 438000 | Total Loss: 0.002303 | Recon Loss: 0.001936 | Commit Loss: 0.000735 | Perplexity: 3222.910372
Trainning Epoch:  85%|████████▌ | 420/494 [128:33:08<24:25:35, 1188.32s/it]Trainning Epoch:  85%|████████▌ | 420/494 [128:33:08<24:25:35, 1188.32s/it]2025-10-13 19:11:01,536 Stage: Train 0.5 | Epoch: 393 | Iter: 438200 | Total Loss: 0.002314 | Recon Loss: 0.001948 | Commit Loss: 0.000731 | Perplexity: 3208.547534
2025-10-13 19:14:57,061 Stage: Train 0.5 | Epoch: 393 | Iter: 438400 | Total Loss: 0.002339 | Recon Loss: 0.001971 | Commit Loss: 0.000736 | Perplexity: 3222.229427
2025-10-13 19:18:52,590 Stage: Train 0.5 | Epoch: 393 | Iter: 438600 | Total Loss: 0.002268 | Recon Loss: 0.001905 | Commit Loss: 0.000725 | Perplexity: 3227.985924
2025-10-13 19:22:48,192 Stage: Train 0.5 | Epoch: 393 | Iter: 438800 | Total Loss: 0.002304 | Recon Loss: 0.001941 | Commit Loss: 0.000725 | Perplexity: 3217.515698
2025-10-13 19:26:43,925 Stage: Train 0.5 | Epoch: 393 | Iter: 439000 | Total Loss: 0.002297 | Recon Loss: 0.001931 | Commit Loss: 0.000733 | Perplexity: 3218.643446
Trainning Epoch:  85%|████████▌ | 421/494 [128:53:05<24:08:45, 1190.76s/it]Trainning Epoch:  85%|████████▌ | 421/494 [128:53:05<24:08:45, 1190.76s/it]2025-10-13 19:30:42,549 Stage: Train 0.5 | Epoch: 394 | Iter: 439200 | Total Loss: 0.002338 | Recon Loss: 0.001975 | Commit Loss: 0.000726 | Perplexity: 3227.897394
2025-10-13 19:34:36,582 Stage: Train 0.5 | Epoch: 394 | Iter: 439400 | Total Loss: 0.002272 | Recon Loss: 0.001905 | Commit Loss: 0.000734 | Perplexity: 3225.233114
2025-10-13 19:38:31,269 Stage: Train 0.5 | Epoch: 394 | Iter: 439600 | Total Loss: 0.002307 | Recon Loss: 0.001938 | Commit Loss: 0.000739 | Perplexity: 3229.566006
2025-10-13 19:42:25,611 Stage: Train 0.5 | Epoch: 394 | Iter: 439800 | Total Loss: 0.002314 | Recon Loss: 0.001949 | Commit Loss: 0.000730 | Perplexity: 3220.443398
2025-10-13 19:46:20,032 Stage: Train 0.5 | Epoch: 394 | Iter: 440000 | Total Loss: 0.002306 | Recon Loss: 0.001939 | Commit Loss: 0.000732 | Perplexity: 3229.735023
2025-10-13 19:46:20,033 Saving model at iteration 440000
2025-10-13 19:46:20,214 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000
2025-10-13 19:46:21,586 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000/model.safetensors
2025-10-13 19:46:23,344 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000/optimizer.bin
2025-10-13 19:46:23,344 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000/scheduler.bin
2025-10-13 19:46:23,344 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000/sampler.bin
2025-10-13 19:46:23,345 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_395_step_440000/random_states_0.pkl
Trainning Epoch:  85%|████████▌ | 422/494 [129:12:58<23:49:54, 1191.58s/it]Trainning Epoch:  85%|████████▌ | 422/494 [129:12:58<23:49:54, 1191.59s/it]2025-10-13 19:50:18,845 Stage: Train 0.5 | Epoch: 395 | Iter: 440200 | Total Loss: 0.002276 | Recon Loss: 0.001912 | Commit Loss: 0.000728 | Perplexity: 3218.790107
2025-10-13 19:54:09,058 Stage: Train 0.5 | Epoch: 395 | Iter: 440400 | Total Loss: 0.002295 | Recon Loss: 0.001929 | Commit Loss: 0.000731 | Perplexity: 3234.218099
2025-10-13 19:57:59,282 Stage: Train 0.5 | Epoch: 395 | Iter: 440600 | Total Loss: 0.002271 | Recon Loss: 0.001904 | Commit Loss: 0.000733 | Perplexity: 3234.361060
2025-10-13 20:01:49,447 Stage: Train 0.5 | Epoch: 395 | Iter: 440800 | Total Loss: 0.002310 | Recon Loss: 0.001944 | Commit Loss: 0.000731 | Perplexity: 3224.834418
2025-10-13 20:05:40,273 Stage: Train 0.5 | Epoch: 395 | Iter: 441000 | Total Loss: 0.002282 | Recon Loss: 0.001918 | Commit Loss: 0.000728 | Perplexity: 3233.217163
Trainning Epoch:  86%|████████▌ | 423/494 [129:32:28<23:22:24, 1185.14s/it]Trainning Epoch:  86%|████████▌ | 423/494 [129:32:28<23:22:25, 1185.14s/it]2025-10-13 20:09:35,931 Stage: Train 0.5 | Epoch: 396 | Iter: 441200 | Total Loss: 0.002275 | Recon Loss: 0.001911 | Commit Loss: 0.000727 | Perplexity: 3224.517599
2025-10-13 20:13:31,056 Stage: Train 0.5 | Epoch: 396 | Iter: 441400 | Total Loss: 0.002291 | Recon Loss: 0.001925 | Commit Loss: 0.000732 | Perplexity: 3221.114542
2025-10-13 20:17:26,170 Stage: Train 0.5 | Epoch: 396 | Iter: 441600 | Total Loss: 0.002298 | Recon Loss: 0.001928 | Commit Loss: 0.000740 | Perplexity: 3220.812524
2025-10-13 20:21:21,384 Stage: Train 0.5 | Epoch: 396 | Iter: 441800 | Total Loss: 0.002300 | Recon Loss: 0.001936 | Commit Loss: 0.000729 | Perplexity: 3229.899773
2025-10-13 20:25:16,581 Stage: Train 0.5 | Epoch: 396 | Iter: 442000 | Total Loss: 0.002302 | Recon Loss: 0.001933 | Commit Loss: 0.000737 | Perplexity: 3236.348685
Trainning Epoch:  86%|████████▌ | 424/494 [129:52:23<23:05:56, 1187.95s/it]Trainning Epoch:  86%|████████▌ | 424/494 [129:52:23<23:05:56, 1187.95s/it]2025-10-13 20:29:14,523 Stage: Train 0.5 | Epoch: 397 | Iter: 442200 | Total Loss: 0.002288 | Recon Loss: 0.001924 | Commit Loss: 0.000728 | Perplexity: 3222.451953
2025-10-13 20:33:06,772 Stage: Train 0.5 | Epoch: 397 | Iter: 442400 | Total Loss: 0.002331 | Recon Loss: 0.001969 | Commit Loss: 0.000724 | Perplexity: 3226.065759
2025-10-13 20:36:59,678 Stage: Train 0.5 | Epoch: 397 | Iter: 442600 | Total Loss: 0.002271 | Recon Loss: 0.001904 | Commit Loss: 0.000734 | Perplexity: 3230.959974
2025-10-13 20:40:53,370 Stage: Train 0.5 | Epoch: 397 | Iter: 442800 | Total Loss: 0.002294 | Recon Loss: 0.001930 | Commit Loss: 0.000728 | Perplexity: 3234.981630
2025-10-13 20:44:46,884 Stage: Train 0.5 | Epoch: 397 | Iter: 443000 | Total Loss: 0.002311 | Recon Loss: 0.001949 | Commit Loss: 0.000725 | Perplexity: 3210.695808
Trainning Epoch:  86%|████████▌ | 425/494 [130:12:08<22:45:00, 1186.97s/it]Trainning Epoch:  86%|████████▌ | 425/494 [130:12:08<22:45:01, 1186.97s/it]2025-10-13 20:48:44,086 Stage: Train 0.5 | Epoch: 398 | Iter: 443200 | Total Loss: 0.002308 | Recon Loss: 0.001943 | Commit Loss: 0.000729 | Perplexity: 3230.062676
2025-10-13 20:52:36,911 Stage: Train 0.5 | Epoch: 398 | Iter: 443400 | Total Loss: 0.002303 | Recon Loss: 0.001941 | Commit Loss: 0.000725 | Perplexity: 3226.588225
2025-10-13 20:56:30,608 Stage: Train 0.5 | Epoch: 398 | Iter: 443600 | Total Loss: 0.002252 | Recon Loss: 0.001885 | Commit Loss: 0.000734 | Perplexity: 3217.768846
2025-10-13 21:00:24,442 Stage: Train 0.5 | Epoch: 398 | Iter: 443800 | Total Loss: 0.002308 | Recon Loss: 0.001941 | Commit Loss: 0.000734 | Perplexity: 3226.420886
2025-10-13 21:04:18,783 Stage: Train 0.5 | Epoch: 398 | Iter: 444000 | Total Loss: 0.002258 | Recon Loss: 0.001893 | Commit Loss: 0.000729 | Perplexity: 3218.367924
Trainning Epoch:  86%|████████▌ | 426/494 [130:31:55<22:25:23, 1187.11s/it]Trainning Epoch:  86%|████████▌ | 426/494 [130:31:55<22:25:24, 1187.12s/it]2025-10-13 21:08:17,005 Stage: Train 0.5 | Epoch: 399 | Iter: 444200 | Total Loss: 0.002290 | Recon Loss: 0.001924 | Commit Loss: 0.000731 | Perplexity: 3233.133119
2025-10-13 21:12:11,795 Stage: Train 0.5 | Epoch: 399 | Iter: 444400 | Total Loss: 0.002310 | Recon Loss: 0.001939 | Commit Loss: 0.000742 | Perplexity: 3221.033519
2025-10-13 21:16:07,070 Stage: Train 0.5 | Epoch: 399 | Iter: 444600 | Total Loss: 0.002270 | Recon Loss: 0.001907 | Commit Loss: 0.000726 | Perplexity: 3229.216191
2025-10-13 21:20:02,268 Stage: Train 0.5 | Epoch: 399 | Iter: 444800 | Total Loss: 0.002271 | Recon Loss: 0.001902 | Commit Loss: 0.000738 | Perplexity: 3224.528361
2025-10-13 21:23:57,754 Stage: Train 0.5 | Epoch: 399 | Iter: 445000 | Total Loss: 0.002297 | Recon Loss: 0.001934 | Commit Loss: 0.000726 | Perplexity: 3223.259193
2025-10-13 21:27:53,372 Stage: Train 0.5 | Epoch: 399 | Iter: 445200 | Total Loss: 0.002271 | Recon Loss: 0.001901 | Commit Loss: 0.000740 | Perplexity: 3234.139524
Trainning Epoch:  86%|████████▋ | 427/494 [130:51:50<22:08:22, 1189.58s/it]Trainning Epoch:  86%|████████▋ | 427/494 [130:51:50<22:08:22, 1189.59s/it]2025-10-13 21:31:48,680 Stage: Train 0.5 | Epoch: 400 | Iter: 445400 | Total Loss: 0.002308 | Recon Loss: 0.001945 | Commit Loss: 0.000727 | Perplexity: 3230.196561
2025-10-13 21:35:42,188 Stage: Train 0.5 | Epoch: 400 | Iter: 445600 | Total Loss: 0.002291 | Recon Loss: 0.001927 | Commit Loss: 0.000728 | Perplexity: 3215.706516
2025-10-13 21:39:36,063 Stage: Train 0.5 | Epoch: 400 | Iter: 445800 | Total Loss: 0.002291 | Recon Loss: 0.001926 | Commit Loss: 0.000730 | Perplexity: 3224.407885
2025-10-13 21:43:29,651 Stage: Train 0.5 | Epoch: 400 | Iter: 446000 | Total Loss: 0.002287 | Recon Loss: 0.001921 | Commit Loss: 0.000733 | Perplexity: 3231.230085
2025-10-13 21:47:23,766 Stage: Train 0.5 | Epoch: 400 | Iter: 446200 | Total Loss: 0.002257 | Recon Loss: 0.001891 | Commit Loss: 0.000731 | Perplexity: 3236.112251
Trainning Epoch:  87%|████████▋ | 428/494 [131:11:36<21:47:12, 1188.37s/it]Trainning Epoch:  87%|████████▋ | 428/494 [131:11:36<21:47:12, 1188.37s/it]2025-10-13 21:51:22,363 Stage: Train 0.5 | Epoch: 401 | Iter: 446400 | Total Loss: 0.002284 | Recon Loss: 0.001920 | Commit Loss: 0.000728 | Perplexity: 3233.971074
2025-10-13 21:55:17,064 Stage: Train 0.5 | Epoch: 401 | Iter: 446600 | Total Loss: 0.002319 | Recon Loss: 0.001952 | Commit Loss: 0.000733 | Perplexity: 3235.699265
2025-10-13 21:59:11,995 Stage: Train 0.5 | Epoch: 401 | Iter: 446800 | Total Loss: 0.002295 | Recon Loss: 0.001931 | Commit Loss: 0.000728 | Perplexity: 3229.412877
2025-10-13 22:03:07,061 Stage: Train 0.5 | Epoch: 401 | Iter: 447000 | Total Loss: 0.002310 | Recon Loss: 0.001949 | Commit Loss: 0.000723 | Perplexity: 3223.237372
2025-10-13 22:07:01,884 Stage: Train 0.5 | Epoch: 401 | Iter: 447200 | Total Loss: 0.002298 | Recon Loss: 0.001934 | Commit Loss: 0.000728 | Perplexity: 3227.079596
Trainning Epoch:  87%|████████▋ | 429/494 [131:31:29<21:29:00, 1189.85s/it]Trainning Epoch:  87%|████████▋ | 429/494 [131:31:29<21:29:00, 1189.85s/it]2025-10-13 22:10:58,040 Stage: Train 0.5 | Epoch: 402 | Iter: 447400 | Total Loss: 0.002243 | Recon Loss: 0.001882 | Commit Loss: 0.000721 | Perplexity: 3225.339418
2025-10-13 22:14:51,034 Stage: Train 0.5 | Epoch: 402 | Iter: 447600 | Total Loss: 0.002284 | Recon Loss: 0.001919 | Commit Loss: 0.000730 | Perplexity: 3224.597552
2025-10-13 22:18:44,294 Stage: Train 0.5 | Epoch: 402 | Iter: 447800 | Total Loss: 0.002271 | Recon Loss: 0.001905 | Commit Loss: 0.000731 | Perplexity: 3232.636251
2025-10-13 22:22:37,533 Stage: Train 0.5 | Epoch: 402 | Iter: 448000 | Total Loss: 0.002296 | Recon Loss: 0.001930 | Commit Loss: 0.000731 | Perplexity: 3228.368196
2025-10-13 22:26:30,768 Stage: Train 0.5 | Epoch: 402 | Iter: 448200 | Total Loss: 0.002316 | Recon Loss: 0.001949 | Commit Loss: 0.000734 | Perplexity: 3228.187216
Trainning Epoch:  87%|████████▋ | 430/494 [131:51:13<21:07:13, 1188.03s/it]Trainning Epoch:  87%|████████▋ | 430/494 [131:51:13<21:07:14, 1188.03s/it]2025-10-13 22:30:27,451 Stage: Train 0.5 | Epoch: 403 | Iter: 448400 | Total Loss: 0.002252 | Recon Loss: 0.001888 | Commit Loss: 0.000727 | Perplexity: 3226.098273
2025-10-13 22:34:21,937 Stage: Train 0.5 | Epoch: 403 | Iter: 448600 | Total Loss: 0.002296 | Recon Loss: 0.001930 | Commit Loss: 0.000733 | Perplexity: 3228.479839
2025-10-13 22:38:16,607 Stage: Train 0.5 | Epoch: 403 | Iter: 448800 | Total Loss: 0.002287 | Recon Loss: 0.001922 | Commit Loss: 0.000730 | Perplexity: 3226.340402
2025-10-13 22:42:11,195 Stage: Train 0.5 | Epoch: 403 | Iter: 449000 | Total Loss: 0.002276 | Recon Loss: 0.001911 | Commit Loss: 0.000731 | Perplexity: 3234.930807
2025-10-13 22:46:06,122 Stage: Train 0.5 | Epoch: 403 | Iter: 449200 | Total Loss: 0.002296 | Recon Loss: 0.001932 | Commit Loss: 0.000728 | Perplexity: 3233.407635
Trainning Epoch:  87%|████████▋ | 431/494 [132:11:04<20:48:26, 1188.99s/it]Trainning Epoch:  87%|████████▋ | 431/494 [132:11:04<20:48:26, 1188.99s/it]2025-10-13 22:50:04,259 Stage: Train 0.5 | Epoch: 404 | Iter: 449400 | Total Loss: 0.002261 | Recon Loss: 0.001891 | Commit Loss: 0.000739 | Perplexity: 3227.763363
2025-10-13 22:53:59,639 Stage: Train 0.5 | Epoch: 404 | Iter: 449600 | Total Loss: 0.002286 | Recon Loss: 0.001920 | Commit Loss: 0.000732 | Perplexity: 3226.624222
2025-10-13 22:57:55,233 Stage: Train 0.5 | Epoch: 404 | Iter: 449800 | Total Loss: 0.002280 | Recon Loss: 0.001916 | Commit Loss: 0.000728 | Perplexity: 3221.117333
2025-10-13 23:01:50,528 Stage: Train 0.5 | Epoch: 404 | Iter: 450000 | Total Loss: 0.002324 | Recon Loss: 0.001956 | Commit Loss: 0.000736 | Perplexity: 3227.289028
2025-10-13 23:05:46,455 Stage: Train 0.5 | Epoch: 404 | Iter: 450200 | Total Loss: 0.002292 | Recon Loss: 0.001928 | Commit Loss: 0.000728 | Perplexity: 3237.358906
Trainning Epoch:  87%|████████▋ | 432/494 [132:31:00<20:30:38, 1190.94s/it]Trainning Epoch:  87%|████████▋ | 432/494 [132:31:00<20:30:38, 1190.94s/it]2025-10-13 23:09:43,162 Stage: Train 0.5 | Epoch: 405 | Iter: 450400 | Total Loss: 0.002263 | Recon Loss: 0.001900 | Commit Loss: 0.000725 | Perplexity: 3238.539327
2025-10-13 23:13:37,011 Stage: Train 0.5 | Epoch: 405 | Iter: 450600 | Total Loss: 0.002288 | Recon Loss: 0.001923 | Commit Loss: 0.000730 | Perplexity: 3227.229691
2025-10-13 23:17:30,782 Stage: Train 0.5 | Epoch: 405 | Iter: 450800 | Total Loss: 0.002312 | Recon Loss: 0.001947 | Commit Loss: 0.000731 | Perplexity: 3230.853743
2025-10-13 23:21:24,700 Stage: Train 0.5 | Epoch: 405 | Iter: 451000 | Total Loss: 0.002314 | Recon Loss: 0.001937 | Commit Loss: 0.000755 | Perplexity: 3227.111887
2025-10-13 23:25:18,458 Stage: Train 0.5 | Epoch: 405 | Iter: 451200 | Total Loss: 0.002284 | Recon Loss: 0.001920 | Commit Loss: 0.000728 | Perplexity: 3220.818466
Trainning Epoch:  88%|████████▊ | 433/494 [132:50:47<20:09:31, 1189.70s/it]Trainning Epoch:  88%|████████▊ | 433/494 [132:50:47<20:09:31, 1189.70s/it]2025-10-13 23:29:15,096 Stage: Train 0.5 | Epoch: 406 | Iter: 451400 | Total Loss: 0.002237 | Recon Loss: 0.001873 | Commit Loss: 0.000727 | Perplexity: 3226.290187
2025-10-13 23:33:09,326 Stage: Train 0.5 | Epoch: 406 | Iter: 451600 | Total Loss: 0.002283 | Recon Loss: 0.001916 | Commit Loss: 0.000733 | Perplexity: 3231.241792
2025-10-13 23:37:03,733 Stage: Train 0.5 | Epoch: 406 | Iter: 451800 | Total Loss: 0.002259 | Recon Loss: 0.001892 | Commit Loss: 0.000735 | Perplexity: 3226.584260
2025-10-13 23:40:58,255 Stage: Train 0.5 | Epoch: 406 | Iter: 452000 | Total Loss: 0.002303 | Recon Loss: 0.001941 | Commit Loss: 0.000724 | Perplexity: 3227.757236
2025-10-13 23:44:52,465 Stage: Train 0.5 | Epoch: 406 | Iter: 452200 | Total Loss: 0.002275 | Recon Loss: 0.001909 | Commit Loss: 0.000732 | Perplexity: 3234.370781
Trainning Epoch:  88%|████████▊ | 434/494 [133:10:35<19:49:27, 1189.46s/it]Trainning Epoch:  88%|████████▊ | 434/494 [133:10:35<19:49:27, 1189.46s/it]2025-10-13 23:48:48,197 Stage: Train 0.5 | Epoch: 407 | Iter: 452400 | Total Loss: 0.002272 | Recon Loss: 0.001904 | Commit Loss: 0.000735 | Perplexity: 3231.709136
2025-10-13 23:52:41,092 Stage: Train 0.5 | Epoch: 407 | Iter: 452600 | Total Loss: 0.002272 | Recon Loss: 0.001906 | Commit Loss: 0.000731 | Perplexity: 3235.932242
2025-10-13 23:56:35,122 Stage: Train 0.5 | Epoch: 407 | Iter: 452800 | Total Loss: 0.002286 | Recon Loss: 0.001923 | Commit Loss: 0.000725 | Perplexity: 3224.953550
2025-10-14 00:00:29,292 Stage: Train 0.5 | Epoch: 407 | Iter: 453000 | Total Loss: 0.002359 | Recon Loss: 0.002005 | Commit Loss: 0.000709 | Perplexity: 3214.779044
2025-10-14 00:04:24,048 Stage: Train 0.5 | Epoch: 407 | Iter: 453200 | Total Loss: 0.002284 | Recon Loss: 0.001917 | Commit Loss: 0.000735 | Perplexity: 3225.150741
Trainning Epoch:  88%|████████▊ | 435/494 [133:30:23<19:29:03, 1188.87s/it]Trainning Epoch:  88%|████████▊ | 435/494 [133:30:23<19:29:03, 1188.87s/it]2025-10-14 00:08:22,100 Stage: Train 0.5 | Epoch: 408 | Iter: 453400 | Total Loss: 0.002293 | Recon Loss: 0.001928 | Commit Loss: 0.000731 | Perplexity: 3227.647397
2025-10-14 00:12:17,896 Stage: Train 0.5 | Epoch: 408 | Iter: 453600 | Total Loss: 0.002288 | Recon Loss: 0.001924 | Commit Loss: 0.000728 | Perplexity: 3230.881193
2025-10-14 00:16:13,906 Stage: Train 0.5 | Epoch: 408 | Iter: 453800 | Total Loss: 0.002265 | Recon Loss: 0.001899 | Commit Loss: 0.000731 | Perplexity: 3231.302858
2025-10-14 00:20:09,820 Stage: Train 0.5 | Epoch: 408 | Iter: 454000 | Total Loss: 0.002257 | Recon Loss: 0.001888 | Commit Loss: 0.000738 | Perplexity: 3227.878840
2025-10-14 00:24:06,190 Stage: Train 0.5 | Epoch: 408 | Iter: 454200 | Total Loss: 0.002300 | Recon Loss: 0.001928 | Commit Loss: 0.000743 | Perplexity: 3222.270549
Trainning Epoch:  88%|████████▊ | 436/494 [133:50:21<19:11:54, 1191.63s/it]Trainning Epoch:  88%|████████▊ | 436/494 [133:50:21<19:11:54, 1191.63s/it]2025-10-14 00:28:04,546 Stage: Train 0.5 | Epoch: 409 | Iter: 454400 | Total Loss: 0.002286 | Recon Loss: 0.001921 | Commit Loss: 0.000731 | Perplexity: 3229.188767
2025-10-14 00:31:59,343 Stage: Train 0.5 | Epoch: 409 | Iter: 454600 | Total Loss: 0.002229 | Recon Loss: 0.001864 | Commit Loss: 0.000730 | Perplexity: 3224.847322
2025-10-14 00:35:54,097 Stage: Train 0.5 | Epoch: 409 | Iter: 454800 | Total Loss: 0.002298 | Recon Loss: 0.001929 | Commit Loss: 0.000738 | Perplexity: 3222.348043
2025-10-14 00:39:49,031 Stage: Train 0.5 | Epoch: 409 | Iter: 455000 | Total Loss: 0.002309 | Recon Loss: 0.001938 | Commit Loss: 0.000741 | Perplexity: 3233.301200
2025-10-14 00:43:44,325 Stage: Train 0.5 | Epoch: 409 | Iter: 455200 | Total Loss: 0.002270 | Recon Loss: 0.001907 | Commit Loss: 0.000727 | Perplexity: 3229.353418
Trainning Epoch:  88%|████████▊ | 437/494 [134:10:14<18:52:24, 1192.00s/it]Trainning Epoch:  88%|████████▊ | 437/494 [134:10:14<18:52:24, 1192.00s/it]2025-10-14 00:47:41,961 Stage: Train 0.5 | Epoch: 410 | Iter: 455400 | Total Loss: 0.002290 | Recon Loss: 0.001927 | Commit Loss: 0.000725 | Perplexity: 3229.283640
2025-10-14 00:51:34,185 Stage: Train 0.5 | Epoch: 410 | Iter: 455600 | Total Loss: 0.002266 | Recon Loss: 0.001898 | Commit Loss: 0.000736 | Perplexity: 3226.289175
2025-10-14 00:55:27,012 Stage: Train 0.5 | Epoch: 410 | Iter: 455800 | Total Loss: 0.002267 | Recon Loss: 0.001902 | Commit Loss: 0.000731 | Perplexity: 3232.243558
2025-10-14 00:59:20,535 Stage: Train 0.5 | Epoch: 410 | Iter: 456000 | Total Loss: 0.002301 | Recon Loss: 0.001931 | Commit Loss: 0.000741 | Perplexity: 3217.256461
2025-10-14 01:03:14,192 Stage: Train 0.5 | Epoch: 410 | Iter: 456200 | Total Loss: 0.002272 | Recon Loss: 0.001906 | Commit Loss: 0.000731 | Perplexity: 3237.112318
Trainning Epoch:  89%|████████▊ | 438/494 [134:29:58<18:30:22, 1189.69s/it]Trainning Epoch:  89%|████████▊ | 438/494 [134:29:58<18:30:22, 1189.69s/it]2025-10-14 01:07:11,660 Stage: Train 0.5 | Epoch: 411 | Iter: 456400 | Total Loss: 0.002281 | Recon Loss: 0.001919 | Commit Loss: 0.000723 | Perplexity: 3231.453118
2025-10-14 01:11:06,313 Stage: Train 0.5 | Epoch: 411 | Iter: 456600 | Total Loss: 0.002267 | Recon Loss: 0.001902 | Commit Loss: 0.000731 | Perplexity: 3230.590837
2025-10-14 01:15:00,948 Stage: Train 0.5 | Epoch: 411 | Iter: 456800 | Total Loss: 0.002272 | Recon Loss: 0.001907 | Commit Loss: 0.000729 | Perplexity: 3221.786294
2025-10-14 01:18:55,060 Stage: Train 0.5 | Epoch: 411 | Iter: 457000 | Total Loss: 0.002264 | Recon Loss: 0.001894 | Commit Loss: 0.000740 | Perplexity: 3223.665836
2025-10-14 01:22:49,655 Stage: Train 0.5 | Epoch: 411 | Iter: 457200 | Total Loss: 0.002274 | Recon Loss: 0.001906 | Commit Loss: 0.000737 | Perplexity: 3221.349363
Trainning Epoch:  89%|████████▉ | 439/494 [134:49:49<18:10:52, 1190.05s/it]Trainning Epoch:  89%|████████▉ | 439/494 [134:49:49<18:10:52, 1190.05s/it]2025-10-14 01:26:47,469 Stage: Train 0.5 | Epoch: 412 | Iter: 457400 | Total Loss: 0.002274 | Recon Loss: 0.001912 | Commit Loss: 0.000724 | Perplexity: 3224.201046
2025-10-14 01:30:42,386 Stage: Train 0.5 | Epoch: 412 | Iter: 457600 | Total Loss: 0.002269 | Recon Loss: 0.001902 | Commit Loss: 0.000733 | Perplexity: 3238.030275
2025-10-14 01:34:37,515 Stage: Train 0.5 | Epoch: 412 | Iter: 457800 | Total Loss: 0.002265 | Recon Loss: 0.001901 | Commit Loss: 0.000727 | Perplexity: 3228.176740
2025-10-14 01:38:32,781 Stage: Train 0.5 | Epoch: 412 | Iter: 458000 | Total Loss: 0.002280 | Recon Loss: 0.001914 | Commit Loss: 0.000731 | Perplexity: 3221.450740
2025-10-14 01:42:27,917 Stage: Train 0.5 | Epoch: 412 | Iter: 458200 | Total Loss: 0.002250 | Recon Loss: 0.001883 | Commit Loss: 0.000734 | Perplexity: 3230.894965
Trainning Epoch:  89%|████████▉ | 440/494 [135:09:43<17:52:11, 1191.32s/it]Trainning Epoch:  89%|████████▉ | 440/494 [135:09:43<17:52:11, 1191.32s/it]2025-10-14 01:46:25,214 Stage: Train 0.5 | Epoch: 413 | Iter: 458400 | Total Loss: 0.002281 | Recon Loss: 0.001912 | Commit Loss: 0.000738 | Perplexity: 3227.191608
2025-10-14 01:50:18,971 Stage: Train 0.5 | Epoch: 413 | Iter: 458600 | Total Loss: 0.002286 | Recon Loss: 0.001920 | Commit Loss: 0.000732 | Perplexity: 3223.827518
2025-10-14 01:54:12,675 Stage: Train 0.5 | Epoch: 413 | Iter: 458800 | Total Loss: 0.002279 | Recon Loss: 0.001914 | Commit Loss: 0.000729 | Perplexity: 3230.323423
2025-10-14 01:58:06,460 Stage: Train 0.5 | Epoch: 413 | Iter: 459000 | Total Loss: 0.002255 | Recon Loss: 0.001890 | Commit Loss: 0.000730 | Perplexity: 3225.285427
2025-10-14 02:02:00,403 Stage: Train 0.5 | Epoch: 413 | Iter: 459200 | Total Loss: 0.002303 | Recon Loss: 0.001935 | Commit Loss: 0.000737 | Perplexity: 3232.904037
Trainning Epoch:  89%|████████▉ | 441/494 [135:29:30<17:31:07, 1189.95s/it]Trainning Epoch:  89%|████████▉ | 441/494 [135:29:30<17:31:07, 1189.95s/it]2025-10-14 02:05:56,895 Stage: Train 0.5 | Epoch: 414 | Iter: 459400 | Total Loss: 0.002267 | Recon Loss: 0.001902 | Commit Loss: 0.000730 | Perplexity: 3230.403436
2025-10-14 02:09:49,604 Stage: Train 0.5 | Epoch: 414 | Iter: 459600 | Total Loss: 0.002269 | Recon Loss: 0.001905 | Commit Loss: 0.000728 | Perplexity: 3228.082510
2025-10-14 02:13:42,767 Stage: Train 0.5 | Epoch: 414 | Iter: 459800 | Total Loss: 0.002295 | Recon Loss: 0.001929 | Commit Loss: 0.000731 | Perplexity: 3235.900334
2025-10-14 02:17:36,174 Stage: Train 0.5 | Epoch: 414 | Iter: 460000 | Total Loss: 0.002291 | Recon Loss: 0.001927 | Commit Loss: 0.000729 | Perplexity: 3231.636609
2025-10-14 02:17:36,174 Saving model at iteration 460000
2025-10-14 02:17:36,326 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000
2025-10-14 02:17:37,868 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000/model.safetensors
2025-10-14 02:17:39,652 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000/optimizer.bin
2025-10-14 02:17:39,653 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000/scheduler.bin
2025-10-14 02:17:39,653 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000/sampler.bin
2025-10-14 02:17:39,654 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_415_step_460000/random_states_0.pkl
2025-10-14 02:21:32,848 Stage: Train 0.5 | Epoch: 414 | Iter: 460200 | Total Loss: 0.002276 | Recon Loss: 0.001908 | Commit Loss: 0.000736 | Perplexity: 3226.910386
Trainning Epoch:  89%|████████▉ | 442/494 [135:49:18<17:10:39, 1189.22s/it]Trainning Epoch:  89%|████████▉ | 442/494 [135:49:18<17:10:39, 1189.23s/it]2025-10-14 02:25:29,825 Stage: Train 0.5 | Epoch: 415 | Iter: 460400 | Total Loss: 0.002261 | Recon Loss: 0.001894 | Commit Loss: 0.000735 | Perplexity: 3234.301877
2025-10-14 02:29:21,626 Stage: Train 0.5 | Epoch: 415 | Iter: 460600 | Total Loss: 0.002276 | Recon Loss: 0.001909 | Commit Loss: 0.000734 | Perplexity: 3237.352324
2025-10-14 02:33:14,475 Stage: Train 0.5 | Epoch: 415 | Iter: 460800 | Total Loss: 0.002257 | Recon Loss: 0.001891 | Commit Loss: 0.000733 | Perplexity: 3234.355013
2025-10-14 02:37:08,039 Stage: Train 0.5 | Epoch: 415 | Iter: 461000 | Total Loss: 0.002265 | Recon Loss: 0.001902 | Commit Loss: 0.000725 | Perplexity: 3219.996880
2025-10-14 02:41:01,908 Stage: Train 0.5 | Epoch: 415 | Iter: 461200 | Total Loss: 0.002290 | Recon Loss: 0.001920 | Commit Loss: 0.000740 | Perplexity: 3223.747722
2025-10-14 02:44:55,330 Stage: Train 0.5 | Epoch: 415 | Iter: 461400 | Total Loss: 0.002291 | Recon Loss: 0.001924 | Commit Loss: 0.000734 | Perplexity: 3226.488121
Trainning Epoch:  90%|████████▉ | 443/494 [136:09:02<16:49:31, 1187.68s/it]Trainning Epoch:  90%|████████▉ | 443/494 [136:09:02<16:49:31, 1187.68s/it]2025-10-14 02:48:50,703 Stage: Train 0.5 | Epoch: 416 | Iter: 461600 | Total Loss: 0.002265 | Recon Loss: 0.001904 | Commit Loss: 0.000723 | Perplexity: 3236.121528
2025-10-14 02:52:43,490 Stage: Train 0.5 | Epoch: 416 | Iter: 461800 | Total Loss: 0.002257 | Recon Loss: 0.001890 | Commit Loss: 0.000733 | Perplexity: 3225.592255
2025-10-14 02:56:36,494 Stage: Train 0.5 | Epoch: 416 | Iter: 462000 | Total Loss: 0.002242 | Recon Loss: 0.001876 | Commit Loss: 0.000731 | Perplexity: 3234.821947
2025-10-14 03:00:30,097 Stage: Train 0.5 | Epoch: 416 | Iter: 462200 | Total Loss: 0.002289 | Recon Loss: 0.001921 | Commit Loss: 0.000735 | Perplexity: 3237.001112
2025-10-14 03:04:24,032 Stage: Train 0.5 | Epoch: 416 | Iter: 462400 | Total Loss: 0.002283 | Recon Loss: 0.001919 | Commit Loss: 0.000728 | Perplexity: 3222.198724
Trainning Epoch:  90%|████████▉ | 444/494 [136:28:46<16:28:45, 1186.52s/it]Trainning Epoch:  90%|████████▉ | 444/494 [136:28:46<16:28:45, 1186.52s/it]2025-10-14 03:08:23,471 Stage: Train 0.5 | Epoch: 417 | Iter: 462600 | Total Loss: 0.002275 | Recon Loss: 0.001911 | Commit Loss: 0.000728 | Perplexity: 3228.001493
2025-10-14 03:12:19,726 Stage: Train 0.5 | Epoch: 417 | Iter: 462800 | Total Loss: 0.002276 | Recon Loss: 0.001910 | Commit Loss: 0.000732 | Perplexity: 3235.637854
2025-10-14 03:16:15,785 Stage: Train 0.5 | Epoch: 417 | Iter: 463000 | Total Loss: 0.002292 | Recon Loss: 0.001927 | Commit Loss: 0.000730 | Perplexity: 3228.087148
2025-10-14 03:20:11,686 Stage: Train 0.5 | Epoch: 417 | Iter: 463200 | Total Loss: 0.002273 | Recon Loss: 0.001906 | Commit Loss: 0.000735 | Perplexity: 3227.415092
2025-10-14 03:24:07,811 Stage: Train 0.5 | Epoch: 417 | Iter: 463400 | Total Loss: 0.002278 | Recon Loss: 0.001910 | Commit Loss: 0.000737 | Perplexity: 3227.048252
Trainning Epoch:  90%|█████████ | 445/494 [136:48:45<16:12:04, 1190.29s/it]Trainning Epoch:  90%|█████████ | 445/494 [136:48:45<16:12:04, 1190.30s/it]2025-10-14 03:28:06,256 Stage: Train 0.5 | Epoch: 418 | Iter: 463600 | Total Loss: 0.002246 | Recon Loss: 0.001881 | Commit Loss: 0.000730 | Perplexity: 3230.494285
2025-10-14 03:32:01,826 Stage: Train 0.5 | Epoch: 418 | Iter: 463800 | Total Loss: 0.002287 | Recon Loss: 0.001921 | Commit Loss: 0.000731 | Perplexity: 3221.917543
2025-10-14 03:35:57,356 Stage: Train 0.5 | Epoch: 418 | Iter: 464000 | Total Loss: 0.002285 | Recon Loss: 0.001916 | Commit Loss: 0.000738 | Perplexity: 3235.911448
2025-10-14 03:39:52,769 Stage: Train 0.5 | Epoch: 418 | Iter: 464200 | Total Loss: 0.002257 | Recon Loss: 0.001891 | Commit Loss: 0.000731 | Perplexity: 3232.281626
2025-10-14 03:43:48,809 Stage: Train 0.5 | Epoch: 418 | Iter: 464400 | Total Loss: 0.002267 | Recon Loss: 0.001902 | Commit Loss: 0.000730 | Perplexity: 3224.417642
Trainning Epoch:  90%|█████████ | 446/494 [137:08:41<15:53:40, 1192.10s/it]Trainning Epoch:  90%|█████████ | 446/494 [137:08:41<15:53:41, 1192.10s/it]2025-10-14 03:47:42,771 Stage: Train 0.5 | Epoch: 419 | Iter: 464600 | Total Loss: 0.002276 | Recon Loss: 0.001910 | Commit Loss: 0.000733 | Perplexity: 3238.894270
2025-10-14 03:51:33,813 Stage: Train 0.5 | Epoch: 419 | Iter: 464800 | Total Loss: 0.002271 | Recon Loss: 0.001907 | Commit Loss: 0.000728 | Perplexity: 3231.231667
2025-10-14 03:55:25,158 Stage: Train 0.5 | Epoch: 419 | Iter: 465000 | Total Loss: 0.002276 | Recon Loss: 0.001912 | Commit Loss: 0.000729 | Perplexity: 3234.268480
2025-10-14 03:59:16,903 Stage: Train 0.5 | Epoch: 419 | Iter: 465200 | Total Loss: 0.002282 | Recon Loss: 0.001916 | Commit Loss: 0.000732 | Perplexity: 3232.529510
2025-10-14 04:03:09,040 Stage: Train 0.5 | Epoch: 419 | Iter: 465400 | Total Loss: 0.002259 | Recon Loss: 0.001894 | Commit Loss: 0.000731 | Perplexity: 3223.743865
Trainning Epoch:  90%|█████████ | 447/494 [137:28:16<15:29:45, 1186.93s/it]Trainning Epoch:  90%|█████████ | 447/494 [137:28:16<15:29:45, 1186.93s/it]2025-10-14 04:07:07,561 Stage: Train 0.5 | Epoch: 420 | Iter: 465600 | Total Loss: 0.002267 | Recon Loss: 0.001903 | Commit Loss: 0.000729 | Perplexity: 3236.049451
2025-10-14 04:11:03,584 Stage: Train 0.5 | Epoch: 420 | Iter: 465800 | Total Loss: 0.002254 | Recon Loss: 0.001887 | Commit Loss: 0.000735 | Perplexity: 3236.972295
2025-10-14 04:14:59,520 Stage: Train 0.5 | Epoch: 420 | Iter: 466000 | Total Loss: 0.002267 | Recon Loss: 0.001901 | Commit Loss: 0.000731 | Perplexity: 3229.150302
2025-10-14 04:18:55,797 Stage: Train 0.5 | Epoch: 420 | Iter: 466200 | Total Loss: 0.002266 | Recon Loss: 0.001901 | Commit Loss: 0.000730 | Perplexity: 3227.713285
2025-10-14 04:22:51,923 Stage: Train 0.5 | Epoch: 420 | Iter: 466400 | Total Loss: 0.002241 | Recon Loss: 0.001871 | Commit Loss: 0.000741 | Perplexity: 3227.024283
Trainning Epoch:  91%|█████████ | 448/494 [137:48:15<15:12:46, 1190.58s/it]Trainning Epoch:  91%|█████████ | 448/494 [137:48:15<15:12:46, 1190.58s/it]2025-10-14 04:26:50,232 Stage: Train 0.5 | Epoch: 421 | Iter: 466600 | Total Loss: 0.002291 | Recon Loss: 0.001929 | Commit Loss: 0.000725 | Perplexity: 3233.041292
2025-10-14 04:30:43,807 Stage: Train 0.5 | Epoch: 421 | Iter: 466800 | Total Loss: 0.002266 | Recon Loss: 0.001902 | Commit Loss: 0.000727 | Perplexity: 3238.435839
2025-10-14 04:34:37,548 Stage: Train 0.5 | Epoch: 421 | Iter: 467000 | Total Loss: 0.002255 | Recon Loss: 0.001891 | Commit Loss: 0.000729 | Perplexity: 3239.155739
2025-10-14 04:38:31,807 Stage: Train 0.5 | Epoch: 421 | Iter: 467200 | Total Loss: 0.002297 | Recon Loss: 0.001929 | Commit Loss: 0.000735 | Perplexity: 3226.836484
2025-10-14 04:42:26,254 Stage: Train 0.5 | Epoch: 421 | Iter: 467400 | Total Loss: 0.002248 | Recon Loss: 0.001879 | Commit Loss: 0.000739 | Perplexity: 3235.260757
Trainning Epoch:  91%|█████████ | 449/494 [138:08:04<14:52:31, 1190.03s/it]Trainning Epoch:  91%|█████████ | 449/494 [138:08:04<14:52:31, 1190.03s/it]2025-10-14 04:46:23,880 Stage: Train 0.5 | Epoch: 422 | Iter: 467600 | Total Loss: 0.002275 | Recon Loss: 0.001908 | Commit Loss: 0.000735 | Perplexity: 3223.449432
2025-10-14 04:50:18,983 Stage: Train 0.5 | Epoch: 422 | Iter: 467800 | Total Loss: 0.002252 | Recon Loss: 0.001888 | Commit Loss: 0.000728 | Perplexity: 3238.243093
2025-10-14 04:54:13,918 Stage: Train 0.5 | Epoch: 422 | Iter: 468000 | Total Loss: 0.002246 | Recon Loss: 0.001880 | Commit Loss: 0.000731 | Perplexity: 3224.006920
2025-10-14 04:58:08,817 Stage: Train 0.5 | Epoch: 422 | Iter: 468200 | Total Loss: 0.002287 | Recon Loss: 0.001921 | Commit Loss: 0.000731 | Perplexity: 3226.581230
2025-10-14 05:02:03,522 Stage: Train 0.5 | Epoch: 422 | Iter: 468400 | Total Loss: 0.002281 | Recon Loss: 0.001914 | Commit Loss: 0.000735 | Perplexity: 3237.371060
Trainning Epoch:  91%|█████████ | 450/494 [138:27:57<14:33:22, 1190.98s/it]Trainning Epoch:  91%|█████████ | 450/494 [138:27:57<14:33:23, 1190.98s/it]2025-10-14 05:06:00,323 Stage: Train 0.5 | Epoch: 423 | Iter: 468600 | Total Loss: 0.002238 | Recon Loss: 0.001871 | Commit Loss: 0.000733 | Perplexity: 3230.068330
2025-10-14 05:09:53,433 Stage: Train 0.5 | Epoch: 423 | Iter: 468800 | Total Loss: 0.002245 | Recon Loss: 0.001879 | Commit Loss: 0.000732 | Perplexity: 3233.061946
2025-10-14 05:13:46,724 Stage: Train 0.5 | Epoch: 423 | Iter: 469000 | Total Loss: 0.002272 | Recon Loss: 0.001907 | Commit Loss: 0.000729 | Perplexity: 3222.606326
2025-10-14 05:17:40,585 Stage: Train 0.5 | Epoch: 423 | Iter: 469200 | Total Loss: 0.002283 | Recon Loss: 0.001921 | Commit Loss: 0.000724 | Perplexity: 3232.231877
2025-10-14 05:21:34,543 Stage: Train 0.5 | Epoch: 423 | Iter: 469400 | Total Loss: 0.002265 | Recon Loss: 0.001895 | Commit Loss: 0.000741 | Perplexity: 3225.130867
Trainning Epoch:  91%|█████████▏| 451/494 [138:47:42<14:12:20, 1189.31s/it]Trainning Epoch:  91%|█████████▏| 451/494 [138:47:42<14:12:20, 1189.31s/it]2025-10-14 05:25:30,379 Stage: Train 0.5 | Epoch: 424 | Iter: 469600 | Total Loss: 0.002257 | Recon Loss: 0.001889 | Commit Loss: 0.000735 | Perplexity: 3234.181250
2025-10-14 05:29:23,829 Stage: Train 0.5 | Epoch: 424 | Iter: 469800 | Total Loss: 0.002263 | Recon Loss: 0.001898 | Commit Loss: 0.000731 | Perplexity: 3238.119454
2025-10-14 05:33:18,159 Stage: Train 0.5 | Epoch: 424 | Iter: 470000 | Total Loss: 0.002230 | Recon Loss: 0.001865 | Commit Loss: 0.000729 | Perplexity: 3228.656825
2025-10-14 05:37:12,240 Stage: Train 0.5 | Epoch: 424 | Iter: 470200 | Total Loss: 0.002270 | Recon Loss: 0.001904 | Commit Loss: 0.000731 | Perplexity: 3246.349703
2025-10-14 05:41:06,196 Stage: Train 0.5 | Epoch: 424 | Iter: 470400 | Total Loss: 0.002292 | Recon Loss: 0.001926 | Commit Loss: 0.000732 | Perplexity: 3233.214834
Trainning Epoch:  91%|█████████▏| 452/494 [139:07:29<13:52:02, 1188.62s/it]Trainning Epoch:  91%|█████████▏| 452/494 [139:07:29<13:52:02, 1188.63s/it]2025-10-14 05:45:02,886 Stage: Train 0.5 | Epoch: 425 | Iter: 470600 | Total Loss: 0.002264 | Recon Loss: 0.001898 | Commit Loss: 0.000731 | Perplexity: 3225.582244
2025-10-14 05:48:57,542 Stage: Train 0.5 | Epoch: 425 | Iter: 470800 | Total Loss: 0.002247 | Recon Loss: 0.001881 | Commit Loss: 0.000731 | Perplexity: 3230.911262
2025-10-14 05:52:52,303 Stage: Train 0.5 | Epoch: 425 | Iter: 471000 | Total Loss: 0.002267 | Recon Loss: 0.001901 | Commit Loss: 0.000732 | Perplexity: 3229.317542
2025-10-14 05:56:46,935 Stage: Train 0.5 | Epoch: 425 | Iter: 471200 | Total Loss: 0.002261 | Recon Loss: 0.001894 | Commit Loss: 0.000734 | Perplexity: 3235.380425
2025-10-14 06:00:42,486 Stage: Train 0.5 | Epoch: 425 | Iter: 471400 | Total Loss: 0.002246 | Recon Loss: 0.001875 | Commit Loss: 0.000742 | Perplexity: 3232.772775
Trainning Epoch:  92%|█████████▏| 453/494 [139:27:21<13:32:56, 1189.67s/it]Trainning Epoch:  92%|█████████▏| 453/494 [139:27:21<13:32:56, 1189.67s/it]2025-10-14 06:04:38,687 Stage: Train 0.5 | Epoch: 426 | Iter: 471600 | Total Loss: 0.002276 | Recon Loss: 0.001910 | Commit Loss: 0.000732 | Perplexity: 3224.640820
2025-10-14 06:08:29,656 Stage: Train 0.5 | Epoch: 426 | Iter: 471800 | Total Loss: 0.002259 | Recon Loss: 0.001894 | Commit Loss: 0.000730 | Perplexity: 3236.604845
2025-10-14 06:12:21,192 Stage: Train 0.5 | Epoch: 426 | Iter: 472000 | Total Loss: 0.002261 | Recon Loss: 0.001897 | Commit Loss: 0.000728 | Perplexity: 3231.967375
2025-10-14 06:16:12,687 Stage: Train 0.5 | Epoch: 426 | Iter: 472200 | Total Loss: 0.002268 | Recon Loss: 0.001904 | Commit Loss: 0.000729 | Perplexity: 3222.467418
2025-10-14 06:20:04,205 Stage: Train 0.5 | Epoch: 426 | Iter: 472400 | Total Loss: 0.002253 | Recon Loss: 0.001886 | Commit Loss: 0.000735 | Perplexity: 3227.148588
Trainning Epoch:  92%|█████████▏| 454/494 [139:46:56<13:10:06, 1185.16s/it]Trainning Epoch:  92%|█████████▏| 454/494 [139:46:56<13:10:06, 1185.16s/it]2025-10-14 06:23:59,842 Stage: Train 0.5 | Epoch: 427 | Iter: 472600 | Total Loss: 0.002273 | Recon Loss: 0.001906 | Commit Loss: 0.000734 | Perplexity: 3228.251860
2025-10-14 06:27:53,055 Stage: Train 0.5 | Epoch: 427 | Iter: 472800 | Total Loss: 0.002238 | Recon Loss: 0.001875 | Commit Loss: 0.000726 | Perplexity: 3228.889949
2025-10-14 06:31:46,440 Stage: Train 0.5 | Epoch: 427 | Iter: 473000 | Total Loss: 0.002262 | Recon Loss: 0.001890 | Commit Loss: 0.000744 | Perplexity: 3241.124509
2025-10-14 06:35:39,675 Stage: Train 0.5 | Epoch: 427 | Iter: 473200 | Total Loss: 0.002283 | Recon Loss: 0.001920 | Commit Loss: 0.000726 | Perplexity: 3237.804290
2025-10-14 06:39:32,963 Stage: Train 0.5 | Epoch: 427 | Iter: 473400 | Total Loss: 0.002250 | Recon Loss: 0.001878 | Commit Loss: 0.000744 | Perplexity: 3237.624050
Trainning Epoch:  92%|█████████▏| 455/494 [140:06:41<12:50:18, 1185.08s/it]Trainning Epoch:  92%|█████████▏| 455/494 [140:06:41<12:50:18, 1185.08s/it]2025-10-14 06:43:29,008 Stage: Train 0.5 | Epoch: 428 | Iter: 473600 | Total Loss: 0.002290 | Recon Loss: 0.001926 | Commit Loss: 0.000728 | Perplexity: 3221.512727
2025-10-14 06:47:21,540 Stage: Train 0.5 | Epoch: 428 | Iter: 473800 | Total Loss: 0.002251 | Recon Loss: 0.001890 | Commit Loss: 0.000723 | Perplexity: 3225.316605
2025-10-14 06:51:15,143 Stage: Train 0.5 | Epoch: 428 | Iter: 474000 | Total Loss: 0.002233 | Recon Loss: 0.001868 | Commit Loss: 0.000729 | Perplexity: 3231.129695
2025-10-14 06:55:08,813 Stage: Train 0.5 | Epoch: 428 | Iter: 474200 | Total Loss: 0.002255 | Recon Loss: 0.001889 | Commit Loss: 0.000732 | Perplexity: 3236.956849
2025-10-14 06:59:02,885 Stage: Train 0.5 | Epoch: 428 | Iter: 474400 | Total Loss: 0.002285 | Recon Loss: 0.001919 | Commit Loss: 0.000732 | Perplexity: 3228.630837
Trainning Epoch:  92%|█████████▏| 456/494 [140:26:27<12:30:42, 1185.33s/it]Trainning Epoch:  92%|█████████▏| 456/494 [140:26:27<12:30:42, 1185.33s/it]2025-10-14 07:03:00,347 Stage: Train 0.5 | Epoch: 429 | Iter: 474600 | Total Loss: 0.002249 | Recon Loss: 0.001880 | Commit Loss: 0.000739 | Perplexity: 3234.732251
2025-10-14 07:06:54,143 Stage: Train 0.5 | Epoch: 429 | Iter: 474800 | Total Loss: 0.002255 | Recon Loss: 0.001893 | Commit Loss: 0.000726 | Perplexity: 3234.822394
2025-10-14 07:10:48,425 Stage: Train 0.5 | Epoch: 429 | Iter: 475000 | Total Loss: 0.002272 | Recon Loss: 0.001902 | Commit Loss: 0.000739 | Perplexity: 3230.768088
2025-10-14 07:14:42,920 Stage: Train 0.5 | Epoch: 429 | Iter: 475200 | Total Loss: 0.002251 | Recon Loss: 0.001883 | Commit Loss: 0.000736 | Perplexity: 3236.232031
2025-10-14 07:18:36,897 Stage: Train 0.5 | Epoch: 429 | Iter: 475400 | Total Loss: 0.002283 | Recon Loss: 0.001910 | Commit Loss: 0.000744 | Perplexity: 3236.625974
Trainning Epoch:  93%|█████████▎| 457/494 [140:46:16<12:11:42, 1186.54s/it]Trainning Epoch:  93%|█████████▎| 457/494 [140:46:16<12:11:42, 1186.54s/it]2025-10-14 07:22:33,966 Stage: Train 0.5 | Epoch: 430 | Iter: 475600 | Total Loss: 0.002244 | Recon Loss: 0.001877 | Commit Loss: 0.000733 | Perplexity: 3233.419498
2025-10-14 07:26:28,195 Stage: Train 0.5 | Epoch: 430 | Iter: 475800 | Total Loss: 0.002272 | Recon Loss: 0.001906 | Commit Loss: 0.000732 | Perplexity: 3225.803877
2025-10-14 07:30:22,731 Stage: Train 0.5 | Epoch: 430 | Iter: 476000 | Total Loss: 0.002275 | Recon Loss: 0.001909 | Commit Loss: 0.000732 | Perplexity: 3239.999421
2025-10-14 07:34:17,108 Stage: Train 0.5 | Epoch: 430 | Iter: 476200 | Total Loss: 0.002269 | Recon Loss: 0.001902 | Commit Loss: 0.000735 | Perplexity: 3237.208185
2025-10-14 07:38:12,030 Stage: Train 0.5 | Epoch: 430 | Iter: 476400 | Total Loss: 0.002268 | Recon Loss: 0.001898 | Commit Loss: 0.000741 | Perplexity: 3242.015145
2025-10-14 07:42:06,542 Stage: Train 0.5 | Epoch: 430 | Iter: 476600 | Total Loss: 0.002241 | Recon Loss: 0.001875 | Commit Loss: 0.000733 | Perplexity: 3222.819070
Trainning Epoch:  93%|█████████▎| 458/494 [141:06:07<11:52:42, 1187.86s/it]Trainning Epoch:  93%|█████████▎| 458/494 [141:06:07<11:52:42, 1187.85s/it]2025-10-14 07:46:05,802 Stage: Train 0.5 | Epoch: 431 | Iter: 476800 | Total Loss: 0.002272 | Recon Loss: 0.001909 | Commit Loss: 0.000726 | Perplexity: 3226.051471
2025-10-14 07:50:02,104 Stage: Train 0.5 | Epoch: 431 | Iter: 477000 | Total Loss: 0.002253 | Recon Loss: 0.001887 | Commit Loss: 0.000732 | Perplexity: 3231.316489
2025-10-14 07:53:58,152 Stage: Train 0.5 | Epoch: 431 | Iter: 477200 | Total Loss: 0.002244 | Recon Loss: 0.001880 | Commit Loss: 0.000729 | Perplexity: 3232.014872
2025-10-14 07:57:54,083 Stage: Train 0.5 | Epoch: 431 | Iter: 477400 | Total Loss: 0.002235 | Recon Loss: 0.001865 | Commit Loss: 0.000739 | Perplexity: 3233.745800
2025-10-14 08:01:50,234 Stage: Train 0.5 | Epoch: 431 | Iter: 477600 | Total Loss: 0.002235 | Recon Loss: 0.001868 | Commit Loss: 0.000733 | Perplexity: 3227.383378
Trainning Epoch:  93%|█████████▎| 459/494 [141:26:06<11:34:48, 1191.09s/it]Trainning Epoch:  93%|█████████▎| 459/494 [141:26:06<11:34:48, 1191.10s/it]2025-10-14 08:05:46,399 Stage: Train 0.5 | Epoch: 432 | Iter: 477800 | Total Loss: 0.002260 | Recon Loss: 0.001894 | Commit Loss: 0.000732 | Perplexity: 3244.497721
2025-10-14 08:09:40,491 Stage: Train 0.5 | Epoch: 432 | Iter: 478000 | Total Loss: 0.002250 | Recon Loss: 0.001883 | Commit Loss: 0.000735 | Perplexity: 3240.720361
2025-10-14 08:13:34,700 Stage: Train 0.5 | Epoch: 432 | Iter: 478200 | Total Loss: 0.002259 | Recon Loss: 0.001889 | Commit Loss: 0.000739 | Perplexity: 3236.495723
2025-10-14 08:17:29,308 Stage: Train 0.5 | Epoch: 432 | Iter: 478400 | Total Loss: 0.002268 | Recon Loss: 0.001901 | Commit Loss: 0.000733 | Perplexity: 3234.765494
2025-10-14 08:21:23,882 Stage: Train 0.5 | Epoch: 432 | Iter: 478600 | Total Loss: 0.002247 | Recon Loss: 0.001884 | Commit Loss: 0.000727 | Perplexity: 3233.753126
Trainning Epoch:  93%|█████████▎| 460/494 [141:45:55<11:14:34, 1190.43s/it]Trainning Epoch:  93%|█████████▎| 460/494 [141:45:55<11:14:34, 1190.43s/it]2025-10-14 08:25:19,966 Stage: Train 0.5 | Epoch: 433 | Iter: 478800 | Total Loss: 0.002243 | Recon Loss: 0.001873 | Commit Loss: 0.000741 | Perplexity: 3237.265814
2025-10-14 08:29:13,314 Stage: Train 0.5 | Epoch: 433 | Iter: 479000 | Total Loss: 0.002252 | Recon Loss: 0.001888 | Commit Loss: 0.000727 | Perplexity: 3232.712329
2025-10-14 08:33:06,030 Stage: Train 0.5 | Epoch: 433 | Iter: 479200 | Total Loss: 0.002260 | Recon Loss: 0.001890 | Commit Loss: 0.000741 | Perplexity: 3240.001367
2025-10-14 08:36:59,518 Stage: Train 0.5 | Epoch: 433 | Iter: 479400 | Total Loss: 0.002260 | Recon Loss: 0.001894 | Commit Loss: 0.000733 | Perplexity: 3236.669023
2025-10-14 08:40:53,880 Stage: Train 0.5 | Epoch: 433 | Iter: 479600 | Total Loss: 0.002230 | Recon Loss: 0.001862 | Commit Loss: 0.000735 | Perplexity: 3228.137368
Trainning Epoch:  93%|█████████▎| 461/494 [142:05:40<10:53:51, 1188.82s/it]Trainning Epoch:  93%|█████████▎| 461/494 [142:05:40<10:53:51, 1188.82s/it]2025-10-14 08:44:52,321 Stage: Train 0.5 | Epoch: 434 | Iter: 479800 | Total Loss: 0.002236 | Recon Loss: 0.001868 | Commit Loss: 0.000735 | Perplexity: 3223.680623
2025-10-14 08:48:47,241 Stage: Train 0.5 | Epoch: 434 | Iter: 480000 | Total Loss: 0.002275 | Recon Loss: 0.001910 | Commit Loss: 0.000731 | Perplexity: 3233.259454
2025-10-14 08:48:47,241 Saving model at iteration 480000
2025-10-14 08:48:47,437 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000
2025-10-14 08:48:48,996 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000/model.safetensors
2025-10-14 08:48:50,699 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000/optimizer.bin
2025-10-14 08:48:50,700 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000/scheduler.bin
2025-10-14 08:48:50,700 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000/sampler.bin
2025-10-14 08:48:50,701 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_435_step_480000/random_states_0.pkl
2025-10-14 08:52:45,777 Stage: Train 0.5 | Epoch: 434 | Iter: 480200 | Total Loss: 0.002251 | Recon Loss: 0.001885 | Commit Loss: 0.000731 | Perplexity: 3234.255507
2025-10-14 08:56:41,037 Stage: Train 0.5 | Epoch: 434 | Iter: 480400 | Total Loss: 0.002261 | Recon Loss: 0.001891 | Commit Loss: 0.000739 | Perplexity: 3241.656891
2025-10-14 09:00:36,485 Stage: Train 0.5 | Epoch: 434 | Iter: 480600 | Total Loss: 0.002252 | Recon Loss: 0.001887 | Commit Loss: 0.000730 | Perplexity: 3222.094049
Trainning Epoch:  94%|█████████▎| 462/494 [142:25:38<10:35:32, 1191.64s/it]Trainning Epoch:  94%|█████████▎| 462/494 [142:25:38<10:35:32, 1191.64s/it]2025-10-14 09:04:35,308 Stage: Train 0.5 | Epoch: 435 | Iter: 480800 | Total Loss: 0.002264 | Recon Loss: 0.001900 | Commit Loss: 0.000727 | Perplexity: 3226.506949
2025-10-14 09:08:30,291 Stage: Train 0.5 | Epoch: 435 | Iter: 481000 | Total Loss: 0.002251 | Recon Loss: 0.001885 | Commit Loss: 0.000733 | Perplexity: 3228.642156
2025-10-14 09:12:26,072 Stage: Train 0.5 | Epoch: 435 | Iter: 481200 | Total Loss: 0.002273 | Recon Loss: 0.001903 | Commit Loss: 0.000740 | Perplexity: 3239.165966
2025-10-14 09:16:21,815 Stage: Train 0.5 | Epoch: 435 | Iter: 481400 | Total Loss: 0.002248 | Recon Loss: 0.001879 | Commit Loss: 0.000739 | Perplexity: 3234.734257
2025-10-14 09:20:17,520 Stage: Train 0.5 | Epoch: 435 | Iter: 481600 | Total Loss: 0.002259 | Recon Loss: 0.001887 | Commit Loss: 0.000744 | Perplexity: 3239.566907
Trainning Epoch:  94%|█████████▎| 463/494 [142:45:34<10:16:26, 1193.11s/it]Trainning Epoch:  94%|█████████▎| 463/494 [142:45:34<10:16:26, 1193.11s/it]2025-10-14 09:24:15,551 Stage: Train 0.5 | Epoch: 436 | Iter: 481800 | Total Loss: 0.002237 | Recon Loss: 0.001871 | Commit Loss: 0.000732 | Perplexity: 3239.391187
2025-10-14 09:28:10,257 Stage: Train 0.5 | Epoch: 436 | Iter: 482000 | Total Loss: 0.002242 | Recon Loss: 0.001878 | Commit Loss: 0.000728 | Perplexity: 3230.387539
2025-10-14 09:32:05,267 Stage: Train 0.5 | Epoch: 436 | Iter: 482200 | Total Loss: 0.002247 | Recon Loss: 0.001881 | Commit Loss: 0.000732 | Perplexity: 3230.202371
2025-10-14 09:36:00,441 Stage: Train 0.5 | Epoch: 436 | Iter: 482400 | Total Loss: 0.002251 | Recon Loss: 0.001885 | Commit Loss: 0.000733 | Perplexity: 3231.822423
2025-10-14 09:39:55,129 Stage: Train 0.5 | Epoch: 436 | Iter: 482600 | Total Loss: 0.002249 | Recon Loss: 0.001879 | Commit Loss: 0.000741 | Perplexity: 3238.277511
Trainning Epoch:  94%|█████████▍| 464/494 [143:05:27<9:56:25, 1192.87s/it] Trainning Epoch:  94%|█████████▍| 464/494 [143:05:27<9:56:26, 1192.87s/it] 2025-10-14 09:43:50,903 Stage: Train 0.5 | Epoch: 437 | Iter: 482800 | Total Loss: 0.002250 | Recon Loss: 0.001888 | Commit Loss: 0.000722 | Perplexity: 3223.091635
2025-10-14 09:47:43,414 Stage: Train 0.5 | Epoch: 437 | Iter: 483000 | Total Loss: 0.002213 | Recon Loss: 0.001848 | Commit Loss: 0.000730 | Perplexity: 3226.272828
2025-10-14 09:51:36,304 Stage: Train 0.5 | Epoch: 437 | Iter: 483200 | Total Loss: 0.002251 | Recon Loss: 0.001881 | Commit Loss: 0.000740 | Perplexity: 3236.284996
2025-10-14 09:55:29,335 Stage: Train 0.5 | Epoch: 437 | Iter: 483400 | Total Loss: 0.002269 | Recon Loss: 0.001900 | Commit Loss: 0.000738 | Perplexity: 3237.713727
2025-10-14 09:59:22,488 Stage: Train 0.5 | Epoch: 437 | Iter: 483600 | Total Loss: 0.002252 | Recon Loss: 0.001885 | Commit Loss: 0.000733 | Perplexity: 3230.552385
Trainning Epoch:  94%|█████████▍| 465/494 [143:25:09<9:34:59, 1189.62s/it]Trainning Epoch:  94%|█████████▍| 465/494 [143:25:09<9:34:59, 1189.62s/it]2025-10-14 10:03:18,870 Stage: Train 0.5 | Epoch: 438 | Iter: 483800 | Total Loss: 0.002260 | Recon Loss: 0.001892 | Commit Loss: 0.000736 | Perplexity: 3244.533385
2025-10-14 10:07:13,289 Stage: Train 0.5 | Epoch: 438 | Iter: 484000 | Total Loss: 0.002250 | Recon Loss: 0.001883 | Commit Loss: 0.000734 | Perplexity: 3232.523687
2025-10-14 10:11:08,135 Stage: Train 0.5 | Epoch: 438 | Iter: 484200 | Total Loss: 0.002236 | Recon Loss: 0.001871 | Commit Loss: 0.000731 | Perplexity: 3239.764229
2025-10-14 10:15:02,902 Stage: Train 0.5 | Epoch: 438 | Iter: 484400 | Total Loss: 0.002235 | Recon Loss: 0.001868 | Commit Loss: 0.000732 | Perplexity: 3234.058118
2025-10-14 10:18:57,610 Stage: Train 0.5 | Epoch: 438 | Iter: 484600 | Total Loss: 0.002257 | Recon Loss: 0.001886 | Commit Loss: 0.000742 | Perplexity: 3234.674580
Trainning Epoch:  94%|█████████▍| 466/494 [143:45:00<9:15:23, 1190.13s/it]Trainning Epoch:  94%|█████████▍| 466/494 [143:45:00<9:15:23, 1190.13s/it]2025-10-14 10:22:54,400 Stage: Train 0.5 | Epoch: 439 | Iter: 484800 | Total Loss: 0.002243 | Recon Loss: 0.001875 | Commit Loss: 0.000737 | Perplexity: 3235.395137
2025-10-14 10:26:48,031 Stage: Train 0.5 | Epoch: 439 | Iter: 485000 | Total Loss: 0.002258 | Recon Loss: 0.001892 | Commit Loss: 0.000733 | Perplexity: 3249.120708
2025-10-14 10:30:41,573 Stage: Train 0.5 | Epoch: 439 | Iter: 485200 | Total Loss: 0.002233 | Recon Loss: 0.001871 | Commit Loss: 0.000724 | Perplexity: 3226.706104
2025-10-14 10:34:35,882 Stage: Train 0.5 | Epoch: 439 | Iter: 485400 | Total Loss: 0.002240 | Recon Loss: 0.001872 | Commit Loss: 0.000737 | Perplexity: 3240.371793
2025-10-14 10:38:30,137 Stage: Train 0.5 | Epoch: 439 | Iter: 485600 | Total Loss: 0.002232 | Recon Loss: 0.001863 | Commit Loss: 0.000739 | Perplexity: 3231.314766
Trainning Epoch:  95%|█████████▍| 467/494 [144:04:47<8:55:06, 1189.14s/it]Trainning Epoch:  95%|█████████▍| 467/494 [144:04:47<8:55:06, 1189.14s/it]2025-10-14 10:42:27,595 Stage: Train 0.5 | Epoch: 440 | Iter: 485800 | Total Loss: 0.002231 | Recon Loss: 0.001869 | Commit Loss: 0.000725 | Perplexity: 3231.788049
2025-10-14 10:46:22,698 Stage: Train 0.5 | Epoch: 440 | Iter: 486000 | Total Loss: 0.002223 | Recon Loss: 0.001857 | Commit Loss: 0.000732 | Perplexity: 3232.148728
2025-10-14 10:50:18,299 Stage: Train 0.5 | Epoch: 440 | Iter: 486200 | Total Loss: 0.002273 | Recon Loss: 0.001906 | Commit Loss: 0.000736 | Perplexity: 3234.050094
2025-10-14 10:54:14,236 Stage: Train 0.5 | Epoch: 440 | Iter: 486400 | Total Loss: 0.002243 | Recon Loss: 0.001876 | Commit Loss: 0.000733 | Perplexity: 3231.120809
2025-10-14 10:58:09,624 Stage: Train 0.5 | Epoch: 440 | Iter: 486600 | Total Loss: 0.002225 | Recon Loss: 0.001853 | Commit Loss: 0.000745 | Perplexity: 3233.865120
Trainning Epoch:  95%|█████████▍| 468/494 [144:24:43<8:36:12, 1191.25s/it]Trainning Epoch:  95%|█████████▍| 468/494 [144:24:43<8:36:12, 1191.25s/it]2025-10-14 11:02:07,938 Stage: Train 0.5 | Epoch: 441 | Iter: 486800 | Total Loss: 0.002227 | Recon Loss: 0.001856 | Commit Loss: 0.000743 | Perplexity: 3243.413267
2025-10-14 11:06:02,442 Stage: Train 0.5 | Epoch: 441 | Iter: 487000 | Total Loss: 0.002223 | Recon Loss: 0.001854 | Commit Loss: 0.000738 | Perplexity: 3228.887799
2025-10-14 11:09:56,770 Stage: Train 0.5 | Epoch: 441 | Iter: 487200 | Total Loss: 0.002257 | Recon Loss: 0.001891 | Commit Loss: 0.000732 | Perplexity: 3239.478800
2025-10-14 11:13:51,168 Stage: Train 0.5 | Epoch: 441 | Iter: 487400 | Total Loss: 0.002233 | Recon Loss: 0.001865 | Commit Loss: 0.000737 | Perplexity: 3234.986964
2025-10-14 11:17:45,522 Stage: Train 0.5 | Epoch: 441 | Iter: 487600 | Total Loss: 0.002286 | Recon Loss: 0.001919 | Commit Loss: 0.000734 | Perplexity: 3225.336729
Trainning Epoch:  95%|█████████▍| 469/494 [144:44:34<8:16:17, 1191.08s/it]Trainning Epoch:  95%|█████████▍| 469/494 [144:44:34<8:16:17, 1191.09s/it]2025-10-14 11:21:42,901 Stage: Train 0.5 | Epoch: 442 | Iter: 487800 | Total Loss: 0.002257 | Recon Loss: 0.001887 | Commit Loss: 0.000740 | Perplexity: 3234.961865
2025-10-14 11:25:37,341 Stage: Train 0.5 | Epoch: 442 | Iter: 488000 | Total Loss: 0.002233 | Recon Loss: 0.001866 | Commit Loss: 0.000733 | Perplexity: 3238.631776
2025-10-14 11:29:31,930 Stage: Train 0.5 | Epoch: 442 | Iter: 488200 | Total Loss: 0.002213 | Recon Loss: 0.001849 | Commit Loss: 0.000729 | Perplexity: 3227.128036
2025-10-14 11:33:26,050 Stage: Train 0.5 | Epoch: 442 | Iter: 488400 | Total Loss: 0.002248 | Recon Loss: 0.001884 | Commit Loss: 0.000728 | Perplexity: 3235.954150
2025-10-14 11:37:20,437 Stage: Train 0.5 | Epoch: 442 | Iter: 488600 | Total Loss: 0.002257 | Recon Loss: 0.001885 | Commit Loss: 0.000743 | Perplexity: 3231.416759
Trainning Epoch:  95%|█████████▌| 470/494 [145:04:24<7:56:17, 1190.71s/it]Trainning Epoch:  95%|█████████▌| 470/494 [145:04:24<7:56:17, 1190.71s/it]2025-10-14 11:41:17,708 Stage: Train 0.5 | Epoch: 443 | Iter: 488800 | Total Loss: 0.002253 | Recon Loss: 0.001884 | Commit Loss: 0.000737 | Perplexity: 3236.917299
2025-10-14 11:45:12,148 Stage: Train 0.5 | Epoch: 443 | Iter: 489000 | Total Loss: 0.002237 | Recon Loss: 0.001870 | Commit Loss: 0.000734 | Perplexity: 3234.182310
2025-10-14 11:49:06,866 Stage: Train 0.5 | Epoch: 443 | Iter: 489200 | Total Loss: 0.002249 | Recon Loss: 0.001886 | Commit Loss: 0.000726 | Perplexity: 3234.151724
2025-10-14 11:53:01,681 Stage: Train 0.5 | Epoch: 443 | Iter: 489400 | Total Loss: 0.002233 | Recon Loss: 0.001863 | Commit Loss: 0.000740 | Perplexity: 3237.553787
2025-10-14 11:56:56,602 Stage: Train 0.5 | Epoch: 443 | Iter: 489600 | Total Loss: 0.002261 | Recon Loss: 0.001891 | Commit Loss: 0.000740 | Perplexity: 3237.568376
Trainning Epoch:  95%|█████████▌| 471/494 [145:24:15<7:36:31, 1190.93s/it]Trainning Epoch:  95%|█████████▌| 471/494 [145:24:15<7:36:31, 1190.93s/it]2025-10-14 12:00:53,659 Stage: Train 0.5 | Epoch: 444 | Iter: 489800 | Total Loss: 0.002215 | Recon Loss: 0.001849 | Commit Loss: 0.000731 | Perplexity: 3234.516771
2025-10-14 12:04:47,761 Stage: Train 0.5 | Epoch: 444 | Iter: 490000 | Total Loss: 0.002243 | Recon Loss: 0.001875 | Commit Loss: 0.000735 | Perplexity: 3236.207297
2025-10-14 12:08:41,995 Stage: Train 0.5 | Epoch: 444 | Iter: 490200 | Total Loss: 0.002248 | Recon Loss: 0.001883 | Commit Loss: 0.000730 | Perplexity: 3231.855983
2025-10-14 12:12:36,042 Stage: Train 0.5 | Epoch: 444 | Iter: 490400 | Total Loss: 0.002250 | Recon Loss: 0.001887 | Commit Loss: 0.000727 | Perplexity: 3230.344238
2025-10-14 12:16:30,674 Stage: Train 0.5 | Epoch: 444 | Iter: 490600 | Total Loss: 0.002234 | Recon Loss: 0.001868 | Commit Loss: 0.000733 | Perplexity: 3234.560981
Trainning Epoch:  96%|█████████▌| 472/494 [145:44:04<7:16:29, 1190.45s/it]Trainning Epoch:  96%|█████████▌| 472/494 [145:44:04<7:16:29, 1190.45s/it]2025-10-14 12:20:28,242 Stage: Train 0.5 | Epoch: 445 | Iter: 490800 | Total Loss: 0.002239 | Recon Loss: 0.001870 | Commit Loss: 0.000737 | Perplexity: 3231.247177
2025-10-14 12:24:23,251 Stage: Train 0.5 | Epoch: 445 | Iter: 491000 | Total Loss: 0.002248 | Recon Loss: 0.001883 | Commit Loss: 0.000728 | Perplexity: 3232.129203
2025-10-14 12:28:18,442 Stage: Train 0.5 | Epoch: 445 | Iter: 491200 | Total Loss: 0.002218 | Recon Loss: 0.001847 | Commit Loss: 0.000741 | Perplexity: 3232.403182
2025-10-14 12:32:13,553 Stage: Train 0.5 | Epoch: 445 | Iter: 491400 | Total Loss: 0.002247 | Recon Loss: 0.001877 | Commit Loss: 0.000740 | Perplexity: 3240.883329
2025-10-14 12:36:08,563 Stage: Train 0.5 | Epoch: 445 | Iter: 491600 | Total Loss: 0.002233 | Recon Loss: 0.001863 | Commit Loss: 0.000740 | Perplexity: 3243.889985
Trainning Epoch:  96%|█████████▌| 473/494 [146:03:58<6:57:00, 1191.46s/it]Trainning Epoch:  96%|█████████▌| 473/494 [146:03:58<6:57:00, 1191.47s/it]2025-10-14 12:40:06,573 Stage: Train 0.5 | Epoch: 446 | Iter: 491800 | Total Loss: 0.002253 | Recon Loss: 0.001883 | Commit Loss: 0.000739 | Perplexity: 3237.140426
2025-10-14 12:44:00,314 Stage: Train 0.5 | Epoch: 446 | Iter: 492000 | Total Loss: 0.002241 | Recon Loss: 0.001877 | Commit Loss: 0.000728 | Perplexity: 3226.472738
2025-10-14 12:47:55,257 Stage: Train 0.5 | Epoch: 446 | Iter: 492200 | Total Loss: 0.002225 | Recon Loss: 0.001856 | Commit Loss: 0.000739 | Perplexity: 3241.272369
2025-10-14 12:51:49,930 Stage: Train 0.5 | Epoch: 446 | Iter: 492400 | Total Loss: 0.002265 | Recon Loss: 0.001898 | Commit Loss: 0.000734 | Perplexity: 3233.311774
2025-10-14 12:55:44,610 Stage: Train 0.5 | Epoch: 446 | Iter: 492600 | Total Loss: 0.002260 | Recon Loss: 0.001893 | Commit Loss: 0.000733 | Perplexity: 3227.940979
2025-10-14 12:59:39,501 Stage: Train 0.5 | Epoch: 446 | Iter: 492800 | Total Loss: 0.002228 | Recon Loss: 0.001860 | Commit Loss: 0.000735 | Perplexity: 3235.100675
Trainning Epoch:  96%|█████████▌| 474/494 [146:23:50<6:37:08, 1191.40s/it]Trainning Epoch:  96%|█████████▌| 474/494 [146:23:50<6:37:08, 1191.40s/it]2025-10-14 13:03:36,747 Stage: Train 0.5 | Epoch: 447 | Iter: 493000 | Total Loss: 0.002231 | Recon Loss: 0.001864 | Commit Loss: 0.000735 | Perplexity: 3238.157699
2025-10-14 13:07:30,959 Stage: Train 0.5 | Epoch: 447 | Iter: 493200 | Total Loss: 0.002247 | Recon Loss: 0.001879 | Commit Loss: 0.000736 | Perplexity: 3233.188182
2025-10-14 13:11:25,439 Stage: Train 0.5 | Epoch: 447 | Iter: 493400 | Total Loss: 0.002257 | Recon Loss: 0.001887 | Commit Loss: 0.000740 | Perplexity: 3233.621622
2025-10-14 13:15:20,419 Stage: Train 0.5 | Epoch: 447 | Iter: 493600 | Total Loss: 0.002235 | Recon Loss: 0.001868 | Commit Loss: 0.000734 | Perplexity: 3234.775612
2025-10-14 13:19:15,329 Stage: Train 0.5 | Epoch: 447 | Iter: 493800 | Total Loss: 0.002258 | Recon Loss: 0.001889 | Commit Loss: 0.000738 | Perplexity: 3234.709510
Trainning Epoch:  96%|█████████▌| 475/494 [146:43:40<6:17:13, 1191.24s/it]Trainning Epoch:  96%|█████████▌| 475/494 [146:43:40<6:17:13, 1191.24s/it]2025-10-14 13:23:14,164 Stage: Train 0.5 | Epoch: 448 | Iter: 494000 | Total Loss: 0.002204 | Recon Loss: 0.001842 | Commit Loss: 0.000725 | Perplexity: 3233.401890
2025-10-14 13:27:10,132 Stage: Train 0.5 | Epoch: 448 | Iter: 494200 | Total Loss: 0.002244 | Recon Loss: 0.001876 | Commit Loss: 0.000735 | Perplexity: 3233.282665
2025-10-14 13:31:06,378 Stage: Train 0.5 | Epoch: 448 | Iter: 494400 | Total Loss: 0.002243 | Recon Loss: 0.001875 | Commit Loss: 0.000735 | Perplexity: 3235.657820
2025-10-14 13:35:02,714 Stage: Train 0.5 | Epoch: 448 | Iter: 494600 | Total Loss: 0.002226 | Recon Loss: 0.001859 | Commit Loss: 0.000735 | Perplexity: 3237.437427
2025-10-14 13:38:59,312 Stage: Train 0.5 | Epoch: 448 | Iter: 494800 | Total Loss: 0.002252 | Recon Loss: 0.001884 | Commit Loss: 0.000735 | Perplexity: 3248.892419
Trainning Epoch:  96%|█████████▋| 476/494 [147:03:40<5:58:06, 1193.68s/it]Trainning Epoch:  96%|█████████▋| 476/494 [147:03:40<5:58:06, 1193.68s/it]2025-10-14 13:42:55,288 Stage: Train 0.5 | Epoch: 449 | Iter: 495000 | Total Loss: 0.002233 | Recon Loss: 0.001869 | Commit Loss: 0.000729 | Perplexity: 3232.454640
2025-10-14 13:46:49,033 Stage: Train 0.5 | Epoch: 449 | Iter: 495200 | Total Loss: 0.002256 | Recon Loss: 0.001891 | Commit Loss: 0.000730 | Perplexity: 3240.013987
2025-10-14 13:50:43,079 Stage: Train 0.5 | Epoch: 449 | Iter: 495400 | Total Loss: 0.002258 | Recon Loss: 0.001891 | Commit Loss: 0.000733 | Perplexity: 3225.657599
2025-10-14 13:54:37,220 Stage: Train 0.5 | Epoch: 449 | Iter: 495600 | Total Loss: 0.002234 | Recon Loss: 0.001865 | Commit Loss: 0.000737 | Perplexity: 3223.537029
2025-10-14 13:58:31,389 Stage: Train 0.5 | Epoch: 449 | Iter: 495800 | Total Loss: 0.002229 | Recon Loss: 0.001862 | Commit Loss: 0.000733 | Perplexity: 3241.398298
Trainning Epoch:  97%|█████████▋| 477/494 [147:23:27<5:37:38, 1191.66s/it]Trainning Epoch:  97%|█████████▋| 477/494 [147:23:27<5:37:38, 1191.66s/it]2025-10-14 14:02:28,389 Stage: Train 0.5 | Epoch: 450 | Iter: 496000 | Total Loss: 0.002214 | Recon Loss: 0.001847 | Commit Loss: 0.000734 | Perplexity: 3238.168212
2025-10-14 14:06:22,804 Stage: Train 0.5 | Epoch: 450 | Iter: 496200 | Total Loss: 0.002231 | Recon Loss: 0.001866 | Commit Loss: 0.000730 | Perplexity: 3238.894930
2025-10-14 14:10:17,004 Stage: Train 0.5 | Epoch: 450 | Iter: 496400 | Total Loss: 0.002230 | Recon Loss: 0.001860 | Commit Loss: 0.000739 | Perplexity: 3240.619769
2025-10-14 14:14:11,343 Stage: Train 0.5 | Epoch: 450 | Iter: 496600 | Total Loss: 0.002280 | Recon Loss: 0.001908 | Commit Loss: 0.000744 | Perplexity: 3238.723130
2025-10-14 14:18:05,785 Stage: Train 0.5 | Epoch: 450 | Iter: 496800 | Total Loss: 0.002245 | Recon Loss: 0.001879 | Commit Loss: 0.000732 | Perplexity: 3234.296859
Trainning Epoch:  97%|█████████▋| 478/494 [147:43:16<5:17:37, 1191.06s/it]Trainning Epoch:  97%|█████████▋| 478/494 [147:43:16<5:17:37, 1191.06s/it]2025-10-14 14:22:04,312 Stage: Train 0.5 | Epoch: 451 | Iter: 497000 | Total Loss: 0.002233 | Recon Loss: 0.001864 | Commit Loss: 0.000737 | Perplexity: 3236.953253
2025-10-14 14:26:00,714 Stage: Train 0.5 | Epoch: 451 | Iter: 497200 | Total Loss: 0.002214 | Recon Loss: 0.001848 | Commit Loss: 0.000732 | Perplexity: 3235.038997
2025-10-14 14:29:56,951 Stage: Train 0.5 | Epoch: 451 | Iter: 497400 | Total Loss: 0.002278 | Recon Loss: 0.001899 | Commit Loss: 0.000757 | Perplexity: 3236.415308
2025-10-14 14:33:53,285 Stage: Train 0.5 | Epoch: 451 | Iter: 497600 | Total Loss: 0.002261 | Recon Loss: 0.001894 | Commit Loss: 0.000734 | Perplexity: 3233.451012
2025-10-14 14:37:49,709 Stage: Train 0.5 | Epoch: 451 | Iter: 497800 | Total Loss: 0.002255 | Recon Loss: 0.001891 | Commit Loss: 0.000728 | Perplexity: 3234.112982
Trainning Epoch:  97%|█████████▋| 479/494 [148:03:16<4:58:25, 1193.73s/it]Trainning Epoch:  97%|█████████▋| 479/494 [148:03:16<4:58:25, 1193.73s/it]2025-10-14 14:41:47,675 Stage: Train 0.5 | Epoch: 452 | Iter: 498000 | Total Loss: 0.002249 | Recon Loss: 0.001881 | Commit Loss: 0.000737 | Perplexity: 3224.675575
2025-10-14 14:45:42,295 Stage: Train 0.5 | Epoch: 452 | Iter: 498200 | Total Loss: 0.002221 | Recon Loss: 0.001853 | Commit Loss: 0.000735 | Perplexity: 3245.868591
2025-10-14 14:49:36,918 Stage: Train 0.5 | Epoch: 452 | Iter: 498400 | Total Loss: 0.002239 | Recon Loss: 0.001871 | Commit Loss: 0.000736 | Perplexity: 3229.395378
2025-10-14 14:53:31,961 Stage: Train 0.5 | Epoch: 452 | Iter: 498600 | Total Loss: 0.002230 | Recon Loss: 0.001861 | Commit Loss: 0.000737 | Perplexity: 3239.864689
2025-10-14 14:57:26,784 Stage: Train 0.5 | Epoch: 452 | Iter: 498800 | Total Loss: 0.002217 | Recon Loss: 0.001850 | Commit Loss: 0.000734 | Perplexity: 3237.993793
Trainning Epoch:  97%|█████████▋| 480/494 [148:23:08<4:38:23, 1193.13s/it]Trainning Epoch:  97%|█████████▋| 480/494 [148:23:08<4:38:23, 1193.13s/it]2025-10-14 15:01:24,443 Stage: Train 0.5 | Epoch: 453 | Iter: 499000 | Total Loss: 0.002249 | Recon Loss: 0.001887 | Commit Loss: 0.000724 | Perplexity: 3227.039524
2025-10-14 15:05:18,559 Stage: Train 0.5 | Epoch: 453 | Iter: 499200 | Total Loss: 0.002221 | Recon Loss: 0.001853 | Commit Loss: 0.000737 | Perplexity: 3237.793898
2025-10-14 15:09:12,670 Stage: Train 0.5 | Epoch: 453 | Iter: 499400 | Total Loss: 0.002243 | Recon Loss: 0.001874 | Commit Loss: 0.000739 | Perplexity: 3242.389613
2025-10-14 15:13:06,839 Stage: Train 0.5 | Epoch: 453 | Iter: 499600 | Total Loss: 0.002215 | Recon Loss: 0.001847 | Commit Loss: 0.000736 | Perplexity: 3239.079825
2025-10-14 15:17:01,079 Stage: Train 0.5 | Epoch: 453 | Iter: 499800 | Total Loss: 0.002230 | Recon Loss: 0.001860 | Commit Loss: 0.000740 | Perplexity: 3240.180820
Trainning Epoch:  97%|█████████▋| 481/494 [148:42:57<4:18:15, 1191.98s/it]Trainning Epoch:  97%|█████████▋| 481/494 [148:42:57<4:18:15, 1191.98s/it]2025-10-14 15:20:57,889 Stage: Train 0.5 | Epoch: 454 | Iter: 500000 | Total Loss: 0.002256 | Recon Loss: 0.001894 | Commit Loss: 0.000725 | Perplexity: 3241.652568
2025-10-14 15:20:57,889 Saving model at iteration 500000
2025-10-14 15:20:58,065 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000
Trainning Epoch:  97%|█████████▋| 481/494 [148:44:56<4:15:33, 1179.51s/it]
2025-10-14 15:20:59,495 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000/model.safetensors
2025-10-14 15:21:01,236 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000/optimizer.bin
2025-10-14 15:21:01,236 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000/scheduler.bin
2025-10-14 15:21:01,236 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000/sampler.bin
2025-10-14 15:21:01,237 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_192x256/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_455_step_500000/random_states_0.pkl
Trainning Epoch:  97%|█████████▋| 481/494 [148:44:58<4:15:33, 1179.51s/it]
2025-10-14 15:21:01,503 Training finished
[rank0]:[W1014 15:21:02.644940070 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
