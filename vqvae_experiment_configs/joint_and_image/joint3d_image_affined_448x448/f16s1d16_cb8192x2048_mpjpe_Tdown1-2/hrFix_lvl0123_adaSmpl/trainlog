The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-10-11 15:23:23,521 
python train_vqvae_new.py --batch_size 16 --config vqvae_experiment_configs/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/config.yaml --data_mode joint3d --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl --not_find_unused_parameters --nb_code 8192 --codebook_dim 2048 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level [0,1,2,3] --vision_guidance_ratio 0.5 --downsample_time [1,2] --frame_upsample_rate [2.0,1.0] --fix_weights --resume_pth  --vision_guidance_where enc --vision_guidance_fuse ada_sample
2025-10-11 15:23:23,522 
PID: 1091473
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
vision backbone weights are fixed
2025-10-11 15:24:57,733 Data loaded with 97196 samples
vision backbone weights are fixed
2025-10-11 15:24:59,059 Trainable parameters: 179,398,051
2025-10-11 15:24:59,059 Non-trainable parameters: 28,535,552
2025-10-11 15:25:00,874 Number of trainable parameters: 179.398051 M
2025-10-11 15:25:00,875 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'joint3d', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [['image']], 'normalize': 'anisotropic', 'filter_invalid_images': True, 'processed_image_shape': [448, 448], 'backbone': 'hrnet_32', 'get_item_list': ['factor_2_5d', 'video_rgb', 'joint3d_image_affined', 'joint3d_image_affined_normed', 'joint3d_image_affined_scale', 'joint3d_image_affined_transl', 'affine_trans', 'affine_trans_inv', 'joint_2_5d_image'], 'config': 'vqvae_experiment_configs/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/config.yaml', 'resume_pth': '', 'batch_size': 16, 'commit_ratio': 0.5, 'nb_code': 8192, 'codebook_dim': 2048, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_image_affined_normed', 'hrnet_output_level': [0, 1, 2, 3], 'fix_weights': True, 'fix_weights_except': 'PLACEHOLDERPLACEHOLDERPLACEHOLDER', 'vision_guidance_ratio': 0.5, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0], 'vision_guidance_where': 'enc', 'vision_guidance_fuse': 'ada_sample'}
Trainning Epoch:   0%|          | 0/165 [00:00<?, ?it/s]Trainning Epoch:   0%|          | 0/165 [00:00<?, ?it/s]Adaptive sampling applied successfully.
Adaptive sampling applied successfully.
2025-10-11 15:30:28,829 current_lr 0.000008 at iteration 200
2025-10-11 15:30:30,183 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.207557 | Recon Loss: 0.198444 | Commit Loss: 0.018226 | Perplexity: 225.538571
2025-10-11 15:35:52,999 current_lr 0.000016 at iteration 400
2025-10-11 15:35:54,355 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.137854 | Recon Loss: 0.111679 | Commit Loss: 0.052350 | Perplexity: 275.506158
2025-10-11 15:41:16,738 current_lr 0.000024 at iteration 600
2025-10-11 15:41:18,080 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.095377 | Recon Loss: 0.070582 | Commit Loss: 0.049589 | Perplexity: 566.163088
2025-10-11 15:46:40,566 current_lr 0.000032 at iteration 800
2025-10-11 15:46:41,911 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.078806 | Recon Loss: 0.056503 | Commit Loss: 0.044607 | Perplexity: 754.750464
2025-10-11 15:52:05,057 current_lr 0.000040 at iteration 1000
2025-10-11 15:52:06,408 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.069946 | Recon Loss: 0.049750 | Commit Loss: 0.040392 | Perplexity: 800.147519
2025-10-11 15:57:29,892 current_lr 0.000048 at iteration 1200
2025-10-11 15:57:31,244 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.062988 | Recon Loss: 0.045052 | Commit Loss: 0.035871 | Perplexity: 811.847959
2025-10-11 16:02:54,489 current_lr 0.000056 at iteration 1400
2025-10-11 16:02:55,840 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.057536 | Recon Loss: 0.041734 | Commit Loss: 0.031603 | Perplexity: 824.753231
2025-10-11 16:08:19,378 current_lr 0.000064 at iteration 1600
2025-10-11 16:08:20,725 Stage: Warm Up | Epoch: 0 | Iter: 1600 | Total Loss: 0.052720 | Recon Loss: 0.038844 | Commit Loss: 0.027751 | Perplexity: 825.692024
2025-10-11 16:13:43,387 current_lr 0.000072 at iteration 1800
2025-10-11 16:13:44,733 Stage: Warm Up | Epoch: 0 | Iter: 1800 | Total Loss: 0.049373 | Recon Loss: 0.037390 | Commit Loss: 0.023966 | Perplexity: 816.236896
2025-10-11 16:19:07,281 current_lr 0.000080 at iteration 2000
2025-10-11 16:19:08,628 Stage: Warm Up | Epoch: 0 | Iter: 2000 | Total Loss: 0.045451 | Recon Loss: 0.035143 | Commit Loss: 0.020615 | Perplexity: 814.020716
2025-10-11 16:24:31,210 current_lr 0.000088 at iteration 2200
2025-10-11 16:24:32,559 Stage: Warm Up | Epoch: 0 | Iter: 2200 | Total Loss: 0.043037 | Recon Loss: 0.034128 | Commit Loss: 0.017817 | Perplexity: 798.079161
2025-10-11 16:29:55,291 current_lr 0.000096 at iteration 2400
2025-10-11 16:29:56,643 Stage: Warm Up | Epoch: 0 | Iter: 2400 | Total Loss: 0.039682 | Recon Loss: 0.032184 | Commit Loss: 0.014996 | Perplexity: 780.166095
2025-10-11 16:35:19,496 current_lr 0.000104 at iteration 2600
2025-10-11 16:35:20,847 Stage: Warm Up | Epoch: 0 | Iter: 2600 | Total Loss: 0.036478 | Recon Loss: 0.030134 | Commit Loss: 0.012687 | Perplexity: 752.758063
2025-10-11 16:40:43,673 current_lr 0.000112 at iteration 2800
2025-10-11 16:40:45,023 Stage: Warm Up | Epoch: 0 | Iter: 2800 | Total Loss: 0.035772 | Recon Loss: 0.030298 | Commit Loss: 0.010949 | Perplexity: 726.102053
2025-10-11 16:46:07,387 current_lr 0.000120 at iteration 3000
2025-10-11 16:46:08,741 Stage: Warm Up | Epoch: 0 | Iter: 3000 | Total Loss: 0.033163 | Recon Loss: 0.028158 | Commit Loss: 0.010009 | Perplexity: 699.081922
Trainning Epoch:   1%|          | 1/165 [1:22:09<224:34:13, 4929.60s/it]Trainning Epoch:   1%|          | 1/165 [1:22:09<224:34:01, 4929.52s/it]2025-10-11 16:51:36,285 current_lr 0.000128 at iteration 3200
2025-10-11 16:51:37,635 Stage: Warm Up | Epoch: 1 | Iter: 3200 | Total Loss: 0.032627 | Recon Loss: 0.028286 | Commit Loss: 0.008681 | Perplexity: 673.719911
2025-10-11 16:57:01,321 current_lr 0.000136 at iteration 3400
2025-10-11 16:57:02,675 Stage: Warm Up | Epoch: 1 | Iter: 3400 | Total Loss: 0.031672 | Recon Loss: 0.027534 | Commit Loss: 0.008277 | Perplexity: 655.774857
2025-10-11 17:02:25,663 current_lr 0.000144 at iteration 3600
2025-10-11 17:02:27,012 Stage: Warm Up | Epoch: 1 | Iter: 3600 | Total Loss: 0.030879 | Recon Loss: 0.027180 | Commit Loss: 0.007399 | Perplexity: 628.184928
2025-10-11 17:07:50,605 current_lr 0.000152 at iteration 3800
2025-10-11 17:07:51,964 Stage: Warm Up | Epoch: 1 | Iter: 3800 | Total Loss: 0.030495 | Recon Loss: 0.027003 | Commit Loss: 0.006983 | Perplexity: 616.608899
2025-10-11 17:13:15,429 current_lr 0.000160 at iteration 4000
2025-10-11 17:13:16,789 Stage: Warm Up | Epoch: 1 | Iter: 4000 | Total Loss: 0.028850 | Recon Loss: 0.025531 | Commit Loss: 0.006639 | Perplexity: 609.251825
2025-10-11 17:18:39,804 current_lr 0.000168 at iteration 4200
2025-10-11 17:18:41,157 Stage: Warm Up | Epoch: 1 | Iter: 4200 | Total Loss: 0.028651 | Recon Loss: 0.025551 | Commit Loss: 0.006199 | Perplexity: 598.180520
2025-10-11 17:24:04,482 current_lr 0.000176 at iteration 4400
2025-10-11 17:24:05,835 Stage: Warm Up | Epoch: 1 | Iter: 4400 | Total Loss: 0.028439 | Recon Loss: 0.025572 | Commit Loss: 0.005734 | Perplexity: 592.201882
2025-10-11 17:29:29,491 current_lr 0.000184 at iteration 4600
2025-10-11 17:29:30,848 Stage: Warm Up | Epoch: 1 | Iter: 4600 | Total Loss: 0.027375 | Recon Loss: 0.024719 | Commit Loss: 0.005311 | Perplexity: 595.668504
2025-10-11 17:34:54,050 current_lr 0.000192 at iteration 4800
2025-10-11 17:34:55,403 Stage: Warm Up | Epoch: 1 | Iter: 4800 | Total Loss: 0.027129 | Recon Loss: 0.024485 | Commit Loss: 0.005289 | Perplexity: 581.119370
2025-10-11 17:40:18,295 current_lr 0.000200 at iteration 5000
2025-10-11 17:40:19,653 Stage: Warm Up | Epoch: 1 | Iter: 5000 | Total Loss: 0.026204 | Recon Loss: 0.023740 | Commit Loss: 0.004927 | Perplexity: 576.802052
2025-10-11 17:45:44,138 Stage: Train 0.5 | Epoch: 1 | Iter: 5200 | Total Loss: 0.026024 | Recon Loss: 0.023618 | Commit Loss: 0.004812 | Perplexity: 581.249105
2025-10-11 17:51:08,663 Stage: Train 0.5 | Epoch: 1 | Iter: 5400 | Total Loss: 0.025229 | Recon Loss: 0.022947 | Commit Loss: 0.004564 | Perplexity: 572.817504
2025-10-11 17:56:32,869 Stage: Train 0.5 | Epoch: 1 | Iter: 5600 | Total Loss: 0.024464 | Recon Loss: 0.022113 | Commit Loss: 0.004702 | Perplexity: 571.333666
2025-10-11 18:01:57,140 Stage: Train 0.5 | Epoch: 1 | Iter: 5800 | Total Loss: 0.024175 | Recon Loss: 0.021900 | Commit Loss: 0.004550 | Perplexity: 574.093715
2025-10-11 18:07:21,691 Stage: Train 0.5 | Epoch: 1 | Iter: 6000 | Total Loss: 0.023455 | Recon Loss: 0.021160 | Commit Loss: 0.004590 | Perplexity: 563.620815
Trainning Epoch:   1%|          | 2/165 [2:44:23<223:19:46, 4932.43s/it]Trainning Epoch:   1%|          | 2/165 [2:44:24<223:19:51, 4932.47s/it]2025-10-11 18:12:50,409 Stage: Train 0.5 | Epoch: 2 | Iter: 6200 | Total Loss: 0.022882 | Recon Loss: 0.020676 | Commit Loss: 0.004411 | Perplexity: 557.146172
2025-10-11 18:18:15,143 Stage: Train 0.5 | Epoch: 2 | Iter: 6400 | Total Loss: 0.022243 | Recon Loss: 0.020018 | Commit Loss: 0.004450 | Perplexity: 556.276864
2025-10-11 18:23:39,725 Stage: Train 0.5 | Epoch: 2 | Iter: 6600 | Total Loss: 0.021912 | Recon Loss: 0.019665 | Commit Loss: 0.004493 | Perplexity: 557.027649
2025-10-11 18:29:04,111 Stage: Train 0.5 | Epoch: 2 | Iter: 6800 | Total Loss: 0.021834 | Recon Loss: 0.019631 | Commit Loss: 0.004406 | Perplexity: 556.364659
2025-10-11 18:34:28,527 Stage: Train 0.5 | Epoch: 2 | Iter: 7000 | Total Loss: 0.021590 | Recon Loss: 0.019402 | Commit Loss: 0.004378 | Perplexity: 562.301792
2025-10-11 18:39:52,724 Stage: Train 0.5 | Epoch: 2 | Iter: 7200 | Total Loss: 0.020894 | Recon Loss: 0.018778 | Commit Loss: 0.004233 | Perplexity: 573.713187
2025-10-11 18:45:17,006 Stage: Train 0.5 | Epoch: 2 | Iter: 7400 | Total Loss: 0.020651 | Recon Loss: 0.018514 | Commit Loss: 0.004274 | Perplexity: 567.635169
2025-10-11 18:50:41,395 Stage: Train 0.5 | Epoch: 2 | Iter: 7600 | Total Loss: 0.020373 | Recon Loss: 0.018296 | Commit Loss: 0.004154 | Perplexity: 574.486027
2025-10-11 18:56:05,618 Stage: Train 0.5 | Epoch: 2 | Iter: 7800 | Total Loss: 0.019567 | Recon Loss: 0.017434 | Commit Loss: 0.004266 | Perplexity: 582.393615
2025-10-11 19:01:30,237 Stage: Train 0.5 | Epoch: 2 | Iter: 8000 | Total Loss: 0.019545 | Recon Loss: 0.017501 | Commit Loss: 0.004089 | Perplexity: 597.979664
2025-10-11 19:06:55,384 Stage: Train 0.5 | Epoch: 2 | Iter: 8200 | Total Loss: 0.019473 | Recon Loss: 0.017526 | Commit Loss: 0.003894 | Perplexity: 601.801325
2025-10-11 19:12:20,508 Stage: Train 0.5 | Epoch: 2 | Iter: 8400 | Total Loss: 0.018839 | Recon Loss: 0.016820 | Commit Loss: 0.004037 | Perplexity: 610.200822
2025-10-11 19:17:45,581 Stage: Train 0.5 | Epoch: 2 | Iter: 8600 | Total Loss: 0.018570 | Recon Loss: 0.016663 | Commit Loss: 0.003814 | Perplexity: 628.124083
2025-10-11 19:23:10,182 Stage: Train 0.5 | Epoch: 2 | Iter: 8800 | Total Loss: 0.018544 | Recon Loss: 0.016614 | Commit Loss: 0.003860 | Perplexity: 625.252994
2025-10-11 19:28:34,970 Stage: Train 0.5 | Epoch: 2 | Iter: 9000 | Total Loss: 0.017947 | Recon Loss: 0.016020 | Commit Loss: 0.003854 | Perplexity: 641.142101
Trainning Epoch:   2%|▏         | 3/165 [4:06:39<222:00:52, 4933.66s/it]Trainning Epoch:   2%|▏         | 3/165 [4:06:39<222:00:51, 4933.65s/it]2025-10-11 19:34:04,347 Stage: Train 0.5 | Epoch: 3 | Iter: 9200 | Total Loss: 0.017637 | Recon Loss: 0.015745 | Commit Loss: 0.003784 | Perplexity: 647.235096
2025-10-11 19:39:29,120 Stage: Train 0.5 | Epoch: 3 | Iter: 9400 | Total Loss: 0.017828 | Recon Loss: 0.015963 | Commit Loss: 0.003729 | Perplexity: 649.862075
2025-10-11 19:44:54,002 Stage: Train 0.5 | Epoch: 3 | Iter: 9600 | Total Loss: 0.017588 | Recon Loss: 0.015660 | Commit Loss: 0.003855 | Perplexity: 657.768270
2025-10-11 19:50:18,888 Stage: Train 0.5 | Epoch: 3 | Iter: 9800 | Total Loss: 0.016761 | Recon Loss: 0.014859 | Commit Loss: 0.003803 | Perplexity: 672.322699
2025-10-11 19:55:43,746 Stage: Train 0.5 | Epoch: 3 | Iter: 10000 | Total Loss: 0.016826 | Recon Loss: 0.015001 | Commit Loss: 0.003651 | Perplexity: 674.311561
2025-10-11 20:01:08,571 Stage: Train 0.5 | Epoch: 3 | Iter: 10200 | Total Loss: 0.017204 | Recon Loss: 0.015314 | Commit Loss: 0.003779 | Perplexity: 675.913621
2025-10-11 20:06:33,375 Stage: Train 0.5 | Epoch: 3 | Iter: 10400 | Total Loss: 0.016703 | Recon Loss: 0.014877 | Commit Loss: 0.003652 | Perplexity: 673.713884
2025-10-11 20:11:58,168 Stage: Train 0.5 | Epoch: 3 | Iter: 10600 | Total Loss: 0.016318 | Recon Loss: 0.014483 | Commit Loss: 0.003671 | Perplexity: 684.379557
2025-10-11 20:17:22,894 Stage: Train 0.5 | Epoch: 3 | Iter: 10800 | Total Loss: 0.016525 | Recon Loss: 0.014747 | Commit Loss: 0.003557 | Perplexity: 684.997829
2025-10-11 20:22:47,455 Stage: Train 0.5 | Epoch: 3 | Iter: 11000 | Total Loss: 0.016362 | Recon Loss: 0.014522 | Commit Loss: 0.003680 | Perplexity: 695.601736
2025-10-11 20:28:12,363 Stage: Train 0.5 | Epoch: 3 | Iter: 11200 | Total Loss: 0.016521 | Recon Loss: 0.014659 | Commit Loss: 0.003725 | Perplexity: 693.365628
2025-10-11 20:33:36,935 Stage: Train 0.5 | Epoch: 3 | Iter: 11400 | Total Loss: 0.015795 | Recon Loss: 0.013978 | Commit Loss: 0.003633 | Perplexity: 707.101367
2025-10-11 20:39:01,590 Stage: Train 0.5 | Epoch: 3 | Iter: 11600 | Total Loss: 0.016062 | Recon Loss: 0.014226 | Commit Loss: 0.003671 | Perplexity: 709.318490
2025-10-11 20:44:25,712 Stage: Train 0.5 | Epoch: 3 | Iter: 11800 | Total Loss: 0.016012 | Recon Loss: 0.014160 | Commit Loss: 0.003705 | Perplexity: 700.779882
2025-10-11 20:49:50,224 Stage: Train 0.5 | Epoch: 3 | Iter: 12000 | Total Loss: 0.015490 | Recon Loss: 0.013685 | Commit Loss: 0.003610 | Perplexity: 710.875797
Trainning Epoch:   2%|▏         | 4/165 [5:28:56<220:42:12, 4934.98s/it]Trainning Epoch:   2%|▏         | 4/165 [5:28:56<220:42:14, 4934.99s/it]2025-10-11 20:55:19,469 Stage: Train 0.5 | Epoch: 4 | Iter: 12200 | Total Loss: 0.015284 | Recon Loss: 0.013452 | Commit Loss: 0.003664 | Perplexity: 712.722813
2025-10-11 21:00:43,967 Stage: Train 0.5 | Epoch: 4 | Iter: 12400 | Total Loss: 0.015907 | Recon Loss: 0.014061 | Commit Loss: 0.003691 | Perplexity: 715.259149
2025-10-11 21:06:08,314 Stage: Train 0.5 | Epoch: 4 | Iter: 12600 | Total Loss: 0.014962 | Recon Loss: 0.013143 | Commit Loss: 0.003640 | Perplexity: 721.448204
2025-10-11 21:11:32,923 Stage: Train 0.5 | Epoch: 4 | Iter: 12800 | Total Loss: 0.015261 | Recon Loss: 0.013442 | Commit Loss: 0.003637 | Perplexity: 725.531721
2025-10-11 21:16:57,257 Stage: Train 0.5 | Epoch: 4 | Iter: 13000 | Total Loss: 0.014979 | Recon Loss: 0.013175 | Commit Loss: 0.003608 | Perplexity: 733.804983
2025-10-11 21:22:22,009 Stage: Train 0.5 | Epoch: 4 | Iter: 13200 | Total Loss: 0.014793 | Recon Loss: 0.013033 | Commit Loss: 0.003521 | Perplexity: 738.672333
2025-10-11 21:27:46,297 Stage: Train 0.5 | Epoch: 4 | Iter: 13400 | Total Loss: 0.014895 | Recon Loss: 0.013070 | Commit Loss: 0.003651 | Perplexity: 746.929683
2025-10-11 21:33:10,540 Stage: Train 0.5 | Epoch: 4 | Iter: 13600 | Total Loss: 0.014558 | Recon Loss: 0.012765 | Commit Loss: 0.003585 | Perplexity: 749.376542
2025-10-11 21:38:35,045 Stage: Train 0.5 | Epoch: 4 | Iter: 13800 | Total Loss: 0.015024 | Recon Loss: 0.013238 | Commit Loss: 0.003572 | Perplexity: 751.925594
2025-10-11 21:43:59,344 Stage: Train 0.5 | Epoch: 4 | Iter: 14000 | Total Loss: 0.014654 | Recon Loss: 0.012888 | Commit Loss: 0.003532 | Perplexity: 742.407324
2025-10-11 21:49:23,699 Stage: Train 0.5 | Epoch: 4 | Iter: 14200 | Total Loss: 0.014708 | Recon Loss: 0.012908 | Commit Loss: 0.003599 | Perplexity: 756.165310
2025-10-11 21:54:48,304 Stage: Train 0.5 | Epoch: 4 | Iter: 14400 | Total Loss: 0.014345 | Recon Loss: 0.012565 | Commit Loss: 0.003560 | Perplexity: 754.438847
2025-10-11 22:00:12,657 Stage: Train 0.5 | Epoch: 4 | Iter: 14600 | Total Loss: 0.014868 | Recon Loss: 0.013034 | Commit Loss: 0.003668 | Perplexity: 752.032944
2025-10-11 22:05:37,085 Stage: Train 0.5 | Epoch: 4 | Iter: 14800 | Total Loss: 0.014263 | Recon Loss: 0.012507 | Commit Loss: 0.003511 | Perplexity: 760.280994
2025-10-11 22:11:01,221 Stage: Train 0.5 | Epoch: 4 | Iter: 15000 | Total Loss: 0.014517 | Recon Loss: 0.012670 | Commit Loss: 0.003695 | Perplexity: 755.613557
Trainning Epoch:   3%|▎         | 5/165 [6:51:08<219:17:30, 4934.06s/it]Trainning Epoch:   3%|▎         | 5/165 [6:51:08<219:17:30, 4934.06s/it]2025-10-11 22:16:30,214 Stage: Train 0.5 | Epoch: 5 | Iter: 15200 | Total Loss: 0.014253 | Recon Loss: 0.012486 | Commit Loss: 0.003536 | Perplexity: 763.094226
2025-10-11 22:21:54,454 Stage: Train 0.5 | Epoch: 5 | Iter: 15400 | Total Loss: 0.013925 | Recon Loss: 0.012153 | Commit Loss: 0.003544 | Perplexity: 762.749084
2025-10-11 22:27:18,900 Stage: Train 0.5 | Epoch: 5 | Iter: 15600 | Total Loss: 0.014337 | Recon Loss: 0.012568 | Commit Loss: 0.003538 | Perplexity: 764.971511
2025-10-11 22:32:42,996 Stage: Train 0.5 | Epoch: 5 | Iter: 15800 | Total Loss: 0.014123 | Recon Loss: 0.012317 | Commit Loss: 0.003611 | Perplexity: 774.040543
2025-10-11 22:38:07,169 Stage: Train 0.5 | Epoch: 5 | Iter: 16000 | Total Loss: 0.014101 | Recon Loss: 0.012285 | Commit Loss: 0.003633 | Perplexity: 770.270288
2025-10-11 22:43:31,564 Stage: Train 0.5 | Epoch: 5 | Iter: 16200 | Total Loss: 0.013758 | Recon Loss: 0.012021 | Commit Loss: 0.003475 | Perplexity: 778.162944
2025-10-11 22:48:55,799 Stage: Train 0.5 | Epoch: 5 | Iter: 16400 | Total Loss: 0.013802 | Recon Loss: 0.012010 | Commit Loss: 0.003585 | Perplexity: 776.927556
2025-10-11 22:54:20,296 Stage: Train 0.5 | Epoch: 5 | Iter: 16600 | Total Loss: 0.013949 | Recon Loss: 0.012120 | Commit Loss: 0.003659 | Perplexity: 768.341834
2025-10-11 22:59:44,661 Stage: Train 0.5 | Epoch: 5 | Iter: 16800 | Total Loss: 0.013851 | Recon Loss: 0.012077 | Commit Loss: 0.003547 | Perplexity: 781.884942
2025-10-11 23:05:09,106 Stage: Train 0.5 | Epoch: 5 | Iter: 17000 | Total Loss: 0.013745 | Recon Loss: 0.011930 | Commit Loss: 0.003631 | Perplexity: 785.079913
2025-10-11 23:10:33,641 Stage: Train 0.5 | Epoch: 5 | Iter: 17200 | Total Loss: 0.013692 | Recon Loss: 0.011898 | Commit Loss: 0.003589 | Perplexity: 785.173132
2025-10-11 23:15:58,090 Stage: Train 0.5 | Epoch: 5 | Iter: 17400 | Total Loss: 0.013447 | Recon Loss: 0.011659 | Commit Loss: 0.003574 | Perplexity: 789.009859
2025-10-11 23:21:22,096 Stage: Train 0.5 | Epoch: 5 | Iter: 17600 | Total Loss: 0.013484 | Recon Loss: 0.011694 | Commit Loss: 0.003579 | Perplexity: 793.676677
2025-10-11 23:26:46,537 Stage: Train 0.5 | Epoch: 5 | Iter: 17800 | Total Loss: 0.013630 | Recon Loss: 0.011841 | Commit Loss: 0.003580 | Perplexity: 787.081669
2025-10-11 23:32:10,773 Stage: Train 0.5 | Epoch: 5 | Iter: 18000 | Total Loss: 0.013282 | Recon Loss: 0.011511 | Commit Loss: 0.003542 | Perplexity: 803.826147
2025-10-11 23:37:34,897 Stage: Train 0.5 | Epoch: 5 | Iter: 18200 | Total Loss: 0.013555 | Recon Loss: 0.011745 | Commit Loss: 0.003622 | Perplexity: 805.293744
Trainning Epoch:   4%|▎         | 6/165 [8:13:19<217:52:22, 4932.97s/it]Trainning Epoch:   4%|▎         | 6/165 [8:13:19<217:52:23, 4932.98s/it]2025-10-11 23:43:04,729 Stage: Train 0.5 | Epoch: 6 | Iter: 18400 | Total Loss: 0.012995 | Recon Loss: 0.011275 | Commit Loss: 0.003441 | Perplexity: 806.958173
2025-10-11 23:48:29,016 Stage: Train 0.5 | Epoch: 6 | Iter: 18600 | Total Loss: 0.013354 | Recon Loss: 0.011535 | Commit Loss: 0.003638 | Perplexity: 798.653987
2025-10-11 23:53:53,280 Stage: Train 0.5 | Epoch: 6 | Iter: 18800 | Total Loss: 0.012893 | Recon Loss: 0.011182 | Commit Loss: 0.003423 | Perplexity: 811.523347
2025-10-11 23:59:17,409 Stage: Train 0.5 | Epoch: 6 | Iter: 19000 | Total Loss: 0.013071 | Recon Loss: 0.011329 | Commit Loss: 0.003483 | Perplexity: 808.125835
2025-10-12 00:04:41,848 Stage: Train 0.5 | Epoch: 6 | Iter: 19200 | Total Loss: 0.013007 | Recon Loss: 0.011225 | Commit Loss: 0.003564 | Perplexity: 819.914737
2025-10-12 00:10:06,235 Stage: Train 0.5 | Epoch: 6 | Iter: 19400 | Total Loss: 0.013083 | Recon Loss: 0.011311 | Commit Loss: 0.003544 | Perplexity: 805.819502
2025-10-12 00:15:30,644 Stage: Train 0.5 | Epoch: 6 | Iter: 19600 | Total Loss: 0.012797 | Recon Loss: 0.011051 | Commit Loss: 0.003493 | Perplexity: 813.036807
2025-10-12 00:20:55,207 Stage: Train 0.5 | Epoch: 6 | Iter: 19800 | Total Loss: 0.012883 | Recon Loss: 0.011121 | Commit Loss: 0.003524 | Perplexity: 823.341085
2025-10-12 00:26:20,111 Stage: Train 0.5 | Epoch: 6 | Iter: 20000 | Total Loss: 0.012514 | Recon Loss: 0.010819 | Commit Loss: 0.003390 | Perplexity: 823.174453
2025-10-12 00:26:20,111 Saving model at iteration 20000
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
2025-10-12 00:26:20,494 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000
2025-10-12 00:26:21,903 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000/model.safetensors
2025-10-12 00:26:23,736 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000/optimizer.bin
2025-10-12 00:26:23,737 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000/scheduler.bin
2025-10-12 00:26:23,737 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000/sampler.bin
2025-10-12 00:26:23,738 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_7_step_20000/random_states_0.pkl
2025-10-12 00:31:48,192 Stage: Train 0.5 | Epoch: 6 | Iter: 20200 | Total Loss: 0.012666 | Recon Loss: 0.010957 | Commit Loss: 0.003417 | Perplexity: 825.780548
2025-10-12 00:37:12,892 Stage: Train 0.5 | Epoch: 6 | Iter: 20400 | Total Loss: 0.012898 | Recon Loss: 0.011150 | Commit Loss: 0.003497 | Perplexity: 826.444639
2025-10-12 00:42:36,967 Stage: Train 0.5 | Epoch: 6 | Iter: 20600 | Total Loss: 0.012413 | Recon Loss: 0.010707 | Commit Loss: 0.003411 | Perplexity: 835.837883
2025-10-12 00:48:01,011 Stage: Train 0.5 | Epoch: 6 | Iter: 20800 | Total Loss: 0.012758 | Recon Loss: 0.011052 | Commit Loss: 0.003411 | Perplexity: 827.671148
2025-10-12 00:53:25,265 Stage: Train 0.5 | Epoch: 6 | Iter: 21000 | Total Loss: 0.012504 | Recon Loss: 0.010812 | Commit Loss: 0.003382 | Perplexity: 830.923275
2025-10-12 00:58:50,017 Stage: Train 0.5 | Epoch: 6 | Iter: 21200 | Total Loss: 0.012321 | Recon Loss: 0.010639 | Commit Loss: 0.003364 | Perplexity: 839.444180
Trainning Epoch:   4%|▍         | 7/165 [9:35:36<216:33:24, 4934.20s/it]Trainning Epoch:   4%|▍         | 7/165 [9:35:36<216:33:23, 4934.20s/it]2025-10-12 01:04:18,939 Stage: Train 0.5 | Epoch: 7 | Iter: 21400 | Total Loss: 0.012390 | Recon Loss: 0.010680 | Commit Loss: 0.003421 | Perplexity: 848.993219
2025-10-12 01:09:42,957 Stage: Train 0.5 | Epoch: 7 | Iter: 21600 | Total Loss: 0.012924 | Recon Loss: 0.011034 | Commit Loss: 0.003779 | Perplexity: 847.817140
2025-10-12 01:15:07,266 Stage: Train 0.5 | Epoch: 7 | Iter: 21800 | Total Loss: 0.012246 | Recon Loss: 0.010537 | Commit Loss: 0.003417 | Perplexity: 846.471300
2025-10-12 01:20:31,305 Stage: Train 0.5 | Epoch: 7 | Iter: 22000 | Total Loss: 0.012441 | Recon Loss: 0.010739 | Commit Loss: 0.003405 | Perplexity: 849.393993
2025-10-12 01:25:55,721 Stage: Train 0.5 | Epoch: 7 | Iter: 22200 | Total Loss: 0.012174 | Recon Loss: 0.010567 | Commit Loss: 0.003215 | Perplexity: 854.151807
2025-10-12 01:31:20,207 Stage: Train 0.5 | Epoch: 7 | Iter: 22400 | Total Loss: 0.012049 | Recon Loss: 0.010396 | Commit Loss: 0.003307 | Perplexity: 857.520226
2025-10-12 01:36:44,314 Stage: Train 0.5 | Epoch: 7 | Iter: 22600 | Total Loss: 0.012365 | Recon Loss: 0.010661 | Commit Loss: 0.003409 | Perplexity: 846.877508
2025-10-12 01:42:08,562 Stage: Train 0.5 | Epoch: 7 | Iter: 22800 | Total Loss: 0.012085 | Recon Loss: 0.010412 | Commit Loss: 0.003347 | Perplexity: 853.018930
2025-10-12 01:47:33,011 Stage: Train 0.5 | Epoch: 7 | Iter: 23000 | Total Loss: 0.012129 | Recon Loss: 0.010446 | Commit Loss: 0.003366 | Perplexity: 865.576283
2025-10-12 01:52:57,416 Stage: Train 0.5 | Epoch: 7 | Iter: 23200 | Total Loss: 0.011792 | Recon Loss: 0.010136 | Commit Loss: 0.003312 | Perplexity: 866.235742
2025-10-12 01:58:21,531 Stage: Train 0.5 | Epoch: 7 | Iter: 23400 | Total Loss: 0.012146 | Recon Loss: 0.010461 | Commit Loss: 0.003370 | Perplexity: 853.772408
2025-10-12 02:03:45,608 Stage: Train 0.5 | Epoch: 7 | Iter: 23600 | Total Loss: 0.012112 | Recon Loss: 0.010424 | Commit Loss: 0.003375 | Perplexity: 859.518165
2025-10-12 02:09:10,079 Stage: Train 0.5 | Epoch: 7 | Iter: 23800 | Total Loss: 0.012095 | Recon Loss: 0.010443 | Commit Loss: 0.003305 | Perplexity: 871.354125
2025-10-12 02:14:34,321 Stage: Train 0.5 | Epoch: 7 | Iter: 24000 | Total Loss: 0.012008 | Recon Loss: 0.010316 | Commit Loss: 0.003383 | Perplexity: 859.726013
2025-10-12 02:19:58,569 Stage: Train 0.5 | Epoch: 7 | Iter: 24200 | Total Loss: 0.012010 | Recon Loss: 0.010327 | Commit Loss: 0.003366 | Perplexity: 869.006360
Trainning Epoch:   5%|▍         | 8/165 [10:57:46<215:07:33, 4932.83s/it]Trainning Epoch:   5%|▍         | 8/165 [10:57:46<215:07:33, 4932.82s/it]2025-10-12 02:25:27,826 Stage: Train 0.5 | Epoch: 8 | Iter: 24400 | Total Loss: 0.011976 | Recon Loss: 0.010357 | Commit Loss: 0.003238 | Perplexity: 867.427488
2025-10-12 02:30:51,998 Stage: Train 0.5 | Epoch: 8 | Iter: 24600 | Total Loss: 0.011715 | Recon Loss: 0.010081 | Commit Loss: 0.003267 | Perplexity: 868.279652
2025-10-12 02:36:16,607 Stage: Train 0.5 | Epoch: 8 | Iter: 24800 | Total Loss: 0.011897 | Recon Loss: 0.010220 | Commit Loss: 0.003356 | Perplexity: 868.138969
2025-10-12 02:41:41,128 Stage: Train 0.5 | Epoch: 8 | Iter: 25000 | Total Loss: 0.011862 | Recon Loss: 0.010196 | Commit Loss: 0.003331 | Perplexity: 873.575286
2025-10-12 02:47:05,201 Stage: Train 0.5 | Epoch: 8 | Iter: 25200 | Total Loss: 0.011697 | Recon Loss: 0.010051 | Commit Loss: 0.003292 | Perplexity: 879.567328
2025-10-12 02:52:29,266 Stage: Train 0.5 | Epoch: 8 | Iter: 25400 | Total Loss: 0.011624 | Recon Loss: 0.009958 | Commit Loss: 0.003331 | Perplexity: 877.501056
2025-10-12 02:57:53,285 Stage: Train 0.5 | Epoch: 8 | Iter: 25600 | Total Loss: 0.011999 | Recon Loss: 0.010342 | Commit Loss: 0.003315 | Perplexity: 875.820771
2025-10-12 03:03:17,251 Stage: Train 0.5 | Epoch: 8 | Iter: 25800 | Total Loss: 0.011496 | Recon Loss: 0.009857 | Commit Loss: 0.003279 | Perplexity: 884.328248
2025-10-12 03:08:41,410 Stage: Train 0.5 | Epoch: 8 | Iter: 26000 | Total Loss: 0.011820 | Recon Loss: 0.010158 | Commit Loss: 0.003325 | Perplexity: 888.131767
2025-10-12 03:14:05,532 Stage: Train 0.5 | Epoch: 8 | Iter: 26200 | Total Loss: 0.011497 | Recon Loss: 0.009851 | Commit Loss: 0.003291 | Perplexity: 876.259102
2025-10-12 03:19:29,605 Stage: Train 0.5 | Epoch: 8 | Iter: 26400 | Total Loss: 0.011724 | Recon Loss: 0.010093 | Commit Loss: 0.003262 | Perplexity: 884.435101
2025-10-12 03:24:53,214 Stage: Train 0.5 | Epoch: 8 | Iter: 26600 | Total Loss: 0.011570 | Recon Loss: 0.009897 | Commit Loss: 0.003347 | Perplexity: 879.673506
2025-10-12 03:30:17,362 Stage: Train 0.5 | Epoch: 8 | Iter: 26800 | Total Loss: 0.011513 | Recon Loss: 0.009865 | Commit Loss: 0.003296 | Perplexity: 884.152787
2025-10-12 03:35:41,530 Stage: Train 0.5 | Epoch: 8 | Iter: 27000 | Total Loss: 0.011499 | Recon Loss: 0.009868 | Commit Loss: 0.003262 | Perplexity: 883.128228
2025-10-12 03:41:05,429 Stage: Train 0.5 | Epoch: 8 | Iter: 27200 | Total Loss: 0.011612 | Recon Loss: 0.009943 | Commit Loss: 0.003339 | Perplexity: 885.940148
Trainning Epoch:   5%|▌         | 9/165 [12:19:54<213:41:52, 4931.49s/it]Trainning Epoch:   5%|▌         | 9/165 [12:19:54<213:41:53, 4931.50s/it]2025-10-12 03:46:33,854 Stage: Train 0.5 | Epoch: 9 | Iter: 27400 | Total Loss: 0.011458 | Recon Loss: 0.009805 | Commit Loss: 0.003306 | Perplexity: 894.786034
2025-10-12 03:51:57,875 Stage: Train 0.5 | Epoch: 9 | Iter: 27600 | Total Loss: 0.012687 | Recon Loss: 0.010626 | Commit Loss: 0.004122 | Perplexity: 894.821730
2025-10-12 03:57:21,968 Stage: Train 0.5 | Epoch: 9 | Iter: 27800 | Total Loss: 0.011190 | Recon Loss: 0.009551 | Commit Loss: 0.003277 | Perplexity: 883.035430
2025-10-12 04:02:46,155 Stage: Train 0.5 | Epoch: 9 | Iter: 28000 | Total Loss: 0.011275 | Recon Loss: 0.009656 | Commit Loss: 0.003239 | Perplexity: 894.889174
2025-10-12 04:08:10,511 Stage: Train 0.5 | Epoch: 9 | Iter: 28200 | Total Loss: 0.011185 | Recon Loss: 0.009575 | Commit Loss: 0.003219 | Perplexity: 896.049898
2025-10-12 04:13:34,669 Stage: Train 0.5 | Epoch: 9 | Iter: 28400 | Total Loss: 0.011361 | Recon Loss: 0.009720 | Commit Loss: 0.003283 | Perplexity: 896.371259
2025-10-12 04:18:58,633 Stage: Train 0.5 | Epoch: 9 | Iter: 28600 | Total Loss: 0.011508 | Recon Loss: 0.009907 | Commit Loss: 0.003204 | Perplexity: 899.508643
2025-10-12 04:24:22,591 Stage: Train 0.5 | Epoch: 9 | Iter: 28800 | Total Loss: 0.011463 | Recon Loss: 0.009845 | Commit Loss: 0.003237 | Perplexity: 905.356817
2025-10-12 04:29:46,397 Stage: Train 0.5 | Epoch: 9 | Iter: 29000 | Total Loss: 0.011165 | Recon Loss: 0.009558 | Commit Loss: 0.003214 | Perplexity: 909.425657
2025-10-12 04:35:10,533 Stage: Train 0.5 | Epoch: 9 | Iter: 29200 | Total Loss: 0.011162 | Recon Loss: 0.009550 | Commit Loss: 0.003225 | Perplexity: 912.197800
2025-10-12 04:40:34,553 Stage: Train 0.5 | Epoch: 9 | Iter: 29400 | Total Loss: 0.011106 | Recon Loss: 0.009489 | Commit Loss: 0.003235 | Perplexity: 909.654453
2025-10-12 04:45:58,723 Stage: Train 0.5 | Epoch: 9 | Iter: 29600 | Total Loss: 0.010978 | Recon Loss: 0.009390 | Commit Loss: 0.003175 | Perplexity: 915.024242
2025-10-12 04:51:22,617 Stage: Train 0.5 | Epoch: 9 | Iter: 29800 | Total Loss: 0.011073 | Recon Loss: 0.009466 | Commit Loss: 0.003214 | Perplexity: 914.023877
2025-10-12 04:56:46,654 Stage: Train 0.5 | Epoch: 9 | Iter: 30000 | Total Loss: 0.011286 | Recon Loss: 0.009685 | Commit Loss: 0.003203 | Perplexity: 906.256755
2025-10-12 05:02:10,803 Stage: Train 0.5 | Epoch: 9 | Iter: 30200 | Total Loss: 0.011199 | Recon Loss: 0.009560 | Commit Loss: 0.003278 | Perplexity: 916.442596
Trainning Epoch:   6%|▌         | 10/165 [13:42:01<212:16:00, 4930.07s/it]Trainning Epoch:   6%|▌         | 10/165 [13:42:01<212:16:00, 4930.07s/it]2025-10-12 05:07:39,300 Stage: Train 0.5 | Epoch: 10 | Iter: 30400 | Total Loss: 0.011187 | Recon Loss: 0.009530 | Commit Loss: 0.003315 | Perplexity: 910.068112
2025-10-12 05:13:03,439 Stage: Train 0.5 | Epoch: 10 | Iter: 30600 | Total Loss: 0.010943 | Recon Loss: 0.009370 | Commit Loss: 0.003147 | Perplexity: 917.424710
2025-10-12 05:18:27,691 Stage: Train 0.5 | Epoch: 10 | Iter: 30800 | Total Loss: 0.010903 | Recon Loss: 0.009306 | Commit Loss: 0.003192 | Perplexity: 911.510049
2025-10-12 05:23:51,573 Stage: Train 0.5 | Epoch: 10 | Iter: 31000 | Total Loss: 0.010858 | Recon Loss: 0.009271 | Commit Loss: 0.003174 | Perplexity: 921.399420
2025-10-12 05:29:15,490 Stage: Train 0.5 | Epoch: 10 | Iter: 31200 | Total Loss: 0.010909 | Recon Loss: 0.009297 | Commit Loss: 0.003223 | Perplexity: 922.451385
2025-10-12 05:34:39,521 Stage: Train 0.5 | Epoch: 10 | Iter: 31400 | Total Loss: 0.010929 | Recon Loss: 0.009303 | Commit Loss: 0.003252 | Perplexity: 923.425055
2025-10-12 05:40:03,233 Stage: Train 0.5 | Epoch: 10 | Iter: 31600 | Total Loss: 0.010681 | Recon Loss: 0.009088 | Commit Loss: 0.003186 | Perplexity: 925.856307
2025-10-12 05:45:27,164 Stage: Train 0.5 | Epoch: 10 | Iter: 31800 | Total Loss: 0.010818 | Recon Loss: 0.009238 | Commit Loss: 0.003159 | Perplexity: 935.009317
2025-10-12 05:50:51,283 Stage: Train 0.5 | Epoch: 10 | Iter: 32000 | Total Loss: 0.010784 | Recon Loss: 0.009203 | Commit Loss: 0.003160 | Perplexity: 925.501628
2025-10-12 05:56:15,213 Stage: Train 0.5 | Epoch: 10 | Iter: 32200 | Total Loss: 0.010685 | Recon Loss: 0.009084 | Commit Loss: 0.003202 | Perplexity: 936.934637
2025-10-12 06:01:39,358 Stage: Train 0.5 | Epoch: 10 | Iter: 32400 | Total Loss: 0.010863 | Recon Loss: 0.009280 | Commit Loss: 0.003166 | Perplexity: 933.185067
2025-10-12 06:07:03,410 Stage: Train 0.5 | Epoch: 10 | Iter: 32600 | Total Loss: 0.010875 | Recon Loss: 0.009260 | Commit Loss: 0.003231 | Perplexity: 934.457171
2025-10-12 06:12:27,720 Stage: Train 0.5 | Epoch: 10 | Iter: 32800 | Total Loss: 0.010890 | Recon Loss: 0.009270 | Commit Loss: 0.003239 | Perplexity: 934.388684
2025-10-12 06:17:51,803 Stage: Train 0.5 | Epoch: 10 | Iter: 33000 | Total Loss: 0.010696 | Recon Loss: 0.009101 | Commit Loss: 0.003190 | Perplexity: 935.764332
2025-10-12 06:23:15,913 Stage: Train 0.5 | Epoch: 10 | Iter: 33200 | Total Loss: 0.010798 | Recon Loss: 0.009244 | Commit Loss: 0.003107 | Perplexity: 940.935361
2025-10-12 06:28:39,839 Stage: Train 0.5 | Epoch: 10 | Iter: 33400 | Total Loss: 0.010623 | Recon Loss: 0.009023 | Commit Loss: 0.003200 | Perplexity: 942.547648
Trainning Epoch:   7%|▋         | 11/165 [15:04:08<210:51:04, 4928.99s/it]Trainning Epoch:   7%|▋         | 11/165 [15:04:07<210:51:04, 4928.99s/it]2025-10-12 06:34:07,942 Stage: Train 0.5 | Epoch: 11 | Iter: 33600 | Total Loss: 0.010627 | Recon Loss: 0.009063 | Commit Loss: 0.003129 | Perplexity: 939.907346
2025-10-12 06:39:32,336 Stage: Train 0.5 | Epoch: 11 | Iter: 33800 | Total Loss: 0.010725 | Recon Loss: 0.009163 | Commit Loss: 0.003124 | Perplexity: 934.847438
2025-10-12 06:44:56,447 Stage: Train 0.5 | Epoch: 11 | Iter: 34000 | Total Loss: 0.010696 | Recon Loss: 0.009141 | Commit Loss: 0.003111 | Perplexity: 945.022814
2025-10-12 06:50:20,678 Stage: Train 0.5 | Epoch: 11 | Iter: 34200 | Total Loss: 0.010445 | Recon Loss: 0.008870 | Commit Loss: 0.003151 | Perplexity: 948.322753
2025-10-12 06:55:44,752 Stage: Train 0.5 | Epoch: 11 | Iter: 34400 | Total Loss: 0.010447 | Recon Loss: 0.008901 | Commit Loss: 0.003093 | Perplexity: 955.043741
2025-10-12 07:01:08,898 Stage: Train 0.5 | Epoch: 11 | Iter: 34600 | Total Loss: 0.010576 | Recon Loss: 0.009003 | Commit Loss: 0.003146 | Perplexity: 948.172972
2025-10-12 07:06:32,674 Stage: Train 0.5 | Epoch: 11 | Iter: 34800 | Total Loss: 0.010579 | Recon Loss: 0.009014 | Commit Loss: 0.003129 | Perplexity: 947.460251
2025-10-12 07:11:56,622 Stage: Train 0.5 | Epoch: 11 | Iter: 35000 | Total Loss: 0.010270 | Recon Loss: 0.008754 | Commit Loss: 0.003033 | Perplexity: 959.486616
2025-10-12 07:17:20,260 Stage: Train 0.5 | Epoch: 11 | Iter: 35200 | Total Loss: 0.010501 | Recon Loss: 0.008960 | Commit Loss: 0.003083 | Perplexity: 947.995068
2025-10-12 07:22:44,241 Stage: Train 0.5 | Epoch: 11 | Iter: 35400 | Total Loss: 0.010221 | Recon Loss: 0.008682 | Commit Loss: 0.003079 | Perplexity: 965.632744
2025-10-12 07:28:08,234 Stage: Train 0.5 | Epoch: 11 | Iter: 35600 | Total Loss: 0.010170 | Recon Loss: 0.008676 | Commit Loss: 0.002988 | Perplexity: 963.591766
2025-10-12 07:33:32,142 Stage: Train 0.5 | Epoch: 11 | Iter: 35800 | Total Loss: 0.010363 | Recon Loss: 0.008813 | Commit Loss: 0.003099 | Perplexity: 966.052216
2025-10-12 07:38:55,920 Stage: Train 0.5 | Epoch: 11 | Iter: 36000 | Total Loss: 0.010404 | Recon Loss: 0.008869 | Commit Loss: 0.003070 | Perplexity: 963.876934
2025-10-12 07:44:19,677 Stage: Train 0.5 | Epoch: 11 | Iter: 36200 | Total Loss: 0.010261 | Recon Loss: 0.008759 | Commit Loss: 0.003005 | Perplexity: 970.297859
2025-10-12 07:49:43,654 Stage: Train 0.5 | Epoch: 11 | Iter: 36400 | Total Loss: 0.010161 | Recon Loss: 0.008645 | Commit Loss: 0.003033 | Perplexity: 966.379220
Trainning Epoch:   7%|▋         | 12/165 [16:26:13<209:26:07, 4927.89s/it]Trainning Epoch:   7%|▋         | 12/165 [16:26:13<209:26:07, 4927.89s/it]2025-10-12 07:55:12,290 Stage: Train 0.5 | Epoch: 12 | Iter: 36600 | Total Loss: 0.010211 | Recon Loss: 0.008718 | Commit Loss: 0.002985 | Perplexity: 969.150331
2025-10-12 08:00:36,420 Stage: Train 0.5 | Epoch: 12 | Iter: 36800 | Total Loss: 0.010291 | Recon Loss: 0.008796 | Commit Loss: 0.002991 | Perplexity: 964.463663
2025-10-12 08:06:00,620 Stage: Train 0.5 | Epoch: 12 | Iter: 37000 | Total Loss: 0.010038 | Recon Loss: 0.008576 | Commit Loss: 0.002924 | Perplexity: 965.018030
2025-10-12 08:11:24,660 Stage: Train 0.5 | Epoch: 12 | Iter: 37200 | Total Loss: 0.010020 | Recon Loss: 0.008536 | Commit Loss: 0.002968 | Perplexity: 976.734571
2025-10-12 08:16:49,649 Stage: Train 0.5 | Epoch: 12 | Iter: 37400 | Total Loss: 0.009985 | Recon Loss: 0.008527 | Commit Loss: 0.002917 | Perplexity: 983.751927
2025-10-12 08:22:14,789 Stage: Train 0.5 | Epoch: 12 | Iter: 37600 | Total Loss: 0.010138 | Recon Loss: 0.008639 | Commit Loss: 0.002999 | Perplexity: 975.654987
2025-10-12 08:27:39,214 Stage: Train 0.5 | Epoch: 12 | Iter: 37800 | Total Loss: 0.009928 | Recon Loss: 0.008435 | Commit Loss: 0.002985 | Perplexity: 979.495077
2025-10-12 08:33:03,328 Stage: Train 0.5 | Epoch: 12 | Iter: 38000 | Total Loss: 0.010018 | Recon Loss: 0.008543 | Commit Loss: 0.002950 | Perplexity: 982.821046
2025-10-12 08:38:27,606 Stage: Train 0.5 | Epoch: 12 | Iter: 38200 | Total Loss: 0.010078 | Recon Loss: 0.008634 | Commit Loss: 0.002889 | Perplexity: 981.483357
2025-10-12 08:43:51,850 Stage: Train 0.5 | Epoch: 12 | Iter: 38400 | Total Loss: 0.009836 | Recon Loss: 0.008421 | Commit Loss: 0.002830 | Perplexity: 983.939714
2025-10-12 08:49:17,283 Stage: Train 0.5 | Epoch: 12 | Iter: 38600 | Total Loss: 0.009709 | Recon Loss: 0.008279 | Commit Loss: 0.002859 | Perplexity: 991.255039
2025-10-12 08:54:42,374 Stage: Train 0.5 | Epoch: 12 | Iter: 38800 | Total Loss: 0.009736 | Recon Loss: 0.008306 | Commit Loss: 0.002860 | Perplexity: 994.694131
2025-10-12 09:00:07,284 Stage: Train 0.5 | Epoch: 12 | Iter: 39000 | Total Loss: 0.009940 | Recon Loss: 0.008511 | Commit Loss: 0.002857 | Perplexity: 996.916093
2025-10-12 09:05:32,188 Stage: Train 0.5 | Epoch: 12 | Iter: 39200 | Total Loss: 0.009956 | Recon Loss: 0.008529 | Commit Loss: 0.002852 | Perplexity: 988.223652
2025-10-12 09:10:57,700 Stage: Train 0.5 | Epoch: 12 | Iter: 39400 | Total Loss: 0.009707 | Recon Loss: 0.008306 | Commit Loss: 0.002803 | Perplexity: 991.993883
Trainning Epoch:   8%|▊         | 13/165 [17:48:30<208:10:44, 4930.55s/it]Trainning Epoch:   8%|▊         | 13/165 [17:48:30<208:10:44, 4930.55s/it]2025-10-12 09:16:28,178 Stage: Train 0.5 | Epoch: 13 | Iter: 39600 | Total Loss: 0.009979 | Recon Loss: 0.008552 | Commit Loss: 0.002856 | Perplexity: 995.492472
2025-10-12 09:21:53,943 Stage: Train 0.5 | Epoch: 13 | Iter: 39800 | Total Loss: 0.009523 | Recon Loss: 0.008160 | Commit Loss: 0.002726 | Perplexity: 995.345204
2025-10-12 09:27:18,031 Stage: Train 0.5 | Epoch: 13 | Iter: 40000 | Total Loss: 0.009514 | Recon Loss: 0.008139 | Commit Loss: 0.002749 | Perplexity: 998.043457
2025-10-12 09:27:18,031 Saving model at iteration 40000
2025-10-12 09:27:18,232 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000
2025-10-12 09:27:19,807 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000/model.safetensors
2025-10-12 09:27:21,659 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000/optimizer.bin
2025-10-12 09:27:21,660 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000/scheduler.bin
2025-10-12 09:27:21,660 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000/sampler.bin
2025-10-12 09:27:21,661 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_14_step_40000/random_states_0.pkl
2025-10-12 09:32:46,633 Stage: Train 0.5 | Epoch: 13 | Iter: 40200 | Total Loss: 0.009541 | Recon Loss: 0.008207 | Commit Loss: 0.002667 | Perplexity: 991.747930
2025-10-12 09:38:10,635 Stage: Train 0.5 | Epoch: 13 | Iter: 40400 | Total Loss: 0.009534 | Recon Loss: 0.008179 | Commit Loss: 0.002710 | Perplexity: 993.762186
2025-10-12 09:43:34,788 Stage: Train 0.5 | Epoch: 13 | Iter: 40600 | Total Loss: 0.009237 | Recon Loss: 0.007919 | Commit Loss: 0.002636 | Perplexity: 994.841356
2025-10-12 09:48:58,909 Stage: Train 0.5 | Epoch: 13 | Iter: 40800 | Total Loss: 0.009287 | Recon Loss: 0.007986 | Commit Loss: 0.002601 | Perplexity: 993.956210
2025-10-12 09:54:23,198 Stage: Train 0.5 | Epoch: 13 | Iter: 41000 | Total Loss: 0.009498 | Recon Loss: 0.008197 | Commit Loss: 0.002601 | Perplexity: 1001.784789
2025-10-12 09:59:47,611 Stage: Train 0.5 | Epoch: 13 | Iter: 41200 | Total Loss: 0.009072 | Recon Loss: 0.007844 | Commit Loss: 0.002455 | Perplexity: 1005.716415
2025-10-12 10:05:11,414 Stage: Train 0.5 | Epoch: 13 | Iter: 41400 | Total Loss: 0.009187 | Recon Loss: 0.007919 | Commit Loss: 0.002535 | Perplexity: 1010.569946
2025-10-12 10:10:35,539 Stage: Train 0.5 | Epoch: 13 | Iter: 41600 | Total Loss: 0.009054 | Recon Loss: 0.007818 | Commit Loss: 0.002471 | Perplexity: 1007.123999
2025-10-12 10:15:59,612 Stage: Train 0.5 | Epoch: 13 | Iter: 41800 | Total Loss: 0.008993 | Recon Loss: 0.007782 | Commit Loss: 0.002421 | Perplexity: 1008.250452
2025-10-12 10:21:24,152 Stage: Train 0.5 | Epoch: 13 | Iter: 42000 | Total Loss: 0.009136 | Recon Loss: 0.007914 | Commit Loss: 0.002444 | Perplexity: 1001.417374
2025-10-12 10:26:48,582 Stage: Train 0.5 | Epoch: 13 | Iter: 42200 | Total Loss: 0.008739 | Recon Loss: 0.007570 | Commit Loss: 0.002339 | Perplexity: 1016.294036
2025-10-12 10:32:13,119 Stage: Train 0.5 | Epoch: 13 | Iter: 42400 | Total Loss: 0.008826 | Recon Loss: 0.007648 | Commit Loss: 0.002356 | Perplexity: 1001.743972
Trainning Epoch:   8%|▊         | 14/165 [19:10:46<206:52:40, 4932.19s/it]Trainning Epoch:   8%|▊         | 14/165 [19:10:46<206:52:40, 4932.19s/it]2025-10-12 10:37:42,189 Stage: Train 0.5 | Epoch: 14 | Iter: 42600 | Total Loss: 0.008616 | Recon Loss: 0.007474 | Commit Loss: 0.002284 | Perplexity: 1004.599781
2025-10-12 10:43:06,958 Stage: Train 0.5 | Epoch: 14 | Iter: 42800 | Total Loss: 0.008701 | Recon Loss: 0.007587 | Commit Loss: 0.002229 | Perplexity: 1005.967592
2025-10-12 10:48:31,830 Stage: Train 0.5 | Epoch: 14 | Iter: 43000 | Total Loss: 0.008578 | Recon Loss: 0.007491 | Commit Loss: 0.002175 | Perplexity: 1010.848734
2025-10-12 10:53:56,400 Stage: Train 0.5 | Epoch: 14 | Iter: 43200 | Total Loss: 0.008224 | Recon Loss: 0.007184 | Commit Loss: 0.002080 | Perplexity: 1009.231766
2025-10-12 10:59:20,844 Stage: Train 0.5 | Epoch: 14 | Iter: 43400 | Total Loss: 0.008356 | Recon Loss: 0.007307 | Commit Loss: 0.002097 | Perplexity: 1014.247245
2025-10-12 11:04:45,370 Stage: Train 0.5 | Epoch: 14 | Iter: 43600 | Total Loss: 0.008426 | Recon Loss: 0.007391 | Commit Loss: 0.002070 | Perplexity: 1013.078692
2025-10-12 11:10:09,788 Stage: Train 0.5 | Epoch: 14 | Iter: 43800 | Total Loss: 0.008269 | Recon Loss: 0.007268 | Commit Loss: 0.002003 | Perplexity: 1017.692145
2025-10-12 11:15:34,199 Stage: Train 0.5 | Epoch: 14 | Iter: 44000 | Total Loss: 0.008292 | Recon Loss: 0.007330 | Commit Loss: 0.001924 | Perplexity: 1016.253676
2025-10-12 11:20:58,729 Stage: Train 0.5 | Epoch: 14 | Iter: 44200 | Total Loss: 0.008051 | Recon Loss: 0.007100 | Commit Loss: 0.001900 | Perplexity: 1016.607115
2025-10-12 11:26:23,682 Stage: Train 0.5 | Epoch: 14 | Iter: 44400 | Total Loss: 0.008109 | Recon Loss: 0.007180 | Commit Loss: 0.001857 | Perplexity: 1020.023607
2025-10-12 11:31:48,614 Stage: Train 0.5 | Epoch: 14 | Iter: 44600 | Total Loss: 0.008240 | Recon Loss: 0.007238 | Commit Loss: 0.002005 | Perplexity: 1019.987440
2025-10-12 11:37:13,130 Stage: Train 0.5 | Epoch: 14 | Iter: 44800 | Total Loss: 0.007979 | Recon Loss: 0.007092 | Commit Loss: 0.001774 | Perplexity: 1013.149670
2025-10-12 11:42:37,612 Stage: Train 0.5 | Epoch: 14 | Iter: 45000 | Total Loss: 0.007820 | Recon Loss: 0.006954 | Commit Loss: 0.001732 | Perplexity: 1010.680946
2025-10-12 11:48:02,413 Stage: Train 0.5 | Epoch: 14 | Iter: 45200 | Total Loss: 0.007709 | Recon Loss: 0.006862 | Commit Loss: 0.001694 | Perplexity: 1020.941512
2025-10-12 11:53:26,968 Stage: Train 0.5 | Epoch: 14 | Iter: 45400 | Total Loss: 0.007631 | Recon Loss: 0.006801 | Commit Loss: 0.001659 | Perplexity: 1027.258524
Trainning Epoch:   9%|▉         | 15/165 [20:33:01<205:33:08, 4933.26s/it]Trainning Epoch:   9%|▉         | 15/165 [20:33:01<205:33:08, 4933.26s/it]2025-10-12 11:58:55,877 Stage: Train 0.5 | Epoch: 15 | Iter: 45600 | Total Loss: 0.007529 | Recon Loss: 0.006736 | Commit Loss: 0.001586 | Perplexity: 1029.808795
2025-10-12 12:04:20,812 Stage: Train 0.5 | Epoch: 15 | Iter: 45800 | Total Loss: 0.007529 | Recon Loss: 0.006740 | Commit Loss: 0.001577 | Perplexity: 1029.237913
2025-10-12 12:09:45,740 Stage: Train 0.5 | Epoch: 15 | Iter: 46000 | Total Loss: 0.007350 | Recon Loss: 0.006572 | Commit Loss: 0.001555 | Perplexity: 1033.063652
2025-10-12 12:15:10,672 Stage: Train 0.5 | Epoch: 15 | Iter: 46200 | Total Loss: 0.007338 | Recon Loss: 0.006571 | Commit Loss: 0.001533 | Perplexity: 1045.744189
2025-10-12 12:20:35,506 Stage: Train 0.5 | Epoch: 15 | Iter: 46400 | Total Loss: 0.007458 | Recon Loss: 0.006717 | Commit Loss: 0.001481 | Perplexity: 1037.624430
2025-10-12 12:26:00,124 Stage: Train 0.5 | Epoch: 15 | Iter: 46600 | Total Loss: 0.007390 | Recon Loss: 0.006645 | Commit Loss: 0.001490 | Perplexity: 1049.308178
2025-10-12 12:31:25,051 Stage: Train 0.5 | Epoch: 15 | Iter: 46800 | Total Loss: 0.007061 | Recon Loss: 0.006356 | Commit Loss: 0.001408 | Perplexity: 1043.793153
2025-10-12 12:36:49,583 Stage: Train 0.5 | Epoch: 15 | Iter: 47000 | Total Loss: 0.007307 | Recon Loss: 0.006593 | Commit Loss: 0.001429 | Perplexity: 1057.214431
2025-10-12 12:42:14,062 Stage: Train 0.5 | Epoch: 15 | Iter: 47200 | Total Loss: 0.007341 | Recon Loss: 0.006637 | Commit Loss: 0.001408 | Perplexity: 1060.349361
2025-10-12 12:47:38,564 Stage: Train 0.5 | Epoch: 15 | Iter: 47400 | Total Loss: 0.007066 | Recon Loss: 0.006375 | Commit Loss: 0.001382 | Perplexity: 1062.939646
2025-10-12 12:53:02,749 Stage: Train 0.5 | Epoch: 15 | Iter: 47600 | Total Loss: 0.007036 | Recon Loss: 0.006351 | Commit Loss: 0.001371 | Perplexity: 1077.486565
2025-10-12 12:58:26,992 Stage: Train 0.5 | Epoch: 15 | Iter: 47800 | Total Loss: 0.007033 | Recon Loss: 0.006357 | Commit Loss: 0.001351 | Perplexity: 1079.098878
2025-10-12 13:03:51,355 Stage: Train 0.5 | Epoch: 15 | Iter: 48000 | Total Loss: 0.006874 | Recon Loss: 0.006222 | Commit Loss: 0.001305 | Perplexity: 1077.542970
2025-10-12 13:09:15,322 Stage: Train 0.5 | Epoch: 15 | Iter: 48200 | Total Loss: 0.007182 | Recon Loss: 0.006535 | Commit Loss: 0.001295 | Perplexity: 1077.213787
2025-10-12 13:14:39,250 Stage: Train 0.5 | Epoch: 15 | Iter: 48400 | Total Loss: 0.007872 | Recon Loss: 0.006947 | Commit Loss: 0.001850 | Perplexity: 1083.123388
2025-10-12 13:20:03,197 Stage: Train 0.5 | Epoch: 15 | Iter: 48600 | Total Loss: 0.006899 | Recon Loss: 0.006260 | Commit Loss: 0.001277 | Perplexity: 1084.879179
Trainning Epoch:  10%|▉         | 16/165 [21:55:15<204:11:04, 4933.32s/it]Trainning Epoch:  10%|▉         | 16/165 [21:55:15<204:11:05, 4933.32s/it]2025-10-12 13:25:31,537 Stage: Train 0.5 | Epoch: 16 | Iter: 48800 | Total Loss: 0.006820 | Recon Loss: 0.006170 | Commit Loss: 0.001300 | Perplexity: 1087.228726
2025-10-12 13:30:55,681 Stage: Train 0.5 | Epoch: 16 | Iter: 49000 | Total Loss: 0.006768 | Recon Loss: 0.006102 | Commit Loss: 0.001332 | Perplexity: 1079.030476
2025-10-12 13:36:20,178 Stage: Train 0.5 | Epoch: 16 | Iter: 49200 | Total Loss: 0.006810 | Recon Loss: 0.006161 | Commit Loss: 0.001297 | Perplexity: 1080.706726
2025-10-12 13:41:44,305 Stage: Train 0.5 | Epoch: 16 | Iter: 49400 | Total Loss: 0.006658 | Recon Loss: 0.006006 | Commit Loss: 0.001304 | Perplexity: 1096.513015
2025-10-12 13:47:08,458 Stage: Train 0.5 | Epoch: 16 | Iter: 49600 | Total Loss: 0.006631 | Recon Loss: 0.005995 | Commit Loss: 0.001271 | Perplexity: 1095.584638
2025-10-12 13:52:32,823 Stage: Train 0.5 | Epoch: 16 | Iter: 49800 | Total Loss: 0.006760 | Recon Loss: 0.006126 | Commit Loss: 0.001267 | Perplexity: 1091.165475
2025-10-12 13:57:56,860 Stage: Train 0.5 | Epoch: 16 | Iter: 50000 | Total Loss: 0.006785 | Recon Loss: 0.006142 | Commit Loss: 0.001286 | Perplexity: 1092.146035
2025-10-12 14:03:20,865 Stage: Train 0.5 | Epoch: 16 | Iter: 50200 | Total Loss: 0.006684 | Recon Loss: 0.006055 | Commit Loss: 0.001258 | Perplexity: 1093.846314
2025-10-12 14:08:45,203 Stage: Train 0.5 | Epoch: 16 | Iter: 50400 | Total Loss: 0.006680 | Recon Loss: 0.006056 | Commit Loss: 0.001248 | Perplexity: 1098.361378
2025-10-12 14:14:09,386 Stage: Train 0.5 | Epoch: 16 | Iter: 50600 | Total Loss: 0.006864 | Recon Loss: 0.006230 | Commit Loss: 0.001269 | Perplexity: 1100.458593
2025-10-12 14:19:33,330 Stage: Train 0.5 | Epoch: 16 | Iter: 50800 | Total Loss: 0.006619 | Recon Loss: 0.006012 | Commit Loss: 0.001213 | Perplexity: 1099.012477
2025-10-12 14:24:57,403 Stage: Train 0.5 | Epoch: 16 | Iter: 51000 | Total Loss: 0.006679 | Recon Loss: 0.006050 | Commit Loss: 0.001258 | Perplexity: 1099.177406
2025-10-12 14:30:21,846 Stage: Train 0.5 | Epoch: 16 | Iter: 51200 | Total Loss: 0.006740 | Recon Loss: 0.006111 | Commit Loss: 0.001259 | Perplexity: 1100.270877
2025-10-12 14:35:45,997 Stage: Train 0.5 | Epoch: 16 | Iter: 51400 | Total Loss: 0.006505 | Recon Loss: 0.005887 | Commit Loss: 0.001236 | Perplexity: 1102.711698
2025-10-12 14:41:10,346 Stage: Train 0.5 | Epoch: 16 | Iter: 51600 | Total Loss: 0.006701 | Recon Loss: 0.006089 | Commit Loss: 0.001224 | Perplexity: 1101.466743
Trainning Epoch:  10%|█         | 17/165 [23:17:24<202:45:27, 4931.95s/it]Trainning Epoch:  10%|█         | 17/165 [23:17:23<202:45:28, 4931.95s/it]2025-10-12 14:46:39,262 Stage: Train 0.5 | Epoch: 17 | Iter: 51800 | Total Loss: 0.006649 | Recon Loss: 0.006022 | Commit Loss: 0.001253 | Perplexity: 1100.203422
2025-10-12 14:52:04,022 Stage: Train 0.5 | Epoch: 17 | Iter: 52000 | Total Loss: 0.006628 | Recon Loss: 0.006012 | Commit Loss: 0.001233 | Perplexity: 1102.617813
2025-10-12 14:57:28,520 Stage: Train 0.5 | Epoch: 17 | Iter: 52200 | Total Loss: 0.006473 | Recon Loss: 0.005852 | Commit Loss: 0.001243 | Perplexity: 1106.253405
2025-10-12 15:02:53,735 Stage: Train 0.5 | Epoch: 17 | Iter: 52400 | Total Loss: 0.006684 | Recon Loss: 0.006093 | Commit Loss: 0.001180 | Perplexity: 1095.412971
2025-10-12 15:08:19,419 Stage: Train 0.5 | Epoch: 17 | Iter: 52600 | Total Loss: 0.006659 | Recon Loss: 0.006063 | Commit Loss: 0.001192 | Perplexity: 1110.463746
2025-10-12 15:13:43,475 Stage: Train 0.5 | Epoch: 17 | Iter: 52800 | Total Loss: 0.006486 | Recon Loss: 0.005870 | Commit Loss: 0.001232 | Perplexity: 1104.056655
2025-10-12 15:19:07,364 Stage: Train 0.5 | Epoch: 17 | Iter: 53000 | Total Loss: 0.006358 | Recon Loss: 0.005744 | Commit Loss: 0.001227 | Perplexity: 1100.996907
2025-10-12 15:24:31,546 Stage: Train 0.5 | Epoch: 17 | Iter: 53200 | Total Loss: 0.006876 | Recon Loss: 0.006201 | Commit Loss: 0.001350 | Perplexity: 1121.674025
2025-10-12 15:29:56,150 Stage: Train 0.5 | Epoch: 17 | Iter: 53400 | Total Loss: 0.006435 | Recon Loss: 0.005828 | Commit Loss: 0.001214 | Perplexity: 1100.224264
2025-10-12 15:35:20,882 Stage: Train 0.5 | Epoch: 17 | Iter: 53600 | Total Loss: 0.006378 | Recon Loss: 0.005776 | Commit Loss: 0.001203 | Perplexity: 1109.636246
2025-10-12 15:40:45,708 Stage: Train 0.5 | Epoch: 17 | Iter: 53800 | Total Loss: 0.006616 | Recon Loss: 0.006009 | Commit Loss: 0.001213 | Perplexity: 1102.764361
2025-10-12 15:46:10,216 Stage: Train 0.5 | Epoch: 17 | Iter: 54000 | Total Loss: 0.006520 | Recon Loss: 0.005902 | Commit Loss: 0.001237 | Perplexity: 1110.618522
2025-10-12 15:51:35,321 Stage: Train 0.5 | Epoch: 17 | Iter: 54200 | Total Loss: 0.006619 | Recon Loss: 0.005998 | Commit Loss: 0.001242 | Perplexity: 1105.343205
2025-10-12 15:56:59,601 Stage: Train 0.5 | Epoch: 17 | Iter: 54400 | Total Loss: 0.006392 | Recon Loss: 0.005801 | Commit Loss: 0.001182 | Perplexity: 1101.033157
2025-10-12 16:02:24,100 Stage: Train 0.5 | Epoch: 17 | Iter: 54600 | Total Loss: 0.006455 | Recon Loss: 0.005838 | Commit Loss: 0.001233 | Perplexity: 1111.336219
Trainning Epoch:  11%|█         | 18/165 [24:39:39<201:25:45, 4932.96s/it]Trainning Epoch:  11%|█         | 18/165 [24:39:39<201:25:45, 4932.96s/it]2025-10-12 16:07:53,177 Stage: Train 0.5 | Epoch: 18 | Iter: 54800 | Total Loss: 0.006465 | Recon Loss: 0.005856 | Commit Loss: 0.001217 | Perplexity: 1111.432221
2025-10-12 16:13:17,078 Stage: Train 0.5 | Epoch: 18 | Iter: 55000 | Total Loss: 0.006470 | Recon Loss: 0.005866 | Commit Loss: 0.001209 | Perplexity: 1111.675259
2025-10-12 16:18:41,588 Stage: Train 0.5 | Epoch: 18 | Iter: 55200 | Total Loss: 0.006598 | Recon Loss: 0.005988 | Commit Loss: 0.001221 | Perplexity: 1106.154604
2025-10-12 16:24:06,365 Stage: Train 0.5 | Epoch: 18 | Iter: 55400 | Total Loss: 0.006363 | Recon Loss: 0.005762 | Commit Loss: 0.001202 | Perplexity: 1110.856925
2025-10-12 16:29:30,558 Stage: Train 0.5 | Epoch: 18 | Iter: 55600 | Total Loss: 0.006333 | Recon Loss: 0.005741 | Commit Loss: 0.001183 | Perplexity: 1113.341507
2025-10-12 16:34:54,348 Stage: Train 0.5 | Epoch: 18 | Iter: 55800 | Total Loss: 0.006287 | Recon Loss: 0.005695 | Commit Loss: 0.001186 | Perplexity: 1110.044016
2025-10-12 16:40:18,133 Stage: Train 0.5 | Epoch: 18 | Iter: 56000 | Total Loss: 0.006319 | Recon Loss: 0.005724 | Commit Loss: 0.001192 | Perplexity: 1111.078563
2025-10-12 16:45:41,920 Stage: Train 0.5 | Epoch: 18 | Iter: 56200 | Total Loss: 0.006376 | Recon Loss: 0.005775 | Commit Loss: 0.001202 | Perplexity: 1114.360685
2025-10-12 16:51:06,111 Stage: Train 0.5 | Epoch: 18 | Iter: 56400 | Total Loss: 0.006272 | Recon Loss: 0.005661 | Commit Loss: 0.001223 | Perplexity: 1106.004001
2025-10-12 16:56:30,184 Stage: Train 0.5 | Epoch: 18 | Iter: 56600 | Total Loss: 0.006318 | Recon Loss: 0.005710 | Commit Loss: 0.001216 | Perplexity: 1113.143130
2025-10-12 17:01:54,468 Stage: Train 0.5 | Epoch: 18 | Iter: 56800 | Total Loss: 0.006255 | Recon Loss: 0.005651 | Commit Loss: 0.001208 | Perplexity: 1109.811840
2025-10-12 17:07:18,449 Stage: Train 0.5 | Epoch: 18 | Iter: 57000 | Total Loss: 0.006419 | Recon Loss: 0.005821 | Commit Loss: 0.001196 | Perplexity: 1115.283921
2025-10-12 17:12:42,608 Stage: Train 0.5 | Epoch: 18 | Iter: 57200 | Total Loss: 0.006407 | Recon Loss: 0.005807 | Commit Loss: 0.001199 | Perplexity: 1104.387215
2025-10-12 17:18:06,583 Stage: Train 0.5 | Epoch: 18 | Iter: 57400 | Total Loss: 0.006163 | Recon Loss: 0.005579 | Commit Loss: 0.001169 | Perplexity: 1113.488991
2025-10-12 17:23:30,798 Stage: Train 0.5 | Epoch: 18 | Iter: 57600 | Total Loss: 0.006320 | Recon Loss: 0.005725 | Commit Loss: 0.001190 | Perplexity: 1108.373653
Trainning Epoch:  12%|█▏        | 19/165 [26:01:47<200:00:03, 4931.53s/it]Trainning Epoch:  12%|█▏        | 19/165 [26:01:47<200:00:03, 4931.53s/it]2025-10-12 17:28:59,324 Stage: Train 0.5 | Epoch: 19 | Iter: 57800 | Total Loss: 0.006292 | Recon Loss: 0.005691 | Commit Loss: 0.001203 | Perplexity: 1112.424359
2025-10-12 17:34:23,511 Stage: Train 0.5 | Epoch: 19 | Iter: 58000 | Total Loss: 0.006415 | Recon Loss: 0.005810 | Commit Loss: 0.001210 | Perplexity: 1117.059557
2025-10-12 17:39:47,281 Stage: Train 0.5 | Epoch: 19 | Iter: 58200 | Total Loss: 0.006165 | Recon Loss: 0.005551 | Commit Loss: 0.001228 | Perplexity: 1112.955112
2025-10-12 17:45:11,325 Stage: Train 0.5 | Epoch: 19 | Iter: 58400 | Total Loss: 0.006294 | Recon Loss: 0.005698 | Commit Loss: 0.001192 | Perplexity: 1115.702615
2025-10-12 17:50:35,579 Stage: Train 0.5 | Epoch: 19 | Iter: 58600 | Total Loss: 0.006097 | Recon Loss: 0.005491 | Commit Loss: 0.001213 | Perplexity: 1114.291471
2025-10-12 17:55:59,530 Stage: Train 0.5 | Epoch: 19 | Iter: 58800 | Total Loss: 0.006192 | Recon Loss: 0.005597 | Commit Loss: 0.001191 | Perplexity: 1114.821176
2025-10-12 18:01:23,737 Stage: Train 0.5 | Epoch: 19 | Iter: 59000 | Total Loss: 0.006323 | Recon Loss: 0.005709 | Commit Loss: 0.001227 | Perplexity: 1111.007079
2025-10-12 18:06:48,233 Stage: Train 0.5 | Epoch: 19 | Iter: 59200 | Total Loss: 0.006201 | Recon Loss: 0.005609 | Commit Loss: 0.001184 | Perplexity: 1105.134149
2025-10-12 18:12:12,372 Stage: Train 0.5 | Epoch: 19 | Iter: 59400 | Total Loss: 0.006230 | Recon Loss: 0.005636 | Commit Loss: 0.001188 | Perplexity: 1105.247745
2025-10-12 18:17:36,502 Stage: Train 0.5 | Epoch: 19 | Iter: 59600 | Total Loss: 0.006338 | Recon Loss: 0.005740 | Commit Loss: 0.001197 | Perplexity: 1112.882128
2025-10-12 18:23:01,319 Stage: Train 0.5 | Epoch: 19 | Iter: 59800 | Total Loss: 0.006065 | Recon Loss: 0.005477 | Commit Loss: 0.001175 | Perplexity: 1112.655222
2025-10-12 18:28:26,485 Stage: Train 0.5 | Epoch: 19 | Iter: 60000 | Total Loss: 0.006309 | Recon Loss: 0.005718 | Commit Loss: 0.001182 | Perplexity: 1108.941263
2025-10-12 18:28:26,486 Saving model at iteration 60000
2025-10-12 18:28:26,850 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000
2025-10-12 18:28:28,380 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/model.safetensors
2025-10-12 18:28:30,330 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/optimizer.bin
2025-10-12 18:28:30,330 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/scheduler.bin
2025-10-12 18:28:30,330 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/sampler.bin
2025-10-12 18:28:30,331 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_20_step_60000/random_states_0.pkl
2025-10-12 18:33:55,664 Stage: Train 0.5 | Epoch: 19 | Iter: 60200 | Total Loss: 0.006050 | Recon Loss: 0.005457 | Commit Loss: 0.001186 | Perplexity: 1120.018316
2025-10-12 18:39:19,721 Stage: Train 0.5 | Epoch: 19 | Iter: 60400 | Total Loss: 0.005980 | Recon Loss: 0.005390 | Commit Loss: 0.001180 | Perplexity: 1112.989722
2025-10-12 18:44:44,033 Stage: Train 0.5 | Epoch: 19 | Iter: 60600 | Total Loss: 0.007325 | Recon Loss: 0.006341 | Commit Loss: 0.001968 | Perplexity: 1103.106838
Trainning Epoch:  12%|█▏        | 20/165 [27:24:02<198:40:19, 4932.55s/it]Trainning Epoch:  12%|█▏        | 20/165 [27:24:02<198:40:20, 4932.55s/it]2025-10-12 18:50:12,827 Stage: Train 0.5 | Epoch: 20 | Iter: 60800 | Total Loss: 0.005997 | Recon Loss: 0.005428 | Commit Loss: 0.001139 | Perplexity: 1096.304420
2025-10-12 18:55:36,714 Stage: Train 0.5 | Epoch: 20 | Iter: 61000 | Total Loss: 0.006210 | Recon Loss: 0.005536 | Commit Loss: 0.001348 | Perplexity: 1109.533339
2025-10-12 19:01:00,830 Stage: Train 0.5 | Epoch: 20 | Iter: 61200 | Total Loss: 0.006010 | Recon Loss: 0.005415 | Commit Loss: 0.001189 | Perplexity: 1104.254738
2025-10-12 19:06:25,278 Stage: Train 0.5 | Epoch: 20 | Iter: 61400 | Total Loss: 0.006021 | Recon Loss: 0.005440 | Commit Loss: 0.001163 | Perplexity: 1112.807139
2025-10-12 19:11:49,488 Stage: Train 0.5 | Epoch: 20 | Iter: 61600 | Total Loss: 0.006107 | Recon Loss: 0.005520 | Commit Loss: 0.001175 | Perplexity: 1114.998419
2025-10-12 19:17:13,247 Stage: Train 0.5 | Epoch: 20 | Iter: 61800 | Total Loss: 0.006034 | Recon Loss: 0.005439 | Commit Loss: 0.001191 | Perplexity: 1117.814442
2025-10-12 19:22:37,094 Stage: Train 0.5 | Epoch: 20 | Iter: 62000 | Total Loss: 0.006107 | Recon Loss: 0.005498 | Commit Loss: 0.001218 | Perplexity: 1113.994485
2025-10-12 19:28:01,047 Stage: Train 0.5 | Epoch: 20 | Iter: 62200 | Total Loss: 0.006089 | Recon Loss: 0.005518 | Commit Loss: 0.001141 | Perplexity: 1112.605475
2025-10-12 19:33:25,112 Stage: Train 0.5 | Epoch: 20 | Iter: 62400 | Total Loss: 0.006127 | Recon Loss: 0.005550 | Commit Loss: 0.001153 | Perplexity: 1112.467016
2025-10-12 19:38:49,002 Stage: Train 0.5 | Epoch: 20 | Iter: 62600 | Total Loss: 0.006168 | Recon Loss: 0.005577 | Commit Loss: 0.001181 | Perplexity: 1115.159942
2025-10-12 19:44:12,842 Stage: Train 0.5 | Epoch: 20 | Iter: 62800 | Total Loss: 0.006001 | Recon Loss: 0.005405 | Commit Loss: 0.001193 | Perplexity: 1099.945285
2025-10-12 19:49:36,912 Stage: Train 0.5 | Epoch: 20 | Iter: 63000 | Total Loss: 0.005910 | Recon Loss: 0.005306 | Commit Loss: 0.001208 | Perplexity: 1117.319650
2025-10-12 19:55:00,867 Stage: Train 0.5 | Epoch: 20 | Iter: 63200 | Total Loss: 0.006009 | Recon Loss: 0.005416 | Commit Loss: 0.001185 | Perplexity: 1119.008083
2025-10-12 20:00:24,960 Stage: Train 0.5 | Epoch: 20 | Iter: 63400 | Total Loss: 0.005957 | Recon Loss: 0.005371 | Commit Loss: 0.001171 | Perplexity: 1116.681802
2025-10-12 20:05:49,273 Stage: Train 0.5 | Epoch: 20 | Iter: 63600 | Total Loss: 0.006161 | Recon Loss: 0.005570 | Commit Loss: 0.001183 | Perplexity: 1113.729502
Trainning Epoch:  13%|█▎        | 21/165 [28:46:09<197:13:57, 4930.82s/it]Trainning Epoch:  13%|█▎        | 21/165 [28:46:09<197:13:57, 4930.82s/it]2025-10-12 20:11:17,779 Stage: Train 0.5 | Epoch: 21 | Iter: 63800 | Total Loss: 0.005876 | Recon Loss: 0.005278 | Commit Loss: 0.001197 | Perplexity: 1119.052418
2025-10-12 20:16:42,071 Stage: Train 0.5 | Epoch: 21 | Iter: 64000 | Total Loss: 0.005882 | Recon Loss: 0.005298 | Commit Loss: 0.001168 | Perplexity: 1120.478316
2025-10-12 20:22:06,141 Stage: Train 0.5 | Epoch: 21 | Iter: 64200 | Total Loss: 0.005960 | Recon Loss: 0.005379 | Commit Loss: 0.001162 | Perplexity: 1111.528558
2025-10-12 20:27:30,143 Stage: Train 0.5 | Epoch: 21 | Iter: 64400 | Total Loss: 0.005897 | Recon Loss: 0.005296 | Commit Loss: 0.001203 | Perplexity: 1108.142693
2025-10-12 20:32:53,972 Stage: Train 0.5 | Epoch: 21 | Iter: 64600 | Total Loss: 0.005948 | Recon Loss: 0.005366 | Commit Loss: 0.001165 | Perplexity: 1113.276859
2025-10-12 20:38:18,050 Stage: Train 0.5 | Epoch: 21 | Iter: 64800 | Total Loss: 0.006036 | Recon Loss: 0.005448 | Commit Loss: 0.001175 | Perplexity: 1115.873501
2025-10-12 20:43:41,986 Stage: Train 0.5 | Epoch: 21 | Iter: 65000 | Total Loss: 0.006027 | Recon Loss: 0.005426 | Commit Loss: 0.001202 | Perplexity: 1111.590477
2025-10-12 20:49:05,875 Stage: Train 0.5 | Epoch: 21 | Iter: 65200 | Total Loss: 0.005835 | Recon Loss: 0.005235 | Commit Loss: 0.001199 | Perplexity: 1110.160328
2025-10-12 20:54:29,802 Stage: Train 0.5 | Epoch: 21 | Iter: 65400 | Total Loss: 0.005929 | Recon Loss: 0.005336 | Commit Loss: 0.001185 | Perplexity: 1108.421132
2025-10-12 20:59:53,588 Stage: Train 0.5 | Epoch: 21 | Iter: 65600 | Total Loss: 0.005948 | Recon Loss: 0.005356 | Commit Loss: 0.001184 | Perplexity: 1109.577746
2025-10-12 21:05:18,089 Stage: Train 0.5 | Epoch: 21 | Iter: 65800 | Total Loss: 0.006124 | Recon Loss: 0.005523 | Commit Loss: 0.001202 | Perplexity: 1108.622355
2025-10-12 21:10:42,159 Stage: Train 0.5 | Epoch: 21 | Iter: 66000 | Total Loss: 0.006320 | Recon Loss: 0.005551 | Commit Loss: 0.001539 | Perplexity: 1116.387130
2025-10-12 21:16:06,413 Stage: Train 0.5 | Epoch: 21 | Iter: 66200 | Total Loss: 0.005790 | Recon Loss: 0.005217 | Commit Loss: 0.001147 | Perplexity: 1101.143598
2025-10-12 21:21:30,399 Stage: Train 0.5 | Epoch: 21 | Iter: 66400 | Total Loss: 0.005796 | Recon Loss: 0.005210 | Commit Loss: 0.001173 | Perplexity: 1107.353390
2025-10-12 21:26:54,599 Stage: Train 0.5 | Epoch: 21 | Iter: 66600 | Total Loss: 0.005886 | Recon Loss: 0.005298 | Commit Loss: 0.001176 | Perplexity: 1122.436169
2025-10-12 21:32:18,283 Stage: Train 0.5 | Epoch: 21 | Iter: 66800 | Total Loss: 0.006005 | Recon Loss: 0.005422 | Commit Loss: 0.001166 | Perplexity: 1111.542267
Trainning Epoch:  13%|█▎        | 22/165 [30:08:15<195:48:36, 4929.48s/it]Trainning Epoch:  13%|█▎        | 22/165 [30:08:15<195:48:36, 4929.48s/it]2025-10-12 21:37:46,692 Stage: Train 0.5 | Epoch: 22 | Iter: 67000 | Total Loss: 0.005982 | Recon Loss: 0.005401 | Commit Loss: 0.001163 | Perplexity: 1113.879592
2025-10-12 21:43:10,464 Stage: Train 0.5 | Epoch: 22 | Iter: 67200 | Total Loss: 0.005887 | Recon Loss: 0.005311 | Commit Loss: 0.001152 | Perplexity: 1118.905749
2025-10-12 21:48:34,595 Stage: Train 0.5 | Epoch: 22 | Iter: 67400 | Total Loss: 0.005863 | Recon Loss: 0.005270 | Commit Loss: 0.001187 | Perplexity: 1114.967536
2025-10-12 21:53:59,035 Stage: Train 0.5 | Epoch: 22 | Iter: 67600 | Total Loss: 0.005894 | Recon Loss: 0.005304 | Commit Loss: 0.001179 | Perplexity: 1110.033932
2025-10-12 21:59:22,839 Stage: Train 0.5 | Epoch: 22 | Iter: 67800 | Total Loss: 0.005943 | Recon Loss: 0.005360 | Commit Loss: 0.001165 | Perplexity: 1115.821033
2025-10-12 22:04:47,112 Stage: Train 0.5 | Epoch: 22 | Iter: 68000 | Total Loss: 0.005880 | Recon Loss: 0.005300 | Commit Loss: 0.001159 | Perplexity: 1113.961538
2025-10-12 22:10:11,262 Stage: Train 0.5 | Epoch: 22 | Iter: 68200 | Total Loss: 0.005919 | Recon Loss: 0.005337 | Commit Loss: 0.001164 | Perplexity: 1116.467817
2025-10-12 22:15:35,451 Stage: Train 0.5 | Epoch: 22 | Iter: 68400 | Total Loss: 0.005876 | Recon Loss: 0.005289 | Commit Loss: 0.001175 | Perplexity: 1111.798846
2025-10-12 22:20:59,621 Stage: Train 0.5 | Epoch: 22 | Iter: 68600 | Total Loss: 0.005875 | Recon Loss: 0.005282 | Commit Loss: 0.001186 | Perplexity: 1109.108815
2025-10-12 22:26:23,904 Stage: Train 0.5 | Epoch: 22 | Iter: 68800 | Total Loss: 0.005829 | Recon Loss: 0.005262 | Commit Loss: 0.001135 | Perplexity: 1115.659950
2025-10-12 22:31:47,729 Stage: Train 0.5 | Epoch: 22 | Iter: 69000 | Total Loss: 0.005890 | Recon Loss: 0.005312 | Commit Loss: 0.001157 | Perplexity: 1108.810155
2025-10-12 22:37:11,833 Stage: Train 0.5 | Epoch: 22 | Iter: 69200 | Total Loss: 0.005727 | Recon Loss: 0.005167 | Commit Loss: 0.001122 | Perplexity: 1125.568156
2025-10-12 22:42:36,198 Stage: Train 0.5 | Epoch: 22 | Iter: 69400 | Total Loss: 0.005781 | Recon Loss: 0.005199 | Commit Loss: 0.001165 | Perplexity: 1121.867781
2025-10-12 22:48:00,345 Stage: Train 0.5 | Epoch: 22 | Iter: 69600 | Total Loss: 0.005842 | Recon Loss: 0.005259 | Commit Loss: 0.001167 | Perplexity: 1119.155809
2025-10-12 22:53:24,582 Stage: Train 0.5 | Epoch: 22 | Iter: 69800 | Total Loss: 0.005718 | Recon Loss: 0.005120 | Commit Loss: 0.001195 | Perplexity: 1112.737706
Trainning Epoch:  14%|█▍        | 23/165 [31:30:23<194:25:21, 4929.02s/it]Trainning Epoch:  14%|█▍        | 23/165 [31:30:23<194:25:22, 4929.03s/it]2025-10-12 22:58:53,150 Stage: Train 0.5 | Epoch: 23 | Iter: 70000 | Total Loss: 0.005772 | Recon Loss: 0.005195 | Commit Loss: 0.001153 | Perplexity: 1121.153204
2025-10-12 23:04:17,411 Stage: Train 0.5 | Epoch: 23 | Iter: 70200 | Total Loss: 0.005647 | Recon Loss: 0.005078 | Commit Loss: 0.001138 | Perplexity: 1113.139925
2025-10-12 23:09:41,543 Stage: Train 0.5 | Epoch: 23 | Iter: 70400 | Total Loss: 0.005836 | Recon Loss: 0.005259 | Commit Loss: 0.001155 | Perplexity: 1105.084525
2025-10-12 23:15:05,817 Stage: Train 0.5 | Epoch: 23 | Iter: 70600 | Total Loss: 0.005791 | Recon Loss: 0.005210 | Commit Loss: 0.001160 | Perplexity: 1111.736764
2025-10-12 23:20:30,380 Stage: Train 0.5 | Epoch: 23 | Iter: 70800 | Total Loss: 0.005879 | Recon Loss: 0.005300 | Commit Loss: 0.001158 | Perplexity: 1111.914426
2025-10-12 23:25:54,842 Stage: Train 0.5 | Epoch: 23 | Iter: 71000 | Total Loss: 0.005795 | Recon Loss: 0.005219 | Commit Loss: 0.001152 | Perplexity: 1110.262192
2025-10-12 23:31:19,081 Stage: Train 0.5 | Epoch: 23 | Iter: 71200 | Total Loss: 0.005755 | Recon Loss: 0.005169 | Commit Loss: 0.001172 | Perplexity: 1111.155790
2025-10-12 23:36:43,408 Stage: Train 0.5 | Epoch: 23 | Iter: 71400 | Total Loss: 0.005819 | Recon Loss: 0.005228 | Commit Loss: 0.001180 | Perplexity: 1109.507961
2025-10-12 23:42:07,633 Stage: Train 0.5 | Epoch: 23 | Iter: 71600 | Total Loss: 0.005747 | Recon Loss: 0.005171 | Commit Loss: 0.001153 | Perplexity: 1110.302614
2025-10-12 23:47:31,752 Stage: Train 0.5 | Epoch: 23 | Iter: 71800 | Total Loss: 0.005627 | Recon Loss: 0.005054 | Commit Loss: 0.001146 | Perplexity: 1110.510605
2025-10-12 23:52:55,867 Stage: Train 0.5 | Epoch: 23 | Iter: 72000 | Total Loss: 0.005852 | Recon Loss: 0.005273 | Commit Loss: 0.001157 | Perplexity: 1114.173989
2025-10-12 23:58:20,023 Stage: Train 0.5 | Epoch: 23 | Iter: 72200 | Total Loss: 0.005600 | Recon Loss: 0.005020 | Commit Loss: 0.001161 | Perplexity: 1115.153565
2025-10-13 00:03:44,431 Stage: Train 0.5 | Epoch: 23 | Iter: 72400 | Total Loss: 0.005833 | Recon Loss: 0.005252 | Commit Loss: 0.001162 | Perplexity: 1106.647837
2025-10-13 00:09:08,583 Stage: Train 0.5 | Epoch: 23 | Iter: 72600 | Total Loss: 0.005567 | Recon Loss: 0.004985 | Commit Loss: 0.001163 | Perplexity: 1119.864096
2025-10-13 00:14:32,473 Stage: Train 0.5 | Epoch: 23 | Iter: 72800 | Total Loss: 0.005656 | Recon Loss: 0.005084 | Commit Loss: 0.001145 | Perplexity: 1117.746430
Trainning Epoch:  15%|█▍        | 24/165 [32:52:32<193:03:18, 4929.07s/it]Trainning Epoch:  15%|█▍        | 24/165 [32:52:32<193:03:18, 4929.07s/it]2025-10-13 00:20:00,374 Stage: Train 0.5 | Epoch: 24 | Iter: 73000 | Total Loss: 0.005687 | Recon Loss: 0.005116 | Commit Loss: 0.001143 | Perplexity: 1115.713758
2025-10-13 00:25:24,771 Stage: Train 0.5 | Epoch: 24 | Iter: 73200 | Total Loss: 0.005724 | Recon Loss: 0.005139 | Commit Loss: 0.001170 | Perplexity: 1115.712533
2025-10-13 00:30:49,028 Stage: Train 0.5 | Epoch: 24 | Iter: 73400 | Total Loss: 0.005952 | Recon Loss: 0.005380 | Commit Loss: 0.001145 | Perplexity: 1119.556046
2025-10-13 00:36:13,413 Stage: Train 0.5 | Epoch: 24 | Iter: 73600 | Total Loss: 0.005647 | Recon Loss: 0.005055 | Commit Loss: 0.001183 | Perplexity: 1117.672364
2025-10-13 00:41:37,459 Stage: Train 0.5 | Epoch: 24 | Iter: 73800 | Total Loss: 0.005842 | Recon Loss: 0.005260 | Commit Loss: 0.001163 | Perplexity: 1112.690140
2025-10-13 00:47:01,333 Stage: Train 0.5 | Epoch: 24 | Iter: 74000 | Total Loss: 0.005736 | Recon Loss: 0.005157 | Commit Loss: 0.001160 | Perplexity: 1122.990695
2025-10-13 00:52:25,234 Stage: Train 0.5 | Epoch: 24 | Iter: 74200 | Total Loss: 0.005735 | Recon Loss: 0.005157 | Commit Loss: 0.001156 | Perplexity: 1110.136111
2025-10-13 00:57:49,379 Stage: Train 0.5 | Epoch: 24 | Iter: 74400 | Total Loss: 0.005700 | Recon Loss: 0.005121 | Commit Loss: 0.001157 | Perplexity: 1113.041632
2025-10-13 01:03:13,540 Stage: Train 0.5 | Epoch: 24 | Iter: 74600 | Total Loss: 0.005695 | Recon Loss: 0.005108 | Commit Loss: 0.001174 | Perplexity: 1118.620606
2025-10-13 01:08:38,018 Stage: Train 0.5 | Epoch: 24 | Iter: 74800 | Total Loss: 0.005758 | Recon Loss: 0.005175 | Commit Loss: 0.001167 | Perplexity: 1111.308667
2025-10-13 01:14:02,058 Stage: Train 0.5 | Epoch: 24 | Iter: 75000 | Total Loss: 0.005624 | Recon Loss: 0.005037 | Commit Loss: 0.001173 | Perplexity: 1115.305662
2025-10-13 01:19:26,182 Stage: Train 0.5 | Epoch: 24 | Iter: 75200 | Total Loss: 0.005532 | Recon Loss: 0.004960 | Commit Loss: 0.001144 | Perplexity: 1115.394015
2025-10-13 01:24:50,376 Stage: Train 0.5 | Epoch: 24 | Iter: 75400 | Total Loss: 0.005622 | Recon Loss: 0.005040 | Commit Loss: 0.001164 | Perplexity: 1113.485711
2025-10-13 01:30:14,538 Stage: Train 0.5 | Epoch: 24 | Iter: 75600 | Total Loss: 0.005740 | Recon Loss: 0.005170 | Commit Loss: 0.001140 | Perplexity: 1111.575170
2025-10-13 01:35:38,540 Stage: Train 0.5 | Epoch: 24 | Iter: 75800 | Total Loss: 0.005644 | Recon Loss: 0.005062 | Commit Loss: 0.001165 | Perplexity: 1114.213993
Trainning Epoch:  15%|█▌        | 25/165 [34:14:40<191:40:25, 4928.75s/it]Trainning Epoch:  15%|█▌        | 25/165 [34:14:40<191:40:25, 4928.76s/it]2025-10-13 01:41:07,135 Stage: Train 0.5 | Epoch: 25 | Iter: 76000 | Total Loss: 0.007722 | Recon Loss: 0.006618 | Commit Loss: 0.002208 | Perplexity: 1088.317369
2025-10-13 01:46:31,387 Stage: Train 0.5 | Epoch: 25 | Iter: 76200 | Total Loss: 0.005529 | Recon Loss: 0.004958 | Commit Loss: 0.001143 | Perplexity: 1100.785333
2025-10-13 01:51:55,182 Stage: Train 0.5 | Epoch: 25 | Iter: 76400 | Total Loss: 0.005681 | Recon Loss: 0.005096 | Commit Loss: 0.001170 | Perplexity: 1102.449909
2025-10-13 01:57:19,252 Stage: Train 0.5 | Epoch: 25 | Iter: 76600 | Total Loss: 0.005694 | Recon Loss: 0.005124 | Commit Loss: 0.001139 | Perplexity: 1106.192328
2025-10-13 02:02:43,444 Stage: Train 0.5 | Epoch: 25 | Iter: 76800 | Total Loss: 0.005644 | Recon Loss: 0.005061 | Commit Loss: 0.001166 | Perplexity: 1115.711852
2025-10-13 02:08:07,616 Stage: Train 0.5 | Epoch: 25 | Iter: 77000 | Total Loss: 0.005634 | Recon Loss: 0.005058 | Commit Loss: 0.001153 | Perplexity: 1111.675117
2025-10-13 02:13:31,627 Stage: Train 0.5 | Epoch: 25 | Iter: 77200 | Total Loss: 0.005545 | Recon Loss: 0.004965 | Commit Loss: 0.001161 | Perplexity: 1110.684723
2025-10-13 02:18:55,654 Stage: Train 0.5 | Epoch: 25 | Iter: 77400 | Total Loss: 0.005593 | Recon Loss: 0.005031 | Commit Loss: 0.001124 | Perplexity: 1110.127466
2025-10-13 02:24:19,776 Stage: Train 0.5 | Epoch: 25 | Iter: 77600 | Total Loss: 0.005706 | Recon Loss: 0.005135 | Commit Loss: 0.001143 | Perplexity: 1110.956897
2025-10-13 02:29:43,920 Stage: Train 0.5 | Epoch: 25 | Iter: 77800 | Total Loss: 0.005489 | Recon Loss: 0.004916 | Commit Loss: 0.001145 | Perplexity: 1114.310140
2025-10-13 02:35:08,017 Stage: Train 0.5 | Epoch: 25 | Iter: 78000 | Total Loss: 0.005750 | Recon Loss: 0.005188 | Commit Loss: 0.001124 | Perplexity: 1112.194264
2025-10-13 02:40:32,019 Stage: Train 0.5 | Epoch: 25 | Iter: 78200 | Total Loss: 0.005525 | Recon Loss: 0.004935 | Commit Loss: 0.001181 | Perplexity: 1109.101852
2025-10-13 02:45:55,903 Stage: Train 0.5 | Epoch: 25 | Iter: 78400 | Total Loss: 0.005586 | Recon Loss: 0.005023 | Commit Loss: 0.001127 | Perplexity: 1108.534719
2025-10-13 02:51:19,883 Stage: Train 0.5 | Epoch: 25 | Iter: 78600 | Total Loss: 0.005517 | Recon Loss: 0.004939 | Commit Loss: 0.001157 | Perplexity: 1117.172128
2025-10-13 02:56:43,730 Stage: Train 0.5 | Epoch: 25 | Iter: 78800 | Total Loss: 0.005630 | Recon Loss: 0.005058 | Commit Loss: 0.001144 | Perplexity: 1108.826662
Trainning Epoch:  16%|█▌        | 26/165 [35:36:47<190:17:08, 4928.26s/it]Trainning Epoch:  16%|█▌        | 26/165 [35:36:47<190:17:08, 4928.26s/it]2025-10-13 03:02:12,755 Stage: Train 0.5 | Epoch: 26 | Iter: 79000 | Total Loss: 0.005505 | Recon Loss: 0.004926 | Commit Loss: 0.001157 | Perplexity: 1113.505891
2025-10-13 03:07:36,997 Stage: Train 0.5 | Epoch: 26 | Iter: 79200 | Total Loss: 0.005465 | Recon Loss: 0.004885 | Commit Loss: 0.001160 | Perplexity: 1114.120556
2025-10-13 03:13:00,940 Stage: Train 0.5 | Epoch: 26 | Iter: 79400 | Total Loss: 0.005617 | Recon Loss: 0.005053 | Commit Loss: 0.001129 | Perplexity: 1120.009803
2025-10-13 03:18:24,940 Stage: Train 0.5 | Epoch: 26 | Iter: 79600 | Total Loss: 0.005494 | Recon Loss: 0.004930 | Commit Loss: 0.001128 | Perplexity: 1110.252134
2025-10-13 03:23:49,205 Stage: Train 0.5 | Epoch: 26 | Iter: 79800 | Total Loss: 0.005582 | Recon Loss: 0.005006 | Commit Loss: 0.001153 | Perplexity: 1114.610596
2025-10-13 03:29:13,249 Stage: Train 0.5 | Epoch: 26 | Iter: 80000 | Total Loss: 0.005541 | Recon Loss: 0.004980 | Commit Loss: 0.001122 | Perplexity: 1114.232738
2025-10-13 03:29:13,249 Saving model at iteration 80000
2025-10-13 03:29:13,829 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000
2025-10-13 03:29:15,257 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000/model.safetensors
2025-10-13 03:29:17,108 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000/optimizer.bin
2025-10-13 03:29:17,109 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000/scheduler.bin
2025-10-13 03:29:17,109 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000/sampler.bin
2025-10-13 03:29:17,110 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_27_step_80000/random_states_0.pkl
2025-10-13 03:34:41,329 Stage: Train 0.5 | Epoch: 26 | Iter: 80200 | Total Loss: 0.005502 | Recon Loss: 0.004941 | Commit Loss: 0.001122 | Perplexity: 1107.950964
2025-10-13 03:40:05,597 Stage: Train 0.5 | Epoch: 26 | Iter: 80400 | Total Loss: 0.005551 | Recon Loss: 0.004977 | Commit Loss: 0.001148 | Perplexity: 1114.079007
2025-10-13 03:45:29,874 Stage: Train 0.5 | Epoch: 26 | Iter: 80600 | Total Loss: 0.005487 | Recon Loss: 0.004909 | Commit Loss: 0.001157 | Perplexity: 1108.020787
2025-10-13 03:50:54,738 Stage: Train 0.5 | Epoch: 26 | Iter: 80800 | Total Loss: 0.005526 | Recon Loss: 0.004935 | Commit Loss: 0.001181 | Perplexity: 1108.824037
2025-10-13 03:56:18,754 Stage: Train 0.5 | Epoch: 26 | Iter: 81000 | Total Loss: 0.005482 | Recon Loss: 0.004912 | Commit Loss: 0.001140 | Perplexity: 1112.132260
2025-10-13 04:01:42,859 Stage: Train 0.5 | Epoch: 26 | Iter: 81200 | Total Loss: 0.005563 | Recon Loss: 0.004991 | Commit Loss: 0.001143 | Perplexity: 1100.933198
2025-10-13 04:07:06,890 Stage: Train 0.5 | Epoch: 26 | Iter: 81400 | Total Loss: 0.005440 | Recon Loss: 0.004867 | Commit Loss: 0.001146 | Perplexity: 1113.095010
2025-10-13 04:12:30,992 Stage: Train 0.5 | Epoch: 26 | Iter: 81600 | Total Loss: 0.005479 | Recon Loss: 0.004905 | Commit Loss: 0.001149 | Perplexity: 1117.890781
2025-10-13 04:17:55,469 Stage: Train 0.5 | Epoch: 26 | Iter: 81800 | Total Loss: 0.005590 | Recon Loss: 0.005023 | Commit Loss: 0.001134 | Perplexity: 1113.113752
2025-10-13 04:23:19,705 Stage: Train 0.5 | Epoch: 26 | Iter: 82000 | Total Loss: 0.005417 | Recon Loss: 0.004848 | Commit Loss: 0.001138 | Perplexity: 1118.985269
Trainning Epoch:  16%|█▋        | 27/165 [36:59:00<188:58:18, 4929.70s/it]Trainning Epoch:  16%|█▋        | 27/165 [36:59:00<188:58:18, 4929.70s/it]2025-10-13 04:28:48,200 Stage: Train 0.5 | Epoch: 27 | Iter: 82200 | Total Loss: 0.005563 | Recon Loss: 0.004996 | Commit Loss: 0.001132 | Perplexity: 1118.180554
2025-10-13 04:34:12,315 Stage: Train 0.5 | Epoch: 27 | Iter: 82400 | Total Loss: 0.005462 | Recon Loss: 0.004881 | Commit Loss: 0.001162 | Perplexity: 1111.624307
2025-10-13 04:39:36,390 Stage: Train 0.5 | Epoch: 27 | Iter: 82600 | Total Loss: 0.005393 | Recon Loss: 0.004828 | Commit Loss: 0.001131 | Perplexity: 1120.545363
2025-10-13 04:45:00,582 Stage: Train 0.5 | Epoch: 27 | Iter: 82800 | Total Loss: 0.005679 | Recon Loss: 0.005052 | Commit Loss: 0.001255 | Perplexity: 1126.585538
2025-10-13 04:50:24,952 Stage: Train 0.5 | Epoch: 27 | Iter: 83000 | Total Loss: 0.005444 | Recon Loss: 0.004873 | Commit Loss: 0.001143 | Perplexity: 1117.254206
2025-10-13 04:55:49,217 Stage: Train 0.5 | Epoch: 27 | Iter: 83200 | Total Loss: 0.005478 | Recon Loss: 0.004898 | Commit Loss: 0.001160 | Perplexity: 1121.921025
2025-10-13 05:01:13,324 Stage: Train 0.5 | Epoch: 27 | Iter: 83400 | Total Loss: 0.005446 | Recon Loss: 0.004878 | Commit Loss: 0.001135 | Perplexity: 1113.771977
2025-10-13 05:06:37,570 Stage: Train 0.5 | Epoch: 27 | Iter: 83600 | Total Loss: 0.005486 | Recon Loss: 0.004923 | Commit Loss: 0.001126 | Perplexity: 1111.403618
2025-10-13 05:12:01,909 Stage: Train 0.5 | Epoch: 27 | Iter: 83800 | Total Loss: 0.005457 | Recon Loss: 0.004891 | Commit Loss: 0.001132 | Perplexity: 1124.576934
2025-10-13 05:17:26,011 Stage: Train 0.5 | Epoch: 27 | Iter: 84000 | Total Loss: 0.005374 | Recon Loss: 0.004809 | Commit Loss: 0.001130 | Perplexity: 1112.548193
2025-10-13 05:22:50,045 Stage: Train 0.5 | Epoch: 27 | Iter: 84200 | Total Loss: 0.005320 | Recon Loss: 0.004770 | Commit Loss: 0.001099 | Perplexity: 1119.995303
2025-10-13 05:28:13,791 Stage: Train 0.5 | Epoch: 27 | Iter: 84400 | Total Loss: 0.005395 | Recon Loss: 0.004837 | Commit Loss: 0.001115 | Perplexity: 1117.786262
2025-10-13 05:33:38,264 Stage: Train 0.5 | Epoch: 27 | Iter: 84600 | Total Loss: 0.005353 | Recon Loss: 0.004793 | Commit Loss: 0.001120 | Perplexity: 1117.245789
2025-10-13 05:39:02,692 Stage: Train 0.5 | Epoch: 27 | Iter: 84800 | Total Loss: 0.005345 | Recon Loss: 0.004780 | Commit Loss: 0.001130 | Perplexity: 1115.364286
2025-10-13 05:44:27,478 Stage: Train 0.5 | Epoch: 27 | Iter: 85000 | Total Loss: 0.005433 | Recon Loss: 0.004883 | Commit Loss: 0.001100 | Perplexity: 1108.723680
Trainning Epoch:  17%|█▋        | 28/165 [38:21:10<187:36:01, 4929.65s/it]Trainning Epoch:  17%|█▋        | 28/165 [38:21:10<187:36:01, 4929.65s/it]2025-10-13 05:49:56,686 Stage: Train 0.5 | Epoch: 28 | Iter: 85200 | Total Loss: 0.005305 | Recon Loss: 0.004749 | Commit Loss: 0.001112 | Perplexity: 1121.807411
2025-10-13 05:55:21,470 Stage: Train 0.5 | Epoch: 28 | Iter: 85400 | Total Loss: 0.005422 | Recon Loss: 0.004877 | Commit Loss: 0.001090 | Perplexity: 1119.873521
2025-10-13 06:00:46,514 Stage: Train 0.5 | Epoch: 28 | Iter: 85600 | Total Loss: 0.005361 | Recon Loss: 0.004809 | Commit Loss: 0.001106 | Perplexity: 1111.489619
2025-10-13 06:06:11,651 Stage: Train 0.5 | Epoch: 28 | Iter: 85800 | Total Loss: 0.005391 | Recon Loss: 0.004842 | Commit Loss: 0.001097 | Perplexity: 1123.967944
2025-10-13 06:11:36,714 Stage: Train 0.5 | Epoch: 28 | Iter: 86000 | Total Loss: 0.005456 | Recon Loss: 0.004847 | Commit Loss: 0.001216 | Perplexity: 1127.979413
2025-10-13 06:17:01,677 Stage: Train 0.5 | Epoch: 28 | Iter: 86200 | Total Loss: 0.005345 | Recon Loss: 0.004790 | Commit Loss: 0.001111 | Perplexity: 1119.338837
2025-10-13 06:22:26,297 Stage: Train 0.5 | Epoch: 28 | Iter: 86400 | Total Loss: 0.005352 | Recon Loss: 0.004802 | Commit Loss: 0.001100 | Perplexity: 1122.110795
2025-10-13 06:27:50,579 Stage: Train 0.5 | Epoch: 28 | Iter: 86600 | Total Loss: 0.005419 | Recon Loss: 0.004865 | Commit Loss: 0.001108 | Perplexity: 1121.148805
2025-10-13 06:33:14,694 Stage: Train 0.5 | Epoch: 28 | Iter: 86800 | Total Loss: 0.005374 | Recon Loss: 0.004817 | Commit Loss: 0.001115 | Perplexity: 1114.894627
2025-10-13 06:38:38,677 Stage: Train 0.5 | Epoch: 28 | Iter: 87000 | Total Loss: 0.005301 | Recon Loss: 0.004758 | Commit Loss: 0.001085 | Perplexity: 1113.554953
2025-10-13 06:44:02,981 Stage: Train 0.5 | Epoch: 28 | Iter: 87200 | Total Loss: 0.005342 | Recon Loss: 0.004801 | Commit Loss: 0.001082 | Perplexity: 1119.533669
2025-10-13 06:49:26,992 Stage: Train 0.5 | Epoch: 28 | Iter: 87400 | Total Loss: 0.005406 | Recon Loss: 0.004862 | Commit Loss: 0.001088 | Perplexity: 1120.651544
2025-10-13 06:54:51,507 Stage: Train 0.5 | Epoch: 28 | Iter: 87600 | Total Loss: 0.005355 | Recon Loss: 0.004790 | Commit Loss: 0.001131 | Perplexity: 1117.170835
2025-10-13 07:00:15,712 Stage: Train 0.5 | Epoch: 28 | Iter: 87800 | Total Loss: 0.005237 | Recon Loss: 0.004682 | Commit Loss: 0.001110 | Perplexity: 1119.766830
2025-10-13 07:05:40,030 Stage: Train 0.5 | Epoch: 28 | Iter: 88000 | Total Loss: 0.005332 | Recon Loss: 0.004776 | Commit Loss: 0.001113 | Perplexity: 1128.972265
Trainning Epoch:  18%|█▊        | 29/165 [39:43:24<186:16:39, 4930.88s/it]Trainning Epoch:  18%|█▊        | 29/165 [39:43:24<186:16:39, 4930.88s/it]2025-10-13 07:11:08,416 Stage: Train 0.5 | Epoch: 29 | Iter: 88200 | Total Loss: 0.005374 | Recon Loss: 0.004824 | Commit Loss: 0.001100 | Perplexity: 1118.609030
2025-10-13 07:16:32,688 Stage: Train 0.5 | Epoch: 29 | Iter: 88400 | Total Loss: 0.005312 | Recon Loss: 0.004768 | Commit Loss: 0.001087 | Perplexity: 1121.904186
2025-10-13 07:21:57,005 Stage: Train 0.5 | Epoch: 29 | Iter: 88600 | Total Loss: 0.005293 | Recon Loss: 0.004747 | Commit Loss: 0.001092 | Perplexity: 1124.072656
2025-10-13 07:27:21,400 Stage: Train 0.5 | Epoch: 29 | Iter: 88800 | Total Loss: 0.005182 | Recon Loss: 0.004635 | Commit Loss: 0.001092 | Perplexity: 1122.592600
2025-10-13 07:32:45,534 Stage: Train 0.5 | Epoch: 29 | Iter: 89000 | Total Loss: 0.005259 | Recon Loss: 0.004721 | Commit Loss: 0.001076 | Perplexity: 1122.084960
2025-10-13 07:38:09,515 Stage: Train 0.5 | Epoch: 29 | Iter: 89200 | Total Loss: 0.005321 | Recon Loss: 0.004773 | Commit Loss: 0.001096 | Perplexity: 1123.824962
2025-10-13 07:43:33,562 Stage: Train 0.5 | Epoch: 29 | Iter: 89400 | Total Loss: 0.005120 | Recon Loss: 0.004581 | Commit Loss: 0.001078 | Perplexity: 1130.531980
2025-10-13 07:48:57,365 Stage: Train 0.5 | Epoch: 29 | Iter: 89600 | Total Loss: 0.005320 | Recon Loss: 0.004791 | Commit Loss: 0.001057 | Perplexity: 1129.154013
2025-10-13 07:54:21,427 Stage: Train 0.5 | Epoch: 29 | Iter: 89800 | Total Loss: 0.005283 | Recon Loss: 0.004741 | Commit Loss: 0.001083 | Perplexity: 1125.201698
2025-10-13 07:59:45,501 Stage: Train 0.5 | Epoch: 29 | Iter: 90000 | Total Loss: 0.005254 | Recon Loss: 0.004721 | Commit Loss: 0.001067 | Perplexity: 1119.484441
2025-10-13 08:05:09,225 Stage: Train 0.5 | Epoch: 29 | Iter: 90200 | Total Loss: 0.005162 | Recon Loss: 0.004629 | Commit Loss: 0.001065 | Perplexity: 1119.278129
2025-10-13 08:10:33,058 Stage: Train 0.5 | Epoch: 29 | Iter: 90400 | Total Loss: 0.005149 | Recon Loss: 0.004607 | Commit Loss: 0.001085 | Perplexity: 1136.863664
2025-10-13 08:15:56,652 Stage: Train 0.5 | Epoch: 29 | Iter: 90600 | Total Loss: 0.005398 | Recon Loss: 0.004867 | Commit Loss: 0.001062 | Perplexity: 1131.646323
2025-10-13 08:21:20,317 Stage: Train 0.5 | Epoch: 29 | Iter: 90800 | Total Loss: 0.005182 | Recon Loss: 0.004631 | Commit Loss: 0.001102 | Perplexity: 1133.783845
2025-10-13 08:26:44,404 Stage: Train 0.5 | Epoch: 29 | Iter: 91000 | Total Loss: 0.005201 | Recon Loss: 0.004667 | Commit Loss: 0.001068 | Perplexity: 1130.487813
Trainning Epoch:  18%|█▊        | 30/165 [41:05:30<184:51:11, 4929.41s/it]Trainning Epoch:  18%|█▊        | 30/165 [41:05:30<184:51:11, 4929.42s/it]2025-10-13 08:32:12,834 Stage: Train 0.5 | Epoch: 30 | Iter: 91200 | Total Loss: 0.005079 | Recon Loss: 0.004544 | Commit Loss: 0.001071 | Perplexity: 1126.308784
2025-10-13 08:37:37,655 Stage: Train 0.5 | Epoch: 30 | Iter: 91400 | Total Loss: 0.005086 | Recon Loss: 0.004557 | Commit Loss: 0.001059 | Perplexity: 1140.652001
2025-10-13 08:43:02,918 Stage: Train 0.5 | Epoch: 30 | Iter: 91600 | Total Loss: 0.005246 | Recon Loss: 0.004714 | Commit Loss: 0.001066 | Perplexity: 1126.328966
2025-10-13 08:48:28,077 Stage: Train 0.5 | Epoch: 30 | Iter: 91800 | Total Loss: 0.005284 | Recon Loss: 0.004743 | Commit Loss: 0.001082 | Perplexity: 1132.046289
2025-10-13 08:53:52,617 Stage: Train 0.5 | Epoch: 30 | Iter: 92000 | Total Loss: 0.005192 | Recon Loss: 0.004646 | Commit Loss: 0.001092 | Perplexity: 1127.510038
2025-10-13 08:59:17,476 Stage: Train 0.5 | Epoch: 30 | Iter: 92200 | Total Loss: 0.005234 | Recon Loss: 0.004691 | Commit Loss: 0.001085 | Perplexity: 1130.924279
2025-10-13 09:04:42,179 Stage: Train 0.5 | Epoch: 30 | Iter: 92400 | Total Loss: 0.005298 | Recon Loss: 0.004755 | Commit Loss: 0.001087 | Perplexity: 1136.706500
2025-10-13 09:10:06,886 Stage: Train 0.5 | Epoch: 30 | Iter: 92600 | Total Loss: 0.005169 | Recon Loss: 0.004623 | Commit Loss: 0.001092 | Perplexity: 1138.930658
2025-10-13 09:15:31,400 Stage: Train 0.5 | Epoch: 30 | Iter: 92800 | Total Loss: 0.005184 | Recon Loss: 0.004652 | Commit Loss: 0.001064 | Perplexity: 1135.635403
2025-10-13 09:20:55,655 Stage: Train 0.5 | Epoch: 30 | Iter: 93000 | Total Loss: 0.005146 | Recon Loss: 0.004624 | Commit Loss: 0.001043 | Perplexity: 1134.108733
2025-10-13 09:26:20,014 Stage: Train 0.5 | Epoch: 30 | Iter: 93200 | Total Loss: 0.005194 | Recon Loss: 0.004661 | Commit Loss: 0.001066 | Perplexity: 1126.125197
2025-10-13 09:31:44,797 Stage: Train 0.5 | Epoch: 30 | Iter: 93400 | Total Loss: 0.005157 | Recon Loss: 0.004617 | Commit Loss: 0.001079 | Perplexity: 1132.214141
2025-10-13 09:37:09,250 Stage: Train 0.5 | Epoch: 30 | Iter: 93600 | Total Loss: 0.005171 | Recon Loss: 0.004641 | Commit Loss: 0.001059 | Perplexity: 1135.957088
2025-10-13 09:42:33,953 Stage: Train 0.5 | Epoch: 30 | Iter: 93800 | Total Loss: 0.005156 | Recon Loss: 0.004623 | Commit Loss: 0.001066 | Perplexity: 1138.845645
2025-10-13 09:47:58,634 Stage: Train 0.5 | Epoch: 30 | Iter: 94000 | Total Loss: 0.005051 | Recon Loss: 0.004517 | Commit Loss: 0.001067 | Perplexity: 1138.422820
Trainning Epoch:  19%|█▉        | 31/165 [42:27:46<183:33:48, 4931.56s/it]Trainning Epoch:  19%|█▉        | 31/165 [42:27:46<183:33:49, 4931.56s/it]2025-10-13 09:53:28,073 Stage: Train 0.5 | Epoch: 31 | Iter: 94200 | Total Loss: 0.005156 | Recon Loss: 0.004596 | Commit Loss: 0.001121 | Perplexity: 1143.793457
2025-10-13 09:58:53,220 Stage: Train 0.5 | Epoch: 31 | Iter: 94400 | Total Loss: 0.005056 | Recon Loss: 0.004529 | Commit Loss: 0.001056 | Perplexity: 1137.348907
2025-10-13 10:04:17,895 Stage: Train 0.5 | Epoch: 31 | Iter: 94600 | Total Loss: 0.005132 | Recon Loss: 0.004588 | Commit Loss: 0.001087 | Perplexity: 1130.563174
2025-10-13 10:09:42,763 Stage: Train 0.5 | Epoch: 31 | Iter: 94800 | Total Loss: 0.005104 | Recon Loss: 0.004574 | Commit Loss: 0.001059 | Perplexity: 1135.048173
2025-10-13 10:15:07,525 Stage: Train 0.5 | Epoch: 31 | Iter: 95000 | Total Loss: 0.005187 | Recon Loss: 0.004648 | Commit Loss: 0.001077 | Perplexity: 1140.658428
2025-10-13 10:20:32,519 Stage: Train 0.5 | Epoch: 31 | Iter: 95200 | Total Loss: 0.005056 | Recon Loss: 0.004518 | Commit Loss: 0.001075 | Perplexity: 1137.073881
2025-10-13 10:25:57,332 Stage: Train 0.5 | Epoch: 31 | Iter: 95400 | Total Loss: 0.005105 | Recon Loss: 0.004570 | Commit Loss: 0.001070 | Perplexity: 1142.897519
2025-10-13 10:31:22,375 Stage: Train 0.5 | Epoch: 31 | Iter: 95600 | Total Loss: 0.005181 | Recon Loss: 0.004661 | Commit Loss: 0.001040 | Perplexity: 1139.880336
2025-10-13 10:36:47,485 Stage: Train 0.5 | Epoch: 31 | Iter: 95800 | Total Loss: 0.005088 | Recon Loss: 0.004549 | Commit Loss: 0.001077 | Perplexity: 1130.410500
2025-10-13 10:42:12,331 Stage: Train 0.5 | Epoch: 31 | Iter: 96000 | Total Loss: 0.004999 | Recon Loss: 0.004480 | Commit Loss: 0.001038 | Perplexity: 1144.786180
2025-10-13 10:47:37,320 Stage: Train 0.5 | Epoch: 31 | Iter: 96200 | Total Loss: 0.005114 | Recon Loss: 0.004576 | Commit Loss: 0.001077 | Perplexity: 1141.920449
2025-10-13 10:53:02,261 Stage: Train 0.5 | Epoch: 31 | Iter: 96400 | Total Loss: 0.005071 | Recon Loss: 0.004538 | Commit Loss: 0.001066 | Perplexity: 1138.271635
2025-10-13 10:58:27,157 Stage: Train 0.5 | Epoch: 31 | Iter: 96600 | Total Loss: 0.006577 | Recon Loss: 0.005603 | Commit Loss: 0.001949 | Perplexity: 1109.560763
2025-10-13 11:03:51,956 Stage: Train 0.5 | Epoch: 31 | Iter: 96800 | Total Loss: 0.005051 | Recon Loss: 0.004535 | Commit Loss: 0.001033 | Perplexity: 1121.448979
2025-10-13 11:09:16,667 Stage: Train 0.5 | Epoch: 31 | Iter: 97000 | Total Loss: 0.005006 | Recon Loss: 0.004494 | Commit Loss: 0.001025 | Perplexity: 1125.419806
2025-10-13 11:14:41,190 Stage: Train 0.5 | Epoch: 31 | Iter: 97200 | Total Loss: 0.005135 | Recon Loss: 0.004617 | Commit Loss: 0.001037 | Perplexity: 1135.658325
Trainning Epoch:  19%|█▉        | 32/165 [43:50:06<182:16:54, 4933.94s/it]Trainning Epoch:  19%|█▉        | 32/165 [43:50:06<182:16:54, 4933.94s/it]2025-10-13 11:20:10,378 Stage: Train 0.5 | Epoch: 32 | Iter: 97400 | Total Loss: 0.005168 | Recon Loss: 0.004658 | Commit Loss: 0.001021 | Perplexity: 1140.169150
2025-10-13 11:25:35,193 Stage: Train 0.5 | Epoch: 32 | Iter: 97600 | Total Loss: 0.005037 | Recon Loss: 0.004516 | Commit Loss: 0.001043 | Perplexity: 1137.808204
2025-10-13 11:30:59,493 Stage: Train 0.5 | Epoch: 32 | Iter: 97800 | Total Loss: 0.005118 | Recon Loss: 0.004600 | Commit Loss: 0.001036 | Perplexity: 1143.731597
2025-10-13 11:36:24,093 Stage: Train 0.5 | Epoch: 32 | Iter: 98000 | Total Loss: 0.005012 | Recon Loss: 0.004489 | Commit Loss: 0.001045 | Perplexity: 1138.364828
2025-10-13 11:41:48,420 Stage: Train 0.5 | Epoch: 32 | Iter: 98200 | Total Loss: 0.005085 | Recon Loss: 0.004572 | Commit Loss: 0.001026 | Perplexity: 1130.625912
2025-10-13 11:47:12,588 Stage: Train 0.5 | Epoch: 32 | Iter: 98400 | Total Loss: 0.005021 | Recon Loss: 0.004502 | Commit Loss: 0.001037 | Perplexity: 1138.707080
2025-10-13 11:52:36,946 Stage: Train 0.5 | Epoch: 32 | Iter: 98600 | Total Loss: 0.005264 | Recon Loss: 0.004639 | Commit Loss: 0.001251 | Perplexity: 1147.200813
2025-10-13 11:58:01,893 Stage: Train 0.5 | Epoch: 32 | Iter: 98800 | Total Loss: 0.005099 | Recon Loss: 0.004572 | Commit Loss: 0.001053 | Perplexity: 1134.742460
2025-10-13 12:03:26,068 Stage: Train 0.5 | Epoch: 32 | Iter: 99000 | Total Loss: 0.004936 | Recon Loss: 0.004407 | Commit Loss: 0.001057 | Perplexity: 1144.611899
2025-10-13 12:08:50,390 Stage: Train 0.5 | Epoch: 32 | Iter: 99200 | Total Loss: 0.005093 | Recon Loss: 0.004575 | Commit Loss: 0.001036 | Perplexity: 1142.645300
2025-10-13 12:14:14,792 Stage: Train 0.5 | Epoch: 32 | Iter: 99400 | Total Loss: 0.005027 | Recon Loss: 0.004508 | Commit Loss: 0.001039 | Perplexity: 1136.554775
2025-10-13 12:19:39,181 Stage: Train 0.5 | Epoch: 32 | Iter: 99600 | Total Loss: 0.005026 | Recon Loss: 0.004508 | Commit Loss: 0.001037 | Perplexity: 1144.284956
2025-10-13 12:25:03,631 Stage: Train 0.5 | Epoch: 32 | Iter: 99800 | Total Loss: 0.005117 | Recon Loss: 0.004569 | Commit Loss: 0.001095 | Perplexity: 1138.440730
2025-10-13 12:30:28,161 Stage: Train 0.5 | Epoch: 32 | Iter: 100000 | Total Loss: 0.004942 | Recon Loss: 0.004425 | Commit Loss: 0.001034 | Perplexity: 1136.515543
2025-10-13 12:30:28,162 Saving model at iteration 100000
2025-10-13 12:30:28,698 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000
2025-10-13 12:30:30,087 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000/model.safetensors
2025-10-13 12:30:31,897 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000/optimizer.bin
2025-10-13 12:30:31,897 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000/scheduler.bin
2025-10-13 12:30:31,898 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000/sampler.bin
2025-10-13 12:30:31,898 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_33_step_100000/random_states_0.pkl
2025-10-13 12:35:56,339 Stage: Train 0.5 | Epoch: 32 | Iter: 100200 | Total Loss: 0.005040 | Recon Loss: 0.004520 | Commit Loss: 0.001040 | Perplexity: 1146.989260
Trainning Epoch:  20%|██        | 33/165 [45:12:22<180:56:25, 4934.74s/it]Trainning Epoch:  20%|██        | 33/165 [45:12:22<180:56:26, 4934.74s/it]2025-10-13 12:41:24,910 Stage: Train 0.5 | Epoch: 33 | Iter: 100400 | Total Loss: 0.004915 | Recon Loss: 0.004394 | Commit Loss: 0.001042 | Perplexity: 1146.603822
2025-10-13 12:46:49,027 Stage: Train 0.5 | Epoch: 33 | Iter: 100600 | Total Loss: 0.005067 | Recon Loss: 0.004558 | Commit Loss: 0.001018 | Perplexity: 1136.878343
2025-10-13 12:52:13,005 Stage: Train 0.5 | Epoch: 33 | Iter: 100800 | Total Loss: 0.004959 | Recon Loss: 0.004441 | Commit Loss: 0.001035 | Perplexity: 1140.981322
2025-10-13 12:57:37,560 Stage: Train 0.5 | Epoch: 33 | Iter: 101000 | Total Loss: 0.004979 | Recon Loss: 0.004472 | Commit Loss: 0.001015 | Perplexity: 1147.019305
2025-10-13 13:03:01,307 Stage: Train 0.5 | Epoch: 33 | Iter: 101200 | Total Loss: 0.005085 | Recon Loss: 0.004575 | Commit Loss: 0.001020 | Perplexity: 1141.121369
2025-10-13 13:08:25,475 Stage: Train 0.5 | Epoch: 33 | Iter: 101400 | Total Loss: 0.004985 | Recon Loss: 0.004473 | Commit Loss: 0.001024 | Perplexity: 1152.688226
2025-10-13 13:13:49,551 Stage: Train 0.5 | Epoch: 33 | Iter: 101600 | Total Loss: 0.004977 | Recon Loss: 0.004454 | Commit Loss: 0.001045 | Perplexity: 1142.835752
2025-10-13 13:19:13,447 Stage: Train 0.5 | Epoch: 33 | Iter: 101800 | Total Loss: 0.004961 | Recon Loss: 0.004443 | Commit Loss: 0.001037 | Perplexity: 1146.595770
2025-10-13 13:24:37,296 Stage: Train 0.5 | Epoch: 33 | Iter: 102000 | Total Loss: 0.004825 | Recon Loss: 0.004307 | Commit Loss: 0.001036 | Perplexity: 1148.301398
2025-10-13 13:30:01,278 Stage: Train 0.5 | Epoch: 33 | Iter: 102200 | Total Loss: 0.004873 | Recon Loss: 0.004358 | Commit Loss: 0.001029 | Perplexity: 1142.118777
2025-10-13 13:35:25,199 Stage: Train 0.5 | Epoch: 33 | Iter: 102400 | Total Loss: 0.005023 | Recon Loss: 0.004504 | Commit Loss: 0.001037 | Perplexity: 1142.958669
2025-10-13 13:40:49,337 Stage: Train 0.5 | Epoch: 33 | Iter: 102600 | Total Loss: 0.004927 | Recon Loss: 0.004407 | Commit Loss: 0.001040 | Perplexity: 1146.634908
2025-10-13 13:46:13,202 Stage: Train 0.5 | Epoch: 33 | Iter: 102800 | Total Loss: 0.004830 | Recon Loss: 0.004316 | Commit Loss: 0.001028 | Perplexity: 1143.012357
2025-10-13 13:51:37,067 Stage: Train 0.5 | Epoch: 33 | Iter: 103000 | Total Loss: 0.005028 | Recon Loss: 0.004508 | Commit Loss: 0.001040 | Perplexity: 1149.678981
2025-10-13 13:57:00,935 Stage: Train 0.5 | Epoch: 33 | Iter: 103200 | Total Loss: 0.005013 | Recon Loss: 0.004486 | Commit Loss: 0.001055 | Perplexity: 1146.198810
Trainning Epoch:  21%|██        | 34/165 [46:34:29<179:28:33, 4932.16s/it]Trainning Epoch:  21%|██        | 34/165 [46:34:28<179:28:33, 4932.17s/it]2025-10-13 14:02:29,319 Stage: Train 0.5 | Epoch: 34 | Iter: 103400 | Total Loss: 0.004991 | Recon Loss: 0.004462 | Commit Loss: 0.001059 | Perplexity: 1151.687933
2025-10-13 14:07:52,981 Stage: Train 0.5 | Epoch: 34 | Iter: 103600 | Total Loss: 0.004931 | Recon Loss: 0.004412 | Commit Loss: 0.001038 | Perplexity: 1141.979297
2025-10-13 14:13:16,606 Stage: Train 0.5 | Epoch: 34 | Iter: 103800 | Total Loss: 0.004837 | Recon Loss: 0.004315 | Commit Loss: 0.001044 | Perplexity: 1146.872852
2025-10-13 14:18:40,602 Stage: Train 0.5 | Epoch: 34 | Iter: 104000 | Total Loss: 0.004948 | Recon Loss: 0.004430 | Commit Loss: 0.001037 | Perplexity: 1141.148910
2025-10-13 14:24:04,596 Stage: Train 0.5 | Epoch: 34 | Iter: 104200 | Total Loss: 0.004945 | Recon Loss: 0.004419 | Commit Loss: 0.001052 | Perplexity: 1148.991646
2025-10-13 14:29:28,367 Stage: Train 0.5 | Epoch: 34 | Iter: 104400 | Total Loss: 0.005015 | Recon Loss: 0.004499 | Commit Loss: 0.001033 | Perplexity: 1143.425366
2025-10-13 14:34:52,126 Stage: Train 0.5 | Epoch: 34 | Iter: 104600 | Total Loss: 0.004830 | Recon Loss: 0.004317 | Commit Loss: 0.001026 | Perplexity: 1151.366517
2025-10-13 14:40:16,089 Stage: Train 0.5 | Epoch: 34 | Iter: 104800 | Total Loss: 0.004964 | Recon Loss: 0.004451 | Commit Loss: 0.001025 | Perplexity: 1146.295248
2025-10-13 14:45:39,973 Stage: Train 0.5 | Epoch: 34 | Iter: 105000 | Total Loss: 0.005325 | Recon Loss: 0.004683 | Commit Loss: 0.001283 | Perplexity: 1148.609647
2025-10-13 14:51:03,963 Stage: Train 0.5 | Epoch: 34 | Iter: 105200 | Total Loss: 0.004963 | Recon Loss: 0.004454 | Commit Loss: 0.001018 | Perplexity: 1136.695063
2025-10-13 14:56:27,587 Stage: Train 0.5 | Epoch: 34 | Iter: 105400 | Total Loss: 0.004875 | Recon Loss: 0.004355 | Commit Loss: 0.001040 | Perplexity: 1146.926819
2025-10-13 15:01:51,420 Stage: Train 0.5 | Epoch: 34 | Iter: 105600 | Total Loss: 0.004814 | Recon Loss: 0.004308 | Commit Loss: 0.001013 | Perplexity: 1153.840387
2025-10-13 15:07:15,339 Stage: Train 0.5 | Epoch: 34 | Iter: 105800 | Total Loss: 0.004879 | Recon Loss: 0.004377 | Commit Loss: 0.001005 | Perplexity: 1147.478650
2025-10-13 15:12:39,362 Stage: Train 0.5 | Epoch: 34 | Iter: 106000 | Total Loss: 0.004842 | Recon Loss: 0.004326 | Commit Loss: 0.001031 | Perplexity: 1140.464605
2025-10-13 15:18:03,212 Stage: Train 0.5 | Epoch: 34 | Iter: 106200 | Total Loss: 0.004915 | Recon Loss: 0.004410 | Commit Loss: 0.001010 | Perplexity: 1152.235756
Trainning Epoch:  21%|██        | 35/165 [47:56:32<178:00:56, 4929.66s/it]Trainning Epoch:  21%|██        | 35/165 [47:56:32<178:00:55, 4929.66s/it]2025-10-13 15:23:31,560 Stage: Train 0.5 | Epoch: 35 | Iter: 106400 | Total Loss: 0.005061 | Recon Loss: 0.004546 | Commit Loss: 0.001031 | Perplexity: 1140.900066
2025-10-13 15:28:55,685 Stage: Train 0.5 | Epoch: 35 | Iter: 106600 | Total Loss: 0.004839 | Recon Loss: 0.004321 | Commit Loss: 0.001035 | Perplexity: 1142.841951
2025-10-13 15:34:19,647 Stage: Train 0.5 | Epoch: 35 | Iter: 106800 | Total Loss: 0.004924 | Recon Loss: 0.004403 | Commit Loss: 0.001043 | Perplexity: 1147.644849
2025-10-13 15:39:43,431 Stage: Train 0.5 | Epoch: 35 | Iter: 107000 | Total Loss: 0.004912 | Recon Loss: 0.004393 | Commit Loss: 0.001038 | Perplexity: 1148.664071
2025-10-13 15:45:07,126 Stage: Train 0.5 | Epoch: 35 | Iter: 107200 | Total Loss: 0.004959 | Recon Loss: 0.004442 | Commit Loss: 0.001035 | Perplexity: 1144.467313
2025-10-13 15:50:30,818 Stage: Train 0.5 | Epoch: 35 | Iter: 107400 | Total Loss: 0.004972 | Recon Loss: 0.004456 | Commit Loss: 0.001034 | Perplexity: 1152.819127
2025-10-13 15:55:54,515 Stage: Train 0.5 | Epoch: 35 | Iter: 107600 | Total Loss: 0.004965 | Recon Loss: 0.004434 | Commit Loss: 0.001063 | Perplexity: 1146.999459
2025-10-13 16:01:18,508 Stage: Train 0.5 | Epoch: 35 | Iter: 107800 | Total Loss: 0.004786 | Recon Loss: 0.004273 | Commit Loss: 0.001025 | Perplexity: 1146.308681
2025-10-13 16:06:42,418 Stage: Train 0.5 | Epoch: 35 | Iter: 108000 | Total Loss: 0.004837 | Recon Loss: 0.004321 | Commit Loss: 0.001031 | Perplexity: 1151.852113
2025-10-13 16:12:06,205 Stage: Train 0.5 | Epoch: 35 | Iter: 108200 | Total Loss: 0.004890 | Recon Loss: 0.004370 | Commit Loss: 0.001040 | Perplexity: 1150.142921
2025-10-13 16:17:29,611 Stage: Train 0.5 | Epoch: 35 | Iter: 108400 | Total Loss: 0.004907 | Recon Loss: 0.004402 | Commit Loss: 0.001011 | Perplexity: 1143.406424
2025-10-13 16:22:53,549 Stage: Train 0.5 | Epoch: 35 | Iter: 108600 | Total Loss: 0.004753 | Recon Loss: 0.004240 | Commit Loss: 0.001026 | Perplexity: 1146.331747
2025-10-13 16:28:17,611 Stage: Train 0.5 | Epoch: 35 | Iter: 108800 | Total Loss: 0.004932 | Recon Loss: 0.004406 | Commit Loss: 0.001052 | Perplexity: 1148.381878
2025-10-13 16:33:41,501 Stage: Train 0.5 | Epoch: 35 | Iter: 109000 | Total Loss: 0.004910 | Recon Loss: 0.004387 | Commit Loss: 0.001047 | Perplexity: 1156.749543
2025-10-13 16:39:05,633 Stage: Train 0.5 | Epoch: 35 | Iter: 109200 | Total Loss: 0.004789 | Recon Loss: 0.004280 | Commit Loss: 0.001019 | Perplexity: 1150.551510
Trainning Epoch:  22%|██▏       | 36/165 [49:18:36<176:34:56, 4927.88s/it]Trainning Epoch:  22%|██▏       | 36/165 [49:18:36<176:34:56, 4927.88s/it]2025-10-13 16:44:33,749 Stage: Train 0.5 | Epoch: 36 | Iter: 109400 | Total Loss: 0.005401 | Recon Loss: 0.004701 | Commit Loss: 0.001400 | Perplexity: 1152.436047
2025-10-13 16:49:57,561 Stage: Train 0.5 | Epoch: 36 | Iter: 109600 | Total Loss: 0.004803 | Recon Loss: 0.004289 | Commit Loss: 0.001028 | Perplexity: 1153.800849
2025-10-13 16:55:21,328 Stage: Train 0.5 | Epoch: 36 | Iter: 109800 | Total Loss: 0.004750 | Recon Loss: 0.004256 | Commit Loss: 0.000989 | Perplexity: 1151.378176
2025-10-13 17:00:45,117 Stage: Train 0.5 | Epoch: 36 | Iter: 110000 | Total Loss: 0.004941 | Recon Loss: 0.004424 | Commit Loss: 0.001033 | Perplexity: 1146.071261
2025-10-13 17:06:09,020 Stage: Train 0.5 | Epoch: 36 | Iter: 110200 | Total Loss: 0.004867 | Recon Loss: 0.004356 | Commit Loss: 0.001021 | Perplexity: 1149.326255
2025-10-13 17:11:33,127 Stage: Train 0.5 | Epoch: 36 | Iter: 110400 | Total Loss: 0.004913 | Recon Loss: 0.004395 | Commit Loss: 0.001036 | Perplexity: 1145.657171
2025-10-13 17:16:57,155 Stage: Train 0.5 | Epoch: 36 | Iter: 110600 | Total Loss: 0.004834 | Recon Loss: 0.004317 | Commit Loss: 0.001035 | Perplexity: 1148.260040
2025-10-13 17:22:21,128 Stage: Train 0.5 | Epoch: 36 | Iter: 110800 | Total Loss: 0.004863 | Recon Loss: 0.004339 | Commit Loss: 0.001048 | Perplexity: 1156.081747
2025-10-13 17:27:45,139 Stage: Train 0.5 | Epoch: 36 | Iter: 111000 | Total Loss: 0.004803 | Recon Loss: 0.004295 | Commit Loss: 0.001017 | Perplexity: 1146.453090
2025-10-13 17:33:09,256 Stage: Train 0.5 | Epoch: 36 | Iter: 111200 | Total Loss: 0.004775 | Recon Loss: 0.004252 | Commit Loss: 0.001046 | Perplexity: 1146.828116
2025-10-13 17:38:32,951 Stage: Train 0.5 | Epoch: 36 | Iter: 111400 | Total Loss: 0.004781 | Recon Loss: 0.004270 | Commit Loss: 0.001021 | Perplexity: 1150.775851
2025-10-13 17:43:56,969 Stage: Train 0.5 | Epoch: 36 | Iter: 111600 | Total Loss: 0.004759 | Recon Loss: 0.004242 | Commit Loss: 0.001034 | Perplexity: 1158.526918
2025-10-13 17:49:20,864 Stage: Train 0.5 | Epoch: 36 | Iter: 111800 | Total Loss: 0.004823 | Recon Loss: 0.004323 | Commit Loss: 0.001002 | Perplexity: 1150.071109
2025-10-13 17:54:44,972 Stage: Train 0.5 | Epoch: 36 | Iter: 112000 | Total Loss: 0.004965 | Recon Loss: 0.004445 | Commit Loss: 0.001039 | Perplexity: 1149.453300
2025-10-13 18:00:09,264 Stage: Train 0.5 | Epoch: 36 | Iter: 112200 | Total Loss: 0.004789 | Recon Loss: 0.004264 | Commit Loss: 0.001050 | Perplexity: 1158.689295
2025-10-13 18:05:33,122 Stage: Train 0.5 | Epoch: 36 | Iter: 112400 | Total Loss: 0.004744 | Recon Loss: 0.004248 | Commit Loss: 0.000992 | Perplexity: 1148.401666
Trainning Epoch:  22%|██▏       | 37/165 [50:40:41<175:11:11, 4927.12s/it]Trainning Epoch:  22%|██▏       | 37/165 [50:40:41<175:11:12, 4927.13s/it]2025-10-13 18:11:02,425 Stage: Train 0.5 | Epoch: 37 | Iter: 112600 | Total Loss: 0.004856 | Recon Loss: 0.004359 | Commit Loss: 0.000994 | Perplexity: 1151.033894
2025-10-13 18:16:26,270 Stage: Train 0.5 | Epoch: 37 | Iter: 112800 | Total Loss: 0.004707 | Recon Loss: 0.004207 | Commit Loss: 0.001000 | Perplexity: 1141.305935
2025-10-13 18:21:50,328 Stage: Train 0.5 | Epoch: 37 | Iter: 113000 | Total Loss: 0.004810 | Recon Loss: 0.004299 | Commit Loss: 0.001022 | Perplexity: 1151.782455
2025-10-13 18:27:14,212 Stage: Train 0.5 | Epoch: 37 | Iter: 113200 | Total Loss: 0.004746 | Recon Loss: 0.004238 | Commit Loss: 0.001017 | Perplexity: 1155.416922
2025-10-13 18:32:38,345 Stage: Train 0.5 | Epoch: 37 | Iter: 113400 | Total Loss: 0.004737 | Recon Loss: 0.004226 | Commit Loss: 0.001024 | Perplexity: 1152.019699
2025-10-13 18:38:01,964 Stage: Train 0.5 | Epoch: 37 | Iter: 113600 | Total Loss: 0.004748 | Recon Loss: 0.004224 | Commit Loss: 0.001049 | Perplexity: 1150.769331
2025-10-13 18:43:25,873 Stage: Train 0.5 | Epoch: 37 | Iter: 113800 | Total Loss: 0.004739 | Recon Loss: 0.004234 | Commit Loss: 0.001011 | Perplexity: 1155.498241
2025-10-13 18:48:49,573 Stage: Train 0.5 | Epoch: 37 | Iter: 114000 | Total Loss: 0.004830 | Recon Loss: 0.004323 | Commit Loss: 0.001012 | Perplexity: 1148.731921
2025-10-13 18:54:13,460 Stage: Train 0.5 | Epoch: 37 | Iter: 114200 | Total Loss: 0.004792 | Recon Loss: 0.004281 | Commit Loss: 0.001021 | Perplexity: 1149.750893
2025-10-13 18:59:37,450 Stage: Train 0.5 | Epoch: 37 | Iter: 114400 | Total Loss: 0.004826 | Recon Loss: 0.004314 | Commit Loss: 0.001024 | Perplexity: 1150.045255
2025-10-13 19:05:00,926 Stage: Train 0.5 | Epoch: 37 | Iter: 114600 | Total Loss: 0.004665 | Recon Loss: 0.004145 | Commit Loss: 0.001038 | Perplexity: 1157.567174
2025-10-13 19:10:24,589 Stage: Train 0.5 | Epoch: 37 | Iter: 114800 | Total Loss: 0.004775 | Recon Loss: 0.004257 | Commit Loss: 0.001037 | Perplexity: 1155.782183
2025-10-13 19:15:48,542 Stage: Train 0.5 | Epoch: 37 | Iter: 115000 | Total Loss: 0.004783 | Recon Loss: 0.004262 | Commit Loss: 0.001042 | Perplexity: 1155.512550
2025-10-13 19:21:12,255 Stage: Train 0.5 | Epoch: 37 | Iter: 115200 | Total Loss: 0.004728 | Recon Loss: 0.004215 | Commit Loss: 0.001027 | Perplexity: 1152.979868
2025-10-13 19:26:35,755 Stage: Train 0.5 | Epoch: 37 | Iter: 115400 | Total Loss: 0.004967 | Recon Loss: 0.004439 | Commit Loss: 0.001057 | Perplexity: 1152.360025
Trainning Epoch:  23%|██▎       | 38/165 [52:02:46<173:47:12, 4926.24s/it]Trainning Epoch:  23%|██▎       | 38/165 [52:02:46<173:47:12, 4926.24s/it]2025-10-13 19:32:04,386 Stage: Train 0.5 | Epoch: 38 | Iter: 115600 | Total Loss: 0.004666 | Recon Loss: 0.004155 | Commit Loss: 0.001023 | Perplexity: 1159.633982
2025-10-13 19:37:28,337 Stage: Train 0.5 | Epoch: 38 | Iter: 115800 | Total Loss: 0.004782 | Recon Loss: 0.004270 | Commit Loss: 0.001024 | Perplexity: 1157.099760
2025-10-13 19:42:51,804 Stage: Train 0.5 | Epoch: 38 | Iter: 116000 | Total Loss: 0.004749 | Recon Loss: 0.004238 | Commit Loss: 0.001023 | Perplexity: 1158.466400
2025-10-13 19:48:15,510 Stage: Train 0.5 | Epoch: 38 | Iter: 116200 | Total Loss: 0.004725 | Recon Loss: 0.004203 | Commit Loss: 0.001044 | Perplexity: 1155.375789
2025-10-13 19:53:39,104 Stage: Train 0.5 | Epoch: 38 | Iter: 116400 | Total Loss: 0.004725 | Recon Loss: 0.004216 | Commit Loss: 0.001016 | Perplexity: 1158.425930
2025-10-13 19:59:02,727 Stage: Train 0.5 | Epoch: 38 | Iter: 116600 | Total Loss: 0.004795 | Recon Loss: 0.004286 | Commit Loss: 0.001017 | Perplexity: 1152.626856
2025-10-13 20:04:26,683 Stage: Train 0.5 | Epoch: 38 | Iter: 116800 | Total Loss: 0.004748 | Recon Loss: 0.004229 | Commit Loss: 0.001037 | Perplexity: 1142.742746
2025-10-13 20:09:50,429 Stage: Train 0.5 | Epoch: 38 | Iter: 117000 | Total Loss: 0.004734 | Recon Loss: 0.004224 | Commit Loss: 0.001019 | Perplexity: 1154.081056
2025-10-13 20:15:14,526 Stage: Train 0.5 | Epoch: 38 | Iter: 117200 | Total Loss: 0.004785 | Recon Loss: 0.004269 | Commit Loss: 0.001031 | Perplexity: 1146.932275
2025-10-13 20:20:38,592 Stage: Train 0.5 | Epoch: 38 | Iter: 117400 | Total Loss: 0.004763 | Recon Loss: 0.004251 | Commit Loss: 0.001025 | Perplexity: 1157.997408
2025-10-13 20:26:02,463 Stage: Train 0.5 | Epoch: 38 | Iter: 117600 | Total Loss: 0.004783 | Recon Loss: 0.004251 | Commit Loss: 0.001063 | Perplexity: 1160.895187
2025-10-13 20:31:26,182 Stage: Train 0.5 | Epoch: 38 | Iter: 117800 | Total Loss: 0.004826 | Recon Loss: 0.004305 | Commit Loss: 0.001042 | Perplexity: 1156.477453
2025-10-13 20:36:50,077 Stage: Train 0.5 | Epoch: 38 | Iter: 118000 | Total Loss: 0.004678 | Recon Loss: 0.004162 | Commit Loss: 0.001033 | Perplexity: 1149.411000
2025-10-13 20:42:13,876 Stage: Train 0.5 | Epoch: 38 | Iter: 118200 | Total Loss: 0.005016 | Recon Loss: 0.004462 | Commit Loss: 0.001109 | Perplexity: 1161.279504
2025-10-13 20:47:37,512 Stage: Train 0.5 | Epoch: 38 | Iter: 118400 | Total Loss: 0.004715 | Recon Loss: 0.004222 | Commit Loss: 0.000986 | Perplexity: 1156.539467
Trainning Epoch:  24%|██▎       | 39/165 [53:24:49<172:23:15, 4925.36s/it]Trainning Epoch:  24%|██▎       | 39/165 [53:24:49<172:23:15, 4925.36s/it]2025-10-13 20:53:05,732 Stage: Train 0.5 | Epoch: 39 | Iter: 118600 | Total Loss: 0.004663 | Recon Loss: 0.004160 | Commit Loss: 0.001006 | Perplexity: 1153.216813
2025-10-13 20:58:29,479 Stage: Train 0.5 | Epoch: 39 | Iter: 118800 | Total Loss: 0.004832 | Recon Loss: 0.004314 | Commit Loss: 0.001037 | Perplexity: 1150.136557
2025-10-13 21:03:53,173 Stage: Train 0.5 | Epoch: 39 | Iter: 119000 | Total Loss: 0.004767 | Recon Loss: 0.004254 | Commit Loss: 0.001026 | Perplexity: 1151.494546
2025-10-13 21:09:17,261 Stage: Train 0.5 | Epoch: 39 | Iter: 119200 | Total Loss: 0.004759 | Recon Loss: 0.004246 | Commit Loss: 0.001026 | Perplexity: 1155.540667
2025-10-13 21:14:40,970 Stage: Train 0.5 | Epoch: 39 | Iter: 119400 | Total Loss: 0.004751 | Recon Loss: 0.004227 | Commit Loss: 0.001048 | Perplexity: 1162.483550
2025-10-13 21:20:04,940 Stage: Train 0.5 | Epoch: 39 | Iter: 119600 | Total Loss: 0.004736 | Recon Loss: 0.004240 | Commit Loss: 0.000993 | Perplexity: 1150.595582
2025-10-13 21:25:28,852 Stage: Train 0.5 | Epoch: 39 | Iter: 119800 | Total Loss: 0.004692 | Recon Loss: 0.004183 | Commit Loss: 0.001018 | Perplexity: 1156.532454
2025-10-13 21:30:52,674 Stage: Train 0.5 | Epoch: 39 | Iter: 120000 | Total Loss: 0.004746 | Recon Loss: 0.004224 | Commit Loss: 0.001042 | Perplexity: 1147.644667
2025-10-13 21:30:52,674 Saving model at iteration 120000
2025-10-13 21:30:52,847 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000
2025-10-13 21:30:54,336 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000/model.safetensors
2025-10-13 21:30:56,097 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000/optimizer.bin
2025-10-13 21:30:56,098 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000/scheduler.bin
2025-10-13 21:30:56,098 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000/sampler.bin
2025-10-13 21:30:56,099 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_40_step_120000/random_states_0.pkl
2025-10-13 21:36:20,085 Stage: Train 0.5 | Epoch: 39 | Iter: 120200 | Total Loss: 0.004731 | Recon Loss: 0.004222 | Commit Loss: 0.001018 | Perplexity: 1154.374158
2025-10-13 21:41:44,031 Stage: Train 0.5 | Epoch: 39 | Iter: 120400 | Total Loss: 0.004648 | Recon Loss: 0.004132 | Commit Loss: 0.001033 | Perplexity: 1154.741362
2025-10-13 21:47:08,116 Stage: Train 0.5 | Epoch: 39 | Iter: 120600 | Total Loss: 0.004804 | Recon Loss: 0.004297 | Commit Loss: 0.001015 | Perplexity: 1154.794872
2025-10-13 21:52:32,209 Stage: Train 0.5 | Epoch: 39 | Iter: 120800 | Total Loss: 0.004579 | Recon Loss: 0.004073 | Commit Loss: 0.001013 | Perplexity: 1159.358085
2025-10-13 21:57:55,985 Stage: Train 0.5 | Epoch: 39 | Iter: 121000 | Total Loss: 0.004660 | Recon Loss: 0.004146 | Commit Loss: 0.001029 | Perplexity: 1162.589040
2025-10-13 22:03:19,867 Stage: Train 0.5 | Epoch: 39 | Iter: 121200 | Total Loss: 0.004708 | Recon Loss: 0.004187 | Commit Loss: 0.001041 | Perplexity: 1157.333619
2025-10-13 22:08:43,882 Stage: Train 0.5 | Epoch: 39 | Iter: 121400 | Total Loss: 0.005370 | Recon Loss: 0.004731 | Commit Loss: 0.001277 | Perplexity: 1154.555047
Trainning Epoch:  24%|██▍       | 40/165 [54:46:57<171:02:47, 4926.14s/it]Trainning Epoch:  24%|██▍       | 40/165 [54:46:57<171:02:47, 4926.14s/it]2025-10-13 22:14:12,531 Stage: Train 0.5 | Epoch: 40 | Iter: 121600 | Total Loss: 0.004743 | Recon Loss: 0.004251 | Commit Loss: 0.000984 | Perplexity: 1151.191642
2025-10-13 22:19:36,762 Stage: Train 0.5 | Epoch: 40 | Iter: 121800 | Total Loss: 0.004706 | Recon Loss: 0.004201 | Commit Loss: 0.001010 | Perplexity: 1156.936515
2025-10-13 22:25:00,653 Stage: Train 0.5 | Epoch: 40 | Iter: 122000 | Total Loss: 0.004751 | Recon Loss: 0.004244 | Commit Loss: 0.001013 | Perplexity: 1151.320088
2025-10-13 22:30:24,624 Stage: Train 0.5 | Epoch: 40 | Iter: 122200 | Total Loss: 0.004644 | Recon Loss: 0.004148 | Commit Loss: 0.000993 | Perplexity: 1156.084593
2025-10-13 22:35:48,635 Stage: Train 0.5 | Epoch: 40 | Iter: 122400 | Total Loss: 0.004734 | Recon Loss: 0.004229 | Commit Loss: 0.001009 | Perplexity: 1167.677216
2025-10-13 22:41:12,556 Stage: Train 0.5 | Epoch: 40 | Iter: 122600 | Total Loss: 0.004819 | Recon Loss: 0.004312 | Commit Loss: 0.001015 | Perplexity: 1162.273780
2025-10-13 22:46:36,754 Stage: Train 0.5 | Epoch: 40 | Iter: 122800 | Total Loss: 0.004506 | Recon Loss: 0.004003 | Commit Loss: 0.001006 | Perplexity: 1161.762136
2025-10-13 22:52:00,917 Stage: Train 0.5 | Epoch: 40 | Iter: 123000 | Total Loss: 0.004725 | Recon Loss: 0.004223 | Commit Loss: 0.001005 | Perplexity: 1156.441625
2025-10-13 22:57:24,967 Stage: Train 0.5 | Epoch: 40 | Iter: 123200 | Total Loss: 0.004834 | Recon Loss: 0.004329 | Commit Loss: 0.001010 | Perplexity: 1165.767741
2025-10-13 23:02:48,929 Stage: Train 0.5 | Epoch: 40 | Iter: 123400 | Total Loss: 0.004634 | Recon Loss: 0.004118 | Commit Loss: 0.001033 | Perplexity: 1157.388702
2025-10-13 23:08:12,881 Stage: Train 0.5 | Epoch: 40 | Iter: 123600 | Total Loss: 0.004648 | Recon Loss: 0.004134 | Commit Loss: 0.001028 | Perplexity: 1158.371878
2025-10-13 23:13:36,610 Stage: Train 0.5 | Epoch: 40 | Iter: 123800 | Total Loss: 0.004593 | Recon Loss: 0.004091 | Commit Loss: 0.001004 | Perplexity: 1154.694123
2025-10-13 23:19:00,754 Stage: Train 0.5 | Epoch: 40 | Iter: 124000 | Total Loss: 0.004648 | Recon Loss: 0.004128 | Commit Loss: 0.001039 | Perplexity: 1150.356006
2025-10-13 23:24:24,710 Stage: Train 0.5 | Epoch: 40 | Iter: 124200 | Total Loss: 0.004656 | Recon Loss: 0.004144 | Commit Loss: 0.001025 | Perplexity: 1159.993908
2025-10-13 23:29:48,463 Stage: Train 0.5 | Epoch: 40 | Iter: 124400 | Total Loss: 0.004639 | Recon Loss: 0.004130 | Commit Loss: 0.001019 | Perplexity: 1151.730644
Trainning Epoch:  25%|██▍       | 41/165 [56:09:03<169:40:47, 4926.19s/it]Trainning Epoch:  25%|██▍       | 41/165 [56:09:03<169:40:47, 4926.19s/it]2025-10-13 23:35:17,177 Stage: Train 0.5 | Epoch: 41 | Iter: 124600 | Total Loss: 0.004727 | Recon Loss: 0.004213 | Commit Loss: 0.001027 | Perplexity: 1158.405662
2025-10-13 23:40:41,206 Stage: Train 0.5 | Epoch: 41 | Iter: 124800 | Total Loss: 0.004610 | Recon Loss: 0.004099 | Commit Loss: 0.001022 | Perplexity: 1161.767230
2025-10-13 23:46:05,215 Stage: Train 0.5 | Epoch: 41 | Iter: 125000 | Total Loss: 0.004659 | Recon Loss: 0.004164 | Commit Loss: 0.000989 | Perplexity: 1153.489669
2025-10-13 23:51:29,182 Stage: Train 0.5 | Epoch: 41 | Iter: 125200 | Total Loss: 0.004744 | Recon Loss: 0.004228 | Commit Loss: 0.001031 | Perplexity: 1162.188942
2025-10-13 23:56:53,393 Stage: Train 0.5 | Epoch: 41 | Iter: 125400 | Total Loss: 0.004597 | Recon Loss: 0.004087 | Commit Loss: 0.001020 | Perplexity: 1161.662371
2025-10-14 00:02:17,313 Stage: Train 0.5 | Epoch: 41 | Iter: 125600 | Total Loss: 0.004733 | Recon Loss: 0.004232 | Commit Loss: 0.001002 | Perplexity: 1154.688908
2025-10-14 00:07:41,125 Stage: Train 0.5 | Epoch: 41 | Iter: 125800 | Total Loss: 0.004584 | Recon Loss: 0.004069 | Commit Loss: 0.001031 | Perplexity: 1148.637116
2025-10-14 00:13:04,957 Stage: Train 0.5 | Epoch: 41 | Iter: 126000 | Total Loss: 0.004494 | Recon Loss: 0.003993 | Commit Loss: 0.001003 | Perplexity: 1159.407213
2025-10-14 00:18:28,820 Stage: Train 0.5 | Epoch: 41 | Iter: 126200 | Total Loss: 0.004736 | Recon Loss: 0.004219 | Commit Loss: 0.001034 | Perplexity: 1164.263735
2025-10-14 00:23:52,902 Stage: Train 0.5 | Epoch: 41 | Iter: 126400 | Total Loss: 0.004552 | Recon Loss: 0.004041 | Commit Loss: 0.001022 | Perplexity: 1164.736108
2025-10-14 00:29:17,119 Stage: Train 0.5 | Epoch: 41 | Iter: 126600 | Total Loss: 0.004719 | Recon Loss: 0.004196 | Commit Loss: 0.001045 | Perplexity: 1157.408461
2025-10-14 00:34:41,148 Stage: Train 0.5 | Epoch: 41 | Iter: 126800 | Total Loss: 0.004657 | Recon Loss: 0.004152 | Commit Loss: 0.001009 | Perplexity: 1153.004283
2025-10-14 00:40:05,215 Stage: Train 0.5 | Epoch: 41 | Iter: 127000 | Total Loss: 0.004668 | Recon Loss: 0.004159 | Commit Loss: 0.001018 | Perplexity: 1163.963691
2025-10-14 00:45:29,105 Stage: Train 0.5 | Epoch: 41 | Iter: 127200 | Total Loss: 0.004703 | Recon Loss: 0.004191 | Commit Loss: 0.001025 | Perplexity: 1153.793358
2025-10-14 00:50:53,216 Stage: Train 0.5 | Epoch: 41 | Iter: 127400 | Total Loss: 0.004658 | Recon Loss: 0.004150 | Commit Loss: 0.001017 | Perplexity: 1163.726937
Trainning Epoch:  25%|██▌       | 42/165 [57:31:10<168:18:51, 4926.28s/it]Trainning Epoch:  25%|██▌       | 42/165 [57:31:10<168:18:51, 4926.28s/it]2025-10-14 00:56:22,041 Stage: Train 0.5 | Epoch: 42 | Iter: 127600 | Total Loss: 0.004729 | Recon Loss: 0.004216 | Commit Loss: 0.001026 | Perplexity: 1160.560056
2025-10-14 01:01:46,201 Stage: Train 0.5 | Epoch: 42 | Iter: 127800 | Total Loss: 0.004561 | Recon Loss: 0.004058 | Commit Loss: 0.001005 | Perplexity: 1151.171356
2025-10-14 01:07:10,626 Stage: Train 0.5 | Epoch: 42 | Iter: 128000 | Total Loss: 0.004569 | Recon Loss: 0.004048 | Commit Loss: 0.001042 | Perplexity: 1166.371547
2025-10-14 01:12:34,539 Stage: Train 0.5 | Epoch: 42 | Iter: 128200 | Total Loss: 0.004643 | Recon Loss: 0.004127 | Commit Loss: 0.001032 | Perplexity: 1163.443532
2025-10-14 01:17:58,565 Stage: Train 0.5 | Epoch: 42 | Iter: 128400 | Total Loss: 0.004672 | Recon Loss: 0.004151 | Commit Loss: 0.001042 | Perplexity: 1155.030240
2025-10-14 01:23:22,748 Stage: Train 0.5 | Epoch: 42 | Iter: 128600 | Total Loss: 0.004589 | Recon Loss: 0.004073 | Commit Loss: 0.001031 | Perplexity: 1166.409162
2025-10-14 01:28:46,932 Stage: Train 0.5 | Epoch: 42 | Iter: 128800 | Total Loss: 0.004543 | Recon Loss: 0.004030 | Commit Loss: 0.001026 | Perplexity: 1165.768698
2025-10-14 01:34:10,895 Stage: Train 0.5 | Epoch: 42 | Iter: 129000 | Total Loss: 0.004554 | Recon Loss: 0.004039 | Commit Loss: 0.001029 | Perplexity: 1152.197552
2025-10-14 01:39:35,044 Stage: Train 0.5 | Epoch: 42 | Iter: 129200 | Total Loss: 0.004639 | Recon Loss: 0.004119 | Commit Loss: 0.001040 | Perplexity: 1164.334258
2025-10-14 01:44:58,834 Stage: Train 0.5 | Epoch: 42 | Iter: 129400 | Total Loss: 0.004644 | Recon Loss: 0.004114 | Commit Loss: 0.001061 | Perplexity: 1155.189503
2025-10-14 01:50:22,687 Stage: Train 0.5 | Epoch: 42 | Iter: 129600 | Total Loss: 0.004605 | Recon Loss: 0.004082 | Commit Loss: 0.001046 | Perplexity: 1157.146890
2025-10-14 01:55:46,763 Stage: Train 0.5 | Epoch: 42 | Iter: 129800 | Total Loss: 0.004614 | Recon Loss: 0.004097 | Commit Loss: 0.001034 | Perplexity: 1156.659837
2025-10-14 02:01:10,981 Stage: Train 0.5 | Epoch: 42 | Iter: 130000 | Total Loss: 0.004565 | Recon Loss: 0.004050 | Commit Loss: 0.001030 | Perplexity: 1165.372933
2025-10-14 02:06:34,973 Stage: Train 0.5 | Epoch: 42 | Iter: 130200 | Total Loss: 0.004639 | Recon Loss: 0.004121 | Commit Loss: 0.001036 | Perplexity: 1159.291664
2025-10-14 02:11:59,165 Stage: Train 0.5 | Epoch: 42 | Iter: 130400 | Total Loss: 0.004497 | Recon Loss: 0.004001 | Commit Loss: 0.000992 | Perplexity: 1166.040586
2025-10-14 02:17:23,263 Stage: Train 0.5 | Epoch: 42 | Iter: 130600 | Total Loss: 0.004709 | Recon Loss: 0.004188 | Commit Loss: 0.001043 | Perplexity: 1153.008416
Trainning Epoch:  26%|██▌       | 43/165 [58:53:17<166:57:21, 4926.57s/it]Trainning Epoch:  26%|██▌       | 43/165 [58:53:17<166:57:21, 4926.57s/it]2025-10-14 02:22:51,891 Stage: Train 0.5 | Epoch: 43 | Iter: 130800 | Total Loss: 0.004391 | Recon Loss: 0.003890 | Commit Loss: 0.001001 | Perplexity: 1164.124932
2025-10-14 02:28:16,112 Stage: Train 0.5 | Epoch: 43 | Iter: 131000 | Total Loss: 0.004545 | Recon Loss: 0.004023 | Commit Loss: 0.001044 | Perplexity: 1161.489796
2025-10-14 02:33:40,282 Stage: Train 0.5 | Epoch: 43 | Iter: 131200 | Total Loss: 0.004461 | Recon Loss: 0.003951 | Commit Loss: 0.001020 | Perplexity: 1164.575330
2025-10-14 02:39:04,118 Stage: Train 0.5 | Epoch: 43 | Iter: 131400 | Total Loss: 0.004590 | Recon Loss: 0.004078 | Commit Loss: 0.001024 | Perplexity: 1159.164250
2025-10-14 02:44:28,002 Stage: Train 0.5 | Epoch: 43 | Iter: 131600 | Total Loss: 0.004675 | Recon Loss: 0.004159 | Commit Loss: 0.001032 | Perplexity: 1167.719069
2025-10-14 02:49:51,896 Stage: Train 0.5 | Epoch: 43 | Iter: 131800 | Total Loss: 0.004637 | Recon Loss: 0.004132 | Commit Loss: 0.001010 | Perplexity: 1161.072639
2025-10-14 02:55:15,488 Stage: Train 0.5 | Epoch: 43 | Iter: 132000 | Total Loss: 0.004598 | Recon Loss: 0.004068 | Commit Loss: 0.001059 | Perplexity: 1161.024111
2025-10-14 03:00:39,134 Stage: Train 0.5 | Epoch: 43 | Iter: 132200 | Total Loss: 0.004657 | Recon Loss: 0.004139 | Commit Loss: 0.001036 | Perplexity: 1164.862639
2025-10-14 03:06:03,283 Stage: Train 0.5 | Epoch: 43 | Iter: 132400 | Total Loss: 0.004559 | Recon Loss: 0.004036 | Commit Loss: 0.001045 | Perplexity: 1166.672812
2025-10-14 03:11:27,411 Stage: Train 0.5 | Epoch: 43 | Iter: 132600 | Total Loss: 0.004683 | Recon Loss: 0.004160 | Commit Loss: 0.001047 | Perplexity: 1159.109380
2025-10-14 03:16:51,484 Stage: Train 0.5 | Epoch: 43 | Iter: 132800 | Total Loss: 0.004458 | Recon Loss: 0.003939 | Commit Loss: 0.001037 | Perplexity: 1170.113123
2025-10-14 03:22:15,179 Stage: Train 0.5 | Epoch: 43 | Iter: 133000 | Total Loss: 0.004602 | Recon Loss: 0.004085 | Commit Loss: 0.001034 | Perplexity: 1165.843246
2025-10-14 03:27:39,136 Stage: Train 0.5 | Epoch: 43 | Iter: 133200 | Total Loss: 0.004606 | Recon Loss: 0.004089 | Commit Loss: 0.001034 | Perplexity: 1162.199830
2025-10-14 03:33:03,094 Stage: Train 0.5 | Epoch: 43 | Iter: 133400 | Total Loss: 0.004777 | Recon Loss: 0.004279 | Commit Loss: 0.000997 | Perplexity: 1164.438167
2025-10-14 03:38:26,922 Stage: Train 0.5 | Epoch: 43 | Iter: 133600 | Total Loss: 0.004639 | Recon Loss: 0.004126 | Commit Loss: 0.001026 | Perplexity: 1156.959182
Trainning Epoch:  27%|██▋       | 44/165 [60:15:22<165:34:26, 4926.17s/it]Trainning Epoch:  27%|██▋       | 44/165 [60:15:22<165:34:26, 4926.17s/it]2025-10-14 03:43:55,585 Stage: Train 0.5 | Epoch: 44 | Iter: 133800 | Total Loss: 0.004460 | Recon Loss: 0.003961 | Commit Loss: 0.000999 | Perplexity: 1166.132978
2025-10-14 03:49:19,291 Stage: Train 0.5 | Epoch: 44 | Iter: 134000 | Total Loss: 0.004541 | Recon Loss: 0.004031 | Commit Loss: 0.001019 | Perplexity: 1169.056682
2025-10-14 03:54:43,058 Stage: Train 0.5 | Epoch: 44 | Iter: 134200 | Total Loss: 0.004531 | Recon Loss: 0.004021 | Commit Loss: 0.001020 | Perplexity: 1169.023997
2025-10-14 04:00:07,276 Stage: Train 0.5 | Epoch: 44 | Iter: 134400 | Total Loss: 0.004416 | Recon Loss: 0.003911 | Commit Loss: 0.001010 | Perplexity: 1167.543632
2025-10-14 04:05:31,530 Stage: Train 0.5 | Epoch: 44 | Iter: 134600 | Total Loss: 0.004396 | Recon Loss: 0.003889 | Commit Loss: 0.001015 | Perplexity: 1165.833072
2025-10-14 04:10:55,506 Stage: Train 0.5 | Epoch: 44 | Iter: 134800 | Total Loss: 0.004517 | Recon Loss: 0.004007 | Commit Loss: 0.001022 | Perplexity: 1167.864928
2025-10-14 04:16:19,355 Stage: Train 0.5 | Epoch: 44 | Iter: 135000 | Total Loss: 0.004563 | Recon Loss: 0.004051 | Commit Loss: 0.001023 | Perplexity: 1164.884518
2025-10-14 04:21:43,339 Stage: Train 0.5 | Epoch: 44 | Iter: 135200 | Total Loss: 0.004504 | Recon Loss: 0.003997 | Commit Loss: 0.001015 | Perplexity: 1165.321935
2025-10-14 04:27:07,222 Stage: Train 0.5 | Epoch: 44 | Iter: 135400 | Total Loss: 0.004576 | Recon Loss: 0.004075 | Commit Loss: 0.001003 | Perplexity: 1168.980926
2025-10-14 04:32:31,020 Stage: Train 0.5 | Epoch: 44 | Iter: 135600 | Total Loss: 0.004451 | Recon Loss: 0.003951 | Commit Loss: 0.001000 | Perplexity: 1159.981991
2025-10-14 04:37:54,717 Stage: Train 0.5 | Epoch: 44 | Iter: 135800 | Total Loss: 0.004547 | Recon Loss: 0.004040 | Commit Loss: 0.001013 | Perplexity: 1159.055688
2025-10-14 04:43:18,374 Stage: Train 0.5 | Epoch: 44 | Iter: 136000 | Total Loss: 0.004607 | Recon Loss: 0.004088 | Commit Loss: 0.001038 | Perplexity: 1156.845340
2025-10-14 04:48:42,195 Stage: Train 0.5 | Epoch: 44 | Iter: 136200 | Total Loss: 0.004516 | Recon Loss: 0.003996 | Commit Loss: 0.001040 | Perplexity: 1163.318350
2025-10-14 04:54:05,899 Stage: Train 0.5 | Epoch: 44 | Iter: 136400 | Total Loss: 0.004620 | Recon Loss: 0.004107 | Commit Loss: 0.001025 | Perplexity: 1158.014914
2025-10-14 04:59:29,609 Stage: Train 0.5 | Epoch: 44 | Iter: 136600 | Total Loss: 0.004441 | Recon Loss: 0.003935 | Commit Loss: 0.001014 | Perplexity: 1156.938835
Trainning Epoch:  27%|██▋       | 45/165 [61:37:26<164:11:13, 4925.61s/it]Trainning Epoch:  27%|██▋       | 45/165 [61:37:27<164:11:13, 4925.61s/it]2025-10-14 05:04:58,322 Stage: Train 0.5 | Epoch: 45 | Iter: 136800 | Total Loss: 0.004582 | Recon Loss: 0.004073 | Commit Loss: 0.001018 | Perplexity: 1157.060514
2025-10-14 05:10:22,082 Stage: Train 0.5 | Epoch: 45 | Iter: 137000 | Total Loss: 0.004439 | Recon Loss: 0.003924 | Commit Loss: 0.001030 | Perplexity: 1165.046534
2025-10-14 05:15:45,666 Stage: Train 0.5 | Epoch: 45 | Iter: 137200 | Total Loss: 0.004565 | Recon Loss: 0.004058 | Commit Loss: 0.001015 | Perplexity: 1158.022491
2025-10-14 05:21:09,515 Stage: Train 0.5 | Epoch: 45 | Iter: 137400 | Total Loss: 0.004501 | Recon Loss: 0.003984 | Commit Loss: 0.001034 | Perplexity: 1162.238657
2025-10-14 05:26:33,450 Stage: Train 0.5 | Epoch: 45 | Iter: 137600 | Total Loss: 0.004579 | Recon Loss: 0.004069 | Commit Loss: 0.001020 | Perplexity: 1163.405188
2025-10-14 05:31:57,320 Stage: Train 0.5 | Epoch: 45 | Iter: 137800 | Total Loss: 0.004521 | Recon Loss: 0.004009 | Commit Loss: 0.001024 | Perplexity: 1159.644678
2025-10-14 05:37:20,911 Stage: Train 0.5 | Epoch: 45 | Iter: 138000 | Total Loss: 0.004495 | Recon Loss: 0.003985 | Commit Loss: 0.001020 | Perplexity: 1160.831414
2025-10-14 05:42:44,833 Stage: Train 0.5 | Epoch: 45 | Iter: 138200 | Total Loss: 0.004532 | Recon Loss: 0.004024 | Commit Loss: 0.001017 | Perplexity: 1165.616658
2025-10-14 05:48:08,816 Stage: Train 0.5 | Epoch: 45 | Iter: 138400 | Total Loss: 0.004591 | Recon Loss: 0.004067 | Commit Loss: 0.001047 | Perplexity: 1162.710814
2025-10-14 05:53:32,712 Stage: Train 0.5 | Epoch: 45 | Iter: 138600 | Total Loss: 0.004506 | Recon Loss: 0.003994 | Commit Loss: 0.001024 | Perplexity: 1161.467126
2025-10-14 05:58:56,969 Stage: Train 0.5 | Epoch: 45 | Iter: 138800 | Total Loss: 0.004561 | Recon Loss: 0.004049 | Commit Loss: 0.001025 | Perplexity: 1166.934724
2025-10-14 06:04:21,170 Stage: Train 0.5 | Epoch: 45 | Iter: 139000 | Total Loss: 0.004547 | Recon Loss: 0.004037 | Commit Loss: 0.001020 | Perplexity: 1168.108465
2025-10-14 06:09:45,097 Stage: Train 0.5 | Epoch: 45 | Iter: 139200 | Total Loss: 0.004438 | Recon Loss: 0.003938 | Commit Loss: 0.000999 | Perplexity: 1157.348965
2025-10-14 06:15:09,164 Stage: Train 0.5 | Epoch: 45 | Iter: 139400 | Total Loss: 0.004533 | Recon Loss: 0.004025 | Commit Loss: 0.001015 | Perplexity: 1163.210139
2025-10-14 06:20:33,045 Stage: Train 0.5 | Epoch: 45 | Iter: 139600 | Total Loss: 0.004437 | Recon Loss: 0.003931 | Commit Loss: 0.001011 | Perplexity: 1163.172086
Trainning Epoch:  28%|██▊       | 46/165 [62:59:31<162:48:39, 4925.37s/it]Trainning Epoch:  28%|██▊       | 46/165 [62:59:31<162:48:39, 4925.38s/it]2025-10-14 06:26:01,630 Stage: Train 0.5 | Epoch: 46 | Iter: 139800 | Total Loss: 0.004546 | Recon Loss: 0.004034 | Commit Loss: 0.001023 | Perplexity: 1161.486872
2025-10-14 06:31:25,742 Stage: Train 0.5 | Epoch: 46 | Iter: 140000 | Total Loss: 0.004441 | Recon Loss: 0.003927 | Commit Loss: 0.001028 | Perplexity: 1164.956328
2025-10-14 06:31:25,742 Saving model at iteration 140000
2025-10-14 06:31:25,935 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000
2025-10-14 06:31:27,395 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000/model.safetensors
2025-10-14 06:31:29,219 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000/optimizer.bin
2025-10-14 06:31:29,219 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000/scheduler.bin
2025-10-14 06:31:29,219 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000/sampler.bin
2025-10-14 06:31:29,220 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_47_step_140000/random_states_0.pkl
2025-10-14 06:36:53,084 Stage: Train 0.5 | Epoch: 46 | Iter: 140200 | Total Loss: 0.004453 | Recon Loss: 0.003949 | Commit Loss: 0.001008 | Perplexity: 1153.839627
2025-10-14 06:42:16,805 Stage: Train 0.5 | Epoch: 46 | Iter: 140400 | Total Loss: 0.004489 | Recon Loss: 0.003986 | Commit Loss: 0.001007 | Perplexity: 1168.653316
2025-10-14 06:47:40,548 Stage: Train 0.5 | Epoch: 46 | Iter: 140600 | Total Loss: 0.004515 | Recon Loss: 0.003989 | Commit Loss: 0.001052 | Perplexity: 1170.411332
2025-10-14 06:53:04,374 Stage: Train 0.5 | Epoch: 46 | Iter: 140800 | Total Loss: 0.004448 | Recon Loss: 0.003927 | Commit Loss: 0.001041 | Perplexity: 1170.410686
2025-10-14 06:58:28,431 Stage: Train 0.5 | Epoch: 46 | Iter: 141000 | Total Loss: 0.004469 | Recon Loss: 0.003964 | Commit Loss: 0.001010 | Perplexity: 1163.694175
2025-10-14 07:03:52,493 Stage: Train 0.5 | Epoch: 46 | Iter: 141200 | Total Loss: 0.004472 | Recon Loss: 0.003955 | Commit Loss: 0.001033 | Perplexity: 1159.407981
2025-10-14 07:09:16,557 Stage: Train 0.5 | Epoch: 46 | Iter: 141400 | Total Loss: 0.004523 | Recon Loss: 0.004012 | Commit Loss: 0.001022 | Perplexity: 1160.909326
2025-10-14 07:14:40,526 Stage: Train 0.5 | Epoch: 46 | Iter: 141600 | Total Loss: 0.004384 | Recon Loss: 0.003884 | Commit Loss: 0.000999 | Perplexity: 1164.137189
2025-10-14 07:20:04,358 Stage: Train 0.5 | Epoch: 46 | Iter: 141800 | Total Loss: 0.004552 | Recon Loss: 0.004041 | Commit Loss: 0.001021 | Perplexity: 1165.129428
2025-10-14 07:25:28,562 Stage: Train 0.5 | Epoch: 46 | Iter: 142000 | Total Loss: 0.004470 | Recon Loss: 0.003969 | Commit Loss: 0.001000 | Perplexity: 1165.418641
2025-10-14 07:30:52,559 Stage: Train 0.5 | Epoch: 46 | Iter: 142200 | Total Loss: 0.004392 | Recon Loss: 0.003893 | Commit Loss: 0.000999 | Perplexity: 1163.988542
2025-10-14 07:36:16,502 Stage: Train 0.5 | Epoch: 46 | Iter: 142400 | Total Loss: 0.004561 | Recon Loss: 0.004049 | Commit Loss: 0.001023 | Perplexity: 1167.057335
2025-10-14 07:41:40,478 Stage: Train 0.5 | Epoch: 46 | Iter: 142600 | Total Loss: 0.004417 | Recon Loss: 0.003908 | Commit Loss: 0.001017 | Perplexity: 1164.145089
Trainning Epoch:  28%|██▊       | 47/165 [64:21:41<161:29:03, 4926.64s/it]Trainning Epoch:  28%|██▊       | 47/165 [64:21:41<161:29:03, 4926.64s/it]2025-10-14 07:47:09,646 Stage: Train 0.5 | Epoch: 47 | Iter: 142800 | Total Loss: 0.004510 | Recon Loss: 0.003988 | Commit Loss: 0.001044 | Perplexity: 1168.723611
2025-10-14 07:52:33,891 Stage: Train 0.5 | Epoch: 47 | Iter: 143000 | Total Loss: 0.004535 | Recon Loss: 0.004020 | Commit Loss: 0.001029 | Perplexity: 1159.545731
2025-10-14 07:57:58,018 Stage: Train 0.5 | Epoch: 47 | Iter: 143200 | Total Loss: 0.004518 | Recon Loss: 0.004004 | Commit Loss: 0.001028 | Perplexity: 1163.827810
2025-10-14 08:03:21,982 Stage: Train 0.5 | Epoch: 47 | Iter: 143400 | Total Loss: 0.004411 | Recon Loss: 0.003900 | Commit Loss: 0.001023 | Perplexity: 1158.279619
2025-10-14 08:08:45,581 Stage: Train 0.5 | Epoch: 47 | Iter: 143600 | Total Loss: 0.004462 | Recon Loss: 0.003948 | Commit Loss: 0.001027 | Perplexity: 1170.503382
2025-10-14 08:14:09,545 Stage: Train 0.5 | Epoch: 47 | Iter: 143800 | Total Loss: 0.004417 | Recon Loss: 0.003911 | Commit Loss: 0.001011 | Perplexity: 1163.976222
2025-10-14 08:19:33,227 Stage: Train 0.5 | Epoch: 47 | Iter: 144000 | Total Loss: 0.004425 | Recon Loss: 0.003914 | Commit Loss: 0.001022 | Perplexity: 1167.046074
2025-10-14 08:24:57,147 Stage: Train 0.5 | Epoch: 47 | Iter: 144200 | Total Loss: 0.004503 | Recon Loss: 0.003998 | Commit Loss: 0.001010 | Perplexity: 1164.105154
2025-10-14 08:30:20,912 Stage: Train 0.5 | Epoch: 47 | Iter: 144400 | Total Loss: 0.004439 | Recon Loss: 0.003933 | Commit Loss: 0.001011 | Perplexity: 1173.082961
2025-10-14 08:35:44,972 Stage: Train 0.5 | Epoch: 47 | Iter: 144600 | Total Loss: 0.004453 | Recon Loss: 0.003944 | Commit Loss: 0.001019 | Perplexity: 1169.352417
2025-10-14 08:41:09,374 Stage: Train 0.5 | Epoch: 47 | Iter: 144800 | Total Loss: 0.004374 | Recon Loss: 0.003866 | Commit Loss: 0.001016 | Perplexity: 1167.151816
2025-10-14 08:46:34,578 Stage: Train 0.5 | Epoch: 47 | Iter: 145000 | Total Loss: 0.004423 | Recon Loss: 0.003917 | Commit Loss: 0.001013 | Perplexity: 1172.117949
2025-10-14 08:51:58,656 Stage: Train 0.5 | Epoch: 47 | Iter: 145200 | Total Loss: 0.004585 | Recon Loss: 0.004052 | Commit Loss: 0.001067 | Perplexity: 1174.829590
2025-10-14 08:57:22,865 Stage: Train 0.5 | Epoch: 47 | Iter: 145400 | Total Loss: 0.004490 | Recon Loss: 0.003982 | Commit Loss: 0.001016 | Perplexity: 1164.193954
2025-10-14 09:02:47,354 Stage: Train 0.5 | Epoch: 47 | Iter: 145600 | Total Loss: 0.004409 | Recon Loss: 0.003896 | Commit Loss: 0.001026 | Perplexity: 1164.607448
2025-10-14 09:08:11,413 Stage: Train 0.5 | Epoch: 47 | Iter: 145800 | Total Loss: 0.004432 | Recon Loss: 0.003923 | Commit Loss: 0.001018 | Perplexity: 1162.584211
Trainning Epoch:  29%|██▉       | 48/165 [65:43:49<160:07:43, 4927.04s/it]Trainning Epoch:  29%|██▉       | 48/165 [65:43:49<160:07:43, 4927.04s/it]2025-10-14 09:13:39,862 Stage: Train 0.5 | Epoch: 48 | Iter: 146000 | Total Loss: 0.004542 | Recon Loss: 0.004043 | Commit Loss: 0.000998 | Perplexity: 1159.851359
2025-10-14 09:19:04,299 Stage: Train 0.5 | Epoch: 48 | Iter: 146200 | Total Loss: 0.004505 | Recon Loss: 0.003990 | Commit Loss: 0.001031 | Perplexity: 1165.039260
2025-10-14 09:24:28,443 Stage: Train 0.5 | Epoch: 48 | Iter: 146400 | Total Loss: 0.004438 | Recon Loss: 0.003923 | Commit Loss: 0.001029 | Perplexity: 1165.451518
2025-10-14 09:29:52,410 Stage: Train 0.5 | Epoch: 48 | Iter: 146600 | Total Loss: 0.004357 | Recon Loss: 0.003854 | Commit Loss: 0.001007 | Perplexity: 1167.622155
2025-10-14 09:35:16,216 Stage: Train 0.5 | Epoch: 48 | Iter: 146800 | Total Loss: 0.004407 | Recon Loss: 0.003889 | Commit Loss: 0.001036 | Perplexity: 1159.467599
2025-10-14 09:40:40,208 Stage: Train 0.5 | Epoch: 48 | Iter: 147000 | Total Loss: 0.004518 | Recon Loss: 0.003999 | Commit Loss: 0.001039 | Perplexity: 1166.264795
2025-10-14 09:46:03,871 Stage: Train 0.5 | Epoch: 48 | Iter: 147200 | Total Loss: 0.004463 | Recon Loss: 0.003940 | Commit Loss: 0.001045 | Perplexity: 1161.533224
2025-10-14 09:51:27,697 Stage: Train 0.5 | Epoch: 48 | Iter: 147400 | Total Loss: 0.004399 | Recon Loss: 0.003890 | Commit Loss: 0.001019 | Perplexity: 1170.524655
2025-10-14 09:56:51,366 Stage: Train 0.5 | Epoch: 48 | Iter: 147600 | Total Loss: 0.004483 | Recon Loss: 0.003968 | Commit Loss: 0.001029 | Perplexity: 1169.738571
2025-10-14 10:02:15,290 Stage: Train 0.5 | Epoch: 48 | Iter: 147800 | Total Loss: 0.004367 | Recon Loss: 0.003867 | Commit Loss: 0.001001 | Perplexity: 1171.188321
2025-10-14 10:07:39,268 Stage: Train 0.5 | Epoch: 48 | Iter: 148000 | Total Loss: 0.004495 | Recon Loss: 0.003992 | Commit Loss: 0.001005 | Perplexity: 1165.920718
2025-10-14 10:13:02,916 Stage: Train 0.5 | Epoch: 48 | Iter: 148200 | Total Loss: 0.004371 | Recon Loss: 0.003847 | Commit Loss: 0.001048 | Perplexity: 1166.360370
2025-10-14 10:18:26,815 Stage: Train 0.5 | Epoch: 48 | Iter: 148400 | Total Loss: 0.004443 | Recon Loss: 0.003933 | Commit Loss: 0.001021 | Perplexity: 1168.002069
2025-10-14 10:23:50,674 Stage: Train 0.5 | Epoch: 48 | Iter: 148600 | Total Loss: 0.004465 | Recon Loss: 0.003957 | Commit Loss: 0.001016 | Perplexity: 1163.513190
2025-10-14 10:29:14,507 Stage: Train 0.5 | Epoch: 48 | Iter: 148800 | Total Loss: 0.004451 | Recon Loss: 0.003937 | Commit Loss: 0.001028 | Perplexity: 1163.668095
Trainning Epoch:  30%|██▉       | 49/165 [67:05:54<158:44:14, 4926.34s/it]Trainning Epoch:  30%|██▉       | 49/165 [67:05:54<158:44:15, 4926.34s/it]2025-10-14 10:34:43,040 Stage: Train 0.5 | Epoch: 49 | Iter: 149000 | Total Loss: 0.004427 | Recon Loss: 0.003908 | Commit Loss: 0.001039 | Perplexity: 1168.970343
2025-10-14 10:40:06,898 Stage: Train 0.5 | Epoch: 49 | Iter: 149200 | Total Loss: 0.004368 | Recon Loss: 0.003853 | Commit Loss: 0.001030 | Perplexity: 1167.227196
2025-10-14 10:45:30,978 Stage: Train 0.5 | Epoch: 49 | Iter: 149400 | Total Loss: 0.004496 | Recon Loss: 0.003982 | Commit Loss: 0.001029 | Perplexity: 1162.914855
2025-10-14 10:50:55,011 Stage: Train 0.5 | Epoch: 49 | Iter: 149600 | Total Loss: 0.004510 | Recon Loss: 0.004003 | Commit Loss: 0.001013 | Perplexity: 1158.632677
2025-10-14 10:56:18,796 Stage: Train 0.5 | Epoch: 49 | Iter: 149800 | Total Loss: 0.004365 | Recon Loss: 0.003865 | Commit Loss: 0.001001 | Perplexity: 1167.198466
2025-10-14 11:01:42,899 Stage: Train 0.5 | Epoch: 49 | Iter: 150000 | Total Loss: 0.004462 | Recon Loss: 0.003947 | Commit Loss: 0.001031 | Perplexity: 1166.320057
2025-10-14 11:07:06,894 Stage: Train 0.5 | Epoch: 49 | Iter: 150200 | Total Loss: 0.004420 | Recon Loss: 0.003909 | Commit Loss: 0.001023 | Perplexity: 1162.952603
2025-10-14 11:12:30,693 Stage: Train 0.5 | Epoch: 49 | Iter: 150400 | Total Loss: 0.004356 | Recon Loss: 0.003845 | Commit Loss: 0.001020 | Perplexity: 1170.711681
2025-10-14 11:17:54,809 Stage: Train 0.5 | Epoch: 49 | Iter: 150600 | Total Loss: 0.004422 | Recon Loss: 0.003918 | Commit Loss: 0.001009 | Perplexity: 1165.909735
2025-10-14 11:23:18,910 Stage: Train 0.5 | Epoch: 49 | Iter: 150800 | Total Loss: 0.004389 | Recon Loss: 0.003871 | Commit Loss: 0.001037 | Perplexity: 1175.254865
2025-10-14 11:28:42,714 Stage: Train 0.5 | Epoch: 49 | Iter: 151000 | Total Loss: 0.004409 | Recon Loss: 0.003891 | Commit Loss: 0.001036 | Perplexity: 1168.238695
2025-10-14 11:34:06,553 Stage: Train 0.5 | Epoch: 49 | Iter: 151200 | Total Loss: 0.004412 | Recon Loss: 0.003902 | Commit Loss: 0.001019 | Perplexity: 1169.504887
2025-10-14 11:39:30,505 Stage: Train 0.5 | Epoch: 49 | Iter: 151400 | Total Loss: 0.004377 | Recon Loss: 0.003855 | Commit Loss: 0.001045 | Perplexity: 1166.237561
2025-10-14 11:44:54,340 Stage: Train 0.5 | Epoch: 49 | Iter: 151600 | Total Loss: 0.004480 | Recon Loss: 0.003960 | Commit Loss: 0.001040 | Perplexity: 1172.492941
2025-10-14 11:50:18,392 Stage: Train 0.5 | Epoch: 49 | Iter: 151800 | Total Loss: 0.004392 | Recon Loss: 0.003865 | Commit Loss: 0.001054 | Perplexity: 1170.206021
Trainning Epoch:  30%|███       | 50/165 [68:27:59<157:21:30, 4926.00s/it]Trainning Epoch:  30%|███       | 50/165 [68:27:59<157:21:30, 4926.00s/it]2025-10-14 11:55:46,583 Stage: Train 0.5 | Epoch: 50 | Iter: 152000 | Total Loss: 0.004451 | Recon Loss: 0.003931 | Commit Loss: 0.001040 | Perplexity: 1167.366508
2025-10-14 12:01:10,467 Stage: Train 0.5 | Epoch: 50 | Iter: 152200 | Total Loss: 0.004372 | Recon Loss: 0.003856 | Commit Loss: 0.001031 | Perplexity: 1165.639465
2025-10-14 12:06:34,448 Stage: Train 0.5 | Epoch: 50 | Iter: 152400 | Total Loss: 0.004426 | Recon Loss: 0.003910 | Commit Loss: 0.001031 | Perplexity: 1174.088582
2025-10-14 12:11:58,388 Stage: Train 0.5 | Epoch: 50 | Iter: 152600 | Total Loss: 0.004326 | Recon Loss: 0.003817 | Commit Loss: 0.001019 | Perplexity: 1173.323081
2025-10-14 12:17:22,525 Stage: Train 0.5 | Epoch: 50 | Iter: 152800 | Total Loss: 0.004315 | Recon Loss: 0.003812 | Commit Loss: 0.001007 | Perplexity: 1165.263393
2025-10-14 12:22:46,663 Stage: Train 0.5 | Epoch: 50 | Iter: 153000 | Total Loss: 0.005476 | Recon Loss: 0.004573 | Commit Loss: 0.001806 | Perplexity: 1168.994715
2025-10-14 12:28:10,683 Stage: Train 0.5 | Epoch: 50 | Iter: 153200 | Total Loss: 0.004557 | Recon Loss: 0.004047 | Commit Loss: 0.001020 | Perplexity: 1150.732638
2025-10-14 12:33:34,446 Stage: Train 0.5 | Epoch: 50 | Iter: 153400 | Total Loss: 0.004269 | Recon Loss: 0.003770 | Commit Loss: 0.000999 | Perplexity: 1169.671469
2025-10-14 12:38:58,226 Stage: Train 0.5 | Epoch: 50 | Iter: 153600 | Total Loss: 0.004442 | Recon Loss: 0.003940 | Commit Loss: 0.001005 | Perplexity: 1171.568430
2025-10-14 12:44:22,109 Stage: Train 0.5 | Epoch: 50 | Iter: 153800 | Total Loss: 0.004314 | Recon Loss: 0.003821 | Commit Loss: 0.000986 | Perplexity: 1171.156324
2025-10-14 12:49:46,412 Stage: Train 0.5 | Epoch: 50 | Iter: 154000 | Total Loss: 0.004367 | Recon Loss: 0.003848 | Commit Loss: 0.001038 | Perplexity: 1169.908187
2025-10-14 12:55:10,608 Stage: Train 0.5 | Epoch: 50 | Iter: 154200 | Total Loss: 0.004427 | Recon Loss: 0.003904 | Commit Loss: 0.001046 | Perplexity: 1171.446739
2025-10-14 13:00:34,630 Stage: Train 0.5 | Epoch: 50 | Iter: 154400 | Total Loss: 0.004425 | Recon Loss: 0.003921 | Commit Loss: 0.001008 | Perplexity: 1166.345304
2025-10-14 13:05:58,427 Stage: Train 0.5 | Epoch: 50 | Iter: 154600 | Total Loss: 0.004355 | Recon Loss: 0.003840 | Commit Loss: 0.001029 | Perplexity: 1167.716782
2025-10-14 13:11:22,498 Stage: Train 0.5 | Epoch: 50 | Iter: 154800 | Total Loss: 0.004400 | Recon Loss: 0.003888 | Commit Loss: 0.001024 | Perplexity: 1167.653189
Trainning Epoch:  31%|███       | 51/165 [69:50:05<155:59:32, 4926.08s/it]Trainning Epoch:  31%|███       | 51/165 [69:50:05<155:59:33, 4926.08s/it]2025-10-14 13:16:51,742 Stage: Train 0.5 | Epoch: 51 | Iter: 155000 | Total Loss: 0.004914 | Recon Loss: 0.004258 | Commit Loss: 0.001310 | Perplexity: 1172.297567
2025-10-14 13:22:15,945 Stage: Train 0.5 | Epoch: 51 | Iter: 155200 | Total Loss: 0.004417 | Recon Loss: 0.003916 | Commit Loss: 0.001002 | Perplexity: 1165.868804
2025-10-14 13:27:56,524 Stage: Train 0.5 | Epoch: 51 | Iter: 155400 | Total Loss: 0.004338 | Recon Loss: 0.003830 | Commit Loss: 0.001018 | Perplexity: 1174.538559
2025-10-14 13:33:26,918 Stage: Train 0.5 | Epoch: 51 | Iter: 155600 | Total Loss: 0.004449 | Recon Loss: 0.003940 | Commit Loss: 0.001017 | Perplexity: 1160.289877
2025-10-14 13:38:50,891 Stage: Train 0.5 | Epoch: 51 | Iter: 155800 | Total Loss: 0.004326 | Recon Loss: 0.003827 | Commit Loss: 0.000999 | Perplexity: 1168.190681
2025-10-14 13:44:14,831 Stage: Train 0.5 | Epoch: 51 | Iter: 156000 | Total Loss: 0.004453 | Recon Loss: 0.003949 | Commit Loss: 0.001007 | Perplexity: 1178.335904
2025-10-14 13:49:38,593 Stage: Train 0.5 | Epoch: 51 | Iter: 156200 | Total Loss: 0.004392 | Recon Loss: 0.003876 | Commit Loss: 0.001032 | Perplexity: 1165.276085
2025-10-14 13:55:02,610 Stage: Train 0.5 | Epoch: 51 | Iter: 156400 | Total Loss: 0.004384 | Recon Loss: 0.003875 | Commit Loss: 0.001017 | Perplexity: 1173.070359
2025-10-14 14:00:26,461 Stage: Train 0.5 | Epoch: 51 | Iter: 156600 | Total Loss: 0.004412 | Recon Loss: 0.003902 | Commit Loss: 0.001021 | Perplexity: 1167.443305
2025-10-14 14:05:50,689 Stage: Train 0.5 | Epoch: 51 | Iter: 156800 | Total Loss: 0.004367 | Recon Loss: 0.003859 | Commit Loss: 0.001015 | Perplexity: 1170.804953
2025-10-14 14:11:14,578 Stage: Train 0.5 | Epoch: 51 | Iter: 157000 | Total Loss: 0.004280 | Recon Loss: 0.003777 | Commit Loss: 0.001006 | Perplexity: 1168.939241
2025-10-14 14:16:38,615 Stage: Train 0.5 | Epoch: 51 | Iter: 157200 | Total Loss: 0.004549 | Recon Loss: 0.004034 | Commit Loss: 0.001029 | Perplexity: 1162.967334
2025-10-14 14:22:02,496 Stage: Train 0.5 | Epoch: 51 | Iter: 157400 | Total Loss: 0.004332 | Recon Loss: 0.003815 | Commit Loss: 0.001036 | Perplexity: 1175.633830
2025-10-14 14:27:26,647 Stage: Train 0.5 | Epoch: 51 | Iter: 157600 | Total Loss: 0.004407 | Recon Loss: 0.003891 | Commit Loss: 0.001033 | Perplexity: 1162.153566
2025-10-14 14:32:50,581 Stage: Train 0.5 | Epoch: 51 | Iter: 157800 | Total Loss: 0.004315 | Recon Loss: 0.003800 | Commit Loss: 0.001029 | Perplexity: 1171.810201
Trainning Epoch:  32%|███▏      | 52/165 [71:12:34<154:50:31, 4933.02s/it]Trainning Epoch:  32%|███▏      | 52/165 [71:12:34<154:50:31, 4933.02s/it]2025-10-14 14:38:19,053 Stage: Train 0.5 | Epoch: 52 | Iter: 158000 | Total Loss: 0.004284 | Recon Loss: 0.003777 | Commit Loss: 0.001012 | Perplexity: 1168.366067
2025-10-14 14:43:43,202 Stage: Train 0.5 | Epoch: 52 | Iter: 158200 | Total Loss: 0.004244 | Recon Loss: 0.003735 | Commit Loss: 0.001019 | Perplexity: 1167.304725
2025-10-14 14:49:07,318 Stage: Train 0.5 | Epoch: 52 | Iter: 158400 | Total Loss: 0.004470 | Recon Loss: 0.003948 | Commit Loss: 0.001043 | Perplexity: 1165.609639
2025-10-14 14:54:31,664 Stage: Train 0.5 | Epoch: 52 | Iter: 158600 | Total Loss: 0.004320 | Recon Loss: 0.003802 | Commit Loss: 0.001034 | Perplexity: 1170.056058
2025-10-14 14:59:55,683 Stage: Train 0.5 | Epoch: 52 | Iter: 158800 | Total Loss: 0.004320 | Recon Loss: 0.003808 | Commit Loss: 0.001022 | Perplexity: 1173.194603
2025-10-14 15:05:19,759 Stage: Train 0.5 | Epoch: 52 | Iter: 159000 | Total Loss: 0.004288 | Recon Loss: 0.003778 | Commit Loss: 0.001019 | Perplexity: 1164.326660
2025-10-14 15:10:43,964 Stage: Train 0.5 | Epoch: 52 | Iter: 159200 | Total Loss: 0.004403 | Recon Loss: 0.003884 | Commit Loss: 0.001037 | Perplexity: 1169.564947
2025-10-14 15:16:08,163 Stage: Train 0.5 | Epoch: 52 | Iter: 159400 | Total Loss: 0.004349 | Recon Loss: 0.003833 | Commit Loss: 0.001031 | Perplexity: 1161.907580
2025-10-14 15:21:32,220 Stage: Train 0.5 | Epoch: 52 | Iter: 159600 | Total Loss: 0.004353 | Recon Loss: 0.003844 | Commit Loss: 0.001018 | Perplexity: 1168.075636
2025-10-14 15:26:55,311 Stage: Train 0.5 | Epoch: 52 | Iter: 159800 | Total Loss: 0.004329 | Recon Loss: 0.003815 | Commit Loss: 0.001028 | Perplexity: 1167.933691
2025-10-14 15:32:18,412 Stage: Train 0.5 | Epoch: 52 | Iter: 160000 | Total Loss: 0.004208 | Recon Loss: 0.003709 | Commit Loss: 0.000997 | Perplexity: 1167.885256
2025-10-14 15:32:18,412 Saving model at iteration 160000
2025-10-14 15:32:18,630 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000
2025-10-14 15:32:20,068 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000/model.safetensors
2025-10-14 15:32:21,923 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000/optimizer.bin
2025-10-14 15:32:21,924 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000/scheduler.bin
2025-10-14 15:32:21,924 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000/sampler.bin
2025-10-14 15:32:21,925 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_53_step_160000/random_states_0.pkl
2025-10-14 15:37:45,073 Stage: Train 0.5 | Epoch: 52 | Iter: 160200 | Total Loss: 0.004411 | Recon Loss: 0.003895 | Commit Loss: 0.001032 | Perplexity: 1167.258085
2025-10-14 15:43:08,442 Stage: Train 0.5 | Epoch: 52 | Iter: 160400 | Total Loss: 0.004190 | Recon Loss: 0.003694 | Commit Loss: 0.000992 | Perplexity: 1171.091081
2025-10-14 15:48:31,899 Stage: Train 0.5 | Epoch: 52 | Iter: 160600 | Total Loss: 0.004375 | Recon Loss: 0.003849 | Commit Loss: 0.001053 | Perplexity: 1183.501726
2025-10-14 15:53:55,313 Stage: Train 0.5 | Epoch: 52 | Iter: 160800 | Total Loss: 0.004366 | Recon Loss: 0.003846 | Commit Loss: 0.001039 | Perplexity: 1173.390113
2025-10-14 15:59:18,229 Stage: Train 0.5 | Epoch: 52 | Iter: 161000 | Total Loss: 0.004338 | Recon Loss: 0.003828 | Commit Loss: 0.001020 | Perplexity: 1170.717899
Trainning Epoch:  32%|███▏      | 53/165 [72:34:39<153:23:54, 4930.66s/it]Trainning Epoch:  32%|███▏      | 53/165 [72:34:39<153:23:54, 4930.66s/it]2025-10-14 16:04:45,670 Stage: Train 0.5 | Epoch: 53 | Iter: 161200 | Total Loss: 0.004260 | Recon Loss: 0.003752 | Commit Loss: 0.001014 | Perplexity: 1174.328724
2025-10-14 16:10:09,238 Stage: Train 0.5 | Epoch: 53 | Iter: 161400 | Total Loss: 0.004368 | Recon Loss: 0.003854 | Commit Loss: 0.001027 | Perplexity: 1171.532636
2025-10-14 16:15:32,306 Stage: Train 0.5 | Epoch: 53 | Iter: 161600 | Total Loss: 0.004368 | Recon Loss: 0.003853 | Commit Loss: 0.001029 | Perplexity: 1166.244135
2025-10-14 16:20:55,127 Stage: Train 0.5 | Epoch: 53 | Iter: 161800 | Total Loss: 0.004257 | Recon Loss: 0.003739 | Commit Loss: 0.001037 | Perplexity: 1170.579828
2025-10-14 16:26:18,435 Stage: Train 0.5 | Epoch: 53 | Iter: 162000 | Total Loss: 0.004417 | Recon Loss: 0.003898 | Commit Loss: 0.001037 | Perplexity: 1175.573253
2025-10-14 16:31:41,375 Stage: Train 0.5 | Epoch: 53 | Iter: 162200 | Total Loss: 0.004192 | Recon Loss: 0.003691 | Commit Loss: 0.001002 | Perplexity: 1177.660479
2025-10-14 16:37:04,625 Stage: Train 0.5 | Epoch: 53 | Iter: 162400 | Total Loss: 0.004279 | Recon Loss: 0.003763 | Commit Loss: 0.001031 | Perplexity: 1171.509760
2025-10-14 16:42:27,765 Stage: Train 0.5 | Epoch: 53 | Iter: 162600 | Total Loss: 0.004426 | Recon Loss: 0.003911 | Commit Loss: 0.001030 | Perplexity: 1167.624332
2025-10-14 16:47:50,648 Stage: Train 0.5 | Epoch: 53 | Iter: 162800 | Total Loss: 0.004260 | Recon Loss: 0.003738 | Commit Loss: 0.001044 | Perplexity: 1167.561143
2025-10-14 16:53:13,754 Stage: Train 0.5 | Epoch: 53 | Iter: 163000 | Total Loss: 0.004332 | Recon Loss: 0.003801 | Commit Loss: 0.001062 | Perplexity: 1178.580835
2025-10-14 16:58:36,980 Stage: Train 0.5 | Epoch: 53 | Iter: 163200 | Total Loss: 0.004322 | Recon Loss: 0.003807 | Commit Loss: 0.001030 | Perplexity: 1167.126645
2025-10-14 17:04:00,286 Stage: Train 0.5 | Epoch: 53 | Iter: 163400 | Total Loss: 0.004282 | Recon Loss: 0.003772 | Commit Loss: 0.001020 | Perplexity: 1169.361019
2025-10-14 17:09:23,522 Stage: Train 0.5 | Epoch: 53 | Iter: 163600 | Total Loss: 0.004196 | Recon Loss: 0.003685 | Commit Loss: 0.001022 | Perplexity: 1176.640150
2025-10-14 17:14:46,666 Stage: Train 0.5 | Epoch: 53 | Iter: 163800 | Total Loss: 0.004361 | Recon Loss: 0.003849 | Commit Loss: 0.001024 | Perplexity: 1164.638744
2025-10-14 17:20:09,478 Stage: Train 0.5 | Epoch: 53 | Iter: 164000 | Total Loss: 0.004257 | Recon Loss: 0.003733 | Commit Loss: 0.001048 | Perplexity: 1172.707351
Trainning Epoch:  33%|███▎      | 54/165 [73:56:32<151:51:39, 4925.22s/it]Trainning Epoch:  33%|███▎      | 54/165 [73:56:32<151:51:39, 4925.22s/it]2025-10-14 17:25:36,924 Stage: Train 0.5 | Epoch: 54 | Iter: 164200 | Total Loss: 0.004269 | Recon Loss: 0.003764 | Commit Loss: 0.001010 | Perplexity: 1174.898947
2025-10-14 17:30:59,907 Stage: Train 0.5 | Epoch: 54 | Iter: 164400 | Total Loss: 0.004233 | Recon Loss: 0.003731 | Commit Loss: 0.001003 | Perplexity: 1164.475169
2025-10-14 17:36:23,241 Stage: Train 0.5 | Epoch: 54 | Iter: 164600 | Total Loss: 0.004356 | Recon Loss: 0.003847 | Commit Loss: 0.001017 | Perplexity: 1171.432310
2025-10-14 17:41:46,561 Stage: Train 0.5 | Epoch: 54 | Iter: 164800 | Total Loss: 0.004294 | Recon Loss: 0.003774 | Commit Loss: 0.001040 | Perplexity: 1169.596259
2025-10-14 17:47:09,944 Stage: Train 0.5 | Epoch: 54 | Iter: 165000 | Total Loss: 0.004276 | Recon Loss: 0.003766 | Commit Loss: 0.001020 | Perplexity: 1168.278752
2025-10-14 17:52:32,975 Stage: Train 0.5 | Epoch: 54 | Iter: 165200 | Total Loss: 0.004285 | Recon Loss: 0.003769 | Commit Loss: 0.001033 | Perplexity: 1178.237896
2025-10-14 17:57:55,864 Stage: Train 0.5 | Epoch: 54 | Iter: 165400 | Total Loss: 0.004290 | Recon Loss: 0.003775 | Commit Loss: 0.001030 | Perplexity: 1175.780517
2025-10-14 18:03:18,908 Stage: Train 0.5 | Epoch: 54 | Iter: 165600 | Total Loss: 0.004329 | Recon Loss: 0.003817 | Commit Loss: 0.001025 | Perplexity: 1171.145458
2025-10-14 18:08:42,207 Stage: Train 0.5 | Epoch: 54 | Iter: 165800 | Total Loss: 0.004225 | Recon Loss: 0.003724 | Commit Loss: 0.001001 | Perplexity: 1175.379496
2025-10-14 18:14:05,416 Stage: Train 0.5 | Epoch: 54 | Iter: 166000 | Total Loss: 0.004341 | Recon Loss: 0.003829 | Commit Loss: 0.001025 | Perplexity: 1166.572498
2025-10-14 18:19:28,738 Stage: Train 0.5 | Epoch: 54 | Iter: 166200 | Total Loss: 0.004295 | Recon Loss: 0.003779 | Commit Loss: 0.001032 | Perplexity: 1170.870838
2025-10-14 18:24:51,967 Stage: Train 0.5 | Epoch: 54 | Iter: 166400 | Total Loss: 0.004378 | Recon Loss: 0.003850 | Commit Loss: 0.001056 | Perplexity: 1170.326599
2025-10-14 18:30:15,091 Stage: Train 0.5 | Epoch: 54 | Iter: 166600 | Total Loss: 0.004289 | Recon Loss: 0.003776 | Commit Loss: 0.001027 | Perplexity: 1168.012467
2025-10-14 18:35:37,712 Stage: Train 0.5 | Epoch: 54 | Iter: 166800 | Total Loss: 0.004364 | Recon Loss: 0.003847 | Commit Loss: 0.001033 | Perplexity: 1171.448089
2025-10-14 18:41:00,839 Stage: Train 0.5 | Epoch: 54 | Iter: 167000 | Total Loss: 0.004242 | Recon Loss: 0.003734 | Commit Loss: 0.001015 | Perplexity: 1171.271530
Trainning Epoch:  33%|███▎      | 55/165 [75:18:25<150:22:48, 4921.54s/it]Trainning Epoch:  33%|███▎      | 55/165 [75:18:25<150:22:49, 4921.54s/it]2025-10-14 18:46:28,415 Stage: Train 0.5 | Epoch: 55 | Iter: 167200 | Total Loss: 0.004267 | Recon Loss: 0.003749 | Commit Loss: 0.001035 | Perplexity: 1176.459625
2025-10-14 18:51:51,549 Stage: Train 0.5 | Epoch: 55 | Iter: 167400 | Total Loss: 0.004308 | Recon Loss: 0.003791 | Commit Loss: 0.001034 | Perplexity: 1167.525376
2025-10-14 18:57:14,944 Stage: Train 0.5 | Epoch: 55 | Iter: 167600 | Total Loss: 0.004233 | Recon Loss: 0.003720 | Commit Loss: 0.001025 | Perplexity: 1174.359667
2025-10-14 19:02:38,263 Stage: Train 0.5 | Epoch: 55 | Iter: 167800 | Total Loss: 0.004369 | Recon Loss: 0.003857 | Commit Loss: 0.001023 | Perplexity: 1165.464012
2025-10-14 19:08:01,846 Stage: Train 0.5 | Epoch: 55 | Iter: 168000 | Total Loss: 0.004211 | Recon Loss: 0.003699 | Commit Loss: 0.001025 | Perplexity: 1167.525067
2025-10-14 19:13:25,230 Stage: Train 0.5 | Epoch: 55 | Iter: 168200 | Total Loss: 0.004311 | Recon Loss: 0.003798 | Commit Loss: 0.001026 | Perplexity: 1173.211689
2025-10-14 19:18:48,325 Stage: Train 0.5 | Epoch: 55 | Iter: 168400 | Total Loss: 0.004253 | Recon Loss: 0.003747 | Commit Loss: 0.001013 | Perplexity: 1170.383453
2025-10-14 19:24:11,317 Stage: Train 0.5 | Epoch: 55 | Iter: 168600 | Total Loss: 0.004244 | Recon Loss: 0.003731 | Commit Loss: 0.001026 | Perplexity: 1165.732280
2025-10-14 19:29:34,355 Stage: Train 0.5 | Epoch: 55 | Iter: 168800 | Total Loss: 0.005208 | Recon Loss: 0.004452 | Commit Loss: 0.001512 | Perplexity: 1170.454191
2025-10-14 19:34:57,150 Stage: Train 0.5 | Epoch: 55 | Iter: 169000 | Total Loss: 0.004204 | Recon Loss: 0.003712 | Commit Loss: 0.000984 | Perplexity: 1164.535765
2025-10-14 19:40:19,985 Stage: Train 0.5 | Epoch: 55 | Iter: 169200 | Total Loss: 0.004170 | Recon Loss: 0.003659 | Commit Loss: 0.001022 | Perplexity: 1167.854961
2025-10-14 19:45:43,324 Stage: Train 0.5 | Epoch: 55 | Iter: 169400 | Total Loss: 0.004381 | Recon Loss: 0.003853 | Commit Loss: 0.001056 | Perplexity: 1163.881608
2025-10-14 19:51:06,863 Stage: Train 0.5 | Epoch: 55 | Iter: 169600 | Total Loss: 0.004281 | Recon Loss: 0.003771 | Commit Loss: 0.001021 | Perplexity: 1169.578857
2025-10-14 19:56:30,086 Stage: Train 0.5 | Epoch: 55 | Iter: 169800 | Total Loss: 0.004292 | Recon Loss: 0.003790 | Commit Loss: 0.001004 | Perplexity: 1170.968882
2025-10-14 20:01:52,809 Stage: Train 0.5 | Epoch: 55 | Iter: 170000 | Total Loss: 0.004273 | Recon Loss: 0.003764 | Commit Loss: 0.001018 | Perplexity: 1168.813930
Trainning Epoch:  34%|███▍      | 56/165 [76:40:18<148:56:08, 4918.98s/it]Trainning Epoch:  34%|███▍      | 56/165 [76:40:18<148:56:08, 4918.98s/it]2025-10-14 20:07:20,216 Stage: Train 0.5 | Epoch: 56 | Iter: 170200 | Total Loss: 0.004337 | Recon Loss: 0.003828 | Commit Loss: 0.001017 | Perplexity: 1165.700643
2025-10-14 20:12:43,069 Stage: Train 0.5 | Epoch: 56 | Iter: 170400 | Total Loss: 0.004241 | Recon Loss: 0.003730 | Commit Loss: 0.001023 | Perplexity: 1171.483505
2025-10-14 20:18:06,046 Stage: Train 0.5 | Epoch: 56 | Iter: 170600 | Total Loss: 0.004236 | Recon Loss: 0.003723 | Commit Loss: 0.001026 | Perplexity: 1167.678989
2025-10-14 20:23:28,969 Stage: Train 0.5 | Epoch: 56 | Iter: 170800 | Total Loss: 0.004265 | Recon Loss: 0.003749 | Commit Loss: 0.001031 | Perplexity: 1168.134193
2025-10-14 20:28:52,246 Stage: Train 0.5 | Epoch: 56 | Iter: 171000 | Total Loss: 0.004244 | Recon Loss: 0.003730 | Commit Loss: 0.001028 | Perplexity: 1174.106153
2025-10-14 20:34:15,388 Stage: Train 0.5 | Epoch: 56 | Iter: 171200 | Total Loss: 0.004205 | Recon Loss: 0.003684 | Commit Loss: 0.001040 | Perplexity: 1179.942406
2025-10-14 20:39:38,577 Stage: Train 0.5 | Epoch: 56 | Iter: 171400 | Total Loss: 0.004299 | Recon Loss: 0.003779 | Commit Loss: 0.001040 | Perplexity: 1170.757371
2025-10-14 20:45:02,162 Stage: Train 0.5 | Epoch: 56 | Iter: 171600 | Total Loss: 0.004282 | Recon Loss: 0.003765 | Commit Loss: 0.001034 | Perplexity: 1169.491472
2025-10-14 20:50:25,568 Stage: Train 0.5 | Epoch: 56 | Iter: 171800 | Total Loss: 0.004264 | Recon Loss: 0.003740 | Commit Loss: 0.001048 | Perplexity: 1174.581569
2025-10-14 20:55:48,679 Stage: Train 0.5 | Epoch: 56 | Iter: 172000 | Total Loss: 0.004232 | Recon Loss: 0.003723 | Commit Loss: 0.001018 | Perplexity: 1171.113543
2025-10-14 21:01:11,653 Stage: Train 0.5 | Epoch: 56 | Iter: 172200 | Total Loss: 0.004313 | Recon Loss: 0.003784 | Commit Loss: 0.001057 | Perplexity: 1182.738355
2025-10-14 21:06:34,642 Stage: Train 0.5 | Epoch: 56 | Iter: 172400 | Total Loss: 0.004264 | Recon Loss: 0.003759 | Commit Loss: 0.001011 | Perplexity: 1169.843050
2025-10-14 21:11:57,941 Stage: Train 0.5 | Epoch: 56 | Iter: 172600 | Total Loss: 0.004259 | Recon Loss: 0.003738 | Commit Loss: 0.001042 | Perplexity: 1163.758658
2025-10-14 21:17:20,882 Stage: Train 0.5 | Epoch: 56 | Iter: 172800 | Total Loss: 0.004296 | Recon Loss: 0.003788 | Commit Loss: 0.001016 | Perplexity: 1176.289587
2025-10-14 21:22:43,940 Stage: Train 0.5 | Epoch: 56 | Iter: 173000 | Total Loss: 0.004198 | Recon Loss: 0.003689 | Commit Loss: 0.001018 | Perplexity: 1170.723804
Trainning Epoch:  35%|███▍      | 57/165 [78:02:11<147:30:59, 4917.22s/it]Trainning Epoch:  35%|███▍      | 57/165 [78:02:11<147:30:59, 4917.22s/it]2025-10-14 21:28:11,624 Stage: Train 0.5 | Epoch: 57 | Iter: 173200 | Total Loss: 0.004231 | Recon Loss: 0.003720 | Commit Loss: 0.001022 | Perplexity: 1168.028926
2025-10-14 21:33:34,789 Stage: Train 0.5 | Epoch: 57 | Iter: 173400 | Total Loss: 0.004176 | Recon Loss: 0.003675 | Commit Loss: 0.001001 | Perplexity: 1172.562563
2025-10-14 21:38:57,758 Stage: Train 0.5 | Epoch: 57 | Iter: 173600 | Total Loss: 0.004268 | Recon Loss: 0.003746 | Commit Loss: 0.001045 | Perplexity: 1171.213721
2025-10-14 21:44:20,758 Stage: Train 0.5 | Epoch: 57 | Iter: 173800 | Total Loss: 0.004152 | Recon Loss: 0.003635 | Commit Loss: 0.001035 | Perplexity: 1177.018476
2025-10-14 21:49:44,349 Stage: Train 0.5 | Epoch: 57 | Iter: 174000 | Total Loss: 0.004160 | Recon Loss: 0.003658 | Commit Loss: 0.001006 | Perplexity: 1164.491486
2025-10-14 21:55:07,475 Stage: Train 0.5 | Epoch: 57 | Iter: 174200 | Total Loss: 0.004243 | Recon Loss: 0.003725 | Commit Loss: 0.001037 | Perplexity: 1175.343969
2025-10-14 22:00:30,763 Stage: Train 0.5 | Epoch: 57 | Iter: 174400 | Total Loss: 0.004224 | Recon Loss: 0.003703 | Commit Loss: 0.001042 | Perplexity: 1175.206387
2025-10-14 22:05:53,705 Stage: Train 0.5 | Epoch: 57 | Iter: 174600 | Total Loss: 0.004337 | Recon Loss: 0.003815 | Commit Loss: 0.001044 | Perplexity: 1181.239792
2025-10-14 22:11:16,553 Stage: Train 0.5 | Epoch: 57 | Iter: 174800 | Total Loss: 0.004171 | Recon Loss: 0.003648 | Commit Loss: 0.001045 | Perplexity: 1171.876805
2025-10-14 22:16:39,780 Stage: Train 0.5 | Epoch: 57 | Iter: 175000 | Total Loss: 0.004237 | Recon Loss: 0.003732 | Commit Loss: 0.001011 | Perplexity: 1179.597235
2025-10-14 22:22:02,881 Stage: Train 0.5 | Epoch: 57 | Iter: 175200 | Total Loss: 0.004207 | Recon Loss: 0.003702 | Commit Loss: 0.001011 | Perplexity: 1176.281878
2025-10-14 22:27:25,566 Stage: Train 0.5 | Epoch: 57 | Iter: 175400 | Total Loss: 0.004282 | Recon Loss: 0.003757 | Commit Loss: 0.001049 | Perplexity: 1176.451544
2025-10-14 22:32:48,829 Stage: Train 0.5 | Epoch: 57 | Iter: 175600 | Total Loss: 0.004268 | Recon Loss: 0.003744 | Commit Loss: 0.001047 | Perplexity: 1171.375634
2025-10-14 22:38:11,249 Stage: Train 0.5 | Epoch: 57 | Iter: 175800 | Total Loss: 0.004196 | Recon Loss: 0.003689 | Commit Loss: 0.001014 | Perplexity: 1175.839285
2025-10-14 22:43:33,940 Stage: Train 0.5 | Epoch: 57 | Iter: 176000 | Total Loss: 0.004277 | Recon Loss: 0.003758 | Commit Loss: 0.001037 | Perplexity: 1164.122290
2025-10-14 22:48:56,613 Stage: Train 0.5 | Epoch: 57 | Iter: 176200 | Total Loss: 0.004236 | Recon Loss: 0.003694 | Commit Loss: 0.001083 | Perplexity: 1167.920751
Trainning Epoch:  35%|███▌      | 58/165 [79:24:02<146:05:33, 4915.26s/it]Trainning Epoch:  35%|███▌      | 58/165 [79:24:02<146:05:32, 4915.26s/it]2025-10-14 22:54:24,110 Stage: Train 0.5 | Epoch: 58 | Iter: 176400 | Total Loss: 0.004231 | Recon Loss: 0.003716 | Commit Loss: 0.001031 | Perplexity: 1165.771866
2025-10-14 22:59:47,422 Stage: Train 0.5 | Epoch: 58 | Iter: 176600 | Total Loss: 0.004196 | Recon Loss: 0.003674 | Commit Loss: 0.001045 | Perplexity: 1169.998300
2025-10-14 23:05:10,595 Stage: Train 0.5 | Epoch: 58 | Iter: 176800 | Total Loss: 0.004212 | Recon Loss: 0.003702 | Commit Loss: 0.001020 | Perplexity: 1172.102038
2025-10-14 23:10:33,420 Stage: Train 0.5 | Epoch: 58 | Iter: 177000 | Total Loss: 0.004202 | Recon Loss: 0.003693 | Commit Loss: 0.001017 | Perplexity: 1176.500698
2025-10-14 23:15:56,490 Stage: Train 0.5 | Epoch: 58 | Iter: 177200 | Total Loss: 0.004188 | Recon Loss: 0.003678 | Commit Loss: 0.001019 | Perplexity: 1175.733982
2025-10-14 23:21:19,582 Stage: Train 0.5 | Epoch: 58 | Iter: 177400 | Total Loss: 0.004178 | Recon Loss: 0.003656 | Commit Loss: 0.001044 | Perplexity: 1176.485794
2025-10-14 23:26:42,699 Stage: Train 0.5 | Epoch: 58 | Iter: 177600 | Total Loss: 0.004114 | Recon Loss: 0.003607 | Commit Loss: 0.001013 | Perplexity: 1170.501890
2025-10-14 23:32:05,920 Stage: Train 0.5 | Epoch: 58 | Iter: 177800 | Total Loss: 0.004234 | Recon Loss: 0.003728 | Commit Loss: 0.001010 | Perplexity: 1166.834333
2025-10-14 23:37:29,024 Stage: Train 0.5 | Epoch: 58 | Iter: 178000 | Total Loss: 0.004186 | Recon Loss: 0.003671 | Commit Loss: 0.001029 | Perplexity: 1174.666915
2025-10-14 23:42:51,648 Stage: Train 0.5 | Epoch: 58 | Iter: 178200 | Total Loss: 0.004263 | Recon Loss: 0.003737 | Commit Loss: 0.001053 | Perplexity: 1171.683558
2025-10-14 23:48:14,225 Stage: Train 0.5 | Epoch: 58 | Iter: 178400 | Total Loss: 0.004217 | Recon Loss: 0.003702 | Commit Loss: 0.001029 | Perplexity: 1172.603043
2025-10-14 23:53:36,584 Stage: Train 0.5 | Epoch: 58 | Iter: 178600 | Total Loss: 0.004178 | Recon Loss: 0.003664 | Commit Loss: 0.001028 | Perplexity: 1174.232788
2025-10-14 23:58:59,213 Stage: Train 0.5 | Epoch: 58 | Iter: 178800 | Total Loss: 0.004278 | Recon Loss: 0.003756 | Commit Loss: 0.001045 | Perplexity: 1170.816906
2025-10-15 00:04:22,622 Stage: Train 0.5 | Epoch: 58 | Iter: 179000 | Total Loss: 0.004175 | Recon Loss: 0.003664 | Commit Loss: 0.001023 | Perplexity: 1177.409317
2025-10-15 00:09:45,676 Stage: Train 0.5 | Epoch: 58 | Iter: 179200 | Total Loss: 0.004235 | Recon Loss: 0.003709 | Commit Loss: 0.001052 | Perplexity: 1173.691621
Trainning Epoch:  36%|███▌      | 59/165 [80:45:52<144:41:03, 4913.81s/it]Trainning Epoch:  36%|███▌      | 59/165 [80:45:52<144:41:03, 4913.81s/it]2025-10-15 00:15:13,494 Stage: Train 0.5 | Epoch: 59 | Iter: 179400 | Total Loss: 0.004260 | Recon Loss: 0.003757 | Commit Loss: 0.001007 | Perplexity: 1176.125817
2025-10-15 00:20:36,497 Stage: Train 0.5 | Epoch: 59 | Iter: 179600 | Total Loss: 0.004154 | Recon Loss: 0.003637 | Commit Loss: 0.001034 | Perplexity: 1177.191535
2025-10-15 00:25:59,674 Stage: Train 0.5 | Epoch: 59 | Iter: 179800 | Total Loss: 0.004139 | Recon Loss: 0.003630 | Commit Loss: 0.001017 | Perplexity: 1172.580947
2025-10-15 00:31:22,618 Stage: Train 0.5 | Epoch: 59 | Iter: 180000 | Total Loss: 0.004234 | Recon Loss: 0.003714 | Commit Loss: 0.001040 | Perplexity: 1179.092432
2025-10-15 00:31:22,619 Saving model at iteration 180000
2025-10-15 00:31:22,863 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000
2025-10-15 00:31:24,242 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000/model.safetensors
2025-10-15 00:31:26,028 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000/optimizer.bin
2025-10-15 00:31:26,029 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000/scheduler.bin
2025-10-15 00:31:26,029 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000/sampler.bin
2025-10-15 00:31:26,030 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_60_step_180000/random_states_0.pkl
2025-10-15 00:36:48,637 Stage: Train 0.5 | Epoch: 59 | Iter: 180200 | Total Loss: 0.004180 | Recon Loss: 0.003664 | Commit Loss: 0.001032 | Perplexity: 1168.800299
2025-10-15 00:42:12,249 Stage: Train 0.5 | Epoch: 59 | Iter: 180400 | Total Loss: 0.004165 | Recon Loss: 0.003649 | Commit Loss: 0.001032 | Perplexity: 1176.813718
2025-10-15 00:47:34,542 Stage: Train 0.5 | Epoch: 59 | Iter: 180600 | Total Loss: 0.004163 | Recon Loss: 0.003652 | Commit Loss: 0.001023 | Perplexity: 1172.769616
2025-10-15 00:52:57,427 Stage: Train 0.5 | Epoch: 59 | Iter: 180800 | Total Loss: 0.004089 | Recon Loss: 0.003572 | Commit Loss: 0.001033 | Perplexity: 1169.828770
2025-10-15 00:58:20,556 Stage: Train 0.5 | Epoch: 59 | Iter: 181000 | Total Loss: 0.004284 | Recon Loss: 0.003760 | Commit Loss: 0.001046 | Perplexity: 1163.649025
2025-10-15 01:03:43,969 Stage: Train 0.5 | Epoch: 59 | Iter: 181200 | Total Loss: 0.004198 | Recon Loss: 0.003674 | Commit Loss: 0.001047 | Perplexity: 1170.408218
2025-10-15 01:09:07,242 Stage: Train 0.5 | Epoch: 59 | Iter: 181400 | Total Loss: 0.004154 | Recon Loss: 0.003640 | Commit Loss: 0.001027 | Perplexity: 1174.400654
2025-10-15 01:14:29,784 Stage: Train 0.5 | Epoch: 59 | Iter: 181600 | Total Loss: 0.004200 | Recon Loss: 0.003673 | Commit Loss: 0.001053 | Perplexity: 1175.829364
2025-10-15 01:19:52,076 Stage: Train 0.5 | Epoch: 59 | Iter: 181800 | Total Loss: 0.004203 | Recon Loss: 0.003674 | Commit Loss: 0.001058 | Perplexity: 1174.459872
2025-10-15 01:25:14,551 Stage: Train 0.5 | Epoch: 59 | Iter: 182000 | Total Loss: 0.004208 | Recon Loss: 0.003696 | Commit Loss: 0.001025 | Perplexity: 1180.583569
2025-10-15 01:30:37,342 Stage: Train 0.5 | Epoch: 59 | Iter: 182200 | Total Loss: 0.004279 | Recon Loss: 0.003754 | Commit Loss: 0.001049 | Perplexity: 1174.951782
Trainning Epoch:  36%|███▋      | 60/165 [82:07:45<143:18:47, 4913.59s/it]Trainning Epoch:  36%|███▋      | 60/165 [82:07:45<143:18:47, 4913.59s/it]2025-10-15 01:36:04,568 Stage: Train 0.5 | Epoch: 60 | Iter: 182400 | Total Loss: 0.004068 | Recon Loss: 0.003551 | Commit Loss: 0.001034 | Perplexity: 1173.403146
2025-10-15 01:41:27,699 Stage: Train 0.5 | Epoch: 60 | Iter: 182600 | Total Loss: 0.004209 | Recon Loss: 0.003697 | Commit Loss: 0.001023 | Perplexity: 1173.383721
2025-10-15 01:46:50,561 Stage: Train 0.5 | Epoch: 60 | Iter: 182800 | Total Loss: 0.004095 | Recon Loss: 0.003585 | Commit Loss: 0.001021 | Perplexity: 1174.840798
2025-10-15 01:52:12,542 Stage: Train 0.5 | Epoch: 60 | Iter: 183000 | Total Loss: 0.004243 | Recon Loss: 0.003719 | Commit Loss: 0.001048 | Perplexity: 1175.203109
2025-10-15 01:57:34,967 Stage: Train 0.5 | Epoch: 60 | Iter: 183200 | Total Loss: 0.004089 | Recon Loss: 0.003578 | Commit Loss: 0.001022 | Perplexity: 1165.917394
2025-10-15 02:02:57,760 Stage: Train 0.5 | Epoch: 60 | Iter: 183400 | Total Loss: 0.004289 | Recon Loss: 0.003771 | Commit Loss: 0.001035 | Perplexity: 1164.169557
2025-10-15 02:08:20,653 Stage: Train 0.5 | Epoch: 60 | Iter: 183600 | Total Loss: 0.004205 | Recon Loss: 0.003692 | Commit Loss: 0.001025 | Perplexity: 1183.999406
2025-10-15 02:13:43,500 Stage: Train 0.5 | Epoch: 60 | Iter: 183800 | Total Loss: 0.004129 | Recon Loss: 0.003613 | Commit Loss: 0.001033 | Perplexity: 1181.726972
2025-10-15 02:19:05,948 Stage: Train 0.5 | Epoch: 60 | Iter: 184000 | Total Loss: 0.004168 | Recon Loss: 0.003646 | Commit Loss: 0.001042 | Perplexity: 1174.696127
2025-10-15 02:24:28,717 Stage: Train 0.5 | Epoch: 60 | Iter: 184200 | Total Loss: 0.004125 | Recon Loss: 0.003601 | Commit Loss: 0.001047 | Perplexity: 1180.454466
2025-10-15 02:29:51,716 Stage: Train 0.5 | Epoch: 60 | Iter: 184400 | Total Loss: 0.004282 | Recon Loss: 0.003764 | Commit Loss: 0.001037 | Perplexity: 1176.774446
2025-10-15 02:35:14,206 Stage: Train 0.5 | Epoch: 60 | Iter: 184600 | Total Loss: 0.004166 | Recon Loss: 0.003661 | Commit Loss: 0.001009 | Perplexity: 1166.175070
2025-10-15 02:40:36,479 Stage: Train 0.5 | Epoch: 60 | Iter: 184800 | Total Loss: 0.004238 | Recon Loss: 0.003730 | Commit Loss: 0.001015 | Perplexity: 1174.367776
2025-10-15 02:45:59,348 Stage: Train 0.5 | Epoch: 60 | Iter: 185000 | Total Loss: 0.004207 | Recon Loss: 0.003685 | Commit Loss: 0.001043 | Perplexity: 1180.690635
2025-10-15 02:51:22,465 Stage: Train 0.5 | Epoch: 60 | Iter: 185200 | Total Loss: 0.004308 | Recon Loss: 0.003791 | Commit Loss: 0.001033 | Perplexity: 1173.518683
Trainning Epoch:  37%|███▋      | 61/165 [83:29:31<141:53:01, 4911.37s/it]Trainning Epoch:  37%|███▋      | 61/165 [83:29:31<141:53:02, 4911.37s/it]2025-10-15 02:56:49,644 Stage: Train 0.5 | Epoch: 61 | Iter: 185400 | Total Loss: 0.004165 | Recon Loss: 0.003640 | Commit Loss: 0.001050 | Perplexity: 1171.717617
2025-10-15 03:02:12,643 Stage: Train 0.5 | Epoch: 61 | Iter: 185600 | Total Loss: 0.004172 | Recon Loss: 0.003665 | Commit Loss: 0.001014 | Perplexity: 1179.118467
2025-10-15 03:07:35,657 Stage: Train 0.5 | Epoch: 61 | Iter: 185800 | Total Loss: 0.004006 | Recon Loss: 0.003503 | Commit Loss: 0.001007 | Perplexity: 1178.663152
2025-10-15 03:12:58,297 Stage: Train 0.5 | Epoch: 61 | Iter: 186000 | Total Loss: 0.004280 | Recon Loss: 0.003760 | Commit Loss: 0.001041 | Perplexity: 1179.211852
2025-10-15 03:18:21,430 Stage: Train 0.5 | Epoch: 61 | Iter: 186200 | Total Loss: 0.004116 | Recon Loss: 0.003598 | Commit Loss: 0.001037 | Perplexity: 1173.058666
2025-10-15 03:23:44,463 Stage: Train 0.5 | Epoch: 61 | Iter: 186400 | Total Loss: 0.004129 | Recon Loss: 0.003605 | Commit Loss: 0.001048 | Perplexity: 1165.908826
2025-10-15 03:29:07,112 Stage: Train 0.5 | Epoch: 61 | Iter: 186600 | Total Loss: 0.004294 | Recon Loss: 0.003771 | Commit Loss: 0.001046 | Perplexity: 1177.252358
2025-10-15 03:34:29,784 Stage: Train 0.5 | Epoch: 61 | Iter: 186800 | Total Loss: 0.004151 | Recon Loss: 0.003643 | Commit Loss: 0.001016 | Perplexity: 1171.268310
2025-10-15 03:39:52,622 Stage: Train 0.5 | Epoch: 61 | Iter: 187000 | Total Loss: 0.004162 | Recon Loss: 0.003640 | Commit Loss: 0.001043 | Perplexity: 1174.322834
2025-10-15 03:45:15,123 Stage: Train 0.5 | Epoch: 61 | Iter: 187200 | Total Loss: 0.004229 | Recon Loss: 0.003701 | Commit Loss: 0.001055 | Perplexity: 1181.551732
2025-10-15 03:50:37,863 Stage: Train 0.5 | Epoch: 61 | Iter: 187400 | Total Loss: 0.004117 | Recon Loss: 0.003594 | Commit Loss: 0.001045 | Perplexity: 1183.043310
2025-10-15 03:56:00,408 Stage: Train 0.5 | Epoch: 61 | Iter: 187600 | Total Loss: 0.004147 | Recon Loss: 0.003636 | Commit Loss: 0.001022 | Perplexity: 1170.538365
2025-10-15 04:01:23,731 Stage: Train 0.5 | Epoch: 61 | Iter: 187800 | Total Loss: 0.004226 | Recon Loss: 0.003704 | Commit Loss: 0.001044 | Perplexity: 1183.716476
2025-10-15 04:06:45,474 Stage: Train 0.5 | Epoch: 61 | Iter: 188000 | Total Loss: 0.004136 | Recon Loss: 0.003620 | Commit Loss: 0.001032 | Perplexity: 1168.756982
2025-10-15 04:12:07,883 Stage: Train 0.5 | Epoch: 61 | Iter: 188200 | Total Loss: 0.004171 | Recon Loss: 0.003647 | Commit Loss: 0.001049 | Perplexity: 1171.128678
Trainning Epoch:  38%|███▊      | 62/165 [84:51:18<140:28:59, 4910.09s/it]Trainning Epoch:  38%|███▊      | 62/165 [84:51:19<140:28:59, 4910.09s/it]2025-10-15 04:17:35,228 Stage: Train 0.5 | Epoch: 62 | Iter: 188400 | Total Loss: 0.004156 | Recon Loss: 0.003639 | Commit Loss: 0.001035 | Perplexity: 1174.451202
2025-10-15 04:22:58,082 Stage: Train 0.5 | Epoch: 62 | Iter: 188600 | Total Loss: 0.004200 | Recon Loss: 0.003687 | Commit Loss: 0.001026 | Perplexity: 1167.832099
2025-10-15 04:28:20,598 Stage: Train 0.5 | Epoch: 62 | Iter: 188800 | Total Loss: 0.004127 | Recon Loss: 0.003610 | Commit Loss: 0.001034 | Perplexity: 1173.189606
2025-10-15 04:33:44,174 Stage: Train 0.5 | Epoch: 62 | Iter: 189000 | Total Loss: 0.004100 | Recon Loss: 0.003581 | Commit Loss: 0.001039 | Perplexity: 1171.630954
2025-10-15 04:39:06,833 Stage: Train 0.5 | Epoch: 62 | Iter: 189200 | Total Loss: 0.004291 | Recon Loss: 0.003766 | Commit Loss: 0.001050 | Perplexity: 1166.707360
2025-10-15 04:44:29,608 Stage: Train 0.5 | Epoch: 62 | Iter: 189400 | Total Loss: 0.004171 | Recon Loss: 0.003658 | Commit Loss: 0.001026 | Perplexity: 1170.223542
2025-10-15 04:49:51,978 Stage: Train 0.5 | Epoch: 62 | Iter: 189600 | Total Loss: 0.004224 | Recon Loss: 0.003703 | Commit Loss: 0.001041 | Perplexity: 1171.185901
2025-10-15 04:55:14,699 Stage: Train 0.5 | Epoch: 62 | Iter: 189800 | Total Loss: 0.004095 | Recon Loss: 0.003585 | Commit Loss: 0.001021 | Perplexity: 1167.365121
2025-10-15 05:00:37,321 Stage: Train 0.5 | Epoch: 62 | Iter: 190000 | Total Loss: 0.004170 | Recon Loss: 0.003662 | Commit Loss: 0.001015 | Perplexity: 1179.581057
2025-10-15 05:06:00,162 Stage: Train 0.5 | Epoch: 62 | Iter: 190200 | Total Loss: 0.004178 | Recon Loss: 0.003660 | Commit Loss: 0.001036 | Perplexity: 1170.264004
2025-10-15 05:11:22,825 Stage: Train 0.5 | Epoch: 62 | Iter: 190400 | Total Loss: 0.004037 | Recon Loss: 0.003528 | Commit Loss: 0.001020 | Perplexity: 1173.190611
2025-10-15 05:16:45,865 Stage: Train 0.5 | Epoch: 62 | Iter: 190600 | Total Loss: 0.004229 | Recon Loss: 0.003703 | Commit Loss: 0.001052 | Perplexity: 1173.963730
2025-10-15 05:22:08,928 Stage: Train 0.5 | Epoch: 62 | Iter: 190800 | Total Loss: 0.004180 | Recon Loss: 0.003648 | Commit Loss: 0.001063 | Perplexity: 1171.438496
2025-10-15 05:27:32,024 Stage: Train 0.5 | Epoch: 62 | Iter: 191000 | Total Loss: 0.004161 | Recon Loss: 0.003638 | Commit Loss: 0.001047 | Perplexity: 1171.423944
2025-10-15 05:32:54,700 Stage: Train 0.5 | Epoch: 62 | Iter: 191200 | Total Loss: 0.004183 | Recon Loss: 0.003666 | Commit Loss: 0.001034 | Perplexity: 1180.491437
Trainning Epoch:  38%|███▊      | 63/165 [86:13:06<139:05:55, 4909.37s/it]Trainning Epoch:  38%|███▊      | 63/165 [86:13:06<139:05:55, 4909.37s/it]2025-10-15 05:38:21,607 Stage: Train 0.5 | Epoch: 63 | Iter: 191400 | Total Loss: 0.004166 | Recon Loss: 0.003652 | Commit Loss: 0.001029 | Perplexity: 1168.882365
2025-10-15 05:43:44,204 Stage: Train 0.5 | Epoch: 63 | Iter: 191600 | Total Loss: 0.004212 | Recon Loss: 0.003696 | Commit Loss: 0.001032 | Perplexity: 1178.206044
2025-10-15 05:49:07,283 Stage: Train 0.5 | Epoch: 63 | Iter: 191800 | Total Loss: 0.004163 | Recon Loss: 0.003635 | Commit Loss: 0.001056 | Perplexity: 1178.701527
2025-10-15 05:54:30,366 Stage: Train 0.5 | Epoch: 63 | Iter: 192000 | Total Loss: 0.004193 | Recon Loss: 0.003665 | Commit Loss: 0.001057 | Perplexity: 1178.488133
2025-10-15 05:59:53,366 Stage: Train 0.5 | Epoch: 63 | Iter: 192200 | Total Loss: 0.004228 | Recon Loss: 0.003710 | Commit Loss: 0.001037 | Perplexity: 1172.091982
2025-10-15 06:05:16,320 Stage: Train 0.5 | Epoch: 63 | Iter: 192400 | Total Loss: 0.004127 | Recon Loss: 0.003618 | Commit Loss: 0.001018 | Perplexity: 1169.994156
2025-10-15 06:10:38,963 Stage: Train 0.5 | Epoch: 63 | Iter: 192600 | Total Loss: 0.004154 | Recon Loss: 0.003637 | Commit Loss: 0.001034 | Perplexity: 1167.784320
2025-10-15 06:16:02,560 Stage: Train 0.5 | Epoch: 63 | Iter: 192800 | Total Loss: 0.004125 | Recon Loss: 0.003605 | Commit Loss: 0.001041 | Perplexity: 1176.606042
2025-10-15 06:21:25,742 Stage: Train 0.5 | Epoch: 63 | Iter: 193000 | Total Loss: 0.004083 | Recon Loss: 0.003568 | Commit Loss: 0.001030 | Perplexity: 1183.235629
2025-10-15 06:26:48,554 Stage: Train 0.5 | Epoch: 63 | Iter: 193200 | Total Loss: 0.004190 | Recon Loss: 0.003671 | Commit Loss: 0.001039 | Perplexity: 1172.785659
2025-10-15 06:32:11,699 Stage: Train 0.5 | Epoch: 63 | Iter: 193400 | Total Loss: 0.004195 | Recon Loss: 0.003671 | Commit Loss: 0.001047 | Perplexity: 1180.300770
2025-10-15 06:37:33,800 Stage: Train 0.5 | Epoch: 63 | Iter: 193600 | Total Loss: 0.004049 | Recon Loss: 0.003528 | Commit Loss: 0.001041 | Perplexity: 1175.383893
2025-10-15 06:42:56,724 Stage: Train 0.5 | Epoch: 63 | Iter: 193800 | Total Loss: 0.004052 | Recon Loss: 0.003534 | Commit Loss: 0.001036 | Perplexity: 1167.293730
2025-10-15 06:48:19,658 Stage: Train 0.5 | Epoch: 63 | Iter: 194000 | Total Loss: 0.004135 | Recon Loss: 0.003618 | Commit Loss: 0.001034 | Perplexity: 1179.053176
2025-10-15 06:53:42,022 Stage: Train 0.5 | Epoch: 63 | Iter: 194200 | Total Loss: 0.004072 | Recon Loss: 0.003547 | Commit Loss: 0.001050 | Perplexity: 1169.998144
2025-10-15 06:59:04,030 Stage: Train 0.5 | Epoch: 63 | Iter: 194400 | Total Loss: 0.004060 | Recon Loss: 0.003548 | Commit Loss: 0.001025 | Perplexity: 1170.653710
Trainning Epoch:  39%|███▉      | 64/165 [87:34:54<137:43:23, 4908.94s/it]Trainning Epoch:  39%|███▉      | 64/165 [87:34:54<137:43:23, 4908.95s/it]2025-10-15 07:04:30,835 Stage: Train 0.5 | Epoch: 64 | Iter: 194600 | Total Loss: 0.004246 | Recon Loss: 0.003732 | Commit Loss: 0.001028 | Perplexity: 1173.631134
2025-10-15 07:09:53,862 Stage: Train 0.5 | Epoch: 64 | Iter: 194800 | Total Loss: 0.004110 | Recon Loss: 0.003586 | Commit Loss: 0.001048 | Perplexity: 1178.703094
2025-10-15 07:15:16,711 Stage: Train 0.5 | Epoch: 64 | Iter: 195000 | Total Loss: 0.004066 | Recon Loss: 0.003550 | Commit Loss: 0.001032 | Perplexity: 1175.104363
2025-10-15 07:20:39,734 Stage: Train 0.5 | Epoch: 64 | Iter: 195200 | Total Loss: 0.004145 | Recon Loss: 0.003620 | Commit Loss: 0.001049 | Perplexity: 1174.288549
2025-10-15 07:26:02,347 Stage: Train 0.5 | Epoch: 64 | Iter: 195400 | Total Loss: 0.004163 | Recon Loss: 0.003643 | Commit Loss: 0.001041 | Perplexity: 1172.410912
2025-10-15 07:31:25,380 Stage: Train 0.5 | Epoch: 64 | Iter: 195600 | Total Loss: 0.004143 | Recon Loss: 0.003627 | Commit Loss: 0.001032 | Perplexity: 1168.707991
2025-10-15 07:36:48,375 Stage: Train 0.5 | Epoch: 64 | Iter: 195800 | Total Loss: 0.004128 | Recon Loss: 0.003606 | Commit Loss: 0.001042 | Perplexity: 1176.937123
2025-10-15 07:42:11,202 Stage: Train 0.5 | Epoch: 64 | Iter: 196000 | Total Loss: 0.004168 | Recon Loss: 0.003650 | Commit Loss: 0.001035 | Perplexity: 1178.929775
2025-10-15 07:47:34,102 Stage: Train 0.5 | Epoch: 64 | Iter: 196200 | Total Loss: 0.004047 | Recon Loss: 0.003533 | Commit Loss: 0.001028 | Perplexity: 1176.208532
2025-10-15 07:52:57,069 Stage: Train 0.5 | Epoch: 64 | Iter: 196400 | Total Loss: 0.004134 | Recon Loss: 0.003620 | Commit Loss: 0.001028 | Perplexity: 1173.867120
2025-10-15 07:58:19,798 Stage: Train 0.5 | Epoch: 64 | Iter: 196600 | Total Loss: 0.004145 | Recon Loss: 0.003620 | Commit Loss: 0.001049 | Perplexity: 1170.519539
2025-10-15 08:03:42,950 Stage: Train 0.5 | Epoch: 64 | Iter: 196800 | Total Loss: 0.004066 | Recon Loss: 0.003544 | Commit Loss: 0.001043 | Perplexity: 1171.343416
2025-10-15 08:09:05,375 Stage: Train 0.5 | Epoch: 64 | Iter: 197000 | Total Loss: 0.004138 | Recon Loss: 0.003620 | Commit Loss: 0.001036 | Perplexity: 1181.946823
2025-10-15 08:14:27,270 Stage: Train 0.5 | Epoch: 64 | Iter: 197200 | Total Loss: 0.004141 | Recon Loss: 0.003627 | Commit Loss: 0.001030 | Perplexity: 1169.824637
2025-10-15 08:19:49,932 Stage: Train 0.5 | Epoch: 64 | Iter: 197400 | Total Loss: 0.004058 | Recon Loss: 0.003544 | Commit Loss: 0.001028 | Perplexity: 1174.534775
Trainning Epoch:  39%|███▉      | 65/165 [88:56:42<136:20:50, 4908.51s/it]Trainning Epoch:  39%|███▉      | 65/165 [88:56:42<136:20:50, 4908.51s/it]2025-10-15 08:25:16,957 Stage: Train 0.5 | Epoch: 65 | Iter: 197600 | Total Loss: 0.004089 | Recon Loss: 0.003571 | Commit Loss: 0.001035 | Perplexity: 1166.433533
2025-10-15 08:30:40,063 Stage: Train 0.5 | Epoch: 65 | Iter: 197800 | Total Loss: 0.004070 | Recon Loss: 0.003564 | Commit Loss: 0.001013 | Perplexity: 1168.658572
2025-10-15 08:36:02,825 Stage: Train 0.5 | Epoch: 65 | Iter: 198000 | Total Loss: 0.004103 | Recon Loss: 0.003591 | Commit Loss: 0.001025 | Perplexity: 1182.438653
2025-10-15 08:41:26,204 Stage: Train 0.5 | Epoch: 65 | Iter: 198200 | Total Loss: 0.004100 | Recon Loss: 0.003572 | Commit Loss: 0.001056 | Perplexity: 1172.688301
2025-10-15 08:46:48,944 Stage: Train 0.5 | Epoch: 65 | Iter: 198400 | Total Loss: 0.004138 | Recon Loss: 0.003623 | Commit Loss: 0.001031 | Perplexity: 1175.774637
2025-10-15 08:52:11,728 Stage: Train 0.5 | Epoch: 65 | Iter: 198600 | Total Loss: 0.004057 | Recon Loss: 0.003542 | Commit Loss: 0.001030 | Perplexity: 1173.633725
2025-10-15 08:57:34,674 Stage: Train 0.5 | Epoch: 65 | Iter: 198800 | Total Loss: 0.004179 | Recon Loss: 0.003654 | Commit Loss: 0.001050 | Perplexity: 1181.482693
2025-10-15 09:02:58,274 Stage: Train 0.5 | Epoch: 65 | Iter: 199000 | Total Loss: 0.004061 | Recon Loss: 0.003536 | Commit Loss: 0.001051 | Perplexity: 1176.378522
2025-10-15 09:08:21,898 Stage: Train 0.5 | Epoch: 65 | Iter: 199200 | Total Loss: 0.004162 | Recon Loss: 0.003638 | Commit Loss: 0.001049 | Perplexity: 1180.692746
2025-10-15 09:13:45,069 Stage: Train 0.5 | Epoch: 65 | Iter: 199400 | Total Loss: 0.004199 | Recon Loss: 0.003685 | Commit Loss: 0.001028 | Perplexity: 1179.973268
2025-10-15 09:19:08,058 Stage: Train 0.5 | Epoch: 65 | Iter: 199600 | Total Loss: 0.004112 | Recon Loss: 0.003587 | Commit Loss: 0.001050 | Perplexity: 1176.634575
2025-10-15 09:24:30,167 Stage: Train 0.5 | Epoch: 65 | Iter: 199800 | Total Loss: 0.004086 | Recon Loss: 0.003567 | Commit Loss: 0.001037 | Perplexity: 1178.466993
2025-10-15 09:29:52,841 Stage: Train 0.5 | Epoch: 65 | Iter: 200000 | Total Loss: 0.004142 | Recon Loss: 0.003618 | Commit Loss: 0.001047 | Perplexity: 1178.710048
2025-10-15 09:29:52,841 Saving model at iteration 200000
2025-10-15 09:29:52,998 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000
2025-10-15 09:29:54,354 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000/model.safetensors
2025-10-15 09:29:56,096 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000/optimizer.bin
2025-10-15 09:29:56,097 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000/scheduler.bin
2025-10-15 09:29:56,097 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000/sampler.bin
2025-10-15 09:29:56,098 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_66_step_200000/random_states_0.pkl
2025-10-15 09:35:18,937 Stage: Train 0.5 | Epoch: 65 | Iter: 200200 | Total Loss: 0.004033 | Recon Loss: 0.003515 | Commit Loss: 0.001035 | Perplexity: 1181.988925
2025-10-15 09:40:41,395 Stage: Train 0.5 | Epoch: 65 | Iter: 200400 | Total Loss: 0.004038 | Recon Loss: 0.003518 | Commit Loss: 0.001041 | Perplexity: 1175.098202
Trainning Epoch:  40%|████      | 66/165 [90:18:34<135:01:06, 4909.76s/it]Trainning Epoch:  40%|████      | 66/165 [90:18:34<135:01:06, 4909.77s/it]2025-10-15 09:46:08,514 Stage: Train 0.5 | Epoch: 66 | Iter: 200600 | Total Loss: 0.004081 | Recon Loss: 0.003564 | Commit Loss: 0.001034 | Perplexity: 1170.481282
2025-10-15 09:51:31,713 Stage: Train 0.5 | Epoch: 66 | Iter: 200800 | Total Loss: 0.004112 | Recon Loss: 0.003589 | Commit Loss: 0.001045 | Perplexity: 1172.354860
2025-10-15 09:56:54,917 Stage: Train 0.5 | Epoch: 66 | Iter: 201000 | Total Loss: 0.004068 | Recon Loss: 0.003548 | Commit Loss: 0.001040 | Perplexity: 1170.084150
2025-10-15 10:02:18,078 Stage: Train 0.5 | Epoch: 66 | Iter: 201200 | Total Loss: 0.004144 | Recon Loss: 0.003619 | Commit Loss: 0.001048 | Perplexity: 1172.496347
2025-10-15 10:07:41,761 Stage: Train 0.5 | Epoch: 66 | Iter: 201400 | Total Loss: 0.004018 | Recon Loss: 0.003504 | Commit Loss: 0.001027 | Perplexity: 1174.673622
2025-10-15 10:13:05,021 Stage: Train 0.5 | Epoch: 66 | Iter: 201600 | Total Loss: 0.004103 | Recon Loss: 0.003587 | Commit Loss: 0.001034 | Perplexity: 1171.369279
2025-10-15 10:18:28,580 Stage: Train 0.5 | Epoch: 66 | Iter: 201800 | Total Loss: 0.004026 | Recon Loss: 0.003513 | Commit Loss: 0.001026 | Perplexity: 1175.656340
2025-10-15 10:23:51,885 Stage: Train 0.5 | Epoch: 66 | Iter: 202000 | Total Loss: 0.004135 | Recon Loss: 0.003620 | Commit Loss: 0.001032 | Perplexity: 1166.758860
2025-10-15 10:29:14,789 Stage: Train 0.5 | Epoch: 66 | Iter: 202200 | Total Loss: 0.004111 | Recon Loss: 0.003590 | Commit Loss: 0.001041 | Perplexity: 1177.273941
2025-10-15 10:34:37,644 Stage: Train 0.5 | Epoch: 66 | Iter: 202400 | Total Loss: 0.004110 | Recon Loss: 0.003596 | Commit Loss: 0.001029 | Perplexity: 1171.141565
2025-10-15 10:40:00,273 Stage: Train 0.5 | Epoch: 66 | Iter: 202600 | Total Loss: 0.004055 | Recon Loss: 0.003540 | Commit Loss: 0.001028 | Perplexity: 1171.508719
2025-10-15 10:45:23,274 Stage: Train 0.5 | Epoch: 66 | Iter: 202800 | Total Loss: 0.004171 | Recon Loss: 0.003658 | Commit Loss: 0.001027 | Perplexity: 1173.101249
2025-10-15 10:50:46,344 Stage: Train 0.5 | Epoch: 66 | Iter: 203000 | Total Loss: 0.004039 | Recon Loss: 0.003530 | Commit Loss: 0.001017 | Perplexity: 1165.986080
2025-10-15 10:56:09,571 Stage: Train 0.5 | Epoch: 66 | Iter: 203200 | Total Loss: 0.004075 | Recon Loss: 0.003560 | Commit Loss: 0.001030 | Perplexity: 1182.344642
2025-10-15 11:01:32,435 Stage: Train 0.5 | Epoch: 66 | Iter: 203400 | Total Loss: 0.004682 | Recon Loss: 0.003818 | Commit Loss: 0.001728 | Perplexity: 1173.614509
Trainning Epoch:  41%|████      | 67/165 [91:40:26<133:40:24, 4910.45s/it]Trainning Epoch:  41%|████      | 67/165 [91:40:26<133:40:24, 4910.45s/it]2025-10-15 11:06:59,169 Stage: Train 0.5 | Epoch: 67 | Iter: 203600 | Total Loss: 0.004409 | Recon Loss: 0.003808 | Commit Loss: 0.001202 | Perplexity: 1162.921303
2025-10-15 11:12:22,628 Stage: Train 0.5 | Epoch: 67 | Iter: 203800 | Total Loss: 0.004103 | Recon Loss: 0.003578 | Commit Loss: 0.001050 | Perplexity: 1173.882434
2025-10-15 11:17:45,246 Stage: Train 0.5 | Epoch: 67 | Iter: 204000 | Total Loss: 0.004059 | Recon Loss: 0.003550 | Commit Loss: 0.001017 | Perplexity: 1165.942652
2025-10-15 11:23:07,737 Stage: Train 0.5 | Epoch: 67 | Iter: 204200 | Total Loss: 0.004013 | Recon Loss: 0.003498 | Commit Loss: 0.001030 | Perplexity: 1164.397742
2025-10-15 11:28:30,474 Stage: Train 0.5 | Epoch: 67 | Iter: 204400 | Total Loss: 0.004043 | Recon Loss: 0.003537 | Commit Loss: 0.001012 | Perplexity: 1169.951537
2025-10-15 11:33:52,959 Stage: Train 0.5 | Epoch: 67 | Iter: 204600 | Total Loss: 0.004142 | Recon Loss: 0.003622 | Commit Loss: 0.001041 | Perplexity: 1172.227657
2025-10-15 11:39:15,499 Stage: Train 0.5 | Epoch: 67 | Iter: 204800 | Total Loss: 0.004030 | Recon Loss: 0.003521 | Commit Loss: 0.001018 | Perplexity: 1171.413247
2025-10-15 11:44:37,408 Stage: Train 0.5 | Epoch: 67 | Iter: 205000 | Total Loss: 0.004081 | Recon Loss: 0.003564 | Commit Loss: 0.001034 | Perplexity: 1179.073339
2025-10-15 11:49:59,496 Stage: Train 0.5 | Epoch: 67 | Iter: 205200 | Total Loss: 0.004153 | Recon Loss: 0.003640 | Commit Loss: 0.001027 | Perplexity: 1177.731306
2025-10-15 11:55:22,154 Stage: Train 0.5 | Epoch: 67 | Iter: 205400 | Total Loss: 0.004008 | Recon Loss: 0.003498 | Commit Loss: 0.001019 | Perplexity: 1175.216982
2025-10-15 12:00:44,889 Stage: Train 0.5 | Epoch: 67 | Iter: 205600 | Total Loss: 0.003998 | Recon Loss: 0.003492 | Commit Loss: 0.001013 | Perplexity: 1172.462572
2025-10-15 12:06:07,672 Stage: Train 0.5 | Epoch: 67 | Iter: 205800 | Total Loss: 0.004100 | Recon Loss: 0.003584 | Commit Loss: 0.001031 | Perplexity: 1172.698312
2025-10-15 12:11:30,152 Stage: Train 0.5 | Epoch: 67 | Iter: 206000 | Total Loss: 0.004049 | Recon Loss: 0.003525 | Commit Loss: 0.001047 | Perplexity: 1171.234495
2025-10-15 12:16:52,730 Stage: Train 0.5 | Epoch: 67 | Iter: 206200 | Total Loss: 0.004061 | Recon Loss: 0.003554 | Commit Loss: 0.001014 | Perplexity: 1179.417181
2025-10-15 12:22:15,268 Stage: Train 0.5 | Epoch: 67 | Iter: 206400 | Total Loss: 0.004115 | Recon Loss: 0.003601 | Commit Loss: 0.001028 | Perplexity: 1181.497851
Trainning Epoch:  41%|████      | 68/165 [93:02:10<132:15:14, 4908.40s/it]Trainning Epoch:  41%|████      | 68/165 [93:02:10<132:15:15, 4908.40s/it]2025-10-15 12:27:41,339 Stage: Train 0.5 | Epoch: 68 | Iter: 206600 | Total Loss: 0.004020 | Recon Loss: 0.003497 | Commit Loss: 0.001046 | Perplexity: 1174.656978
2025-10-15 12:33:04,285 Stage: Train 0.5 | Epoch: 68 | Iter: 206800 | Total Loss: 0.004009 | Recon Loss: 0.003498 | Commit Loss: 0.001023 | Perplexity: 1172.603150
2025-10-15 12:38:27,232 Stage: Train 0.5 | Epoch: 68 | Iter: 207000 | Total Loss: 0.004142 | Recon Loss: 0.003625 | Commit Loss: 0.001034 | Perplexity: 1185.043788
2025-10-15 12:43:49,878 Stage: Train 0.5 | Epoch: 68 | Iter: 207200 | Total Loss: 0.004073 | Recon Loss: 0.003550 | Commit Loss: 0.001046 | Perplexity: 1182.298380
2025-10-15 12:49:12,751 Stage: Train 0.5 | Epoch: 68 | Iter: 207400 | Total Loss: 0.004182 | Recon Loss: 0.003654 | Commit Loss: 0.001056 | Perplexity: 1173.268308
2025-10-15 12:54:35,479 Stage: Train 0.5 | Epoch: 68 | Iter: 207600 | Total Loss: 0.003953 | Recon Loss: 0.003429 | Commit Loss: 0.001049 | Perplexity: 1180.775721
2025-10-15 12:59:58,324 Stage: Train 0.5 | Epoch: 68 | Iter: 207800 | Total Loss: 0.004064 | Recon Loss: 0.003543 | Commit Loss: 0.001042 | Perplexity: 1177.009558
2025-10-15 13:05:21,306 Stage: Train 0.5 | Epoch: 68 | Iter: 208000 | Total Loss: 0.004113 | Recon Loss: 0.003596 | Commit Loss: 0.001033 | Perplexity: 1170.384959
2025-10-15 13:10:43,746 Stage: Train 0.5 | Epoch: 68 | Iter: 208200 | Total Loss: 0.003985 | Recon Loss: 0.003466 | Commit Loss: 0.001038 | Perplexity: 1169.900136
2025-10-15 13:16:06,089 Stage: Train 0.5 | Epoch: 68 | Iter: 208400 | Total Loss: 0.004142 | Recon Loss: 0.003629 | Commit Loss: 0.001026 | Perplexity: 1172.817982
2025-10-15 13:21:28,649 Stage: Train 0.5 | Epoch: 68 | Iter: 208600 | Total Loss: 0.004054 | Recon Loss: 0.003523 | Commit Loss: 0.001063 | Perplexity: 1174.564600
2025-10-15 13:26:51,426 Stage: Train 0.5 | Epoch: 68 | Iter: 208800 | Total Loss: 0.004068 | Recon Loss: 0.003551 | Commit Loss: 0.001034 | Perplexity: 1168.947558
2025-10-15 13:32:13,646 Stage: Train 0.5 | Epoch: 68 | Iter: 209000 | Total Loss: 0.004059 | Recon Loss: 0.003528 | Commit Loss: 0.001063 | Perplexity: 1176.989862
2025-10-15 13:37:36,448 Stage: Train 0.5 | Epoch: 68 | Iter: 209200 | Total Loss: 0.003974 | Recon Loss: 0.003463 | Commit Loss: 0.001022 | Perplexity: 1178.207223
2025-10-15 13:42:59,013 Stage: Train 0.5 | Epoch: 68 | Iter: 209400 | Total Loss: 0.003996 | Recon Loss: 0.003476 | Commit Loss: 0.001040 | Perplexity: 1175.731008
2025-10-15 13:48:21,609 Stage: Train 0.5 | Epoch: 68 | Iter: 209600 | Total Loss: 0.004041 | Recon Loss: 0.003520 | Commit Loss: 0.001043 | Perplexity: 1172.247053
Trainning Epoch:  42%|████▏     | 69/165 [94:23:56<130:52:10, 4907.61s/it]Trainning Epoch:  42%|████▏     | 69/165 [94:23:56<130:52:10, 4907.61s/it]2025-10-15 13:53:48,897 Stage: Train 0.5 | Epoch: 69 | Iter: 209800 | Total Loss: 0.004045 | Recon Loss: 0.003528 | Commit Loss: 0.001035 | Perplexity: 1182.461483
2025-10-15 13:59:12,343 Stage: Train 0.5 | Epoch: 69 | Iter: 210000 | Total Loss: 0.004019 | Recon Loss: 0.003505 | Commit Loss: 0.001028 | Perplexity: 1176.284447
2025-10-15 14:04:35,393 Stage: Train 0.5 | Epoch: 69 | Iter: 210200 | Total Loss: 0.004053 | Recon Loss: 0.003535 | Commit Loss: 0.001035 | Perplexity: 1188.017427
2025-10-15 14:09:58,423 Stage: Train 0.5 | Epoch: 69 | Iter: 210400 | Total Loss: 0.004133 | Recon Loss: 0.003604 | Commit Loss: 0.001060 | Perplexity: 1179.176277
2025-10-15 14:15:21,116 Stage: Train 0.5 | Epoch: 69 | Iter: 210600 | Total Loss: 0.004019 | Recon Loss: 0.003497 | Commit Loss: 0.001043 | Perplexity: 1174.598684
2025-10-15 14:20:43,926 Stage: Train 0.5 | Epoch: 69 | Iter: 210800 | Total Loss: 0.004029 | Recon Loss: 0.003513 | Commit Loss: 0.001033 | Perplexity: 1173.955205
2025-10-15 14:26:06,531 Stage: Train 0.5 | Epoch: 69 | Iter: 211000 | Total Loss: 0.003973 | Recon Loss: 0.003454 | Commit Loss: 0.001038 | Perplexity: 1175.358597
2025-10-15 14:31:29,378 Stage: Train 0.5 | Epoch: 69 | Iter: 211200 | Total Loss: 0.004042 | Recon Loss: 0.003523 | Commit Loss: 0.001038 | Perplexity: 1176.884247
2025-10-15 14:36:52,844 Stage: Train 0.5 | Epoch: 69 | Iter: 211400 | Total Loss: 0.004038 | Recon Loss: 0.003515 | Commit Loss: 0.001045 | Perplexity: 1171.922607
2025-10-15 14:42:15,017 Stage: Train 0.5 | Epoch: 69 | Iter: 211600 | Total Loss: 0.004144 | Recon Loss: 0.003608 | Commit Loss: 0.001072 | Perplexity: 1171.827332
2025-10-15 14:47:37,569 Stage: Train 0.5 | Epoch: 69 | Iter: 211800 | Total Loss: 0.003970 | Recon Loss: 0.003451 | Commit Loss: 0.001037 | Perplexity: 1178.603581
2025-10-15 14:52:59,946 Stage: Train 0.5 | Epoch: 69 | Iter: 212000 | Total Loss: 0.004065 | Recon Loss: 0.003555 | Commit Loss: 0.001019 | Perplexity: 1168.288666
2025-10-15 14:58:22,936 Stage: Train 0.5 | Epoch: 69 | Iter: 212200 | Total Loss: 0.004077 | Recon Loss: 0.003564 | Commit Loss: 0.001026 | Perplexity: 1175.099499
2025-10-15 15:03:46,139 Stage: Train 0.5 | Epoch: 69 | Iter: 212400 | Total Loss: 0.004064 | Recon Loss: 0.003543 | Commit Loss: 0.001042 | Perplexity: 1179.916968
2025-10-15 15:09:09,365 Stage: Train 0.5 | Epoch: 69 | Iter: 212600 | Total Loss: 0.004035 | Recon Loss: 0.003508 | Commit Loss: 0.001055 | Perplexity: 1182.219915
Trainning Epoch:  42%|████▏     | 70/165 [95:45:45<129:31:04, 4908.05s/it]Trainning Epoch:  42%|████▏     | 70/165 [95:45:45<129:31:04, 4908.05s/it]2025-10-15 15:14:36,342 Stage: Train 0.5 | Epoch: 70 | Iter: 212800 | Total Loss: 0.004038 | Recon Loss: 0.003520 | Commit Loss: 0.001036 | Perplexity: 1172.989421
2025-10-15 15:19:59,407 Stage: Train 0.5 | Epoch: 70 | Iter: 213000 | Total Loss: 0.004399 | Recon Loss: 0.003752 | Commit Loss: 0.001294 | Perplexity: 1181.678316
2025-10-15 15:25:22,195 Stage: Train 0.5 | Epoch: 70 | Iter: 213200 | Total Loss: 0.003932 | Recon Loss: 0.003414 | Commit Loss: 0.001037 | Perplexity: 1170.036128
2025-10-15 15:30:45,108 Stage: Train 0.5 | Epoch: 70 | Iter: 213400 | Total Loss: 0.003995 | Recon Loss: 0.003479 | Commit Loss: 0.001032 | Perplexity: 1176.026229
2025-10-15 15:36:08,041 Stage: Train 0.5 | Epoch: 70 | Iter: 213600 | Total Loss: 0.004128 | Recon Loss: 0.003586 | Commit Loss: 0.001084 | Perplexity: 1178.844619
2025-10-15 15:41:31,181 Stage: Train 0.5 | Epoch: 70 | Iter: 213800 | Total Loss: 0.003939 | Recon Loss: 0.003425 | Commit Loss: 0.001028 | Perplexity: 1176.627945
2025-10-15 15:46:54,262 Stage: Train 0.5 | Epoch: 70 | Iter: 214000 | Total Loss: 0.004072 | Recon Loss: 0.003543 | Commit Loss: 0.001056 | Perplexity: 1180.261545
2025-10-15 15:52:17,091 Stage: Train 0.5 | Epoch: 70 | Iter: 214200 | Total Loss: 0.004086 | Recon Loss: 0.003563 | Commit Loss: 0.001045 | Perplexity: 1180.950674
2025-10-15 15:57:39,739 Stage: Train 0.5 | Epoch: 70 | Iter: 214400 | Total Loss: 0.004004 | Recon Loss: 0.003489 | Commit Loss: 0.001030 | Perplexity: 1176.593456
2025-10-15 16:03:02,532 Stage: Train 0.5 | Epoch: 70 | Iter: 214600 | Total Loss: 0.003999 | Recon Loss: 0.003480 | Commit Loss: 0.001037 | Perplexity: 1180.851059
2025-10-15 16:08:25,600 Stage: Train 0.5 | Epoch: 70 | Iter: 214800 | Total Loss: 0.004008 | Recon Loss: 0.003484 | Commit Loss: 0.001049 | Perplexity: 1176.558488
2025-10-15 16:13:48,491 Stage: Train 0.5 | Epoch: 70 | Iter: 215000 | Total Loss: 0.004060 | Recon Loss: 0.003531 | Commit Loss: 0.001058 | Perplexity: 1175.927658
2025-10-15 16:19:11,509 Stage: Train 0.5 | Epoch: 70 | Iter: 215200 | Total Loss: 0.003972 | Recon Loss: 0.003453 | Commit Loss: 0.001037 | Perplexity: 1177.054608
2025-10-15 16:24:34,357 Stage: Train 0.5 | Epoch: 70 | Iter: 215400 | Total Loss: 0.004010 | Recon Loss: 0.003488 | Commit Loss: 0.001044 | Perplexity: 1176.306310
2025-10-15 16:29:56,729 Stage: Train 0.5 | Epoch: 70 | Iter: 215600 | Total Loss: 0.004002 | Recon Loss: 0.003482 | Commit Loss: 0.001041 | Perplexity: 1179.412134
Trainning Epoch:  43%|████▎     | 71/165 [97:07:33<128:09:17, 4908.06s/it]Trainning Epoch:  43%|████▎     | 71/165 [97:07:33<128:09:17, 4908.06s/it]2025-10-15 16:35:23,316 Stage: Train 0.5 | Epoch: 71 | Iter: 215800 | Total Loss: 0.004149 | Recon Loss: 0.003624 | Commit Loss: 0.001050 | Perplexity: 1173.390681
2025-10-15 16:40:46,063 Stage: Train 0.5 | Epoch: 71 | Iter: 216000 | Total Loss: 0.003951 | Recon Loss: 0.003437 | Commit Loss: 0.001029 | Perplexity: 1180.405982
2025-10-15 16:46:09,042 Stage: Train 0.5 | Epoch: 71 | Iter: 216200 | Total Loss: 0.004011 | Recon Loss: 0.003487 | Commit Loss: 0.001047 | Perplexity: 1180.270522
2025-10-15 16:51:32,056 Stage: Train 0.5 | Epoch: 71 | Iter: 216400 | Total Loss: 0.004024 | Recon Loss: 0.003503 | Commit Loss: 0.001041 | Perplexity: 1179.232021
2025-10-15 16:56:55,103 Stage: Train 0.5 | Epoch: 71 | Iter: 216600 | Total Loss: 0.004033 | Recon Loss: 0.003497 | Commit Loss: 0.001073 | Perplexity: 1182.706754
2025-10-15 17:02:18,058 Stage: Train 0.5 | Epoch: 71 | Iter: 216800 | Total Loss: 0.003983 | Recon Loss: 0.003471 | Commit Loss: 0.001025 | Perplexity: 1180.969507
2025-10-15 17:07:40,495 Stage: Train 0.5 | Epoch: 71 | Iter: 217000 | Total Loss: 0.004035 | Recon Loss: 0.003508 | Commit Loss: 0.001054 | Perplexity: 1178.769235
2025-10-15 17:13:03,083 Stage: Train 0.5 | Epoch: 71 | Iter: 217200 | Total Loss: 0.004004 | Recon Loss: 0.003478 | Commit Loss: 0.001051 | Perplexity: 1178.424524
2025-10-15 17:18:26,248 Stage: Train 0.5 | Epoch: 71 | Iter: 217400 | Total Loss: 0.003928 | Recon Loss: 0.003415 | Commit Loss: 0.001027 | Perplexity: 1173.974626
2025-10-15 17:23:49,444 Stage: Train 0.5 | Epoch: 71 | Iter: 217600 | Total Loss: 0.004033 | Recon Loss: 0.003519 | Commit Loss: 0.001026 | Perplexity: 1178.926820
2025-10-15 17:29:12,632 Stage: Train 0.5 | Epoch: 71 | Iter: 217800 | Total Loss: 0.004065 | Recon Loss: 0.003535 | Commit Loss: 0.001060 | Perplexity: 1175.690159
2025-10-15 17:34:35,088 Stage: Train 0.5 | Epoch: 71 | Iter: 218000 | Total Loss: 0.004036 | Recon Loss: 0.003511 | Commit Loss: 0.001050 | Perplexity: 1173.906674
2025-10-15 17:39:57,794 Stage: Train 0.5 | Epoch: 71 | Iter: 218200 | Total Loss: 0.003931 | Recon Loss: 0.003414 | Commit Loss: 0.001033 | Perplexity: 1176.407584
2025-10-15 17:45:21,433 Stage: Train 0.5 | Epoch: 71 | Iter: 218400 | Total Loss: 0.004078 | Recon Loss: 0.003545 | Commit Loss: 0.001066 | Perplexity: 1180.756730
2025-10-15 17:50:43,682 Stage: Train 0.5 | Epoch: 71 | Iter: 218600 | Total Loss: 0.004033 | Recon Loss: 0.003509 | Commit Loss: 0.001048 | Perplexity: 1170.468379
Trainning Epoch:  44%|████▎     | 72/165 [98:29:22<126:47:56, 4908.34s/it]Trainning Epoch:  44%|████▎     | 72/165 [98:29:22<126:47:55, 4908.34s/it]2025-10-15 17:56:11,150 Stage: Train 0.5 | Epoch: 72 | Iter: 218800 | Total Loss: 0.004004 | Recon Loss: 0.003482 | Commit Loss: 0.001043 | Perplexity: 1180.405618
2025-10-15 18:01:33,846 Stage: Train 0.5 | Epoch: 72 | Iter: 219000 | Total Loss: 0.004012 | Recon Loss: 0.003494 | Commit Loss: 0.001035 | Perplexity: 1178.735954
2025-10-15 18:06:57,391 Stage: Train 0.5 | Epoch: 72 | Iter: 219200 | Total Loss: 0.004015 | Recon Loss: 0.003491 | Commit Loss: 0.001048 | Perplexity: 1181.607455
2025-10-15 18:12:20,460 Stage: Train 0.5 | Epoch: 72 | Iter: 219400 | Total Loss: 0.003959 | Recon Loss: 0.003442 | Commit Loss: 0.001033 | Perplexity: 1175.771604
2025-10-15 18:17:43,498 Stage: Train 0.5 | Epoch: 72 | Iter: 219600 | Total Loss: 0.003973 | Recon Loss: 0.003443 | Commit Loss: 0.001059 | Perplexity: 1169.859237
2025-10-15 18:23:05,973 Stage: Train 0.5 | Epoch: 72 | Iter: 219800 | Total Loss: 0.004121 | Recon Loss: 0.003593 | Commit Loss: 0.001056 | Perplexity: 1177.126828
2025-10-15 18:28:29,105 Stage: Train 0.5 | Epoch: 72 | Iter: 220000 | Total Loss: 0.003909 | Recon Loss: 0.003379 | Commit Loss: 0.001060 | Perplexity: 1181.626444
2025-10-15 18:28:29,105 Saving model at iteration 220000
2025-10-15 18:28:29,298 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000
2025-10-15 18:28:30,668 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000/model.safetensors
2025-10-15 18:28:32,411 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000/optimizer.bin
2025-10-15 18:28:32,411 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000/scheduler.bin
2025-10-15 18:28:32,411 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000/sampler.bin
2025-10-15 18:28:32,412 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_73_step_220000/random_states_0.pkl
2025-10-15 18:33:56,818 Stage: Train 0.5 | Epoch: 72 | Iter: 220200 | Total Loss: 0.003996 | Recon Loss: 0.003470 | Commit Loss: 0.001052 | Perplexity: 1176.147903
2025-10-15 18:39:19,406 Stage: Train 0.5 | Epoch: 72 | Iter: 220400 | Total Loss: 0.004007 | Recon Loss: 0.003487 | Commit Loss: 0.001041 | Perplexity: 1175.569233
2025-10-15 18:44:41,878 Stage: Train 0.5 | Epoch: 72 | Iter: 220600 | Total Loss: 0.004049 | Recon Loss: 0.003523 | Commit Loss: 0.001052 | Perplexity: 1172.719338
2025-10-15 18:50:03,947 Stage: Train 0.5 | Epoch: 72 | Iter: 220800 | Total Loss: 0.003879 | Recon Loss: 0.003363 | Commit Loss: 0.001033 | Perplexity: 1179.437574
2025-10-15 18:55:26,341 Stage: Train 0.5 | Epoch: 72 | Iter: 221000 | Total Loss: 0.004072 | Recon Loss: 0.003549 | Commit Loss: 0.001047 | Perplexity: 1175.784730
2025-10-15 19:00:48,506 Stage: Train 0.5 | Epoch: 72 | Iter: 221200 | Total Loss: 0.004023 | Recon Loss: 0.003499 | Commit Loss: 0.001046 | Perplexity: 1178.583396
2025-10-15 19:06:11,048 Stage: Train 0.5 | Epoch: 72 | Iter: 221400 | Total Loss: 0.003949 | Recon Loss: 0.003424 | Commit Loss: 0.001049 | Perplexity: 1172.588543
2025-10-15 19:11:33,666 Stage: Train 0.5 | Epoch: 72 | Iter: 221600 | Total Loss: 0.004004 | Recon Loss: 0.003480 | Commit Loss: 0.001048 | Perplexity: 1176.641907
Trainning Epoch:  44%|████▍     | 73/165 [99:51:13<125:27:22, 4909.16s/it]Trainning Epoch:  44%|████▍     | 73/165 [99:51:13<125:27:22, 4909.16s/it]2025-10-15 19:17:00,475 Stage: Train 0.5 | Epoch: 73 | Iter: 221800 | Total Loss: 0.003961 | Recon Loss: 0.003426 | Commit Loss: 0.001069 | Perplexity: 1172.607144
2025-10-15 19:22:22,871 Stage: Train 0.5 | Epoch: 73 | Iter: 222000 | Total Loss: 0.004051 | Recon Loss: 0.003527 | Commit Loss: 0.001049 | Perplexity: 1177.336464
2025-10-15 19:27:45,533 Stage: Train 0.5 | Epoch: 73 | Iter: 222200 | Total Loss: 0.003981 | Recon Loss: 0.003462 | Commit Loss: 0.001040 | Perplexity: 1178.385549
2025-10-15 19:33:07,907 Stage: Train 0.5 | Epoch: 73 | Iter: 222400 | Total Loss: 0.004016 | Recon Loss: 0.003484 | Commit Loss: 0.001063 | Perplexity: 1178.438963
2025-10-15 19:38:30,634 Stage: Train 0.5 | Epoch: 73 | Iter: 222600 | Total Loss: 0.003900 | Recon Loss: 0.003385 | Commit Loss: 0.001031 | Perplexity: 1181.304353
2025-10-15 19:43:53,472 Stage: Train 0.5 | Epoch: 73 | Iter: 222800 | Total Loss: 0.004011 | Recon Loss: 0.003487 | Commit Loss: 0.001049 | Perplexity: 1175.674780
2025-10-15 19:49:16,081 Stage: Train 0.5 | Epoch: 73 | Iter: 223000 | Total Loss: 0.003992 | Recon Loss: 0.003454 | Commit Loss: 0.001078 | Perplexity: 1183.192656
2025-10-15 19:54:38,797 Stage: Train 0.5 | Epoch: 73 | Iter: 223200 | Total Loss: 0.004031 | Recon Loss: 0.003486 | Commit Loss: 0.001090 | Perplexity: 1181.796106
2025-10-15 20:00:01,202 Stage: Train 0.5 | Epoch: 73 | Iter: 223400 | Total Loss: 0.004027 | Recon Loss: 0.003509 | Commit Loss: 0.001035 | Perplexity: 1175.757585
2025-10-15 20:05:23,560 Stage: Train 0.5 | Epoch: 73 | Iter: 223600 | Total Loss: 0.004004 | Recon Loss: 0.003490 | Commit Loss: 0.001027 | Perplexity: 1176.946550
2025-10-15 20:10:45,965 Stage: Train 0.5 | Epoch: 73 | Iter: 223800 | Total Loss: 0.003989 | Recon Loss: 0.003463 | Commit Loss: 0.001052 | Perplexity: 1182.443468
2025-10-15 20:16:08,498 Stage: Train 0.5 | Epoch: 73 | Iter: 224000 | Total Loss: 0.003925 | Recon Loss: 0.003408 | Commit Loss: 0.001034 | Perplexity: 1181.842572
2025-10-15 20:21:30,983 Stage: Train 0.5 | Epoch: 73 | Iter: 224200 | Total Loss: 0.003964 | Recon Loss: 0.003439 | Commit Loss: 0.001050 | Perplexity: 1184.380150
2025-10-15 20:26:53,634 Stage: Train 0.5 | Epoch: 73 | Iter: 224400 | Total Loss: 0.004038 | Recon Loss: 0.003517 | Commit Loss: 0.001041 | Perplexity: 1180.401823
2025-10-15 20:32:16,247 Stage: Train 0.5 | Epoch: 73 | Iter: 224600 | Total Loss: 0.003949 | Recon Loss: 0.003423 | Commit Loss: 0.001053 | Perplexity: 1177.760793
2025-10-15 20:37:38,956 Stage: Train 0.5 | Epoch: 73 | Iter: 224800 | Total Loss: 0.004036 | Recon Loss: 0.003511 | Commit Loss: 0.001050 | Perplexity: 1178.825632
Trainning Epoch:  45%|████▍     | 74/165 [101:12:57<124:03:10, 4907.59s/it]Trainning Epoch:  45%|████▍     | 74/165 [101:12:57<124:03:10, 4907.59s/it]2025-10-15 20:43:05,906 Stage: Train 0.5 | Epoch: 74 | Iter: 225000 | Total Loss: 0.003967 | Recon Loss: 0.003449 | Commit Loss: 0.001036 | Perplexity: 1183.063821
2025-10-15 20:48:28,210 Stage: Train 0.5 | Epoch: 74 | Iter: 225200 | Total Loss: 0.003976 | Recon Loss: 0.003450 | Commit Loss: 0.001052 | Perplexity: 1178.508457
2025-10-15 20:53:50,956 Stage: Train 0.5 | Epoch: 74 | Iter: 225400 | Total Loss: 0.003949 | Recon Loss: 0.003430 | Commit Loss: 0.001037 | Perplexity: 1174.306128
2025-10-15 20:59:13,530 Stage: Train 0.5 | Epoch: 74 | Iter: 225600 | Total Loss: 0.003935 | Recon Loss: 0.003420 | Commit Loss: 0.001029 | Perplexity: 1180.792902
2025-10-15 21:04:36,010 Stage: Train 0.5 | Epoch: 74 | Iter: 225800 | Total Loss: 0.003934 | Recon Loss: 0.003411 | Commit Loss: 0.001048 | Perplexity: 1178.693420
2025-10-15 21:09:58,261 Stage: Train 0.5 | Epoch: 74 | Iter: 226000 | Total Loss: 0.004132 | Recon Loss: 0.003593 | Commit Loss: 0.001077 | Perplexity: 1190.159581
2025-10-15 21:15:20,911 Stage: Train 0.5 | Epoch: 74 | Iter: 226200 | Total Loss: 0.003997 | Recon Loss: 0.003480 | Commit Loss: 0.001035 | Perplexity: 1173.758566
2025-10-15 21:20:43,423 Stage: Train 0.5 | Epoch: 74 | Iter: 226400 | Total Loss: 0.003989 | Recon Loss: 0.003454 | Commit Loss: 0.001069 | Perplexity: 1174.303902
2025-10-15 21:26:05,996 Stage: Train 0.5 | Epoch: 74 | Iter: 226600 | Total Loss: 0.003993 | Recon Loss: 0.003475 | Commit Loss: 0.001037 | Perplexity: 1172.563171
2025-10-15 21:31:28,662 Stage: Train 0.5 | Epoch: 74 | Iter: 226800 | Total Loss: 0.003936 | Recon Loss: 0.003417 | Commit Loss: 0.001038 | Perplexity: 1179.676404
2025-10-15 21:36:51,180 Stage: Train 0.5 | Epoch: 74 | Iter: 227000 | Total Loss: 0.003954 | Recon Loss: 0.003438 | Commit Loss: 0.001033 | Perplexity: 1164.109030
2025-10-15 21:42:13,691 Stage: Train 0.5 | Epoch: 74 | Iter: 227200 | Total Loss: 0.004010 | Recon Loss: 0.003490 | Commit Loss: 0.001039 | Perplexity: 1177.452240
2025-10-15 21:47:36,239 Stage: Train 0.5 | Epoch: 74 | Iter: 227400 | Total Loss: 0.003940 | Recon Loss: 0.003417 | Commit Loss: 0.001045 | Perplexity: 1180.752894
2025-10-15 21:52:58,589 Stage: Train 0.5 | Epoch: 74 | Iter: 227600 | Total Loss: 0.004073 | Recon Loss: 0.003556 | Commit Loss: 0.001035 | Perplexity: 1178.118608
2025-10-15 21:58:21,297 Stage: Train 0.5 | Epoch: 74 | Iter: 227800 | Total Loss: 0.003881 | Recon Loss: 0.003362 | Commit Loss: 0.001038 | Perplexity: 1175.315422
Trainning Epoch:  45%|████▌     | 75/165 [102:34:40<122:39:32, 4906.36s/it]Trainning Epoch:  45%|████▌     | 75/165 [102:34:40<122:39:32, 4906.36s/it]2025-10-15 22:03:48,196 Stage: Train 0.5 | Epoch: 75 | Iter: 228000 | Total Loss: 0.004004 | Recon Loss: 0.003475 | Commit Loss: 0.001058 | Perplexity: 1172.040869
2025-10-15 22:09:10,865 Stage: Train 0.5 | Epoch: 75 | Iter: 228200 | Total Loss: 0.003940 | Recon Loss: 0.003419 | Commit Loss: 0.001041 | Perplexity: 1172.626769
2025-10-15 22:14:33,235 Stage: Train 0.5 | Epoch: 75 | Iter: 228400 | Total Loss: 0.004039 | Recon Loss: 0.003514 | Commit Loss: 0.001049 | Perplexity: 1181.087236
2025-10-15 22:19:55,879 Stage: Train 0.5 | Epoch: 75 | Iter: 228600 | Total Loss: 0.003835 | Recon Loss: 0.003315 | Commit Loss: 0.001038 | Perplexity: 1171.854690
2025-10-15 22:25:18,969 Stage: Train 0.5 | Epoch: 75 | Iter: 228800 | Total Loss: 0.003966 | Recon Loss: 0.003448 | Commit Loss: 0.001037 | Perplexity: 1174.353461
2025-10-15 22:30:41,716 Stage: Train 0.5 | Epoch: 75 | Iter: 229000 | Total Loss: 0.003826 | Recon Loss: 0.003307 | Commit Loss: 0.001038 | Perplexity: 1178.175395
2025-10-15 22:36:04,558 Stage: Train 0.5 | Epoch: 75 | Iter: 229200 | Total Loss: 0.003959 | Recon Loss: 0.003439 | Commit Loss: 0.001040 | Perplexity: 1187.297755
2025-10-15 22:41:27,480 Stage: Train 0.5 | Epoch: 75 | Iter: 229400 | Total Loss: 0.003995 | Recon Loss: 0.003467 | Commit Loss: 0.001057 | Perplexity: 1177.073709
2025-10-15 22:46:49,703 Stage: Train 0.5 | Epoch: 75 | Iter: 229600 | Total Loss: 0.004044 | Recon Loss: 0.003529 | Commit Loss: 0.001031 | Perplexity: 1184.427941
2025-10-15 22:52:12,355 Stage: Train 0.5 | Epoch: 75 | Iter: 229800 | Total Loss: 0.004019 | Recon Loss: 0.003490 | Commit Loss: 0.001059 | Perplexity: 1180.579237
2025-10-15 22:57:34,999 Stage: Train 0.5 | Epoch: 75 | Iter: 230000 | Total Loss: 0.003955 | Recon Loss: 0.003417 | Commit Loss: 0.001076 | Perplexity: 1185.043521
2025-10-15 23:02:57,677 Stage: Train 0.5 | Epoch: 75 | Iter: 230200 | Total Loss: 0.005072 | Recon Loss: 0.004156 | Commit Loss: 0.001832 | Perplexity: 1179.500963
2025-10-15 23:08:20,366 Stage: Train 0.5 | Epoch: 75 | Iter: 230400 | Total Loss: 0.003933 | Recon Loss: 0.003421 | Commit Loss: 0.001024 | Perplexity: 1155.668192
2025-10-15 23:13:42,973 Stage: Train 0.5 | Epoch: 75 | Iter: 230600 | Total Loss: 0.003943 | Recon Loss: 0.003439 | Commit Loss: 0.001006 | Perplexity: 1175.337529
2025-10-15 23:19:05,706 Stage: Train 0.5 | Epoch: 75 | Iter: 230800 | Total Loss: 0.003956 | Recon Loss: 0.003439 | Commit Loss: 0.001035 | Perplexity: 1169.523114
Trainning Epoch:  46%|████▌     | 76/165 [103:56:26<121:17:29, 4906.18s/it]Trainning Epoch:  46%|████▌     | 76/165 [103:56:26<121:17:29, 4906.18s/it]2025-10-15 23:24:32,434 Stage: Train 0.5 | Epoch: 76 | Iter: 231000 | Total Loss: 0.003924 | Recon Loss: 0.003386 | Commit Loss: 0.001077 | Perplexity: 1178.378879
2025-10-15 23:29:55,258 Stage: Train 0.5 | Epoch: 76 | Iter: 231200 | Total Loss: 0.003945 | Recon Loss: 0.003439 | Commit Loss: 0.001010 | Perplexity: 1175.117083
2025-10-15 23:35:17,783 Stage: Train 0.5 | Epoch: 76 | Iter: 231400 | Total Loss: 0.003921 | Recon Loss: 0.003405 | Commit Loss: 0.001032 | Perplexity: 1180.760094
2025-10-15 23:40:40,472 Stage: Train 0.5 | Epoch: 76 | Iter: 231600 | Total Loss: 0.004056 | Recon Loss: 0.003546 | Commit Loss: 0.001020 | Perplexity: 1171.344156
2025-10-15 23:46:03,042 Stage: Train 0.5 | Epoch: 76 | Iter: 231800 | Total Loss: 0.003907 | Recon Loss: 0.003394 | Commit Loss: 0.001026 | Perplexity: 1171.488932
2025-10-15 23:51:25,618 Stage: Train 0.5 | Epoch: 76 | Iter: 232000 | Total Loss: 0.003994 | Recon Loss: 0.003476 | Commit Loss: 0.001037 | Perplexity: 1172.489540
2025-10-15 23:56:48,370 Stage: Train 0.5 | Epoch: 76 | Iter: 232200 | Total Loss: 0.003978 | Recon Loss: 0.003458 | Commit Loss: 0.001040 | Perplexity: 1177.389856
2025-10-16 00:02:10,516 Stage: Train 0.5 | Epoch: 76 | Iter: 232400 | Total Loss: 0.003918 | Recon Loss: 0.003401 | Commit Loss: 0.001035 | Perplexity: 1179.039686
2025-10-16 00:07:33,763 Stage: Train 0.5 | Epoch: 76 | Iter: 232600 | Total Loss: 0.004046 | Recon Loss: 0.003532 | Commit Loss: 0.001029 | Perplexity: 1178.297171
2025-10-16 00:12:56,163 Stage: Train 0.5 | Epoch: 76 | Iter: 232800 | Total Loss: 0.003996 | Recon Loss: 0.003464 | Commit Loss: 0.001063 | Perplexity: 1179.434533
2025-10-16 00:18:18,549 Stage: Train 0.5 | Epoch: 76 | Iter: 233000 | Total Loss: 0.003963 | Recon Loss: 0.003439 | Commit Loss: 0.001048 | Perplexity: 1176.535225
2025-10-16 00:23:41,354 Stage: Train 0.5 | Epoch: 76 | Iter: 233200 | Total Loss: 0.003925 | Recon Loss: 0.003404 | Commit Loss: 0.001041 | Perplexity: 1177.059508
2025-10-16 00:29:03,852 Stage: Train 0.5 | Epoch: 76 | Iter: 233400 | Total Loss: 0.003973 | Recon Loss: 0.003462 | Commit Loss: 0.001023 | Perplexity: 1184.077932
2025-10-16 00:34:26,730 Stage: Train 0.5 | Epoch: 76 | Iter: 233600 | Total Loss: 0.003894 | Recon Loss: 0.003374 | Commit Loss: 0.001040 | Perplexity: 1181.492643
2025-10-16 00:39:49,852 Stage: Train 0.5 | Epoch: 76 | Iter: 233800 | Total Loss: 0.003974 | Recon Loss: 0.003456 | Commit Loss: 0.001036 | Perplexity: 1176.428856
Trainning Epoch:  47%|████▋     | 77/165 [105:18:12<119:55:31, 4906.04s/it]Trainning Epoch:  47%|████▋     | 77/165 [105:18:12<119:55:31, 4906.04s/it]2025-10-16 00:45:17,029 Stage: Train 0.5 | Epoch: 77 | Iter: 234000 | Total Loss: 0.003916 | Recon Loss: 0.003397 | Commit Loss: 0.001038 | Perplexity: 1181.837438
2025-10-16 00:50:39,903 Stage: Train 0.5 | Epoch: 77 | Iter: 234200 | Total Loss: 0.003943 | Recon Loss: 0.003425 | Commit Loss: 0.001035 | Perplexity: 1178.088640
2025-10-16 00:56:02,933 Stage: Train 0.5 | Epoch: 77 | Iter: 234400 | Total Loss: 0.003903 | Recon Loss: 0.003375 | Commit Loss: 0.001057 | Perplexity: 1181.407317
2025-10-16 01:01:25,861 Stage: Train 0.5 | Epoch: 77 | Iter: 234600 | Total Loss: 0.003911 | Recon Loss: 0.003384 | Commit Loss: 0.001053 | Perplexity: 1184.557684
2025-10-16 01:06:48,650 Stage: Train 0.5 | Epoch: 77 | Iter: 234800 | Total Loss: 0.003976 | Recon Loss: 0.003461 | Commit Loss: 0.001029 | Perplexity: 1175.641194
2025-10-16 01:12:11,687 Stage: Train 0.5 | Epoch: 77 | Iter: 235000 | Total Loss: 0.003927 | Recon Loss: 0.003401 | Commit Loss: 0.001053 | Perplexity: 1186.617078
2025-10-16 01:17:34,460 Stage: Train 0.5 | Epoch: 77 | Iter: 235200 | Total Loss: 0.003993 | Recon Loss: 0.003470 | Commit Loss: 0.001047 | Perplexity: 1177.498234
2025-10-16 01:22:57,191 Stage: Train 0.5 | Epoch: 77 | Iter: 235400 | Total Loss: 0.003964 | Recon Loss: 0.003439 | Commit Loss: 0.001050 | Perplexity: 1171.682341
2025-10-16 01:28:20,210 Stage: Train 0.5 | Epoch: 77 | Iter: 235600 | Total Loss: 0.003901 | Recon Loss: 0.003388 | Commit Loss: 0.001028 | Perplexity: 1183.364679
2025-10-16 01:33:42,813 Stage: Train 0.5 | Epoch: 77 | Iter: 235800 | Total Loss: 0.003915 | Recon Loss: 0.003395 | Commit Loss: 0.001040 | Perplexity: 1176.000112
2025-10-16 01:39:05,348 Stage: Train 0.5 | Epoch: 77 | Iter: 236000 | Total Loss: 0.003938 | Recon Loss: 0.003418 | Commit Loss: 0.001039 | Perplexity: 1179.863242
2025-10-16 01:44:28,322 Stage: Train 0.5 | Epoch: 77 | Iter: 236200 | Total Loss: 0.003950 | Recon Loss: 0.003425 | Commit Loss: 0.001050 | Perplexity: 1183.798436
2025-10-16 01:49:50,738 Stage: Train 0.5 | Epoch: 77 | Iter: 236400 | Total Loss: 0.003959 | Recon Loss: 0.003423 | Commit Loss: 0.001071 | Perplexity: 1176.374588
2025-10-16 01:55:12,720 Stage: Train 0.5 | Epoch: 77 | Iter: 236600 | Total Loss: 0.003975 | Recon Loss: 0.003446 | Commit Loss: 0.001058 | Perplexity: 1180.468122
2025-10-16 02:00:35,343 Stage: Train 0.5 | Epoch: 77 | Iter: 236800 | Total Loss: 0.003883 | Recon Loss: 0.003364 | Commit Loss: 0.001037 | Perplexity: 1177.061011
Trainning Epoch:  47%|████▋     | 78/165 [106:39:59<118:34:06, 4906.28s/it]Trainning Epoch:  47%|████▋     | 78/165 [106:39:59<118:34:06, 4906.28s/it]2025-10-16 02:06:02,428 Stage: Train 0.5 | Epoch: 78 | Iter: 237000 | Total Loss: 0.003958 | Recon Loss: 0.003444 | Commit Loss: 0.001027 | Perplexity: 1175.890587
2025-10-16 02:11:25,421 Stage: Train 0.5 | Epoch: 78 | Iter: 237200 | Total Loss: 0.003903 | Recon Loss: 0.003377 | Commit Loss: 0.001053 | Perplexity: 1179.328157
2025-10-16 02:16:48,263 Stage: Train 0.5 | Epoch: 78 | Iter: 237400 | Total Loss: 0.003976 | Recon Loss: 0.003451 | Commit Loss: 0.001050 | Perplexity: 1175.199485
2025-10-16 02:22:10,726 Stage: Train 0.5 | Epoch: 78 | Iter: 237600 | Total Loss: 0.003999 | Recon Loss: 0.003465 | Commit Loss: 0.001067 | Perplexity: 1179.323400
2025-10-16 02:27:32,903 Stage: Train 0.5 | Epoch: 78 | Iter: 237800 | Total Loss: 0.003913 | Recon Loss: 0.003393 | Commit Loss: 0.001041 | Perplexity: 1178.211487
2025-10-16 02:32:55,329 Stage: Train 0.5 | Epoch: 78 | Iter: 238000 | Total Loss: 0.003950 | Recon Loss: 0.003424 | Commit Loss: 0.001053 | Perplexity: 1178.383117
2025-10-16 02:38:18,107 Stage: Train 0.5 | Epoch: 78 | Iter: 238200 | Total Loss: 0.003827 | Recon Loss: 0.003304 | Commit Loss: 0.001047 | Perplexity: 1181.218688
2025-10-16 02:43:40,149 Stage: Train 0.5 | Epoch: 78 | Iter: 238400 | Total Loss: 0.004059 | Recon Loss: 0.003533 | Commit Loss: 0.001051 | Perplexity: 1179.964880
2025-10-16 02:49:03,094 Stage: Train 0.5 | Epoch: 78 | Iter: 238600 | Total Loss: 0.003901 | Recon Loss: 0.003389 | Commit Loss: 0.001024 | Perplexity: 1180.704510
2025-10-16 02:54:26,116 Stage: Train 0.5 | Epoch: 78 | Iter: 238800 | Total Loss: 0.003905 | Recon Loss: 0.003378 | Commit Loss: 0.001054 | Perplexity: 1171.808862
2025-10-16 02:59:48,553 Stage: Train 0.5 | Epoch: 78 | Iter: 239000 | Total Loss: 0.003966 | Recon Loss: 0.003446 | Commit Loss: 0.001039 | Perplexity: 1180.630501
2025-10-16 03:05:11,292 Stage: Train 0.5 | Epoch: 78 | Iter: 239200 | Total Loss: 0.003880 | Recon Loss: 0.003348 | Commit Loss: 0.001065 | Perplexity: 1187.856843
2025-10-16 03:10:34,391 Stage: Train 0.5 | Epoch: 78 | Iter: 239400 | Total Loss: 0.003964 | Recon Loss: 0.003440 | Commit Loss: 0.001049 | Perplexity: 1185.347212
2025-10-16 03:15:56,941 Stage: Train 0.5 | Epoch: 78 | Iter: 239600 | Total Loss: 0.003922 | Recon Loss: 0.003402 | Commit Loss: 0.001039 | Perplexity: 1178.408949
2025-10-16 03:21:19,095 Stage: Train 0.5 | Epoch: 78 | Iter: 239800 | Total Loss: 0.003951 | Recon Loss: 0.003421 | Commit Loss: 0.001059 | Perplexity: 1182.740416
2025-10-16 03:26:41,105 Stage: Train 0.5 | Epoch: 78 | Iter: 240000 | Total Loss: 0.003868 | Recon Loss: 0.003350 | Commit Loss: 0.001037 | Perplexity: 1179.484475
2025-10-16 03:26:41,105 Saving model at iteration 240000
2025-10-16 03:26:41,250 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000
2025-10-16 03:26:42,593 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000/model.safetensors
2025-10-16 03:26:44,376 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000/optimizer.bin
2025-10-16 03:26:44,376 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000/scheduler.bin
2025-10-16 03:26:44,376 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000/sampler.bin
2025-10-16 03:26:44,377 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_79_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 79/165 [108:01:47<117:13:02, 4906.77s/it]Trainning Epoch:  48%|████▊     | 79/165 [108:01:47<117:13:02, 4906.77s/it]2025-10-16 03:32:12,046 Stage: Train 0.5 | Epoch: 79 | Iter: 240200 | Total Loss: 0.003986 | Recon Loss: 0.003458 | Commit Loss: 0.001055 | Perplexity: 1173.887382
2025-10-16 03:37:34,413 Stage: Train 0.5 | Epoch: 79 | Iter: 240400 | Total Loss: 0.003838 | Recon Loss: 0.003316 | Commit Loss: 0.001044 | Perplexity: 1188.860692
2025-10-16 03:42:56,444 Stage: Train 0.5 | Epoch: 79 | Iter: 240600 | Total Loss: 0.003937 | Recon Loss: 0.003415 | Commit Loss: 0.001044 | Perplexity: 1176.298786
2025-10-16 03:48:19,066 Stage: Train 0.5 | Epoch: 79 | Iter: 240800 | Total Loss: 0.003896 | Recon Loss: 0.003376 | Commit Loss: 0.001040 | Perplexity: 1186.249948
2025-10-16 03:53:41,776 Stage: Train 0.5 | Epoch: 79 | Iter: 241000 | Total Loss: 0.003835 | Recon Loss: 0.003321 | Commit Loss: 0.001029 | Perplexity: 1178.622961
2025-10-16 03:59:04,294 Stage: Train 0.5 | Epoch: 79 | Iter: 241200 | Total Loss: 0.003839 | Recon Loss: 0.003319 | Commit Loss: 0.001039 | Perplexity: 1176.099687
2025-10-16 04:04:26,789 Stage: Train 0.5 | Epoch: 79 | Iter: 241400 | Total Loss: 0.003968 | Recon Loss: 0.003446 | Commit Loss: 0.001044 | Perplexity: 1185.533560
2025-10-16 04:09:48,737 Stage: Train 0.5 | Epoch: 79 | Iter: 241600 | Total Loss: 0.003883 | Recon Loss: 0.003352 | Commit Loss: 0.001062 | Perplexity: 1184.578784
2025-10-16 04:15:10,654 Stage: Train 0.5 | Epoch: 79 | Iter: 241800 | Total Loss: 0.003861 | Recon Loss: 0.003337 | Commit Loss: 0.001049 | Perplexity: 1181.361338
2025-10-16 04:20:32,682 Stage: Train 0.5 | Epoch: 79 | Iter: 242000 | Total Loss: 0.003924 | Recon Loss: 0.003396 | Commit Loss: 0.001056 | Perplexity: 1181.529646
2025-10-16 04:25:54,920 Stage: Train 0.5 | Epoch: 79 | Iter: 242200 | Total Loss: 0.004012 | Recon Loss: 0.003480 | Commit Loss: 0.001063 | Perplexity: 1179.957203
2025-10-16 04:31:17,786 Stage: Train 0.5 | Epoch: 79 | Iter: 242400 | Total Loss: 0.003950 | Recon Loss: 0.003429 | Commit Loss: 0.001041 | Perplexity: 1176.127622
2025-10-16 04:36:40,164 Stage: Train 0.5 | Epoch: 79 | Iter: 242600 | Total Loss: 0.003903 | Recon Loss: 0.003380 | Commit Loss: 0.001047 | Perplexity: 1189.072367
2025-10-16 04:42:02,700 Stage: Train 0.5 | Epoch: 79 | Iter: 242800 | Total Loss: 0.003958 | Recon Loss: 0.003422 | Commit Loss: 0.001071 | Perplexity: 1174.243123
2025-10-16 04:47:25,048 Stage: Train 0.5 | Epoch: 79 | Iter: 243000 | Total Loss: 0.003920 | Recon Loss: 0.003403 | Commit Loss: 0.001036 | Perplexity: 1180.081530
Trainning Epoch:  48%|████▊     | 80/165 [109:23:28<115:49:01, 4905.20s/it]Trainning Epoch:  48%|████▊     | 80/165 [109:23:28<115:49:01, 4905.19s/it]2025-10-16 04:52:51,823 Stage: Train 0.5 | Epoch: 80 | Iter: 243200 | Total Loss: 0.003881 | Recon Loss: 0.003365 | Commit Loss: 0.001032 | Perplexity: 1182.324097
2025-10-16 04:58:14,512 Stage: Train 0.5 | Epoch: 80 | Iter: 243400 | Total Loss: 0.003907 | Recon Loss: 0.003390 | Commit Loss: 0.001033 | Perplexity: 1174.994150
2025-10-16 05:03:37,197 Stage: Train 0.5 | Epoch: 80 | Iter: 243600 | Total Loss: 0.003882 | Recon Loss: 0.003364 | Commit Loss: 0.001035 | Perplexity: 1171.450158
2025-10-16 05:08:59,657 Stage: Train 0.5 | Epoch: 80 | Iter: 243800 | Total Loss: 0.003902 | Recon Loss: 0.003380 | Commit Loss: 0.001044 | Perplexity: 1181.512197
2025-10-16 05:14:22,303 Stage: Train 0.5 | Epoch: 80 | Iter: 244000 | Total Loss: 0.003956 | Recon Loss: 0.003433 | Commit Loss: 0.001046 | Perplexity: 1179.497024
2025-10-16 05:19:44,811 Stage: Train 0.5 | Epoch: 80 | Iter: 244200 | Total Loss: 0.003902 | Recon Loss: 0.003369 | Commit Loss: 0.001065 | Perplexity: 1179.875962
2025-10-16 05:25:07,275 Stage: Train 0.5 | Epoch: 80 | Iter: 244400 | Total Loss: 0.003853 | Recon Loss: 0.003314 | Commit Loss: 0.001076 | Perplexity: 1189.326473
2025-10-16 05:30:29,939 Stage: Train 0.5 | Epoch: 80 | Iter: 244600 | Total Loss: 0.003901 | Recon Loss: 0.003387 | Commit Loss: 0.001029 | Perplexity: 1180.137395
2025-10-16 05:35:52,559 Stage: Train 0.5 | Epoch: 80 | Iter: 244800 | Total Loss: 0.003885 | Recon Loss: 0.003356 | Commit Loss: 0.001058 | Perplexity: 1169.518483
2025-10-16 05:41:14,705 Stage: Train 0.5 | Epoch: 80 | Iter: 245000 | Total Loss: 0.003888 | Recon Loss: 0.003377 | Commit Loss: 0.001023 | Perplexity: 1178.851285
2025-10-16 05:46:36,279 Stage: Train 0.5 | Epoch: 80 | Iter: 245200 | Total Loss: 0.003865 | Recon Loss: 0.003344 | Commit Loss: 0.001041 | Perplexity: 1186.931357
2025-10-16 05:51:58,736 Stage: Train 0.5 | Epoch: 80 | Iter: 245400 | Total Loss: 0.003883 | Recon Loss: 0.003373 | Commit Loss: 0.001019 | Perplexity: 1177.685214
2025-10-16 05:57:21,532 Stage: Train 0.5 | Epoch: 80 | Iter: 245600 | Total Loss: 0.003871 | Recon Loss: 0.003349 | Commit Loss: 0.001044 | Perplexity: 1179.941749
2025-10-16 06:02:43,903 Stage: Train 0.5 | Epoch: 80 | Iter: 245800 | Total Loss: 0.003968 | Recon Loss: 0.003446 | Commit Loss: 0.001044 | Perplexity: 1182.747234
2025-10-16 06:08:06,743 Stage: Train 0.5 | Epoch: 80 | Iter: 246000 | Total Loss: 0.003895 | Recon Loss: 0.003374 | Commit Loss: 0.001042 | Perplexity: 1174.427095
Trainning Epoch:  49%|████▉     | 81/165 [110:45:11<114:26:24, 4904.57s/it]Trainning Epoch:  49%|████▉     | 81/165 [110:45:11<114:26:24, 4904.58s/it]2025-10-16 06:13:33,849 Stage: Train 0.5 | Epoch: 81 | Iter: 246200 | Total Loss: 0.003924 | Recon Loss: 0.003398 | Commit Loss: 0.001052 | Perplexity: 1181.394612
2025-10-16 06:18:56,566 Stage: Train 0.5 | Epoch: 81 | Iter: 246400 | Total Loss: 0.003833 | Recon Loss: 0.003304 | Commit Loss: 0.001057 | Perplexity: 1185.984152
2025-10-16 06:24:19,617 Stage: Train 0.5 | Epoch: 81 | Iter: 246600 | Total Loss: 0.003894 | Recon Loss: 0.003377 | Commit Loss: 0.001034 | Perplexity: 1176.346358
2025-10-16 06:29:42,000 Stage: Train 0.5 | Epoch: 81 | Iter: 246800 | Total Loss: 0.003875 | Recon Loss: 0.003360 | Commit Loss: 0.001030 | Perplexity: 1180.304830
2025-10-16 06:35:04,890 Stage: Train 0.5 | Epoch: 81 | Iter: 247000 | Total Loss: 0.003888 | Recon Loss: 0.003370 | Commit Loss: 0.001036 | Perplexity: 1187.883748
2025-10-16 06:40:27,670 Stage: Train 0.5 | Epoch: 81 | Iter: 247200 | Total Loss: 0.004004 | Recon Loss: 0.003477 | Commit Loss: 0.001053 | Perplexity: 1184.395684
2025-10-16 06:45:50,305 Stage: Train 0.5 | Epoch: 81 | Iter: 247400 | Total Loss: 0.003826 | Recon Loss: 0.003311 | Commit Loss: 0.001030 | Perplexity: 1180.505175
2025-10-16 06:51:13,052 Stage: Train 0.5 | Epoch: 81 | Iter: 247600 | Total Loss: 0.003898 | Recon Loss: 0.003369 | Commit Loss: 0.001058 | Perplexity: 1187.509689
2025-10-16 06:56:35,986 Stage: Train 0.5 | Epoch: 81 | Iter: 247800 | Total Loss: 0.003933 | Recon Loss: 0.003412 | Commit Loss: 0.001043 | Perplexity: 1182.033394
2025-10-16 07:01:58,331 Stage: Train 0.5 | Epoch: 81 | Iter: 248000 | Total Loss: 0.003881 | Recon Loss: 0.003341 | Commit Loss: 0.001081 | Perplexity: 1175.132029
2025-10-16 07:07:21,529 Stage: Train 0.5 | Epoch: 81 | Iter: 248200 | Total Loss: 0.003999 | Recon Loss: 0.003456 | Commit Loss: 0.001086 | Perplexity: 1173.509506
2025-10-16 07:12:44,909 Stage: Train 0.5 | Epoch: 81 | Iter: 248400 | Total Loss: 0.003834 | Recon Loss: 0.003321 | Commit Loss: 0.001026 | Perplexity: 1181.961311
2025-10-16 07:18:07,743 Stage: Train 0.5 | Epoch: 81 | Iter: 248600 | Total Loss: 0.003932 | Recon Loss: 0.003409 | Commit Loss: 0.001046 | Perplexity: 1183.094792
2025-10-16 07:23:30,871 Stage: Train 0.5 | Epoch: 81 | Iter: 248800 | Total Loss: 0.003908 | Recon Loss: 0.003391 | Commit Loss: 0.001032 | Perplexity: 1176.542845
2025-10-16 07:28:53,844 Stage: Train 0.5 | Epoch: 81 | Iter: 249000 | Total Loss: 0.003895 | Recon Loss: 0.003372 | Commit Loss: 0.001047 | Perplexity: 1180.667075
Trainning Epoch:  50%|████▉     | 82/165 [112:06:59<113:06:10, 4905.67s/it]Trainning Epoch:  50%|████▉     | 82/165 [112:07:00<113:06:10, 4905.67s/it]2025-10-16 07:34:20,936 Stage: Train 0.5 | Epoch: 82 | Iter: 249200 | Total Loss: 0.003845 | Recon Loss: 0.003333 | Commit Loss: 0.001025 | Perplexity: 1181.321796
2025-10-16 07:39:43,149 Stage: Train 0.5 | Epoch: 82 | Iter: 249400 | Total Loss: 0.003821 | Recon Loss: 0.003308 | Commit Loss: 0.001025 | Perplexity: 1184.009097
2025-10-16 07:45:05,803 Stage: Train 0.5 | Epoch: 82 | Iter: 249600 | Total Loss: 0.003893 | Recon Loss: 0.003369 | Commit Loss: 0.001048 | Perplexity: 1174.967764
2025-10-16 07:50:28,392 Stage: Train 0.5 | Epoch: 82 | Iter: 249800 | Total Loss: 0.003851 | Recon Loss: 0.003333 | Commit Loss: 0.001035 | Perplexity: 1179.519536
2025-10-16 07:55:50,783 Stage: Train 0.5 | Epoch: 82 | Iter: 250000 | Total Loss: 0.003848 | Recon Loss: 0.003331 | Commit Loss: 0.001034 | Perplexity: 1176.223359
2025-10-16 08:01:13,088 Stage: Train 0.5 | Epoch: 82 | Iter: 250200 | Total Loss: 0.003848 | Recon Loss: 0.003329 | Commit Loss: 0.001038 | Perplexity: 1185.000312
2025-10-16 08:06:36,363 Stage: Train 0.5 | Epoch: 82 | Iter: 250400 | Total Loss: 0.003897 | Recon Loss: 0.003375 | Commit Loss: 0.001045 | Perplexity: 1187.790221
2025-10-16 08:11:58,657 Stage: Train 0.5 | Epoch: 82 | Iter: 250600 | Total Loss: 0.003948 | Recon Loss: 0.003423 | Commit Loss: 0.001049 | Perplexity: 1184.570341
2025-10-16 08:17:20,779 Stage: Train 0.5 | Epoch: 82 | Iter: 250800 | Total Loss: 0.003833 | Recon Loss: 0.003319 | Commit Loss: 0.001029 | Perplexity: 1176.924614
2025-10-16 08:22:43,121 Stage: Train 0.5 | Epoch: 82 | Iter: 251000 | Total Loss: 0.003880 | Recon Loss: 0.003360 | Commit Loss: 0.001040 | Perplexity: 1187.172586
2025-10-16 08:28:05,518 Stage: Train 0.5 | Epoch: 82 | Iter: 251200 | Total Loss: 0.003878 | Recon Loss: 0.003352 | Commit Loss: 0.001051 | Perplexity: 1189.714957
2025-10-16 08:33:27,720 Stage: Train 0.5 | Epoch: 82 | Iter: 251400 | Total Loss: 0.003908 | Recon Loss: 0.003381 | Commit Loss: 0.001053 | Perplexity: 1172.663986
2025-10-16 08:38:50,044 Stage: Train 0.5 | Epoch: 82 | Iter: 251600 | Total Loss: 0.003853 | Recon Loss: 0.003336 | Commit Loss: 0.001035 | Perplexity: 1176.084687
2025-10-16 08:44:11,912 Stage: Train 0.5 | Epoch: 82 | Iter: 251800 | Total Loss: 0.003880 | Recon Loss: 0.003362 | Commit Loss: 0.001035 | Perplexity: 1186.212899
2025-10-16 08:49:34,268 Stage: Train 0.5 | Epoch: 82 | Iter: 252000 | Total Loss: 0.003859 | Recon Loss: 0.003329 | Commit Loss: 0.001061 | Perplexity: 1178.673537
Trainning Epoch:  50%|█████     | 83/165 [113:28:41<111:42:49, 4904.51s/it]Trainning Epoch:  50%|█████     | 83/165 [113:28:41<111:42:49, 4904.51s/it]2025-10-16 08:55:01,150 Stage: Train 0.5 | Epoch: 83 | Iter: 252200 | Total Loss: 0.003852 | Recon Loss: 0.003326 | Commit Loss: 0.001051 | Perplexity: 1183.020875
2025-10-16 09:00:23,756 Stage: Train 0.5 | Epoch: 83 | Iter: 252400 | Total Loss: 0.003922 | Recon Loss: 0.003404 | Commit Loss: 0.001037 | Perplexity: 1180.974934
2025-10-16 09:05:45,752 Stage: Train 0.5 | Epoch: 83 | Iter: 252600 | Total Loss: 0.003856 | Recon Loss: 0.003336 | Commit Loss: 0.001039 | Perplexity: 1174.154353
2025-10-16 09:11:07,863 Stage: Train 0.5 | Epoch: 83 | Iter: 252800 | Total Loss: 0.003851 | Recon Loss: 0.003326 | Commit Loss: 0.001050 | Perplexity: 1183.984377
2025-10-16 09:16:30,193 Stage: Train 0.5 | Epoch: 83 | Iter: 253000 | Total Loss: 0.003982 | Recon Loss: 0.003453 | Commit Loss: 0.001057 | Perplexity: 1183.720907
2025-10-16 09:21:52,519 Stage: Train 0.5 | Epoch: 83 | Iter: 253200 | Total Loss: 0.003918 | Recon Loss: 0.003388 | Commit Loss: 0.001060 | Perplexity: 1183.857333
2025-10-16 09:27:14,342 Stage: Train 0.5 | Epoch: 83 | Iter: 253400 | Total Loss: 0.003834 | Recon Loss: 0.003309 | Commit Loss: 0.001052 | Perplexity: 1186.778829
2025-10-16 09:32:36,236 Stage: Train 0.5 | Epoch: 83 | Iter: 253600 | Total Loss: 0.003802 | Recon Loss: 0.003290 | Commit Loss: 0.001024 | Perplexity: 1186.026390
2025-10-16 09:37:58,220 Stage: Train 0.5 | Epoch: 83 | Iter: 253800 | Total Loss: 0.003909 | Recon Loss: 0.003377 | Commit Loss: 0.001064 | Perplexity: 1188.393058
2025-10-16 09:43:20,775 Stage: Train 0.5 | Epoch: 83 | Iter: 254000 | Total Loss: 0.003895 | Recon Loss: 0.003375 | Commit Loss: 0.001040 | Perplexity: 1185.459551
2025-10-16 09:48:43,139 Stage: Train 0.5 | Epoch: 83 | Iter: 254200 | Total Loss: 0.003848 | Recon Loss: 0.003327 | Commit Loss: 0.001041 | Perplexity: 1180.371218
2025-10-16 09:54:05,435 Stage: Train 0.5 | Epoch: 83 | Iter: 254400 | Total Loss: 0.003794 | Recon Loss: 0.003275 | Commit Loss: 0.001037 | Perplexity: 1183.962675
2025-10-16 09:59:27,842 Stage: Train 0.5 | Epoch: 83 | Iter: 254600 | Total Loss: 0.003840 | Recon Loss: 0.003321 | Commit Loss: 0.001039 | Perplexity: 1183.629288
2025-10-16 10:04:50,233 Stage: Train 0.5 | Epoch: 83 | Iter: 254800 | Total Loss: 0.004768 | Recon Loss: 0.003962 | Commit Loss: 0.001610 | Perplexity: 1183.435128
2025-10-16 10:10:12,558 Stage: Train 0.5 | Epoch: 83 | Iter: 255000 | Total Loss: 0.003915 | Recon Loss: 0.003375 | Commit Loss: 0.001080 | Perplexity: 1169.110176
Trainning Epoch:  51%|█████     | 84/165 [114:50:20<110:18:40, 4902.72s/it]Trainning Epoch:  51%|█████     | 84/165 [114:50:20<110:18:40, 4902.72s/it]2025-10-16 10:15:38,750 Stage: Train 0.5 | Epoch: 84 | Iter: 255200 | Total Loss: 0.003851 | Recon Loss: 0.003318 | Commit Loss: 0.001066 | Perplexity: 1184.803345
2025-10-16 10:21:01,688 Stage: Train 0.5 | Epoch: 84 | Iter: 255400 | Total Loss: 0.003819 | Recon Loss: 0.003295 | Commit Loss: 0.001048 | Perplexity: 1174.560233
2025-10-16 10:26:24,826 Stage: Train 0.5 | Epoch: 84 | Iter: 255600 | Total Loss: 0.003857 | Recon Loss: 0.003339 | Commit Loss: 0.001035 | Perplexity: 1180.060624
2025-10-16 10:31:48,270 Stage: Train 0.5 | Epoch: 84 | Iter: 255800 | Total Loss: 0.003805 | Recon Loss: 0.003295 | Commit Loss: 0.001021 | Perplexity: 1183.450358
2025-10-16 10:37:11,331 Stage: Train 0.5 | Epoch: 84 | Iter: 256000 | Total Loss: 0.003905 | Recon Loss: 0.003395 | Commit Loss: 0.001020 | Perplexity: 1174.713645
2025-10-16 10:42:34,411 Stage: Train 0.5 | Epoch: 84 | Iter: 256200 | Total Loss: 0.003799 | Recon Loss: 0.003279 | Commit Loss: 0.001040 | Perplexity: 1183.378291
2025-10-16 10:47:57,150 Stage: Train 0.5 | Epoch: 84 | Iter: 256400 | Total Loss: 0.003932 | Recon Loss: 0.003401 | Commit Loss: 0.001063 | Perplexity: 1182.127812
2025-10-16 10:53:19,932 Stage: Train 0.5 | Epoch: 84 | Iter: 256600 | Total Loss: 0.003839 | Recon Loss: 0.003313 | Commit Loss: 0.001052 | Perplexity: 1187.982948
2025-10-16 10:58:42,589 Stage: Train 0.5 | Epoch: 84 | Iter: 256800 | Total Loss: 0.003847 | Recon Loss: 0.003322 | Commit Loss: 0.001049 | Perplexity: 1180.767202
2025-10-16 11:04:05,129 Stage: Train 0.5 | Epoch: 84 | Iter: 257000 | Total Loss: 0.003895 | Recon Loss: 0.003368 | Commit Loss: 0.001054 | Perplexity: 1185.896534
2025-10-16 11:09:27,938 Stage: Train 0.5 | Epoch: 84 | Iter: 257200 | Total Loss: 0.003875 | Recon Loss: 0.003340 | Commit Loss: 0.001070 | Perplexity: 1185.062776
2025-10-16 11:14:50,231 Stage: Train 0.5 | Epoch: 84 | Iter: 257400 | Total Loss: 0.003863 | Recon Loss: 0.003340 | Commit Loss: 0.001048 | Perplexity: 1179.852237
2025-10-16 11:20:12,672 Stage: Train 0.5 | Epoch: 84 | Iter: 257600 | Total Loss: 0.003909 | Recon Loss: 0.003376 | Commit Loss: 0.001067 | Perplexity: 1182.362585
2025-10-16 11:25:35,483 Stage: Train 0.5 | Epoch: 84 | Iter: 257800 | Total Loss: 0.003874 | Recon Loss: 0.003353 | Commit Loss: 0.001041 | Perplexity: 1177.728522
2025-10-16 11:30:58,195 Stage: Train 0.5 | Epoch: 84 | Iter: 258000 | Total Loss: 0.003800 | Recon Loss: 0.003286 | Commit Loss: 0.001027 | Perplexity: 1186.449019
2025-10-16 11:36:20,199 Stage: Train 0.5 | Epoch: 84 | Iter: 258200 | Total Loss: 0.003876 | Recon Loss: 0.003353 | Commit Loss: 0.001046 | Perplexity: 1180.832992
Trainning Epoch:  52%|█████▏    | 85/165 [116:12:07<108:58:46, 4904.09s/it]Trainning Epoch:  52%|█████▏    | 85/165 [116:12:07<108:58:47, 4904.09s/it]2025-10-16 11:41:46,682 Stage: Train 0.5 | Epoch: 85 | Iter: 258400 | Total Loss: 0.003918 | Recon Loss: 0.003402 | Commit Loss: 0.001034 | Perplexity: 1179.818150
2025-10-16 11:47:09,154 Stage: Train 0.5 | Epoch: 85 | Iter: 258600 | Total Loss: 0.003827 | Recon Loss: 0.003315 | Commit Loss: 0.001024 | Perplexity: 1175.399497
2025-10-16 11:52:31,189 Stage: Train 0.5 | Epoch: 85 | Iter: 258800 | Total Loss: 0.003829 | Recon Loss: 0.003307 | Commit Loss: 0.001044 | Perplexity: 1183.347292
2025-10-16 11:57:53,609 Stage: Train 0.5 | Epoch: 85 | Iter: 259000 | Total Loss: 0.003825 | Recon Loss: 0.003300 | Commit Loss: 0.001050 | Perplexity: 1180.671335
2025-10-16 12:03:15,913 Stage: Train 0.5 | Epoch: 85 | Iter: 259200 | Total Loss: 0.003888 | Recon Loss: 0.003367 | Commit Loss: 0.001042 | Perplexity: 1184.113295
2025-10-16 12:08:38,567 Stage: Train 0.5 | Epoch: 85 | Iter: 259400 | Total Loss: 0.003876 | Recon Loss: 0.003342 | Commit Loss: 0.001068 | Perplexity: 1184.772516
2025-10-16 12:14:00,227 Stage: Train 0.5 | Epoch: 85 | Iter: 259600 | Total Loss: 0.003801 | Recon Loss: 0.003267 | Commit Loss: 0.001068 | Perplexity: 1186.808757
2025-10-16 12:19:22,807 Stage: Train 0.5 | Epoch: 85 | Iter: 259800 | Total Loss: 0.003846 | Recon Loss: 0.003325 | Commit Loss: 0.001043 | Perplexity: 1172.538934
2025-10-16 12:24:45,147 Stage: Train 0.5 | Epoch: 85 | Iter: 260000 | Total Loss: 0.003814 | Recon Loss: 0.003289 | Commit Loss: 0.001051 | Perplexity: 1178.572376
2025-10-16 12:24:45,148 Saving model at iteration 260000
2025-10-16 12:24:45,538 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000
2025-10-16 12:24:46,859 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000/model.safetensors
2025-10-16 12:24:48,572 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000/optimizer.bin
2025-10-16 12:24:48,573 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000/scheduler.bin
2025-10-16 12:24:48,573 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000/sampler.bin
2025-10-16 12:24:48,574 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_86_step_260000/random_states_0.pkl
2025-10-16 12:30:11,283 Stage: Train 0.5 | Epoch: 85 | Iter: 260200 | Total Loss: 0.003818 | Recon Loss: 0.003294 | Commit Loss: 0.001048 | Perplexity: 1187.432300
2025-10-16 12:35:33,267 Stage: Train 0.5 | Epoch: 85 | Iter: 260400 | Total Loss: 0.003854 | Recon Loss: 0.003322 | Commit Loss: 0.001063 | Perplexity: 1179.212342
2025-10-16 12:40:55,177 Stage: Train 0.5 | Epoch: 85 | Iter: 260600 | Total Loss: 0.003890 | Recon Loss: 0.003358 | Commit Loss: 0.001065 | Perplexity: 1182.407218
2025-10-16 12:46:17,352 Stage: Train 0.5 | Epoch: 85 | Iter: 260800 | Total Loss: 0.003882 | Recon Loss: 0.003350 | Commit Loss: 0.001063 | Perplexity: 1181.499776
2025-10-16 12:51:39,597 Stage: Train 0.5 | Epoch: 85 | Iter: 261000 | Total Loss: 0.003824 | Recon Loss: 0.003303 | Commit Loss: 0.001042 | Perplexity: 1183.312478
2025-10-16 12:57:01,603 Stage: Train 0.5 | Epoch: 85 | Iter: 261200 | Total Loss: 0.003809 | Recon Loss: 0.003283 | Commit Loss: 0.001052 | Perplexity: 1191.258268
Trainning Epoch:  52%|█████▏    | 86/165 [117:33:50<107:36:30, 4903.68s/it]Trainning Epoch:  52%|█████▏    | 86/165 [117:33:50<107:36:30, 4903.68s/it]2025-10-16 13:02:28,551 Stage: Train 0.5 | Epoch: 86 | Iter: 261400 | Total Loss: 0.003881 | Recon Loss: 0.003353 | Commit Loss: 0.001057 | Perplexity: 1188.454319
2025-10-16 13:07:50,733 Stage: Train 0.5 | Epoch: 86 | Iter: 261600 | Total Loss: 0.003937 | Recon Loss: 0.003407 | Commit Loss: 0.001061 | Perplexity: 1180.824503
2025-10-16 13:13:13,715 Stage: Train 0.5 | Epoch: 86 | Iter: 261800 | Total Loss: 0.003826 | Recon Loss: 0.003300 | Commit Loss: 0.001052 | Perplexity: 1182.795469
2025-10-16 13:18:36,824 Stage: Train 0.5 | Epoch: 86 | Iter: 262000 | Total Loss: 0.003849 | Recon Loss: 0.003327 | Commit Loss: 0.001043 | Perplexity: 1185.119598
2025-10-16 13:24:00,029 Stage: Train 0.5 | Epoch: 86 | Iter: 262200 | Total Loss: 0.003960 | Recon Loss: 0.003436 | Commit Loss: 0.001049 | Perplexity: 1185.663024
2025-10-16 13:29:23,203 Stage: Train 0.5 | Epoch: 86 | Iter: 262400 | Total Loss: 0.003756 | Recon Loss: 0.003231 | Commit Loss: 0.001050 | Perplexity: 1180.632729
2025-10-16 13:34:46,424 Stage: Train 0.5 | Epoch: 86 | Iter: 262600 | Total Loss: 0.003904 | Recon Loss: 0.003388 | Commit Loss: 0.001032 | Perplexity: 1185.600012
2025-10-16 13:40:09,523 Stage: Train 0.5 | Epoch: 86 | Iter: 262800 | Total Loss: 0.003761 | Recon Loss: 0.003241 | Commit Loss: 0.001039 | Perplexity: 1190.190970
2025-10-16 13:45:32,239 Stage: Train 0.5 | Epoch: 86 | Iter: 263000 | Total Loss: 0.003827 | Recon Loss: 0.003311 | Commit Loss: 0.001033 | Perplexity: 1186.163701
2025-10-16 13:50:55,282 Stage: Train 0.5 | Epoch: 86 | Iter: 263200 | Total Loss: 0.003806 | Recon Loss: 0.003284 | Commit Loss: 0.001043 | Perplexity: 1186.219045
2025-10-16 13:56:18,248 Stage: Train 0.5 | Epoch: 86 | Iter: 263400 | Total Loss: 0.003852 | Recon Loss: 0.003334 | Commit Loss: 0.001035 | Perplexity: 1185.267211
2025-10-16 14:01:40,918 Stage: Train 0.5 | Epoch: 86 | Iter: 263600 | Total Loss: 0.003767 | Recon Loss: 0.003245 | Commit Loss: 0.001045 | Perplexity: 1183.470439
2025-10-16 14:07:03,210 Stage: Train 0.5 | Epoch: 86 | Iter: 263800 | Total Loss: 0.003838 | Recon Loss: 0.003306 | Commit Loss: 0.001063 | Perplexity: 1188.648264
2025-10-16 14:12:25,701 Stage: Train 0.5 | Epoch: 86 | Iter: 264000 | Total Loss: 0.003829 | Recon Loss: 0.003310 | Commit Loss: 0.001039 | Perplexity: 1186.597197
2025-10-16 14:17:48,438 Stage: Train 0.5 | Epoch: 86 | Iter: 264200 | Total Loss: 0.003794 | Recon Loss: 0.003271 | Commit Loss: 0.001046 | Perplexity: 1185.655012
Trainning Epoch:  53%|█████▎    | 87/165 [118:55:38<106:16:30, 4905.00s/it]Trainning Epoch:  53%|█████▎    | 87/165 [118:55:38<106:16:30, 4905.00s/it]2025-10-16 14:23:15,368 Stage: Train 0.5 | Epoch: 87 | Iter: 264400 | Total Loss: 0.003836 | Recon Loss: 0.003309 | Commit Loss: 0.001055 | Perplexity: 1192.724005
2025-10-16 14:28:38,486 Stage: Train 0.5 | Epoch: 87 | Iter: 264600 | Total Loss: 0.003987 | Recon Loss: 0.003435 | Commit Loss: 0.001104 | Perplexity: 1191.505802
2025-10-16 14:34:01,542 Stage: Train 0.5 | Epoch: 87 | Iter: 264800 | Total Loss: 0.003822 | Recon Loss: 0.003300 | Commit Loss: 0.001043 | Perplexity: 1186.854701
2025-10-16 14:39:24,393 Stage: Train 0.5 | Epoch: 87 | Iter: 265000 | Total Loss: 0.003890 | Recon Loss: 0.003357 | Commit Loss: 0.001066 | Perplexity: 1183.005953
2025-10-16 14:44:47,073 Stage: Train 0.5 | Epoch: 87 | Iter: 265200 | Total Loss: 0.003821 | Recon Loss: 0.003300 | Commit Loss: 0.001042 | Perplexity: 1179.772222
2025-10-16 14:50:09,772 Stage: Train 0.5 | Epoch: 87 | Iter: 265400 | Total Loss: 0.003842 | Recon Loss: 0.003314 | Commit Loss: 0.001056 | Perplexity: 1189.536014
2025-10-16 14:55:32,220 Stage: Train 0.5 | Epoch: 87 | Iter: 265600 | Total Loss: 0.003787 | Recon Loss: 0.003264 | Commit Loss: 0.001046 | Perplexity: 1183.260769
2025-10-16 15:00:55,404 Stage: Train 0.5 | Epoch: 87 | Iter: 265800 | Total Loss: 0.003837 | Recon Loss: 0.003314 | Commit Loss: 0.001045 | Perplexity: 1180.174465
2025-10-16 15:06:17,401 Stage: Train 0.5 | Epoch: 87 | Iter: 266000 | Total Loss: 0.003835 | Recon Loss: 0.003305 | Commit Loss: 0.001060 | Perplexity: 1191.162786
2025-10-16 15:11:39,818 Stage: Train 0.5 | Epoch: 87 | Iter: 266200 | Total Loss: 0.003831 | Recon Loss: 0.003309 | Commit Loss: 0.001043 | Perplexity: 1185.228880
2025-10-16 15:17:02,618 Stage: Train 0.5 | Epoch: 87 | Iter: 266400 | Total Loss: 0.003891 | Recon Loss: 0.003371 | Commit Loss: 0.001038 | Perplexity: 1181.768674
2025-10-16 15:22:25,530 Stage: Train 0.5 | Epoch: 87 | Iter: 266600 | Total Loss: 0.003723 | Recon Loss: 0.003203 | Commit Loss: 0.001041 | Perplexity: 1187.673116
2025-10-16 15:27:47,995 Stage: Train 0.5 | Epoch: 87 | Iter: 266800 | Total Loss: 0.003864 | Recon Loss: 0.003340 | Commit Loss: 0.001049 | Perplexity: 1181.222974
2025-10-16 15:33:10,494 Stage: Train 0.5 | Epoch: 87 | Iter: 267000 | Total Loss: 0.003773 | Recon Loss: 0.003259 | Commit Loss: 0.001028 | Perplexity: 1188.101375
2025-10-16 15:38:33,476 Stage: Train 0.5 | Epoch: 87 | Iter: 267200 | Total Loss: 0.003951 | Recon Loss: 0.003419 | Commit Loss: 0.001062 | Perplexity: 1171.786837
Trainning Epoch:  53%|█████▎    | 88/165 [120:17:25<104:55:23, 4905.51s/it]Trainning Epoch:  53%|█████▎    | 88/165 [120:17:25<104:55:23, 4905.51s/it]2025-10-16 15:44:00,547 Stage: Train 0.5 | Epoch: 88 | Iter: 267400 | Total Loss: 0.003872 | Recon Loss: 0.003337 | Commit Loss: 0.001070 | Perplexity: 1187.248975
2025-10-16 15:49:23,268 Stage: Train 0.5 | Epoch: 88 | Iter: 267600 | Total Loss: 0.003841 | Recon Loss: 0.003320 | Commit Loss: 0.001042 | Perplexity: 1185.216331
2025-10-16 15:54:46,410 Stage: Train 0.5 | Epoch: 88 | Iter: 267800 | Total Loss: 0.003768 | Recon Loss: 0.003249 | Commit Loss: 0.001039 | Perplexity: 1184.805964
2025-10-16 16:00:09,536 Stage: Train 0.5 | Epoch: 88 | Iter: 268000 | Total Loss: 0.003803 | Recon Loss: 0.003283 | Commit Loss: 0.001040 | Perplexity: 1187.221098
2025-10-16 16:05:32,242 Stage: Train 0.5 | Epoch: 88 | Iter: 268200 | Total Loss: 0.003839 | Recon Loss: 0.003309 | Commit Loss: 0.001060 | Perplexity: 1184.016935
2025-10-16 16:10:55,047 Stage: Train 0.5 | Epoch: 88 | Iter: 268400 | Total Loss: 0.003813 | Recon Loss: 0.003282 | Commit Loss: 0.001062 | Perplexity: 1184.934644
2025-10-16 16:16:18,081 Stage: Train 0.5 | Epoch: 88 | Iter: 268600 | Total Loss: 0.003689 | Recon Loss: 0.003175 | Commit Loss: 0.001028 | Perplexity: 1191.395264
2025-10-16 16:21:40,954 Stage: Train 0.5 | Epoch: 88 | Iter: 268800 | Total Loss: 0.003882 | Recon Loss: 0.003366 | Commit Loss: 0.001033 | Perplexity: 1178.389468
2025-10-16 16:27:03,325 Stage: Train 0.5 | Epoch: 88 | Iter: 269000 | Total Loss: 0.004040 | Recon Loss: 0.003471 | Commit Loss: 0.001138 | Perplexity: 1177.910526
2025-10-16 16:32:26,201 Stage: Train 0.5 | Epoch: 88 | Iter: 269200 | Total Loss: 0.003914 | Recon Loss: 0.003389 | Commit Loss: 0.001049 | Perplexity: 1184.817386
2025-10-16 16:37:49,465 Stage: Train 0.5 | Epoch: 88 | Iter: 269400 | Total Loss: 0.003835 | Recon Loss: 0.003297 | Commit Loss: 0.001076 | Perplexity: 1186.602108
2025-10-16 16:43:12,494 Stage: Train 0.5 | Epoch: 88 | Iter: 269600 | Total Loss: 0.003825 | Recon Loss: 0.003303 | Commit Loss: 0.001045 | Perplexity: 1186.152930
2025-10-16 16:48:35,366 Stage: Train 0.5 | Epoch: 88 | Iter: 269800 | Total Loss: 0.003859 | Recon Loss: 0.003331 | Commit Loss: 0.001058 | Perplexity: 1185.307255
2025-10-16 16:53:58,147 Stage: Train 0.5 | Epoch: 88 | Iter: 270000 | Total Loss: 0.003779 | Recon Loss: 0.003254 | Commit Loss: 0.001049 | Perplexity: 1181.879548
2025-10-16 16:59:20,643 Stage: Train 0.5 | Epoch: 88 | Iter: 270200 | Total Loss: 0.003788 | Recon Loss: 0.003267 | Commit Loss: 0.001043 | Perplexity: 1186.722599
Trainning Epoch:  54%|█████▍    | 89/165 [121:39:13<103:34:50, 4906.45s/it]Trainning Epoch:  54%|█████▍    | 89/165 [121:39:13<103:34:50, 4906.45s/it]2025-10-16 17:04:47,944 Stage: Train 0.5 | Epoch: 89 | Iter: 270400 | Total Loss: 0.003866 | Recon Loss: 0.003338 | Commit Loss: 0.001056 | Perplexity: 1178.655960
2025-10-16 17:10:11,130 Stage: Train 0.5 | Epoch: 89 | Iter: 270600 | Total Loss: 0.003893 | Recon Loss: 0.003368 | Commit Loss: 0.001050 | Perplexity: 1189.144371
2025-10-16 17:15:34,248 Stage: Train 0.5 | Epoch: 89 | Iter: 270800 | Total Loss: 0.003785 | Recon Loss: 0.003253 | Commit Loss: 0.001066 | Perplexity: 1192.707609
2025-10-16 17:20:56,445 Stage: Train 0.5 | Epoch: 89 | Iter: 271000 | Total Loss: 0.003800 | Recon Loss: 0.003269 | Commit Loss: 0.001060 | Perplexity: 1190.160337
2025-10-16 17:26:19,408 Stage: Train 0.5 | Epoch: 89 | Iter: 271200 | Total Loss: 0.003807 | Recon Loss: 0.003282 | Commit Loss: 0.001050 | Perplexity: 1178.248148
2025-10-16 17:31:42,556 Stage: Train 0.5 | Epoch: 89 | Iter: 271400 | Total Loss: 0.003863 | Recon Loss: 0.003335 | Commit Loss: 0.001056 | Perplexity: 1179.038895
2025-10-16 17:37:04,461 Stage: Train 0.5 | Epoch: 89 | Iter: 271600 | Total Loss: 0.003796 | Recon Loss: 0.003278 | Commit Loss: 0.001037 | Perplexity: 1185.774941
2025-10-16 17:42:27,522 Stage: Train 0.5 | Epoch: 89 | Iter: 271800 | Total Loss: 0.003864 | Recon Loss: 0.003336 | Commit Loss: 0.001056 | Perplexity: 1181.706501
2025-10-16 17:47:50,491 Stage: Train 0.5 | Epoch: 89 | Iter: 272000 | Total Loss: 0.003733 | Recon Loss: 0.003212 | Commit Loss: 0.001043 | Perplexity: 1189.228803
2025-10-16 17:53:13,490 Stage: Train 0.5 | Epoch: 89 | Iter: 272200 | Total Loss: 0.003853 | Recon Loss: 0.003328 | Commit Loss: 0.001050 | Perplexity: 1180.239883
2025-10-16 17:58:36,490 Stage: Train 0.5 | Epoch: 89 | Iter: 272400 | Total Loss: 0.003800 | Recon Loss: 0.003271 | Commit Loss: 0.001059 | Perplexity: 1181.037988
2025-10-16 18:03:59,452 Stage: Train 0.5 | Epoch: 89 | Iter: 272600 | Total Loss: 0.003804 | Recon Loss: 0.003288 | Commit Loss: 0.001032 | Perplexity: 1178.959830
2025-10-16 18:09:21,600 Stage: Train 0.5 | Epoch: 89 | Iter: 272800 | Total Loss: 0.003770 | Recon Loss: 0.003248 | Commit Loss: 0.001045 | Perplexity: 1184.937227
2025-10-16 18:14:43,280 Stage: Train 0.5 | Epoch: 89 | Iter: 273000 | Total Loss: 0.003881 | Recon Loss: 0.003354 | Commit Loss: 0.001055 | Perplexity: 1184.464899
2025-10-16 18:20:05,957 Stage: Train 0.5 | Epoch: 89 | Iter: 273200 | Total Loss: 0.003743 | Recon Loss: 0.003222 | Commit Loss: 0.001041 | Perplexity: 1181.070320
2025-10-16 18:25:29,079 Stage: Train 0.5 | Epoch: 89 | Iter: 273400 | Total Loss: 0.003781 | Recon Loss: 0.003255 | Commit Loss: 0.001052 | Perplexity: 1189.949291
Trainning Epoch:  55%|█████▍    | 90/165 [123:01:00<102:13:08, 4906.51s/it]Trainning Epoch:  55%|█████▍    | 90/165 [123:01:00<102:13:08, 4906.51s/it]2025-10-16 18:30:55,439 Stage: Train 0.5 | Epoch: 90 | Iter: 273600 | Total Loss: 0.003791 | Recon Loss: 0.003265 | Commit Loss: 0.001053 | Perplexity: 1189.419502
2025-10-16 18:36:17,522 Stage: Train 0.5 | Epoch: 90 | Iter: 273800 | Total Loss: 0.003784 | Recon Loss: 0.003260 | Commit Loss: 0.001047 | Perplexity: 1184.695175
2025-10-16 18:41:40,524 Stage: Train 0.5 | Epoch: 90 | Iter: 274000 | Total Loss: 0.003703 | Recon Loss: 0.003175 | Commit Loss: 0.001055 | Perplexity: 1190.533626
2025-10-16 18:47:03,131 Stage: Train 0.5 | Epoch: 90 | Iter: 274200 | Total Loss: 0.003764 | Recon Loss: 0.003246 | Commit Loss: 0.001035 | Perplexity: 1182.856617
2025-10-16 18:52:25,908 Stage: Train 0.5 | Epoch: 90 | Iter: 274400 | Total Loss: 0.003790 | Recon Loss: 0.003270 | Commit Loss: 0.001040 | Perplexity: 1187.785060
2025-10-16 18:57:48,726 Stage: Train 0.5 | Epoch: 90 | Iter: 274600 | Total Loss: 0.003735 | Recon Loss: 0.003216 | Commit Loss: 0.001039 | Perplexity: 1189.991302
2025-10-16 19:03:11,210 Stage: Train 0.5 | Epoch: 90 | Iter: 274800 | Total Loss: 0.003848 | Recon Loss: 0.003323 | Commit Loss: 0.001049 | Perplexity: 1185.382643
2025-10-16 19:08:33,863 Stage: Train 0.5 | Epoch: 90 | Iter: 275000 | Total Loss: 0.003777 | Recon Loss: 0.003250 | Commit Loss: 0.001053 | Perplexity: 1185.231438
2025-10-16 19:13:56,813 Stage: Train 0.5 | Epoch: 90 | Iter: 275200 | Total Loss: 0.003787 | Recon Loss: 0.003248 | Commit Loss: 0.001077 | Perplexity: 1186.483194
2025-10-16 19:19:19,702 Stage: Train 0.5 | Epoch: 90 | Iter: 275400 | Total Loss: 0.003849 | Recon Loss: 0.003328 | Commit Loss: 0.001043 | Perplexity: 1187.730974
2025-10-16 19:24:42,653 Stage: Train 0.5 | Epoch: 90 | Iter: 275600 | Total Loss: 0.003877 | Recon Loss: 0.003341 | Commit Loss: 0.001072 | Perplexity: 1190.520786
2025-10-16 19:30:05,629 Stage: Train 0.5 | Epoch: 90 | Iter: 275800 | Total Loss: 0.003818 | Recon Loss: 0.003289 | Commit Loss: 0.001057 | Perplexity: 1185.147090
2025-10-16 19:35:28,560 Stage: Train 0.5 | Epoch: 90 | Iter: 276000 | Total Loss: 0.003903 | Recon Loss: 0.003368 | Commit Loss: 0.001071 | Perplexity: 1182.053993
2025-10-16 19:40:51,764 Stage: Train 0.5 | Epoch: 90 | Iter: 276200 | Total Loss: 0.003787 | Recon Loss: 0.003266 | Commit Loss: 0.001042 | Perplexity: 1185.987610
2025-10-16 19:46:14,620 Stage: Train 0.5 | Epoch: 90 | Iter: 276400 | Total Loss: 0.003826 | Recon Loss: 0.003307 | Commit Loss: 0.001037 | Perplexity: 1175.409699
Trainning Epoch:  55%|█████▌    | 91/165 [124:22:47<100:51:34, 4906.69s/it]Trainning Epoch:  55%|█████▌    | 91/165 [124:22:47<100:51:34, 4906.69s/it]2025-10-16 19:51:41,494 Stage: Train 0.5 | Epoch: 91 | Iter: 276600 | Total Loss: 0.003706 | Recon Loss: 0.003183 | Commit Loss: 0.001046 | Perplexity: 1182.907625
2025-10-16 19:57:04,294 Stage: Train 0.5 | Epoch: 91 | Iter: 276800 | Total Loss: 0.003755 | Recon Loss: 0.003224 | Commit Loss: 0.001062 | Perplexity: 1189.683706
2025-10-16 20:02:27,525 Stage: Train 0.5 | Epoch: 91 | Iter: 277000 | Total Loss: 0.003845 | Recon Loss: 0.003316 | Commit Loss: 0.001057 | Perplexity: 1179.992410
2025-10-16 20:07:50,541 Stage: Train 0.5 | Epoch: 91 | Iter: 277200 | Total Loss: 0.003810 | Recon Loss: 0.003276 | Commit Loss: 0.001068 | Perplexity: 1184.381326
2025-10-16 20:13:13,183 Stage: Train 0.5 | Epoch: 91 | Iter: 277400 | Total Loss: 0.004028 | Recon Loss: 0.003467 | Commit Loss: 0.001123 | Perplexity: 1187.667653
2025-10-16 20:18:36,064 Stage: Train 0.5 | Epoch: 91 | Iter: 277600 | Total Loss: 0.003803 | Recon Loss: 0.003278 | Commit Loss: 0.001048 | Perplexity: 1183.688303
2025-10-16 20:23:58,971 Stage: Train 0.5 | Epoch: 91 | Iter: 277800 | Total Loss: 0.003804 | Recon Loss: 0.003269 | Commit Loss: 0.001070 | Perplexity: 1187.154660
2025-10-16 20:29:21,915 Stage: Train 0.5 | Epoch: 91 | Iter: 278000 | Total Loss: 0.003847 | Recon Loss: 0.003321 | Commit Loss: 0.001052 | Perplexity: 1186.528936
2025-10-16 20:34:44,597 Stage: Train 0.5 | Epoch: 91 | Iter: 278200 | Total Loss: 0.003754 | Recon Loss: 0.003238 | Commit Loss: 0.001033 | Perplexity: 1182.752221
2025-10-16 20:40:07,511 Stage: Train 0.5 | Epoch: 91 | Iter: 278400 | Total Loss: 0.004401 | Recon Loss: 0.003636 | Commit Loss: 0.001529 | Perplexity: 1183.936132
2025-10-16 20:45:29,774 Stage: Train 0.5 | Epoch: 91 | Iter: 278600 | Total Loss: 0.003896 | Recon Loss: 0.003353 | Commit Loss: 0.001086 | Perplexity: 1180.742052
2025-10-16 20:50:52,812 Stage: Train 0.5 | Epoch: 91 | Iter: 278800 | Total Loss: 0.003782 | Recon Loss: 0.003261 | Commit Loss: 0.001042 | Perplexity: 1185.237089
2025-10-16 20:56:15,645 Stage: Train 0.5 | Epoch: 91 | Iter: 279000 | Total Loss: 0.003805 | Recon Loss: 0.003289 | Commit Loss: 0.001033 | Perplexity: 1177.986543
2025-10-16 21:01:37,455 Stage: Train 0.5 | Epoch: 91 | Iter: 279200 | Total Loss: 0.003852 | Recon Loss: 0.003329 | Commit Loss: 0.001047 | Perplexity: 1190.373044
2025-10-16 21:06:59,577 Stage: Train 0.5 | Epoch: 91 | Iter: 279400 | Total Loss: 0.003787 | Recon Loss: 0.003261 | Commit Loss: 0.001051 | Perplexity: 1183.489155
Trainning Epoch:  56%|█████▌    | 92/165 [125:44:33<99:29:35, 4906.51s/it] Trainning Epoch:  56%|█████▌    | 92/165 [125:44:33<99:29:35, 4906.51s/it] 2025-10-16 21:12:26,646 Stage: Train 0.5 | Epoch: 92 | Iter: 279600 | Total Loss: 0.003798 | Recon Loss: 0.003283 | Commit Loss: 0.001031 | Perplexity: 1178.708006
2025-10-16 21:17:49,883 Stage: Train 0.5 | Epoch: 92 | Iter: 279800 | Total Loss: 0.003803 | Recon Loss: 0.003282 | Commit Loss: 0.001040 | Perplexity: 1185.658820
2025-10-16 21:23:12,878 Stage: Train 0.5 | Epoch: 92 | Iter: 280000 | Total Loss: 0.003723 | Recon Loss: 0.003198 | Commit Loss: 0.001050 | Perplexity: 1180.215539
2025-10-16 21:23:12,878 Saving model at iteration 280000
2025-10-16 21:23:13,055 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000
2025-10-16 21:23:14,495 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000/model.safetensors
2025-10-16 21:23:16,341 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000/optimizer.bin
2025-10-16 21:23:16,342 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000/scheduler.bin
2025-10-16 21:23:16,342 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000/sampler.bin
2025-10-16 21:23:16,343 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_93_step_280000/random_states_0.pkl
2025-10-16 21:28:39,544 Stage: Train 0.5 | Epoch: 92 | Iter: 280200 | Total Loss: 0.003792 | Recon Loss: 0.003270 | Commit Loss: 0.001045 | Perplexity: 1184.265451
2025-10-16 21:34:02,621 Stage: Train 0.5 | Epoch: 92 | Iter: 280400 | Total Loss: 0.003783 | Recon Loss: 0.003260 | Commit Loss: 0.001047 | Perplexity: 1183.474844
2025-10-16 21:39:25,549 Stage: Train 0.5 | Epoch: 92 | Iter: 280600 | Total Loss: 0.003666 | Recon Loss: 0.003152 | Commit Loss: 0.001027 | Perplexity: 1188.856696
2025-10-16 21:44:48,585 Stage: Train 0.5 | Epoch: 92 | Iter: 280800 | Total Loss: 0.003818 | Recon Loss: 0.003292 | Commit Loss: 0.001052 | Perplexity: 1186.617068
2025-10-16 21:50:11,177 Stage: Train 0.5 | Epoch: 92 | Iter: 281000 | Total Loss: 0.003732 | Recon Loss: 0.003215 | Commit Loss: 0.001034 | Perplexity: 1189.112124
2025-10-16 21:55:32,882 Stage: Train 0.5 | Epoch: 92 | Iter: 281200 | Total Loss: 0.003790 | Recon Loss: 0.003264 | Commit Loss: 0.001052 | Perplexity: 1184.941004
2025-10-16 22:00:55,884 Stage: Train 0.5 | Epoch: 92 | Iter: 281400 | Total Loss: 0.003752 | Recon Loss: 0.003237 | Commit Loss: 0.001031 | Perplexity: 1184.855496
2025-10-16 22:06:19,082 Stage: Train 0.5 | Epoch: 92 | Iter: 281600 | Total Loss: 0.003774 | Recon Loss: 0.003248 | Commit Loss: 0.001052 | Perplexity: 1183.900286
2025-10-16 22:11:42,284 Stage: Train 0.5 | Epoch: 92 | Iter: 281800 | Total Loss: 0.003824 | Recon Loss: 0.003295 | Commit Loss: 0.001058 | Perplexity: 1182.333729
2025-10-16 22:17:05,294 Stage: Train 0.5 | Epoch: 92 | Iter: 282000 | Total Loss: 0.003862 | Recon Loss: 0.003342 | Commit Loss: 0.001040 | Perplexity: 1188.530162
2025-10-16 22:22:28,094 Stage: Train 0.5 | Epoch: 92 | Iter: 282200 | Total Loss: 0.003777 | Recon Loss: 0.003263 | Commit Loss: 0.001028 | Perplexity: 1172.126548
2025-10-16 22:27:50,703 Stage: Train 0.5 | Epoch: 92 | Iter: 282400 | Total Loss: 0.003696 | Recon Loss: 0.003179 | Commit Loss: 0.001035 | Perplexity: 1180.835815
Trainning Epoch:  56%|█████▋    | 93/165 [127:06:26<98:10:00, 4908.33s/it]Trainning Epoch:  56%|█████▋    | 93/165 [127:06:26<98:10:00, 4908.33s/it]2025-10-16 22:33:18,006 Stage: Train 0.5 | Epoch: 93 | Iter: 282600 | Total Loss: 0.003813 | Recon Loss: 0.003297 | Commit Loss: 0.001033 | Perplexity: 1184.564796
2025-10-16 22:38:41,059 Stage: Train 0.5 | Epoch: 93 | Iter: 282800 | Total Loss: 0.003774 | Recon Loss: 0.003255 | Commit Loss: 0.001038 | Perplexity: 1178.340606
2025-10-16 22:44:04,146 Stage: Train 0.5 | Epoch: 93 | Iter: 283000 | Total Loss: 0.003760 | Recon Loss: 0.003225 | Commit Loss: 0.001069 | Perplexity: 1187.162380
2025-10-16 22:49:27,207 Stage: Train 0.5 | Epoch: 93 | Iter: 283200 | Total Loss: 0.003725 | Recon Loss: 0.003208 | Commit Loss: 0.001035 | Perplexity: 1185.274417
2025-10-16 22:54:50,248 Stage: Train 0.5 | Epoch: 93 | Iter: 283400 | Total Loss: 0.003820 | Recon Loss: 0.003291 | Commit Loss: 0.001059 | Perplexity: 1185.497272
2025-10-16 23:00:13,036 Stage: Train 0.5 | Epoch: 93 | Iter: 283600 | Total Loss: 0.003721 | Recon Loss: 0.003196 | Commit Loss: 0.001050 | Perplexity: 1186.312813
2025-10-16 23:05:36,132 Stage: Train 0.5 | Epoch: 93 | Iter: 283800 | Total Loss: 0.003810 | Recon Loss: 0.003288 | Commit Loss: 0.001044 | Perplexity: 1190.364714
2025-10-16 23:10:58,798 Stage: Train 0.5 | Epoch: 93 | Iter: 284000 | Total Loss: 0.003786 | Recon Loss: 0.003263 | Commit Loss: 0.001047 | Perplexity: 1177.806091
2025-10-16 23:16:21,808 Stage: Train 0.5 | Epoch: 93 | Iter: 284200 | Total Loss: 0.003799 | Recon Loss: 0.003274 | Commit Loss: 0.001050 | Perplexity: 1188.181773
2025-10-16 23:21:44,481 Stage: Train 0.5 | Epoch: 93 | Iter: 284400 | Total Loss: 0.003781 | Recon Loss: 0.003265 | Commit Loss: 0.001032 | Perplexity: 1192.944159
2025-10-16 23:27:06,724 Stage: Train 0.5 | Epoch: 93 | Iter: 284600 | Total Loss: 0.003766 | Recon Loss: 0.003244 | Commit Loss: 0.001045 | Perplexity: 1174.812828
2025-10-16 23:32:29,108 Stage: Train 0.5 | Epoch: 93 | Iter: 284800 | Total Loss: 0.003774 | Recon Loss: 0.003247 | Commit Loss: 0.001054 | Perplexity: 1186.242019
2025-10-16 23:37:52,248 Stage: Train 0.5 | Epoch: 93 | Iter: 285000 | Total Loss: 0.003809 | Recon Loss: 0.003288 | Commit Loss: 0.001043 | Perplexity: 1186.075896
2025-10-16 23:43:15,184 Stage: Train 0.5 | Epoch: 93 | Iter: 285200 | Total Loss: 0.003798 | Recon Loss: 0.003271 | Commit Loss: 0.001053 | Perplexity: 1179.975552
2025-10-16 23:48:37,807 Stage: Train 0.5 | Epoch: 93 | Iter: 285400 | Total Loss: 0.003793 | Recon Loss: 0.003264 | Commit Loss: 0.001058 | Perplexity: 1183.056833
Trainning Epoch:  57%|█████▋    | 94/165 [128:28:13<96:47:58, 4908.15s/it]Trainning Epoch:  57%|█████▋    | 94/165 [128:28:13<96:47:58, 4908.15s/it]2025-10-16 23:54:04,402 Stage: Train 0.5 | Epoch: 94 | Iter: 285600 | Total Loss: 0.003755 | Recon Loss: 0.003230 | Commit Loss: 0.001049 | Perplexity: 1169.993455
2025-10-16 23:59:27,279 Stage: Train 0.5 | Epoch: 94 | Iter: 285800 | Total Loss: 0.003716 | Recon Loss: 0.003193 | Commit Loss: 0.001046 | Perplexity: 1189.846770
2025-10-17 00:04:50,322 Stage: Train 0.5 | Epoch: 94 | Iter: 286000 | Total Loss: 0.003780 | Recon Loss: 0.003272 | Commit Loss: 0.001015 | Perplexity: 1181.203568
2025-10-17 00:10:13,179 Stage: Train 0.5 | Epoch: 94 | Iter: 286200 | Total Loss: 0.003738 | Recon Loss: 0.003208 | Commit Loss: 0.001059 | Perplexity: 1184.293836
2025-10-17 00:15:34,841 Stage: Train 0.5 | Epoch: 94 | Iter: 286400 | Total Loss: 0.003745 | Recon Loss: 0.003225 | Commit Loss: 0.001039 | Perplexity: 1186.114064
2025-10-17 00:20:56,496 Stage: Train 0.5 | Epoch: 94 | Iter: 286600 | Total Loss: 0.003807 | Recon Loss: 0.003281 | Commit Loss: 0.001051 | Perplexity: 1185.331140
2025-10-17 00:26:19,036 Stage: Train 0.5 | Epoch: 94 | Iter: 286800 | Total Loss: 0.003733 | Recon Loss: 0.003209 | Commit Loss: 0.001048 | Perplexity: 1184.168174
2025-10-17 00:31:42,006 Stage: Train 0.5 | Epoch: 94 | Iter: 287000 | Total Loss: 0.003799 | Recon Loss: 0.003272 | Commit Loss: 0.001054 | Perplexity: 1188.377780
2025-10-17 00:37:05,209 Stage: Train 0.5 | Epoch: 94 | Iter: 287200 | Total Loss: 0.003742 | Recon Loss: 0.003223 | Commit Loss: 0.001038 | Perplexity: 1184.153519
2025-10-17 00:42:28,429 Stage: Train 0.5 | Epoch: 94 | Iter: 287400 | Total Loss: 0.003747 | Recon Loss: 0.003229 | Commit Loss: 0.001037 | Perplexity: 1197.299796
2025-10-17 00:47:51,418 Stage: Train 0.5 | Epoch: 94 | Iter: 287600 | Total Loss: 0.003735 | Recon Loss: 0.003201 | Commit Loss: 0.001068 | Perplexity: 1190.185043
2025-10-17 00:53:14,564 Stage: Train 0.5 | Epoch: 94 | Iter: 287800 | Total Loss: 0.003791 | Recon Loss: 0.003257 | Commit Loss: 0.001068 | Perplexity: 1184.415016
2025-10-17 00:58:37,509 Stage: Train 0.5 | Epoch: 94 | Iter: 288000 | Total Loss: 0.003805 | Recon Loss: 0.003279 | Commit Loss: 0.001053 | Perplexity: 1189.959162
2025-10-17 01:04:00,533 Stage: Train 0.5 | Epoch: 94 | Iter: 288200 | Total Loss: 0.003827 | Recon Loss: 0.003287 | Commit Loss: 0.001080 | Perplexity: 1191.138537
2025-10-17 01:09:23,882 Stage: Train 0.5 | Epoch: 94 | Iter: 288400 | Total Loss: 0.003745 | Recon Loss: 0.003217 | Commit Loss: 0.001056 | Perplexity: 1185.206483
2025-10-17 01:14:46,939 Stage: Train 0.5 | Epoch: 94 | Iter: 288600 | Total Loss: 0.003734 | Recon Loss: 0.003218 | Commit Loss: 0.001033 | Perplexity: 1185.029975
Trainning Epoch:  58%|█████▊    | 95/165 [129:50:02<95:26:12, 4908.18s/it]Trainning Epoch:  58%|█████▊    | 95/165 [129:50:02<95:26:12, 4908.18s/it]2025-10-17 01:20:14,416 Stage: Train 0.5 | Epoch: 95 | Iter: 288800 | Total Loss: 0.003742 | Recon Loss: 0.003217 | Commit Loss: 0.001050 | Perplexity: 1189.008799
2025-10-17 01:25:37,501 Stage: Train 0.5 | Epoch: 95 | Iter: 289000 | Total Loss: 0.003691 | Recon Loss: 0.003167 | Commit Loss: 0.001048 | Perplexity: 1178.546752
2025-10-17 01:31:00,763 Stage: Train 0.5 | Epoch: 95 | Iter: 289200 | Total Loss: 0.003776 | Recon Loss: 0.003249 | Commit Loss: 0.001056 | Perplexity: 1184.526899
2025-10-17 01:36:23,872 Stage: Train 0.5 | Epoch: 95 | Iter: 289400 | Total Loss: 0.003754 | Recon Loss: 0.003227 | Commit Loss: 0.001055 | Perplexity: 1188.069980
2025-10-17 01:41:47,147 Stage: Train 0.5 | Epoch: 95 | Iter: 289600 | Total Loss: 0.003721 | Recon Loss: 0.003200 | Commit Loss: 0.001042 | Perplexity: 1181.237213
2025-10-17 01:47:09,725 Stage: Train 0.5 | Epoch: 95 | Iter: 289800 | Total Loss: 0.003858 | Recon Loss: 0.003337 | Commit Loss: 0.001043 | Perplexity: 1193.456852
2025-10-17 01:52:32,950 Stage: Train 0.5 | Epoch: 95 | Iter: 290000 | Total Loss: 0.003742 | Recon Loss: 0.003223 | Commit Loss: 0.001038 | Perplexity: 1185.631704
2025-10-17 01:57:55,983 Stage: Train 0.5 | Epoch: 95 | Iter: 290200 | Total Loss: 0.003659 | Recon Loss: 0.003142 | Commit Loss: 0.001034 | Perplexity: 1189.768499
2025-10-17 02:03:18,834 Stage: Train 0.5 | Epoch: 95 | Iter: 290400 | Total Loss: 0.003810 | Recon Loss: 0.003294 | Commit Loss: 0.001031 | Perplexity: 1180.592166
2025-10-17 02:08:39,835 Stage: Train 0.5 | Epoch: 95 | Iter: 290600 | Total Loss: 0.003755 | Recon Loss: 0.003236 | Commit Loss: 0.001039 | Perplexity: 1188.116996
2025-10-17 02:14:03,111 Stage: Train 0.5 | Epoch: 95 | Iter: 290800 | Total Loss: 0.003762 | Recon Loss: 0.003236 | Commit Loss: 0.001053 | Perplexity: 1184.497951
2025-10-17 02:19:25,672 Stage: Train 0.5 | Epoch: 95 | Iter: 291000 | Total Loss: 0.003775 | Recon Loss: 0.003253 | Commit Loss: 0.001044 | Perplexity: 1195.636106
2025-10-17 02:24:48,541 Stage: Train 0.5 | Epoch: 95 | Iter: 291200 | Total Loss: 0.003775 | Recon Loss: 0.003253 | Commit Loss: 0.001043 | Perplexity: 1187.340975
2025-10-17 02:30:10,672 Stage: Train 0.5 | Epoch: 95 | Iter: 291400 | Total Loss: 0.003758 | Recon Loss: 0.003241 | Commit Loss: 0.001034 | Perplexity: 1183.755544
2025-10-17 02:35:33,101 Stage: Train 0.5 | Epoch: 95 | Iter: 291600 | Total Loss: 0.003748 | Recon Loss: 0.003231 | Commit Loss: 0.001035 | Perplexity: 1188.767666
Trainning Epoch:  58%|█████▊    | 96/165 [131:11:49<94:04:10, 4907.98s/it]Trainning Epoch:  58%|█████▊    | 96/165 [131:11:49<94:04:10, 4907.98s/it]2025-10-17 02:40:59,951 Stage: Train 0.5 | Epoch: 96 | Iter: 291800 | Total Loss: 0.003758 | Recon Loss: 0.003232 | Commit Loss: 0.001053 | Perplexity: 1190.451301
2025-10-17 02:46:22,089 Stage: Train 0.5 | Epoch: 96 | Iter: 292000 | Total Loss: 0.003651 | Recon Loss: 0.003136 | Commit Loss: 0.001031 | Perplexity: 1188.982825
2025-10-17 02:51:45,193 Stage: Train 0.5 | Epoch: 96 | Iter: 292200 | Total Loss: 0.003727 | Recon Loss: 0.003205 | Commit Loss: 0.001044 | Perplexity: 1189.195753
2025-10-17 02:57:07,648 Stage: Train 0.5 | Epoch: 96 | Iter: 292400 | Total Loss: 0.003760 | Recon Loss: 0.003241 | Commit Loss: 0.001039 | Perplexity: 1180.006030
2025-10-17 03:02:30,729 Stage: Train 0.5 | Epoch: 96 | Iter: 292600 | Total Loss: 0.003788 | Recon Loss: 0.003253 | Commit Loss: 0.001070 | Perplexity: 1191.659789
2025-10-17 03:07:53,710 Stage: Train 0.5 | Epoch: 96 | Iter: 292800 | Total Loss: 0.003751 | Recon Loss: 0.003236 | Commit Loss: 0.001030 | Perplexity: 1179.498440
2025-10-17 03:13:16,639 Stage: Train 0.5 | Epoch: 96 | Iter: 293000 | Total Loss: 0.003697 | Recon Loss: 0.003178 | Commit Loss: 0.001038 | Perplexity: 1194.806978
2025-10-17 03:18:39,184 Stage: Train 0.5 | Epoch: 96 | Iter: 293200 | Total Loss: 0.003685 | Recon Loss: 0.003168 | Commit Loss: 0.001033 | Perplexity: 1190.734661
2025-10-17 03:24:01,278 Stage: Train 0.5 | Epoch: 96 | Iter: 293400 | Total Loss: 0.003820 | Recon Loss: 0.003299 | Commit Loss: 0.001042 | Perplexity: 1189.860100
2025-10-17 03:29:23,943 Stage: Train 0.5 | Epoch: 96 | Iter: 293600 | Total Loss: 0.003748 | Recon Loss: 0.003230 | Commit Loss: 0.001036 | Perplexity: 1190.697903
2025-10-17 03:34:46,797 Stage: Train 0.5 | Epoch: 96 | Iter: 293800 | Total Loss: 0.003736 | Recon Loss: 0.003215 | Commit Loss: 0.001041 | Perplexity: 1187.270607
2025-10-17 03:40:08,488 Stage: Train 0.5 | Epoch: 96 | Iter: 294000 | Total Loss: 0.003705 | Recon Loss: 0.003184 | Commit Loss: 0.001043 | Perplexity: 1189.356958
2025-10-17 03:45:31,498 Stage: Train 0.5 | Epoch: 96 | Iter: 294200 | Total Loss: 0.003820 | Recon Loss: 0.003296 | Commit Loss: 0.001048 | Perplexity: 1192.173598
2025-10-17 03:50:53,725 Stage: Train 0.5 | Epoch: 96 | Iter: 294400 | Total Loss: 0.003736 | Recon Loss: 0.003211 | Commit Loss: 0.001052 | Perplexity: 1188.717169
2025-10-17 03:56:16,002 Stage: Train 0.5 | Epoch: 96 | Iter: 294600 | Total Loss: 0.003692 | Recon Loss: 0.003169 | Commit Loss: 0.001046 | Perplexity: 1186.864425
Trainning Epoch:  59%|█████▉    | 97/165 [132:33:33<92:41:05, 4906.84s/it]Trainning Epoch:  59%|█████▉    | 97/165 [132:33:33<92:41:05, 4906.84s/it]2025-10-17 04:01:43,309 Stage: Train 0.5 | Epoch: 97 | Iter: 294800 | Total Loss: 0.003747 | Recon Loss: 0.003223 | Commit Loss: 0.001047 | Perplexity: 1183.357213
2025-10-17 04:07:05,831 Stage: Train 0.5 | Epoch: 97 | Iter: 295000 | Total Loss: 0.003655 | Recon Loss: 0.003146 | Commit Loss: 0.001018 | Perplexity: 1187.075249
2025-10-17 04:12:27,100 Stage: Train 0.5 | Epoch: 97 | Iter: 295200 | Total Loss: 0.003716 | Recon Loss: 0.003195 | Commit Loss: 0.001042 | Perplexity: 1196.158470
2025-10-17 04:17:50,030 Stage: Train 0.5 | Epoch: 97 | Iter: 295400 | Total Loss: 0.003758 | Recon Loss: 0.003232 | Commit Loss: 0.001050 | Perplexity: 1186.890948
2025-10-17 04:23:13,456 Stage: Train 0.5 | Epoch: 97 | Iter: 295600 | Total Loss: 0.003727 | Recon Loss: 0.003200 | Commit Loss: 0.001054 | Perplexity: 1193.069338
2025-10-17 04:28:36,542 Stage: Train 0.5 | Epoch: 97 | Iter: 295800 | Total Loss: 0.003649 | Recon Loss: 0.003130 | Commit Loss: 0.001038 | Perplexity: 1188.827479
2025-10-17 04:33:59,434 Stage: Train 0.5 | Epoch: 97 | Iter: 296000 | Total Loss: 0.003784 | Recon Loss: 0.003262 | Commit Loss: 0.001044 | Perplexity: 1189.646503
2025-10-17 04:39:22,372 Stage: Train 0.5 | Epoch: 97 | Iter: 296200 | Total Loss: 0.003754 | Recon Loss: 0.003223 | Commit Loss: 0.001063 | Perplexity: 1188.840263
2025-10-17 04:44:45,467 Stage: Train 0.5 | Epoch: 97 | Iter: 296400 | Total Loss: 0.003790 | Recon Loss: 0.003257 | Commit Loss: 0.001066 | Perplexity: 1192.870448
2025-10-17 04:50:08,316 Stage: Train 0.5 | Epoch: 97 | Iter: 296600 | Total Loss: 0.003749 | Recon Loss: 0.003232 | Commit Loss: 0.001034 | Perplexity: 1190.259427
2025-10-17 04:55:31,145 Stage: Train 0.5 | Epoch: 97 | Iter: 296800 | Total Loss: 0.003760 | Recon Loss: 0.003234 | Commit Loss: 0.001052 | Perplexity: 1190.458694
2025-10-17 05:00:54,170 Stage: Train 0.5 | Epoch: 97 | Iter: 297000 | Total Loss: 0.003698 | Recon Loss: 0.003172 | Commit Loss: 0.001051 | Perplexity: 1181.312059
2025-10-17 05:06:17,271 Stage: Train 0.5 | Epoch: 97 | Iter: 297200 | Total Loss: 0.003707 | Recon Loss: 0.003181 | Commit Loss: 0.001052 | Perplexity: 1192.666519
2025-10-17 05:11:40,305 Stage: Train 0.5 | Epoch: 97 | Iter: 297400 | Total Loss: 0.003771 | Recon Loss: 0.003248 | Commit Loss: 0.001046 | Perplexity: 1180.714126
2025-10-17 05:17:03,090 Stage: Train 0.5 | Epoch: 97 | Iter: 297600 | Total Loss: 0.003813 | Recon Loss: 0.003293 | Commit Loss: 0.001041 | Perplexity: 1187.425939
Trainning Epoch:  59%|█████▉    | 98/165 [133:55:22<91:19:51, 4907.33s/it]Trainning Epoch:  59%|█████▉    | 98/165 [133:55:22<91:19:51, 4907.33s/it]2025-10-17 05:22:30,480 Stage: Train 0.5 | Epoch: 98 | Iter: 297800 | Total Loss: 0.003730 | Recon Loss: 0.003210 | Commit Loss: 0.001040 | Perplexity: 1196.692841
2025-10-17 05:27:53,582 Stage: Train 0.5 | Epoch: 98 | Iter: 298000 | Total Loss: 0.003773 | Recon Loss: 0.003247 | Commit Loss: 0.001052 | Perplexity: 1182.091326
2025-10-17 05:33:16,251 Stage: Train 0.5 | Epoch: 98 | Iter: 298200 | Total Loss: 0.003690 | Recon Loss: 0.003180 | Commit Loss: 0.001021 | Perplexity: 1194.115110
2025-10-17 05:38:38,901 Stage: Train 0.5 | Epoch: 98 | Iter: 298400 | Total Loss: 0.003756 | Recon Loss: 0.003235 | Commit Loss: 0.001042 | Perplexity: 1189.532403
2025-10-17 05:44:01,934 Stage: Train 0.5 | Epoch: 98 | Iter: 298600 | Total Loss: 0.003812 | Recon Loss: 0.003292 | Commit Loss: 0.001041 | Perplexity: 1183.801701
2025-10-17 05:49:25,086 Stage: Train 0.5 | Epoch: 98 | Iter: 298800 | Total Loss: 0.003651 | Recon Loss: 0.003137 | Commit Loss: 0.001027 | Perplexity: 1194.246238
2025-10-17 05:54:48,622 Stage: Train 0.5 | Epoch: 98 | Iter: 299000 | Total Loss: 0.003688 | Recon Loss: 0.003170 | Commit Loss: 0.001035 | Perplexity: 1181.806151
2025-10-17 06:00:11,622 Stage: Train 0.5 | Epoch: 98 | Iter: 299200 | Total Loss: 0.003680 | Recon Loss: 0.003162 | Commit Loss: 0.001037 | Perplexity: 1181.254761
2025-10-17 06:05:34,528 Stage: Train 0.5 | Epoch: 98 | Iter: 299400 | Total Loss: 0.003795 | Recon Loss: 0.003262 | Commit Loss: 0.001066 | Perplexity: 1187.181621
2025-10-17 06:10:56,966 Stage: Train 0.5 | Epoch: 98 | Iter: 299600 | Total Loss: 0.003729 | Recon Loss: 0.003204 | Commit Loss: 0.001048 | Perplexity: 1185.710472
2025-10-17 06:16:19,986 Stage: Train 0.5 | Epoch: 98 | Iter: 299800 | Total Loss: 0.003741 | Recon Loss: 0.003210 | Commit Loss: 0.001063 | Perplexity: 1194.166700
2025-10-17 06:21:42,585 Stage: Train 0.5 | Epoch: 98 | Iter: 300000 | Total Loss: 0.003810 | Recon Loss: 0.003276 | Commit Loss: 0.001068 | Perplexity: 1191.423591
2025-10-17 06:21:42,586 Saving model at iteration 300000
2025-10-17 06:21:42,745 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000
2025-10-17 06:21:44,140 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000/model.safetensors
2025-10-17 06:21:45,917 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000/optimizer.bin
2025-10-17 06:21:45,918 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000/scheduler.bin
2025-10-17 06:21:45,918 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000/sampler.bin
2025-10-17 06:21:45,919 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_99_step_300000/random_states_0.pkl
2025-10-17 06:27:08,368 Stage: Train 0.5 | Epoch: 98 | Iter: 300200 | Total Loss: 0.003609 | Recon Loss: 0.003089 | Commit Loss: 0.001041 | Perplexity: 1194.899101
2025-10-17 06:32:31,275 Stage: Train 0.5 | Epoch: 98 | Iter: 300400 | Total Loss: 0.003796 | Recon Loss: 0.003268 | Commit Loss: 0.001055 | Perplexity: 1189.710548
2025-10-17 06:37:54,065 Stage: Train 0.5 | Epoch: 98 | Iter: 300600 | Total Loss: 0.003815 | Recon Loss: 0.003276 | Commit Loss: 0.001078 | Perplexity: 1191.127985
Trainning Epoch:  60%|██████    | 99/165 [135:17:14<89:59:42, 4908.83s/it]Trainning Epoch:  60%|██████    | 99/165 [135:17:14<89:59:42, 4908.83s/it]2025-10-17 06:43:21,223 Stage: Train 0.5 | Epoch: 99 | Iter: 300800 | Total Loss: 0.003698 | Recon Loss: 0.003175 | Commit Loss: 0.001046 | Perplexity: 1187.833607
2025-10-17 06:48:43,518 Stage: Train 0.5 | Epoch: 99 | Iter: 301000 | Total Loss: 0.003825 | Recon Loss: 0.003289 | Commit Loss: 0.001071 | Perplexity: 1205.095493
2025-10-17 06:54:05,977 Stage: Train 0.5 | Epoch: 99 | Iter: 301200 | Total Loss: 0.003656 | Recon Loss: 0.003139 | Commit Loss: 0.001034 | Perplexity: 1186.946523
2025-10-17 06:59:28,960 Stage: Train 0.5 | Epoch: 99 | Iter: 301400 | Total Loss: 0.003768 | Recon Loss: 0.003247 | Commit Loss: 0.001041 | Perplexity: 1189.889820
2025-10-17 07:04:51,724 Stage: Train 0.5 | Epoch: 99 | Iter: 301600 | Total Loss: 0.003665 | Recon Loss: 0.003134 | Commit Loss: 0.001062 | Perplexity: 1187.391186
2025-10-17 07:10:14,513 Stage: Train 0.5 | Epoch: 99 | Iter: 301800 | Total Loss: 0.003826 | Recon Loss: 0.003301 | Commit Loss: 0.001050 | Perplexity: 1188.101274
2025-10-17 07:15:36,057 Stage: Train 0.5 | Epoch: 99 | Iter: 302000 | Total Loss: 0.003709 | Recon Loss: 0.003180 | Commit Loss: 0.001058 | Perplexity: 1191.269870
2025-10-17 07:20:58,291 Stage: Train 0.5 | Epoch: 99 | Iter: 302200 | Total Loss: 0.003784 | Recon Loss: 0.003260 | Commit Loss: 0.001047 | Perplexity: 1174.032885
2025-10-17 07:26:20,245 Stage: Train 0.5 | Epoch: 99 | Iter: 302400 | Total Loss: 0.003784 | Recon Loss: 0.003263 | Commit Loss: 0.001043 | Perplexity: 1193.997355
2025-10-17 07:31:42,459 Stage: Train 0.5 | Epoch: 99 | Iter: 302600 | Total Loss: 0.003684 | Recon Loss: 0.003147 | Commit Loss: 0.001074 | Perplexity: 1191.424782
2025-10-17 07:37:05,435 Stage: Train 0.5 | Epoch: 99 | Iter: 302800 | Total Loss: 0.003738 | Recon Loss: 0.003213 | Commit Loss: 0.001050 | Perplexity: 1181.051649
2025-10-17 07:42:28,099 Stage: Train 0.5 | Epoch: 99 | Iter: 303000 | Total Loss: 0.003826 | Recon Loss: 0.003302 | Commit Loss: 0.001047 | Perplexity: 1190.549924
2025-10-17 07:47:50,977 Stage: Train 0.5 | Epoch: 99 | Iter: 303200 | Total Loss: 0.003658 | Recon Loss: 0.003132 | Commit Loss: 0.001052 | Perplexity: 1192.414283
2025-10-17 07:53:13,839 Stage: Train 0.5 | Epoch: 99 | Iter: 303400 | Total Loss: 0.003691 | Recon Loss: 0.003170 | Commit Loss: 0.001043 | Perplexity: 1180.045883
2025-10-17 07:58:36,987 Stage: Train 0.5 | Epoch: 99 | Iter: 303600 | Total Loss: 0.003682 | Recon Loss: 0.003163 | Commit Loss: 0.001037 | Perplexity: 1189.911623
2025-10-17 08:03:59,766 Stage: Train 0.5 | Epoch: 99 | Iter: 303800 | Total Loss: 0.003719 | Recon Loss: 0.003193 | Commit Loss: 0.001052 | Perplexity: 1187.709370
Trainning Epoch:  61%|██████    | 100/165 [136:38:58<88:36:24, 4907.46s/it]Trainning Epoch:  61%|██████    | 100/165 [136:38:58<88:36:24, 4907.46s/it]2025-10-17 08:09:26,994 Stage: Train 0.5 | Epoch: 100 | Iter: 304000 | Total Loss: 0.003726 | Recon Loss: 0.003211 | Commit Loss: 0.001030 | Perplexity: 1183.284618
2025-10-17 08:14:49,862 Stage: Train 0.5 | Epoch: 100 | Iter: 304200 | Total Loss: 0.003656 | Recon Loss: 0.003144 | Commit Loss: 0.001025 | Perplexity: 1187.198526
2025-10-17 08:20:12,903 Stage: Train 0.5 | Epoch: 100 | Iter: 304400 | Total Loss: 0.003698 | Recon Loss: 0.003168 | Commit Loss: 0.001059 | Perplexity: 1191.644616
2025-10-17 08:25:35,645 Stage: Train 0.5 | Epoch: 100 | Iter: 304600 | Total Loss: 0.003753 | Recon Loss: 0.003214 | Commit Loss: 0.001078 | Perplexity: 1190.667026
2025-10-17 08:30:57,234 Stage: Train 0.5 | Epoch: 100 | Iter: 304800 | Total Loss: 0.003712 | Recon Loss: 0.003186 | Commit Loss: 0.001051 | Perplexity: 1193.180430
2025-10-17 08:36:20,254 Stage: Train 0.5 | Epoch: 100 | Iter: 305000 | Total Loss: 0.003704 | Recon Loss: 0.003180 | Commit Loss: 0.001047 | Perplexity: 1191.185211
2025-10-17 08:41:43,039 Stage: Train 0.5 | Epoch: 100 | Iter: 305200 | Total Loss: 0.003736 | Recon Loss: 0.003219 | Commit Loss: 0.001035 | Perplexity: 1189.999442
2025-10-17 08:47:05,791 Stage: Train 0.5 | Epoch: 100 | Iter: 305400 | Total Loss: 0.003677 | Recon Loss: 0.003147 | Commit Loss: 0.001060 | Perplexity: 1195.873754
2025-10-17 08:52:28,831 Stage: Train 0.5 | Epoch: 100 | Iter: 305600 | Total Loss: 0.003710 | Recon Loss: 0.003193 | Commit Loss: 0.001033 | Perplexity: 1196.583315
2025-10-17 08:57:51,606 Stage: Train 0.5 | Epoch: 100 | Iter: 305800 | Total Loss: 0.003907 | Recon Loss: 0.003352 | Commit Loss: 0.001110 | Perplexity: 1183.569131
2025-10-17 09:03:14,620 Stage: Train 0.5 | Epoch: 100 | Iter: 306000 | Total Loss: 0.003651 | Recon Loss: 0.003136 | Commit Loss: 0.001030 | Perplexity: 1179.195939
2025-10-17 09:08:37,369 Stage: Train 0.5 | Epoch: 100 | Iter: 306200 | Total Loss: 0.003831 | Recon Loss: 0.003285 | Commit Loss: 0.001092 | Perplexity: 1188.954743
2025-10-17 09:14:00,027 Stage: Train 0.5 | Epoch: 100 | Iter: 306400 | Total Loss: 0.003717 | Recon Loss: 0.003181 | Commit Loss: 0.001072 | Perplexity: 1186.989188
2025-10-17 09:19:22,739 Stage: Train 0.5 | Epoch: 100 | Iter: 306600 | Total Loss: 0.003639 | Recon Loss: 0.003122 | Commit Loss: 0.001034 | Perplexity: 1186.018125
2025-10-17 09:24:44,970 Stage: Train 0.5 | Epoch: 100 | Iter: 306800 | Total Loss: 0.003682 | Recon Loss: 0.003149 | Commit Loss: 0.001067 | Perplexity: 1183.823193
Trainning Epoch:  61%|██████    | 101/165 [138:00:45<87:14:17, 4907.15s/it]Trainning Epoch:  61%|██████    | 101/165 [138:00:45<87:14:17, 4907.15s/it]2025-10-17 09:30:12,190 Stage: Train 0.5 | Epoch: 101 | Iter: 307000 | Total Loss: 0.003705 | Recon Loss: 0.003183 | Commit Loss: 0.001045 | Perplexity: 1192.147516
2025-10-17 09:35:35,164 Stage: Train 0.5 | Epoch: 101 | Iter: 307200 | Total Loss: 0.003644 | Recon Loss: 0.003121 | Commit Loss: 0.001047 | Perplexity: 1193.592583
2025-10-17 09:40:58,015 Stage: Train 0.5 | Epoch: 101 | Iter: 307400 | Total Loss: 0.003793 | Recon Loss: 0.003271 | Commit Loss: 0.001045 | Perplexity: 1186.392780
2025-10-17 09:46:20,575 Stage: Train 0.5 | Epoch: 101 | Iter: 307600 | Total Loss: 0.003713 | Recon Loss: 0.003186 | Commit Loss: 0.001055 | Perplexity: 1195.480285
2025-10-17 09:51:42,999 Stage: Train 0.5 | Epoch: 101 | Iter: 307800 | Total Loss: 0.003693 | Recon Loss: 0.003177 | Commit Loss: 0.001031 | Perplexity: 1190.077479
2025-10-17 09:57:06,094 Stage: Train 0.5 | Epoch: 101 | Iter: 308000 | Total Loss: 0.003700 | Recon Loss: 0.003168 | Commit Loss: 0.001063 | Perplexity: 1187.742434
2025-10-17 10:02:28,975 Stage: Train 0.5 | Epoch: 101 | Iter: 308200 | Total Loss: 0.003777 | Recon Loss: 0.003247 | Commit Loss: 0.001060 | Perplexity: 1187.685227
2025-10-17 10:07:51,574 Stage: Train 0.5 | Epoch: 101 | Iter: 308400 | Total Loss: 0.003682 | Recon Loss: 0.003153 | Commit Loss: 0.001057 | Perplexity: 1196.387754
2025-10-17 10:13:13,770 Stage: Train 0.5 | Epoch: 101 | Iter: 308600 | Total Loss: 0.003626 | Recon Loss: 0.003105 | Commit Loss: 0.001042 | Perplexity: 1186.562526
2025-10-17 10:18:36,388 Stage: Train 0.5 | Epoch: 101 | Iter: 308800 | Total Loss: 0.003697 | Recon Loss: 0.003172 | Commit Loss: 0.001052 | Perplexity: 1190.967878
2025-10-17 10:23:58,550 Stage: Train 0.5 | Epoch: 101 | Iter: 309000 | Total Loss: 0.003785 | Recon Loss: 0.003263 | Commit Loss: 0.001044 | Perplexity: 1190.170406
2025-10-17 10:29:21,526 Stage: Train 0.5 | Epoch: 101 | Iter: 309200 | Total Loss: 0.003782 | Recon Loss: 0.003248 | Commit Loss: 0.001068 | Perplexity: 1183.075212
2025-10-17 10:34:44,341 Stage: Train 0.5 | Epoch: 101 | Iter: 309400 | Total Loss: 0.003663 | Recon Loss: 0.003130 | Commit Loss: 0.001067 | Perplexity: 1185.336844
2025-10-17 10:40:07,029 Stage: Train 0.5 | Epoch: 101 | Iter: 309600 | Total Loss: 0.003735 | Recon Loss: 0.003208 | Commit Loss: 0.001055 | Perplexity: 1184.299537
2025-10-17 10:45:29,086 Stage: Train 0.5 | Epoch: 101 | Iter: 309800 | Total Loss: 0.003794 | Recon Loss: 0.003264 | Commit Loss: 0.001060 | Perplexity: 1195.182262
Trainning Epoch:  62%|██████▏   | 102/165 [139:22:30<85:51:56, 4906.61s/it]Trainning Epoch:  62%|██████▏   | 102/165 [139:22:30<85:51:56, 4906.61s/it]2025-10-17 10:50:56,060 Stage: Train 0.5 | Epoch: 102 | Iter: 310000 | Total Loss: 0.003729 | Recon Loss: 0.003205 | Commit Loss: 0.001049 | Perplexity: 1188.480176
2025-10-17 10:56:18,454 Stage: Train 0.5 | Epoch: 102 | Iter: 310200 | Total Loss: 0.003675 | Recon Loss: 0.003155 | Commit Loss: 0.001038 | Perplexity: 1193.840419
2025-10-17 11:01:41,083 Stage: Train 0.5 | Epoch: 102 | Iter: 310400 | Total Loss: 0.003662 | Recon Loss: 0.003148 | Commit Loss: 0.001028 | Perplexity: 1196.041166
2025-10-17 11:07:04,118 Stage: Train 0.5 | Epoch: 102 | Iter: 310600 | Total Loss: 0.003740 | Recon Loss: 0.003214 | Commit Loss: 0.001051 | Perplexity: 1188.816724
2025-10-17 11:12:26,817 Stage: Train 0.5 | Epoch: 102 | Iter: 310800 | Total Loss: 0.003681 | Recon Loss: 0.003159 | Commit Loss: 0.001043 | Perplexity: 1187.533225
2025-10-17 11:17:49,509 Stage: Train 0.5 | Epoch: 102 | Iter: 311000 | Total Loss: 0.003686 | Recon Loss: 0.003160 | Commit Loss: 0.001052 | Perplexity: 1196.912404
2025-10-17 11:23:12,336 Stage: Train 0.5 | Epoch: 102 | Iter: 311200 | Total Loss: 0.003780 | Recon Loss: 0.003244 | Commit Loss: 0.001072 | Perplexity: 1186.270135
2025-10-17 11:28:35,215 Stage: Train 0.5 | Epoch: 102 | Iter: 311400 | Total Loss: 0.003698 | Recon Loss: 0.003175 | Commit Loss: 0.001046 | Perplexity: 1187.666770
2025-10-17 11:33:58,089 Stage: Train 0.5 | Epoch: 102 | Iter: 311600 | Total Loss: 0.003721 | Recon Loss: 0.003200 | Commit Loss: 0.001041 | Perplexity: 1186.441522
2025-10-17 11:39:21,148 Stage: Train 0.5 | Epoch: 102 | Iter: 311800 | Total Loss: 0.003665 | Recon Loss: 0.003137 | Commit Loss: 0.001056 | Perplexity: 1191.948235
2025-10-17 11:44:43,989 Stage: Train 0.5 | Epoch: 102 | Iter: 312000 | Total Loss: 0.003743 | Recon Loss: 0.003223 | Commit Loss: 0.001040 | Perplexity: 1201.396631
2025-10-17 11:50:06,782 Stage: Train 0.5 | Epoch: 102 | Iter: 312200 | Total Loss: 0.003671 | Recon Loss: 0.003146 | Commit Loss: 0.001051 | Perplexity: 1195.029109
2025-10-17 11:55:29,595 Stage: Train 0.5 | Epoch: 102 | Iter: 312400 | Total Loss: 0.003702 | Recon Loss: 0.003188 | Commit Loss: 0.001030 | Perplexity: 1181.673035
2025-10-17 12:00:52,503 Stage: Train 0.5 | Epoch: 102 | Iter: 312600 | Total Loss: 0.003656 | Recon Loss: 0.003131 | Commit Loss: 0.001051 | Perplexity: 1187.754939
2025-10-17 12:06:15,319 Stage: Train 0.5 | Epoch: 102 | Iter: 312800 | Total Loss: 0.003670 | Recon Loss: 0.003154 | Commit Loss: 0.001033 | Perplexity: 1188.111834
Trainning Epoch:  62%|██████▏   | 103/165 [140:44:17<84:30:12, 4906.66s/it]Trainning Epoch:  62%|██████▏   | 103/165 [140:44:17<84:30:12, 4906.66s/it]2025-10-17 12:11:41,292 Stage: Train 0.5 | Epoch: 103 | Iter: 313000 | Total Loss: 0.003665 | Recon Loss: 0.003146 | Commit Loss: 0.001038 | Perplexity: 1202.617712
2025-10-17 12:17:04,361 Stage: Train 0.5 | Epoch: 103 | Iter: 313200 | Total Loss: 0.004010 | Recon Loss: 0.003397 | Commit Loss: 0.001226 | Perplexity: 1191.826836
2025-10-17 12:22:27,116 Stage: Train 0.5 | Epoch: 103 | Iter: 313400 | Total Loss: 0.003703 | Recon Loss: 0.003184 | Commit Loss: 0.001040 | Perplexity: 1188.757038
2025-10-17 12:27:49,671 Stage: Train 0.5 | Epoch: 103 | Iter: 313600 | Total Loss: 0.003717 | Recon Loss: 0.003192 | Commit Loss: 0.001050 | Perplexity: 1194.904425
2025-10-17 12:33:12,949 Stage: Train 0.5 | Epoch: 103 | Iter: 313800 | Total Loss: 0.003615 | Recon Loss: 0.003100 | Commit Loss: 0.001031 | Perplexity: 1187.617868
2025-10-17 12:38:36,008 Stage: Train 0.5 | Epoch: 103 | Iter: 314000 | Total Loss: 0.003696 | Recon Loss: 0.003173 | Commit Loss: 0.001047 | Perplexity: 1193.453220
2025-10-17 12:43:58,735 Stage: Train 0.5 | Epoch: 103 | Iter: 314200 | Total Loss: 0.003706 | Recon Loss: 0.003179 | Commit Loss: 0.001054 | Perplexity: 1186.372683
2025-10-17 12:49:21,532 Stage: Train 0.5 | Epoch: 103 | Iter: 314400 | Total Loss: 0.003655 | Recon Loss: 0.003135 | Commit Loss: 0.001039 | Perplexity: 1188.885134
2025-10-17 12:54:44,197 Stage: Train 0.5 | Epoch: 103 | Iter: 314600 | Total Loss: 0.003702 | Recon Loss: 0.003179 | Commit Loss: 0.001046 | Perplexity: 1193.427450
2025-10-17 13:00:07,099 Stage: Train 0.5 | Epoch: 103 | Iter: 314800 | Total Loss: 0.003722 | Recon Loss: 0.003208 | Commit Loss: 0.001027 | Perplexity: 1188.662545
2025-10-17 13:05:29,981 Stage: Train 0.5 | Epoch: 103 | Iter: 315000 | Total Loss: 0.004126 | Recon Loss: 0.003463 | Commit Loss: 0.001326 | Perplexity: 1202.484398
2025-10-17 13:10:52,923 Stage: Train 0.5 | Epoch: 103 | Iter: 315200 | Total Loss: 0.003737 | Recon Loss: 0.003216 | Commit Loss: 0.001042 | Perplexity: 1188.972430
2025-10-17 13:16:15,363 Stage: Train 0.5 | Epoch: 103 | Iter: 315400 | Total Loss: 0.003681 | Recon Loss: 0.003166 | Commit Loss: 0.001030 | Perplexity: 1190.342021
2025-10-17 13:21:38,466 Stage: Train 0.5 | Epoch: 103 | Iter: 315600 | Total Loss: 0.003617 | Recon Loss: 0.003098 | Commit Loss: 0.001038 | Perplexity: 1187.666455
2025-10-17 13:27:01,084 Stage: Train 0.5 | Epoch: 103 | Iter: 315800 | Total Loss: 0.003592 | Recon Loss: 0.003071 | Commit Loss: 0.001042 | Perplexity: 1191.272117
Trainning Epoch:  63%|██████▎   | 104/165 [142:06:05<83:08:53, 4907.11s/it]Trainning Epoch:  63%|██████▎   | 104/165 [142:06:05<83:08:53, 4907.11s/it]2025-10-17 13:32:28,079 Stage: Train 0.5 | Epoch: 104 | Iter: 316000 | Total Loss: 0.003719 | Recon Loss: 0.003197 | Commit Loss: 0.001045 | Perplexity: 1188.277881
2025-10-17 13:37:50,678 Stage: Train 0.5 | Epoch: 104 | Iter: 316200 | Total Loss: 0.003708 | Recon Loss: 0.003187 | Commit Loss: 0.001043 | Perplexity: 1183.262559
2025-10-17 13:43:13,791 Stage: Train 0.5 | Epoch: 104 | Iter: 316400 | Total Loss: 0.003604 | Recon Loss: 0.003087 | Commit Loss: 0.001034 | Perplexity: 1200.285521
2025-10-17 13:48:37,022 Stage: Train 0.5 | Epoch: 104 | Iter: 316600 | Total Loss: 0.003646 | Recon Loss: 0.003121 | Commit Loss: 0.001051 | Perplexity: 1191.904984
2025-10-17 13:54:00,105 Stage: Train 0.5 | Epoch: 104 | Iter: 316800 | Total Loss: 0.003675 | Recon Loss: 0.003165 | Commit Loss: 0.001020 | Perplexity: 1184.300363
2025-10-17 13:59:23,133 Stage: Train 0.5 | Epoch: 104 | Iter: 317000 | Total Loss: 0.003636 | Recon Loss: 0.003118 | Commit Loss: 0.001037 | Perplexity: 1194.092346
2025-10-17 14:04:45,566 Stage: Train 0.5 | Epoch: 104 | Iter: 317200 | Total Loss: 0.003640 | Recon Loss: 0.003111 | Commit Loss: 0.001058 | Perplexity: 1198.255745
2025-10-17 14:10:08,077 Stage: Train 0.5 | Epoch: 104 | Iter: 317400 | Total Loss: 0.003728 | Recon Loss: 0.003208 | Commit Loss: 0.001040 | Perplexity: 1190.615448
2025-10-17 14:15:31,014 Stage: Train 0.5 | Epoch: 104 | Iter: 317600 | Total Loss: 0.003696 | Recon Loss: 0.003172 | Commit Loss: 0.001048 | Perplexity: 1194.816415
2025-10-17 14:20:52,541 Stage: Train 0.5 | Epoch: 104 | Iter: 317800 | Total Loss: 0.003662 | Recon Loss: 0.003141 | Commit Loss: 0.001042 | Perplexity: 1186.737309
2025-10-17 14:26:14,622 Stage: Train 0.5 | Epoch: 104 | Iter: 318000 | Total Loss: 0.003657 | Recon Loss: 0.003138 | Commit Loss: 0.001037 | Perplexity: 1195.484679
2025-10-17 14:31:37,644 Stage: Train 0.5 | Epoch: 104 | Iter: 318200 | Total Loss: 0.003721 | Recon Loss: 0.003199 | Commit Loss: 0.001044 | Perplexity: 1193.499682
2025-10-17 14:37:00,240 Stage: Train 0.5 | Epoch: 104 | Iter: 318400 | Total Loss: 0.003611 | Recon Loss: 0.003094 | Commit Loss: 0.001035 | Perplexity: 1189.215341
2025-10-17 14:42:22,643 Stage: Train 0.5 | Epoch: 104 | Iter: 318600 | Total Loss: 0.003704 | Recon Loss: 0.003181 | Commit Loss: 0.001047 | Perplexity: 1191.074355
2025-10-17 14:47:45,345 Stage: Train 0.5 | Epoch: 104 | Iter: 318800 | Total Loss: 0.003657 | Recon Loss: 0.003138 | Commit Loss: 0.001038 | Perplexity: 1194.934064
Trainning Epoch:  64%|██████▎   | 105/165 [143:27:50<81:46:35, 4906.59s/it]Trainning Epoch:  64%|██████▎   | 105/165 [143:27:51<81:46:35, 4906.59s/it]2025-10-17 14:53:12,303 Stage: Train 0.5 | Epoch: 105 | Iter: 319000 | Total Loss: 0.003683 | Recon Loss: 0.003163 | Commit Loss: 0.001040 | Perplexity: 1191.861822
2025-10-17 14:58:35,188 Stage: Train 0.5 | Epoch: 105 | Iter: 319200 | Total Loss: 0.003682 | Recon Loss: 0.003157 | Commit Loss: 0.001049 | Perplexity: 1188.384671
2025-10-17 15:03:58,314 Stage: Train 0.5 | Epoch: 105 | Iter: 319400 | Total Loss: 0.003697 | Recon Loss: 0.003163 | Commit Loss: 0.001068 | Perplexity: 1201.612718
2025-10-17 15:09:21,233 Stage: Train 0.5 | Epoch: 105 | Iter: 319600 | Total Loss: 0.003637 | Recon Loss: 0.003113 | Commit Loss: 0.001046 | Perplexity: 1189.597122
2025-10-17 15:14:43,454 Stage: Train 0.5 | Epoch: 105 | Iter: 319800 | Total Loss: 0.003639 | Recon Loss: 0.003118 | Commit Loss: 0.001042 | Perplexity: 1180.886267
2025-10-17 15:20:06,624 Stage: Train 0.5 | Epoch: 105 | Iter: 320000 | Total Loss: 0.003702 | Recon Loss: 0.003177 | Commit Loss: 0.001049 | Perplexity: 1192.594725
2025-10-17 15:20:06,624 Saving model at iteration 320000
2025-10-17 15:20:06,780 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000
2025-10-17 15:20:08,165 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000/model.safetensors
2025-10-17 15:20:09,937 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000/optimizer.bin
2025-10-17 15:20:09,937 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000/scheduler.bin
2025-10-17 15:20:09,937 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000/sampler.bin
2025-10-17 15:20:09,938 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_106_step_320000/random_states_0.pkl
2025-10-17 15:25:33,067 Stage: Train 0.5 | Epoch: 105 | Iter: 320200 | Total Loss: 0.003679 | Recon Loss: 0.003162 | Commit Loss: 0.001034 | Perplexity: 1189.634632
2025-10-17 15:30:55,985 Stage: Train 0.5 | Epoch: 105 | Iter: 320400 | Total Loss: 0.003638 | Recon Loss: 0.003120 | Commit Loss: 0.001035 | Perplexity: 1190.872295
2025-10-17 15:36:18,961 Stage: Train 0.5 | Epoch: 105 | Iter: 320600 | Total Loss: 0.003617 | Recon Loss: 0.003102 | Commit Loss: 0.001030 | Perplexity: 1197.726582
2025-10-17 15:41:41,964 Stage: Train 0.5 | Epoch: 105 | Iter: 320800 | Total Loss: 0.003627 | Recon Loss: 0.003101 | Commit Loss: 0.001053 | Perplexity: 1194.324841
2025-10-17 15:47:05,068 Stage: Train 0.5 | Epoch: 105 | Iter: 321000 | Total Loss: 0.003694 | Recon Loss: 0.003168 | Commit Loss: 0.001050 | Perplexity: 1188.311175
2025-10-17 15:52:28,183 Stage: Train 0.5 | Epoch: 105 | Iter: 321200 | Total Loss: 0.003687 | Recon Loss: 0.003164 | Commit Loss: 0.001046 | Perplexity: 1198.676123
2025-10-17 15:57:51,042 Stage: Train 0.5 | Epoch: 105 | Iter: 321400 | Total Loss: 0.003670 | Recon Loss: 0.003149 | Commit Loss: 0.001040 | Perplexity: 1195.998678
2025-10-17 16:03:14,142 Stage: Train 0.5 | Epoch: 105 | Iter: 321600 | Total Loss: 0.003673 | Recon Loss: 0.003152 | Commit Loss: 0.001043 | Perplexity: 1190.780767
2025-10-17 16:08:36,996 Stage: Train 0.5 | Epoch: 105 | Iter: 321800 | Total Loss: 0.003700 | Recon Loss: 0.003176 | Commit Loss: 0.001047 | Perplexity: 1189.387474
2025-10-17 16:13:59,772 Stage: Train 0.5 | Epoch: 105 | Iter: 322000 | Total Loss: 0.003665 | Recon Loss: 0.003142 | Commit Loss: 0.001047 | Perplexity: 1194.072263
Trainning Epoch:  64%|██████▍   | 106/165 [144:49:43<80:26:42, 4908.51s/it]Trainning Epoch:  64%|██████▍   | 106/165 [144:49:44<80:26:42, 4908.51s/it]2025-10-17 16:19:26,302 Stage: Train 0.5 | Epoch: 106 | Iter: 322200 | Total Loss: 0.003645 | Recon Loss: 0.003124 | Commit Loss: 0.001042 | Perplexity: 1197.702498
2025-10-17 16:24:49,321 Stage: Train 0.5 | Epoch: 106 | Iter: 322400 | Total Loss: 0.003781 | Recon Loss: 0.003232 | Commit Loss: 0.001098 | Perplexity: 1192.587864
2025-10-17 16:30:11,845 Stage: Train 0.5 | Epoch: 106 | Iter: 322600 | Total Loss: 0.003604 | Recon Loss: 0.003082 | Commit Loss: 0.001045 | Perplexity: 1196.673418
2025-10-17 16:35:34,894 Stage: Train 0.5 | Epoch: 106 | Iter: 322800 | Total Loss: 0.003689 | Recon Loss: 0.003176 | Commit Loss: 0.001025 | Perplexity: 1187.724307
2025-10-17 16:40:58,005 Stage: Train 0.5 | Epoch: 106 | Iter: 323000 | Total Loss: 0.003607 | Recon Loss: 0.003087 | Commit Loss: 0.001041 | Perplexity: 1188.662579
2025-10-17 16:46:19,415 Stage: Train 0.5 | Epoch: 106 | Iter: 323200 | Total Loss: 0.003660 | Recon Loss: 0.003138 | Commit Loss: 0.001045 | Perplexity: 1197.138678
2025-10-17 16:51:40,695 Stage: Train 0.5 | Epoch: 106 | Iter: 323400 | Total Loss: 0.003711 | Recon Loss: 0.003190 | Commit Loss: 0.001043 | Perplexity: 1191.656966
2025-10-17 16:57:03,240 Stage: Train 0.5 | Epoch: 106 | Iter: 323600 | Total Loss: 0.003677 | Recon Loss: 0.003159 | Commit Loss: 0.001036 | Perplexity: 1189.475531
2025-10-17 17:02:25,569 Stage: Train 0.5 | Epoch: 106 | Iter: 323800 | Total Loss: 0.003670 | Recon Loss: 0.003146 | Commit Loss: 0.001048 | Perplexity: 1192.907097
2025-10-17 17:07:48,686 Stage: Train 0.5 | Epoch: 106 | Iter: 324000 | Total Loss: 0.003636 | Recon Loss: 0.003116 | Commit Loss: 0.001041 | Perplexity: 1190.167258
2025-10-17 17:13:11,535 Stage: Train 0.5 | Epoch: 106 | Iter: 324200 | Total Loss: 0.003703 | Recon Loss: 0.003183 | Commit Loss: 0.001040 | Perplexity: 1188.733441
2025-10-17 17:18:34,288 Stage: Train 0.5 | Epoch: 106 | Iter: 324400 | Total Loss: 0.003661 | Recon Loss: 0.003142 | Commit Loss: 0.001037 | Perplexity: 1189.580810
2025-10-17 17:23:57,084 Stage: Train 0.5 | Epoch: 106 | Iter: 324600 | Total Loss: 0.003726 | Recon Loss: 0.003209 | Commit Loss: 0.001035 | Perplexity: 1191.568090
2025-10-17 17:29:19,913 Stage: Train 0.5 | Epoch: 106 | Iter: 324800 | Total Loss: 0.003646 | Recon Loss: 0.003125 | Commit Loss: 0.001042 | Perplexity: 1193.462728
2025-10-17 17:34:42,819 Stage: Train 0.5 | Epoch: 106 | Iter: 325000 | Total Loss: 0.003715 | Recon Loss: 0.003198 | Commit Loss: 0.001035 | Perplexity: 1192.266435
Trainning Epoch:  65%|██████▍   | 107/165 [146:11:28<79:03:42, 4907.29s/it]Trainning Epoch:  65%|██████▍   | 107/165 [146:11:28<79:03:43, 4907.30s/it]2025-10-17 17:40:09,847 Stage: Train 0.5 | Epoch: 107 | Iter: 325200 | Total Loss: 0.003644 | Recon Loss: 0.003127 | Commit Loss: 0.001034 | Perplexity: 1199.318771
2025-10-17 17:45:32,710 Stage: Train 0.5 | Epoch: 107 | Iter: 325400 | Total Loss: 0.003682 | Recon Loss: 0.003165 | Commit Loss: 0.001032 | Perplexity: 1192.879269
2025-10-17 17:50:55,656 Stage: Train 0.5 | Epoch: 107 | Iter: 325600 | Total Loss: 0.003659 | Recon Loss: 0.003130 | Commit Loss: 0.001059 | Perplexity: 1190.693580
2025-10-17 17:56:18,646 Stage: Train 0.5 | Epoch: 107 | Iter: 325800 | Total Loss: 0.003722 | Recon Loss: 0.003191 | Commit Loss: 0.001063 | Perplexity: 1191.234418
2025-10-17 18:01:41,552 Stage: Train 0.5 | Epoch: 107 | Iter: 326000 | Total Loss: 0.003765 | Recon Loss: 0.003223 | Commit Loss: 0.001084 | Perplexity: 1192.736990
2025-10-17 18:07:04,622 Stage: Train 0.5 | Epoch: 107 | Iter: 326200 | Total Loss: 0.003689 | Recon Loss: 0.003167 | Commit Loss: 0.001044 | Perplexity: 1188.689834
2025-10-17 18:12:28,015 Stage: Train 0.5 | Epoch: 107 | Iter: 326400 | Total Loss: 0.003622 | Recon Loss: 0.003107 | Commit Loss: 0.001029 | Perplexity: 1182.222546
2025-10-17 18:17:50,868 Stage: Train 0.5 | Epoch: 107 | Iter: 326600 | Total Loss: 0.003608 | Recon Loss: 0.003087 | Commit Loss: 0.001041 | Perplexity: 1189.414534
2025-10-17 18:23:13,732 Stage: Train 0.5 | Epoch: 107 | Iter: 326800 | Total Loss: 0.003727 | Recon Loss: 0.003221 | Commit Loss: 0.001011 | Perplexity: 1191.426648
2025-10-17 18:28:36,705 Stage: Train 0.5 | Epoch: 107 | Iter: 327000 | Total Loss: 0.003617 | Recon Loss: 0.003107 | Commit Loss: 0.001020 | Perplexity: 1193.562494
2025-10-17 18:33:59,452 Stage: Train 0.5 | Epoch: 107 | Iter: 327200 | Total Loss: 0.003619 | Recon Loss: 0.003093 | Commit Loss: 0.001052 | Perplexity: 1192.905411
2025-10-17 18:39:22,489 Stage: Train 0.5 | Epoch: 107 | Iter: 327400 | Total Loss: 0.003627 | Recon Loss: 0.003105 | Commit Loss: 0.001043 | Perplexity: 1198.540878
2025-10-17 18:44:44,350 Stage: Train 0.5 | Epoch: 107 | Iter: 327600 | Total Loss: 0.003718 | Recon Loss: 0.003190 | Commit Loss: 0.001056 | Perplexity: 1192.462867
2025-10-17 18:50:05,974 Stage: Train 0.5 | Epoch: 107 | Iter: 327800 | Total Loss: 0.003648 | Recon Loss: 0.003125 | Commit Loss: 0.001045 | Perplexity: 1195.803400
2025-10-17 18:55:29,043 Stage: Train 0.5 | Epoch: 107 | Iter: 328000 | Total Loss: 0.003570 | Recon Loss: 0.003045 | Commit Loss: 0.001049 | Perplexity: 1195.274315
Trainning Epoch:  65%|██████▌   | 108/165 [147:33:15<77:41:59, 4907.35s/it]Trainning Epoch:  65%|██████▌   | 108/165 [147:33:15<77:41:59, 4907.35s/it]2025-10-17 19:00:55,999 Stage: Train 0.5 | Epoch: 108 | Iter: 328200 | Total Loss: 0.003651 | Recon Loss: 0.003139 | Commit Loss: 0.001024 | Perplexity: 1189.483919
2025-10-17 19:06:17,524 Stage: Train 0.5 | Epoch: 108 | Iter: 328400 | Total Loss: 0.003641 | Recon Loss: 0.003117 | Commit Loss: 0.001047 | Perplexity: 1197.545120
2025-10-17 19:11:40,385 Stage: Train 0.5 | Epoch: 108 | Iter: 328600 | Total Loss: 0.003628 | Recon Loss: 0.003114 | Commit Loss: 0.001029 | Perplexity: 1197.049979
2025-10-17 19:17:03,322 Stage: Train 0.5 | Epoch: 108 | Iter: 328800 | Total Loss: 0.003605 | Recon Loss: 0.003078 | Commit Loss: 0.001053 | Perplexity: 1195.093202
2025-10-17 19:22:26,497 Stage: Train 0.5 | Epoch: 108 | Iter: 329000 | Total Loss: 0.003645 | Recon Loss: 0.003126 | Commit Loss: 0.001038 | Perplexity: 1187.506368
2025-10-17 19:27:49,145 Stage: Train 0.5 | Epoch: 108 | Iter: 329200 | Total Loss: 0.003707 | Recon Loss: 0.003186 | Commit Loss: 0.001042 | Perplexity: 1191.807380
2025-10-17 19:33:10,952 Stage: Train 0.5 | Epoch: 108 | Iter: 329400 | Total Loss: 0.003660 | Recon Loss: 0.003129 | Commit Loss: 0.001062 | Perplexity: 1191.706526
2025-10-17 19:38:33,878 Stage: Train 0.5 | Epoch: 108 | Iter: 329600 | Total Loss: 0.003637 | Recon Loss: 0.003104 | Commit Loss: 0.001065 | Perplexity: 1194.617325
2025-10-17 19:43:56,745 Stage: Train 0.5 | Epoch: 108 | Iter: 329800 | Total Loss: 0.003693 | Recon Loss: 0.003165 | Commit Loss: 0.001057 | Perplexity: 1190.988928
2025-10-17 19:49:19,711 Stage: Train 0.5 | Epoch: 108 | Iter: 330000 | Total Loss: 0.003660 | Recon Loss: 0.003136 | Commit Loss: 0.001048 | Perplexity: 1199.830831
2025-10-17 19:54:42,065 Stage: Train 0.5 | Epoch: 108 | Iter: 330200 | Total Loss: 0.003608 | Recon Loss: 0.003080 | Commit Loss: 0.001055 | Perplexity: 1194.502608
2025-10-17 20:00:04,512 Stage: Train 0.5 | Epoch: 108 | Iter: 330400 | Total Loss: 0.003683 | Recon Loss: 0.003159 | Commit Loss: 0.001047 | Perplexity: 1196.242939
2025-10-17 20:05:27,359 Stage: Train 0.5 | Epoch: 108 | Iter: 330600 | Total Loss: 0.003634 | Recon Loss: 0.003111 | Commit Loss: 0.001046 | Perplexity: 1192.794800
2025-10-17 20:10:50,242 Stage: Train 0.5 | Epoch: 108 | Iter: 330800 | Total Loss: 0.003683 | Recon Loss: 0.003159 | Commit Loss: 0.001048 | Perplexity: 1185.010532
2025-10-17 20:16:13,264 Stage: Train 0.5 | Epoch: 108 | Iter: 331000 | Total Loss: 0.003689 | Recon Loss: 0.003161 | Commit Loss: 0.001055 | Perplexity: 1190.453959
Trainning Epoch:  66%|██████▌   | 109/165 [148:55:01<76:19:42, 4906.83s/it]Trainning Epoch:  66%|██████▌   | 109/165 [148:55:01<76:19:42, 4906.83s/it]2025-10-17 20:21:40,280 Stage: Train 0.5 | Epoch: 109 | Iter: 331200 | Total Loss: 0.003602 | Recon Loss: 0.003088 | Commit Loss: 0.001028 | Perplexity: 1188.617992
2025-10-17 20:27:02,688 Stage: Train 0.5 | Epoch: 109 | Iter: 331400 | Total Loss: 0.003565 | Recon Loss: 0.003043 | Commit Loss: 0.001044 | Perplexity: 1194.883352
2025-10-17 20:32:25,248 Stage: Train 0.5 | Epoch: 109 | Iter: 331600 | Total Loss: 0.003584 | Recon Loss: 0.003060 | Commit Loss: 0.001047 | Perplexity: 1190.693062
2025-10-17 20:37:47,493 Stage: Train 0.5 | Epoch: 109 | Iter: 331800 | Total Loss: 0.003775 | Recon Loss: 0.003249 | Commit Loss: 0.001052 | Perplexity: 1196.912871
2025-10-17 20:43:09,508 Stage: Train 0.5 | Epoch: 109 | Iter: 332000 | Total Loss: 0.003592 | Recon Loss: 0.003061 | Commit Loss: 0.001062 | Perplexity: 1200.037246
2025-10-17 20:48:31,427 Stage: Train 0.5 | Epoch: 109 | Iter: 332200 | Total Loss: 0.003629 | Recon Loss: 0.003110 | Commit Loss: 0.001039 | Perplexity: 1192.391909
2025-10-17 20:53:54,161 Stage: Train 0.5 | Epoch: 109 | Iter: 332400 | Total Loss: 0.003618 | Recon Loss: 0.003098 | Commit Loss: 0.001040 | Perplexity: 1189.956877
2025-10-17 20:59:17,473 Stage: Train 0.5 | Epoch: 109 | Iter: 332600 | Total Loss: 0.003628 | Recon Loss: 0.003107 | Commit Loss: 0.001042 | Perplexity: 1192.396178
2025-10-17 21:04:40,103 Stage: Train 0.5 | Epoch: 109 | Iter: 332800 | Total Loss: 0.003806 | Recon Loss: 0.003224 | Commit Loss: 0.001164 | Perplexity: 1196.712680
2025-10-17 21:10:03,086 Stage: Train 0.5 | Epoch: 109 | Iter: 333000 | Total Loss: 0.003668 | Recon Loss: 0.003147 | Commit Loss: 0.001042 | Perplexity: 1193.427293
2025-10-17 21:15:25,887 Stage: Train 0.5 | Epoch: 109 | Iter: 333200 | Total Loss: 0.003563 | Recon Loss: 0.003050 | Commit Loss: 0.001025 | Perplexity: 1191.902638
2025-10-17 21:20:48,920 Stage: Train 0.5 | Epoch: 109 | Iter: 333400 | Total Loss: 0.003639 | Recon Loss: 0.003117 | Commit Loss: 0.001043 | Perplexity: 1194.963425
2025-10-17 21:26:11,683 Stage: Train 0.5 | Epoch: 109 | Iter: 333600 | Total Loss: 0.003696 | Recon Loss: 0.003178 | Commit Loss: 0.001038 | Perplexity: 1190.536573
2025-10-17 21:31:34,680 Stage: Train 0.5 | Epoch: 109 | Iter: 333800 | Total Loss: 0.003638 | Recon Loss: 0.003118 | Commit Loss: 0.001040 | Perplexity: 1197.048596
2025-10-17 21:36:57,459 Stage: Train 0.5 | Epoch: 109 | Iter: 334000 | Total Loss: 0.003550 | Recon Loss: 0.003028 | Commit Loss: 0.001045 | Perplexity: 1186.299232
Trainning Epoch:  67%|██████▋   | 110/165 [150:16:47<74:57:36, 4906.48s/it]Trainning Epoch:  67%|██████▋   | 110/165 [150:16:47<74:57:36, 4906.48s/it]2025-10-17 21:42:24,576 Stage: Train 0.5 | Epoch: 110 | Iter: 334200 | Total Loss: 0.003689 | Recon Loss: 0.003165 | Commit Loss: 0.001048 | Perplexity: 1193.906295
2025-10-17 21:47:47,442 Stage: Train 0.5 | Epoch: 110 | Iter: 334400 | Total Loss: 0.003634 | Recon Loss: 0.003111 | Commit Loss: 0.001046 | Perplexity: 1201.198454
2025-10-17 21:53:10,257 Stage: Train 0.5 | Epoch: 110 | Iter: 334600 | Total Loss: 0.003640 | Recon Loss: 0.003122 | Commit Loss: 0.001036 | Perplexity: 1183.791102
2025-10-17 21:58:33,565 Stage: Train 0.5 | Epoch: 110 | Iter: 334800 | Total Loss: 0.003566 | Recon Loss: 0.003049 | Commit Loss: 0.001035 | Perplexity: 1195.432245
2025-10-17 22:03:56,667 Stage: Train 0.5 | Epoch: 110 | Iter: 335000 | Total Loss: 0.003625 | Recon Loss: 0.003094 | Commit Loss: 0.001061 | Perplexity: 1194.412498
2025-10-17 22:09:19,831 Stage: Train 0.5 | Epoch: 110 | Iter: 335200 | Total Loss: 0.003629 | Recon Loss: 0.003110 | Commit Loss: 0.001037 | Perplexity: 1189.521415
2025-10-17 22:14:42,980 Stage: Train 0.5 | Epoch: 110 | Iter: 335400 | Total Loss: 0.003598 | Recon Loss: 0.003073 | Commit Loss: 0.001050 | Perplexity: 1197.264073
2025-10-17 22:20:06,061 Stage: Train 0.5 | Epoch: 110 | Iter: 335600 | Total Loss: 0.003651 | Recon Loss: 0.003132 | Commit Loss: 0.001038 | Perplexity: 1196.041560
2025-10-17 22:25:28,105 Stage: Train 0.5 | Epoch: 110 | Iter: 335800 | Total Loss: 0.003557 | Recon Loss: 0.003033 | Commit Loss: 0.001048 | Perplexity: 1198.871982
2025-10-17 22:30:50,625 Stage: Train 0.5 | Epoch: 110 | Iter: 336000 | Total Loss: 0.003662 | Recon Loss: 0.003145 | Commit Loss: 0.001034 | Perplexity: 1188.917814
2025-10-17 22:36:13,653 Stage: Train 0.5 | Epoch: 110 | Iter: 336200 | Total Loss: 0.003617 | Recon Loss: 0.003090 | Commit Loss: 0.001056 | Perplexity: 1196.664102
2025-10-17 22:41:36,514 Stage: Train 0.5 | Epoch: 110 | Iter: 336400 | Total Loss: 0.003654 | Recon Loss: 0.003138 | Commit Loss: 0.001032 | Perplexity: 1191.336970
2025-10-17 22:46:59,638 Stage: Train 0.5 | Epoch: 110 | Iter: 336600 | Total Loss: 0.003610 | Recon Loss: 0.003090 | Commit Loss: 0.001038 | Perplexity: 1192.821282
2025-10-17 22:52:22,031 Stage: Train 0.5 | Epoch: 110 | Iter: 336800 | Total Loss: 0.003704 | Recon Loss: 0.003168 | Commit Loss: 0.001073 | Perplexity: 1194.111673
2025-10-17 22:57:45,083 Stage: Train 0.5 | Epoch: 110 | Iter: 337000 | Total Loss: 0.003581 | Recon Loss: 0.003062 | Commit Loss: 0.001037 | Perplexity: 1191.637027
2025-10-17 23:03:08,156 Stage: Train 0.5 | Epoch: 110 | Iter: 337200 | Total Loss: 0.003580 | Recon Loss: 0.003065 | Commit Loss: 0.001030 | Perplexity: 1200.554528
Trainning Epoch:  67%|██████▋   | 111/165 [151:38:36<73:36:32, 4907.27s/it]Trainning Epoch:  67%|██████▋   | 111/165 [151:38:36<73:36:32, 4907.27s/it]2025-10-17 23:08:35,787 Stage: Train 0.5 | Epoch: 111 | Iter: 337400 | Total Loss: 0.003650 | Recon Loss: 0.003128 | Commit Loss: 0.001044 | Perplexity: 1195.847354
2025-10-17 23:13:58,894 Stage: Train 0.5 | Epoch: 111 | Iter: 337600 | Total Loss: 0.003593 | Recon Loss: 0.003082 | Commit Loss: 0.001022 | Perplexity: 1195.850730
2025-10-17 23:19:21,141 Stage: Train 0.5 | Epoch: 111 | Iter: 337800 | Total Loss: 0.003604 | Recon Loss: 0.003076 | Commit Loss: 0.001054 | Perplexity: 1193.546153
2025-10-17 23:24:44,223 Stage: Train 0.5 | Epoch: 111 | Iter: 338000 | Total Loss: 0.003645 | Recon Loss: 0.003115 | Commit Loss: 0.001060 | Perplexity: 1194.516260
2025-10-17 23:30:07,297 Stage: Train 0.5 | Epoch: 111 | Iter: 338200 | Total Loss: 0.003594 | Recon Loss: 0.003073 | Commit Loss: 0.001042 | Perplexity: 1202.955165
2025-10-17 23:35:30,234 Stage: Train 0.5 | Epoch: 111 | Iter: 338400 | Total Loss: 0.003588 | Recon Loss: 0.003068 | Commit Loss: 0.001039 | Perplexity: 1193.821529
2025-10-17 23:40:53,051 Stage: Train 0.5 | Epoch: 111 | Iter: 338600 | Total Loss: 0.003597 | Recon Loss: 0.003069 | Commit Loss: 0.001055 | Perplexity: 1200.762160
2025-10-17 23:46:16,038 Stage: Train 0.5 | Epoch: 111 | Iter: 338800 | Total Loss: 0.003547 | Recon Loss: 0.003023 | Commit Loss: 0.001048 | Perplexity: 1196.904621
2025-10-17 23:51:39,033 Stage: Train 0.5 | Epoch: 111 | Iter: 339000 | Total Loss: 0.003629 | Recon Loss: 0.003108 | Commit Loss: 0.001043 | Perplexity: 1189.055674
2025-10-17 23:57:02,025 Stage: Train 0.5 | Epoch: 111 | Iter: 339200 | Total Loss: 0.003688 | Recon Loss: 0.003151 | Commit Loss: 0.001074 | Perplexity: 1198.299028
2025-10-18 00:02:25,197 Stage: Train 0.5 | Epoch: 111 | Iter: 339400 | Total Loss: 0.003668 | Recon Loss: 0.003138 | Commit Loss: 0.001059 | Perplexity: 1195.564412
2025-10-18 00:07:48,124 Stage: Train 0.5 | Epoch: 111 | Iter: 339600 | Total Loss: 0.003565 | Recon Loss: 0.003038 | Commit Loss: 0.001053 | Perplexity: 1193.098636
2025-10-18 00:13:11,062 Stage: Train 0.5 | Epoch: 111 | Iter: 339800 | Total Loss: 0.003635 | Recon Loss: 0.003114 | Commit Loss: 0.001043 | Perplexity: 1193.875832
2025-10-18 00:18:34,095 Stage: Train 0.5 | Epoch: 111 | Iter: 340000 | Total Loss: 0.003599 | Recon Loss: 0.003076 | Commit Loss: 0.001045 | Perplexity: 1198.715966
2025-10-18 00:18:34,096 Saving model at iteration 340000
2025-10-18 00:18:34,252 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000
2025-10-18 00:18:35,637 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000/model.safetensors
2025-10-18 00:18:37,437 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000/optimizer.bin
2025-10-18 00:18:37,437 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000/scheduler.bin
2025-10-18 00:18:37,438 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000/sampler.bin
2025-10-18 00:18:37,438 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_112_step_340000/random_states_0.pkl
2025-10-18 00:24:00,533 Stage: Train 0.5 | Epoch: 111 | Iter: 340200 | Total Loss: 0.003626 | Recon Loss: 0.003105 | Commit Loss: 0.001042 | Perplexity: 1196.532283
Trainning Epoch:  68%|██████▊   | 112/165 [153:00:29<72:16:27, 4909.19s/it]Trainning Epoch:  68%|██████▊   | 112/165 [153:00:30<72:16:26, 4909.19s/it]2025-10-18 00:29:27,671 Stage: Train 0.5 | Epoch: 112 | Iter: 340400 | Total Loss: 0.003652 | Recon Loss: 0.003128 | Commit Loss: 0.001048 | Perplexity: 1199.773848
2025-10-18 00:34:50,590 Stage: Train 0.5 | Epoch: 112 | Iter: 340600 | Total Loss: 0.003587 | Recon Loss: 0.003060 | Commit Loss: 0.001053 | Perplexity: 1198.771857
2025-10-18 00:40:13,293 Stage: Train 0.5 | Epoch: 112 | Iter: 340800 | Total Loss: 0.003662 | Recon Loss: 0.003130 | Commit Loss: 0.001063 | Perplexity: 1192.122328
2025-10-18 00:45:35,471 Stage: Train 0.5 | Epoch: 112 | Iter: 341000 | Total Loss: 0.003748 | Recon Loss: 0.003222 | Commit Loss: 0.001051 | Perplexity: 1196.009927
2025-10-18 00:50:57,627 Stage: Train 0.5 | Epoch: 112 | Iter: 341200 | Total Loss: 0.003556 | Recon Loss: 0.003035 | Commit Loss: 0.001042 | Perplexity: 1191.881115
2025-10-18 00:56:20,441 Stage: Train 0.5 | Epoch: 112 | Iter: 341400 | Total Loss: 0.003619 | Recon Loss: 0.003101 | Commit Loss: 0.001036 | Perplexity: 1190.184528
2025-10-18 01:01:43,276 Stage: Train 0.5 | Epoch: 112 | Iter: 341600 | Total Loss: 0.003650 | Recon Loss: 0.003116 | Commit Loss: 0.001066 | Perplexity: 1198.088387
2025-10-18 01:07:05,994 Stage: Train 0.5 | Epoch: 112 | Iter: 341800 | Total Loss: 0.003611 | Recon Loss: 0.003083 | Commit Loss: 0.001054 | Perplexity: 1201.781545
2025-10-18 01:12:28,932 Stage: Train 0.5 | Epoch: 112 | Iter: 342000 | Total Loss: 0.003618 | Recon Loss: 0.003095 | Commit Loss: 0.001046 | Perplexity: 1189.098891
2025-10-18 01:17:51,865 Stage: Train 0.5 | Epoch: 112 | Iter: 342200 | Total Loss: 0.003588 | Recon Loss: 0.003074 | Commit Loss: 0.001030 | Perplexity: 1188.683206
2025-10-18 01:23:14,709 Stage: Train 0.5 | Epoch: 112 | Iter: 342400 | Total Loss: 0.003581 | Recon Loss: 0.003055 | Commit Loss: 0.001052 | Perplexity: 1199.714475
2025-10-18 01:28:37,345 Stage: Train 0.5 | Epoch: 112 | Iter: 342600 | Total Loss: 0.003594 | Recon Loss: 0.003068 | Commit Loss: 0.001052 | Perplexity: 1194.612003
2025-10-18 01:33:59,727 Stage: Train 0.5 | Epoch: 112 | Iter: 342800 | Total Loss: 0.003692 | Recon Loss: 0.003164 | Commit Loss: 0.001057 | Perplexity: 1200.464807
2025-10-18 01:39:22,931 Stage: Train 0.5 | Epoch: 112 | Iter: 343000 | Total Loss: 0.003627 | Recon Loss: 0.003100 | Commit Loss: 0.001054 | Perplexity: 1191.427213
2025-10-18 01:44:45,773 Stage: Train 0.5 | Epoch: 112 | Iter: 343200 | Total Loss: 0.003646 | Recon Loss: 0.003124 | Commit Loss: 0.001044 | Perplexity: 1188.466090
Trainning Epoch:  68%|██████▊   | 113/165 [154:22:16<70:53:56, 4908.40s/it]Trainning Epoch:  68%|██████▊   | 113/165 [154:22:16<70:53:56, 4908.40s/it]2025-10-18 01:50:12,886 Stage: Train 0.5 | Epoch: 113 | Iter: 343400 | Total Loss: 0.003662 | Recon Loss: 0.003138 | Commit Loss: 0.001049 | Perplexity: 1197.937220
2025-10-18 01:55:35,731 Stage: Train 0.5 | Epoch: 113 | Iter: 343600 | Total Loss: 0.003612 | Recon Loss: 0.003092 | Commit Loss: 0.001040 | Perplexity: 1191.936963
2025-10-18 02:00:57,566 Stage: Train 0.5 | Epoch: 113 | Iter: 343800 | Total Loss: 0.003567 | Recon Loss: 0.003042 | Commit Loss: 0.001050 | Perplexity: 1194.670399
2025-10-18 02:06:19,761 Stage: Train 0.5 | Epoch: 113 | Iter: 344000 | Total Loss: 0.003577 | Recon Loss: 0.003056 | Commit Loss: 0.001042 | Perplexity: 1198.459863
2025-10-18 02:11:42,086 Stage: Train 0.5 | Epoch: 113 | Iter: 344200 | Total Loss: 0.003633 | Recon Loss: 0.003101 | Commit Loss: 0.001065 | Perplexity: 1200.752725
2025-10-18 02:17:04,873 Stage: Train 0.5 | Epoch: 113 | Iter: 344400 | Total Loss: 0.003561 | Recon Loss: 0.003048 | Commit Loss: 0.001026 | Perplexity: 1203.453683
2025-10-18 02:22:27,768 Stage: Train 0.5 | Epoch: 113 | Iter: 344600 | Total Loss: 0.003600 | Recon Loss: 0.003069 | Commit Loss: 0.001062 | Perplexity: 1200.021754
2025-10-18 02:27:50,639 Stage: Train 0.5 | Epoch: 113 | Iter: 344800 | Total Loss: 0.003614 | Recon Loss: 0.003091 | Commit Loss: 0.001047 | Perplexity: 1196.876818
2025-10-18 02:33:13,115 Stage: Train 0.5 | Epoch: 113 | Iter: 345000 | Total Loss: 0.003600 | Recon Loss: 0.003072 | Commit Loss: 0.001056 | Perplexity: 1194.154057
2025-10-18 02:38:34,255 Stage: Train 0.5 | Epoch: 113 | Iter: 345200 | Total Loss: 0.003596 | Recon Loss: 0.003070 | Commit Loss: 0.001052 | Perplexity: 1194.844510
2025-10-18 02:43:57,188 Stage: Train 0.5 | Epoch: 113 | Iter: 345400 | Total Loss: 0.003614 | Recon Loss: 0.003079 | Commit Loss: 0.001070 | Perplexity: 1195.191084
2025-10-18 02:49:19,909 Stage: Train 0.5 | Epoch: 113 | Iter: 345600 | Total Loss: 0.003665 | Recon Loss: 0.003139 | Commit Loss: 0.001052 | Perplexity: 1200.040228
2025-10-18 02:54:42,816 Stage: Train 0.5 | Epoch: 113 | Iter: 345800 | Total Loss: 0.003687 | Recon Loss: 0.003151 | Commit Loss: 0.001072 | Perplexity: 1189.265722
2025-10-18 03:00:05,487 Stage: Train 0.5 | Epoch: 113 | Iter: 346000 | Total Loss: 0.003599 | Recon Loss: 0.003072 | Commit Loss: 0.001053 | Perplexity: 1195.331667
2025-10-18 03:05:27,522 Stage: Train 0.5 | Epoch: 113 | Iter: 346200 | Total Loss: 0.003610 | Recon Loss: 0.003074 | Commit Loss: 0.001072 | Perplexity: 1198.061145
Trainning Epoch:  69%|██████▉   | 114/165 [155:43:58<69:30:33, 4906.54s/it]Trainning Epoch:  69%|██████▉   | 114/165 [155:43:58<69:30:33, 4906.54s/it]2025-10-18 03:10:53,532 Stage: Train 0.5 | Epoch: 114 | Iter: 346400 | Total Loss: 0.003584 | Recon Loss: 0.003059 | Commit Loss: 0.001049 | Perplexity: 1199.747598
2025-10-18 03:16:16,428 Stage: Train 0.5 | Epoch: 114 | Iter: 346600 | Total Loss: 0.003573 | Recon Loss: 0.003046 | Commit Loss: 0.001053 | Perplexity: 1202.481624
2025-10-18 03:21:39,393 Stage: Train 0.5 | Epoch: 114 | Iter: 346800 | Total Loss: 0.003600 | Recon Loss: 0.003078 | Commit Loss: 0.001044 | Perplexity: 1192.662140
2025-10-18 03:27:02,284 Stage: Train 0.5 | Epoch: 114 | Iter: 347000 | Total Loss: 0.003670 | Recon Loss: 0.003134 | Commit Loss: 0.001072 | Perplexity: 1194.827615
2025-10-18 03:32:24,867 Stage: Train 0.5 | Epoch: 114 | Iter: 347200 | Total Loss: 0.003543 | Recon Loss: 0.003029 | Commit Loss: 0.001029 | Perplexity: 1186.118062
2025-10-18 03:37:46,910 Stage: Train 0.5 | Epoch: 114 | Iter: 347400 | Total Loss: 0.003572 | Recon Loss: 0.003046 | Commit Loss: 0.001051 | Perplexity: 1194.227339
2025-10-18 03:43:08,806 Stage: Train 0.5 | Epoch: 114 | Iter: 347600 | Total Loss: 0.003612 | Recon Loss: 0.003083 | Commit Loss: 0.001059 | Perplexity: 1190.396052
2025-10-18 03:48:31,631 Stage: Train 0.5 | Epoch: 114 | Iter: 347800 | Total Loss: 0.003695 | Recon Loss: 0.003158 | Commit Loss: 0.001073 | Perplexity: 1198.253743
2025-10-18 03:53:53,782 Stage: Train 0.5 | Epoch: 114 | Iter: 348000 | Total Loss: 0.003578 | Recon Loss: 0.003054 | Commit Loss: 0.001048 | Perplexity: 1195.483041
2025-10-18 03:59:15,706 Stage: Train 0.5 | Epoch: 114 | Iter: 348200 | Total Loss: 0.003611 | Recon Loss: 0.003078 | Commit Loss: 0.001066 | Perplexity: 1194.931179
2025-10-18 04:04:38,619 Stage: Train 0.5 | Epoch: 114 | Iter: 348400 | Total Loss: 0.003627 | Recon Loss: 0.003097 | Commit Loss: 0.001061 | Perplexity: 1201.504165
2025-10-18 04:10:01,586 Stage: Train 0.5 | Epoch: 114 | Iter: 348600 | Total Loss: 0.003559 | Recon Loss: 0.003046 | Commit Loss: 0.001025 | Perplexity: 1199.165780
2025-10-18 04:15:24,110 Stage: Train 0.5 | Epoch: 114 | Iter: 348800 | Total Loss: 0.003626 | Recon Loss: 0.003114 | Commit Loss: 0.001025 | Perplexity: 1187.954452
2025-10-18 04:20:46,974 Stage: Train 0.5 | Epoch: 114 | Iter: 349000 | Total Loss: 0.003629 | Recon Loss: 0.003100 | Commit Loss: 0.001057 | Perplexity: 1189.707727
2025-10-18 04:26:09,824 Stage: Train 0.5 | Epoch: 114 | Iter: 349200 | Total Loss: 0.003564 | Recon Loss: 0.003040 | Commit Loss: 0.001048 | Perplexity: 1188.871487
Trainning Epoch:  70%|██████▉   | 115/165 [157:05:43<68:08:15, 4905.92s/it]Trainning Epoch:  70%|██████▉   | 115/165 [157:05:43<68:08:15, 4905.92s/it]2025-10-18 04:31:36,720 Stage: Train 0.5 | Epoch: 115 | Iter: 349400 | Total Loss: 0.003621 | Recon Loss: 0.003093 | Commit Loss: 0.001056 | Perplexity: 1198.713034
2025-10-18 04:36:59,591 Stage: Train 0.5 | Epoch: 115 | Iter: 349600 | Total Loss: 0.003604 | Recon Loss: 0.003084 | Commit Loss: 0.001041 | Perplexity: 1195.165597
2025-10-18 04:42:22,291 Stage: Train 0.5 | Epoch: 115 | Iter: 349800 | Total Loss: 0.003603 | Recon Loss: 0.003073 | Commit Loss: 0.001059 | Perplexity: 1198.534156
2025-10-18 04:47:44,860 Stage: Train 0.5 | Epoch: 115 | Iter: 350000 | Total Loss: 0.003507 | Recon Loss: 0.002990 | Commit Loss: 0.001035 | Perplexity: 1192.858890
2025-10-18 04:53:07,712 Stage: Train 0.5 | Epoch: 115 | Iter: 350200 | Total Loss: 0.003666 | Recon Loss: 0.003137 | Commit Loss: 0.001058 | Perplexity: 1195.643071
2025-10-18 04:58:30,131 Stage: Train 0.5 | Epoch: 115 | Iter: 350400 | Total Loss: 0.003578 | Recon Loss: 0.003061 | Commit Loss: 0.001035 | Perplexity: 1196.034705
2025-10-18 05:03:51,863 Stage: Train 0.5 | Epoch: 115 | Iter: 350600 | Total Loss: 0.003579 | Recon Loss: 0.003053 | Commit Loss: 0.001051 | Perplexity: 1195.407133
2025-10-18 05:09:14,555 Stage: Train 0.5 | Epoch: 115 | Iter: 350800 | Total Loss: 0.003537 | Recon Loss: 0.003013 | Commit Loss: 0.001048 | Perplexity: 1190.248193
2025-10-18 05:14:37,401 Stage: Train 0.5 | Epoch: 115 | Iter: 351000 | Total Loss: 0.003697 | Recon Loss: 0.003164 | Commit Loss: 0.001066 | Perplexity: 1199.900245
2025-10-18 05:20:00,142 Stage: Train 0.5 | Epoch: 115 | Iter: 351200 | Total Loss: 0.003622 | Recon Loss: 0.003100 | Commit Loss: 0.001044 | Perplexity: 1199.101738
2025-10-18 05:25:23,055 Stage: Train 0.5 | Epoch: 115 | Iter: 351400 | Total Loss: 0.003495 | Recon Loss: 0.002975 | Commit Loss: 0.001039 | Perplexity: 1199.335197
2025-10-18 05:30:46,099 Stage: Train 0.5 | Epoch: 115 | Iter: 351600 | Total Loss: 0.003614 | Recon Loss: 0.003084 | Commit Loss: 0.001060 | Perplexity: 1198.511882
2025-10-18 05:36:09,109 Stage: Train 0.5 | Epoch: 115 | Iter: 351800 | Total Loss: 0.003543 | Recon Loss: 0.003019 | Commit Loss: 0.001048 | Perplexity: 1200.535034
2025-10-18 05:41:32,010 Stage: Train 0.5 | Epoch: 115 | Iter: 352000 | Total Loss: 0.003583 | Recon Loss: 0.003049 | Commit Loss: 0.001068 | Perplexity: 1197.845367
2025-10-18 05:46:54,996 Stage: Train 0.5 | Epoch: 115 | Iter: 352200 | Total Loss: 0.003597 | Recon Loss: 0.003062 | Commit Loss: 0.001071 | Perplexity: 1194.886922
2025-10-18 05:52:17,569 Stage: Train 0.5 | Epoch: 115 | Iter: 352400 | Total Loss: 0.003632 | Recon Loss: 0.003109 | Commit Loss: 0.001046 | Perplexity: 1184.427897
Trainning Epoch:  70%|███████   | 116/165 [158:27:29<66:46:36, 4906.06s/it]Trainning Epoch:  70%|███████   | 116/165 [158:27:29<66:46:36, 4906.06s/it]2025-10-18 05:57:44,576 Stage: Train 0.5 | Epoch: 116 | Iter: 352600 | Total Loss: 0.003526 | Recon Loss: 0.003003 | Commit Loss: 0.001047 | Perplexity: 1199.152396
2025-10-18 06:03:07,388 Stage: Train 0.5 | Epoch: 116 | Iter: 352800 | Total Loss: 0.003573 | Recon Loss: 0.003051 | Commit Loss: 0.001044 | Perplexity: 1193.155685
2025-10-18 06:08:30,170 Stage: Train 0.5 | Epoch: 116 | Iter: 353000 | Total Loss: 0.003631 | Recon Loss: 0.003100 | Commit Loss: 0.001064 | Perplexity: 1200.484917
2025-10-18 06:13:53,154 Stage: Train 0.5 | Epoch: 116 | Iter: 353200 | Total Loss: 0.003622 | Recon Loss: 0.003093 | Commit Loss: 0.001059 | Perplexity: 1197.136358
2025-10-18 06:19:16,073 Stage: Train 0.5 | Epoch: 116 | Iter: 353400 | Total Loss: 0.003560 | Recon Loss: 0.003039 | Commit Loss: 0.001042 | Perplexity: 1201.413233
2025-10-18 06:24:38,821 Stage: Train 0.5 | Epoch: 116 | Iter: 353600 | Total Loss: 0.003602 | Recon Loss: 0.003085 | Commit Loss: 0.001034 | Perplexity: 1200.822517
2025-10-18 06:30:01,176 Stage: Train 0.5 | Epoch: 116 | Iter: 353800 | Total Loss: 0.003583 | Recon Loss: 0.003059 | Commit Loss: 0.001049 | Perplexity: 1194.594329
2025-10-18 06:35:24,092 Stage: Train 0.5 | Epoch: 116 | Iter: 354000 | Total Loss: 0.003585 | Recon Loss: 0.003055 | Commit Loss: 0.001061 | Perplexity: 1185.261020
2025-10-18 06:40:45,952 Stage: Train 0.5 | Epoch: 116 | Iter: 354200 | Total Loss: 0.003607 | Recon Loss: 0.003074 | Commit Loss: 0.001066 | Perplexity: 1197.834895
2025-10-18 06:46:07,893 Stage: Train 0.5 | Epoch: 116 | Iter: 354400 | Total Loss: 0.003606 | Recon Loss: 0.003081 | Commit Loss: 0.001051 | Perplexity: 1203.456528
2025-10-18 06:51:29,375 Stage: Train 0.5 | Epoch: 116 | Iter: 354600 | Total Loss: 0.003604 | Recon Loss: 0.003071 | Commit Loss: 0.001066 | Perplexity: 1204.161718
2025-10-18 06:56:51,614 Stage: Train 0.5 | Epoch: 116 | Iter: 354800 | Total Loss: 0.003547 | Recon Loss: 0.003022 | Commit Loss: 0.001050 | Perplexity: 1192.946376
2025-10-18 07:02:14,403 Stage: Train 0.5 | Epoch: 116 | Iter: 355000 | Total Loss: 0.003592 | Recon Loss: 0.003067 | Commit Loss: 0.001050 | Perplexity: 1205.436578
2025-10-18 07:07:37,516 Stage: Train 0.5 | Epoch: 116 | Iter: 355200 | Total Loss: 0.003584 | Recon Loss: 0.003062 | Commit Loss: 0.001044 | Perplexity: 1194.136561
2025-10-18 07:12:59,923 Stage: Train 0.5 | Epoch: 116 | Iter: 355400 | Total Loss: 0.003566 | Recon Loss: 0.003035 | Commit Loss: 0.001062 | Perplexity: 1193.883156
Trainning Epoch:  71%|███████   | 117/165 [159:49:13<65:24:14, 4905.30s/it]Trainning Epoch:  71%|███████   | 117/165 [159:49:13<65:24:14, 4905.30s/it]2025-10-18 07:18:26,735 Stage: Train 0.5 | Epoch: 117 | Iter: 355600 | Total Loss: 0.003602 | Recon Loss: 0.003080 | Commit Loss: 0.001044 | Perplexity: 1197.545347
2025-10-18 07:23:49,570 Stage: Train 0.5 | Epoch: 117 | Iter: 355800 | Total Loss: 0.003585 | Recon Loss: 0.003063 | Commit Loss: 0.001044 | Perplexity: 1201.912271
2025-10-18 07:29:12,462 Stage: Train 0.5 | Epoch: 117 | Iter: 356000 | Total Loss: 0.003642 | Recon Loss: 0.003129 | Commit Loss: 0.001027 | Perplexity: 1196.190282
2025-10-18 07:34:35,457 Stage: Train 0.5 | Epoch: 117 | Iter: 356200 | Total Loss: 0.003551 | Recon Loss: 0.003021 | Commit Loss: 0.001059 | Perplexity: 1197.036800
2025-10-18 07:39:58,570 Stage: Train 0.5 | Epoch: 117 | Iter: 356400 | Total Loss: 0.003605 | Recon Loss: 0.003081 | Commit Loss: 0.001049 | Perplexity: 1195.707815
2025-10-18 07:45:20,795 Stage: Train 0.5 | Epoch: 117 | Iter: 356600 | Total Loss: 0.003593 | Recon Loss: 0.003072 | Commit Loss: 0.001042 | Perplexity: 1188.633409
2025-10-18 07:50:43,088 Stage: Train 0.5 | Epoch: 117 | Iter: 356800 | Total Loss: 0.003602 | Recon Loss: 0.003081 | Commit Loss: 0.001043 | Perplexity: 1196.853966
2025-10-18 07:56:05,829 Stage: Train 0.5 | Epoch: 117 | Iter: 357000 | Total Loss: 0.003527 | Recon Loss: 0.002999 | Commit Loss: 0.001057 | Perplexity: 1199.406125
2025-10-18 08:01:28,175 Stage: Train 0.5 | Epoch: 117 | Iter: 357200 | Total Loss: 0.003637 | Recon Loss: 0.003109 | Commit Loss: 0.001056 | Perplexity: 1195.131236
2025-10-18 08:06:49,991 Stage: Train 0.5 | Epoch: 117 | Iter: 357400 | Total Loss: 0.003629 | Recon Loss: 0.003106 | Commit Loss: 0.001046 | Perplexity: 1199.482440
2025-10-18 08:12:12,851 Stage: Train 0.5 | Epoch: 117 | Iter: 357600 | Total Loss: 0.003478 | Recon Loss: 0.002956 | Commit Loss: 0.001044 | Perplexity: 1190.793727
2025-10-18 08:17:35,745 Stage: Train 0.5 | Epoch: 117 | Iter: 357800 | Total Loss: 0.003635 | Recon Loss: 0.003115 | Commit Loss: 0.001041 | Perplexity: 1196.571205
2025-10-18 08:22:58,504 Stage: Train 0.5 | Epoch: 117 | Iter: 358000 | Total Loss: 0.003566 | Recon Loss: 0.003041 | Commit Loss: 0.001051 | Perplexity: 1199.765497
2025-10-18 08:28:21,237 Stage: Train 0.5 | Epoch: 117 | Iter: 358200 | Total Loss: 0.003594 | Recon Loss: 0.003071 | Commit Loss: 0.001045 | Perplexity: 1203.885132
2025-10-18 08:33:44,024 Stage: Train 0.5 | Epoch: 117 | Iter: 358400 | Total Loss: 0.003604 | Recon Loss: 0.003082 | Commit Loss: 0.001045 | Perplexity: 1196.312280
Trainning Epoch:  72%|███████▏  | 118/165 [161:10:58<64:02:31, 4905.35s/it]Trainning Epoch:  72%|███████▏  | 118/165 [161:10:58<64:02:31, 4905.35s/it]2025-10-18 08:39:10,689 Stage: Train 0.5 | Epoch: 118 | Iter: 358600 | Total Loss: 0.003589 | Recon Loss: 0.003069 | Commit Loss: 0.001040 | Perplexity: 1194.782939
2025-10-18 08:44:33,628 Stage: Train 0.5 | Epoch: 118 | Iter: 358800 | Total Loss: 0.003489 | Recon Loss: 0.002967 | Commit Loss: 0.001045 | Perplexity: 1196.838610
2025-10-18 08:49:55,931 Stage: Train 0.5 | Epoch: 118 | Iter: 359000 | Total Loss: 0.003576 | Recon Loss: 0.003051 | Commit Loss: 0.001050 | Perplexity: 1207.214713
2025-10-18 08:55:18,698 Stage: Train 0.5 | Epoch: 118 | Iter: 359200 | Total Loss: 0.003725 | Recon Loss: 0.003195 | Commit Loss: 0.001060 | Perplexity: 1195.808459
2025-10-18 09:00:41,348 Stage: Train 0.5 | Epoch: 118 | Iter: 359400 | Total Loss: 0.003614 | Recon Loss: 0.003098 | Commit Loss: 0.001032 | Perplexity: 1191.696324
2025-10-18 09:06:03,393 Stage: Train 0.5 | Epoch: 118 | Iter: 359600 | Total Loss: 0.003562 | Recon Loss: 0.003032 | Commit Loss: 0.001059 | Perplexity: 1199.770579
2025-10-18 09:11:25,450 Stage: Train 0.5 | Epoch: 118 | Iter: 359800 | Total Loss: 0.003573 | Recon Loss: 0.003044 | Commit Loss: 0.001059 | Perplexity: 1207.233071
2025-10-18 09:16:47,927 Stage: Train 0.5 | Epoch: 118 | Iter: 360000 | Total Loss: 0.003567 | Recon Loss: 0.003048 | Commit Loss: 0.001038 | Perplexity: 1209.154997
2025-10-18 09:16:47,927 Saving model at iteration 360000
2025-10-18 09:16:48,084 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000
2025-10-18 09:16:49,489 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000/model.safetensors
2025-10-18 09:16:51,295 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000/optimizer.bin
2025-10-18 09:16:51,296 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000/scheduler.bin
2025-10-18 09:16:51,296 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000/sampler.bin
2025-10-18 09:16:51,297 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_119_step_360000/random_states_0.pkl
2025-10-18 09:22:14,541 Stage: Train 0.5 | Epoch: 118 | Iter: 360200 | Total Loss: 0.003629 | Recon Loss: 0.003105 | Commit Loss: 0.001047 | Perplexity: 1202.204346
2025-10-18 09:27:37,380 Stage: Train 0.5 | Epoch: 118 | Iter: 360400 | Total Loss: 0.003545 | Recon Loss: 0.003016 | Commit Loss: 0.001057 | Perplexity: 1194.910590
2025-10-18 09:32:59,946 Stage: Train 0.5 | Epoch: 118 | Iter: 360600 | Total Loss: 0.003582 | Recon Loss: 0.003053 | Commit Loss: 0.001057 | Perplexity: 1200.465502
2025-10-18 09:38:21,713 Stage: Train 0.5 | Epoch: 118 | Iter: 360800 | Total Loss: 0.003563 | Recon Loss: 0.003041 | Commit Loss: 0.001043 | Perplexity: 1194.434517
2025-10-18 09:43:43,976 Stage: Train 0.5 | Epoch: 118 | Iter: 361000 | Total Loss: 0.003540 | Recon Loss: 0.003014 | Commit Loss: 0.001051 | Perplexity: 1203.849086
2025-10-18 09:49:07,021 Stage: Train 0.5 | Epoch: 118 | Iter: 361200 | Total Loss: 0.003684 | Recon Loss: 0.003162 | Commit Loss: 0.001044 | Perplexity: 1197.202522
2025-10-18 09:54:29,942 Stage: Train 0.5 | Epoch: 118 | Iter: 361400 | Total Loss: 0.003580 | Recon Loss: 0.003056 | Commit Loss: 0.001048 | Perplexity: 1198.056161
Trainning Epoch:  72%|███████▏  | 119/165 [162:32:45<62:41:13, 4905.94s/it]Trainning Epoch:  72%|███████▏  | 119/165 [162:32:45<62:41:13, 4905.94s/it]2025-10-18 09:59:56,432 Stage: Train 0.5 | Epoch: 119 | Iter: 361600 | Total Loss: 0.003603 | Recon Loss: 0.003085 | Commit Loss: 0.001035 | Perplexity: 1197.441899
2025-10-18 10:05:19,428 Stage: Train 0.5 | Epoch: 119 | Iter: 361800 | Total Loss: 0.003532 | Recon Loss: 0.003016 | Commit Loss: 0.001033 | Perplexity: 1199.632644
2025-10-18 10:10:42,434 Stage: Train 0.5 | Epoch: 119 | Iter: 362000 | Total Loss: 0.003575 | Recon Loss: 0.003059 | Commit Loss: 0.001033 | Perplexity: 1198.158573
2025-10-18 10:16:05,195 Stage: Train 0.5 | Epoch: 119 | Iter: 362200 | Total Loss: 0.003580 | Recon Loss: 0.003054 | Commit Loss: 0.001051 | Perplexity: 1188.170549
2025-10-18 10:21:27,983 Stage: Train 0.5 | Epoch: 119 | Iter: 362400 | Total Loss: 0.003644 | Recon Loss: 0.003113 | Commit Loss: 0.001063 | Perplexity: 1202.470927
2025-10-18 10:26:50,735 Stage: Train 0.5 | Epoch: 119 | Iter: 362600 | Total Loss: 0.003681 | Recon Loss: 0.003133 | Commit Loss: 0.001097 | Perplexity: 1204.200942
2025-10-18 10:32:13,549 Stage: Train 0.5 | Epoch: 119 | Iter: 362800 | Total Loss: 0.003577 | Recon Loss: 0.003054 | Commit Loss: 0.001045 | Perplexity: 1195.829470
2025-10-18 10:37:36,387 Stage: Train 0.5 | Epoch: 119 | Iter: 363000 | Total Loss: 0.003651 | Recon Loss: 0.003126 | Commit Loss: 0.001050 | Perplexity: 1203.576111
2025-10-18 10:42:59,257 Stage: Train 0.5 | Epoch: 119 | Iter: 363200 | Total Loss: 0.003611 | Recon Loss: 0.003077 | Commit Loss: 0.001069 | Perplexity: 1195.110249
2025-10-18 10:48:21,629 Stage: Train 0.5 | Epoch: 119 | Iter: 363400 | Total Loss: 0.003562 | Recon Loss: 0.003042 | Commit Loss: 0.001041 | Perplexity: 1203.664899
2025-10-18 10:53:44,308 Stage: Train 0.5 | Epoch: 119 | Iter: 363600 | Total Loss: 0.003563 | Recon Loss: 0.003037 | Commit Loss: 0.001051 | Perplexity: 1198.553972
2025-10-18 10:59:07,207 Stage: Train 0.5 | Epoch: 119 | Iter: 363800 | Total Loss: 0.003646 | Recon Loss: 0.003111 | Commit Loss: 0.001069 | Perplexity: 1200.309163
2025-10-18 11:04:30,025 Stage: Train 0.5 | Epoch: 119 | Iter: 364000 | Total Loss: 0.003492 | Recon Loss: 0.002962 | Commit Loss: 0.001059 | Perplexity: 1201.775057
2025-10-18 11:09:52,775 Stage: Train 0.5 | Epoch: 119 | Iter: 364200 | Total Loss: 0.003575 | Recon Loss: 0.003048 | Commit Loss: 0.001054 | Perplexity: 1195.598427
2025-10-18 11:15:14,633 Stage: Train 0.5 | Epoch: 119 | Iter: 364400 | Total Loss: 0.003593 | Recon Loss: 0.003066 | Commit Loss: 0.001054 | Perplexity: 1193.953536
Trainning Epoch:  73%|███████▎  | 120/165 [163:54:31<61:19:26, 4905.91s/it]Trainning Epoch:  73%|███████▎  | 120/165 [163:54:31<61:19:26, 4905.91s/it]2025-10-18 11:20:41,363 Stage: Train 0.5 | Epoch: 120 | Iter: 364600 | Total Loss: 0.003609 | Recon Loss: 0.003085 | Commit Loss: 0.001049 | Perplexity: 1199.925660
2025-10-18 11:26:04,199 Stage: Train 0.5 | Epoch: 120 | Iter: 364800 | Total Loss: 0.003587 | Recon Loss: 0.003069 | Commit Loss: 0.001037 | Perplexity: 1194.216927
2025-10-18 11:31:25,525 Stage: Train 0.5 | Epoch: 120 | Iter: 365000 | Total Loss: 0.003503 | Recon Loss: 0.002983 | Commit Loss: 0.001039 | Perplexity: 1200.417869
2025-10-18 11:36:47,484 Stage: Train 0.5 | Epoch: 120 | Iter: 365200 | Total Loss: 0.003472 | Recon Loss: 0.002949 | Commit Loss: 0.001045 | Perplexity: 1198.571024
2025-10-18 11:42:10,567 Stage: Train 0.5 | Epoch: 120 | Iter: 365400 | Total Loss: 0.003595 | Recon Loss: 0.003068 | Commit Loss: 0.001052 | Perplexity: 1194.230479
2025-10-18 11:47:33,084 Stage: Train 0.5 | Epoch: 120 | Iter: 365600 | Total Loss: 0.003590 | Recon Loss: 0.003064 | Commit Loss: 0.001051 | Perplexity: 1203.632310
2025-10-18 11:52:56,131 Stage: Train 0.5 | Epoch: 120 | Iter: 365800 | Total Loss: 0.003584 | Recon Loss: 0.003058 | Commit Loss: 0.001051 | Perplexity: 1197.150361
2025-10-18 11:58:19,090 Stage: Train 0.5 | Epoch: 120 | Iter: 366000 | Total Loss: 0.003605 | Recon Loss: 0.003077 | Commit Loss: 0.001057 | Perplexity: 1202.196563
2025-10-18 12:03:41,833 Stage: Train 0.5 | Epoch: 120 | Iter: 366200 | Total Loss: 0.003602 | Recon Loss: 0.003074 | Commit Loss: 0.001056 | Perplexity: 1203.456752
2025-10-18 12:09:03,978 Stage: Train 0.5 | Epoch: 120 | Iter: 366400 | Total Loss: 0.003556 | Recon Loss: 0.003025 | Commit Loss: 0.001063 | Perplexity: 1200.290458
2025-10-18 12:14:26,028 Stage: Train 0.5 | Epoch: 120 | Iter: 366600 | Total Loss: 0.003540 | Recon Loss: 0.003023 | Commit Loss: 0.001035 | Perplexity: 1201.067213
2025-10-18 12:19:49,073 Stage: Train 0.5 | Epoch: 120 | Iter: 366800 | Total Loss: 0.003536 | Recon Loss: 0.003019 | Commit Loss: 0.001035 | Perplexity: 1199.716683
2025-10-18 12:25:11,936 Stage: Train 0.5 | Epoch: 120 | Iter: 367000 | Total Loss: 0.003576 | Recon Loss: 0.003049 | Commit Loss: 0.001055 | Perplexity: 1198.777462
2025-10-18 12:30:33,281 Stage: Train 0.5 | Epoch: 120 | Iter: 367200 | Total Loss: 0.003526 | Recon Loss: 0.003005 | Commit Loss: 0.001044 | Perplexity: 1202.190458
2025-10-18 12:35:56,244 Stage: Train 0.5 | Epoch: 120 | Iter: 367400 | Total Loss: 0.003609 | Recon Loss: 0.003078 | Commit Loss: 0.001062 | Perplexity: 1200.596324
Trainning Epoch:  73%|███████▎  | 121/165 [165:16:14<59:57:01, 4905.04s/it]Trainning Epoch:  73%|███████▎  | 121/165 [165:16:14<59:57:01, 4905.04s/it]2025-10-18 12:41:23,079 Stage: Train 0.5 | Epoch: 121 | Iter: 367600 | Total Loss: 0.003578 | Recon Loss: 0.003043 | Commit Loss: 0.001069 | Perplexity: 1202.970648
2025-10-18 12:46:46,126 Stage: Train 0.5 | Epoch: 121 | Iter: 367800 | Total Loss: 0.003576 | Recon Loss: 0.003048 | Commit Loss: 0.001056 | Perplexity: 1201.314027
2025-10-18 12:52:08,840 Stage: Train 0.5 | Epoch: 121 | Iter: 368000 | Total Loss: 0.003550 | Recon Loss: 0.003026 | Commit Loss: 0.001049 | Perplexity: 1201.986633
2025-10-18 12:57:31,733 Stage: Train 0.5 | Epoch: 121 | Iter: 368200 | Total Loss: 0.003556 | Recon Loss: 0.003037 | Commit Loss: 0.001037 | Perplexity: 1200.032563
2025-10-18 13:02:53,880 Stage: Train 0.5 | Epoch: 121 | Iter: 368400 | Total Loss: 0.003549 | Recon Loss: 0.003027 | Commit Loss: 0.001044 | Perplexity: 1195.194842
2025-10-18 13:08:16,601 Stage: Train 0.5 | Epoch: 121 | Iter: 368600 | Total Loss: 0.003654 | Recon Loss: 0.003127 | Commit Loss: 0.001054 | Perplexity: 1203.925659
2025-10-18 13:13:39,646 Stage: Train 0.5 | Epoch: 121 | Iter: 368800 | Total Loss: 0.003601 | Recon Loss: 0.003073 | Commit Loss: 0.001057 | Perplexity: 1196.515224
2025-10-18 13:19:02,567 Stage: Train 0.5 | Epoch: 121 | Iter: 369000 | Total Loss: 0.003556 | Recon Loss: 0.003028 | Commit Loss: 0.001056 | Perplexity: 1196.023792
2025-10-18 13:24:25,367 Stage: Train 0.5 | Epoch: 121 | Iter: 369200 | Total Loss: 0.003503 | Recon Loss: 0.002979 | Commit Loss: 0.001047 | Perplexity: 1201.646119
2025-10-18 13:29:48,115 Stage: Train 0.5 | Epoch: 121 | Iter: 369400 | Total Loss: 0.003671 | Recon Loss: 0.003146 | Commit Loss: 0.001049 | Perplexity: 1199.000196
2025-10-18 13:35:10,921 Stage: Train 0.5 | Epoch: 121 | Iter: 369600 | Total Loss: 0.003600 | Recon Loss: 0.003074 | Commit Loss: 0.001053 | Perplexity: 1204.229481
2025-10-18 13:40:33,487 Stage: Train 0.5 | Epoch: 121 | Iter: 369800 | Total Loss: 0.003574 | Recon Loss: 0.003047 | Commit Loss: 0.001053 | Perplexity: 1201.809305
2025-10-18 13:45:56,430 Stage: Train 0.5 | Epoch: 121 | Iter: 370000 | Total Loss: 0.003583 | Recon Loss: 0.003054 | Commit Loss: 0.001058 | Perplexity: 1201.342228
2025-10-18 13:51:19,388 Stage: Train 0.5 | Epoch: 121 | Iter: 370200 | Total Loss: 0.003586 | Recon Loss: 0.003053 | Commit Loss: 0.001066 | Perplexity: 1205.055945
2025-10-18 13:56:41,978 Stage: Train 0.5 | Epoch: 121 | Iter: 370400 | Total Loss: 0.003564 | Recon Loss: 0.003034 | Commit Loss: 0.001060 | Perplexity: 1197.807009
2025-10-18 14:02:04,841 Stage: Train 0.5 | Epoch: 121 | Iter: 370600 | Total Loss: 0.003528 | Recon Loss: 0.002994 | Commit Loss: 0.001067 | Perplexity: 1201.900164
Trainning Epoch:  74%|███████▍  | 122/165 [166:38:02<58:35:45, 4905.72s/it]Trainning Epoch:  74%|███████▍  | 122/165 [166:38:02<58:35:45, 4905.72s/it]2025-10-18 14:07:32,014 Stage: Train 0.5 | Epoch: 122 | Iter: 370800 | Total Loss: 0.003597 | Recon Loss: 0.003079 | Commit Loss: 0.001035 | Perplexity: 1194.538122
2025-10-18 14:12:54,899 Stage: Train 0.5 | Epoch: 122 | Iter: 371000 | Total Loss: 0.003584 | Recon Loss: 0.003056 | Commit Loss: 0.001055 | Perplexity: 1199.685895
2025-10-18 14:18:17,790 Stage: Train 0.5 | Epoch: 122 | Iter: 371200 | Total Loss: 0.003598 | Recon Loss: 0.003051 | Commit Loss: 0.001093 | Perplexity: 1209.144387
2025-10-18 14:23:40,757 Stage: Train 0.5 | Epoch: 122 | Iter: 371400 | Total Loss: 0.003552 | Recon Loss: 0.003027 | Commit Loss: 0.001051 | Perplexity: 1202.288416
2025-10-18 14:29:03,872 Stage: Train 0.5 | Epoch: 122 | Iter: 371600 | Total Loss: 0.003577 | Recon Loss: 0.003059 | Commit Loss: 0.001037 | Perplexity: 1199.876489
2025-10-18 14:34:26,940 Stage: Train 0.5 | Epoch: 122 | Iter: 371800 | Total Loss: 0.003492 | Recon Loss: 0.002973 | Commit Loss: 0.001037 | Perplexity: 1197.259763
2025-10-18 14:39:49,790 Stage: Train 0.5 | Epoch: 122 | Iter: 372000 | Total Loss: 0.003569 | Recon Loss: 0.003036 | Commit Loss: 0.001067 | Perplexity: 1199.283239
2025-10-18 14:45:12,628 Stage: Train 0.5 | Epoch: 122 | Iter: 372200 | Total Loss: 0.003532 | Recon Loss: 0.003004 | Commit Loss: 0.001056 | Perplexity: 1192.655601
2025-10-18 14:50:35,460 Stage: Train 0.5 | Epoch: 122 | Iter: 372400 | Total Loss: 0.003618 | Recon Loss: 0.003093 | Commit Loss: 0.001051 | Perplexity: 1192.744702
2025-10-18 14:55:58,519 Stage: Train 0.5 | Epoch: 122 | Iter: 372600 | Total Loss: 0.003556 | Recon Loss: 0.003035 | Commit Loss: 0.001042 | Perplexity: 1201.808919
2025-10-18 15:01:21,500 Stage: Train 0.5 | Epoch: 122 | Iter: 372800 | Total Loss: 0.003559 | Recon Loss: 0.003034 | Commit Loss: 0.001050 | Perplexity: 1197.384068
2025-10-18 15:06:44,102 Stage: Train 0.5 | Epoch: 122 | Iter: 373000 | Total Loss: 0.003646 | Recon Loss: 0.003115 | Commit Loss: 0.001063 | Perplexity: 1201.439176
2025-10-18 15:12:07,045 Stage: Train 0.5 | Epoch: 122 | Iter: 373200 | Total Loss: 0.003494 | Recon Loss: 0.002979 | Commit Loss: 0.001030 | Perplexity: 1191.625123
2025-10-18 15:17:30,073 Stage: Train 0.5 | Epoch: 122 | Iter: 373400 | Total Loss: 0.003599 | Recon Loss: 0.003072 | Commit Loss: 0.001055 | Perplexity: 1200.079870
2025-10-18 15:22:53,141 Stage: Train 0.5 | Epoch: 122 | Iter: 373600 | Total Loss: 0.003483 | Recon Loss: 0.002971 | Commit Loss: 0.001023 | Perplexity: 1195.930215
Trainning Epoch:  75%|███████▍  | 123/165 [167:59:51<57:14:50, 4906.91s/it]Trainning Epoch:  75%|███████▍  | 123/165 [167:59:51<57:14:50, 4906.91s/it]2025-10-18 15:28:19,961 Stage: Train 0.5 | Epoch: 123 | Iter: 373800 | Total Loss: 0.003557 | Recon Loss: 0.003037 | Commit Loss: 0.001040 | Perplexity: 1199.344511
2025-10-18 15:33:41,582 Stage: Train 0.5 | Epoch: 123 | Iter: 374000 | Total Loss: 0.003522 | Recon Loss: 0.002996 | Commit Loss: 0.001051 | Perplexity: 1200.275728
2025-10-18 15:39:03,998 Stage: Train 0.5 | Epoch: 123 | Iter: 374200 | Total Loss: 0.003539 | Recon Loss: 0.003016 | Commit Loss: 0.001046 | Perplexity: 1189.846743
2025-10-18 15:44:25,755 Stage: Train 0.5 | Epoch: 123 | Iter: 374400 | Total Loss: 0.003518 | Recon Loss: 0.002995 | Commit Loss: 0.001045 | Perplexity: 1204.320050
2025-10-18 15:49:47,515 Stage: Train 0.5 | Epoch: 123 | Iter: 374600 | Total Loss: 0.003501 | Recon Loss: 0.002974 | Commit Loss: 0.001054 | Perplexity: 1201.922327
2025-10-18 15:55:10,543 Stage: Train 0.5 | Epoch: 123 | Iter: 374800 | Total Loss: 0.003534 | Recon Loss: 0.003008 | Commit Loss: 0.001050 | Perplexity: 1196.978928
2025-10-18 16:00:33,278 Stage: Train 0.5 | Epoch: 123 | Iter: 375000 | Total Loss: 0.003535 | Recon Loss: 0.003003 | Commit Loss: 0.001064 | Perplexity: 1205.856137
2025-10-18 16:05:54,826 Stage: Train 0.5 | Epoch: 123 | Iter: 375200 | Total Loss: 0.003534 | Recon Loss: 0.003006 | Commit Loss: 0.001058 | Perplexity: 1202.165155
2025-10-18 16:11:16,779 Stage: Train 0.5 | Epoch: 123 | Iter: 375400 | Total Loss: 0.003741 | Recon Loss: 0.003160 | Commit Loss: 0.001161 | Perplexity: 1200.872583
2025-10-18 16:16:38,794 Stage: Train 0.5 | Epoch: 123 | Iter: 375600 | Total Loss: 0.003461 | Recon Loss: 0.002949 | Commit Loss: 0.001025 | Perplexity: 1188.112161
2025-10-18 16:22:01,800 Stage: Train 0.5 | Epoch: 123 | Iter: 375800 | Total Loss: 0.003540 | Recon Loss: 0.003009 | Commit Loss: 0.001063 | Perplexity: 1196.960453
2025-10-18 16:27:24,418 Stage: Train 0.5 | Epoch: 123 | Iter: 376000 | Total Loss: 0.003509 | Recon Loss: 0.002988 | Commit Loss: 0.001044 | Perplexity: 1198.929395
2025-10-18 16:32:47,340 Stage: Train 0.5 | Epoch: 123 | Iter: 376200 | Total Loss: 0.003533 | Recon Loss: 0.003001 | Commit Loss: 0.001065 | Perplexity: 1203.302026
2025-10-18 16:38:10,517 Stage: Train 0.5 | Epoch: 123 | Iter: 376400 | Total Loss: 0.003632 | Recon Loss: 0.003092 | Commit Loss: 0.001080 | Perplexity: 1199.409264
2025-10-18 16:43:33,465 Stage: Train 0.5 | Epoch: 123 | Iter: 376600 | Total Loss: 0.003548 | Recon Loss: 0.003031 | Commit Loss: 0.001036 | Perplexity: 1194.153788
Trainning Epoch:  75%|███████▌  | 124/165 [169:21:32<55:51:45, 4905.01s/it]Trainning Epoch:  75%|███████▌  | 124/165 [169:21:32<55:51:45, 4905.00s/it]2025-10-18 16:48:59,117 Stage: Train 0.5 | Epoch: 124 | Iter: 376800 | Total Loss: 0.003507 | Recon Loss: 0.002989 | Commit Loss: 0.001034 | Perplexity: 1196.642628
2025-10-18 16:54:22,014 Stage: Train 0.5 | Epoch: 124 | Iter: 377000 | Total Loss: 0.003573 | Recon Loss: 0.003038 | Commit Loss: 0.001070 | Perplexity: 1198.262474
2025-10-18 16:59:44,634 Stage: Train 0.5 | Epoch: 124 | Iter: 377200 | Total Loss: 0.003504 | Recon Loss: 0.002980 | Commit Loss: 0.001048 | Perplexity: 1201.095122
2025-10-18 17:05:06,274 Stage: Train 0.5 | Epoch: 124 | Iter: 377400 | Total Loss: 0.003547 | Recon Loss: 0.003018 | Commit Loss: 0.001058 | Perplexity: 1201.530443
2025-10-18 17:10:28,021 Stage: Train 0.5 | Epoch: 124 | Iter: 377600 | Total Loss: 0.003533 | Recon Loss: 0.003015 | Commit Loss: 0.001036 | Perplexity: 1198.837341
2025-10-18 17:15:50,729 Stage: Train 0.5 | Epoch: 124 | Iter: 377800 | Total Loss: 0.003541 | Recon Loss: 0.003010 | Commit Loss: 0.001062 | Perplexity: 1196.979550
2025-10-18 17:21:12,003 Stage: Train 0.5 | Epoch: 124 | Iter: 378000 | Total Loss: 0.003524 | Recon Loss: 0.002996 | Commit Loss: 0.001056 | Perplexity: 1193.699683
2025-10-18 17:26:33,229 Stage: Train 0.5 | Epoch: 124 | Iter: 378200 | Total Loss: 0.003614 | Recon Loss: 0.003091 | Commit Loss: 0.001047 | Perplexity: 1192.962499
2025-10-18 17:31:55,505 Stage: Train 0.5 | Epoch: 124 | Iter: 378400 | Total Loss: 0.003512 | Recon Loss: 0.002985 | Commit Loss: 0.001054 | Perplexity: 1203.393722
2025-10-18 17:37:18,338 Stage: Train 0.5 | Epoch: 124 | Iter: 378600 | Total Loss: 0.003553 | Recon Loss: 0.003034 | Commit Loss: 0.001036 | Perplexity: 1194.852352
2025-10-18 17:42:39,953 Stage: Train 0.5 | Epoch: 124 | Iter: 378800 | Total Loss: 0.003718 | Recon Loss: 0.003167 | Commit Loss: 0.001104 | Perplexity: 1206.739441
2025-10-18 17:48:02,536 Stage: Train 0.5 | Epoch: 124 | Iter: 379000 | Total Loss: 0.003923 | Recon Loss: 0.003286 | Commit Loss: 0.001274 | Perplexity: 1190.505465
2025-10-18 17:53:24,499 Stage: Train 0.5 | Epoch: 124 | Iter: 379200 | Total Loss: 0.003499 | Recon Loss: 0.002981 | Commit Loss: 0.001037 | Perplexity: 1200.066614
2025-10-18 17:58:45,879 Stage: Train 0.5 | Epoch: 124 | Iter: 379400 | Total Loss: 0.003512 | Recon Loss: 0.003002 | Commit Loss: 0.001020 | Perplexity: 1194.080511
2025-10-18 18:04:07,032 Stage: Train 0.5 | Epoch: 124 | Iter: 379600 | Total Loss: 0.003595 | Recon Loss: 0.003075 | Commit Loss: 0.001040 | Perplexity: 1187.461611
Trainning Epoch:  76%|███████▌  | 125/165 [170:43:07<54:28:08, 4902.20s/it]Trainning Epoch:  76%|███████▌  | 125/165 [170:43:08<54:28:08, 4902.20s/it]2025-10-18 18:09:33,880 Stage: Train 0.5 | Epoch: 125 | Iter: 379800 | Total Loss: 0.003582 | Recon Loss: 0.003053 | Commit Loss: 0.001058 | Perplexity: 1195.077146
2025-10-18 18:14:57,173 Stage: Train 0.5 | Epoch: 125 | Iter: 380000 | Total Loss: 0.003639 | Recon Loss: 0.003120 | Commit Loss: 0.001037 | Perplexity: 1193.269941
2025-10-18 18:14:57,173 Saving model at iteration 380000
2025-10-18 18:14:57,350 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000
2025-10-18 18:14:58,700 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000/model.safetensors
2025-10-18 18:15:00,403 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000/optimizer.bin
2025-10-18 18:15:00,404 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000/scheduler.bin
2025-10-18 18:15:00,404 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000/sampler.bin
2025-10-18 18:15:00,405 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_126_step_380000/random_states_0.pkl
2025-10-18 18:20:23,328 Stage: Train 0.5 | Epoch: 125 | Iter: 380200 | Total Loss: 0.003475 | Recon Loss: 0.002964 | Commit Loss: 0.001022 | Perplexity: 1204.651661
2025-10-18 18:25:46,151 Stage: Train 0.5 | Epoch: 125 | Iter: 380400 | Total Loss: 0.003590 | Recon Loss: 0.003065 | Commit Loss: 0.001049 | Perplexity: 1195.750396
2025-10-18 18:31:08,111 Stage: Train 0.5 | Epoch: 125 | Iter: 380600 | Total Loss: 0.003496 | Recon Loss: 0.002978 | Commit Loss: 0.001036 | Perplexity: 1197.637758
2025-10-18 18:36:30,019 Stage: Train 0.5 | Epoch: 125 | Iter: 380800 | Total Loss: 0.003525 | Recon Loss: 0.003006 | Commit Loss: 0.001038 | Perplexity: 1200.025470
2025-10-18 18:41:52,149 Stage: Train 0.5 | Epoch: 125 | Iter: 381000 | Total Loss: 0.003524 | Recon Loss: 0.003006 | Commit Loss: 0.001037 | Perplexity: 1194.570921
2025-10-18 18:47:14,952 Stage: Train 0.5 | Epoch: 125 | Iter: 381200 | Total Loss: 0.003652 | Recon Loss: 0.003126 | Commit Loss: 0.001051 | Perplexity: 1196.774637
2025-10-18 18:52:37,741 Stage: Train 0.5 | Epoch: 125 | Iter: 381400 | Total Loss: 0.003506 | Recon Loss: 0.002985 | Commit Loss: 0.001040 | Perplexity: 1190.127979
2025-10-18 18:58:00,434 Stage: Train 0.5 | Epoch: 125 | Iter: 381600 | Total Loss: 0.003496 | Recon Loss: 0.002970 | Commit Loss: 0.001053 | Perplexity: 1206.553354
2025-10-18 19:03:23,365 Stage: Train 0.5 | Epoch: 125 | Iter: 381800 | Total Loss: 0.003583 | Recon Loss: 0.003060 | Commit Loss: 0.001047 | Perplexity: 1195.268009
2025-10-18 19:08:45,550 Stage: Train 0.5 | Epoch: 125 | Iter: 382000 | Total Loss: 0.003542 | Recon Loss: 0.003010 | Commit Loss: 0.001064 | Perplexity: 1207.124453
2025-10-18 19:14:06,898 Stage: Train 0.5 | Epoch: 125 | Iter: 382200 | Total Loss: 0.003558 | Recon Loss: 0.003036 | Commit Loss: 0.001044 | Perplexity: 1194.625810
2025-10-18 19:19:28,824 Stage: Train 0.5 | Epoch: 125 | Iter: 382400 | Total Loss: 0.003489 | Recon Loss: 0.002960 | Commit Loss: 0.001058 | Perplexity: 1199.485870
2025-10-18 19:24:50,709 Stage: Train 0.5 | Epoch: 125 | Iter: 382600 | Total Loss: 0.003495 | Recon Loss: 0.002978 | Commit Loss: 0.001034 | Perplexity: 1198.634106
Trainning Epoch:  76%|███████▋  | 126/165 [172:04:53<53:07:02, 4903.14s/it]Trainning Epoch:  76%|███████▋  | 126/165 [172:04:53<53:07:02, 4903.15s/it]2025-10-18 19:30:18,031 Stage: Train 0.5 | Epoch: 126 | Iter: 382800 | Total Loss: 0.003602 | Recon Loss: 0.003074 | Commit Loss: 0.001057 | Perplexity: 1199.668683
2025-10-18 19:35:40,536 Stage: Train 0.5 | Epoch: 126 | Iter: 383000 | Total Loss: 0.003526 | Recon Loss: 0.003001 | Commit Loss: 0.001051 | Perplexity: 1199.515076
2025-10-18 19:41:02,868 Stage: Train 0.5 | Epoch: 126 | Iter: 383200 | Total Loss: 0.003471 | Recon Loss: 0.002963 | Commit Loss: 0.001015 | Perplexity: 1203.495928
2025-10-18 19:46:24,202 Stage: Train 0.5 | Epoch: 126 | Iter: 383400 | Total Loss: 0.003721 | Recon Loss: 0.003167 | Commit Loss: 0.001107 | Perplexity: 1204.809486
2025-10-18 19:51:45,762 Stage: Train 0.5 | Epoch: 126 | Iter: 383600 | Total Loss: 0.003629 | Recon Loss: 0.003091 | Commit Loss: 0.001077 | Perplexity: 1201.406667
2025-10-18 19:57:08,053 Stage: Train 0.5 | Epoch: 126 | Iter: 383800 | Total Loss: 0.003533 | Recon Loss: 0.003021 | Commit Loss: 0.001025 | Perplexity: 1196.775913
2025-10-18 20:02:29,947 Stage: Train 0.5 | Epoch: 126 | Iter: 384000 | Total Loss: 0.003483 | Recon Loss: 0.002957 | Commit Loss: 0.001052 | Perplexity: 1196.307768
2025-10-18 20:07:51,912 Stage: Train 0.5 | Epoch: 126 | Iter: 384200 | Total Loss: 0.003525 | Recon Loss: 0.002994 | Commit Loss: 0.001063 | Perplexity: 1193.920175
2025-10-18 20:13:13,968 Stage: Train 0.5 | Epoch: 126 | Iter: 384400 | Total Loss: 0.003559 | Recon Loss: 0.003035 | Commit Loss: 0.001047 | Perplexity: 1199.045073
2025-10-18 20:18:35,659 Stage: Train 0.5 | Epoch: 126 | Iter: 384600 | Total Loss: 0.003532 | Recon Loss: 0.003010 | Commit Loss: 0.001045 | Perplexity: 1196.195219
2025-10-18 20:23:57,312 Stage: Train 0.5 | Epoch: 126 | Iter: 384800 | Total Loss: 0.003530 | Recon Loss: 0.003017 | Commit Loss: 0.001027 | Perplexity: 1199.466180
2025-10-18 20:29:19,936 Stage: Train 0.5 | Epoch: 126 | Iter: 385000 | Total Loss: 0.003438 | Recon Loss: 0.002929 | Commit Loss: 0.001019 | Perplexity: 1198.548196
2025-10-18 20:34:43,086 Stage: Train 0.5 | Epoch: 126 | Iter: 385200 | Total Loss: 0.003552 | Recon Loss: 0.003033 | Commit Loss: 0.001038 | Perplexity: 1201.074694
2025-10-18 20:40:06,057 Stage: Train 0.5 | Epoch: 126 | Iter: 385400 | Total Loss: 0.003471 | Recon Loss: 0.002952 | Commit Loss: 0.001038 | Perplexity: 1202.009719
2025-10-18 20:45:28,686 Stage: Train 0.5 | Epoch: 126 | Iter: 385600 | Total Loss: 0.003503 | Recon Loss: 0.002982 | Commit Loss: 0.001041 | Perplexity: 1196.629332
2025-10-18 20:50:51,445 Stage: Train 0.5 | Epoch: 126 | Iter: 385800 | Total Loss: 0.003523 | Recon Loss: 0.003002 | Commit Loss: 0.001042 | Perplexity: 1193.133073
Trainning Epoch:  77%|███████▋  | 127/165 [173:26:32<51:44:34, 4901.95s/it]Trainning Epoch:  77%|███████▋  | 127/165 [173:26:32<51:44:34, 4901.95s/it]2025-10-18 20:56:18,495 Stage: Train 0.5 | Epoch: 127 | Iter: 386000 | Total Loss: 0.003494 | Recon Loss: 0.002975 | Commit Loss: 0.001037 | Perplexity: 1199.704466
2025-10-18 21:01:41,018 Stage: Train 0.5 | Epoch: 127 | Iter: 386200 | Total Loss: 0.003600 | Recon Loss: 0.003085 | Commit Loss: 0.001029 | Perplexity: 1189.537070
2025-10-18 21:07:03,340 Stage: Train 0.5 | Epoch: 127 | Iter: 386400 | Total Loss: 0.003477 | Recon Loss: 0.002960 | Commit Loss: 0.001032 | Perplexity: 1201.783685
2025-10-18 21:12:25,755 Stage: Train 0.5 | Epoch: 127 | Iter: 386600 | Total Loss: 0.003520 | Recon Loss: 0.002997 | Commit Loss: 0.001045 | Perplexity: 1209.699022
2025-10-18 21:17:48,341 Stage: Train 0.5 | Epoch: 127 | Iter: 386800 | Total Loss: 0.003487 | Recon Loss: 0.002974 | Commit Loss: 0.001026 | Perplexity: 1197.296989
2025-10-18 21:23:11,143 Stage: Train 0.5 | Epoch: 127 | Iter: 387000 | Total Loss: 0.003502 | Recon Loss: 0.002982 | Commit Loss: 0.001040 | Perplexity: 1196.823655
2025-10-18 21:28:34,318 Stage: Train 0.5 | Epoch: 127 | Iter: 387200 | Total Loss: 0.003569 | Recon Loss: 0.003049 | Commit Loss: 0.001039 | Perplexity: 1198.921554
2025-10-18 21:33:56,679 Stage: Train 0.5 | Epoch: 127 | Iter: 387400 | Total Loss: 0.003476 | Recon Loss: 0.002957 | Commit Loss: 0.001039 | Perplexity: 1203.710794
2025-10-18 21:39:19,019 Stage: Train 0.5 | Epoch: 127 | Iter: 387600 | Total Loss: 0.003510 | Recon Loss: 0.002987 | Commit Loss: 0.001047 | Perplexity: 1204.178423
2025-10-18 21:44:41,847 Stage: Train 0.5 | Epoch: 127 | Iter: 387800 | Total Loss: 0.003594 | Recon Loss: 0.003077 | Commit Loss: 0.001034 | Perplexity: 1207.488796
2025-10-18 21:50:04,549 Stage: Train 0.5 | Epoch: 127 | Iter: 388000 | Total Loss: 0.003556 | Recon Loss: 0.003035 | Commit Loss: 0.001043 | Perplexity: 1201.000135
2025-10-18 21:55:26,551 Stage: Train 0.5 | Epoch: 127 | Iter: 388200 | Total Loss: 0.003519 | Recon Loss: 0.002995 | Commit Loss: 0.001047 | Perplexity: 1195.654115
2025-10-18 22:00:49,584 Stage: Train 0.5 | Epoch: 127 | Iter: 388400 | Total Loss: 0.003566 | Recon Loss: 0.003040 | Commit Loss: 0.001051 | Perplexity: 1200.254750
2025-10-18 22:06:12,511 Stage: Train 0.5 | Epoch: 127 | Iter: 388600 | Total Loss: 0.003546 | Recon Loss: 0.003015 | Commit Loss: 0.001063 | Perplexity: 1192.559244
2025-10-18 22:11:35,462 Stage: Train 0.5 | Epoch: 127 | Iter: 388800 | Total Loss: 0.003573 | Recon Loss: 0.003047 | Commit Loss: 0.001054 | Perplexity: 1204.312266
Trainning Epoch:  78%|███████▊  | 128/165 [174:48:17<50:23:29, 4902.96s/it]Trainning Epoch:  78%|███████▊  | 128/165 [174:48:17<50:23:29, 4902.96s/it]2025-10-18 22:17:03,037 Stage: Train 0.5 | Epoch: 128 | Iter: 389000 | Total Loss: 0.003439 | Recon Loss: 0.002921 | Commit Loss: 0.001036 | Perplexity: 1199.922072
2025-10-18 22:22:25,867 Stage: Train 0.5 | Epoch: 128 | Iter: 389200 | Total Loss: 0.003577 | Recon Loss: 0.003055 | Commit Loss: 0.001043 | Perplexity: 1194.816809
2025-10-18 22:27:48,330 Stage: Train 0.5 | Epoch: 128 | Iter: 389400 | Total Loss: 0.003448 | Recon Loss: 0.002924 | Commit Loss: 0.001048 | Perplexity: 1201.986736
2025-10-18 22:33:11,153 Stage: Train 0.5 | Epoch: 128 | Iter: 389600 | Total Loss: 0.003513 | Recon Loss: 0.002991 | Commit Loss: 0.001044 | Perplexity: 1187.755046
2025-10-18 22:38:34,362 Stage: Train 0.5 | Epoch: 128 | Iter: 389800 | Total Loss: 0.003538 | Recon Loss: 0.003024 | Commit Loss: 0.001029 | Perplexity: 1194.039979
2025-10-18 22:43:57,485 Stage: Train 0.5 | Epoch: 128 | Iter: 390000 | Total Loss: 0.003464 | Recon Loss: 0.002951 | Commit Loss: 0.001027 | Perplexity: 1199.477599
2025-10-18 22:49:20,319 Stage: Train 0.5 | Epoch: 128 | Iter: 390200 | Total Loss: 0.003508 | Recon Loss: 0.002987 | Commit Loss: 0.001043 | Perplexity: 1195.108542
2025-10-18 22:54:43,183 Stage: Train 0.5 | Epoch: 128 | Iter: 390400 | Total Loss: 0.003515 | Recon Loss: 0.002996 | Commit Loss: 0.001038 | Perplexity: 1200.348674
2025-10-18 23:00:05,999 Stage: Train 0.5 | Epoch: 128 | Iter: 390600 | Total Loss: 0.003483 | Recon Loss: 0.002959 | Commit Loss: 0.001047 | Perplexity: 1201.657845
2025-10-18 23:05:28,461 Stage: Train 0.5 | Epoch: 128 | Iter: 390800 | Total Loss: 0.003520 | Recon Loss: 0.003005 | Commit Loss: 0.001031 | Perplexity: 1195.830941
2025-10-18 23:10:51,058 Stage: Train 0.5 | Epoch: 128 | Iter: 391000 | Total Loss: 0.003571 | Recon Loss: 0.003037 | Commit Loss: 0.001068 | Perplexity: 1199.410486
2025-10-18 23:16:13,174 Stage: Train 0.5 | Epoch: 128 | Iter: 391200 | Total Loss: 0.003502 | Recon Loss: 0.002976 | Commit Loss: 0.001053 | Perplexity: 1194.892740
2025-10-18 23:21:35,853 Stage: Train 0.5 | Epoch: 128 | Iter: 391400 | Total Loss: 0.003530 | Recon Loss: 0.003003 | Commit Loss: 0.001055 | Perplexity: 1189.854280
2025-10-18 23:26:58,043 Stage: Train 0.5 | Epoch: 128 | Iter: 391600 | Total Loss: 0.003496 | Recon Loss: 0.002966 | Commit Loss: 0.001060 | Perplexity: 1205.210812
2025-10-18 23:32:20,799 Stage: Train 0.5 | Epoch: 128 | Iter: 391800 | Total Loss: 0.003582 | Recon Loss: 0.003045 | Commit Loss: 0.001075 | Perplexity: 1200.103785
Trainning Epoch:  78%|███████▊  | 129/165 [176:10:04<49:02:25, 4904.06s/it]Trainning Epoch:  78%|███████▊  | 129/165 [176:10:04<49:02:25, 4904.06s/it]2025-10-18 23:37:47,743 Stage: Train 0.5 | Epoch: 129 | Iter: 392000 | Total Loss: 0.003508 | Recon Loss: 0.002980 | Commit Loss: 0.001056 | Perplexity: 1199.912667
2025-10-18 23:43:09,888 Stage: Train 0.5 | Epoch: 129 | Iter: 392200 | Total Loss: 0.003564 | Recon Loss: 0.003040 | Commit Loss: 0.001049 | Perplexity: 1198.155153
2025-10-18 23:48:32,694 Stage: Train 0.5 | Epoch: 129 | Iter: 392400 | Total Loss: 0.003560 | Recon Loss: 0.003034 | Commit Loss: 0.001053 | Perplexity: 1200.911133
2025-10-18 23:53:55,398 Stage: Train 0.5 | Epoch: 129 | Iter: 392600 | Total Loss: 0.003471 | Recon Loss: 0.002942 | Commit Loss: 0.001058 | Perplexity: 1202.573544
2025-10-18 23:59:17,833 Stage: Train 0.5 | Epoch: 129 | Iter: 392800 | Total Loss: 0.003475 | Recon Loss: 0.002947 | Commit Loss: 0.001055 | Perplexity: 1207.337628
2025-10-19 00:04:40,360 Stage: Train 0.5 | Epoch: 129 | Iter: 393000 | Total Loss: 0.003507 | Recon Loss: 0.002983 | Commit Loss: 0.001047 | Perplexity: 1210.789687
2025-10-19 00:10:02,657 Stage: Train 0.5 | Epoch: 129 | Iter: 393200 | Total Loss: 0.003546 | Recon Loss: 0.003018 | Commit Loss: 0.001055 | Perplexity: 1201.351627
2025-10-19 00:15:25,055 Stage: Train 0.5 | Epoch: 129 | Iter: 393400 | Total Loss: 0.003507 | Recon Loss: 0.002981 | Commit Loss: 0.001053 | Perplexity: 1198.859946
2025-10-19 00:20:47,943 Stage: Train 0.5 | Epoch: 129 | Iter: 393600 | Total Loss: 0.003530 | Recon Loss: 0.003001 | Commit Loss: 0.001057 | Perplexity: 1200.291466
2025-10-19 00:26:10,942 Stage: Train 0.5 | Epoch: 129 | Iter: 393800 | Total Loss: 0.003531 | Recon Loss: 0.003003 | Commit Loss: 0.001057 | Perplexity: 1198.063807
2025-10-19 00:31:33,760 Stage: Train 0.5 | Epoch: 129 | Iter: 394000 | Total Loss: 0.003510 | Recon Loss: 0.002987 | Commit Loss: 0.001047 | Perplexity: 1201.101025
2025-10-19 00:36:55,359 Stage: Train 0.5 | Epoch: 129 | Iter: 394200 | Total Loss: 0.003508 | Recon Loss: 0.002991 | Commit Loss: 0.001034 | Perplexity: 1195.825307
2025-10-19 00:42:18,000 Stage: Train 0.5 | Epoch: 129 | Iter: 394400 | Total Loss: 0.003534 | Recon Loss: 0.003001 | Commit Loss: 0.001064 | Perplexity: 1211.225873
2025-10-19 00:47:40,441 Stage: Train 0.5 | Epoch: 129 | Iter: 394600 | Total Loss: 0.003527 | Recon Loss: 0.003002 | Commit Loss: 0.001051 | Perplexity: 1199.330627
2025-10-19 00:53:03,490 Stage: Train 0.5 | Epoch: 129 | Iter: 394800 | Total Loss: 0.003516 | Recon Loss: 0.002983 | Commit Loss: 0.001066 | Perplexity: 1202.768819
Trainning Epoch:  79%|███████▉  | 130/165 [177:31:48<47:40:42, 4904.07s/it]Trainning Epoch:  79%|███████▉  | 130/165 [177:31:48<47:40:42, 4904.07s/it]2025-10-19 00:58:30,652 Stage: Train 0.5 | Epoch: 130 | Iter: 395000 | Total Loss: 0.003533 | Recon Loss: 0.003007 | Commit Loss: 0.001052 | Perplexity: 1204.631090
2025-10-19 01:03:53,568 Stage: Train 0.5 | Epoch: 130 | Iter: 395200 | Total Loss: 0.003493 | Recon Loss: 0.002972 | Commit Loss: 0.001044 | Perplexity: 1201.991786
2025-10-19 01:09:16,145 Stage: Train 0.5 | Epoch: 130 | Iter: 395400 | Total Loss: 0.003504 | Recon Loss: 0.002982 | Commit Loss: 0.001044 | Perplexity: 1197.300522
2025-10-19 01:14:38,810 Stage: Train 0.5 | Epoch: 130 | Iter: 395600 | Total Loss: 0.003468 | Recon Loss: 0.002944 | Commit Loss: 0.001048 | Perplexity: 1200.417498
2025-10-19 01:20:00,905 Stage: Train 0.5 | Epoch: 130 | Iter: 395800 | Total Loss: 0.003507 | Recon Loss: 0.002986 | Commit Loss: 0.001043 | Perplexity: 1207.921069
2025-10-19 01:25:23,673 Stage: Train 0.5 | Epoch: 130 | Iter: 396000 | Total Loss: 0.003555 | Recon Loss: 0.003022 | Commit Loss: 0.001065 | Perplexity: 1198.259111
2025-10-19 01:30:45,608 Stage: Train 0.5 | Epoch: 130 | Iter: 396200 | Total Loss: 0.003485 | Recon Loss: 0.002961 | Commit Loss: 0.001049 | Perplexity: 1201.191768
2025-10-19 01:36:08,437 Stage: Train 0.5 | Epoch: 130 | Iter: 396400 | Total Loss: 0.003574 | Recon Loss: 0.003041 | Commit Loss: 0.001065 | Perplexity: 1209.759051
2025-10-19 01:41:31,178 Stage: Train 0.5 | Epoch: 130 | Iter: 396600 | Total Loss: 0.003510 | Recon Loss: 0.002981 | Commit Loss: 0.001058 | Perplexity: 1203.917987
2025-10-19 01:46:54,022 Stage: Train 0.5 | Epoch: 130 | Iter: 396800 | Total Loss: 0.003437 | Recon Loss: 0.002913 | Commit Loss: 0.001048 | Perplexity: 1201.838381
2025-10-19 01:52:16,816 Stage: Train 0.5 | Epoch: 130 | Iter: 397000 | Total Loss: 0.003533 | Recon Loss: 0.002999 | Commit Loss: 0.001068 | Perplexity: 1204.072702
2025-10-19 01:57:39,453 Stage: Train 0.5 | Epoch: 130 | Iter: 397200 | Total Loss: 0.003577 | Recon Loss: 0.003048 | Commit Loss: 0.001057 | Perplexity: 1206.468700
2025-10-19 02:03:02,265 Stage: Train 0.5 | Epoch: 130 | Iter: 397400 | Total Loss: 0.003497 | Recon Loss: 0.002981 | Commit Loss: 0.001031 | Perplexity: 1201.300547
2025-10-19 02:08:25,297 Stage: Train 0.5 | Epoch: 130 | Iter: 397600 | Total Loss: 0.003514 | Recon Loss: 0.002986 | Commit Loss: 0.001056 | Perplexity: 1202.335725
2025-10-19 02:13:47,802 Stage: Train 0.5 | Epoch: 130 | Iter: 397800 | Total Loss: 0.003511 | Recon Loss: 0.002987 | Commit Loss: 0.001047 | Perplexity: 1202.669218
Trainning Epoch:  79%|███████▉  | 131/165 [178:53:33<46:19:03, 4904.21s/it]Trainning Epoch:  79%|███████▉  | 131/165 [178:53:33<46:19:02, 4904.21s/it]2025-10-19 02:19:13,630 Stage: Train 0.5 | Epoch: 131 | Iter: 398000 | Total Loss: 0.003471 | Recon Loss: 0.002946 | Commit Loss: 0.001051 | Perplexity: 1204.877117
2025-10-19 02:24:36,673 Stage: Train 0.5 | Epoch: 131 | Iter: 398200 | Total Loss: 0.003512 | Recon Loss: 0.002996 | Commit Loss: 0.001032 | Perplexity: 1204.435820
2025-10-19 02:29:59,246 Stage: Train 0.5 | Epoch: 131 | Iter: 398400 | Total Loss: 0.003501 | Recon Loss: 0.002982 | Commit Loss: 0.001038 | Perplexity: 1199.857228
2025-10-19 02:35:21,744 Stage: Train 0.5 | Epoch: 131 | Iter: 398600 | Total Loss: 0.003536 | Recon Loss: 0.003012 | Commit Loss: 0.001048 | Perplexity: 1202.582926
2025-10-19 02:40:44,069 Stage: Train 0.5 | Epoch: 131 | Iter: 398800 | Total Loss: 0.003502 | Recon Loss: 0.002977 | Commit Loss: 0.001050 | Perplexity: 1198.828311
2025-10-19 02:46:05,624 Stage: Train 0.5 | Epoch: 131 | Iter: 399000 | Total Loss: 0.003467 | Recon Loss: 0.002936 | Commit Loss: 0.001062 | Perplexity: 1201.064571
2025-10-19 02:51:26,558 Stage: Train 0.5 | Epoch: 131 | Iter: 399200 | Total Loss: 0.003511 | Recon Loss: 0.002975 | Commit Loss: 0.001072 | Perplexity: 1196.543779
2025-10-19 02:56:49,516 Stage: Train 0.5 | Epoch: 131 | Iter: 399400 | Total Loss: 0.003485 | Recon Loss: 0.002958 | Commit Loss: 0.001053 | Perplexity: 1193.769050
2025-10-19 03:02:12,267 Stage: Train 0.5 | Epoch: 131 | Iter: 399600 | Total Loss: 0.003471 | Recon Loss: 0.002949 | Commit Loss: 0.001044 | Perplexity: 1206.673176
2025-10-19 03:07:33,915 Stage: Train 0.5 | Epoch: 131 | Iter: 399800 | Total Loss: 0.003415 | Recon Loss: 0.002895 | Commit Loss: 0.001039 | Perplexity: 1205.730055
2025-10-19 03:12:55,397 Stage: Train 0.5 | Epoch: 131 | Iter: 400000 | Total Loss: 0.003524 | Recon Loss: 0.003000 | Commit Loss: 0.001048 | Perplexity: 1197.326877
2025-10-19 03:12:55,397 Saving model at iteration 400000
2025-10-19 03:12:55,556 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000
2025-10-19 03:12:56,943 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000/model.safetensors
2025-10-19 03:12:58,738 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000/optimizer.bin
2025-10-19 03:12:58,739 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000/scheduler.bin
2025-10-19 03:12:58,739 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000/sampler.bin
2025-10-19 03:12:58,740 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_132_step_400000/random_states_0.pkl
2025-10-19 03:18:20,716 Stage: Train 0.5 | Epoch: 131 | Iter: 400200 | Total Loss: 0.003416 | Recon Loss: 0.002893 | Commit Loss: 0.001048 | Perplexity: 1205.770530
2025-10-19 03:23:42,413 Stage: Train 0.5 | Epoch: 131 | Iter: 400400 | Total Loss: 0.003463 | Recon Loss: 0.002942 | Commit Loss: 0.001043 | Perplexity: 1194.043279
2025-10-19 03:29:04,516 Stage: Train 0.5 | Epoch: 131 | Iter: 400600 | Total Loss: 0.003472 | Recon Loss: 0.002947 | Commit Loss: 0.001050 | Perplexity: 1193.344021
2025-10-19 03:34:26,143 Stage: Train 0.5 | Epoch: 131 | Iter: 400800 | Total Loss: 0.003502 | Recon Loss: 0.002978 | Commit Loss: 0.001049 | Perplexity: 1200.553266
2025-10-19 03:39:48,773 Stage: Train 0.5 | Epoch: 131 | Iter: 401000 | Total Loss: 0.003497 | Recon Loss: 0.002970 | Commit Loss: 0.001055 | Perplexity: 1199.971020
Trainning Epoch:  80%|████████  | 132/165 [180:15:13<44:56:43, 4903.13s/it]Trainning Epoch:  80%|████████  | 132/165 [180:15:13<44:56:43, 4903.13s/it]2025-10-19 03:45:16,024 Stage: Train 0.5 | Epoch: 132 | Iter: 401200 | Total Loss: 0.003424 | Recon Loss: 0.002899 | Commit Loss: 0.001049 | Perplexity: 1201.687637
2025-10-19 03:50:38,859 Stage: Train 0.5 | Epoch: 132 | Iter: 401400 | Total Loss: 0.003586 | Recon Loss: 0.003054 | Commit Loss: 0.001064 | Perplexity: 1205.691776
2025-10-19 03:56:01,322 Stage: Train 0.5 | Epoch: 132 | Iter: 401600 | Total Loss: 0.003489 | Recon Loss: 0.002966 | Commit Loss: 0.001046 | Perplexity: 1200.914050
2025-10-19 04:01:23,850 Stage: Train 0.5 | Epoch: 132 | Iter: 401800 | Total Loss: 0.003531 | Recon Loss: 0.003008 | Commit Loss: 0.001046 | Perplexity: 1199.325838
2025-10-19 04:06:46,435 Stage: Train 0.5 | Epoch: 132 | Iter: 402000 | Total Loss: 0.003488 | Recon Loss: 0.002963 | Commit Loss: 0.001049 | Perplexity: 1204.405975
2025-10-19 04:12:08,421 Stage: Train 0.5 | Epoch: 132 | Iter: 402200 | Total Loss: 0.003487 | Recon Loss: 0.002960 | Commit Loss: 0.001053 | Perplexity: 1205.112542
2025-10-19 04:17:31,317 Stage: Train 0.5 | Epoch: 132 | Iter: 402400 | Total Loss: 0.003478 | Recon Loss: 0.002945 | Commit Loss: 0.001066 | Perplexity: 1203.644100
2025-10-19 04:22:53,683 Stage: Train 0.5 | Epoch: 132 | Iter: 402600 | Total Loss: 0.003545 | Recon Loss: 0.003022 | Commit Loss: 0.001046 | Perplexity: 1204.341035
2025-10-19 04:28:15,666 Stage: Train 0.5 | Epoch: 132 | Iter: 402800 | Total Loss: 0.003501 | Recon Loss: 0.002980 | Commit Loss: 0.001042 | Perplexity: 1200.281577
2025-10-19 04:33:38,552 Stage: Train 0.5 | Epoch: 132 | Iter: 403000 | Total Loss: 0.003476 | Recon Loss: 0.002949 | Commit Loss: 0.001053 | Perplexity: 1203.490990
2025-10-19 04:39:00,214 Stage: Train 0.5 | Epoch: 132 | Iter: 403200 | Total Loss: 0.003590 | Recon Loss: 0.003057 | Commit Loss: 0.001068 | Perplexity: 1201.066952
2025-10-19 04:44:21,175 Stage: Train 0.5 | Epoch: 132 | Iter: 403400 | Total Loss: 0.003448 | Recon Loss: 0.002927 | Commit Loss: 0.001042 | Perplexity: 1198.361027
2025-10-19 04:49:42,066 Stage: Train 0.5 | Epoch: 132 | Iter: 403600 | Total Loss: 0.003458 | Recon Loss: 0.002933 | Commit Loss: 0.001049 | Perplexity: 1202.663885
2025-10-19 04:55:04,109 Stage: Train 0.5 | Epoch: 132 | Iter: 403800 | Total Loss: 0.003540 | Recon Loss: 0.003020 | Commit Loss: 0.001040 | Perplexity: 1207.714229
2025-10-19 05:00:26,775 Stage: Train 0.5 | Epoch: 132 | Iter: 404000 | Total Loss: 0.003546 | Recon Loss: 0.003015 | Commit Loss: 0.001062 | Perplexity: 1205.487038
Trainning Epoch:  81%|████████  | 133/165 [181:36:52<43:34:23, 4901.99s/it]Trainning Epoch:  81%|████████  | 133/165 [181:36:53<43:34:23, 4901.99s/it]2025-10-19 05:05:53,428 Stage: Train 0.5 | Epoch: 133 | Iter: 404200 | Total Loss: 0.003477 | Recon Loss: 0.002953 | Commit Loss: 0.001049 | Perplexity: 1200.504641
2025-10-19 05:11:15,555 Stage: Train 0.5 | Epoch: 133 | Iter: 404400 | Total Loss: 0.003502 | Recon Loss: 0.002971 | Commit Loss: 0.001061 | Perplexity: 1208.559508
2025-10-19 05:16:37,485 Stage: Train 0.5 | Epoch: 133 | Iter: 404600 | Total Loss: 0.003475 | Recon Loss: 0.002947 | Commit Loss: 0.001055 | Perplexity: 1208.867461
2025-10-19 05:22:00,413 Stage: Train 0.5 | Epoch: 133 | Iter: 404800 | Total Loss: 0.003543 | Recon Loss: 0.003014 | Commit Loss: 0.001058 | Perplexity: 1199.672827
2025-10-19 05:27:22,568 Stage: Train 0.5 | Epoch: 133 | Iter: 405000 | Total Loss: 0.003499 | Recon Loss: 0.002964 | Commit Loss: 0.001070 | Perplexity: 1198.742626
2025-10-19 05:32:44,159 Stage: Train 0.5 | Epoch: 133 | Iter: 405200 | Total Loss: 0.003534 | Recon Loss: 0.003011 | Commit Loss: 0.001045 | Perplexity: 1204.705647
2025-10-19 05:38:06,498 Stage: Train 0.5 | Epoch: 133 | Iter: 405400 | Total Loss: 0.003444 | Recon Loss: 0.002921 | Commit Loss: 0.001045 | Perplexity: 1217.340759
2025-10-19 05:43:28,945 Stage: Train 0.5 | Epoch: 133 | Iter: 405600 | Total Loss: 0.003471 | Recon Loss: 0.002950 | Commit Loss: 0.001041 | Perplexity: 1211.446255
2025-10-19 05:48:51,533 Stage: Train 0.5 | Epoch: 133 | Iter: 405800 | Total Loss: 0.003523 | Recon Loss: 0.003005 | Commit Loss: 0.001036 | Perplexity: 1204.548909
2025-10-19 05:54:13,734 Stage: Train 0.5 | Epoch: 133 | Iter: 406000 | Total Loss: 0.003507 | Recon Loss: 0.002983 | Commit Loss: 0.001048 | Perplexity: 1200.727547
2025-10-19 05:59:35,697 Stage: Train 0.5 | Epoch: 133 | Iter: 406200 | Total Loss: 0.003463 | Recon Loss: 0.002933 | Commit Loss: 0.001061 | Perplexity: 1208.140465
2025-10-19 06:04:57,172 Stage: Train 0.5 | Epoch: 133 | Iter: 406400 | Total Loss: 0.003490 | Recon Loss: 0.002973 | Commit Loss: 0.001034 | Perplexity: 1203.163022
2025-10-19 06:10:19,844 Stage: Train 0.5 | Epoch: 133 | Iter: 406600 | Total Loss: 0.003512 | Recon Loss: 0.002984 | Commit Loss: 0.001057 | Perplexity: 1205.060154
2025-10-19 06:15:42,227 Stage: Train 0.5 | Epoch: 133 | Iter: 406800 | Total Loss: 0.003508 | Recon Loss: 0.002986 | Commit Loss: 0.001044 | Perplexity: 1201.343483
2025-10-19 06:21:03,808 Stage: Train 0.5 | Epoch: 133 | Iter: 407000 | Total Loss: 0.003501 | Recon Loss: 0.002972 | Commit Loss: 0.001057 | Perplexity: 1200.664278
Trainning Epoch:  81%|████████  | 134/165 [182:58:31<42:12:05, 4900.81s/it]Trainning Epoch:  81%|████████  | 134/165 [182:58:31<42:12:05, 4900.81s/it]2025-10-19 06:26:30,215 Stage: Train 0.5 | Epoch: 134 | Iter: 407200 | Total Loss: 0.003508 | Recon Loss: 0.002979 | Commit Loss: 0.001059 | Perplexity: 1203.511367
2025-10-19 06:31:52,267 Stage: Train 0.5 | Epoch: 134 | Iter: 407400 | Total Loss: 0.003535 | Recon Loss: 0.003017 | Commit Loss: 0.001036 | Perplexity: 1199.888228
2025-10-19 06:37:14,843 Stage: Train 0.5 | Epoch: 134 | Iter: 407600 | Total Loss: 0.003482 | Recon Loss: 0.002957 | Commit Loss: 0.001051 | Perplexity: 1203.613521
2025-10-19 06:42:36,643 Stage: Train 0.5 | Epoch: 134 | Iter: 407800 | Total Loss: 0.003581 | Recon Loss: 0.003055 | Commit Loss: 0.001052 | Perplexity: 1207.125467
2025-10-19 06:47:58,953 Stage: Train 0.5 | Epoch: 134 | Iter: 408000 | Total Loss: 0.003438 | Recon Loss: 0.002916 | Commit Loss: 0.001043 | Perplexity: 1202.700061
2025-10-19 06:53:22,006 Stage: Train 0.5 | Epoch: 134 | Iter: 408200 | Total Loss: 0.003473 | Recon Loss: 0.002955 | Commit Loss: 0.001036 | Perplexity: 1204.967618
2025-10-19 06:58:45,023 Stage: Train 0.5 | Epoch: 134 | Iter: 408400 | Total Loss: 0.003475 | Recon Loss: 0.002945 | Commit Loss: 0.001060 | Perplexity: 1204.502526
2025-10-19 07:04:07,756 Stage: Train 0.5 | Epoch: 134 | Iter: 408600 | Total Loss: 0.003413 | Recon Loss: 0.002894 | Commit Loss: 0.001039 | Perplexity: 1201.762878
2025-10-19 07:09:30,369 Stage: Train 0.5 | Epoch: 134 | Iter: 408800 | Total Loss: 0.003524 | Recon Loss: 0.003002 | Commit Loss: 0.001045 | Perplexity: 1198.918780
2025-10-19 07:14:53,409 Stage: Train 0.5 | Epoch: 134 | Iter: 409000 | Total Loss: 0.003499 | Recon Loss: 0.002979 | Commit Loss: 0.001040 | Perplexity: 1204.310325
2025-10-19 07:20:15,642 Stage: Train 0.5 | Epoch: 134 | Iter: 409200 | Total Loss: 0.003479 | Recon Loss: 0.002952 | Commit Loss: 0.001054 | Perplexity: 1201.415320
2025-10-19 07:25:36,289 Stage: Train 0.5 | Epoch: 134 | Iter: 409400 | Total Loss: 0.003488 | Recon Loss: 0.002971 | Commit Loss: 0.001035 | Perplexity: 1202.698127
2025-10-19 07:30:58,628 Stage: Train 0.5 | Epoch: 134 | Iter: 409600 | Total Loss: 0.003531 | Recon Loss: 0.003007 | Commit Loss: 0.001049 | Perplexity: 1203.511486
2025-10-19 07:36:20,949 Stage: Train 0.5 | Epoch: 134 | Iter: 409800 | Total Loss: 0.003431 | Recon Loss: 0.002919 | Commit Loss: 0.001025 | Perplexity: 1196.644729
2025-10-19 07:41:43,783 Stage: Train 0.5 | Epoch: 134 | Iter: 410000 | Total Loss: 0.003484 | Recon Loss: 0.002963 | Commit Loss: 0.001042 | Perplexity: 1203.915684
Trainning Epoch:  82%|████████▏ | 135/165 [184:20:12<40:50:32, 4901.08s/it]Trainning Epoch:  82%|████████▏ | 135/165 [184:20:12<40:50:32, 4901.08s/it]2025-10-19 07:47:10,822 Stage: Train 0.5 | Epoch: 135 | Iter: 410200 | Total Loss: 0.003479 | Recon Loss: 0.002955 | Commit Loss: 0.001047 | Perplexity: 1203.241741
2025-10-19 07:52:33,141 Stage: Train 0.5 | Epoch: 135 | Iter: 410400 | Total Loss: 0.003468 | Recon Loss: 0.002945 | Commit Loss: 0.001044 | Perplexity: 1205.500297
2025-10-19 07:57:54,655 Stage: Train 0.5 | Epoch: 135 | Iter: 410600 | Total Loss: 0.003478 | Recon Loss: 0.002953 | Commit Loss: 0.001052 | Perplexity: 1207.565858
2025-10-19 08:03:16,582 Stage: Train 0.5 | Epoch: 135 | Iter: 410800 | Total Loss: 0.003576 | Recon Loss: 0.003038 | Commit Loss: 0.001077 | Perplexity: 1204.399073
2025-10-19 08:08:38,052 Stage: Train 0.5 | Epoch: 135 | Iter: 411000 | Total Loss: 0.003500 | Recon Loss: 0.002982 | Commit Loss: 0.001036 | Perplexity: 1190.978951
2025-10-19 08:14:00,418 Stage: Train 0.5 | Epoch: 135 | Iter: 411200 | Total Loss: 0.003523 | Recon Loss: 0.003001 | Commit Loss: 0.001044 | Perplexity: 1203.327295
2025-10-19 08:19:22,713 Stage: Train 0.5 | Epoch: 135 | Iter: 411400 | Total Loss: 0.003429 | Recon Loss: 0.002903 | Commit Loss: 0.001053 | Perplexity: 1199.255625
2025-10-19 08:24:45,420 Stage: Train 0.5 | Epoch: 135 | Iter: 411600 | Total Loss: 0.003436 | Recon Loss: 0.002913 | Commit Loss: 0.001046 | Perplexity: 1199.009290
2025-10-19 08:30:08,256 Stage: Train 0.5 | Epoch: 135 | Iter: 411800 | Total Loss: 0.003476 | Recon Loss: 0.002955 | Commit Loss: 0.001042 | Perplexity: 1208.084412
2025-10-19 08:35:31,099 Stage: Train 0.5 | Epoch: 135 | Iter: 412000 | Total Loss: 0.003486 | Recon Loss: 0.002958 | Commit Loss: 0.001056 | Perplexity: 1193.946729
2025-10-19 08:40:53,813 Stage: Train 0.5 | Epoch: 135 | Iter: 412200 | Total Loss: 0.003521 | Recon Loss: 0.002999 | Commit Loss: 0.001044 | Perplexity: 1203.825736
2025-10-19 08:46:16,341 Stage: Train 0.5 | Epoch: 135 | Iter: 412400 | Total Loss: 0.003521 | Recon Loss: 0.003008 | Commit Loss: 0.001027 | Perplexity: 1201.400589
2025-10-19 08:51:39,094 Stage: Train 0.5 | Epoch: 135 | Iter: 412600 | Total Loss: 0.003455 | Recon Loss: 0.002937 | Commit Loss: 0.001036 | Perplexity: 1208.364903
2025-10-19 08:57:01,413 Stage: Train 0.5 | Epoch: 135 | Iter: 412800 | Total Loss: 0.003542 | Recon Loss: 0.003006 | Commit Loss: 0.001073 | Perplexity: 1212.511550
2025-10-19 09:02:23,773 Stage: Train 0.5 | Epoch: 135 | Iter: 413000 | Total Loss: 0.003511 | Recon Loss: 0.002984 | Commit Loss: 0.001054 | Perplexity: 1193.423094
Trainning Epoch:  82%|████████▏ | 136/165 [185:41:53<39:28:50, 4901.07s/it]Trainning Epoch:  82%|████████▏ | 136/165 [185:41:53<39:28:50, 4901.07s/it]2025-10-19 09:07:50,760 Stage: Train 0.5 | Epoch: 136 | Iter: 413200 | Total Loss: 0.003607 | Recon Loss: 0.003079 | Commit Loss: 0.001056 | Perplexity: 1203.227092
2025-10-19 09:13:13,022 Stage: Train 0.5 | Epoch: 136 | Iter: 413400 | Total Loss: 0.003462 | Recon Loss: 0.002944 | Commit Loss: 0.001035 | Perplexity: 1205.223326
2025-10-19 09:18:35,807 Stage: Train 0.5 | Epoch: 136 | Iter: 413600 | Total Loss: 0.003489 | Recon Loss: 0.002966 | Commit Loss: 0.001046 | Perplexity: 1199.981068
2025-10-19 09:23:58,666 Stage: Train 0.5 | Epoch: 136 | Iter: 413800 | Total Loss: 0.003501 | Recon Loss: 0.002978 | Commit Loss: 0.001047 | Perplexity: 1206.437621
2025-10-19 09:29:21,390 Stage: Train 0.5 | Epoch: 136 | Iter: 414000 | Total Loss: 0.003440 | Recon Loss: 0.002924 | Commit Loss: 0.001033 | Perplexity: 1202.861299
2025-10-19 09:34:44,635 Stage: Train 0.5 | Epoch: 136 | Iter: 414200 | Total Loss: 0.003428 | Recon Loss: 0.002907 | Commit Loss: 0.001041 | Perplexity: 1205.759605
2025-10-19 09:40:05,953 Stage: Train 0.5 | Epoch: 136 | Iter: 414400 | Total Loss: 0.003421 | Recon Loss: 0.002899 | Commit Loss: 0.001042 | Perplexity: 1209.077568
2025-10-19 09:45:28,501 Stage: Train 0.5 | Epoch: 136 | Iter: 414600 | Total Loss: 0.003466 | Recon Loss: 0.002935 | Commit Loss: 0.001061 | Perplexity: 1207.440472
2025-10-19 09:50:51,005 Stage: Train 0.5 | Epoch: 136 | Iter: 414800 | Total Loss: 0.003456 | Recon Loss: 0.002926 | Commit Loss: 0.001060 | Perplexity: 1201.713237
2025-10-19 09:56:14,004 Stage: Train 0.5 | Epoch: 136 | Iter: 415000 | Total Loss: 0.003476 | Recon Loss: 0.002953 | Commit Loss: 0.001046 | Perplexity: 1199.721778
2025-10-19 10:01:36,417 Stage: Train 0.5 | Epoch: 136 | Iter: 415200 | Total Loss: 0.003488 | Recon Loss: 0.002966 | Commit Loss: 0.001045 | Perplexity: 1202.578199
2025-10-19 10:06:59,003 Stage: Train 0.5 | Epoch: 136 | Iter: 415400 | Total Loss: 0.003514 | Recon Loss: 0.002991 | Commit Loss: 0.001045 | Perplexity: 1203.499503
2025-10-19 10:12:19,713 Stage: Train 0.5 | Epoch: 136 | Iter: 415600 | Total Loss: 0.003446 | Recon Loss: 0.002924 | Commit Loss: 0.001043 | Perplexity: 1202.434243
2025-10-19 10:17:41,498 Stage: Train 0.5 | Epoch: 136 | Iter: 415800 | Total Loss: 0.003423 | Recon Loss: 0.002898 | Commit Loss: 0.001051 | Perplexity: 1207.392128
2025-10-19 10:23:04,577 Stage: Train 0.5 | Epoch: 136 | Iter: 416000 | Total Loss: 0.003535 | Recon Loss: 0.003016 | Commit Loss: 0.001038 | Perplexity: 1196.494612
2025-10-19 10:28:27,636 Stage: Train 0.5 | Epoch: 136 | Iter: 416200 | Total Loss: 0.003915 | Recon Loss: 0.003253 | Commit Loss: 0.001324 | Perplexity: 1207.649921
Trainning Epoch:  83%|████████▎ | 137/165 [187:03:36<38:07:23, 4901.54s/it]Trainning Epoch:  83%|████████▎ | 137/165 [187:03:36<38:07:23, 4901.54s/it]2025-10-19 10:33:53,995 Stage: Train 0.5 | Epoch: 137 | Iter: 416400 | Total Loss: 0.003452 | Recon Loss: 0.002935 | Commit Loss: 0.001033 | Perplexity: 1194.459583
2025-10-19 10:39:16,842 Stage: Train 0.5 | Epoch: 137 | Iter: 416600 | Total Loss: 0.003460 | Recon Loss: 0.002939 | Commit Loss: 0.001042 | Perplexity: 1197.938243
2025-10-19 10:44:39,124 Stage: Train 0.5 | Epoch: 137 | Iter: 416800 | Total Loss: 0.003445 | Recon Loss: 0.002917 | Commit Loss: 0.001057 | Perplexity: 1203.798592
2025-10-19 10:50:02,105 Stage: Train 0.5 | Epoch: 137 | Iter: 417000 | Total Loss: 0.003519 | Recon Loss: 0.002993 | Commit Loss: 0.001052 | Perplexity: 1202.649204
2025-10-19 10:55:23,803 Stage: Train 0.5 | Epoch: 137 | Iter: 417200 | Total Loss: 0.003418 | Recon Loss: 0.002887 | Commit Loss: 0.001064 | Perplexity: 1205.777294
2025-10-19 11:00:46,017 Stage: Train 0.5 | Epoch: 137 | Iter: 417400 | Total Loss: 0.003536 | Recon Loss: 0.003017 | Commit Loss: 0.001038 | Perplexity: 1206.795028
2025-10-19 11:06:08,404 Stage: Train 0.5 | Epoch: 137 | Iter: 417600 | Total Loss: 0.003441 | Recon Loss: 0.002917 | Commit Loss: 0.001048 | Perplexity: 1209.690609
2025-10-19 11:11:30,693 Stage: Train 0.5 | Epoch: 137 | Iter: 417800 | Total Loss: 0.003432 | Recon Loss: 0.002909 | Commit Loss: 0.001047 | Perplexity: 1201.312596
2025-10-19 11:16:53,601 Stage: Train 0.5 | Epoch: 137 | Iter: 418000 | Total Loss: 0.003502 | Recon Loss: 0.002983 | Commit Loss: 0.001038 | Perplexity: 1197.061551
2025-10-19 11:22:16,199 Stage: Train 0.5 | Epoch: 137 | Iter: 418200 | Total Loss: 0.003448 | Recon Loss: 0.002927 | Commit Loss: 0.001043 | Perplexity: 1198.814416
2025-10-19 11:27:38,940 Stage: Train 0.5 | Epoch: 137 | Iter: 418400 | Total Loss: 0.003463 | Recon Loss: 0.002948 | Commit Loss: 0.001030 | Perplexity: 1199.966659
2025-10-19 11:33:01,139 Stage: Train 0.5 | Epoch: 137 | Iter: 418600 | Total Loss: 0.003487 | Recon Loss: 0.002961 | Commit Loss: 0.001051 | Perplexity: 1205.007403
2025-10-19 11:38:23,325 Stage: Train 0.5 | Epoch: 137 | Iter: 418800 | Total Loss: 0.003423 | Recon Loss: 0.002898 | Commit Loss: 0.001050 | Perplexity: 1205.307167
2025-10-19 11:43:45,817 Stage: Train 0.5 | Epoch: 137 | Iter: 419000 | Total Loss: 0.003522 | Recon Loss: 0.002990 | Commit Loss: 0.001063 | Perplexity: 1205.872017
2025-10-19 11:49:08,658 Stage: Train 0.5 | Epoch: 137 | Iter: 419200 | Total Loss: 0.003449 | Recon Loss: 0.002925 | Commit Loss: 0.001049 | Perplexity: 1198.104778
Trainning Epoch:  84%|████████▎ | 138/165 [188:25:18<36:45:47, 4901.78s/it]Trainning Epoch:  84%|████████▎ | 138/165 [188:25:18<36:45:47, 4901.78s/it]2025-10-19 11:54:35,635 Stage: Train 0.5 | Epoch: 138 | Iter: 419400 | Total Loss: 0.003508 | Recon Loss: 0.002992 | Commit Loss: 0.001031 | Perplexity: 1199.526807
2025-10-19 11:59:58,153 Stage: Train 0.5 | Epoch: 138 | Iter: 419600 | Total Loss: 0.003459 | Recon Loss: 0.002933 | Commit Loss: 0.001053 | Perplexity: 1205.247449
2025-10-19 12:05:19,900 Stage: Train 0.5 | Epoch: 138 | Iter: 419800 | Total Loss: 0.003408 | Recon Loss: 0.002884 | Commit Loss: 0.001048 | Perplexity: 1210.568229
2025-10-19 12:10:42,597 Stage: Train 0.5 | Epoch: 138 | Iter: 420000 | Total Loss: 0.003489 | Recon Loss: 0.002970 | Commit Loss: 0.001038 | Perplexity: 1206.186531
2025-10-19 12:10:42,597 Saving model at iteration 420000
2025-10-19 12:10:42,760 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000
2025-10-19 12:10:44,161 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000/model.safetensors
2025-10-19 12:10:45,916 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000/optimizer.bin
2025-10-19 12:10:45,917 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000/scheduler.bin
2025-10-19 12:10:45,917 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000/sampler.bin
2025-10-19 12:10:45,918 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_139_step_420000/random_states_0.pkl
2025-10-19 12:16:08,830 Stage: Train 0.5 | Epoch: 138 | Iter: 420200 | Total Loss: 0.003444 | Recon Loss: 0.002920 | Commit Loss: 0.001047 | Perplexity: 1210.841035
2025-10-19 12:21:31,691 Stage: Train 0.5 | Epoch: 138 | Iter: 420400 | Total Loss: 0.003535 | Recon Loss: 0.003008 | Commit Loss: 0.001055 | Perplexity: 1202.682942
2025-10-19 12:26:53,862 Stage: Train 0.5 | Epoch: 138 | Iter: 420600 | Total Loss: 0.003407 | Recon Loss: 0.002886 | Commit Loss: 0.001043 | Perplexity: 1202.460694
2025-10-19 12:32:16,272 Stage: Train 0.5 | Epoch: 138 | Iter: 420800 | Total Loss: 0.003443 | Recon Loss: 0.002919 | Commit Loss: 0.001048 | Perplexity: 1203.785676
2025-10-19 12:37:38,908 Stage: Train 0.5 | Epoch: 138 | Iter: 421000 | Total Loss: 0.003458 | Recon Loss: 0.002937 | Commit Loss: 0.001041 | Perplexity: 1201.034731
2025-10-19 12:43:01,523 Stage: Train 0.5 | Epoch: 138 | Iter: 421200 | Total Loss: 0.003432 | Recon Loss: 0.002906 | Commit Loss: 0.001052 | Perplexity: 1206.541500
2025-10-19 12:48:23,792 Stage: Train 0.5 | Epoch: 138 | Iter: 421400 | Total Loss: 0.003449 | Recon Loss: 0.002928 | Commit Loss: 0.001042 | Perplexity: 1203.487147
2025-10-19 12:53:46,128 Stage: Train 0.5 | Epoch: 138 | Iter: 421600 | Total Loss: 0.003539 | Recon Loss: 0.003014 | Commit Loss: 0.001050 | Perplexity: 1203.990947
2025-10-19 12:59:07,456 Stage: Train 0.5 | Epoch: 138 | Iter: 421800 | Total Loss: 0.003387 | Recon Loss: 0.002874 | Commit Loss: 0.001027 | Perplexity: 1196.692878
2025-10-19 13:04:29,656 Stage: Train 0.5 | Epoch: 138 | Iter: 422000 | Total Loss: 0.003537 | Recon Loss: 0.003010 | Commit Loss: 0.001054 | Perplexity: 1199.304200
2025-10-19 13:09:52,296 Stage: Train 0.5 | Epoch: 138 | Iter: 422200 | Total Loss: 0.003491 | Recon Loss: 0.002970 | Commit Loss: 0.001042 | Perplexity: 1199.219351
Trainning Epoch:  84%|████████▍ | 139/165 [189:47:03<35:24:30, 4902.70s/it]Trainning Epoch:  84%|████████▍ | 139/165 [189:47:03<35:24:30, 4902.70s/it]2025-10-19 13:15:19,451 Stage: Train 0.5 | Epoch: 139 | Iter: 422400 | Total Loss: 0.003526 | Recon Loss: 0.002996 | Commit Loss: 0.001060 | Perplexity: 1209.573026
2025-10-19 13:20:41,879 Stage: Train 0.5 | Epoch: 139 | Iter: 422600 | Total Loss: 0.003429 | Recon Loss: 0.002918 | Commit Loss: 0.001022 | Perplexity: 1208.231538
2025-10-19 13:26:04,746 Stage: Train 0.5 | Epoch: 139 | Iter: 422800 | Total Loss: 0.003507 | Recon Loss: 0.002988 | Commit Loss: 0.001038 | Perplexity: 1203.565172
2025-10-19 13:31:25,352 Stage: Train 0.5 | Epoch: 139 | Iter: 423000 | Total Loss: 0.003451 | Recon Loss: 0.002936 | Commit Loss: 0.001032 | Perplexity: 1207.366177
2025-10-19 13:36:47,102 Stage: Train 0.5 | Epoch: 139 | Iter: 423200 | Total Loss: 0.003529 | Recon Loss: 0.003003 | Commit Loss: 0.001052 | Perplexity: 1205.817983
2025-10-19 13:42:09,863 Stage: Train 0.5 | Epoch: 139 | Iter: 423400 | Total Loss: 0.003339 | Recon Loss: 0.002827 | Commit Loss: 0.001024 | Perplexity: 1205.870882
2025-10-19 13:47:32,155 Stage: Train 0.5 | Epoch: 139 | Iter: 423600 | Total Loss: 0.003428 | Recon Loss: 0.002904 | Commit Loss: 0.001047 | Perplexity: 1202.093917
2025-10-19 13:52:54,332 Stage: Train 0.5 | Epoch: 139 | Iter: 423800 | Total Loss: 0.003382 | Recon Loss: 0.002857 | Commit Loss: 0.001049 | Perplexity: 1211.358884
2025-10-19 13:58:16,666 Stage: Train 0.5 | Epoch: 139 | Iter: 424000 | Total Loss: 0.003532 | Recon Loss: 0.003011 | Commit Loss: 0.001043 | Perplexity: 1204.011689
2025-10-19 14:03:38,325 Stage: Train 0.5 | Epoch: 139 | Iter: 424200 | Total Loss: 0.003434 | Recon Loss: 0.002906 | Commit Loss: 0.001056 | Perplexity: 1202.227523
2025-10-19 14:09:00,764 Stage: Train 0.5 | Epoch: 139 | Iter: 424400 | Total Loss: 0.003499 | Recon Loss: 0.002983 | Commit Loss: 0.001031 | Perplexity: 1204.320226
2025-10-19 14:14:22,899 Stage: Train 0.5 | Epoch: 139 | Iter: 424600 | Total Loss: 0.003421 | Recon Loss: 0.002898 | Commit Loss: 0.001046 | Perplexity: 1209.718259
2025-10-19 14:19:45,434 Stage: Train 0.5 | Epoch: 139 | Iter: 424800 | Total Loss: 0.003485 | Recon Loss: 0.002955 | Commit Loss: 0.001060 | Perplexity: 1213.641243
2025-10-19 14:25:08,361 Stage: Train 0.5 | Epoch: 139 | Iter: 425000 | Total Loss: 0.003471 | Recon Loss: 0.002952 | Commit Loss: 0.001038 | Perplexity: 1200.266454
2025-10-19 14:30:30,553 Stage: Train 0.5 | Epoch: 139 | Iter: 425200 | Total Loss: 0.003446 | Recon Loss: 0.002924 | Commit Loss: 0.001044 | Perplexity: 1203.572580
Trainning Epoch:  85%|████████▍ | 140/165 [191:08:43<34:02:24, 4901.79s/it]Trainning Epoch:  85%|████████▍ | 140/165 [191:08:43<34:02:24, 4901.79s/it]2025-10-19 14:35:57,641 Stage: Train 0.5 | Epoch: 140 | Iter: 425400 | Total Loss: 0.003417 | Recon Loss: 0.002898 | Commit Loss: 0.001038 | Perplexity: 1209.365607
2025-10-19 14:41:20,554 Stage: Train 0.5 | Epoch: 140 | Iter: 425600 | Total Loss: 0.003437 | Recon Loss: 0.002921 | Commit Loss: 0.001032 | Perplexity: 1202.550312
2025-10-19 14:46:43,165 Stage: Train 0.5 | Epoch: 140 | Iter: 425800 | Total Loss: 0.003394 | Recon Loss: 0.002879 | Commit Loss: 0.001031 | Perplexity: 1204.278161
2025-10-19 14:52:06,093 Stage: Train 0.5 | Epoch: 140 | Iter: 426000 | Total Loss: 0.003465 | Recon Loss: 0.002952 | Commit Loss: 0.001025 | Perplexity: 1205.001205
2025-10-19 14:57:28,573 Stage: Train 0.5 | Epoch: 140 | Iter: 426200 | Total Loss: 0.003450 | Recon Loss: 0.002923 | Commit Loss: 0.001054 | Perplexity: 1203.330562
2025-10-19 15:02:49,731 Stage: Train 0.5 | Epoch: 140 | Iter: 426400 | Total Loss: 0.003451 | Recon Loss: 0.002925 | Commit Loss: 0.001052 | Perplexity: 1195.328620
2025-10-19 15:08:12,415 Stage: Train 0.5 | Epoch: 140 | Iter: 426600 | Total Loss: 0.003430 | Recon Loss: 0.002916 | Commit Loss: 0.001026 | Perplexity: 1199.087198
2025-10-19 15:13:35,446 Stage: Train 0.5 | Epoch: 140 | Iter: 426800 | Total Loss: 0.003434 | Recon Loss: 0.002909 | Commit Loss: 0.001049 | Perplexity: 1198.659326
2025-10-19 15:18:57,166 Stage: Train 0.5 | Epoch: 140 | Iter: 427000 | Total Loss: 0.003428 | Recon Loss: 0.002902 | Commit Loss: 0.001053 | Perplexity: 1207.658436
2025-10-19 15:24:19,939 Stage: Train 0.5 | Epoch: 140 | Iter: 427200 | Total Loss: 0.003421 | Recon Loss: 0.002894 | Commit Loss: 0.001054 | Perplexity: 1201.798945
2025-10-19 15:29:42,016 Stage: Train 0.5 | Epoch: 140 | Iter: 427400 | Total Loss: 0.003439 | Recon Loss: 0.002922 | Commit Loss: 0.001035 | Perplexity: 1203.234294
2025-10-19 15:35:04,259 Stage: Train 0.5 | Epoch: 140 | Iter: 427600 | Total Loss: 0.003458 | Recon Loss: 0.002945 | Commit Loss: 0.001026 | Perplexity: 1203.104621
2025-10-19 15:40:26,828 Stage: Train 0.5 | Epoch: 140 | Iter: 427800 | Total Loss: 0.003410 | Recon Loss: 0.002884 | Commit Loss: 0.001053 | Perplexity: 1207.123982
2025-10-19 15:45:49,513 Stage: Train 0.5 | Epoch: 140 | Iter: 428000 | Total Loss: 0.003434 | Recon Loss: 0.002920 | Commit Loss: 0.001028 | Perplexity: 1205.050488
2025-10-19 15:51:12,191 Stage: Train 0.5 | Epoch: 140 | Iter: 428200 | Total Loss: 0.003453 | Recon Loss: 0.002934 | Commit Loss: 0.001038 | Perplexity: 1206.637743
Trainning Epoch:  85%|████████▌ | 141/165 [192:30:26<32:40:50, 4902.10s/it]Trainning Epoch:  85%|████████▌ | 141/165 [192:30:26<32:40:50, 4902.10s/it]2025-10-19 15:56:39,227 Stage: Train 0.5 | Epoch: 141 | Iter: 428400 | Total Loss: 0.003409 | Recon Loss: 0.002894 | Commit Loss: 0.001030 | Perplexity: 1209.914070
2025-10-19 16:02:02,108 Stage: Train 0.5 | Epoch: 141 | Iter: 428600 | Total Loss: 0.003492 | Recon Loss: 0.002968 | Commit Loss: 0.001049 | Perplexity: 1207.225531
2025-10-19 16:07:24,251 Stage: Train 0.5 | Epoch: 141 | Iter: 428800 | Total Loss: 0.003443 | Recon Loss: 0.002925 | Commit Loss: 0.001037 | Perplexity: 1203.511619
2025-10-19 16:12:46,986 Stage: Train 0.5 | Epoch: 141 | Iter: 429000 | Total Loss: 0.003496 | Recon Loss: 0.002975 | Commit Loss: 0.001042 | Perplexity: 1195.290558
2025-10-19 16:18:09,895 Stage: Train 0.5 | Epoch: 141 | Iter: 429200 | Total Loss: 0.003439 | Recon Loss: 0.002911 | Commit Loss: 0.001055 | Perplexity: 1203.564209
2025-10-19 16:23:32,249 Stage: Train 0.5 | Epoch: 141 | Iter: 429400 | Total Loss: 0.003417 | Recon Loss: 0.002895 | Commit Loss: 0.001043 | Perplexity: 1203.769344
2025-10-19 16:28:53,978 Stage: Train 0.5 | Epoch: 141 | Iter: 429600 | Total Loss: 0.003457 | Recon Loss: 0.002941 | Commit Loss: 0.001032 | Perplexity: 1203.054448
2025-10-19 16:34:15,760 Stage: Train 0.5 | Epoch: 141 | Iter: 429800 | Total Loss: 0.003379 | Recon Loss: 0.002858 | Commit Loss: 0.001042 | Perplexity: 1207.585557
2025-10-19 16:39:37,758 Stage: Train 0.5 | Epoch: 141 | Iter: 430000 | Total Loss: 0.003569 | Recon Loss: 0.003041 | Commit Loss: 0.001057 | Perplexity: 1204.538144
2025-10-19 16:45:00,575 Stage: Train 0.5 | Epoch: 141 | Iter: 430200 | Total Loss: 0.003426 | Recon Loss: 0.002907 | Commit Loss: 0.001039 | Perplexity: 1210.167890
2025-10-19 16:50:23,228 Stage: Train 0.5 | Epoch: 141 | Iter: 430400 | Total Loss: 0.003441 | Recon Loss: 0.002923 | Commit Loss: 0.001037 | Perplexity: 1212.049008
2025-10-19 16:55:45,924 Stage: Train 0.5 | Epoch: 141 | Iter: 430600 | Total Loss: 0.003394 | Recon Loss: 0.002874 | Commit Loss: 0.001041 | Perplexity: 1208.031592
2025-10-19 17:01:07,966 Stage: Train 0.5 | Epoch: 141 | Iter: 430800 | Total Loss: 0.003366 | Recon Loss: 0.002847 | Commit Loss: 0.001039 | Perplexity: 1211.677207
2025-10-19 17:06:30,567 Stage: Train 0.5 | Epoch: 141 | Iter: 431000 | Total Loss: 0.003410 | Recon Loss: 0.002884 | Commit Loss: 0.001051 | Perplexity: 1216.805190
2025-10-19 17:11:53,051 Stage: Train 0.5 | Epoch: 141 | Iter: 431200 | Total Loss: 0.003559 | Recon Loss: 0.003027 | Commit Loss: 0.001063 | Perplexity: 1211.831117
Trainning Epoch:  86%|████████▌ | 142/165 [193:52:07<31:19:04, 4901.95s/it]Trainning Epoch:  86%|████████▌ | 142/165 [193:52:07<31:19:04, 4901.95s/it]2025-10-19 17:17:19,216 Stage: Train 0.5 | Epoch: 142 | Iter: 431400 | Total Loss: 0.003432 | Recon Loss: 0.002910 | Commit Loss: 0.001044 | Perplexity: 1206.489437
2025-10-19 17:22:40,636 Stage: Train 0.5 | Epoch: 142 | Iter: 431600 | Total Loss: 0.003449 | Recon Loss: 0.002927 | Commit Loss: 0.001043 | Perplexity: 1207.307979
2025-10-19 17:28:03,416 Stage: Train 0.5 | Epoch: 142 | Iter: 431800 | Total Loss: 0.003477 | Recon Loss: 0.002953 | Commit Loss: 0.001048 | Perplexity: 1206.343353
2025-10-19 17:33:25,658 Stage: Train 0.5 | Epoch: 142 | Iter: 432000 | Total Loss: 0.003439 | Recon Loss: 0.002915 | Commit Loss: 0.001049 | Perplexity: 1201.010235
2025-10-19 17:38:47,444 Stage: Train 0.5 | Epoch: 142 | Iter: 432200 | Total Loss: 0.003394 | Recon Loss: 0.002873 | Commit Loss: 0.001043 | Perplexity: 1214.006147
2025-10-19 17:44:08,499 Stage: Train 0.5 | Epoch: 142 | Iter: 432400 | Total Loss: 0.003469 | Recon Loss: 0.002943 | Commit Loss: 0.001051 | Perplexity: 1203.899941
2025-10-19 17:49:30,449 Stage: Train 0.5 | Epoch: 142 | Iter: 432600 | Total Loss: 0.003455 | Recon Loss: 0.002931 | Commit Loss: 0.001047 | Perplexity: 1202.617725
2025-10-19 17:54:53,187 Stage: Train 0.5 | Epoch: 142 | Iter: 432800 | Total Loss: 0.003504 | Recon Loss: 0.002968 | Commit Loss: 0.001072 | Perplexity: 1209.138507
2025-10-19 18:00:15,294 Stage: Train 0.5 | Epoch: 142 | Iter: 433000 | Total Loss: 0.003362 | Recon Loss: 0.002842 | Commit Loss: 0.001039 | Perplexity: 1204.103993
2025-10-19 18:05:37,038 Stage: Train 0.5 | Epoch: 142 | Iter: 433200 | Total Loss: 0.003406 | Recon Loss: 0.002883 | Commit Loss: 0.001046 | Perplexity: 1197.356586
2025-10-19 18:10:59,332 Stage: Train 0.5 | Epoch: 142 | Iter: 433400 | Total Loss: 0.003406 | Recon Loss: 0.002875 | Commit Loss: 0.001061 | Perplexity: 1211.977660
2025-10-19 18:16:20,581 Stage: Train 0.5 | Epoch: 142 | Iter: 433600 | Total Loss: 0.003443 | Recon Loss: 0.002912 | Commit Loss: 0.001062 | Perplexity: 1206.377034
2025-10-19 18:21:42,600 Stage: Train 0.5 | Epoch: 142 | Iter: 433800 | Total Loss: 0.003465 | Recon Loss: 0.002932 | Commit Loss: 0.001067 | Perplexity: 1203.760693
2025-10-19 18:27:04,250 Stage: Train 0.5 | Epoch: 142 | Iter: 434000 | Total Loss: 0.003448 | Recon Loss: 0.002918 | Commit Loss: 0.001059 | Perplexity: 1210.178269
2025-10-19 18:32:26,618 Stage: Train 0.5 | Epoch: 142 | Iter: 434200 | Total Loss: 0.003517 | Recon Loss: 0.002992 | Commit Loss: 0.001051 | Perplexity: 1210.543970
2025-10-19 18:37:49,498 Stage: Train 0.5 | Epoch: 142 | Iter: 434400 | Total Loss: 0.003370 | Recon Loss: 0.002849 | Commit Loss: 0.001041 | Perplexity: 1203.583275
Trainning Epoch:  87%|████████▋ | 143/165 [195:13:43<29:56:41, 4900.09s/it]Trainning Epoch:  87%|████████▋ | 143/165 [195:13:43<29:56:41, 4900.09s/it]2025-10-19 18:43:16,890 Stage: Train 0.5 | Epoch: 143 | Iter: 434600 | Total Loss: 0.003484 | Recon Loss: 0.002963 | Commit Loss: 0.001043 | Perplexity: 1202.800848
2025-10-19 18:48:39,411 Stage: Train 0.5 | Epoch: 143 | Iter: 434800 | Total Loss: 0.003418 | Recon Loss: 0.002891 | Commit Loss: 0.001054 | Perplexity: 1209.916337
2025-10-19 18:54:01,558 Stage: Train 0.5 | Epoch: 143 | Iter: 435000 | Total Loss: 0.003489 | Recon Loss: 0.002954 | Commit Loss: 0.001071 | Perplexity: 1208.705028
2025-10-19 18:59:24,362 Stage: Train 0.5 | Epoch: 143 | Iter: 435200 | Total Loss: 0.003422 | Recon Loss: 0.002898 | Commit Loss: 0.001048 | Perplexity: 1205.588033
2025-10-19 19:04:46,908 Stage: Train 0.5 | Epoch: 143 | Iter: 435400 | Total Loss: 0.003482 | Recon Loss: 0.002956 | Commit Loss: 0.001052 | Perplexity: 1208.161001
2025-10-19 19:10:08,959 Stage: Train 0.5 | Epoch: 143 | Iter: 435600 | Total Loss: 0.003427 | Recon Loss: 0.002897 | Commit Loss: 0.001059 | Perplexity: 1204.281939
2025-10-19 19:15:30,763 Stage: Train 0.5 | Epoch: 143 | Iter: 435800 | Total Loss: 0.003428 | Recon Loss: 0.002902 | Commit Loss: 0.001053 | Perplexity: 1205.508525
2025-10-19 19:20:53,130 Stage: Train 0.5 | Epoch: 143 | Iter: 436000 | Total Loss: 0.003444 | Recon Loss: 0.002920 | Commit Loss: 0.001048 | Perplexity: 1204.683416
2025-10-19 19:26:15,699 Stage: Train 0.5 | Epoch: 143 | Iter: 436200 | Total Loss: 0.003409 | Recon Loss: 0.002892 | Commit Loss: 0.001033 | Perplexity: 1205.912908
2025-10-19 19:31:37,263 Stage: Train 0.5 | Epoch: 143 | Iter: 436400 | Total Loss: 0.003531 | Recon Loss: 0.002996 | Commit Loss: 0.001070 | Perplexity: 1207.247371
2025-10-19 19:36:59,499 Stage: Train 0.5 | Epoch: 143 | Iter: 436600 | Total Loss: 0.003406 | Recon Loss: 0.002886 | Commit Loss: 0.001040 | Perplexity: 1200.137576
2025-10-19 19:42:21,061 Stage: Train 0.5 | Epoch: 143 | Iter: 436800 | Total Loss: 0.003462 | Recon Loss: 0.002944 | Commit Loss: 0.001036 | Perplexity: 1206.492636
2025-10-19 19:47:42,978 Stage: Train 0.5 | Epoch: 143 | Iter: 437000 | Total Loss: 0.003380 | Recon Loss: 0.002848 | Commit Loss: 0.001064 | Perplexity: 1209.298595
2025-10-19 19:53:04,858 Stage: Train 0.5 | Epoch: 143 | Iter: 437200 | Total Loss: 0.003451 | Recon Loss: 0.002927 | Commit Loss: 0.001049 | Perplexity: 1202.713229
2025-10-19 19:58:26,216 Stage: Train 0.5 | Epoch: 143 | Iter: 437400 | Total Loss: 0.003437 | Recon Loss: 0.002905 | Commit Loss: 0.001064 | Perplexity: 1211.399655
Trainning Epoch:  87%|████████▋ | 144/165 [196:35:21<28:34:48, 4899.46s/it]Trainning Epoch:  87%|████████▋ | 144/165 [196:35:21<28:34:48, 4899.46s/it]2025-10-19 20:03:52,956 Stage: Train 0.5 | Epoch: 144 | Iter: 437600 | Total Loss: 0.003439 | Recon Loss: 0.002918 | Commit Loss: 0.001040 | Perplexity: 1206.738288
2025-10-19 20:09:16,002 Stage: Train 0.5 | Epoch: 144 | Iter: 437800 | Total Loss: 0.003432 | Recon Loss: 0.002908 | Commit Loss: 0.001048 | Perplexity: 1209.926969
2025-10-19 20:14:38,535 Stage: Train 0.5 | Epoch: 144 | Iter: 438000 | Total Loss: 0.003359 | Recon Loss: 0.002831 | Commit Loss: 0.001056 | Perplexity: 1206.912652
2025-10-19 20:20:00,418 Stage: Train 0.5 | Epoch: 144 | Iter: 438200 | Total Loss: 0.003460 | Recon Loss: 0.002937 | Commit Loss: 0.001048 | Perplexity: 1211.566249
2025-10-19 20:25:23,462 Stage: Train 0.5 | Epoch: 144 | Iter: 438400 | Total Loss: 0.003362 | Recon Loss: 0.002836 | Commit Loss: 0.001051 | Perplexity: 1208.571790
2025-10-19 20:30:46,796 Stage: Train 0.5 | Epoch: 144 | Iter: 438600 | Total Loss: 0.003459 | Recon Loss: 0.002934 | Commit Loss: 0.001050 | Perplexity: 1202.539650
2025-10-19 20:36:09,699 Stage: Train 0.5 | Epoch: 144 | Iter: 438800 | Total Loss: 0.003464 | Recon Loss: 0.002936 | Commit Loss: 0.001056 | Perplexity: 1208.268286
2025-10-19 20:41:32,621 Stage: Train 0.5 | Epoch: 144 | Iter: 439000 | Total Loss: 0.003474 | Recon Loss: 0.002948 | Commit Loss: 0.001052 | Perplexity: 1211.056132
2025-10-19 20:46:55,884 Stage: Train 0.5 | Epoch: 144 | Iter: 439200 | Total Loss: 0.003470 | Recon Loss: 0.002952 | Commit Loss: 0.001037 | Perplexity: 1202.658391
2025-10-19 20:52:18,613 Stage: Train 0.5 | Epoch: 144 | Iter: 439400 | Total Loss: 0.003413 | Recon Loss: 0.002892 | Commit Loss: 0.001042 | Perplexity: 1203.137462
2025-10-19 20:57:41,621 Stage: Train 0.5 | Epoch: 144 | Iter: 439600 | Total Loss: 0.003451 | Recon Loss: 0.002933 | Commit Loss: 0.001035 | Perplexity: 1200.530826
2025-10-19 21:03:03,317 Stage: Train 0.5 | Epoch: 144 | Iter: 439800 | Total Loss: 0.003477 | Recon Loss: 0.002949 | Commit Loss: 0.001055 | Perplexity: 1210.980150
2025-10-19 21:08:25,754 Stage: Train 0.5 | Epoch: 144 | Iter: 440000 | Total Loss: 0.003401 | Recon Loss: 0.002876 | Commit Loss: 0.001051 | Perplexity: 1208.366601
2025-10-19 21:08:25,754 Saving model at iteration 440000
2025-10-19 21:08:25,915 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000
2025-10-19 21:08:27,294 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000/model.safetensors
2025-10-19 21:08:29,069 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000/optimizer.bin
2025-10-19 21:08:29,070 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000/scheduler.bin
2025-10-19 21:08:29,070 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000/sampler.bin
2025-10-19 21:08:29,071 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_145_step_440000/random_states_0.pkl
2025-10-19 21:13:51,698 Stage: Train 0.5 | Epoch: 144 | Iter: 440200 | Total Loss: 0.003459 | Recon Loss: 0.002941 | Commit Loss: 0.001035 | Perplexity: 1206.998764
2025-10-19 21:19:13,636 Stage: Train 0.5 | Epoch: 144 | Iter: 440400 | Total Loss: 0.003484 | Recon Loss: 0.002957 | Commit Loss: 0.001055 | Perplexity: 1203.998562
Trainning Epoch:  88%|████████▊ | 145/165 [197:57:09<27:14:02, 4902.10s/it]Trainning Epoch:  88%|████████▊ | 145/165 [197:57:09<27:14:02, 4902.10s/it]2025-10-19 21:24:40,494 Stage: Train 0.5 | Epoch: 145 | Iter: 440600 | Total Loss: 0.003433 | Recon Loss: 0.002915 | Commit Loss: 0.001035 | Perplexity: 1196.087910
2025-10-19 21:30:02,999 Stage: Train 0.5 | Epoch: 145 | Iter: 440800 | Total Loss: 0.003384 | Recon Loss: 0.002859 | Commit Loss: 0.001050 | Perplexity: 1205.241487
2025-10-19 21:35:25,958 Stage: Train 0.5 | Epoch: 145 | Iter: 441000 | Total Loss: 0.003420 | Recon Loss: 0.002892 | Commit Loss: 0.001055 | Perplexity: 1212.158417
2025-10-19 21:40:49,076 Stage: Train 0.5 | Epoch: 145 | Iter: 441200 | Total Loss: 0.003422 | Recon Loss: 0.002899 | Commit Loss: 0.001047 | Perplexity: 1208.930734
2025-10-19 21:46:11,636 Stage: Train 0.5 | Epoch: 145 | Iter: 441400 | Total Loss: 0.003480 | Recon Loss: 0.002960 | Commit Loss: 0.001038 | Perplexity: 1210.117383
2025-10-19 21:51:34,452 Stage: Train 0.5 | Epoch: 145 | Iter: 441600 | Total Loss: 0.003452 | Recon Loss: 0.002932 | Commit Loss: 0.001040 | Perplexity: 1204.346451
2025-10-19 21:56:57,436 Stage: Train 0.5 | Epoch: 145 | Iter: 441800 | Total Loss: 0.003435 | Recon Loss: 0.002910 | Commit Loss: 0.001050 | Perplexity: 1206.572910
2025-10-19 22:02:20,104 Stage: Train 0.5 | Epoch: 145 | Iter: 442000 | Total Loss: 0.003364 | Recon Loss: 0.002848 | Commit Loss: 0.001033 | Perplexity: 1205.566688
2025-10-19 22:07:41,308 Stage: Train 0.5 | Epoch: 145 | Iter: 442200 | Total Loss: 0.003469 | Recon Loss: 0.002946 | Commit Loss: 0.001046 | Perplexity: 1210.572274
2025-10-19 22:13:02,344 Stage: Train 0.5 | Epoch: 145 | Iter: 442400 | Total Loss: 0.003408 | Recon Loss: 0.002881 | Commit Loss: 0.001053 | Perplexity: 1207.973948
2025-10-19 22:18:23,205 Stage: Train 0.5 | Epoch: 145 | Iter: 442600 | Total Loss: 0.003437 | Recon Loss: 0.002913 | Commit Loss: 0.001048 | Perplexity: 1208.595917
2025-10-19 22:23:44,945 Stage: Train 0.5 | Epoch: 145 | Iter: 442800 | Total Loss: 0.003418 | Recon Loss: 0.002899 | Commit Loss: 0.001037 | Perplexity: 1207.960377
2025-10-19 22:29:07,823 Stage: Train 0.5 | Epoch: 145 | Iter: 443000 | Total Loss: 0.003501 | Recon Loss: 0.002972 | Commit Loss: 0.001059 | Perplexity: 1210.859420
2025-10-19 22:34:30,303 Stage: Train 0.5 | Epoch: 145 | Iter: 443200 | Total Loss: 0.003377 | Recon Loss: 0.002856 | Commit Loss: 0.001043 | Perplexity: 1213.264250
2025-10-19 22:39:53,363 Stage: Train 0.5 | Epoch: 145 | Iter: 443400 | Total Loss: 0.003512 | Recon Loss: 0.002985 | Commit Loss: 0.001054 | Perplexity: 1205.575783
Trainning Epoch:  88%|████████▊ | 146/165 [199:18:51<25:52:16, 4901.94s/it]Trainning Epoch:  88%|████████▊ | 146/165 [199:18:51<25:52:16, 4901.94s/it]2025-10-19 22:45:20,196 Stage: Train 0.5 | Epoch: 146 | Iter: 443600 | Total Loss: 0.003387 | Recon Loss: 0.002861 | Commit Loss: 0.001052 | Perplexity: 1204.690600
2025-10-19 22:50:42,813 Stage: Train 0.5 | Epoch: 146 | Iter: 443800 | Total Loss: 0.003445 | Recon Loss: 0.002919 | Commit Loss: 0.001051 | Perplexity: 1203.049768
2025-10-19 22:56:05,640 Stage: Train 0.5 | Epoch: 146 | Iter: 444000 | Total Loss: 0.003395 | Recon Loss: 0.002872 | Commit Loss: 0.001045 | Perplexity: 1215.238025
2025-10-19 23:01:28,574 Stage: Train 0.5 | Epoch: 146 | Iter: 444200 | Total Loss: 0.003442 | Recon Loss: 0.002912 | Commit Loss: 0.001060 | Perplexity: 1207.333533
2025-10-19 23:06:51,174 Stage: Train 0.5 | Epoch: 146 | Iter: 444400 | Total Loss: 0.003404 | Recon Loss: 0.002883 | Commit Loss: 0.001043 | Perplexity: 1213.051255
2025-10-19 23:12:12,925 Stage: Train 0.5 | Epoch: 146 | Iter: 444600 | Total Loss: 0.003424 | Recon Loss: 0.002899 | Commit Loss: 0.001050 | Perplexity: 1207.776492
2025-10-19 23:17:35,154 Stage: Train 0.5 | Epoch: 146 | Iter: 444800 | Total Loss: 0.003481 | Recon Loss: 0.002951 | Commit Loss: 0.001060 | Perplexity: 1210.241008
2025-10-19 23:22:57,699 Stage: Train 0.5 | Epoch: 146 | Iter: 445000 | Total Loss: 0.003390 | Recon Loss: 0.002866 | Commit Loss: 0.001047 | Perplexity: 1203.967317
2025-10-19 23:28:19,782 Stage: Train 0.5 | Epoch: 146 | Iter: 445200 | Total Loss: 0.003402 | Recon Loss: 0.002876 | Commit Loss: 0.001053 | Perplexity: 1206.556659
2025-10-19 23:33:41,904 Stage: Train 0.5 | Epoch: 146 | Iter: 445400 | Total Loss: 0.003419 | Recon Loss: 0.002896 | Commit Loss: 0.001046 | Perplexity: 1206.602771
2025-10-19 23:39:03,587 Stage: Train 0.5 | Epoch: 146 | Iter: 445600 | Total Loss: 0.003453 | Recon Loss: 0.002932 | Commit Loss: 0.001044 | Perplexity: 1201.789554
2025-10-19 23:44:25,906 Stage: Train 0.5 | Epoch: 146 | Iter: 445800 | Total Loss: 0.003374 | Recon Loss: 0.002851 | Commit Loss: 0.001046 | Perplexity: 1210.107580
2025-10-19 23:49:48,719 Stage: Train 0.5 | Epoch: 146 | Iter: 446000 | Total Loss: 0.003516 | Recon Loss: 0.002984 | Commit Loss: 0.001064 | Perplexity: 1205.058721
2025-10-19 23:55:11,520 Stage: Train 0.5 | Epoch: 146 | Iter: 446200 | Total Loss: 0.003306 | Recon Loss: 0.002780 | Commit Loss: 0.001052 | Perplexity: 1213.031067
2025-10-20 00:00:34,082 Stage: Train 0.5 | Epoch: 146 | Iter: 446400 | Total Loss: 0.003517 | Recon Loss: 0.002988 | Commit Loss: 0.001059 | Perplexity: 1208.949501
Trainning Epoch:  89%|████████▉ | 147/165 [200:40:32<24:30:33, 4901.86s/it]Trainning Epoch:  89%|████████▉ | 147/165 [200:40:33<24:30:33, 4901.86s/it]2025-10-20 00:06:00,686 Stage: Train 0.5 | Epoch: 147 | Iter: 446600 | Total Loss: 0.003451 | Recon Loss: 0.002912 | Commit Loss: 0.001079 | Perplexity: 1209.161586
2025-10-20 00:11:23,475 Stage: Train 0.5 | Epoch: 147 | Iter: 446800 | Total Loss: 0.003523 | Recon Loss: 0.002988 | Commit Loss: 0.001069 | Perplexity: 1209.176772
2025-10-20 00:16:46,667 Stage: Train 0.5 | Epoch: 147 | Iter: 447000 | Total Loss: 0.003376 | Recon Loss: 0.002845 | Commit Loss: 0.001062 | Perplexity: 1213.864474
2025-10-20 00:22:09,856 Stage: Train 0.5 | Epoch: 147 | Iter: 447200 | Total Loss: 0.003450 | Recon Loss: 0.002921 | Commit Loss: 0.001058 | Perplexity: 1203.359768
2025-10-20 00:27:32,651 Stage: Train 0.5 | Epoch: 147 | Iter: 447400 | Total Loss: 0.003445 | Recon Loss: 0.002921 | Commit Loss: 0.001049 | Perplexity: 1207.841217
2025-10-20 00:32:55,589 Stage: Train 0.5 | Epoch: 147 | Iter: 447600 | Total Loss: 0.003521 | Recon Loss: 0.002991 | Commit Loss: 0.001061 | Perplexity: 1206.823740
2025-10-20 00:38:18,590 Stage: Train 0.5 | Epoch: 147 | Iter: 447800 | Total Loss: 0.003365 | Recon Loss: 0.002841 | Commit Loss: 0.001046 | Perplexity: 1219.146897
2025-10-20 00:43:41,216 Stage: Train 0.5 | Epoch: 147 | Iter: 448000 | Total Loss: 0.003352 | Recon Loss: 0.002839 | Commit Loss: 0.001026 | Perplexity: 1207.779078
2025-10-20 00:49:04,286 Stage: Train 0.5 | Epoch: 147 | Iter: 448200 | Total Loss: 0.003362 | Recon Loss: 0.002831 | Commit Loss: 0.001061 | Perplexity: 1207.121870
2025-10-20 00:54:27,222 Stage: Train 0.5 | Epoch: 147 | Iter: 448400 | Total Loss: 0.003436 | Recon Loss: 0.002901 | Commit Loss: 0.001070 | Perplexity: 1214.471254
2025-10-20 00:59:50,191 Stage: Train 0.5 | Epoch: 147 | Iter: 448600 | Total Loss: 0.003483 | Recon Loss: 0.002956 | Commit Loss: 0.001054 | Perplexity: 1210.595969
2025-10-20 01:05:12,745 Stage: Train 0.5 | Epoch: 147 | Iter: 448800 | Total Loss: 0.003372 | Recon Loss: 0.002851 | Commit Loss: 0.001043 | Perplexity: 1207.075038
2025-10-20 01:10:35,245 Stage: Train 0.5 | Epoch: 147 | Iter: 449000 | Total Loss: 0.003343 | Recon Loss: 0.002826 | Commit Loss: 0.001036 | Perplexity: 1210.897849
2025-10-20 01:15:57,525 Stage: Train 0.5 | Epoch: 147 | Iter: 449200 | Total Loss: 0.003380 | Recon Loss: 0.002865 | Commit Loss: 0.001031 | Perplexity: 1202.580006
2025-10-20 01:21:19,559 Stage: Train 0.5 | Epoch: 147 | Iter: 449400 | Total Loss: 0.003449 | Recon Loss: 0.002921 | Commit Loss: 0.001057 | Perplexity: 1213.458560
2025-10-20 01:26:41,733 Stage: Train 0.5 | Epoch: 147 | Iter: 449600 | Total Loss: 0.003367 | Recon Loss: 0.002838 | Commit Loss: 0.001057 | Perplexity: 1208.520308
Trainning Epoch:  90%|████████▉ | 148/165 [202:02:19<23:09:15, 4903.27s/it]Trainning Epoch:  90%|████████▉ | 148/165 [202:02:19<23:09:15, 4903.27s/it]2025-10-20 01:32:08,570 Stage: Train 0.5 | Epoch: 148 | Iter: 449800 | Total Loss: 0.003452 | Recon Loss: 0.002930 | Commit Loss: 0.001044 | Perplexity: 1208.248994
2025-10-20 01:37:31,539 Stage: Train 0.5 | Epoch: 148 | Iter: 450000 | Total Loss: 0.003421 | Recon Loss: 0.002895 | Commit Loss: 0.001054 | Perplexity: 1211.695040
2025-10-20 01:42:54,375 Stage: Train 0.5 | Epoch: 148 | Iter: 450200 | Total Loss: 0.003418 | Recon Loss: 0.002896 | Commit Loss: 0.001045 | Perplexity: 1207.370930
2025-10-20 01:48:17,002 Stage: Train 0.5 | Epoch: 148 | Iter: 450400 | Total Loss: 0.003451 | Recon Loss: 0.002936 | Commit Loss: 0.001028 | Perplexity: 1204.922452
2025-10-20 01:53:39,835 Stage: Train 0.5 | Epoch: 148 | Iter: 450600 | Total Loss: 0.003424 | Recon Loss: 0.002900 | Commit Loss: 0.001049 | Perplexity: 1210.149717
2025-10-20 01:59:02,784 Stage: Train 0.5 | Epoch: 148 | Iter: 450800 | Total Loss: 0.003391 | Recon Loss: 0.002867 | Commit Loss: 0.001048 | Perplexity: 1209.236302
2025-10-20 02:04:25,326 Stage: Train 0.5 | Epoch: 148 | Iter: 451000 | Total Loss: 0.003382 | Recon Loss: 0.002855 | Commit Loss: 0.001054 | Perplexity: 1211.145478
2025-10-20 02:09:47,612 Stage: Train 0.5 | Epoch: 148 | Iter: 451200 | Total Loss: 0.003373 | Recon Loss: 0.002853 | Commit Loss: 0.001040 | Perplexity: 1204.831690
2025-10-20 02:15:09,990 Stage: Train 0.5 | Epoch: 148 | Iter: 451400 | Total Loss: 0.003450 | Recon Loss: 0.002918 | Commit Loss: 0.001064 | Perplexity: 1215.879450
2025-10-20 02:20:31,602 Stage: Train 0.5 | Epoch: 148 | Iter: 451600 | Total Loss: 0.003421 | Recon Loss: 0.002894 | Commit Loss: 0.001054 | Perplexity: 1209.534028
2025-10-20 02:25:54,566 Stage: Train 0.5 | Epoch: 148 | Iter: 451800 | Total Loss: 0.003366 | Recon Loss: 0.002851 | Commit Loss: 0.001029 | Perplexity: 1213.522273
2025-10-20 02:31:17,652 Stage: Train 0.5 | Epoch: 148 | Iter: 452000 | Total Loss: 0.003441 | Recon Loss: 0.002920 | Commit Loss: 0.001043 | Perplexity: 1218.730122
2025-10-20 02:36:40,570 Stage: Train 0.5 | Epoch: 148 | Iter: 452200 | Total Loss: 0.003420 | Recon Loss: 0.002885 | Commit Loss: 0.001070 | Perplexity: 1205.458105
2025-10-20 02:42:03,478 Stage: Train 0.5 | Epoch: 148 | Iter: 452400 | Total Loss: 0.003393 | Recon Loss: 0.002865 | Commit Loss: 0.001057 | Perplexity: 1211.933614
2025-10-20 02:47:26,416 Stage: Train 0.5 | Epoch: 148 | Iter: 452600 | Total Loss: 0.003395 | Recon Loss: 0.002865 | Commit Loss: 0.001061 | Perplexity: 1214.929631
Trainning Epoch:  90%|█████████ | 149/165 [203:24:05<21:47:45, 4904.08s/it]Trainning Epoch:  90%|█████████ | 149/165 [203:24:05<21:47:45, 4904.08s/it]2025-10-20 02:52:52,854 Stage: Train 0.5 | Epoch: 149 | Iter: 452800 | Total Loss: 0.003296 | Recon Loss: 0.002773 | Commit Loss: 0.001044 | Perplexity: 1207.454869
2025-10-20 02:58:15,843 Stage: Train 0.5 | Epoch: 149 | Iter: 453000 | Total Loss: 0.003420 | Recon Loss: 0.002891 | Commit Loss: 0.001059 | Perplexity: 1214.942142
2025-10-20 03:03:38,481 Stage: Train 0.5 | Epoch: 149 | Iter: 453200 | Total Loss: 0.003474 | Recon Loss: 0.002948 | Commit Loss: 0.001052 | Perplexity: 1210.988471
2025-10-20 03:09:01,498 Stage: Train 0.5 | Epoch: 149 | Iter: 453400 | Total Loss: 0.003445 | Recon Loss: 0.002924 | Commit Loss: 0.001043 | Perplexity: 1206.253052
2025-10-20 03:14:24,352 Stage: Train 0.5 | Epoch: 149 | Iter: 453600 | Total Loss: 0.003381 | Recon Loss: 0.002860 | Commit Loss: 0.001042 | Perplexity: 1205.054767
2025-10-20 03:19:47,190 Stage: Train 0.5 | Epoch: 149 | Iter: 453800 | Total Loss: 0.003441 | Recon Loss: 0.002924 | Commit Loss: 0.001033 | Perplexity: 1197.919676
2025-10-20 03:25:09,833 Stage: Train 0.5 | Epoch: 149 | Iter: 454000 | Total Loss: 0.003361 | Recon Loss: 0.002838 | Commit Loss: 0.001045 | Perplexity: 1208.053781
2025-10-20 03:30:32,158 Stage: Train 0.5 | Epoch: 149 | Iter: 454200 | Total Loss: 0.003453 | Recon Loss: 0.002925 | Commit Loss: 0.001058 | Perplexity: 1210.538675
2025-10-20 03:35:55,452 Stage: Train 0.5 | Epoch: 149 | Iter: 454400 | Total Loss: 0.003332 | Recon Loss: 0.002813 | Commit Loss: 0.001038 | Perplexity: 1208.744531
2025-10-20 03:41:18,558 Stage: Train 0.5 | Epoch: 149 | Iter: 454600 | Total Loss: 0.003397 | Recon Loss: 0.002876 | Commit Loss: 0.001042 | Perplexity: 1206.970316
2025-10-20 03:46:39,760 Stage: Train 0.5 | Epoch: 149 | Iter: 454800 | Total Loss: 0.003425 | Recon Loss: 0.002894 | Commit Loss: 0.001062 | Perplexity: 1213.091551
2025-10-20 03:52:00,949 Stage: Train 0.5 | Epoch: 149 | Iter: 455000 | Total Loss: 0.003437 | Recon Loss: 0.002920 | Commit Loss: 0.001034 | Perplexity: 1213.043316
2025-10-20 03:57:23,584 Stage: Train 0.5 | Epoch: 149 | Iter: 455200 | Total Loss: 0.003372 | Recon Loss: 0.002846 | Commit Loss: 0.001054 | Perplexity: 1212.570535
2025-10-20 04:02:45,954 Stage: Train 0.5 | Epoch: 149 | Iter: 455400 | Total Loss: 0.003429 | Recon Loss: 0.002905 | Commit Loss: 0.001048 | Perplexity: 1208.492757
2025-10-20 04:08:07,942 Stage: Train 0.5 | Epoch: 149 | Iter: 455600 | Total Loss: 0.003394 | Recon Loss: 0.002868 | Commit Loss: 0.001051 | Perplexity: 1205.523578
Trainning Epoch:  91%|█████████ | 150/165 [204:45:48<20:25:55, 4903.69s/it]Trainning Epoch:  91%|█████████ | 150/165 [204:45:48<20:25:55, 4903.69s/it]2025-10-20 04:13:34,631 Stage: Train 0.5 | Epoch: 150 | Iter: 455800 | Total Loss: 0.003360 | Recon Loss: 0.002845 | Commit Loss: 0.001029 | Perplexity: 1205.485320
2025-10-20 04:18:56,979 Stage: Train 0.5 | Epoch: 150 | Iter: 456000 | Total Loss: 0.003381 | Recon Loss: 0.002858 | Commit Loss: 0.001046 | Perplexity: 1202.083807
2025-10-20 04:24:19,823 Stage: Train 0.5 | Epoch: 150 | Iter: 456200 | Total Loss: 0.003406 | Recon Loss: 0.002888 | Commit Loss: 0.001036 | Perplexity: 1208.122790
2025-10-20 04:29:42,751 Stage: Train 0.5 | Epoch: 150 | Iter: 456400 | Total Loss: 0.003446 | Recon Loss: 0.002917 | Commit Loss: 0.001058 | Perplexity: 1212.190443
2025-10-20 04:35:04,938 Stage: Train 0.5 | Epoch: 150 | Iter: 456600 | Total Loss: 0.003385 | Recon Loss: 0.002866 | Commit Loss: 0.001038 | Perplexity: 1205.186709
2025-10-20 04:40:26,473 Stage: Train 0.5 | Epoch: 150 | Iter: 456800 | Total Loss: 0.003398 | Recon Loss: 0.002875 | Commit Loss: 0.001044 | Perplexity: 1217.632879
2025-10-20 04:45:48,613 Stage: Train 0.5 | Epoch: 150 | Iter: 457000 | Total Loss: 0.003368 | Recon Loss: 0.002844 | Commit Loss: 0.001049 | Perplexity: 1206.639081
2025-10-20 04:51:11,411 Stage: Train 0.5 | Epoch: 150 | Iter: 457200 | Total Loss: 0.003433 | Recon Loss: 0.002908 | Commit Loss: 0.001051 | Perplexity: 1209.008886
2025-10-20 04:56:34,254 Stage: Train 0.5 | Epoch: 150 | Iter: 457400 | Total Loss: 0.003452 | Recon Loss: 0.002930 | Commit Loss: 0.001045 | Perplexity: 1207.762527
2025-10-20 05:01:57,029 Stage: Train 0.5 | Epoch: 150 | Iter: 457600 | Total Loss: 0.003354 | Recon Loss: 0.002838 | Commit Loss: 0.001031 | Perplexity: 1202.563445
2025-10-20 05:07:19,898 Stage: Train 0.5 | Epoch: 150 | Iter: 457800 | Total Loss: 0.003428 | Recon Loss: 0.002900 | Commit Loss: 0.001055 | Perplexity: 1215.408428
2025-10-20 05:12:42,651 Stage: Train 0.5 | Epoch: 150 | Iter: 458000 | Total Loss: 0.003356 | Recon Loss: 0.002839 | Commit Loss: 0.001035 | Perplexity: 1214.729711
2025-10-20 05:18:05,179 Stage: Train 0.5 | Epoch: 150 | Iter: 458200 | Total Loss: 0.003340 | Recon Loss: 0.002819 | Commit Loss: 0.001043 | Perplexity: 1213.052598
2025-10-20 05:23:27,892 Stage: Train 0.5 | Epoch: 150 | Iter: 458400 | Total Loss: 0.003410 | Recon Loss: 0.002888 | Commit Loss: 0.001045 | Perplexity: 1211.180048
2025-10-20 05:28:50,859 Stage: Train 0.5 | Epoch: 150 | Iter: 458600 | Total Loss: 0.003458 | Recon Loss: 0.002926 | Commit Loss: 0.001064 | Perplexity: 1204.166204
Trainning Epoch:  92%|█████████▏| 151/165 [206:07:32<19:04:14, 4903.93s/it]Trainning Epoch:  92%|█████████▏| 151/165 [206:07:32<19:04:14, 4903.93s/it]2025-10-20 05:34:17,868 Stage: Train 0.5 | Epoch: 151 | Iter: 458800 | Total Loss: 0.003362 | Recon Loss: 0.002843 | Commit Loss: 0.001037 | Perplexity: 1196.409305
2025-10-20 05:39:39,757 Stage: Train 0.5 | Epoch: 151 | Iter: 459000 | Total Loss: 0.003366 | Recon Loss: 0.002847 | Commit Loss: 0.001039 | Perplexity: 1209.576198
2025-10-20 05:45:01,865 Stage: Train 0.5 | Epoch: 151 | Iter: 459200 | Total Loss: 0.003414 | Recon Loss: 0.002884 | Commit Loss: 0.001060 | Perplexity: 1209.903540
2025-10-20 05:50:23,949 Stage: Train 0.5 | Epoch: 151 | Iter: 459400 | Total Loss: 0.003413 | Recon Loss: 0.002891 | Commit Loss: 0.001044 | Perplexity: 1208.152272
2025-10-20 05:55:46,203 Stage: Train 0.5 | Epoch: 151 | Iter: 459600 | Total Loss: 0.003396 | Recon Loss: 0.002871 | Commit Loss: 0.001049 | Perplexity: 1212.446818
2025-10-20 06:01:08,501 Stage: Train 0.5 | Epoch: 151 | Iter: 459800 | Total Loss: 0.003393 | Recon Loss: 0.002869 | Commit Loss: 0.001048 | Perplexity: 1212.034243
2025-10-20 06:06:31,204 Stage: Train 0.5 | Epoch: 151 | Iter: 460000 | Total Loss: 0.003435 | Recon Loss: 0.002919 | Commit Loss: 0.001032 | Perplexity: 1199.776022
2025-10-20 06:06:31,205 Saving model at iteration 460000
2025-10-20 06:06:31,362 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000
2025-10-20 06:06:32,724 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000/model.safetensors
2025-10-20 06:06:34,422 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000/optimizer.bin
2025-10-20 06:06:34,422 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000/scheduler.bin
2025-10-20 06:06:34,422 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000/sampler.bin
2025-10-20 06:06:34,423 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_152_step_460000/random_states_0.pkl
2025-10-20 06:11:57,366 Stage: Train 0.5 | Epoch: 151 | Iter: 460200 | Total Loss: 0.003370 | Recon Loss: 0.002841 | Commit Loss: 0.001058 | Perplexity: 1210.553023
2025-10-20 06:17:18,950 Stage: Train 0.5 | Epoch: 151 | Iter: 460400 | Total Loss: 0.003414 | Recon Loss: 0.002885 | Commit Loss: 0.001057 | Perplexity: 1206.367637
2025-10-20 06:22:40,718 Stage: Train 0.5 | Epoch: 151 | Iter: 460600 | Total Loss: 0.003330 | Recon Loss: 0.002811 | Commit Loss: 0.001038 | Perplexity: 1207.440332
2025-10-20 06:28:03,325 Stage: Train 0.5 | Epoch: 151 | Iter: 460800 | Total Loss: 0.003464 | Recon Loss: 0.002923 | Commit Loss: 0.001082 | Perplexity: 1215.802673
2025-10-20 06:33:26,262 Stage: Train 0.5 | Epoch: 151 | Iter: 461000 | Total Loss: 0.003477 | Recon Loss: 0.002949 | Commit Loss: 0.001056 | Perplexity: 1208.836979
2025-10-20 06:38:48,866 Stage: Train 0.5 | Epoch: 151 | Iter: 461200 | Total Loss: 0.003367 | Recon Loss: 0.002837 | Commit Loss: 0.001059 | Perplexity: 1210.589073
2025-10-20 06:44:11,857 Stage: Train 0.5 | Epoch: 151 | Iter: 461400 | Total Loss: 0.003375 | Recon Loss: 0.002853 | Commit Loss: 0.001044 | Perplexity: 1206.702395
2025-10-20 06:49:33,562 Stage: Train 0.5 | Epoch: 151 | Iter: 461600 | Total Loss: 0.003506 | Recon Loss: 0.002978 | Commit Loss: 0.001055 | Perplexity: 1202.893065
Trainning Epoch:  92%|█████████▏| 152/165 [207:29:15<17:42:25, 4903.48s/it]Trainning Epoch:  92%|█████████▏| 152/165 [207:29:15<17:42:25, 4903.49s/it]2025-10-20 06:54:59,148 Stage: Train 0.5 | Epoch: 152 | Iter: 461800 | Total Loss: 0.003289 | Recon Loss: 0.002770 | Commit Loss: 0.001037 | Perplexity: 1205.277352
2025-10-20 07:00:21,933 Stage: Train 0.5 | Epoch: 152 | Iter: 462000 | Total Loss: 0.003376 | Recon Loss: 0.002854 | Commit Loss: 0.001043 | Perplexity: 1206.903779
2025-10-20 07:05:44,500 Stage: Train 0.5 | Epoch: 152 | Iter: 462200 | Total Loss: 0.003454 | Recon Loss: 0.002914 | Commit Loss: 0.001080 | Perplexity: 1211.692623
2025-10-20 07:11:07,002 Stage: Train 0.5 | Epoch: 152 | Iter: 462400 | Total Loss: 0.003454 | Recon Loss: 0.002932 | Commit Loss: 0.001044 | Perplexity: 1204.818967
2025-10-20 07:16:30,140 Stage: Train 0.5 | Epoch: 152 | Iter: 462600 | Total Loss: 0.003334 | Recon Loss: 0.002817 | Commit Loss: 0.001034 | Perplexity: 1201.998215
2025-10-20 07:21:53,039 Stage: Train 0.5 | Epoch: 152 | Iter: 462800 | Total Loss: 0.004131 | Recon Loss: 0.003467 | Commit Loss: 0.001327 | Perplexity: 1216.706201
2025-10-20 07:27:15,857 Stage: Train 0.5 | Epoch: 152 | Iter: 463000 | Total Loss: 0.003388 | Recon Loss: 0.002878 | Commit Loss: 0.001021 | Perplexity: 1204.057952
2025-10-20 07:32:38,788 Stage: Train 0.5 | Epoch: 152 | Iter: 463200 | Total Loss: 0.004038 | Recon Loss: 0.003379 | Commit Loss: 0.001317 | Perplexity: 1205.250508
2025-10-20 07:38:01,529 Stage: Train 0.5 | Epoch: 152 | Iter: 463400 | Total Loss: 0.003329 | Recon Loss: 0.002814 | Commit Loss: 0.001029 | Perplexity: 1197.331448
2025-10-20 07:43:23,763 Stage: Train 0.5 | Epoch: 152 | Iter: 463600 | Total Loss: 0.003445 | Recon Loss: 0.002942 | Commit Loss: 0.001005 | Perplexity: 1206.226853
2025-10-20 07:48:46,873 Stage: Train 0.5 | Epoch: 152 | Iter: 463800 | Total Loss: 0.003372 | Recon Loss: 0.002860 | Commit Loss: 0.001026 | Perplexity: 1210.536901
2025-10-20 07:54:09,072 Stage: Train 0.5 | Epoch: 152 | Iter: 464000 | Total Loss: 0.003383 | Recon Loss: 0.002863 | Commit Loss: 0.001039 | Perplexity: 1212.775399
2025-10-20 07:59:31,098 Stage: Train 0.5 | Epoch: 152 | Iter: 464200 | Total Loss: 0.003424 | Recon Loss: 0.002915 | Commit Loss: 0.001018 | Perplexity: 1207.358836
2025-10-20 08:04:52,527 Stage: Train 0.5 | Epoch: 152 | Iter: 464400 | Total Loss: 0.003390 | Recon Loss: 0.002879 | Commit Loss: 0.001021 | Perplexity: 1206.839056
2025-10-20 08:10:15,102 Stage: Train 0.5 | Epoch: 152 | Iter: 464600 | Total Loss: 0.003392 | Recon Loss: 0.002878 | Commit Loss: 0.001029 | Perplexity: 1209.142815
2025-10-20 08:15:37,023 Stage: Train 0.5 | Epoch: 152 | Iter: 464800 | Total Loss: 0.003357 | Recon Loss: 0.002841 | Commit Loss: 0.001032 | Perplexity: 1208.704832
Trainning Epoch:  93%|█████████▎| 153/165 [208:50:58<16:20:41, 4903.49s/it]Trainning Epoch:  93%|█████████▎| 153/165 [208:50:58<16:20:41, 4903.49s/it]2025-10-20 08:21:03,847 Stage: Train 0.5 | Epoch: 153 | Iter: 465000 | Total Loss: 0.003373 | Recon Loss: 0.002860 | Commit Loss: 0.001027 | Perplexity: 1204.116149
2025-10-20 08:26:26,748 Stage: Train 0.5 | Epoch: 153 | Iter: 465200 | Total Loss: 0.003364 | Recon Loss: 0.002846 | Commit Loss: 0.001037 | Perplexity: 1208.564909
2025-10-20 08:31:49,411 Stage: Train 0.5 | Epoch: 153 | Iter: 465400 | Total Loss: 0.003335 | Recon Loss: 0.002816 | Commit Loss: 0.001038 | Perplexity: 1213.723331
2025-10-20 08:37:12,120 Stage: Train 0.5 | Epoch: 153 | Iter: 465600 | Total Loss: 0.003414 | Recon Loss: 0.002892 | Commit Loss: 0.001044 | Perplexity: 1209.219309
2025-10-20 08:42:35,105 Stage: Train 0.5 | Epoch: 153 | Iter: 465800 | Total Loss: 0.003356 | Recon Loss: 0.002834 | Commit Loss: 0.001043 | Perplexity: 1213.068725
2025-10-20 08:47:57,966 Stage: Train 0.5 | Epoch: 153 | Iter: 466000 | Total Loss: 0.003365 | Recon Loss: 0.002845 | Commit Loss: 0.001040 | Perplexity: 1205.982169
2025-10-20 08:53:20,205 Stage: Train 0.5 | Epoch: 153 | Iter: 466200 | Total Loss: 0.003365 | Recon Loss: 0.002850 | Commit Loss: 0.001031 | Perplexity: 1219.751051
2025-10-20 08:58:43,160 Stage: Train 0.5 | Epoch: 153 | Iter: 466400 | Total Loss: 0.003416 | Recon Loss: 0.002891 | Commit Loss: 0.001050 | Perplexity: 1217.575054
2025-10-20 09:04:05,431 Stage: Train 0.5 | Epoch: 153 | Iter: 466600 | Total Loss: 0.003316 | Recon Loss: 0.002794 | Commit Loss: 0.001043 | Perplexity: 1212.946684
2025-10-20 09:09:28,169 Stage: Train 0.5 | Epoch: 153 | Iter: 466800 | Total Loss: 0.003403 | Recon Loss: 0.002879 | Commit Loss: 0.001047 | Perplexity: 1214.727871
2025-10-20 09:14:49,344 Stage: Train 0.5 | Epoch: 153 | Iter: 467000 | Total Loss: 0.003355 | Recon Loss: 0.002838 | Commit Loss: 0.001035 | Perplexity: 1206.084055
2025-10-20 09:20:11,063 Stage: Train 0.5 | Epoch: 153 | Iter: 467200 | Total Loss: 0.003415 | Recon Loss: 0.002891 | Commit Loss: 0.001047 | Perplexity: 1208.082043
2025-10-20 09:25:33,037 Stage: Train 0.5 | Epoch: 153 | Iter: 467400 | Total Loss: 0.003332 | Recon Loss: 0.002813 | Commit Loss: 0.001039 | Perplexity: 1214.439210
2025-10-20 09:30:55,059 Stage: Train 0.5 | Epoch: 153 | Iter: 467600 | Total Loss: 0.003361 | Recon Loss: 0.002837 | Commit Loss: 0.001049 | Perplexity: 1219.918901
2025-10-20 09:36:17,648 Stage: Train 0.5 | Epoch: 153 | Iter: 467800 | Total Loss: 0.003469 | Recon Loss: 0.002946 | Commit Loss: 0.001046 | Perplexity: 1218.603193
Trainning Epoch:  93%|█████████▎| 154/165 [210:12:40<14:58:52, 4902.98s/it]Trainning Epoch:  93%|█████████▎| 154/165 [210:12:40<14:58:52, 4902.98s/it]2025-10-20 09:41:43,881 Stage: Train 0.5 | Epoch: 154 | Iter: 468000 | Total Loss: 0.003328 | Recon Loss: 0.002819 | Commit Loss: 0.001019 | Perplexity: 1207.348851
2025-10-20 09:47:06,032 Stage: Train 0.5 | Epoch: 154 | Iter: 468200 | Total Loss: 0.003348 | Recon Loss: 0.002830 | Commit Loss: 0.001035 | Perplexity: 1217.094876
2025-10-20 09:52:28,646 Stage: Train 0.5 | Epoch: 154 | Iter: 468400 | Total Loss: 0.003410 | Recon Loss: 0.002896 | Commit Loss: 0.001029 | Perplexity: 1210.938150
2025-10-20 09:57:50,463 Stage: Train 0.5 | Epoch: 154 | Iter: 468600 | Total Loss: 0.003414 | Recon Loss: 0.002892 | Commit Loss: 0.001043 | Perplexity: 1213.354046
2025-10-20 10:03:13,149 Stage: Train 0.5 | Epoch: 154 | Iter: 468800 | Total Loss: 0.003342 | Recon Loss: 0.002814 | Commit Loss: 0.001055 | Perplexity: 1220.129609
2025-10-20 10:08:36,059 Stage: Train 0.5 | Epoch: 154 | Iter: 469000 | Total Loss: 0.003378 | Recon Loss: 0.002859 | Commit Loss: 0.001039 | Perplexity: 1216.981720
2025-10-20 10:13:58,953 Stage: Train 0.5 | Epoch: 154 | Iter: 469200 | Total Loss: 0.003327 | Recon Loss: 0.002817 | Commit Loss: 0.001019 | Perplexity: 1205.197388
2025-10-20 10:19:20,883 Stage: Train 0.5 | Epoch: 154 | Iter: 469400 | Total Loss: 0.003419 | Recon Loss: 0.002902 | Commit Loss: 0.001034 | Perplexity: 1209.887136
2025-10-20 10:24:42,540 Stage: Train 0.5 | Epoch: 154 | Iter: 469600 | Total Loss: 0.003286 | Recon Loss: 0.002769 | Commit Loss: 0.001034 | Perplexity: 1209.249250
2025-10-20 10:30:04,173 Stage: Train 0.5 | Epoch: 154 | Iter: 469800 | Total Loss: 0.003401 | Recon Loss: 0.002872 | Commit Loss: 0.001058 | Perplexity: 1214.586823
2025-10-20 10:35:25,849 Stage: Train 0.5 | Epoch: 154 | Iter: 470000 | Total Loss: 0.003403 | Recon Loss: 0.002875 | Commit Loss: 0.001055 | Perplexity: 1212.005788
2025-10-20 10:40:47,368 Stage: Train 0.5 | Epoch: 154 | Iter: 470200 | Total Loss: 0.003373 | Recon Loss: 0.002850 | Commit Loss: 0.001046 | Perplexity: 1210.436862
2025-10-20 10:46:09,029 Stage: Train 0.5 | Epoch: 154 | Iter: 470400 | Total Loss: 0.003378 | Recon Loss: 0.002846 | Commit Loss: 0.001064 | Perplexity: 1219.144973
2025-10-20 10:51:31,886 Stage: Train 0.5 | Epoch: 154 | Iter: 470600 | Total Loss: 0.003431 | Recon Loss: 0.002910 | Commit Loss: 0.001043 | Perplexity: 1219.288361
2025-10-20 10:56:54,172 Stage: Train 0.5 | Epoch: 154 | Iter: 470800 | Total Loss: 0.003392 | Recon Loss: 0.002873 | Commit Loss: 0.001039 | Perplexity: 1212.690115
Trainning Epoch:  94%|█████████▍| 155/165 [211:34:18<13:36:54, 4901.46s/it]Trainning Epoch:  94%|█████████▍| 155/165 [211:34:18<13:36:54, 4901.47s/it]2025-10-20 11:02:21,198 Stage: Train 0.5 | Epoch: 155 | Iter: 471000 | Total Loss: 0.003379 | Recon Loss: 0.002855 | Commit Loss: 0.001050 | Perplexity: 1216.483882
2025-10-20 11:07:44,183 Stage: Train 0.5 | Epoch: 155 | Iter: 471200 | Total Loss: 0.003372 | Recon Loss: 0.002846 | Commit Loss: 0.001051 | Perplexity: 1207.860459
2025-10-20 11:13:06,605 Stage: Train 0.5 | Epoch: 155 | Iter: 471400 | Total Loss: 0.003468 | Recon Loss: 0.002941 | Commit Loss: 0.001055 | Perplexity: 1207.133935
2025-10-20 11:18:28,691 Stage: Train 0.5 | Epoch: 155 | Iter: 471600 | Total Loss: 0.003273 | Recon Loss: 0.002750 | Commit Loss: 0.001048 | Perplexity: 1208.798897
2025-10-20 11:23:51,844 Stage: Train 0.5 | Epoch: 155 | Iter: 471800 | Total Loss: 0.003348 | Recon Loss: 0.002824 | Commit Loss: 0.001049 | Perplexity: 1215.905323
2025-10-20 11:29:13,797 Stage: Train 0.5 | Epoch: 155 | Iter: 472000 | Total Loss: 0.003426 | Recon Loss: 0.002903 | Commit Loss: 0.001045 | Perplexity: 1218.598107
2025-10-20 11:34:34,568 Stage: Train 0.5 | Epoch: 155 | Iter: 472200 | Total Loss: 0.003293 | Recon Loss: 0.002778 | Commit Loss: 0.001031 | Perplexity: 1203.413546
2025-10-20 11:39:55,973 Stage: Train 0.5 | Epoch: 155 | Iter: 472400 | Total Loss: 0.003412 | Recon Loss: 0.002886 | Commit Loss: 0.001052 | Perplexity: 1217.146815
2025-10-20 11:45:18,325 Stage: Train 0.5 | Epoch: 155 | Iter: 472600 | Total Loss: 0.003360 | Recon Loss: 0.002846 | Commit Loss: 0.001029 | Perplexity: 1215.251636
2025-10-20 11:50:41,177 Stage: Train 0.5 | Epoch: 155 | Iter: 472800 | Total Loss: 0.003309 | Recon Loss: 0.002795 | Commit Loss: 0.001029 | Perplexity: 1211.442517
2025-10-20 11:56:04,005 Stage: Train 0.5 | Epoch: 155 | Iter: 473000 | Total Loss: 0.003401 | Recon Loss: 0.002871 | Commit Loss: 0.001061 | Perplexity: 1219.854280
2025-10-20 12:01:26,748 Stage: Train 0.5 | Epoch: 155 | Iter: 473200 | Total Loss: 0.003398 | Recon Loss: 0.002871 | Commit Loss: 0.001054 | Perplexity: 1216.835588
2025-10-20 12:06:49,091 Stage: Train 0.5 | Epoch: 155 | Iter: 473400 | Total Loss: 0.003449 | Recon Loss: 0.002925 | Commit Loss: 0.001048 | Perplexity: 1211.308334
2025-10-20 12:12:11,744 Stage: Train 0.5 | Epoch: 155 | Iter: 473600 | Total Loss: 0.003341 | Recon Loss: 0.002824 | Commit Loss: 0.001034 | Perplexity: 1202.716855
2025-10-20 12:17:34,497 Stage: Train 0.5 | Epoch: 155 | Iter: 473800 | Total Loss: 0.003324 | Recon Loss: 0.002807 | Commit Loss: 0.001035 | Perplexity: 1211.388588
Trainning Epoch:  95%|█████████▍| 156/165 [212:55:59<12:15:13, 4901.47s/it]Trainning Epoch:  95%|█████████▍| 156/165 [212:55:59<12:15:13, 4901.47s/it]2025-10-20 12:23:01,172 Stage: Train 0.5 | Epoch: 156 | Iter: 474000 | Total Loss: 0.003356 | Recon Loss: 0.002830 | Commit Loss: 0.001052 | Perplexity: 1210.297163
2025-10-20 12:28:23,982 Stage: Train 0.5 | Epoch: 156 | Iter: 474200 | Total Loss: 0.003536 | Recon Loss: 0.003001 | Commit Loss: 0.001069 | Perplexity: 1221.588101
2025-10-20 12:33:46,148 Stage: Train 0.5 | Epoch: 156 | Iter: 474400 | Total Loss: 0.003600 | Recon Loss: 0.003010 | Commit Loss: 0.001179 | Perplexity: 1203.455862
2025-10-20 12:39:08,358 Stage: Train 0.5 | Epoch: 156 | Iter: 474600 | Total Loss: 0.003331 | Recon Loss: 0.002819 | Commit Loss: 0.001023 | Perplexity: 1212.683017
2025-10-20 12:44:30,413 Stage: Train 0.5 | Epoch: 156 | Iter: 474800 | Total Loss: 0.003398 | Recon Loss: 0.002869 | Commit Loss: 0.001059 | Perplexity: 1216.143696
2025-10-20 12:49:52,103 Stage: Train 0.5 | Epoch: 156 | Iter: 475000 | Total Loss: 0.003403 | Recon Loss: 0.002879 | Commit Loss: 0.001049 | Perplexity: 1216.181793
2025-10-20 12:55:14,192 Stage: Train 0.5 | Epoch: 156 | Iter: 475200 | Total Loss: 0.003397 | Recon Loss: 0.002880 | Commit Loss: 0.001034 | Perplexity: 1217.226494
2025-10-20 13:00:36,969 Stage: Train 0.5 | Epoch: 156 | Iter: 475400 | Total Loss: 0.003324 | Recon Loss: 0.002816 | Commit Loss: 0.001017 | Perplexity: 1211.877127
2025-10-20 13:05:59,010 Stage: Train 0.5 | Epoch: 156 | Iter: 475600 | Total Loss: 0.003395 | Recon Loss: 0.002882 | Commit Loss: 0.001027 | Perplexity: 1213.971194
2025-10-20 13:11:21,105 Stage: Train 0.5 | Epoch: 156 | Iter: 475800 | Total Loss: 0.003336 | Recon Loss: 0.002820 | Commit Loss: 0.001033 | Perplexity: 1212.151377
2025-10-20 13:16:42,856 Stage: Train 0.5 | Epoch: 156 | Iter: 476000 | Total Loss: 0.003380 | Recon Loss: 0.002868 | Commit Loss: 0.001026 | Perplexity: 1213.468627
2025-10-20 13:22:04,738 Stage: Train 0.5 | Epoch: 156 | Iter: 476200 | Total Loss: 0.003316 | Recon Loss: 0.002803 | Commit Loss: 0.001027 | Perplexity: 1215.591267
2025-10-20 13:27:27,466 Stage: Train 0.5 | Epoch: 156 | Iter: 476400 | Total Loss: 0.003340 | Recon Loss: 0.002824 | Commit Loss: 0.001031 | Perplexity: 1208.243820
2025-10-20 13:32:50,165 Stage: Train 0.5 | Epoch: 156 | Iter: 476600 | Total Loss: 0.003383 | Recon Loss: 0.002862 | Commit Loss: 0.001044 | Perplexity: 1209.190837
2025-10-20 13:38:11,740 Stage: Train 0.5 | Epoch: 156 | Iter: 476800 | Total Loss: 0.003320 | Recon Loss: 0.002804 | Commit Loss: 0.001033 | Perplexity: 1209.960443
Trainning Epoch:  95%|█████████▌| 157/165 [214:17:37<10:53:23, 4900.40s/it]Trainning Epoch:  95%|█████████▌| 157/165 [214:17:37<10:53:23, 4900.40s/it]2025-10-20 13:43:38,096 Stage: Train 0.5 | Epoch: 157 | Iter: 477000 | Total Loss: 0.003316 | Recon Loss: 0.002798 | Commit Loss: 0.001036 | Perplexity: 1219.542586
2025-10-20 13:49:00,738 Stage: Train 0.5 | Epoch: 157 | Iter: 477200 | Total Loss: 0.003297 | Recon Loss: 0.002784 | Commit Loss: 0.001027 | Perplexity: 1211.547983
2025-10-20 13:54:22,368 Stage: Train 0.5 | Epoch: 157 | Iter: 477400 | Total Loss: 0.003352 | Recon Loss: 0.002829 | Commit Loss: 0.001046 | Perplexity: 1218.146037
2025-10-20 13:59:44,096 Stage: Train 0.5 | Epoch: 157 | Iter: 477600 | Total Loss: 0.003381 | Recon Loss: 0.002858 | Commit Loss: 0.001045 | Perplexity: 1214.093824
2025-10-20 14:05:06,214 Stage: Train 0.5 | Epoch: 157 | Iter: 477800 | Total Loss: 0.003355 | Recon Loss: 0.002836 | Commit Loss: 0.001036 | Perplexity: 1208.634434
2025-10-20 14:10:27,955 Stage: Train 0.5 | Epoch: 157 | Iter: 478000 | Total Loss: 0.003428 | Recon Loss: 0.002910 | Commit Loss: 0.001034 | Perplexity: 1209.454294
2025-10-20 14:15:49,962 Stage: Train 0.5 | Epoch: 157 | Iter: 478200 | Total Loss: 0.003355 | Recon Loss: 0.002843 | Commit Loss: 0.001025 | Perplexity: 1209.198290
2025-10-20 14:21:12,771 Stage: Train 0.5 | Epoch: 157 | Iter: 478400 | Total Loss: 0.003392 | Recon Loss: 0.002869 | Commit Loss: 0.001045 | Perplexity: 1212.545912
2025-10-20 14:26:35,349 Stage: Train 0.5 | Epoch: 157 | Iter: 478600 | Total Loss: 0.003401 | Recon Loss: 0.002886 | Commit Loss: 0.001031 | Perplexity: 1215.412243
2025-10-20 14:31:57,905 Stage: Train 0.5 | Epoch: 157 | Iter: 478800 | Total Loss: 0.003340 | Recon Loss: 0.002817 | Commit Loss: 0.001046 | Perplexity: 1219.555016
2025-10-20 14:37:19,425 Stage: Train 0.5 | Epoch: 157 | Iter: 479000 | Total Loss: 0.003280 | Recon Loss: 0.002763 | Commit Loss: 0.001034 | Perplexity: 1213.330696
2025-10-20 14:42:41,795 Stage: Train 0.5 | Epoch: 157 | Iter: 479200 | Total Loss: 0.003300 | Recon Loss: 0.002782 | Commit Loss: 0.001036 | Perplexity: 1225.307037
2025-10-20 14:48:04,678 Stage: Train 0.5 | Epoch: 157 | Iter: 479400 | Total Loss: 0.003365 | Recon Loss: 0.002844 | Commit Loss: 0.001042 | Perplexity: 1214.809518
2025-10-20 14:53:27,011 Stage: Train 0.5 | Epoch: 157 | Iter: 479600 | Total Loss: 0.003300 | Recon Loss: 0.002787 | Commit Loss: 0.001025 | Perplexity: 1219.729941
2025-10-20 14:58:49,664 Stage: Train 0.5 | Epoch: 157 | Iter: 479800 | Total Loss: 0.003347 | Recon Loss: 0.002826 | Commit Loss: 0.001041 | Perplexity: 1211.563619
2025-10-20 15:04:12,761 Stage: Train 0.5 | Epoch: 157 | Iter: 480000 | Total Loss: 0.003358 | Recon Loss: 0.002840 | Commit Loss: 0.001036 | Perplexity: 1213.055252
2025-10-20 15:04:12,761 Saving model at iteration 480000
2025-10-20 15:04:12,906 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000
2025-10-20 15:04:14,264 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000/model.safetensors
2025-10-20 15:04:16,050 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000/optimizer.bin
2025-10-20 15:04:16,051 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000/scheduler.bin
2025-10-20 15:04:16,051 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000/sampler.bin
2025-10-20 15:04:16,052 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_158_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 158/165 [215:39:21<9:31:50, 4901.47s/it] Trainning Epoch:  96%|█████████▌| 158/165 [215:39:21<9:31:50, 4901.47s/it] 2025-10-20 15:09:43,121 Stage: Train 0.5 | Epoch: 158 | Iter: 480200 | Total Loss: 0.003407 | Recon Loss: 0.002889 | Commit Loss: 0.001036 | Perplexity: 1217.523506
2025-10-20 15:15:05,630 Stage: Train 0.5 | Epoch: 158 | Iter: 480400 | Total Loss: 0.003319 | Recon Loss: 0.002800 | Commit Loss: 0.001038 | Perplexity: 1221.992794
2025-10-20 15:20:28,122 Stage: Train 0.5 | Epoch: 158 | Iter: 480600 | Total Loss: 0.003441 | Recon Loss: 0.002924 | Commit Loss: 0.001034 | Perplexity: 1217.001821
2025-10-20 15:25:49,246 Stage: Train 0.5 | Epoch: 158 | Iter: 480800 | Total Loss: 0.003225 | Recon Loss: 0.002714 | Commit Loss: 0.001023 | Perplexity: 1208.253662
2025-10-20 15:31:10,944 Stage: Train 0.5 | Epoch: 158 | Iter: 481000 | Total Loss: 0.003393 | Recon Loss: 0.002879 | Commit Loss: 0.001028 | Perplexity: 1213.949712
2025-10-20 15:36:33,379 Stage: Train 0.5 | Epoch: 158 | Iter: 481200 | Total Loss: 0.003382 | Recon Loss: 0.002863 | Commit Loss: 0.001038 | Perplexity: 1216.768060
2025-10-20 15:41:55,910 Stage: Train 0.5 | Epoch: 158 | Iter: 481400 | Total Loss: 0.003374 | Recon Loss: 0.002851 | Commit Loss: 0.001045 | Perplexity: 1215.249463
2025-10-20 15:47:18,355 Stage: Train 0.5 | Epoch: 158 | Iter: 481600 | Total Loss: 0.003332 | Recon Loss: 0.002809 | Commit Loss: 0.001048 | Perplexity: 1214.830764
2025-10-20 15:52:40,175 Stage: Train 0.5 | Epoch: 158 | Iter: 481800 | Total Loss: 0.003342 | Recon Loss: 0.002815 | Commit Loss: 0.001054 | Perplexity: 1222.556051
2025-10-20 15:58:01,265 Stage: Train 0.5 | Epoch: 158 | Iter: 482000 | Total Loss: 0.003382 | Recon Loss: 0.002860 | Commit Loss: 0.001044 | Perplexity: 1209.023870
2025-10-20 16:03:22,969 Stage: Train 0.5 | Epoch: 158 | Iter: 482200 | Total Loss: 0.003426 | Recon Loss: 0.002906 | Commit Loss: 0.001040 | Perplexity: 1210.806800
2025-10-20 16:08:45,309 Stage: Train 0.5 | Epoch: 158 | Iter: 482400 | Total Loss: 0.003325 | Recon Loss: 0.002802 | Commit Loss: 0.001047 | Perplexity: 1219.231455
2025-10-20 16:14:08,200 Stage: Train 0.5 | Epoch: 158 | Iter: 482600 | Total Loss: 0.003361 | Recon Loss: 0.002843 | Commit Loss: 0.001036 | Perplexity: 1209.367733
2025-10-20 16:19:30,243 Stage: Train 0.5 | Epoch: 158 | Iter: 482800 | Total Loss: 0.003381 | Recon Loss: 0.002853 | Commit Loss: 0.001056 | Perplexity: 1213.293452
2025-10-20 16:24:51,964 Stage: Train 0.5 | Epoch: 158 | Iter: 483000 | Total Loss: 0.003372 | Recon Loss: 0.002856 | Commit Loss: 0.001033 | Perplexity: 1214.308702
Trainning Epoch:  96%|█████████▋| 159/165 [217:00:58<8:10:00, 4900.05s/it]Trainning Epoch:  96%|█████████▋| 159/165 [217:00:58<8:10:00, 4900.04s/it]2025-10-20 16:30:18,428 Stage: Train 0.5 | Epoch: 159 | Iter: 483200 | Total Loss: 0.003358 | Recon Loss: 0.002842 | Commit Loss: 0.001033 | Perplexity: 1206.687570
2025-10-20 16:35:41,153 Stage: Train 0.5 | Epoch: 159 | Iter: 483400 | Total Loss: 0.003291 | Recon Loss: 0.002777 | Commit Loss: 0.001028 | Perplexity: 1218.010774
2025-10-20 16:41:04,092 Stage: Train 0.5 | Epoch: 159 | Iter: 483600 | Total Loss: 0.003347 | Recon Loss: 0.002823 | Commit Loss: 0.001047 | Perplexity: 1218.184110
2025-10-20 16:46:26,765 Stage: Train 0.5 | Epoch: 159 | Iter: 483800 | Total Loss: 0.003355 | Recon Loss: 0.002834 | Commit Loss: 0.001041 | Perplexity: 1215.553323
2025-10-20 16:51:48,737 Stage: Train 0.5 | Epoch: 159 | Iter: 484000 | Total Loss: 0.003410 | Recon Loss: 0.002891 | Commit Loss: 0.001037 | Perplexity: 1203.819924
2025-10-20 16:57:10,543 Stage: Train 0.5 | Epoch: 159 | Iter: 484200 | Total Loss: 0.003320 | Recon Loss: 0.002804 | Commit Loss: 0.001033 | Perplexity: 1211.484075
2025-10-20 17:02:32,690 Stage: Train 0.5 | Epoch: 159 | Iter: 484400 | Total Loss: 0.003304 | Recon Loss: 0.002784 | Commit Loss: 0.001040 | Perplexity: 1218.514264
2025-10-20 17:07:54,942 Stage: Train 0.5 | Epoch: 159 | Iter: 484600 | Total Loss: 0.003391 | Recon Loss: 0.002864 | Commit Loss: 0.001055 | Perplexity: 1212.015614
2025-10-20 17:13:16,960 Stage: Train 0.5 | Epoch: 159 | Iter: 484800 | Total Loss: 0.003340 | Recon Loss: 0.002818 | Commit Loss: 0.001045 | Perplexity: 1208.875534
2025-10-20 17:18:39,731 Stage: Train 0.5 | Epoch: 159 | Iter: 485000 | Total Loss: 0.003333 | Recon Loss: 0.002813 | Commit Loss: 0.001040 | Perplexity: 1222.291235
2025-10-20 17:24:01,863 Stage: Train 0.5 | Epoch: 159 | Iter: 485200 | Total Loss: 0.003396 | Recon Loss: 0.002867 | Commit Loss: 0.001059 | Perplexity: 1219.417878
2025-10-20 17:29:22,555 Stage: Train 0.5 | Epoch: 159 | Iter: 485400 | Total Loss: 0.003380 | Recon Loss: 0.002860 | Commit Loss: 0.001040 | Perplexity: 1212.287900
2025-10-20 17:34:44,616 Stage: Train 0.5 | Epoch: 159 | Iter: 485600 | Total Loss: 0.003368 | Recon Loss: 0.002848 | Commit Loss: 0.001040 | Perplexity: 1215.236862
2025-10-20 17:40:06,540 Stage: Train 0.5 | Epoch: 159 | Iter: 485800 | Total Loss: 0.003359 | Recon Loss: 0.002837 | Commit Loss: 0.001043 | Perplexity: 1212.354052
2025-10-20 17:45:28,579 Stage: Train 0.5 | Epoch: 159 | Iter: 486000 | Total Loss: 0.003365 | Recon Loss: 0.002852 | Commit Loss: 0.001026 | Perplexity: 1217.380113
Trainning Epoch:  97%|█████████▋| 160/165 [218:22:36<6:48:17, 4899.46s/it]Trainning Epoch:  97%|█████████▋| 160/165 [218:22:36<6:48:17, 4899.46s/it]2025-10-20 17:50:55,244 Stage: Train 0.5 | Epoch: 160 | Iter: 486200 | Total Loss: 0.003403 | Recon Loss: 0.002877 | Commit Loss: 0.001050 | Perplexity: 1214.391584
2025-10-20 17:56:17,930 Stage: Train 0.5 | Epoch: 160 | Iter: 486400 | Total Loss: 0.003318 | Recon Loss: 0.002796 | Commit Loss: 0.001044 | Perplexity: 1216.029832
2025-10-20 18:01:39,882 Stage: Train 0.5 | Epoch: 160 | Iter: 486600 | Total Loss: 0.003380 | Recon Loss: 0.002866 | Commit Loss: 0.001027 | Perplexity: 1204.418742
2025-10-20 18:07:02,171 Stage: Train 0.5 | Epoch: 160 | Iter: 486800 | Total Loss: 0.003292 | Recon Loss: 0.002773 | Commit Loss: 0.001038 | Perplexity: 1212.437827
2025-10-20 18:12:24,330 Stage: Train 0.5 | Epoch: 160 | Iter: 487000 | Total Loss: 0.003336 | Recon Loss: 0.002820 | Commit Loss: 0.001033 | Perplexity: 1220.964746
2025-10-20 18:17:46,986 Stage: Train 0.5 | Epoch: 160 | Iter: 487200 | Total Loss: 0.003322 | Recon Loss: 0.002806 | Commit Loss: 0.001031 | Perplexity: 1214.105620
2025-10-20 18:23:08,814 Stage: Train 0.5 | Epoch: 160 | Iter: 487400 | Total Loss: 0.003365 | Recon Loss: 0.002838 | Commit Loss: 0.001055 | Perplexity: 1210.987525
2025-10-20 18:28:30,194 Stage: Train 0.5 | Epoch: 160 | Iter: 487600 | Total Loss: 0.003353 | Recon Loss: 0.002830 | Commit Loss: 0.001046 | Perplexity: 1209.558505
2025-10-20 18:33:52,650 Stage: Train 0.5 | Epoch: 160 | Iter: 487800 | Total Loss: 0.003332 | Recon Loss: 0.002811 | Commit Loss: 0.001043 | Perplexity: 1211.980070
2025-10-20 18:39:14,389 Stage: Train 0.5 | Epoch: 160 | Iter: 488000 | Total Loss: 0.003353 | Recon Loss: 0.002832 | Commit Loss: 0.001042 | Perplexity: 1213.396711
2025-10-20 18:44:36,046 Stage: Train 0.5 | Epoch: 160 | Iter: 488200 | Total Loss: 0.003327 | Recon Loss: 0.002808 | Commit Loss: 0.001037 | Perplexity: 1200.689943
2025-10-20 18:49:58,421 Stage: Train 0.5 | Epoch: 160 | Iter: 488400 | Total Loss: 0.003372 | Recon Loss: 0.002854 | Commit Loss: 0.001035 | Perplexity: 1220.868314
2025-10-20 18:55:21,249 Stage: Train 0.5 | Epoch: 160 | Iter: 488600 | Total Loss: 0.003338 | Recon Loss: 0.002822 | Commit Loss: 0.001032 | Perplexity: 1220.310615
2025-10-20 19:00:43,790 Stage: Train 0.5 | Epoch: 160 | Iter: 488800 | Total Loss: 0.003312 | Recon Loss: 0.002783 | Commit Loss: 0.001059 | Perplexity: 1222.838422
2025-10-20 19:06:06,277 Stage: Train 0.5 | Epoch: 160 | Iter: 489000 | Total Loss: 0.003370 | Recon Loss: 0.002857 | Commit Loss: 0.001027 | Perplexity: 1206.679656
Trainning Epoch:  98%|█████████▊| 161/165 [219:44:15<5:26:37, 4899.42s/it]Trainning Epoch:  98%|█████████▊| 161/165 [219:44:15<5:26:37, 4899.42s/it]2025-10-20 19:11:33,436 Stage: Train 0.5 | Epoch: 161 | Iter: 489200 | Total Loss: 0.003336 | Recon Loss: 0.002823 | Commit Loss: 0.001025 | Perplexity: 1214.909667
2025-10-20 19:16:56,133 Stage: Train 0.5 | Epoch: 161 | Iter: 489400 | Total Loss: 0.003321 | Recon Loss: 0.002806 | Commit Loss: 0.001031 | Perplexity: 1216.775300
2025-10-20 19:22:18,826 Stage: Train 0.5 | Epoch: 161 | Iter: 489600 | Total Loss: 0.003339 | Recon Loss: 0.002826 | Commit Loss: 0.001026 | Perplexity: 1213.998820
2025-10-20 19:27:40,718 Stage: Train 0.5 | Epoch: 161 | Iter: 489800 | Total Loss: 0.003363 | Recon Loss: 0.002840 | Commit Loss: 0.001048 | Perplexity: 1218.105352
2025-10-20 19:33:03,241 Stage: Train 0.5 | Epoch: 161 | Iter: 490000 | Total Loss: 0.003314 | Recon Loss: 0.002796 | Commit Loss: 0.001036 | Perplexity: 1206.642000
2025-10-20 19:38:25,580 Stage: Train 0.5 | Epoch: 161 | Iter: 490200 | Total Loss: 0.003393 | Recon Loss: 0.002861 | Commit Loss: 0.001063 | Perplexity: 1207.409281
2025-10-20 19:43:47,917 Stage: Train 0.5 | Epoch: 161 | Iter: 490400 | Total Loss: 0.003255 | Recon Loss: 0.002731 | Commit Loss: 0.001048 | Perplexity: 1213.697403
2025-10-20 19:49:08,644 Stage: Train 0.5 | Epoch: 161 | Iter: 490600 | Total Loss: 0.003345 | Recon Loss: 0.002833 | Commit Loss: 0.001025 | Perplexity: 1221.090916
2025-10-20 19:54:29,538 Stage: Train 0.5 | Epoch: 161 | Iter: 490800 | Total Loss: 0.003339 | Recon Loss: 0.002816 | Commit Loss: 0.001046 | Perplexity: 1212.889195
2025-10-20 19:59:52,289 Stage: Train 0.5 | Epoch: 161 | Iter: 491000 | Total Loss: 0.003417 | Recon Loss: 0.002893 | Commit Loss: 0.001048 | Perplexity: 1213.897927
2025-10-20 20:05:15,183 Stage: Train 0.5 | Epoch: 161 | Iter: 491200 | Total Loss: 0.003305 | Recon Loss: 0.002785 | Commit Loss: 0.001040 | Perplexity: 1206.774439
2025-10-20 20:10:38,164 Stage: Train 0.5 | Epoch: 161 | Iter: 491400 | Total Loss: 0.003294 | Recon Loss: 0.002782 | Commit Loss: 0.001023 | Perplexity: 1212.509474
2025-10-20 20:16:01,123 Stage: Train 0.5 | Epoch: 161 | Iter: 491600 | Total Loss: 0.003328 | Recon Loss: 0.002802 | Commit Loss: 0.001052 | Perplexity: 1213.951684
2025-10-20 20:21:23,991 Stage: Train 0.5 | Epoch: 161 | Iter: 491800 | Total Loss: 0.003397 | Recon Loss: 0.002876 | Commit Loss: 0.001043 | Perplexity: 1210.450748
2025-10-20 20:26:45,793 Stage: Train 0.5 | Epoch: 161 | Iter: 492000 | Total Loss: 0.003356 | Recon Loss: 0.002830 | Commit Loss: 0.001052 | Perplexity: 1210.674830
Trainning Epoch:  98%|█████████▊| 162/165 [221:05:56<4:04:58, 4899.65s/it]Trainning Epoch:  98%|█████████▊| 162/165 [221:05:56<4:04:58, 4899.65s/it]2025-10-20 20:32:12,368 Stage: Train 0.5 | Epoch: 162 | Iter: 492200 | Total Loss: 0.003317 | Recon Loss: 0.002797 | Commit Loss: 0.001041 | Perplexity: 1211.871208
2025-10-20 20:37:35,222 Stage: Train 0.5 | Epoch: 162 | Iter: 492400 | Total Loss: 0.003342 | Recon Loss: 0.002831 | Commit Loss: 0.001021 | Perplexity: 1217.266011
2025-10-20 20:42:57,387 Stage: Train 0.5 | Epoch: 162 | Iter: 492600 | Total Loss: 0.003399 | Recon Loss: 0.002876 | Commit Loss: 0.001047 | Perplexity: 1210.640639
2025-10-20 20:48:19,565 Stage: Train 0.5 | Epoch: 162 | Iter: 492800 | Total Loss: 0.003355 | Recon Loss: 0.002832 | Commit Loss: 0.001046 | Perplexity: 1212.879220
2025-10-20 20:53:41,746 Stage: Train 0.5 | Epoch: 162 | Iter: 493000 | Total Loss: 0.003313 | Recon Loss: 0.002799 | Commit Loss: 0.001028 | Perplexity: 1213.585458
2025-10-20 20:59:03,710 Stage: Train 0.5 | Epoch: 162 | Iter: 493200 | Total Loss: 0.003333 | Recon Loss: 0.002808 | Commit Loss: 0.001050 | Perplexity: 1211.232631
2025-10-20 21:04:25,552 Stage: Train 0.5 | Epoch: 162 | Iter: 493400 | Total Loss: 0.003300 | Recon Loss: 0.002783 | Commit Loss: 0.001034 | Perplexity: 1213.140423
2025-10-20 21:09:48,324 Stage: Train 0.5 | Epoch: 162 | Iter: 493600 | Total Loss: 0.003377 | Recon Loss: 0.002849 | Commit Loss: 0.001054 | Perplexity: 1218.936491
2025-10-20 21:15:11,237 Stage: Train 0.5 | Epoch: 162 | Iter: 493800 | Total Loss: 0.003299 | Recon Loss: 0.002780 | Commit Loss: 0.001036 | Perplexity: 1211.749031
2025-10-20 21:20:32,669 Stage: Train 0.5 | Epoch: 162 | Iter: 494000 | Total Loss: 0.003401 | Recon Loss: 0.002880 | Commit Loss: 0.001041 | Perplexity: 1219.186051
2025-10-20 21:25:53,498 Stage: Train 0.5 | Epoch: 162 | Iter: 494200 | Total Loss: 0.003294 | Recon Loss: 0.002773 | Commit Loss: 0.001041 | Perplexity: 1216.317721
2025-10-20 21:31:16,053 Stage: Train 0.5 | Epoch: 162 | Iter: 494400 | Total Loss: 0.003311 | Recon Loss: 0.002787 | Commit Loss: 0.001048 | Perplexity: 1212.901116
2025-10-20 21:36:38,781 Stage: Train 0.5 | Epoch: 162 | Iter: 494600 | Total Loss: 0.003331 | Recon Loss: 0.002813 | Commit Loss: 0.001035 | Perplexity: 1219.162104
2025-10-20 21:42:01,623 Stage: Train 0.5 | Epoch: 162 | Iter: 494800 | Total Loss: 0.003299 | Recon Loss: 0.002783 | Commit Loss: 0.001033 | Perplexity: 1213.983691
2025-10-20 21:47:24,264 Stage: Train 0.5 | Epoch: 162 | Iter: 495000 | Total Loss: 0.003326 | Recon Loss: 0.002810 | Commit Loss: 0.001031 | Perplexity: 1211.727441
Trainning Epoch:  99%|█████████▉| 163/165 [222:27:36<2:43:19, 4899.94s/it]Trainning Epoch:  99%|█████████▉| 163/165 [222:27:36<2:43:19, 4899.94s/it]2025-10-20 21:52:51,500 Stage: Train 0.5 | Epoch: 163 | Iter: 495200 | Total Loss: 0.003348 | Recon Loss: 0.002844 | Commit Loss: 0.001008 | Perplexity: 1211.573710
2025-10-20 21:58:13,284 Stage: Train 0.5 | Epoch: 163 | Iter: 495400 | Total Loss: 0.003346 | Recon Loss: 0.002837 | Commit Loss: 0.001017 | Perplexity: 1216.972512
2025-10-20 22:03:34,789 Stage: Train 0.5 | Epoch: 163 | Iter: 495600 | Total Loss: 0.003268 | Recon Loss: 0.002762 | Commit Loss: 0.001013 | Perplexity: 1216.294981
2025-10-20 22:08:56,863 Stage: Train 0.5 | Epoch: 163 | Iter: 495800 | Total Loss: 0.003369 | Recon Loss: 0.002848 | Commit Loss: 0.001042 | Perplexity: 1218.213166
2025-10-20 22:14:18,406 Stage: Train 0.5 | Epoch: 163 | Iter: 496000 | Total Loss: 0.003242 | Recon Loss: 0.002730 | Commit Loss: 0.001024 | Perplexity: 1217.914012
2025-10-20 22:19:39,728 Stage: Train 0.5 | Epoch: 163 | Iter: 496200 | Total Loss: 0.003347 | Recon Loss: 0.002822 | Commit Loss: 0.001049 | Perplexity: 1218.343701
2025-10-20 22:25:01,411 Stage: Train 0.5 | Epoch: 163 | Iter: 496400 | Total Loss: 0.003303 | Recon Loss: 0.002794 | Commit Loss: 0.001018 | Perplexity: 1219.113315
2025-10-20 22:30:23,874 Stage: Train 0.5 | Epoch: 163 | Iter: 496600 | Total Loss: 0.003308 | Recon Loss: 0.002792 | Commit Loss: 0.001032 | Perplexity: 1216.766830
2025-10-20 22:35:46,673 Stage: Train 0.5 | Epoch: 163 | Iter: 496800 | Total Loss: 0.003426 | Recon Loss: 0.002886 | Commit Loss: 0.001081 | Perplexity: 1216.950033
2025-10-20 22:41:09,286 Stage: Train 0.5 | Epoch: 163 | Iter: 497000 | Total Loss: 0.003289 | Recon Loss: 0.002777 | Commit Loss: 0.001024 | Perplexity: 1213.361132
2025-10-20 22:46:32,019 Stage: Train 0.5 | Epoch: 163 | Iter: 497200 | Total Loss: 0.003307 | Recon Loss: 0.002796 | Commit Loss: 0.001022 | Perplexity: 1216.004578
2025-10-20 22:51:53,658 Stage: Train 0.5 | Epoch: 163 | Iter: 497400 | Total Loss: 0.003342 | Recon Loss: 0.002828 | Commit Loss: 0.001028 | Perplexity: 1215.559860
2025-10-20 22:57:14,244 Stage: Train 0.5 | Epoch: 163 | Iter: 497600 | Total Loss: 0.003330 | Recon Loss: 0.002815 | Commit Loss: 0.001032 | Perplexity: 1210.070836
2025-10-20 23:02:36,104 Stage: Train 0.5 | Epoch: 163 | Iter: 497800 | Total Loss: 0.003380 | Recon Loss: 0.002854 | Commit Loss: 0.001054 | Perplexity: 1221.545774
2025-10-20 23:07:58,750 Stage: Train 0.5 | Epoch: 163 | Iter: 498000 | Total Loss: 0.003433 | Recon Loss: 0.002914 | Commit Loss: 0.001039 | Perplexity: 1215.821184
2025-10-20 23:13:21,446 Stage: Train 0.5 | Epoch: 163 | Iter: 498200 | Total Loss: 0.003277 | Recon Loss: 0.002764 | Commit Loss: 0.001025 | Perplexity: 1221.784688
Trainning Epoch:  99%|█████████▉| 164/165 [223:49:12<1:21:38, 4898.59s/it]Trainning Epoch:  99%|█████████▉| 164/165 [223:49:12<1:21:38, 4898.59s/it]2025-10-20 23:18:48,139 Stage: Train 0.5 | Epoch: 164 | Iter: 498400 | Total Loss: 0.003371 | Recon Loss: 0.002860 | Commit Loss: 0.001023 | Perplexity: 1221.306150
2025-10-20 23:24:10,528 Stage: Train 0.5 | Epoch: 164 | Iter: 498600 | Total Loss: 0.003352 | Recon Loss: 0.002842 | Commit Loss: 0.001020 | Perplexity: 1214.656511
2025-10-20 23:29:33,281 Stage: Train 0.5 | Epoch: 164 | Iter: 498800 | Total Loss: 0.003315 | Recon Loss: 0.002811 | Commit Loss: 0.001008 | Perplexity: 1221.994071
2025-10-20 23:34:55,658 Stage: Train 0.5 | Epoch: 164 | Iter: 499000 | Total Loss: 0.003290 | Recon Loss: 0.002773 | Commit Loss: 0.001034 | Perplexity: 1213.078649
2025-10-20 23:40:16,790 Stage: Train 0.5 | Epoch: 164 | Iter: 499200 | Total Loss: 0.003321 | Recon Loss: 0.002819 | Commit Loss: 0.001003 | Perplexity: 1209.128572
2025-10-20 23:45:39,695 Stage: Train 0.5 | Epoch: 164 | Iter: 499400 | Total Loss: 0.003355 | Recon Loss: 0.002849 | Commit Loss: 0.001014 | Perplexity: 1216.130644
2025-10-20 23:51:01,224 Stage: Train 0.5 | Epoch: 164 | Iter: 499600 | Total Loss: 0.003392 | Recon Loss: 0.002881 | Commit Loss: 0.001023 | Perplexity: 1207.987147
2025-10-20 23:56:22,315 Stage: Train 0.5 | Epoch: 164 | Iter: 499800 | Total Loss: 0.003240 | Recon Loss: 0.002726 | Commit Loss: 0.001028 | Perplexity: 1217.204800
2025-10-21 00:01:44,766 Stage: Train 0.5 | Epoch: 164 | Iter: 500000 | Total Loss: 0.003307 | Recon Loss: 0.002794 | Commit Loss: 0.001024 | Perplexity: 1216.884765
2025-10-21 00:01:44,766 Saving model at iteration 500000
2025-10-21 00:01:44,924 Saving current state to vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000
Trainning Epoch:  99%|█████████▉| 164/165 [224:36:45<1:22:10, 4930.52s/it]
2025-10-21 00:01:46,267 Model weights saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000/model.safetensors
2025-10-21 00:01:48,047 Optimizer state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000/optimizer.bin
2025-10-21 00:01:48,048 Scheduler state saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000/scheduler.bin
2025-10-21 00:01:48,048 Sampler state for dataloader 0 saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000/sampler.bin
2025-10-21 00:01:48,049 Random states saved in vqvae_experiment/joint_and_image/joint3d_image_affined_448x448/f16s1d16_cb8192x2048_mpjpe_Tdown1-2/hrFix_lvl0123_adaSmpl/models/checkpoint_epoch_165_step_500000/random_states_0.pkl
Trainning Epoch:  99%|█████████▉| 164/165 [224:36:47<1:22:10, 4930.53s/it]
2025-10-21 00:01:48,330 Training finished
[rank0]:[W1021 00:01:48.346535105 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
