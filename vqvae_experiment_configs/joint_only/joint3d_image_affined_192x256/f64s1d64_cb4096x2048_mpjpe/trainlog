/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-30 12:46:39,572 
python train_vqvae_new.py --batch_size 32 --config vqvae_experiment_configs/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/config.yaml --data_mode joint3d --num_frames 64 --sample_stride 1 --data_stride 64 --project_dir vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe --not_find_unused_parameters --nb_code 4096 --codebook_dim 2048 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level 3 --vision_guidance_ratio 0 --fix_weights --resume_pth 
2025-09-30 12:46:39,572 
PID: 578656
2025-09-30 12:48:30,220 Data loaded with 24076 samples
2025-09-30 12:48:30,961 Trainable parameters: 38,884,867
2025-09-30 12:48:30,962 Non-trainable parameters: 0
2025-09-30 12:48:31,911 Number of trainable parameters: 38.884867 M
2025-09-30 12:48:31,912 Args: {'num_frames': 64, 'sample_stride': 1, 'data_stride': 64, 'data_mode': 'joint3d', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [['image']], 'normalize': 'anisotropic', 'filter_invalid_images': True, 'processed_image_shape': [192, 256], 'backbone': 'hrnet_32', 'get_item_list': ['factor_2_5d', 'video_rgb', 'joint3d_image_affined', 'joint3d_image_affined_normed', 'joint3d_image_affined_scale', 'joint3d_image_affined_transl', 'affine_trans', 'affine_trans_inv', 'joint_2_5d_image'], 'config': 'vqvae_experiment_configs/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/config.yaml', 'resume_pth': '', 'batch_size': 32, 'commit_ratio': 0.5, 'nb_code': 4096, 'codebook_dim': 2048, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_image_affined_normed', 'hrnet_output_level': 3, 'fix_weights': True, 'fix_weights_except': 'PLACEHOLDERPLACEHOLDERPLACEHOLDER', 'vision_guidance_ratio': 0.0}
Trainning Epoch:   0%|          | 0/665 [00:00<?, ?it/s]2025-09-30 12:52:06,056 current_lr 0.000008 at iteration 200
2025-09-30 12:52:06,272 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.144325 | Recon Loss: 0.142539 | Commit Loss: 0.003572 | Perplexity: 754.170344
2025-09-30 12:55:29,103 current_lr 0.000016 at iteration 400
2025-09-30 12:55:29,406 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.083419 | Recon Loss: 0.075780 | Commit Loss: 0.015279 | Perplexity: 658.377466
2025-09-30 12:58:33,887 current_lr 0.000024 at iteration 600
2025-09-30 12:58:34,180 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.088039 | Recon Loss: 0.062095 | Commit Loss: 0.051889 | Perplexity: 661.616808
Trainning Epoch:   0%|          | 1/665 [12:22<136:52:05, 742.06s/it]2025-09-30 13:01:58,015 current_lr 0.000032 at iteration 800
2025-09-30 13:01:58,197 Stage: Warm Up | Epoch: 1 | Iter: 800 | Total Loss: 0.088052 | Recon Loss: 0.053548 | Commit Loss: 0.069007 | Perplexity: 724.804833
2025-09-30 13:05:50,786 current_lr 0.000040 at iteration 1000
2025-09-30 13:05:50,926 Stage: Warm Up | Epoch: 1 | Iter: 1000 | Total Loss: 0.083105 | Recon Loss: 0.048295 | Commit Loss: 0.069621 | Perplexity: 752.246151
2025-09-30 13:09:43,229 current_lr 0.000048 at iteration 1200
2025-09-30 13:09:43,391 Stage: Warm Up | Epoch: 1 | Iter: 1200 | Total Loss: 0.075466 | Recon Loss: 0.044846 | Commit Loss: 0.061241 | Perplexity: 765.329052
2025-09-30 13:13:32,155 current_lr 0.000056 at iteration 1400
2025-09-30 13:13:32,309 Stage: Warm Up | Epoch: 1 | Iter: 1400 | Total Loss: 0.065578 | Recon Loss: 0.040451 | Commit Loss: 0.050254 | Perplexity: 769.999956
Trainning Epoch:   0%|          | 2/665 [26:56<151:01:18, 820.03s/it]2025-09-30 13:17:25,401 current_lr 0.000064 at iteration 1600
2025-09-30 13:17:25,573 Stage: Warm Up | Epoch: 2 | Iter: 1600 | Total Loss: 0.057608 | Recon Loss: 0.037400 | Commit Loss: 0.040415 | Perplexity: 775.539785
2025-09-30 13:21:11,188 current_lr 0.000072 at iteration 1800
2025-09-30 13:21:11,341 Stage: Warm Up | Epoch: 2 | Iter: 1800 | Total Loss: 0.050987 | Recon Loss: 0.034180 | Commit Loss: 0.033614 | Perplexity: 790.823314
2025-09-30 13:24:59,245 current_lr 0.000080 at iteration 2000
2025-09-30 13:24:59,390 Stage: Warm Up | Epoch: 2 | Iter: 2000 | Total Loss: 0.046040 | Recon Loss: 0.032475 | Commit Loss: 0.027131 | Perplexity: 799.158297
2025-09-30 13:28:42,257 current_lr 0.000088 at iteration 2200
2025-09-30 13:28:42,399 Stage: Warm Up | Epoch: 2 | Iter: 2200 | Total Loss: 0.042001 | Recon Loss: 0.031277 | Commit Loss: 0.021448 | Perplexity: 800.685017
Trainning Epoch:   0%|          | 3/665 [41:10<153:38:24, 835.51s/it]2025-09-30 13:32:18,594 current_lr 0.000096 at iteration 2400
2025-09-30 13:32:18,748 Stage: Warm Up | Epoch: 3 | Iter: 2400 | Total Loss: 0.037807 | Recon Loss: 0.028810 | Commit Loss: 0.017994 | Perplexity: 808.292646
2025-09-30 13:35:44,577 current_lr 0.000104 at iteration 2600
2025-09-30 13:35:44,733 Stage: Warm Up | Epoch: 3 | Iter: 2600 | Total Loss: 0.034924 | Recon Loss: 0.027354 | Commit Loss: 0.015141 | Perplexity: 818.361328
2025-09-30 13:39:14,917 current_lr 0.000112 at iteration 2800
2025-09-30 13:39:15,066 Stage: Warm Up | Epoch: 3 | Iter: 2800 | Total Loss: 0.033391 | Recon Loss: 0.026811 | Commit Loss: 0.013160 | Perplexity: 829.954538
2025-09-30 13:42:41,141 current_lr 0.000120 at iteration 3000
2025-09-30 13:42:41,282 Stage: Warm Up | Epoch: 3 | Iter: 3000 | Total Loss: 0.031903 | Recon Loss: 0.026159 | Commit Loss: 0.011487 | Perplexity: 844.585898
Trainning Epoch:   1%|          | 4/665 [54:18<149:58:55, 816.85s/it]2025-09-30 13:46:20,091 current_lr 0.000128 at iteration 3200
2025-09-30 13:46:20,237 Stage: Warm Up | Epoch: 4 | Iter: 3200 | Total Loss: 0.029870 | Recon Loss: 0.024717 | Commit Loss: 0.010306 | Perplexity: 854.567963
2025-09-30 13:49:56,997 current_lr 0.000136 at iteration 3400
2025-09-30 13:49:57,140 Stage: Warm Up | Epoch: 4 | Iter: 3400 | Total Loss: 0.028536 | Recon Loss: 0.024038 | Commit Loss: 0.008995 | Perplexity: 861.533787
2025-09-30 13:53:29,436 current_lr 0.000144 at iteration 3600
2025-09-30 13:53:29,581 Stage: Warm Up | Epoch: 4 | Iter: 3600 | Total Loss: 0.027334 | Recon Loss: 0.023239 | Commit Loss: 0.008191 | Perplexity: 870.982813
Trainning Epoch:   1%|          | 5/665 [1:07:52<149:33:59, 815.82s/it]2025-09-30 13:57:10,791 current_lr 0.000152 at iteration 3800
2025-09-30 13:57:10,936 Stage: Warm Up | Epoch: 5 | Iter: 3800 | Total Loss: 0.026280 | Recon Loss: 0.022458 | Commit Loss: 0.007644 | Perplexity: 877.500714
2025-09-30 14:00:44,406 current_lr 0.000160 at iteration 4000
2025-09-30 14:00:44,569 Stage: Warm Up | Epoch: 5 | Iter: 4000 | Total Loss: 0.025555 | Recon Loss: 0.022083 | Commit Loss: 0.006943 | Perplexity: 885.919356
2025-09-30 14:04:17,255 current_lr 0.000168 at iteration 4200
2025-09-30 14:04:17,398 Stage: Warm Up | Epoch: 5 | Iter: 4200 | Total Loss: 0.024933 | Recon Loss: 0.021724 | Commit Loss: 0.006419 | Perplexity: 891.338485
2025-09-30 14:07:45,634 current_lr 0.000176 at iteration 4400
2025-09-30 14:07:45,779 Stage: Warm Up | Epoch: 5 | Iter: 4400 | Total Loss: 0.024671 | Recon Loss: 0.021695 | Commit Loss: 0.005953 | Perplexity: 894.689797
Trainning Epoch:   1%|          | 6/665 [1:21:12<148:20:34, 810.37s/it]2025-09-30 14:11:19,432 current_lr 0.000184 at iteration 4600
2025-09-30 14:11:19,586 Stage: Warm Up | Epoch: 6 | Iter: 4600 | Total Loss: 0.024479 | Recon Loss: 0.021732 | Commit Loss: 0.005494 | Perplexity: 897.007899
2025-09-30 14:14:42,508 current_lr 0.000192 at iteration 4800
2025-09-30 14:14:42,653 Stage: Warm Up | Epoch: 6 | Iter: 4800 | Total Loss: 0.023678 | Recon Loss: 0.021007 | Commit Loss: 0.005342 | Perplexity: 902.802957
2025-09-30 14:18:10,555 current_lr 0.000200 at iteration 5000
2025-09-30 14:18:10,698 Stage: Warm Up | Epoch: 6 | Iter: 5000 | Total Loss: 0.023345 | Recon Loss: 0.020855 | Commit Loss: 0.004981 | Perplexity: 906.965840
2025-09-30 14:21:35,084 Stage: Train 0.5 | Epoch: 6 | Iter: 5200 | Total Loss: 0.022292 | Recon Loss: 0.019793 | Commit Loss: 0.004997 | Perplexity: 916.686987
Trainning Epoch:   1%|          | 7/665 [1:34:10<146:08:52, 799.59s/it]2025-09-30 14:25:10,713 Stage: Train 0.5 | Epoch: 7 | Iter: 5400 | Total Loss: 0.021997 | Recon Loss: 0.019486 | Commit Loss: 0.005023 | Perplexity: 920.179172
2025-09-30 14:28:39,483 Stage: Train 0.5 | Epoch: 7 | Iter: 5600 | Total Loss: 0.021659 | Recon Loss: 0.019238 | Commit Loss: 0.004842 | Perplexity: 909.450760
2025-09-30 14:32:15,943 Stage: Train 0.5 | Epoch: 7 | Iter: 5800 | Total Loss: 0.020794 | Recon Loss: 0.018306 | Commit Loss: 0.004974 | Perplexity: 916.537558
2025-09-30 14:35:37,636 Stage: Train 0.5 | Epoch: 7 | Iter: 6000 | Total Loss: 0.020296 | Recon Loss: 0.017886 | Commit Loss: 0.004821 | Perplexity: 911.061770
Trainning Epoch:   1%|          | 8/665 [1:47:28<145:51:34, 799.23s/it]2025-09-30 14:39:18,270 Stage: Train 0.5 | Epoch: 8 | Iter: 6200 | Total Loss: 0.019415 | Recon Loss: 0.016951 | Commit Loss: 0.004929 | Perplexity: 916.663195
2025-09-30 14:42:58,875 Stage: Train 0.5 | Epoch: 8 | Iter: 6400 | Total Loss: 0.019769 | Recon Loss: 0.017364 | Commit Loss: 0.004810 | Perplexity: 919.198333
2025-09-30 14:46:27,030 Stage: Train 0.5 | Epoch: 8 | Iter: 6600 | Total Loss: 0.018843 | Recon Loss: 0.016470 | Commit Loss: 0.004747 | Perplexity: 926.300122
Trainning Epoch:   1%|▏         | 9/665 [2:01:06<146:41:47, 805.04s/it]2025-09-30 14:50:12,786 Stage: Train 0.5 | Epoch: 9 | Iter: 6800 | Total Loss: 0.018976 | Recon Loss: 0.016685 | Commit Loss: 0.004581 | Perplexity: 920.254258
2025-09-30 14:53:41,498 Stage: Train 0.5 | Epoch: 9 | Iter: 7000 | Total Loss: 0.018461 | Recon Loss: 0.016103 | Commit Loss: 0.004718 | Perplexity: 929.800348
2025-09-30 14:57:09,884 Stage: Train 0.5 | Epoch: 9 | Iter: 7200 | Total Loss: 0.018045 | Recon Loss: 0.015694 | Commit Loss: 0.004702 | Perplexity: 928.325179
2025-09-30 15:00:48,348 Stage: Train 0.5 | Epoch: 9 | Iter: 7400 | Total Loss: 0.017779 | Recon Loss: 0.015521 | Commit Loss: 0.004516 | Perplexity: 929.171454
Trainning Epoch:   2%|▏         | 10/665 [2:14:35<146:42:49, 806.37s/it]2025-09-30 15:04:28,919 Stage: Train 0.5 | Epoch: 10 | Iter: 7600 | Total Loss: 0.017301 | Recon Loss: 0.014996 | Commit Loss: 0.004611 | Perplexity: 927.214023
2025-09-30 15:07:54,995 Stage: Train 0.5 | Epoch: 10 | Iter: 7800 | Total Loss: 0.017315 | Recon Loss: 0.014995 | Commit Loss: 0.004640 | Perplexity: 930.348452
2025-09-30 15:11:34,182 Stage: Train 0.5 | Epoch: 10 | Iter: 8000 | Total Loss: 0.016972 | Recon Loss: 0.014724 | Commit Loss: 0.004495 | Perplexity: 924.067373
2025-09-30 15:14:58,427 Stage: Train 0.5 | Epoch: 10 | Iter: 8200 | Total Loss: 0.016523 | Recon Loss: 0.014315 | Commit Loss: 0.004416 | Perplexity: 930.783579
Trainning Epoch:   2%|▏         | 11/665 [2:27:48<145:46:00, 802.39s/it]2025-09-30 15:18:33,561 Stage: Train 0.5 | Epoch: 11 | Iter: 8400 | Total Loss: 0.016275 | Recon Loss: 0.014039 | Commit Loss: 0.004472 | Perplexity: 939.114951
2025-09-30 15:22:03,702 Stage: Train 0.5 | Epoch: 11 | Iter: 8600 | Total Loss: 0.015919 | Recon Loss: 0.013742 | Commit Loss: 0.004354 | Perplexity: 934.277244
2025-09-30 15:25:29,011 Stage: Train 0.5 | Epoch: 11 | Iter: 8800 | Total Loss: 0.016101 | Recon Loss: 0.013937 | Commit Loss: 0.004329 | Perplexity: 932.650449
2025-09-30 15:29:08,129 Stage: Train 0.5 | Epoch: 11 | Iter: 9000 | Total Loss: 0.015937 | Recon Loss: 0.013825 | Commit Loss: 0.004223 | Perplexity: 931.370210
Trainning Epoch:   2%|▏         | 12/665 [2:41:10<145:30:29, 802.19s/it]2025-09-30 15:32:44,314 Stage: Train 0.5 | Epoch: 12 | Iter: 9200 | Total Loss: 0.015561 | Recon Loss: 0.013454 | Commit Loss: 0.004214 | Perplexity: 932.057163
2025-09-30 15:36:11,870 Stage: Train 0.5 | Epoch: 12 | Iter: 9400 | Total Loss: 0.015592 | Recon Loss: 0.013503 | Commit Loss: 0.004179 | Perplexity: 935.136452
2025-09-30 15:39:42,919 Stage: Train 0.5 | Epoch: 12 | Iter: 9600 | Total Loss: 0.015491 | Recon Loss: 0.013397 | Commit Loss: 0.004188 | Perplexity: 931.362390
Trainning Epoch:   2%|▏         | 13/665 [2:54:31<145:11:48, 801.70s/it]2025-09-30 15:43:25,074 Stage: Train 0.5 | Epoch: 13 | Iter: 9800 | Total Loss: 0.015178 | Recon Loss: 0.013121 | Commit Loss: 0.004114 | Perplexity: 931.234963
2025-09-30 15:46:51,368 Stage: Train 0.5 | Epoch: 13 | Iter: 10000 | Total Loss: 0.015290 | Recon Loss: 0.013258 | Commit Loss: 0.004064 | Perplexity: 932.250117
2025-09-30 15:50:23,023 Stage: Train 0.5 | Epoch: 13 | Iter: 10200 | Total Loss: 0.014631 | Recon Loss: 0.012608 | Commit Loss: 0.004044 | Perplexity: 933.895702
2025-09-30 15:53:57,218 Stage: Train 0.5 | Epoch: 13 | Iter: 10400 | Total Loss: 0.014616 | Recon Loss: 0.012584 | Commit Loss: 0.004063 | Perplexity: 934.732565
Trainning Epoch:   2%|▏         | 14/665 [3:07:50<144:50:36, 800.98s/it]2025-09-30 15:57:32,685 Stage: Train 0.5 | Epoch: 14 | Iter: 10600 | Total Loss: 0.014607 | Recon Loss: 0.012602 | Commit Loss: 0.004009 | Perplexity: 933.969688
2025-09-30 16:00:59,605 Stage: Train 0.5 | Epoch: 14 | Iter: 10800 | Total Loss: 0.014519 | Recon Loss: 0.012550 | Commit Loss: 0.003937 | Perplexity: 933.188176
2025-09-30 16:04:26,315 Stage: Train 0.5 | Epoch: 14 | Iter: 11000 | Total Loss: 0.014244 | Recon Loss: 0.012227 | Commit Loss: 0.004034 | Perplexity: 941.633146
2025-09-30 16:07:53,342 Stage: Train 0.5 | Epoch: 14 | Iter: 11200 | Total Loss: 0.014257 | Recon Loss: 0.012313 | Commit Loss: 0.003889 | Perplexity: 934.538417
Trainning Epoch:   2%|▏         | 15/665 [3:21:00<144:01:57, 797.72s/it]2025-09-30 16:11:33,082 Stage: Train 0.5 | Epoch: 15 | Iter: 11400 | Total Loss: 0.014251 | Recon Loss: 0.012325 | Commit Loss: 0.003852 | Perplexity: 934.743237
2025-09-30 16:15:04,758 Stage: Train 0.5 | Epoch: 15 | Iter: 11600 | Total Loss: 0.013575 | Recon Loss: 0.011610 | Commit Loss: 0.003929 | Perplexity: 942.154273
2025-09-30 16:18:30,859 Stage: Train 0.5 | Epoch: 15 | Iter: 11800 | Total Loss: 0.013650 | Recon Loss: 0.011728 | Commit Loss: 0.003844 | Perplexity: 939.075049
2025-09-30 16:21:56,825 Stage: Train 0.5 | Epoch: 15 | Iter: 12000 | Total Loss: 0.014056 | Recon Loss: 0.012163 | Commit Loss: 0.003784 | Perplexity: 937.541420
Trainning Epoch:   2%|▏         | 16/665 [3:34:12<143:29:28, 795.94s/it]2025-09-30 16:25:37,702 Stage: Train 0.5 | Epoch: 16 | Iter: 12200 | Total Loss: 0.013475 | Recon Loss: 0.011586 | Commit Loss: 0.003779 | Perplexity: 943.940191
2025-09-30 16:29:07,346 Stage: Train 0.5 | Epoch: 16 | Iter: 12400 | Total Loss: 0.013438 | Recon Loss: 0.011531 | Commit Loss: 0.003815 | Perplexity: 945.623067
2025-09-30 16:32:38,158 Stage: Train 0.5 | Epoch: 16 | Iter: 12600 | Total Loss: 0.013484 | Recon Loss: 0.011630 | Commit Loss: 0.003708 | Perplexity: 936.610930
2025-09-30 16:36:08,166 Stage: Train 0.5 | Epoch: 16 | Iter: 12800 | Total Loss: 0.013445 | Recon Loss: 0.011638 | Commit Loss: 0.003615 | Perplexity: 938.307645
Trainning Epoch:   3%|▎         | 17/665 [3:47:36<143:42:38, 798.39s/it]2025-09-30 16:39:54,072 Stage: Train 0.5 | Epoch: 17 | Iter: 13000 | Total Loss: 0.013061 | Recon Loss: 0.011220 | Commit Loss: 0.003681 | Perplexity: 939.081642
2025-09-30 16:43:24,047 Stage: Train 0.5 | Epoch: 17 | Iter: 13200 | Total Loss: 0.012824 | Recon Loss: 0.010979 | Commit Loss: 0.003690 | Perplexity: 943.194609
2025-09-30 16:46:55,451 Stage: Train 0.5 | Epoch: 17 | Iter: 13400 | Total Loss: 0.012994 | Recon Loss: 0.011207 | Commit Loss: 0.003575 | Perplexity: 945.505698
Trainning Epoch:   3%|▎         | 18/665 [4:01:02<143:53:52, 800.67s/it]2025-09-30 16:50:28,790 Stage: Train 0.5 | Epoch: 18 | Iter: 13600 | Total Loss: 0.012962 | Recon Loss: 0.011193 | Commit Loss: 0.003539 | Perplexity: 940.521805
2025-09-30 16:53:55,596 Stage: Train 0.5 | Epoch: 18 | Iter: 13800 | Total Loss: 0.012810 | Recon Loss: 0.011062 | Commit Loss: 0.003497 | Perplexity: 945.068512
2025-09-30 16:57:24,040 Stage: Train 0.5 | Epoch: 18 | Iter: 14000 | Total Loss: 0.012705 | Recon Loss: 0.010952 | Commit Loss: 0.003505 | Perplexity: 942.683796
2025-09-30 17:00:57,588 Stage: Train 0.5 | Epoch: 18 | Iter: 14200 | Total Loss: 0.012323 | Recon Loss: 0.010581 | Commit Loss: 0.003484 | Perplexity: 942.018829
Trainning Epoch:   3%|▎         | 19/665 [4:14:23<143:41:03, 800.72s/it]2025-09-30 17:04:51,222 Stage: Train 0.5 | Epoch: 19 | Iter: 14400 | Total Loss: 0.012499 | Recon Loss: 0.010789 | Commit Loss: 0.003418 | Perplexity: 939.168596
2025-09-30 17:08:41,853 Stage: Train 0.5 | Epoch: 19 | Iter: 14600 | Total Loss: 0.012221 | Recon Loss: 0.010506 | Commit Loss: 0.003431 | Perplexity: 937.353663
2025-09-30 17:12:25,012 Stage: Train 0.5 | Epoch: 19 | Iter: 14800 | Total Loss: 0.012308 | Recon Loss: 0.010619 | Commit Loss: 0.003378 | Perplexity: 939.327515
2025-09-30 17:15:51,667 Stage: Train 0.5 | Epoch: 19 | Iter: 15000 | Total Loss: 0.011952 | Recon Loss: 0.010283 | Commit Loss: 0.003338 | Perplexity: 939.353970
Trainning Epoch:   3%|▎         | 20/665 [4:28:19<145:21:20, 811.29s/it]2025-09-30 17:19:27,587 Stage: Train 0.5 | Epoch: 20 | Iter: 15200 | Total Loss: 0.012096 | Recon Loss: 0.010449 | Commit Loss: 0.003295 | Perplexity: 933.769504
2025-09-30 17:22:55,949 Stage: Train 0.5 | Epoch: 20 | Iter: 15400 | Total Loss: 0.012064 | Recon Loss: 0.010385 | Commit Loss: 0.003358 | Perplexity: 941.285806
2025-09-30 17:26:22,646 Stage: Train 0.5 | Epoch: 20 | Iter: 15600 | Total Loss: 0.011988 | Recon Loss: 0.010339 | Commit Loss: 0.003299 | Perplexity: 934.770913
2025-09-30 17:29:54,528 Stage: Train 0.5 | Epoch: 20 | Iter: 15800 | Total Loss: 0.011854 | Recon Loss: 0.010161 | Commit Loss: 0.003386 | Perplexity: 942.674603
Trainning Epoch:   3%|▎         | 21/665 [4:41:33<144:13:07, 806.19s/it]2025-09-30 17:33:37,850 Stage: Train 0.5 | Epoch: 21 | Iter: 16000 | Total Loss: 0.011866 | Recon Loss: 0.010220 | Commit Loss: 0.003291 | Perplexity: 938.975683
2025-09-30 17:37:11,025 Stage: Train 0.5 | Epoch: 21 | Iter: 16200 | Total Loss: 0.012053 | Recon Loss: 0.010459 | Commit Loss: 0.003188 | Perplexity: 931.828980
2025-09-30 17:40:40,359 Stage: Train 0.5 | Epoch: 21 | Iter: 16400 | Total Loss: 0.011682 | Recon Loss: 0.010054 | Commit Loss: 0.003257 | Perplexity: 937.974581
Trainning Epoch:   3%|▎         | 22/665 [4:54:57<143:52:19, 805.51s/it]2025-09-30 17:44:16,019 Stage: Train 0.5 | Epoch: 22 | Iter: 16600 | Total Loss: 0.011682 | Recon Loss: 0.010043 | Commit Loss: 0.003278 | Perplexity: 942.298335
2025-09-30 17:47:43,113 Stage: Train 0.5 | Epoch: 22 | Iter: 16800 | Total Loss: 0.011883 | Recon Loss: 0.010284 | Commit Loss: 0.003198 | Perplexity: 938.075609
2025-09-30 17:51:09,029 Stage: Train 0.5 | Epoch: 22 | Iter: 17000 | Total Loss: 0.011759 | Recon Loss: 0.010166 | Commit Loss: 0.003186 | Perplexity: 937.549268
2025-09-30 17:54:37,104 Stage: Train 0.5 | Epoch: 22 | Iter: 17200 | Total Loss: 0.011397 | Recon Loss: 0.009796 | Commit Loss: 0.003201 | Perplexity: 941.780779
Trainning Epoch:   3%|▎         | 23/665 [5:08:06<142:43:59, 800.37s/it]2025-09-30 17:58:11,972 Stage: Train 0.5 | Epoch: 23 | Iter: 17400 | Total Loss: 0.011455 | Recon Loss: 0.009852 | Commit Loss: 0.003208 | Perplexity: 938.004389
2025-09-30 18:01:38,054 Stage: Train 0.5 | Epoch: 23 | Iter: 17600 | Total Loss: 0.011335 | Recon Loss: 0.009724 | Commit Loss: 0.003223 | Perplexity: 938.530342
2025-09-30 18:05:01,646 Stage: Train 0.5 | Epoch: 23 | Iter: 17800 | Total Loss: 0.011688 | Recon Loss: 0.010116 | Commit Loss: 0.003146 | Perplexity: 934.193314
2025-09-30 18:08:31,349 Stage: Train 0.5 | Epoch: 23 | Iter: 18000 | Total Loss: 0.011292 | Recon Loss: 0.009671 | Commit Loss: 0.003242 | Perplexity: 942.715310
Trainning Epoch:   4%|▎         | 24/665 [5:21:11<141:42:19, 795.85s/it]2025-09-30 18:12:07,092 Stage: Train 0.5 | Epoch: 24 | Iter: 18200 | Total Loss: 0.011345 | Recon Loss: 0.009775 | Commit Loss: 0.003139 | Perplexity: 934.549037
2025-09-30 18:15:35,583 Stage: Train 0.5 | Epoch: 24 | Iter: 18400 | Total Loss: 0.011029 | Recon Loss: 0.009435 | Commit Loss: 0.003187 | Perplexity: 940.238314
2025-09-30 18:19:06,191 Stage: Train 0.5 | Epoch: 24 | Iter: 18600 | Total Loss: 0.011370 | Recon Loss: 0.009750 | Commit Loss: 0.003239 | Perplexity: 939.747572
2025-09-30 18:22:37,732 Stage: Train 0.5 | Epoch: 24 | Iter: 18800 | Total Loss: 0.011185 | Recon Loss: 0.009591 | Commit Loss: 0.003188 | Perplexity: 937.451242
Trainning Epoch:   4%|▍         | 25/665 [5:34:30<141:40:37, 796.93s/it]2025-09-30 18:26:14,032 Stage: Train 0.5 | Epoch: 25 | Iter: 19000 | Total Loss: 0.011000 | Recon Loss: 0.009424 | Commit Loss: 0.003153 | Perplexity: 938.637085
2025-09-30 18:29:40,060 Stage: Train 0.5 | Epoch: 25 | Iter: 19200 | Total Loss: 0.010849 | Recon Loss: 0.009241 | Commit Loss: 0.003217 | Perplexity: 945.027943
2025-09-30 18:33:07,551 Stage: Train 0.5 | Epoch: 25 | Iter: 19400 | Total Loss: 0.011070 | Recon Loss: 0.009490 | Commit Loss: 0.003160 | Perplexity: 937.699159
Trainning Epoch:   4%|▍         | 26/665 [5:47:35<140:47:39, 793.21s/it]2025-09-30 18:36:39,780 Stage: Train 0.5 | Epoch: 26 | Iter: 19600 | Total Loss: 0.010965 | Recon Loss: 0.009371 | Commit Loss: 0.003188 | Perplexity: 934.872163
2025-09-30 18:40:11,539 Stage: Train 0.5 | Epoch: 26 | Iter: 19800 | Total Loss: 0.010932 | Recon Loss: 0.009359 | Commit Loss: 0.003145 | Perplexity: 938.495999
2025-09-30 18:43:40,262 Stage: Train 0.5 | Epoch: 26 | Iter: 20000 | Total Loss: 0.010893 | Recon Loss: 0.009315 | Commit Loss: 0.003155 | Perplexity: 938.190694
2025-09-30 18:43:40,262 Saving model at iteration 20000
2025-09-30 18:43:40,919 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000
2025-09-30 18:43:41,248 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000/model.safetensors
2025-09-30 18:43:41,665 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000/optimizer.bin
2025-09-30 18:43:41,666 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000/scheduler.bin
2025-09-30 18:43:41,666 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000/sampler.bin
2025-09-30 18:43:41,667 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_27_step_20000/random_states_0.pkl
2025-09-30 18:47:05,503 Stage: Train 0.5 | Epoch: 26 | Iter: 20200 | Total Loss: 0.010793 | Recon Loss: 0.009216 | Commit Loss: 0.003154 | Perplexity: 939.084290
Trainning Epoch:   4%|▍         | 27/665 [6:00:49<140:36:39, 793.42s/it]2025-09-30 18:50:42,148 Stage: Train 0.5 | Epoch: 27 | Iter: 20400 | Total Loss: 0.010938 | Recon Loss: 0.009380 | Commit Loss: 0.003115 | Perplexity: 935.621838
2025-09-30 18:54:03,741 Stage: Train 0.5 | Epoch: 27 | Iter: 20600 | Total Loss: 0.010727 | Recon Loss: 0.009159 | Commit Loss: 0.003135 | Perplexity: 935.177560
2025-09-30 18:57:28,371 Stage: Train 0.5 | Epoch: 27 | Iter: 20800 | Total Loss: 0.010664 | Recon Loss: 0.009082 | Commit Loss: 0.003165 | Perplexity: 942.292488
2025-09-30 19:00:57,042 Stage: Train 0.5 | Epoch: 27 | Iter: 21000 | Total Loss: 0.010804 | Recon Loss: 0.009237 | Commit Loss: 0.003135 | Perplexity: 937.267007
Trainning Epoch:   4%|▍         | 28/665 [6:13:49<139:43:01, 789.61s/it]2025-09-30 19:04:31,303 Stage: Train 0.5 | Epoch: 28 | Iter: 21200 | Total Loss: 0.010632 | Recon Loss: 0.009081 | Commit Loss: 0.003102 | Perplexity: 937.951777
2025-09-30 19:07:54,669 Stage: Train 0.5 | Epoch: 28 | Iter: 21400 | Total Loss: 0.010655 | Recon Loss: 0.009080 | Commit Loss: 0.003148 | Perplexity: 936.652230
2025-09-30 19:11:26,354 Stage: Train 0.5 | Epoch: 28 | Iter: 21600 | Total Loss: 0.010513 | Recon Loss: 0.008950 | Commit Loss: 0.003126 | Perplexity: 932.914137
2025-09-30 19:14:59,618 Stage: Train 0.5 | Epoch: 28 | Iter: 21800 | Total Loss: 0.010558 | Recon Loss: 0.008982 | Commit Loss: 0.003150 | Perplexity: 942.694476
Trainning Epoch:   4%|▍         | 29/665 [6:27:03<139:41:40, 790.72s/it]2025-09-30 19:18:43,429 Stage: Train 0.5 | Epoch: 29 | Iter: 22000 | Total Loss: 0.010844 | Recon Loss: 0.009317 | Commit Loss: 0.003056 | Perplexity: 934.027110
2025-09-30 19:22:13,706 Stage: Train 0.5 | Epoch: 29 | Iter: 22200 | Total Loss: 0.010327 | Recon Loss: 0.008759 | Commit Loss: 0.003135 | Perplexity: 938.469540
2025-09-30 19:25:39,728 Stage: Train 0.5 | Epoch: 29 | Iter: 22400 | Total Loss: 0.010622 | Recon Loss: 0.009079 | Commit Loss: 0.003087 | Perplexity: 933.986178
Trainning Epoch:   5%|▍         | 30/665 [6:40:20<139:48:37, 792.63s/it]2025-09-30 19:29:12,152 Stage: Train 0.5 | Epoch: 30 | Iter: 22600 | Total Loss: 0.010357 | Recon Loss: 0.008816 | Commit Loss: 0.003081 | Perplexity: 935.585140
2025-09-30 19:32:45,296 Stage: Train 0.5 | Epoch: 30 | Iter: 22800 | Total Loss: 0.010464 | Recon Loss: 0.008925 | Commit Loss: 0.003077 | Perplexity: 935.212972
2025-09-30 19:36:15,571 Stage: Train 0.5 | Epoch: 30 | Iter: 23000 | Total Loss: 0.010492 | Recon Loss: 0.008958 | Commit Loss: 0.003068 | Perplexity: 932.987290
2025-09-30 19:39:43,851 Stage: Train 0.5 | Epoch: 30 | Iter: 23200 | Total Loss: 0.010279 | Recon Loss: 0.008718 | Commit Loss: 0.003122 | Perplexity: 939.408427
Trainning Epoch:   5%|▍         | 31/665 [6:53:40<139:59:02, 794.86s/it]2025-09-30 19:43:23,043 Stage: Train 0.5 | Epoch: 31 | Iter: 23400 | Total Loss: 0.010539 | Recon Loss: 0.009016 | Commit Loss: 0.003045 | Perplexity: 932.805315
2025-09-30 19:46:48,456 Stage: Train 0.5 | Epoch: 31 | Iter: 23600 | Total Loss: 0.010427 | Recon Loss: 0.008898 | Commit Loss: 0.003057 | Perplexity: 928.553636
2025-09-30 19:50:15,356 Stage: Train 0.5 | Epoch: 31 | Iter: 23800 | Total Loss: 0.010264 | Recon Loss: 0.008720 | Commit Loss: 0.003087 | Perplexity: 934.469356
2025-09-30 19:53:42,902 Stage: Train 0.5 | Epoch: 31 | Iter: 24000 | Total Loss: 0.010157 | Recon Loss: 0.008619 | Commit Loss: 0.003075 | Perplexity: 932.601834
Trainning Epoch:   5%|▍         | 32/665 [7:06:50<139:31:02, 793.46s/it]2025-09-30 19:57:25,556 Stage: Train 0.5 | Epoch: 32 | Iter: 24200 | Total Loss: 0.010071 | Recon Loss: 0.008511 | Commit Loss: 0.003119 | Perplexity: 935.900119
2025-09-30 20:00:58,218 Stage: Train 0.5 | Epoch: 32 | Iter: 24400 | Total Loss: 0.010272 | Recon Loss: 0.008725 | Commit Loss: 0.003094 | Perplexity: 933.430585
2025-09-30 20:04:21,873 Stage: Train 0.5 | Epoch: 32 | Iter: 24600 | Total Loss: 0.010167 | Recon Loss: 0.008648 | Commit Loss: 0.003038 | Perplexity: 933.231619
2025-09-30 20:07:55,997 Stage: Train 0.5 | Epoch: 32 | Iter: 24800 | Total Loss: 0.010098 | Recon Loss: 0.008561 | Commit Loss: 0.003073 | Perplexity: 933.515491
Trainning Epoch:   5%|▍         | 33/665 [7:20:14<139:49:35, 796.48s/it]2025-09-30 20:11:33,184 Stage: Train 0.5 | Epoch: 33 | Iter: 25000 | Total Loss: 0.010315 | Recon Loss: 0.008782 | Commit Loss: 0.003066 | Perplexity: 928.977052
2025-09-30 20:14:59,914 Stage: Train 0.5 | Epoch: 33 | Iter: 25200 | Total Loss: 0.009848 | Recon Loss: 0.008313 | Commit Loss: 0.003070 | Perplexity: 939.541104
2025-09-30 20:18:25,740 Stage: Train 0.5 | Epoch: 33 | Iter: 25400 | Total Loss: 0.010248 | Recon Loss: 0.008727 | Commit Loss: 0.003043 | Perplexity: 932.486987
2025-09-30 20:21:46,169 Stage: Train 0.5 | Epoch: 33 | Iter: 25600 | Total Loss: 0.009891 | Recon Loss: 0.008367 | Commit Loss: 0.003048 | Perplexity: 937.820146
Trainning Epoch:   5%|▌         | 34/665 [7:33:15<138:48:00, 791.89s/it]2025-09-30 20:25:19,746 Stage: Train 0.5 | Epoch: 34 | Iter: 25800 | Total Loss: 0.009958 | Recon Loss: 0.008466 | Commit Loss: 0.002985 | Perplexity: 934.198884
2025-09-30 20:28:40,235 Stage: Train 0.5 | Epoch: 34 | Iter: 26000 | Total Loss: 0.010032 | Recon Loss: 0.008529 | Commit Loss: 0.003005 | Perplexity: 938.915111
2025-09-30 20:32:05,584 Stage: Train 0.5 | Epoch: 34 | Iter: 26200 | Total Loss: 0.009837 | Recon Loss: 0.008333 | Commit Loss: 0.003009 | Perplexity: 937.078584
Trainning Epoch:   5%|▌         | 35/665 [7:46:11<137:46:30, 787.29s/it]2025-09-30 20:35:39,568 Stage: Train 0.5 | Epoch: 35 | Iter: 26400 | Total Loss: 0.009823 | Recon Loss: 0.008308 | Commit Loss: 0.003030 | Perplexity: 938.779636
2025-09-30 20:39:03,553 Stage: Train 0.5 | Epoch: 35 | Iter: 26600 | Total Loss: 0.009815 | Recon Loss: 0.008319 | Commit Loss: 0.002992 | Perplexity: 937.491770
2025-09-30 20:42:28,746 Stage: Train 0.5 | Epoch: 35 | Iter: 26800 | Total Loss: 0.009908 | Recon Loss: 0.008414 | Commit Loss: 0.002989 | Perplexity: 938.499756
2025-09-30 20:45:57,949 Stage: Train 0.5 | Epoch: 35 | Iter: 27000 | Total Loss: 0.009861 | Recon Loss: 0.008380 | Commit Loss: 0.002964 | Perplexity: 936.443123
Trainning Epoch:   5%|▌         | 36/665 [7:59:14<137:18:25, 785.86s/it]2025-09-30 20:49:33,073 Stage: Train 0.5 | Epoch: 36 | Iter: 27200 | Total Loss: 0.009775 | Recon Loss: 0.008276 | Commit Loss: 0.002997 | Perplexity: 935.104442
2025-09-30 20:52:54,813 Stage: Train 0.5 | Epoch: 36 | Iter: 27400 | Total Loss: 0.009689 | Recon Loss: 0.008180 | Commit Loss: 0.003017 | Perplexity: 943.222521
2025-09-30 20:56:27,786 Stage: Train 0.5 | Epoch: 36 | Iter: 27600 | Total Loss: 0.009784 | Recon Loss: 0.008306 | Commit Loss: 0.002957 | Perplexity: 936.807129
2025-09-30 20:59:52,657 Stage: Train 0.5 | Epoch: 36 | Iter: 27800 | Total Loss: 0.009842 | Recon Loss: 0.008351 | Commit Loss: 0.002982 | Perplexity: 935.973217
Trainning Epoch:   6%|▌         | 37/665 [8:12:19<137:04:21, 785.77s/it]2025-09-30 21:03:25,527 Stage: Train 0.5 | Epoch: 37 | Iter: 28000 | Total Loss: 0.009485 | Recon Loss: 0.008003 | Commit Loss: 0.002964 | Perplexity: 935.958647
2025-09-30 21:06:55,696 Stage: Train 0.5 | Epoch: 37 | Iter: 28200 | Total Loss: 0.009795 | Recon Loss: 0.008290 | Commit Loss: 0.003010 | Perplexity: 940.332016
2025-09-30 21:10:29,781 Stage: Train 0.5 | Epoch: 37 | Iter: 28400 | Total Loss: 0.009582 | Recon Loss: 0.008092 | Commit Loss: 0.002981 | Perplexity: 937.642997
2025-09-30 21:13:58,885 Stage: Train 0.5 | Epoch: 37 | Iter: 28600 | Total Loss: 0.009537 | Recon Loss: 0.008064 | Commit Loss: 0.002946 | Perplexity: 936.523916
Trainning Epoch:   6%|▌         | 38/665 [8:25:38<137:30:52, 789.56s/it]2025-09-30 21:17:34,596 Stage: Train 0.5 | Epoch: 38 | Iter: 28800 | Total Loss: 0.009560 | Recon Loss: 0.008088 | Commit Loss: 0.002945 | Perplexity: 936.574281
2025-09-30 21:21:04,410 Stage: Train 0.5 | Epoch: 38 | Iter: 29000 | Total Loss: 0.009638 | Recon Loss: 0.008166 | Commit Loss: 0.002944 | Perplexity: 935.442899
2025-09-30 21:24:36,689 Stage: Train 0.5 | Epoch: 38 | Iter: 29200 | Total Loss: 0.009756 | Recon Loss: 0.008287 | Commit Loss: 0.002937 | Perplexity: 936.220168
Trainning Epoch:   6%|▌         | 39/665 [8:38:55<137:41:09, 791.80s/it]2025-09-30 21:28:15,053 Stage: Train 0.5 | Epoch: 39 | Iter: 29400 | Total Loss: 0.009401 | Recon Loss: 0.007922 | Commit Loss: 0.002959 | Perplexity: 937.901632
2025-09-30 21:31:36,962 Stage: Train 0.5 | Epoch: 39 | Iter: 29600 | Total Loss: 0.009681 | Recon Loss: 0.008209 | Commit Loss: 0.002943 | Perplexity: 937.535015
2025-09-30 21:35:00,558 Stage: Train 0.5 | Epoch: 39 | Iter: 29800 | Total Loss: 0.009438 | Recon Loss: 0.007962 | Commit Loss: 0.002953 | Perplexity: 938.241307
2025-09-30 21:38:28,150 Stage: Train 0.5 | Epoch: 39 | Iter: 30000 | Total Loss: 0.009556 | Recon Loss: 0.008104 | Commit Loss: 0.002903 | Perplexity: 934.532994
Trainning Epoch:   6%|▌         | 40/665 [8:51:57<136:56:30, 788.78s/it]2025-09-30 21:42:01,790 Stage: Train 0.5 | Epoch: 40 | Iter: 30200 | Total Loss: 0.009407 | Recon Loss: 0.007928 | Commit Loss: 0.002959 | Perplexity: 935.978095
2025-09-30 21:45:29,304 Stage: Train 0.5 | Epoch: 40 | Iter: 30400 | Total Loss: 0.009383 | Recon Loss: 0.007913 | Commit Loss: 0.002939 | Perplexity: 934.900912
2025-09-30 21:49:00,092 Stage: Train 0.5 | Epoch: 40 | Iter: 30600 | Total Loss: 0.009426 | Recon Loss: 0.007965 | Commit Loss: 0.002922 | Perplexity: 935.328698
2025-09-30 21:52:31,350 Stage: Train 0.5 | Epoch: 40 | Iter: 30800 | Total Loss: 0.009444 | Recon Loss: 0.007968 | Commit Loss: 0.002952 | Perplexity: 938.536743
Trainning Epoch:   6%|▌         | 41/665 [9:05:09<136:53:58, 789.80s/it]2025-09-30 21:56:05,776 Stage: Train 0.5 | Epoch: 41 | Iter: 31000 | Total Loss: 0.009541 | Recon Loss: 0.008082 | Commit Loss: 0.002918 | Perplexity: 937.042344
2025-09-30 21:59:34,105 Stage: Train 0.5 | Epoch: 41 | Iter: 31200 | Total Loss: 0.009323 | Recon Loss: 0.007868 | Commit Loss: 0.002911 | Perplexity: 936.040826
2025-09-30 22:03:00,447 Stage: Train 0.5 | Epoch: 41 | Iter: 31400 | Total Loss: 0.009229 | Recon Loss: 0.007774 | Commit Loss: 0.002909 | Perplexity: 938.598390
2025-09-30 22:06:31,175 Stage: Train 0.5 | Epoch: 41 | Iter: 31600 | Total Loss: 0.009463 | Recon Loss: 0.008005 | Commit Loss: 0.002916 | Perplexity: 940.725086
Trainning Epoch:   6%|▋         | 42/665 [9:18:24<136:56:25, 791.31s/it]2025-09-30 22:10:08,420 Stage: Train 0.5 | Epoch: 42 | Iter: 31800 | Total Loss: 0.009271 | Recon Loss: 0.007852 | Commit Loss: 0.002839 | Perplexity: 931.860517
2025-09-30 22:13:40,608 Stage: Train 0.5 | Epoch: 42 | Iter: 32000 | Total Loss: 0.009431 | Recon Loss: 0.007983 | Commit Loss: 0.002896 | Perplexity: 935.745621
2025-09-30 22:17:06,082 Stage: Train 0.5 | Epoch: 42 | Iter: 32200 | Total Loss: 0.009055 | Recon Loss: 0.007617 | Commit Loss: 0.002877 | Perplexity: 934.881058
Trainning Epoch:   6%|▋         | 43/665 [9:31:33<136:37:49, 790.79s/it]2025-09-30 22:20:37,995 Stage: Train 0.5 | Epoch: 43 | Iter: 32400 | Total Loss: 0.009325 | Recon Loss: 0.007893 | Commit Loss: 0.002864 | Perplexity: 932.168038
2025-09-30 22:24:11,667 Stage: Train 0.5 | Epoch: 43 | Iter: 32600 | Total Loss: 0.009175 | Recon Loss: 0.007751 | Commit Loss: 0.002848 | Perplexity: 935.282030
2025-09-30 22:27:41,807 Stage: Train 0.5 | Epoch: 43 | Iter: 32800 | Total Loss: 0.009281 | Recon Loss: 0.007841 | Commit Loss: 0.002879 | Perplexity: 930.851505
2025-09-30 22:31:10,362 Stage: Train 0.5 | Epoch: 43 | Iter: 33000 | Total Loss: 0.009165 | Recon Loss: 0.007739 | Commit Loss: 0.002852 | Perplexity: 934.678369
Trainning Epoch:   7%|▋         | 44/665 [9:44:53<136:54:03, 793.63s/it]2025-09-30 22:34:46,618 Stage: Train 0.5 | Epoch: 44 | Iter: 33200 | Total Loss: 0.009189 | Recon Loss: 0.007774 | Commit Loss: 0.002829 | Perplexity: 931.569709
2025-09-30 22:38:19,399 Stage: Train 0.5 | Epoch: 44 | Iter: 33400 | Total Loss: 0.009129 | Recon Loss: 0.007701 | Commit Loss: 0.002856 | Perplexity: 930.846865
2025-09-30 22:41:51,940 Stage: Train 0.5 | Epoch: 44 | Iter: 33600 | Total Loss: 0.009043 | Recon Loss: 0.007620 | Commit Loss: 0.002846 | Perplexity: 930.659024
2025-09-30 22:45:20,478 Stage: Train 0.5 | Epoch: 44 | Iter: 33800 | Total Loss: 0.009072 | Recon Loss: 0.007662 | Commit Loss: 0.002819 | Perplexity: 931.624743
Trainning Epoch:   7%|▋         | 45/665 [9:58:17<137:10:53, 796.54s/it]2025-09-30 22:49:00,501 Stage: Train 0.5 | Epoch: 45 | Iter: 34000 | Total Loss: 0.009005 | Recon Loss: 0.007594 | Commit Loss: 0.002822 | Perplexity: 929.894169
2025-09-30 22:52:31,471 Stage: Train 0.5 | Epoch: 45 | Iter: 34200 | Total Loss: 0.008992 | Recon Loss: 0.007576 | Commit Loss: 0.002834 | Perplexity: 933.053160
2025-09-30 22:55:54,592 Stage: Train 0.5 | Epoch: 45 | Iter: 34400 | Total Loss: 0.008951 | Recon Loss: 0.007535 | Commit Loss: 0.002831 | Perplexity: 931.509389
2025-09-30 22:59:22,452 Stage: Train 0.5 | Epoch: 45 | Iter: 34600 | Total Loss: 0.009068 | Recon Loss: 0.007654 | Commit Loss: 0.002828 | Perplexity: 928.946959
Trainning Epoch:   7%|▋         | 46/665 [10:11:25<136:33:25, 794.19s/it]2025-09-30 23:03:00,231 Stage: Train 0.5 | Epoch: 46 | Iter: 34800 | Total Loss: 0.009123 | Recon Loss: 0.007717 | Commit Loss: 0.002810 | Perplexity: 927.863748
2025-09-30 23:06:24,068 Stage: Train 0.5 | Epoch: 46 | Iter: 35000 | Total Loss: 0.008859 | Recon Loss: 0.007451 | Commit Loss: 0.002816 | Perplexity: 932.138687
2025-09-30 23:09:49,454 Stage: Train 0.5 | Epoch: 46 | Iter: 35200 | Total Loss: 0.008958 | Recon Loss: 0.007579 | Commit Loss: 0.002759 | Perplexity: 926.687303
Trainning Epoch:   7%|▋         | 47/665 [10:24:32<135:55:25, 791.79s/it]2025-09-30 23:13:24,415 Stage: Train 0.5 | Epoch: 47 | Iter: 35400 | Total Loss: 0.008961 | Recon Loss: 0.007558 | Commit Loss: 0.002806 | Perplexity: 927.385477
2025-09-30 23:16:49,842 Stage: Train 0.5 | Epoch: 47 | Iter: 35600 | Total Loss: 0.008954 | Recon Loss: 0.007564 | Commit Loss: 0.002780 | Perplexity: 926.269418
2025-09-30 23:20:22,188 Stage: Train 0.5 | Epoch: 47 | Iter: 35800 | Total Loss: 0.008883 | Recon Loss: 0.007484 | Commit Loss: 0.002798 | Perplexity: 931.067276
2025-09-30 23:23:53,454 Stage: Train 0.5 | Epoch: 47 | Iter: 36000 | Total Loss: 0.008983 | Recon Loss: 0.007592 | Commit Loss: 0.002781 | Perplexity: 928.852362
Trainning Epoch:   7%|▋         | 48/665 [10:37:47<135:52:52, 792.82s/it]2025-09-30 23:27:25,554 Stage: Train 0.5 | Epoch: 48 | Iter: 36200 | Total Loss: 0.008927 | Recon Loss: 0.007537 | Commit Loss: 0.002780 | Perplexity: 926.477088
2025-09-30 23:30:52,459 Stage: Train 0.5 | Epoch: 48 | Iter: 36400 | Total Loss: 0.008917 | Recon Loss: 0.007541 | Commit Loss: 0.002752 | Perplexity: 926.498326
2025-09-30 23:34:19,849 Stage: Train 0.5 | Epoch: 48 | Iter: 36600 | Total Loss: 0.008810 | Recon Loss: 0.007434 | Commit Loss: 0.002752 | Perplexity: 927.242302
2025-09-30 23:37:45,500 Stage: Train 0.5 | Epoch: 48 | Iter: 36800 | Total Loss: 0.008805 | Recon Loss: 0.007420 | Commit Loss: 0.002771 | Perplexity: 926.130021
Trainning Epoch:   7%|▋         | 49/665 [10:51:01<135:44:38, 793.31s/it]2025-09-30 23:41:32,006 Stage: Train 0.5 | Epoch: 49 | Iter: 37000 | Total Loss: 0.008752 | Recon Loss: 0.007348 | Commit Loss: 0.002807 | Perplexity: 935.661309
2025-09-30 23:44:59,335 Stage: Train 0.5 | Epoch: 49 | Iter: 37200 | Total Loss: 0.008884 | Recon Loss: 0.007507 | Commit Loss: 0.002753 | Perplexity: 927.543262
2025-09-30 23:48:30,255 Stage: Train 0.5 | Epoch: 49 | Iter: 37400 | Total Loss: 0.008801 | Recon Loss: 0.007424 | Commit Loss: 0.002754 | Perplexity: 929.626589
2025-09-30 23:51:54,633 Stage: Train 0.5 | Epoch: 49 | Iter: 37600 | Total Loss: 0.008834 | Recon Loss: 0.007457 | Commit Loss: 0.002753 | Perplexity: 929.177254
Trainning Epoch:   8%|▊         | 50/665 [11:04:13<135:24:56, 792.68s/it]2025-09-30 23:55:34,646 Stage: Train 0.5 | Epoch: 50 | Iter: 37800 | Total Loss: 0.008752 | Recon Loss: 0.007364 | Commit Loss: 0.002775 | Perplexity: 928.333535
2025-09-30 23:59:06,390 Stage: Train 0.5 | Epoch: 50 | Iter: 38000 | Total Loss: 0.008673 | Recon Loss: 0.007288 | Commit Loss: 0.002769 | Perplexity: 932.376438
2025-10-01 00:02:36,493 Stage: Train 0.5 | Epoch: 50 | Iter: 38200 | Total Loss: 0.008707 | Recon Loss: 0.007346 | Commit Loss: 0.002721 | Perplexity: 928.551891
2025-10-01 00:06:15,936 Stage: Train 0.5 | Epoch: 50 | Iter: 38400 | Total Loss: 0.008815 | Recon Loss: 0.007435 | Commit Loss: 0.002760 | Perplexity: 930.828680
Trainning Epoch:   8%|▊         | 51/665 [11:17:45<136:13:09, 798.68s/it]2025-10-01 00:09:55,455 Stage: Train 0.5 | Epoch: 51 | Iter: 38600 | Total Loss: 0.008734 | Recon Loss: 0.007344 | Commit Loss: 0.002780 | Perplexity: 927.122513
2025-10-01 00:13:21,771 Stage: Train 0.5 | Epoch: 51 | Iter: 38800 | Total Loss: 0.008649 | Recon Loss: 0.007297 | Commit Loss: 0.002704 | Perplexity: 931.506558
2025-10-01 00:16:53,374 Stage: Train 0.5 | Epoch: 51 | Iter: 39000 | Total Loss: 0.008667 | Recon Loss: 0.007290 | Commit Loss: 0.002753 | Perplexity: 928.685070
Trainning Epoch:   8%|▊         | 52/665 [11:31:05<136:04:13, 799.11s/it]2025-10-01 00:20:34,910 Stage: Train 0.5 | Epoch: 52 | Iter: 39200 | Total Loss: 0.008671 | Recon Loss: 0.007291 | Commit Loss: 0.002761 | Perplexity: 927.761964
2025-10-01 00:24:08,841 Stage: Train 0.5 | Epoch: 52 | Iter: 39400 | Total Loss: 0.008501 | Recon Loss: 0.007145 | Commit Loss: 0.002712 | Perplexity: 926.235529
2025-10-01 00:27:40,683 Stage: Train 0.5 | Epoch: 52 | Iter: 39600 | Total Loss: 0.008739 | Recon Loss: 0.007358 | Commit Loss: 0.002762 | Perplexity: 936.067378
2025-10-01 00:31:15,734 Stage: Train 0.5 | Epoch: 52 | Iter: 39800 | Total Loss: 0.008448 | Recon Loss: 0.007070 | Commit Loss: 0.002756 | Perplexity: 932.954884
Trainning Epoch:   8%|▊         | 53/665 [11:44:33<136:18:01, 801.77s/it]2025-10-01 00:34:49,507 Stage: Train 0.5 | Epoch: 53 | Iter: 40000 | Total Loss: 0.008710 | Recon Loss: 0.007342 | Commit Loss: 0.002738 | Perplexity: 928.478252
2025-10-01 00:34:49,508 Saving model at iteration 40000
2025-10-01 00:34:50,088 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000
2025-10-01 00:34:50,397 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000/model.safetensors
2025-10-01 00:34:50,849 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000/optimizer.bin
2025-10-01 00:34:50,849 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000/scheduler.bin
2025-10-01 00:34:50,849 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000/sampler.bin
2025-10-01 00:34:50,850 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_54_step_40000/random_states_0.pkl
2025-10-01 00:38:21,531 Stage: Train 0.5 | Epoch: 53 | Iter: 40200 | Total Loss: 0.008506 | Recon Loss: 0.007151 | Commit Loss: 0.002710 | Perplexity: 928.171418
2025-10-01 00:41:49,855 Stage: Train 0.5 | Epoch: 53 | Iter: 40400 | Total Loss: 0.008744 | Recon Loss: 0.007377 | Commit Loss: 0.002733 | Perplexity: 931.449857
2025-10-01 00:45:11,572 Stage: Train 0.5 | Epoch: 53 | Iter: 40600 | Total Loss: 0.008482 | Recon Loss: 0.007108 | Commit Loss: 0.002749 | Perplexity: 928.774932
Trainning Epoch:   8%|▊         | 54/665 [11:57:42<135:25:38, 797.94s/it]2025-10-01 00:48:45,132 Stage: Train 0.5 | Epoch: 54 | Iter: 40800 | Total Loss: 0.008557 | Recon Loss: 0.007207 | Commit Loss: 0.002701 | Perplexity: 927.072724
2025-10-01 00:52:12,151 Stage: Train 0.5 | Epoch: 54 | Iter: 41000 | Total Loss: 0.008617 | Recon Loss: 0.007252 | Commit Loss: 0.002730 | Perplexity: 933.885277
2025-10-01 00:55:38,582 Stage: Train 0.5 | Epoch: 54 | Iter: 41200 | Total Loss: 0.008517 | Recon Loss: 0.007146 | Commit Loss: 0.002741 | Perplexity: 932.344316
2025-10-01 00:59:09,427 Stage: Train 0.5 | Epoch: 54 | Iter: 41400 | Total Loss: 0.008458 | Recon Loss: 0.007087 | Commit Loss: 0.002743 | Perplexity: 929.510453
Trainning Epoch:   8%|▊         | 55/665 [12:10:49<134:38:01, 794.56s/it]2025-10-01 01:02:46,676 Stage: Train 0.5 | Epoch: 55 | Iter: 41600 | Total Loss: 0.008806 | Recon Loss: 0.007453 | Commit Loss: 0.002706 | Perplexity: 929.042361
2025-10-01 01:06:12,206 Stage: Train 0.5 | Epoch: 55 | Iter: 41800 | Total Loss: 0.008504 | Recon Loss: 0.007148 | Commit Loss: 0.002713 | Perplexity: 929.391849
2025-10-01 01:09:44,992 Stage: Train 0.5 | Epoch: 55 | Iter: 42000 | Total Loss: 0.008474 | Recon Loss: 0.007100 | Commit Loss: 0.002747 | Perplexity: 930.826351
Trainning Epoch:   8%|▊         | 56/665 [12:23:59<134:11:56, 793.29s/it]2025-10-01 01:13:13,958 Stage: Train 0.5 | Epoch: 56 | Iter: 42200 | Total Loss: 0.008562 | Recon Loss: 0.007219 | Commit Loss: 0.002686 | Perplexity: 923.941367
2025-10-01 01:16:37,462 Stage: Train 0.5 | Epoch: 56 | Iter: 42400 | Total Loss: 0.008475 | Recon Loss: 0.007116 | Commit Loss: 0.002717 | Perplexity: 930.559438
2025-10-01 01:20:06,021 Stage: Train 0.5 | Epoch: 56 | Iter: 42600 | Total Loss: 0.008459 | Recon Loss: 0.007106 | Commit Loss: 0.002707 | Perplexity: 925.047641
2025-10-01 01:23:34,027 Stage: Train 0.5 | Epoch: 56 | Iter: 42800 | Total Loss: 0.008419 | Recon Loss: 0.007064 | Commit Loss: 0.002710 | Perplexity: 931.780074
Trainning Epoch:   9%|▊         | 57/665 [12:37:02<133:26:41, 790.13s/it]2025-10-01 01:27:06,188 Stage: Train 0.5 | Epoch: 57 | Iter: 43000 | Total Loss: 0.008495 | Recon Loss: 0.007130 | Commit Loss: 0.002729 | Perplexity: 933.262119
2025-10-01 01:30:36,107 Stage: Train 0.5 | Epoch: 57 | Iter: 43200 | Total Loss: 0.008421 | Recon Loss: 0.007071 | Commit Loss: 0.002700 | Perplexity: 929.713344
2025-10-01 01:34:12,490 Stage: Train 0.5 | Epoch: 57 | Iter: 43400 | Total Loss: 0.008450 | Recon Loss: 0.007091 | Commit Loss: 0.002718 | Perplexity: 927.942089
2025-10-01 01:37:45,504 Stage: Train 0.5 | Epoch: 57 | Iter: 43600 | Total Loss: 0.008425 | Recon Loss: 0.007072 | Commit Loss: 0.002707 | Perplexity: 927.847970
Trainning Epoch:   9%|▊         | 58/665 [12:50:30<134:08:19, 795.55s/it]2025-10-01 01:41:27,840 Stage: Train 0.5 | Epoch: 58 | Iter: 43800 | Total Loss: 0.008353 | Recon Loss: 0.007002 | Commit Loss: 0.002701 | Perplexity: 931.759481
2025-10-01 01:44:54,776 Stage: Train 0.5 | Epoch: 58 | Iter: 44000 | Total Loss: 0.008300 | Recon Loss: 0.006954 | Commit Loss: 0.002693 | Perplexity: 929.375004
2025-10-01 01:48:26,242 Stage: Train 0.5 | Epoch: 58 | Iter: 44200 | Total Loss: 0.008395 | Recon Loss: 0.007018 | Commit Loss: 0.002754 | Perplexity: 931.996560
2025-10-01 01:51:55,950 Stage: Train 0.5 | Epoch: 58 | Iter: 44400 | Total Loss: 0.008499 | Recon Loss: 0.007155 | Commit Loss: 0.002688 | Perplexity: 925.871039
Trainning Epoch:   9%|▉         | 59/665 [13:03:49<134:04:32, 796.49s/it]2025-10-01 01:55:30,860 Stage: Train 0.5 | Epoch: 59 | Iter: 44600 | Total Loss: 0.008313 | Recon Loss: 0.006962 | Commit Loss: 0.002703 | Perplexity: 927.662744
2025-10-01 01:58:59,035 Stage: Train 0.5 | Epoch: 59 | Iter: 44800 | Total Loss: 0.008196 | Recon Loss: 0.006843 | Commit Loss: 0.002705 | Perplexity: 927.975967
2025-10-01 02:02:27,441 Stage: Train 0.5 | Epoch: 59 | Iter: 45000 | Total Loss: 0.008541 | Recon Loss: 0.007187 | Commit Loss: 0.002708 | Perplexity: 927.947866
Trainning Epoch:   9%|▉         | 60/665 [13:16:57<133:26:26, 794.03s/it]2025-10-01 02:05:59,113 Stage: Train 0.5 | Epoch: 60 | Iter: 45200 | Total Loss: 0.008248 | Recon Loss: 0.006890 | Commit Loss: 0.002717 | Perplexity: 929.320875
2025-10-01 02:09:28,046 Stage: Train 0.5 | Epoch: 60 | Iter: 45400 | Total Loss: 0.008179 | Recon Loss: 0.006827 | Commit Loss: 0.002704 | Perplexity: 930.242073
2025-10-01 02:12:56,915 Stage: Train 0.5 | Epoch: 60 | Iter: 45600 | Total Loss: 0.008405 | Recon Loss: 0.007052 | Commit Loss: 0.002705 | Perplexity: 929.654943
2025-10-01 02:16:25,643 Stage: Train 0.5 | Epoch: 60 | Iter: 45800 | Total Loss: 0.008198 | Recon Loss: 0.006854 | Commit Loss: 0.002688 | Perplexity: 929.484057
Trainning Epoch:   9%|▉         | 61/665 [13:30:12<133:13:54, 794.10s/it]2025-10-01 02:20:02,578 Stage: Train 0.5 | Epoch: 61 | Iter: 46000 | Total Loss: 0.008362 | Recon Loss: 0.007008 | Commit Loss: 0.002707 | Perplexity: 929.159845
2025-10-01 02:23:39,037 Stage: Train 0.5 | Epoch: 61 | Iter: 46200 | Total Loss: 0.008327 | Recon Loss: 0.006980 | Commit Loss: 0.002694 | Perplexity: 932.644653
2025-10-01 02:27:08,890 Stage: Train 0.5 | Epoch: 61 | Iter: 46400 | Total Loss: 0.008319 | Recon Loss: 0.006967 | Commit Loss: 0.002703 | Perplexity: 925.216833
2025-10-01 02:30:39,262 Stage: Train 0.5 | Epoch: 61 | Iter: 46600 | Total Loss: 0.008174 | Recon Loss: 0.006815 | Commit Loss: 0.002717 | Perplexity: 931.711419
Trainning Epoch:   9%|▉         | 62/665 [13:43:36<133:31:12, 797.14s/it]2025-10-01 02:34:19,567 Stage: Train 0.5 | Epoch: 62 | Iter: 46800 | Total Loss: 0.008414 | Recon Loss: 0.007055 | Commit Loss: 0.002718 | Perplexity: 929.024579
2025-10-01 02:37:53,336 Stage: Train 0.5 | Epoch: 62 | Iter: 47000 | Total Loss: 0.008130 | Recon Loss: 0.006779 | Commit Loss: 0.002703 | Perplexity: 932.880758
2025-10-01 02:41:27,089 Stage: Train 0.5 | Epoch: 62 | Iter: 47200 | Total Loss: 0.008260 | Recon Loss: 0.006915 | Commit Loss: 0.002688 | Perplexity: 927.201819
2025-10-01 02:44:52,788 Stage: Train 0.5 | Epoch: 62 | Iter: 47400 | Total Loss: 0.008204 | Recon Loss: 0.006854 | Commit Loss: 0.002700 | Perplexity: 930.178009
Trainning Epoch:   9%|▉         | 63/665 [13:56:58<133:34:26, 798.78s/it]2025-10-01 02:48:26,006 Stage: Train 0.5 | Epoch: 63 | Iter: 47600 | Total Loss: 0.008188 | Recon Loss: 0.006823 | Commit Loss: 0.002731 | Perplexity: 931.411576
2025-10-01 02:51:52,879 Stage: Train 0.5 | Epoch: 63 | Iter: 47800 | Total Loss: 0.008212 | Recon Loss: 0.006865 | Commit Loss: 0.002695 | Perplexity: 930.447016
2025-10-01 02:55:28,111 Stage: Train 0.5 | Epoch: 63 | Iter: 48000 | Total Loss: 0.008167 | Recon Loss: 0.006818 | Commit Loss: 0.002699 | Perplexity: 930.416729
Trainning Epoch:  10%|▉         | 64/665 [14:10:15<133:15:21, 798.21s/it]2025-10-01 02:59:07,332 Stage: Train 0.5 | Epoch: 64 | Iter: 48200 | Total Loss: 0.008189 | Recon Loss: 0.006851 | Commit Loss: 0.002678 | Perplexity: 926.224817
2025-10-01 03:02:41,228 Stage: Train 0.5 | Epoch: 64 | Iter: 48400 | Total Loss: 0.008066 | Recon Loss: 0.006713 | Commit Loss: 0.002705 | Perplexity: 931.436924
2025-10-01 03:06:13,825 Stage: Train 0.5 | Epoch: 64 | Iter: 48600 | Total Loss: 0.008202 | Recon Loss: 0.006858 | Commit Loss: 0.002690 | Perplexity: 929.801218
2025-10-01 03:09:44,990 Stage: Train 0.5 | Epoch: 64 | Iter: 48800 | Total Loss: 0.008094 | Recon Loss: 0.006732 | Commit Loss: 0.002723 | Perplexity: 934.545664
Trainning Epoch:  10%|▉         | 65/665 [14:23:45<133:37:19, 801.73s/it]2025-10-01 03:13:24,462 Stage: Train 0.5 | Epoch: 65 | Iter: 49000 | Total Loss: 0.008110 | Recon Loss: 0.006765 | Commit Loss: 0.002689 | Perplexity: 928.498064
2025-10-01 03:16:51,362 Stage: Train 0.5 | Epoch: 65 | Iter: 49200 | Total Loss: 0.008182 | Recon Loss: 0.006810 | Commit Loss: 0.002744 | Perplexity: 935.487695
2025-10-01 03:20:24,357 Stage: Train 0.5 | Epoch: 65 | Iter: 49400 | Total Loss: 0.008018 | Recon Loss: 0.006688 | Commit Loss: 0.002659 | Perplexity: 926.133931
2025-10-01 03:23:52,425 Stage: Train 0.5 | Epoch: 65 | Iter: 49600 | Total Loss: 0.008102 | Recon Loss: 0.006738 | Commit Loss: 0.002726 | Perplexity: 930.457398
Trainning Epoch:  10%|▉         | 66/665 [14:37:00<133:03:47, 799.71s/it]2025-10-01 03:27:25,920 Stage: Train 0.5 | Epoch: 66 | Iter: 49800 | Total Loss: 0.008137 | Recon Loss: 0.006800 | Commit Loss: 0.002673 | Perplexity: 924.094242
2025-10-01 03:30:58,250 Stage: Train 0.5 | Epoch: 66 | Iter: 50000 | Total Loss: 0.008141 | Recon Loss: 0.006777 | Commit Loss: 0.002728 | Perplexity: 930.810763
2025-10-01 03:34:24,186 Stage: Train 0.5 | Epoch: 66 | Iter: 50200 | Total Loss: 0.008109 | Recon Loss: 0.006762 | Commit Loss: 0.002694 | Perplexity: 932.257151
2025-10-01 03:37:48,257 Stage: Train 0.5 | Epoch: 66 | Iter: 50400 | Total Loss: 0.008093 | Recon Loss: 0.006750 | Commit Loss: 0.002686 | Perplexity: 928.040168
Trainning Epoch:  10%|█         | 67/665 [14:50:08<132:15:03, 796.16s/it]2025-10-01 03:41:24,620 Stage: Train 0.5 | Epoch: 67 | Iter: 50600 | Total Loss: 0.008211 | Recon Loss: 0.006867 | Commit Loss: 0.002688 | Perplexity: 930.920600
2025-10-01 03:44:57,531 Stage: Train 0.5 | Epoch: 67 | Iter: 50800 | Total Loss: 0.008000 | Recon Loss: 0.006651 | Commit Loss: 0.002699 | Perplexity: 931.571656
2025-10-01 03:48:28,281 Stage: Train 0.5 | Epoch: 67 | Iter: 51000 | Total Loss: 0.008019 | Recon Loss: 0.006669 | Commit Loss: 0.002700 | Perplexity: 928.378139
2025-10-01 03:51:51,823 Stage: Train 0.5 | Epoch: 67 | Iter: 51200 | Total Loss: 0.008061 | Recon Loss: 0.006718 | Commit Loss: 0.002685 | Perplexity: 930.106733
Trainning Epoch:  10%|█         | 68/665 [15:03:23<131:57:02, 795.68s/it]2025-10-01 03:55:26,570 Stage: Train 0.5 | Epoch: 68 | Iter: 51400 | Total Loss: 0.007992 | Recon Loss: 0.006641 | Commit Loss: 0.002703 | Perplexity: 930.124059
2025-10-01 03:58:56,118 Stage: Train 0.5 | Epoch: 68 | Iter: 51600 | Total Loss: 0.008039 | Recon Loss: 0.006687 | Commit Loss: 0.002704 | Perplexity: 933.833826
2025-10-01 04:02:24,922 Stage: Train 0.5 | Epoch: 68 | Iter: 51800 | Total Loss: 0.007991 | Recon Loss: 0.006636 | Commit Loss: 0.002709 | Perplexity: 928.163483
Trainning Epoch:  10%|█         | 69/665 [15:16:36<131:36:40, 794.97s/it]2025-10-01 04:06:03,755 Stage: Train 0.5 | Epoch: 69 | Iter: 52000 | Total Loss: 0.007991 | Recon Loss: 0.006628 | Commit Loss: 0.002726 | Perplexity: 933.806381
2025-10-01 04:09:35,455 Stage: Train 0.5 | Epoch: 69 | Iter: 52200 | Total Loss: 0.008002 | Recon Loss: 0.006668 | Commit Loss: 0.002669 | Perplexity: 930.268789
2025-10-01 04:13:07,126 Stage: Train 0.5 | Epoch: 69 | Iter: 52400 | Total Loss: 0.008016 | Recon Loss: 0.006664 | Commit Loss: 0.002704 | Perplexity: 931.208596
2025-10-01 04:16:35,698 Stage: Train 0.5 | Epoch: 69 | Iter: 52600 | Total Loss: 0.008075 | Recon Loss: 0.006730 | Commit Loss: 0.002691 | Perplexity: 926.509381
Trainning Epoch:  11%|█         | 70/665 [15:29:54<131:33:47, 796.01s/it]2025-10-01 04:20:12,564 Stage: Train 0.5 | Epoch: 70 | Iter: 52800 | Total Loss: 0.007929 | Recon Loss: 0.006594 | Commit Loss: 0.002672 | Perplexity: 931.940416
2025-10-01 04:23:37,483 Stage: Train 0.5 | Epoch: 70 | Iter: 53000 | Total Loss: 0.007953 | Recon Loss: 0.006615 | Commit Loss: 0.002676 | Perplexity: 930.418413
2025-10-01 04:27:00,573 Stage: Train 0.5 | Epoch: 70 | Iter: 53200 | Total Loss: 0.007851 | Recon Loss: 0.006502 | Commit Loss: 0.002698 | Perplexity: 936.649414
2025-10-01 04:30:30,558 Stage: Train 0.5 | Epoch: 70 | Iter: 53400 | Total Loss: 0.007973 | Recon Loss: 0.006626 | Commit Loss: 0.002695 | Perplexity: 932.554517
Trainning Epoch:  11%|█         | 71/665 [15:43:02<130:54:10, 793.35s/it]2025-10-01 04:34:04,259 Stage: Train 0.5 | Epoch: 71 | Iter: 53600 | Total Loss: 0.007896 | Recon Loss: 0.006555 | Commit Loss: 0.002680 | Perplexity: 929.771603
2025-10-01 04:37:34,251 Stage: Train 0.5 | Epoch: 71 | Iter: 53800 | Total Loss: 0.007947 | Recon Loss: 0.006600 | Commit Loss: 0.002695 | Perplexity: 929.463560
2025-10-01 04:41:03,970 Stage: Train 0.5 | Epoch: 71 | Iter: 54000 | Total Loss: 0.007932 | Recon Loss: 0.006579 | Commit Loss: 0.002705 | Perplexity: 937.387661
2025-10-01 04:44:36,576 Stage: Train 0.5 | Epoch: 71 | Iter: 54200 | Total Loss: 0.008017 | Recon Loss: 0.006669 | Commit Loss: 0.002698 | Perplexity: 933.013707
Trainning Epoch:  11%|█         | 72/665 [15:56:17<130:48:09, 794.08s/it]2025-10-01 04:48:08,899 Stage: Train 0.5 | Epoch: 72 | Iter: 54400 | Total Loss: 0.007867 | Recon Loss: 0.006526 | Commit Loss: 0.002682 | Perplexity: 930.820110
2025-10-01 04:51:33,851 Stage: Train 0.5 | Epoch: 72 | Iter: 54600 | Total Loss: 0.007876 | Recon Loss: 0.006532 | Commit Loss: 0.002688 | Perplexity: 933.597798
2025-10-01 04:55:04,278 Stage: Train 0.5 | Epoch: 72 | Iter: 54800 | Total Loss: 0.007918 | Recon Loss: 0.006558 | Commit Loss: 0.002719 | Perplexity: 934.914594
Trainning Epoch:  11%|█         | 73/665 [16:09:25<130:14:40, 792.03s/it]2025-10-01 04:58:37,213 Stage: Train 0.5 | Epoch: 73 | Iter: 55000 | Total Loss: 0.007965 | Recon Loss: 0.006616 | Commit Loss: 0.002699 | Perplexity: 929.304648
2025-10-01 05:02:08,925 Stage: Train 0.5 | Epoch: 73 | Iter: 55200 | Total Loss: 0.007858 | Recon Loss: 0.006516 | Commit Loss: 0.002683 | Perplexity: 930.601082
2025-10-01 05:05:34,187 Stage: Train 0.5 | Epoch: 73 | Iter: 55400 | Total Loss: 0.007977 | Recon Loss: 0.006645 | Commit Loss: 0.002664 | Perplexity: 930.781465
2025-10-01 05:09:00,176 Stage: Train 0.5 | Epoch: 73 | Iter: 55600 | Total Loss: 0.007884 | Recon Loss: 0.006524 | Commit Loss: 0.002720 | Perplexity: 935.204044
Trainning Epoch:  11%|█         | 74/665 [16:22:36<129:58:28, 791.72s/it]2025-10-01 05:12:40,114 Stage: Train 0.5 | Epoch: 74 | Iter: 55800 | Total Loss: 0.007833 | Recon Loss: 0.006485 | Commit Loss: 0.002697 | Perplexity: 933.566404
2025-10-01 05:16:11,300 Stage: Train 0.5 | Epoch: 74 | Iter: 56000 | Total Loss: 0.007945 | Recon Loss: 0.006614 | Commit Loss: 0.002662 | Perplexity: 930.215545
2025-10-01 05:19:38,163 Stage: Train 0.5 | Epoch: 74 | Iter: 56200 | Total Loss: 0.007850 | Recon Loss: 0.006501 | Commit Loss: 0.002699 | Perplexity: 932.960802
2025-10-01 05:23:09,644 Stage: Train 0.5 | Epoch: 74 | Iter: 56400 | Total Loss: 0.007794 | Recon Loss: 0.006458 | Commit Loss: 0.002671 | Perplexity: 933.332093
Trainning Epoch:  11%|█▏        | 75/665 [16:35:55<130:06:41, 793.90s/it]2025-10-01 05:26:45,375 Stage: Train 0.5 | Epoch: 75 | Iter: 56600 | Total Loss: 0.007831 | Recon Loss: 0.006476 | Commit Loss: 0.002708 | Perplexity: 933.128333
2025-10-01 05:30:16,354 Stage: Train 0.5 | Epoch: 75 | Iter: 56800 | Total Loss: 0.007782 | Recon Loss: 0.006443 | Commit Loss: 0.002678 | Perplexity: 935.880086
2025-10-01 05:33:48,963 Stage: Train 0.5 | Epoch: 75 | Iter: 57000 | Total Loss: 0.007883 | Recon Loss: 0.006544 | Commit Loss: 0.002677 | Perplexity: 932.828112
2025-10-01 05:37:15,702 Stage: Train 0.5 | Epoch: 75 | Iter: 57200 | Total Loss: 0.007877 | Recon Loss: 0.006525 | Commit Loss: 0.002704 | Perplexity: 933.213271
Trainning Epoch:  11%|█▏        | 76/665 [16:49:09<129:54:35, 794.02s/it]2025-10-01 05:40:51,915 Stage: Train 0.5 | Epoch: 76 | Iter: 57400 | Total Loss: 0.007710 | Recon Loss: 0.006377 | Commit Loss: 0.002666 | Perplexity: 931.558087
2025-10-01 05:44:25,219 Stage: Train 0.5 | Epoch: 76 | Iter: 57600 | Total Loss: 0.007880 | Recon Loss: 0.006547 | Commit Loss: 0.002668 | Perplexity: 933.299557
2025-10-01 05:47:59,274 Stage: Train 0.5 | Epoch: 76 | Iter: 57800 | Total Loss: 0.007699 | Recon Loss: 0.006353 | Commit Loss: 0.002693 | Perplexity: 934.075566
Trainning Epoch:  12%|█▏        | 77/665 [17:02:39<130:30:16, 799.01s/it]2025-10-01 05:51:41,828 Stage: Train 0.5 | Epoch: 77 | Iter: 58000 | Total Loss: 0.007846 | Recon Loss: 0.006489 | Commit Loss: 0.002715 | Perplexity: 933.014209
2025-10-01 05:55:11,433 Stage: Train 0.5 | Epoch: 77 | Iter: 58200 | Total Loss: 0.007820 | Recon Loss: 0.006497 | Commit Loss: 0.002647 | Perplexity: 928.957553
2025-10-01 05:58:40,306 Stage: Train 0.5 | Epoch: 77 | Iter: 58400 | Total Loss: 0.007787 | Recon Loss: 0.006449 | Commit Loss: 0.002675 | Perplexity: 936.382858
2025-10-01 06:02:04,348 Stage: Train 0.5 | Epoch: 77 | Iter: 58600 | Total Loss: 0.007779 | Recon Loss: 0.006432 | Commit Loss: 0.002694 | Perplexity: 931.923706
Trainning Epoch:  12%|█▏        | 78/665 [17:15:49<129:48:57, 796.15s/it]2025-10-01 06:05:40,111 Stage: Train 0.5 | Epoch: 78 | Iter: 58800 | Total Loss: 0.007720 | Recon Loss: 0.006381 | Commit Loss: 0.002678 | Perplexity: 932.071569
2025-10-01 06:09:06,497 Stage: Train 0.5 | Epoch: 78 | Iter: 59000 | Total Loss: 0.007877 | Recon Loss: 0.006543 | Commit Loss: 0.002669 | Perplexity: 931.289874
2025-10-01 06:12:34,350 Stage: Train 0.5 | Epoch: 78 | Iter: 59200 | Total Loss: 0.007703 | Recon Loss: 0.006360 | Commit Loss: 0.002686 | Perplexity: 933.430591
2025-10-01 06:16:03,852 Stage: Train 0.5 | Epoch: 78 | Iter: 59400 | Total Loss: 0.007713 | Recon Loss: 0.006376 | Commit Loss: 0.002673 | Perplexity: 934.436147
Trainning Epoch:  12%|█▏        | 79/665 [17:29:00<129:22:06, 794.75s/it]2025-10-01 06:19:43,405 Stage: Train 0.5 | Epoch: 79 | Iter: 59600 | Total Loss: 0.007748 | Recon Loss: 0.006400 | Commit Loss: 0.002695 | Perplexity: 932.126487
2025-10-01 06:23:08,409 Stage: Train 0.5 | Epoch: 79 | Iter: 59800 | Total Loss: 0.007717 | Recon Loss: 0.006389 | Commit Loss: 0.002656 | Perplexity: 933.897461
2025-10-01 06:26:35,430 Stage: Train 0.5 | Epoch: 79 | Iter: 60000 | Total Loss: 0.007796 | Recon Loss: 0.006470 | Commit Loss: 0.002652 | Perplexity: 933.400363
2025-10-01 06:26:35,430 Saving model at iteration 60000
2025-10-01 06:26:35,788 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000
2025-10-01 06:26:36,083 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000/model.safetensors
2025-10-01 06:26:36,526 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000/optimizer.bin
2025-10-01 06:26:36,526 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000/scheduler.bin
2025-10-01 06:26:36,526 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000/sampler.bin
2025-10-01 06:26:36,527 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_80_step_60000/random_states_0.pkl
2025-10-01 06:30:01,871 Stage: Train 0.5 | Epoch: 79 | Iter: 60200 | Total Loss: 0.007708 | Recon Loss: 0.006357 | Commit Loss: 0.002702 | Perplexity: 936.125229
Trainning Epoch:  12%|█▏        | 80/665 [17:42:08<128:47:10, 792.53s/it]2025-10-01 06:33:32,035 Stage: Train 0.5 | Epoch: 80 | Iter: 60400 | Total Loss: 0.007690 | Recon Loss: 0.006354 | Commit Loss: 0.002672 | Perplexity: 932.793997
2025-10-01 06:37:02,752 Stage: Train 0.5 | Epoch: 80 | Iter: 60600 | Total Loss: 0.007669 | Recon Loss: 0.006346 | Commit Loss: 0.002646 | Perplexity: 929.328303
2025-10-01 06:40:31,928 Stage: Train 0.5 | Epoch: 80 | Iter: 60800 | Total Loss: 0.007696 | Recon Loss: 0.006361 | Commit Loss: 0.002670 | Perplexity: 934.246691
Trainning Epoch:  12%|█▏        | 81/665 [17:55:16<128:20:15, 791.12s/it]2025-10-01 06:44:02,264 Stage: Train 0.5 | Epoch: 81 | Iter: 61000 | Total Loss: 0.007731 | Recon Loss: 0.006380 | Commit Loss: 0.002701 | Perplexity: 932.811488
2025-10-01 06:47:33,642 Stage: Train 0.5 | Epoch: 81 | Iter: 61200 | Total Loss: 0.007631 | Recon Loss: 0.006291 | Commit Loss: 0.002680 | Perplexity: 933.956923
2025-10-01 06:51:01,438 Stage: Train 0.5 | Epoch: 81 | Iter: 61400 | Total Loss: 0.007840 | Recon Loss: 0.006515 | Commit Loss: 0.002649 | Perplexity: 932.619166
2025-10-01 06:54:32,198 Stage: Train 0.5 | Epoch: 81 | Iter: 61600 | Total Loss: 0.007565 | Recon Loss: 0.006227 | Commit Loss: 0.002677 | Perplexity: 933.404257
Trainning Epoch:  12%|█▏        | 82/665 [18:08:27<128:07:23, 791.16s/it]2025-10-01 06:58:05,566 Stage: Train 0.5 | Epoch: 82 | Iter: 61800 | Total Loss: 0.007702 | Recon Loss: 0.006363 | Commit Loss: 0.002678 | Perplexity: 934.060262
2025-10-01 07:01:31,045 Stage: Train 0.5 | Epoch: 82 | Iter: 62000 | Total Loss: 0.007525 | Recon Loss: 0.006191 | Commit Loss: 0.002668 | Perplexity: 933.596780
2025-10-01 07:04:56,618 Stage: Train 0.5 | Epoch: 82 | Iter: 62200 | Total Loss: 0.007688 | Recon Loss: 0.006340 | Commit Loss: 0.002696 | Perplexity: 933.648998
2025-10-01 07:08:31,342 Stage: Train 0.5 | Epoch: 82 | Iter: 62400 | Total Loss: 0.007717 | Recon Loss: 0.006378 | Commit Loss: 0.002678 | Perplexity: 934.981514
Trainning Epoch:  12%|█▏        | 83/665 [18:21:40<128:00:27, 791.80s/it]2025-10-01 07:12:07,365 Stage: Train 0.5 | Epoch: 83 | Iter: 62600 | Total Loss: 0.007555 | Recon Loss: 0.006221 | Commit Loss: 0.002668 | Perplexity: 933.405404
2025-10-01 07:15:45,199 Stage: Train 0.5 | Epoch: 83 | Iter: 62800 | Total Loss: 0.007644 | Recon Loss: 0.006326 | Commit Loss: 0.002637 | Perplexity: 931.601969
2025-10-01 07:19:17,066 Stage: Train 0.5 | Epoch: 83 | Iter: 63000 | Total Loss: 0.007663 | Recon Loss: 0.006318 | Commit Loss: 0.002690 | Perplexity: 935.069740
2025-10-01 07:22:43,810 Stage: Train 0.5 | Epoch: 83 | Iter: 63200 | Total Loss: 0.007631 | Recon Loss: 0.006292 | Commit Loss: 0.002679 | Perplexity: 932.837092
Trainning Epoch:  13%|█▎        | 84/665 [18:35:02<128:15:55, 794.76s/it]2025-10-01 07:26:16,828 Stage: Train 0.5 | Epoch: 84 | Iter: 63400 | Total Loss: 0.007674 | Recon Loss: 0.006322 | Commit Loss: 0.002703 | Perplexity: 936.463046
2025-10-01 07:29:51,463 Stage: Train 0.5 | Epoch: 84 | Iter: 63600 | Total Loss: 0.007706 | Recon Loss: 0.006372 | Commit Loss: 0.002668 | Perplexity: 934.414441
2025-10-01 07:33:25,924 Stage: Train 0.5 | Epoch: 84 | Iter: 63800 | Total Loss: 0.007588 | Recon Loss: 0.006270 | Commit Loss: 0.002636 | Perplexity: 930.364266
2025-10-01 07:36:58,077 Stage: Train 0.5 | Epoch: 84 | Iter: 64000 | Total Loss: 0.007664 | Recon Loss: 0.006327 | Commit Loss: 0.002673 | Perplexity: 930.550626
Trainning Epoch:  13%|█▎        | 85/665 [18:48:29<128:37:27, 798.36s/it]2025-10-01 07:40:38,337 Stage: Train 0.5 | Epoch: 85 | Iter: 64200 | Total Loss: 0.007493 | Recon Loss: 0.006170 | Commit Loss: 0.002648 | Perplexity: 929.207754
2025-10-01 07:44:15,595 Stage: Train 0.5 | Epoch: 85 | Iter: 64400 | Total Loss: 0.007650 | Recon Loss: 0.006311 | Commit Loss: 0.002678 | Perplexity: 935.763952
2025-10-01 07:47:49,303 Stage: Train 0.5 | Epoch: 85 | Iter: 64600 | Total Loss: 0.007518 | Recon Loss: 0.006188 | Commit Loss: 0.002660 | Perplexity: 932.970266
Trainning Epoch:  13%|█▎        | 86/665 [19:02:03<129:10:24, 803.15s/it]2025-10-01 07:51:29,798 Stage: Train 0.5 | Epoch: 86 | Iter: 64800 | Total Loss: 0.007764 | Recon Loss: 0.006423 | Commit Loss: 0.002683 | Perplexity: 934.224212
2025-10-01 07:55:03,864 Stage: Train 0.5 | Epoch: 86 | Iter: 65000 | Total Loss: 0.007511 | Recon Loss: 0.006183 | Commit Loss: 0.002656 | Perplexity: 935.118274
2025-10-01 07:58:35,315 Stage: Train 0.5 | Epoch: 86 | Iter: 65200 | Total Loss: 0.007579 | Recon Loss: 0.006252 | Commit Loss: 0.002655 | Perplexity: 933.204300
2025-10-01 08:02:07,704 Stage: Train 0.5 | Epoch: 86 | Iter: 65400 | Total Loss: 0.007553 | Recon Loss: 0.006207 | Commit Loss: 0.002691 | Perplexity: 936.632106
Trainning Epoch:  13%|█▎        | 87/665 [19:15:30<129:09:39, 804.46s/it]2025-10-01 08:05:46,061 Stage: Train 0.5 | Epoch: 87 | Iter: 65600 | Total Loss: 0.007526 | Recon Loss: 0.006191 | Commit Loss: 0.002669 | Perplexity: 934.658994
2025-10-01 08:09:16,399 Stage: Train 0.5 | Epoch: 87 | Iter: 65800 | Total Loss: 0.007651 | Recon Loss: 0.006322 | Commit Loss: 0.002659 | Perplexity: 932.409803
2025-10-01 08:12:47,069 Stage: Train 0.5 | Epoch: 87 | Iter: 66000 | Total Loss: 0.007469 | Recon Loss: 0.006137 | Commit Loss: 0.002664 | Perplexity: 936.442863
2025-10-01 08:16:15,602 Stage: Train 0.5 | Epoch: 87 | Iter: 66200 | Total Loss: 0.007645 | Recon Loss: 0.006320 | Commit Loss: 0.002649 | Perplexity: 929.366059
Trainning Epoch:  13%|█▎        | 88/665 [19:28:46<128:31:29, 801.89s/it]2025-10-01 08:19:52,910 Stage: Train 0.5 | Epoch: 88 | Iter: 66400 | Total Loss: 0.007548 | Recon Loss: 0.006215 | Commit Loss: 0.002667 | Perplexity: 934.333912
2025-10-01 08:23:29,146 Stage: Train 0.5 | Epoch: 88 | Iter: 66600 | Total Loss: 0.007597 | Recon Loss: 0.006258 | Commit Loss: 0.002678 | Perplexity: 934.943880
2025-10-01 08:26:55,041 Stage: Train 0.5 | Epoch: 88 | Iter: 66800 | Total Loss: 0.007613 | Recon Loss: 0.006281 | Commit Loss: 0.002665 | Perplexity: 933.665876
2025-10-01 08:30:21,550 Stage: Train 0.5 | Epoch: 88 | Iter: 67000 | Total Loss: 0.007465 | Recon Loss: 0.006140 | Commit Loss: 0.002652 | Perplexity: 933.693493
Trainning Epoch:  13%|█▎        | 89/665 [19:42:03<128:02:53, 800.30s/it]2025-10-01 08:33:57,845 Stage: Train 0.5 | Epoch: 89 | Iter: 67200 | Total Loss: 0.007468 | Recon Loss: 0.006138 | Commit Loss: 0.002661 | Perplexity: 933.886821
2025-10-01 08:37:32,588 Stage: Train 0.5 | Epoch: 89 | Iter: 67400 | Total Loss: 0.007444 | Recon Loss: 0.006102 | Commit Loss: 0.002684 | Perplexity: 941.304274
2025-10-01 08:40:58,200 Stage: Train 0.5 | Epoch: 89 | Iter: 67600 | Total Loss: 0.007535 | Recon Loss: 0.006208 | Commit Loss: 0.002654 | Perplexity: 933.889401
Trainning Epoch:  14%|█▎        | 90/665 [19:55:22<127:45:17, 799.86s/it]2025-10-01 08:44:33,771 Stage: Train 0.5 | Epoch: 90 | Iter: 67800 | Total Loss: 0.007556 | Recon Loss: 0.006225 | Commit Loss: 0.002662 | Perplexity: 931.748358
2025-10-01 08:48:01,829 Stage: Train 0.5 | Epoch: 90 | Iter: 68000 | Total Loss: 0.007445 | Recon Loss: 0.006123 | Commit Loss: 0.002642 | Perplexity: 935.050464
2025-10-01 08:51:30,814 Stage: Train 0.5 | Epoch: 90 | Iter: 68200 | Total Loss: 0.007593 | Recon Loss: 0.006260 | Commit Loss: 0.002667 | Perplexity: 936.402845
2025-10-01 08:55:01,825 Stage: Train 0.5 | Epoch: 90 | Iter: 68400 | Total Loss: 0.007490 | Recon Loss: 0.006161 | Commit Loss: 0.002657 | Perplexity: 932.104918
Trainning Epoch:  14%|█▎        | 91/665 [20:08:38<127:21:59, 798.81s/it]2025-10-01 08:58:43,389 Stage: Train 0.5 | Epoch: 91 | Iter: 68600 | Total Loss: 0.007542 | Recon Loss: 0.006209 | Commit Loss: 0.002665 | Perplexity: 932.820433
2025-10-01 09:02:19,762 Stage: Train 0.5 | Epoch: 91 | Iter: 68800 | Total Loss: 0.007402 | Recon Loss: 0.006094 | Commit Loss: 0.002617 | Perplexity: 930.617357
2025-10-01 09:05:46,772 Stage: Train 0.5 | Epoch: 91 | Iter: 69000 | Total Loss: 0.007526 | Recon Loss: 0.006194 | Commit Loss: 0.002666 | Perplexity: 935.100620
2025-10-01 09:09:17,392 Stage: Train 0.5 | Epoch: 91 | Iter: 69200 | Total Loss: 0.007435 | Recon Loss: 0.006109 | Commit Loss: 0.002653 | Perplexity: 936.213788
Trainning Epoch:  14%|█▍        | 92/665 [20:22:08<127:39:50, 802.08s/it]2025-10-01 09:13:01,029 Stage: Train 0.5 | Epoch: 92 | Iter: 69400 | Total Loss: 0.007501 | Recon Loss: 0.006165 | Commit Loss: 0.002672 | Perplexity: 939.089571
2025-10-01 09:16:36,563 Stage: Train 0.5 | Epoch: 92 | Iter: 69600 | Total Loss: 0.007496 | Recon Loss: 0.006162 | Commit Loss: 0.002667 | Perplexity: 934.227968
2025-10-01 09:20:08,129 Stage: Train 0.5 | Epoch: 92 | Iter: 69800 | Total Loss: 0.007434 | Recon Loss: 0.006112 | Commit Loss: 0.002646 | Perplexity: 934.270894
2025-10-01 09:23:34,241 Stage: Train 0.5 | Epoch: 92 | Iter: 70000 | Total Loss: 0.007515 | Recon Loss: 0.006171 | Commit Loss: 0.002687 | Perplexity: 940.315173
Trainning Epoch:  14%|█▍        | 93/665 [20:35:31<127:29:01, 802.35s/it]2025-10-01 09:27:12,501 Stage: Train 0.5 | Epoch: 93 | Iter: 70200 | Total Loss: 0.007455 | Recon Loss: 0.006131 | Commit Loss: 0.002648 | Perplexity: 934.293856
2025-10-01 09:30:47,243 Stage: Train 0.5 | Epoch: 93 | Iter: 70400 | Total Loss: 0.007425 | Recon Loss: 0.006098 | Commit Loss: 0.002655 | Perplexity: 936.124915
2025-10-01 09:34:19,113 Stage: Train 0.5 | Epoch: 93 | Iter: 70600 | Total Loss: 0.007406 | Recon Loss: 0.006082 | Commit Loss: 0.002647 | Perplexity: 934.405546
Trainning Epoch:  14%|█▍        | 94/665 [20:48:58<127:28:50, 803.73s/it]2025-10-01 09:38:00,520 Stage: Train 0.5 | Epoch: 94 | Iter: 70800 | Total Loss: 0.007382 | Recon Loss: 0.006054 | Commit Loss: 0.002656 | Perplexity: 932.000920
2025-10-01 09:41:27,931 Stage: Train 0.5 | Epoch: 94 | Iter: 71000 | Total Loss: 0.007405 | Recon Loss: 0.006085 | Commit Loss: 0.002639 | Perplexity: 933.247062
2025-10-01 09:44:56,925 Stage: Train 0.5 | Epoch: 94 | Iter: 71200 | Total Loss: 0.007440 | Recon Loss: 0.006108 | Commit Loss: 0.002665 | Perplexity: 939.395016
2025-10-01 09:48:25,998 Stage: Train 0.5 | Epoch: 94 | Iter: 71400 | Total Loss: 0.007355 | Recon Loss: 0.006032 | Commit Loss: 0.002644 | Perplexity: 937.350216
Trainning Epoch:  14%|█▍        | 95/665 [21:02:17<127:01:48, 802.30s/it]2025-10-01 09:52:10,598 Stage: Train 0.5 | Epoch: 95 | Iter: 71600 | Total Loss: 0.007440 | Recon Loss: 0.006103 | Commit Loss: 0.002673 | Perplexity: 935.442175
2025-10-01 09:55:46,566 Stage: Train 0.5 | Epoch: 95 | Iter: 71800 | Total Loss: 0.007425 | Recon Loss: 0.006102 | Commit Loss: 0.002646 | Perplexity: 934.939245
2025-10-01 09:59:17,116 Stage: Train 0.5 | Epoch: 95 | Iter: 72000 | Total Loss: 0.007504 | Recon Loss: 0.006199 | Commit Loss: 0.002610 | Perplexity: 930.203379
2025-10-01 10:02:51,101 Stage: Train 0.5 | Epoch: 95 | Iter: 72200 | Total Loss: 0.007415 | Recon Loss: 0.006082 | Commit Loss: 0.002665 | Perplexity: 936.677236
Trainning Epoch:  14%|█▍        | 96/665 [21:15:49<127:17:33, 805.37s/it]2025-10-01 10:06:25,436 Stage: Train 0.5 | Epoch: 96 | Iter: 72400 | Total Loss: 0.007416 | Recon Loss: 0.006105 | Commit Loss: 0.002620 | Perplexity: 934.524359
2025-10-01 10:09:52,271 Stage: Train 0.5 | Epoch: 96 | Iter: 72600 | Total Loss: 0.007418 | Recon Loss: 0.006098 | Commit Loss: 0.002640 | Perplexity: 932.074804
2025-10-01 10:13:17,566 Stage: Train 0.5 | Epoch: 96 | Iter: 72800 | Total Loss: 0.007374 | Recon Loss: 0.006036 | Commit Loss: 0.002675 | Perplexity: 939.910317
2025-10-01 10:16:48,437 Stage: Train 0.5 | Epoch: 96 | Iter: 73000 | Total Loss: 0.007537 | Recon Loss: 0.006224 | Commit Loss: 0.002628 | Perplexity: 935.192281
Trainning Epoch:  15%|█▍        | 97/665 [21:28:55<126:09:07, 799.55s/it]2025-10-01 10:20:24,482 Stage: Train 0.5 | Epoch: 97 | Iter: 73200 | Total Loss: 0.007317 | Recon Loss: 0.006014 | Commit Loss: 0.002607 | Perplexity: 931.545796
2025-10-01 10:23:46,670 Stage: Train 0.5 | Epoch: 97 | Iter: 73400 | Total Loss: 0.007354 | Recon Loss: 0.006027 | Commit Loss: 0.002654 | Perplexity: 936.246841
2025-10-01 10:27:09,957 Stage: Train 0.5 | Epoch: 97 | Iter: 73600 | Total Loss: 0.007457 | Recon Loss: 0.006137 | Commit Loss: 0.002640 | Perplexity: 938.940500
Trainning Epoch:  15%|█▍        | 98/665 [21:41:58<125:08:15, 794.52s/it]2025-10-01 10:30:43,970 Stage: Train 0.5 | Epoch: 98 | Iter: 73800 | Total Loss: 0.007445 | Recon Loss: 0.006125 | Commit Loss: 0.002639 | Perplexity: 933.095593
2025-10-01 10:34:17,262 Stage: Train 0.5 | Epoch: 98 | Iter: 74000 | Total Loss: 0.007368 | Recon Loss: 0.006047 | Commit Loss: 0.002643 | Perplexity: 933.338003
2025-10-01 10:37:45,157 Stage: Train 0.5 | Epoch: 98 | Iter: 74200 | Total Loss: 0.007254 | Recon Loss: 0.005937 | Commit Loss: 0.002635 | Perplexity: 939.993185
2025-10-01 10:41:14,213 Stage: Train 0.5 | Epoch: 98 | Iter: 74400 | Total Loss: 0.007399 | Recon Loss: 0.006085 | Commit Loss: 0.002627 | Perplexity: 936.722479
Trainning Epoch:  15%|█▍        | 99/665 [21:55:14<124:58:04, 794.85s/it]2025-10-01 10:44:49,544 Stage: Train 0.5 | Epoch: 99 | Iter: 74600 | Total Loss: 0.007383 | Recon Loss: 0.006058 | Commit Loss: 0.002650 | Perplexity: 937.012832
2025-10-01 10:48:15,857 Stage: Train 0.5 | Epoch: 99 | Iter: 74800 | Total Loss: 0.007259 | Recon Loss: 0.005932 | Commit Loss: 0.002655 | Perplexity: 938.089078
2025-10-01 10:51:48,722 Stage: Train 0.5 | Epoch: 99 | Iter: 75000 | Total Loss: 0.007309 | Recon Loss: 0.005993 | Commit Loss: 0.002631 | Perplexity: 936.248409
2025-10-01 10:55:19,202 Stage: Train 0.5 | Epoch: 99 | Iter: 75200 | Total Loss: 0.007281 | Recon Loss: 0.005963 | Commit Loss: 0.002637 | Perplexity: 940.675398
Trainning Epoch:  15%|█▌        | 100/665 [22:08:28<124:44:49, 794.85s/it]2025-10-01 10:58:57,278 Stage: Train 0.5 | Epoch: 100 | Iter: 75400 | Total Loss: 0.007247 | Recon Loss: 0.005919 | Commit Loss: 0.002656 | Perplexity: 940.940182
2025-10-01 11:02:33,143 Stage: Train 0.5 | Epoch: 100 | Iter: 75600 | Total Loss: 0.007462 | Recon Loss: 0.006142 | Commit Loss: 0.002640 | Perplexity: 939.427956
2025-10-01 11:06:06,799 Stage: Train 0.5 | Epoch: 100 | Iter: 75800 | Total Loss: 0.007239 | Recon Loss: 0.005937 | Commit Loss: 0.002604 | Perplexity: 938.211996
2025-10-01 11:09:46,584 Stage: Train 0.5 | Epoch: 100 | Iter: 76000 | Total Loss: 0.007306 | Recon Loss: 0.005980 | Commit Loss: 0.002653 | Perplexity: 940.531716
Trainning Epoch:  15%|█▌        | 101/665 [22:22:09<125:43:19, 802.48s/it]2025-10-01 11:13:30,479 Stage: Train 0.5 | Epoch: 101 | Iter: 76200 | Total Loss: 0.007287 | Recon Loss: 0.005979 | Commit Loss: 0.002615 | Perplexity: 937.513885
2025-10-01 11:16:58,577 Stage: Train 0.5 | Epoch: 101 | Iter: 76400 | Total Loss: 0.007169 | Recon Loss: 0.005851 | Commit Loss: 0.002636 | Perplexity: 940.319103
2025-10-01 11:20:27,874 Stage: Train 0.5 | Epoch: 101 | Iter: 76600 | Total Loss: 0.007321 | Recon Loss: 0.006013 | Commit Loss: 0.002616 | Perplexity: 940.437371
2025-10-01 11:24:02,399 Stage: Train 0.5 | Epoch: 101 | Iter: 76800 | Total Loss: 0.007352 | Recon Loss: 0.006025 | Commit Loss: 0.002654 | Perplexity: 940.835744
Trainning Epoch:  15%|█▌        | 102/665 [22:35:35<125:39:10, 803.46s/it]2025-10-01 11:27:41,955 Stage: Train 0.5 | Epoch: 102 | Iter: 77000 | Total Loss: 0.007364 | Recon Loss: 0.006039 | Commit Loss: 0.002650 | Perplexity: 942.018131
2025-10-01 11:31:12,627 Stage: Train 0.5 | Epoch: 102 | Iter: 77200 | Total Loss: 0.007144 | Recon Loss: 0.005844 | Commit Loss: 0.002599 | Perplexity: 938.932774
2025-10-01 11:34:46,858 Stage: Train 0.5 | Epoch: 102 | Iter: 77400 | Total Loss: 0.007311 | Recon Loss: 0.006002 | Commit Loss: 0.002618 | Perplexity: 941.005920
Trainning Epoch:  15%|█▌        | 103/665 [22:48:56<125:20:59, 802.95s/it]2025-10-01 11:38:22,526 Stage: Train 0.5 | Epoch: 103 | Iter: 77600 | Total Loss: 0.007291 | Recon Loss: 0.005979 | Commit Loss: 0.002624 | Perplexity: 940.063905
2025-10-01 11:41:51,137 Stage: Train 0.5 | Epoch: 103 | Iter: 77800 | Total Loss: 0.007114 | Recon Loss: 0.005800 | Commit Loss: 0.002627 | Perplexity: 948.607712
2025-10-01 11:45:21,086 Stage: Train 0.5 | Epoch: 103 | Iter: 78000 | Total Loss: 0.007351 | Recon Loss: 0.006048 | Commit Loss: 0.002605 | Perplexity: 941.915173
2025-10-01 11:48:54,571 Stage: Train 0.5 | Epoch: 103 | Iter: 78200 | Total Loss: 0.007396 | Recon Loss: 0.006087 | Commit Loss: 0.002618 | Perplexity: 938.269716
Trainning Epoch:  16%|█▌        | 104/665 [23:02:19<125:08:09, 803.01s/it]2025-10-01 11:52:33,763 Stage: Train 0.5 | Epoch: 104 | Iter: 78400 | Total Loss: 0.007204 | Recon Loss: 0.005903 | Commit Loss: 0.002603 | Perplexity: 943.516744
2025-10-01 11:56:05,197 Stage: Train 0.5 | Epoch: 104 | Iter: 78600 | Total Loss: 0.007176 | Recon Loss: 0.005873 | Commit Loss: 0.002605 | Perplexity: 944.247249
2025-10-01 11:59:39,562 Stage: Train 0.5 | Epoch: 104 | Iter: 78800 | Total Loss: 0.007210 | Recon Loss: 0.005907 | Commit Loss: 0.002607 | Perplexity: 943.140258
2025-10-01 12:03:05,701 Stage: Train 0.5 | Epoch: 104 | Iter: 79000 | Total Loss: 0.007362 | Recon Loss: 0.006060 | Commit Loss: 0.002605 | Perplexity: 940.788842
Trainning Epoch:  16%|█▌        | 105/665 [23:15:42<124:54:44, 803.01s/it]2025-10-01 12:06:52,057 Stage: Train 0.5 | Epoch: 105 | Iter: 79200 | Total Loss: 0.007071 | Recon Loss: 0.005780 | Commit Loss: 0.002582 | Perplexity: 941.270368
2025-10-01 12:10:21,885 Stage: Train 0.5 | Epoch: 105 | Iter: 79400 | Total Loss: 0.007299 | Recon Loss: 0.006009 | Commit Loss: 0.002580 | Perplexity: 943.184075
2025-10-01 12:13:54,480 Stage: Train 0.5 | Epoch: 105 | Iter: 79600 | Total Loss: 0.007148 | Recon Loss: 0.005852 | Commit Loss: 0.002592 | Perplexity: 946.832112
2025-10-01 12:17:24,222 Stage: Train 0.5 | Epoch: 105 | Iter: 79800 | Total Loss: 0.007138 | Recon Loss: 0.005831 | Commit Loss: 0.002614 | Perplexity: 949.942232
Trainning Epoch:  16%|█▌        | 106/665 [23:29:09<124:50:16, 803.97s/it]2025-10-01 12:20:59,497 Stage: Train 0.5 | Epoch: 106 | Iter: 80000 | Total Loss: 0.007265 | Recon Loss: 0.005958 | Commit Loss: 0.002614 | Perplexity: 947.075592
2025-10-01 12:20:59,498 Saving model at iteration 80000
2025-10-01 12:21:00,093 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000
2025-10-01 12:21:00,414 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000/model.safetensors
2025-10-01 12:21:00,837 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000/optimizer.bin
2025-10-01 12:21:00,838 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000/scheduler.bin
2025-10-01 12:21:00,838 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000/sampler.bin
2025-10-01 12:21:00,839 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_107_step_80000/random_states_0.pkl
2025-10-01 12:24:29,080 Stage: Train 0.5 | Epoch: 106 | Iter: 80200 | Total Loss: 0.007197 | Recon Loss: 0.005910 | Commit Loss: 0.002574 | Perplexity: 944.338076
2025-10-01 12:28:03,307 Stage: Train 0.5 | Epoch: 106 | Iter: 80400 | Total Loss: 0.007111 | Recon Loss: 0.005816 | Commit Loss: 0.002590 | Perplexity: 947.305947
Trainning Epoch:  16%|█▌        | 107/665 [23:42:25<124:14:35, 801.57s/it]2025-10-01 12:31:34,019 Stage: Train 0.5 | Epoch: 107 | Iter: 80600 | Total Loss: 0.007204 | Recon Loss: 0.005899 | Commit Loss: 0.002608 | Perplexity: 943.803056
2025-10-01 12:35:06,278 Stage: Train 0.5 | Epoch: 107 | Iter: 80800 | Total Loss: 0.007143 | Recon Loss: 0.005841 | Commit Loss: 0.002603 | Perplexity: 947.587743
2025-10-01 12:38:37,834 Stage: Train 0.5 | Epoch: 107 | Iter: 81000 | Total Loss: 0.007188 | Recon Loss: 0.005912 | Commit Loss: 0.002552 | Perplexity: 943.994714
2025-10-01 12:42:07,915 Stage: Train 0.5 | Epoch: 107 | Iter: 81200 | Total Loss: 0.007088 | Recon Loss: 0.005795 | Commit Loss: 0.002585 | Perplexity: 945.421794
Trainning Epoch:  16%|█▌        | 108/665 [23:55:47<124:03:58, 801.86s/it]2025-10-01 12:45:49,786 Stage: Train 0.5 | Epoch: 108 | Iter: 81400 | Total Loss: 0.007168 | Recon Loss: 0.005861 | Commit Loss: 0.002612 | Perplexity: 948.239454
2025-10-01 12:49:23,470 Stage: Train 0.5 | Epoch: 108 | Iter: 81600 | Total Loss: 0.007101 | Recon Loss: 0.005810 | Commit Loss: 0.002584 | Perplexity: 944.828448
2025-10-01 12:52:55,549 Stage: Train 0.5 | Epoch: 108 | Iter: 81800 | Total Loss: 0.007093 | Recon Loss: 0.005798 | Commit Loss: 0.002589 | Perplexity: 947.006822
2025-10-01 12:56:35,750 Stage: Train 0.5 | Epoch: 108 | Iter: 82000 | Total Loss: 0.007156 | Recon Loss: 0.005859 | Commit Loss: 0.002595 | Perplexity: 945.540093
Trainning Epoch:  16%|█▋        | 109/665 [24:09:30<124:48:21, 808.10s/it]2025-10-01 13:00:25,054 Stage: Train 0.5 | Epoch: 109 | Iter: 82200 | Total Loss: 0.007124 | Recon Loss: 0.005831 | Commit Loss: 0.002586 | Perplexity: 945.749239
2025-10-01 13:03:58,941 Stage: Train 0.5 | Epoch: 109 | Iter: 82400 | Total Loss: 0.007175 | Recon Loss: 0.005875 | Commit Loss: 0.002600 | Perplexity: 946.090364
2025-10-01 13:07:32,453 Stage: Train 0.5 | Epoch: 109 | Iter: 82600 | Total Loss: 0.007153 | Recon Loss: 0.005863 | Commit Loss: 0.002578 | Perplexity: 946.298953
2025-10-01 13:11:03,569 Stage: Train 0.5 | Epoch: 109 | Iter: 82800 | Total Loss: 0.007054 | Recon Loss: 0.005759 | Commit Loss: 0.002589 | Perplexity: 944.804484
Trainning Epoch:  17%|█▋        | 110/665 [24:22:58<124:36:07, 808.23s/it]2025-10-01 13:14:40,384 Stage: Train 0.5 | Epoch: 110 | Iter: 83000 | Total Loss: 0.007155 | Recon Loss: 0.005869 | Commit Loss: 0.002572 | Perplexity: 943.251230
2025-10-01 13:18:12,695 Stage: Train 0.5 | Epoch: 110 | Iter: 83200 | Total Loss: 0.007056 | Recon Loss: 0.005767 | Commit Loss: 0.002578 | Perplexity: 945.455717
2025-10-01 13:21:51,042 Stage: Train 0.5 | Epoch: 110 | Iter: 83400 | Total Loss: 0.007109 | Recon Loss: 0.005812 | Commit Loss: 0.002594 | Perplexity: 948.552953
Trainning Epoch:  17%|█▋        | 111/665 [24:36:32<124:37:06, 809.80s/it]2025-10-01 13:25:34,567 Stage: Train 0.5 | Epoch: 111 | Iter: 83600 | Total Loss: 0.007127 | Recon Loss: 0.005846 | Commit Loss: 0.002561 | Perplexity: 944.088218
2025-10-01 13:29:09,094 Stage: Train 0.5 | Epoch: 111 | Iter: 83800 | Total Loss: 0.007011 | Recon Loss: 0.005724 | Commit Loss: 0.002574 | Perplexity: 946.821672
2025-10-01 13:32:40,057 Stage: Train 0.5 | Epoch: 111 | Iter: 84000 | Total Loss: 0.007024 | Recon Loss: 0.005751 | Commit Loss: 0.002546 | Perplexity: 942.236769
2025-10-01 13:36:09,692 Stage: Train 0.5 | Epoch: 111 | Iter: 84200 | Total Loss: 0.007121 | Recon Loss: 0.005824 | Commit Loss: 0.002594 | Perplexity: 950.606263
Trainning Epoch:  17%|█▋        | 112/665 [24:49:59<124:16:08, 808.98s/it]2025-10-01 13:39:47,901 Stage: Train 0.5 | Epoch: 112 | Iter: 84400 | Total Loss: 0.007038 | Recon Loss: 0.005755 | Commit Loss: 0.002565 | Perplexity: 938.611574
2025-10-01 13:43:20,931 Stage: Train 0.5 | Epoch: 112 | Iter: 84600 | Total Loss: 0.007136 | Recon Loss: 0.005840 | Commit Loss: 0.002594 | Perplexity: 946.937000
2025-10-01 13:46:53,340 Stage: Train 0.5 | Epoch: 112 | Iter: 84800 | Total Loss: 0.007072 | Recon Loss: 0.005789 | Commit Loss: 0.002565 | Perplexity: 944.777816
2025-10-01 13:50:31,341 Stage: Train 0.5 | Epoch: 112 | Iter: 85000 | Total Loss: 0.007015 | Recon Loss: 0.005721 | Commit Loss: 0.002587 | Perplexity: 949.373138
Trainning Epoch:  17%|█▋        | 113/665 [25:03:34<124:20:06, 810.88s/it]2025-10-01 13:54:25,207 Stage: Train 0.5 | Epoch: 113 | Iter: 85200 | Total Loss: 0.007065 | Recon Loss: 0.005792 | Commit Loss: 0.002545 | Perplexity: 944.188945
2025-10-01 13:58:10,155 Stage: Train 0.5 | Epoch: 113 | Iter: 85400 | Total Loss: 0.007038 | Recon Loss: 0.005757 | Commit Loss: 0.002562 | Perplexity: 945.222334
2025-10-01 14:01:56,523 Stage: Train 0.5 | Epoch: 113 | Iter: 85600 | Total Loss: 0.007130 | Recon Loss: 0.005834 | Commit Loss: 0.002591 | Perplexity: 947.958300
2025-10-01 14:05:42,133 Stage: Train 0.5 | Epoch: 113 | Iter: 85800 | Total Loss: 0.007013 | Recon Loss: 0.005722 | Commit Loss: 0.002582 | Perplexity: 944.461261
Trainning Epoch:  17%|█▋        | 114/665 [25:17:54<126:20:48, 825.50s/it]2025-10-01 14:09:34,457 Stage: Train 0.5 | Epoch: 114 | Iter: 86000 | Total Loss: 0.007053 | Recon Loss: 0.005773 | Commit Loss: 0.002559 | Perplexity: 945.214109
2025-10-01 14:13:22,494 Stage: Train 0.5 | Epoch: 114 | Iter: 86200 | Total Loss: 0.006974 | Recon Loss: 0.005683 | Commit Loss: 0.002583 | Perplexity: 947.461084
2025-10-01 14:16:50,435 Stage: Train 0.5 | Epoch: 114 | Iter: 86400 | Total Loss: 0.007023 | Recon Loss: 0.005733 | Commit Loss: 0.002579 | Perplexity: 947.946260
Trainning Epoch:  17%|█▋        | 115/665 [25:31:42<126:13:14, 826.17s/it]2025-10-01 14:20:27,235 Stage: Train 0.5 | Epoch: 115 | Iter: 86600 | Total Loss: 0.007069 | Recon Loss: 0.005775 | Commit Loss: 0.002588 | Perplexity: 948.646240
2025-10-01 14:23:59,785 Stage: Train 0.5 | Epoch: 115 | Iter: 86800 | Total Loss: 0.007045 | Recon Loss: 0.005763 | Commit Loss: 0.002565 | Perplexity: 949.380316
2025-10-01 14:27:30,649 Stage: Train 0.5 | Epoch: 115 | Iter: 87000 | Total Loss: 0.007067 | Recon Loss: 0.005778 | Commit Loss: 0.002577 | Perplexity: 950.253301
2025-10-01 14:30:58,707 Stage: Train 0.5 | Epoch: 115 | Iter: 87200 | Total Loss: 0.007058 | Recon Loss: 0.005787 | Commit Loss: 0.002542 | Perplexity: 946.351095
Trainning Epoch:  17%|█▋        | 116/665 [25:45:04<124:53:27, 818.96s/it]2025-10-01 14:34:41,438 Stage: Train 0.5 | Epoch: 116 | Iter: 87400 | Total Loss: 0.007004 | Recon Loss: 0.005734 | Commit Loss: 0.002541 | Perplexity: 944.030905
2025-10-01 14:38:09,948 Stage: Train 0.5 | Epoch: 116 | Iter: 87600 | Total Loss: 0.007018 | Recon Loss: 0.005731 | Commit Loss: 0.002575 | Perplexity: 948.557456
2025-10-01 14:41:36,982 Stage: Train 0.5 | Epoch: 116 | Iter: 87800 | Total Loss: 0.007028 | Recon Loss: 0.005754 | Commit Loss: 0.002549 | Perplexity: 951.261000
2025-10-01 14:44:58,765 Stage: Train 0.5 | Epoch: 116 | Iter: 88000 | Total Loss: 0.006959 | Recon Loss: 0.005674 | Commit Loss: 0.002568 | Perplexity: 952.984081
Trainning Epoch:  18%|█▊        | 117/665 [25:58:12<123:14:48, 809.65s/it]2025-10-01 14:48:33,555 Stage: Train 0.5 | Epoch: 117 | Iter: 88200 | Total Loss: 0.007048 | Recon Loss: 0.005781 | Commit Loss: 0.002534 | Perplexity: 946.827714
2025-10-01 14:52:06,681 Stage: Train 0.5 | Epoch: 117 | Iter: 88400 | Total Loss: 0.006965 | Recon Loss: 0.005681 | Commit Loss: 0.002569 | Perplexity: 951.048394
2025-10-01 14:55:34,277 Stage: Train 0.5 | Epoch: 117 | Iter: 88600 | Total Loss: 0.006921 | Recon Loss: 0.005633 | Commit Loss: 0.002576 | Perplexity: 951.220229
2025-10-01 14:59:02,968 Stage: Train 0.5 | Epoch: 117 | Iter: 88800 | Total Loss: 0.007030 | Recon Loss: 0.005754 | Commit Loss: 0.002551 | Perplexity: 949.134378
Trainning Epoch:  18%|█▊        | 118/665 [26:11:23<122:11:50, 804.22s/it]2025-10-01 15:02:41,041 Stage: Train 0.5 | Epoch: 118 | Iter: 89000 | Total Loss: 0.006924 | Recon Loss: 0.005653 | Commit Loss: 0.002541 | Perplexity: 945.978813
2025-10-01 15:06:08,977 Stage: Train 0.5 | Epoch: 118 | Iter: 89200 | Total Loss: 0.006968 | Recon Loss: 0.005680 | Commit Loss: 0.002577 | Perplexity: 952.350183
2025-10-01 15:09:36,184 Stage: Train 0.5 | Epoch: 118 | Iter: 89400 | Total Loss: 0.007018 | Recon Loss: 0.005739 | Commit Loss: 0.002558 | Perplexity: 948.947300
2025-10-01 15:13:02,591 Stage: Train 0.5 | Epoch: 118 | Iter: 89600 | Total Loss: 0.006920 | Recon Loss: 0.005642 | Commit Loss: 0.002556 | Perplexity: 950.788213
Trainning Epoch:  18%|█▊        | 119/665 [26:24:34<121:22:55, 800.32s/it]2025-10-01 15:16:40,206 Stage: Train 0.5 | Epoch: 119 | Iter: 89800 | Total Loss: 0.007002 | Recon Loss: 0.005745 | Commit Loss: 0.002514 | Perplexity: 943.819468
2025-10-01 15:20:17,376 Stage: Train 0.5 | Epoch: 119 | Iter: 90000 | Total Loss: 0.006945 | Recon Loss: 0.005659 | Commit Loss: 0.002573 | Perplexity: 953.010048
2025-10-01 15:23:46,999 Stage: Train 0.5 | Epoch: 119 | Iter: 90200 | Total Loss: 0.006914 | Recon Loss: 0.005635 | Commit Loss: 0.002557 | Perplexity: 952.909688
Trainning Epoch:  18%|█▊        | 120/665 [26:38:01<121:26:31, 802.19s/it]2025-10-01 15:27:25,212 Stage: Train 0.5 | Epoch: 120 | Iter: 90400 | Total Loss: 0.006888 | Recon Loss: 0.005606 | Commit Loss: 0.002563 | Perplexity: 951.246950
2025-10-01 15:30:41,322 Stage: Train 0.5 | Epoch: 120 | Iter: 90600 | Total Loss: 0.007070 | Recon Loss: 0.005788 | Commit Loss: 0.002563 | Perplexity: 952.551120
2025-10-01 15:33:43,682 Stage: Train 0.5 | Epoch: 120 | Iter: 90800 | Total Loss: 0.006857 | Recon Loss: 0.005591 | Commit Loss: 0.002532 | Perplexity: 947.235675
2025-10-01 15:36:45,126 Stage: Train 0.5 | Epoch: 120 | Iter: 91000 | Total Loss: 0.007012 | Recon Loss: 0.005746 | Commit Loss: 0.002533 | Perplexity: 947.910934
Trainning Epoch:  18%|█▊        | 121/665 [26:49:57<117:18:39, 776.32s/it]2025-10-01 15:39:56,110 Stage: Train 0.5 | Epoch: 121 | Iter: 91200 | Total Loss: 0.006832 | Recon Loss: 0.005558 | Commit Loss: 0.002548 | Perplexity: 949.013713
2025-10-01 15:42:58,381 Stage: Train 0.5 | Epoch: 121 | Iter: 91400 | Total Loss: 0.007056 | Recon Loss: 0.005788 | Commit Loss: 0.002536 | Perplexity: 947.474370
2025-10-01 15:46:00,860 Stage: Train 0.5 | Epoch: 121 | Iter: 91600 | Total Loss: 0.006857 | Recon Loss: 0.005592 | Commit Loss: 0.002530 | Perplexity: 948.178193
2025-10-01 15:49:06,102 Stage: Train 0.5 | Epoch: 121 | Iter: 91800 | Total Loss: 0.006998 | Recon Loss: 0.005724 | Commit Loss: 0.002548 | Perplexity: 951.790244
Trainning Epoch:  18%|█▊        | 122/665 [27:01:33<113:28:48, 752.36s/it]2025-10-01 15:52:16,842 Stage: Train 0.5 | Epoch: 122 | Iter: 92000 | Total Loss: 0.006954 | Recon Loss: 0.005674 | Commit Loss: 0.002560 | Perplexity: 948.544969
2025-10-01 15:55:21,190 Stage: Train 0.5 | Epoch: 122 | Iter: 92200 | Total Loss: 0.006958 | Recon Loss: 0.005699 | Commit Loss: 0.002519 | Perplexity: 952.070546
2025-10-01 15:58:23,372 Stage: Train 0.5 | Epoch: 122 | Iter: 92400 | Total Loss: 0.006882 | Recon Loss: 0.005604 | Commit Loss: 0.002557 | Perplexity: 948.453916
2025-10-01 16:01:24,843 Stage: Train 0.5 | Epoch: 122 | Iter: 92600 | Total Loss: 0.006888 | Recon Loss: 0.005611 | Commit Loss: 0.002554 | Perplexity: 953.367479
Trainning Epoch:  18%|█▊        | 123/665 [27:13:07<110:38:40, 734.91s/it]2025-10-01 16:04:31,592 Stage: Train 0.5 | Epoch: 123 | Iter: 92800 | Total Loss: 0.006890 | Recon Loss: 0.005625 | Commit Loss: 0.002528 | Perplexity: 949.819007
2025-10-01 16:07:33,403 Stage: Train 0.5 | Epoch: 123 | Iter: 93000 | Total Loss: 0.006816 | Recon Loss: 0.005539 | Commit Loss: 0.002555 | Perplexity: 952.978326
2025-10-01 16:10:33,750 Stage: Train 0.5 | Epoch: 123 | Iter: 93200 | Total Loss: 0.006951 | Recon Loss: 0.005665 | Commit Loss: 0.002572 | Perplexity: 951.958908
Trainning Epoch:  19%|█▊        | 124/665 [27:24:34<108:15:17, 720.37s/it]2025-10-01 16:13:40,015 Stage: Train 0.5 | Epoch: 124 | Iter: 93400 | Total Loss: 0.006864 | Recon Loss: 0.005583 | Commit Loss: 0.002563 | Perplexity: 949.474816
2025-10-01 16:16:44,175 Stage: Train 0.5 | Epoch: 124 | Iter: 93600 | Total Loss: 0.006922 | Recon Loss: 0.005650 | Commit Loss: 0.002545 | Perplexity: 949.669042
2025-10-01 16:19:47,955 Stage: Train 0.5 | Epoch: 124 | Iter: 93800 | Total Loss: 0.006883 | Recon Loss: 0.005612 | Commit Loss: 0.002541 | Perplexity: 950.747559
2025-10-01 16:22:51,473 Stage: Train 0.5 | Epoch: 124 | Iter: 94000 | Total Loss: 0.006821 | Recon Loss: 0.005548 | Commit Loss: 0.002546 | Perplexity: 954.391601
Trainning Epoch:  19%|█▉        | 125/665 [27:36:10<106:58:13, 713.14s/it]2025-10-01 16:26:01,563 Stage: Train 0.5 | Epoch: 125 | Iter: 94200 | Total Loss: 0.006895 | Recon Loss: 0.005629 | Commit Loss: 0.002533 | Perplexity: 946.821356
2025-10-01 16:29:02,869 Stage: Train 0.5 | Epoch: 125 | Iter: 94400 | Total Loss: 0.006947 | Recon Loss: 0.005680 | Commit Loss: 0.002534 | Perplexity: 948.517541
2025-10-01 16:32:04,779 Stage: Train 0.5 | Epoch: 125 | Iter: 94600 | Total Loss: 0.006862 | Recon Loss: 0.005588 | Commit Loss: 0.002547 | Perplexity: 951.850944
2025-10-01 16:35:09,544 Stage: Train 0.5 | Epoch: 125 | Iter: 94800 | Total Loss: 0.006866 | Recon Loss: 0.005603 | Commit Loss: 0.002528 | Perplexity: 952.404149
Trainning Epoch:  19%|█▉        | 126/665 [27:47:44<105:54:48, 707.40s/it]2025-10-01 16:38:18,539 Stage: Train 0.5 | Epoch: 126 | Iter: 95000 | Total Loss: 0.006901 | Recon Loss: 0.005643 | Commit Loss: 0.002515 | Perplexity: 946.757190
2025-10-01 16:41:20,519 Stage: Train 0.5 | Epoch: 126 | Iter: 95200 | Total Loss: 0.006898 | Recon Loss: 0.005627 | Commit Loss: 0.002541 | Perplexity: 954.230824
2025-10-01 16:44:21,609 Stage: Train 0.5 | Epoch: 126 | Iter: 95400 | Total Loss: 0.006837 | Recon Loss: 0.005581 | Commit Loss: 0.002512 | Perplexity: 947.794975
2025-10-01 16:47:23,819 Stage: Train 0.5 | Epoch: 126 | Iter: 95600 | Total Loss: 0.006863 | Recon Loss: 0.005583 | Commit Loss: 0.002560 | Perplexity: 956.859865
Trainning Epoch:  19%|█▉        | 127/665 [27:59:17<105:03:26, 702.99s/it]2025-10-01 16:50:34,239 Stage: Train 0.5 | Epoch: 127 | Iter: 95800 | Total Loss: 0.006874 | Recon Loss: 0.005610 | Commit Loss: 0.002529 | Perplexity: 949.504777
2025-10-01 16:53:37,411 Stage: Train 0.5 | Epoch: 127 | Iter: 96000 | Total Loss: 0.006824 | Recon Loss: 0.005550 | Commit Loss: 0.002548 | Perplexity: 954.310843
2025-10-01 16:56:39,158 Stage: Train 0.5 | Epoch: 127 | Iter: 96200 | Total Loss: 0.006848 | Recon Loss: 0.005585 | Commit Loss: 0.002526 | Perplexity: 954.490674
Trainning Epoch:  19%|█▉        | 128/665 [28:10:52<104:30:09, 700.58s/it]2025-10-01 16:59:48,193 Stage: Train 0.5 | Epoch: 128 | Iter: 96400 | Total Loss: 0.006915 | Recon Loss: 0.005649 | Commit Loss: 0.002532 | Perplexity: 950.888890
2025-10-01 17:02:49,521 Stage: Train 0.5 | Epoch: 128 | Iter: 96600 | Total Loss: 0.006798 | Recon Loss: 0.005521 | Commit Loss: 0.002555 | Perplexity: 954.687796
2025-10-01 17:05:51,356 Stage: Train 0.5 | Epoch: 128 | Iter: 96800 | Total Loss: 0.006789 | Recon Loss: 0.005519 | Commit Loss: 0.002539 | Perplexity: 955.945134
2025-10-01 17:08:52,581 Stage: Train 0.5 | Epoch: 128 | Iter: 97000 | Total Loss: 0.006785 | Recon Loss: 0.005532 | Commit Loss: 0.002506 | Perplexity: 950.451802
Trainning Epoch:  19%|█▉        | 129/665 [28:22:23<103:53:50, 697.82s/it]2025-10-01 17:12:02,567 Stage: Train 0.5 | Epoch: 129 | Iter: 97200 | Total Loss: 0.006837 | Recon Loss: 0.005570 | Commit Loss: 0.002533 | Perplexity: 951.297357
2025-10-01 17:15:05,328 Stage: Train 0.5 | Epoch: 129 | Iter: 97400 | Total Loss: 0.006810 | Recon Loss: 0.005536 | Commit Loss: 0.002548 | Perplexity: 956.609828
2025-10-01 17:18:09,801 Stage: Train 0.5 | Epoch: 129 | Iter: 97600 | Total Loss: 0.006816 | Recon Loss: 0.005567 | Commit Loss: 0.002498 | Perplexity: 948.989608
2025-10-01 17:21:17,535 Stage: Train 0.5 | Epoch: 129 | Iter: 97800 | Total Loss: 0.006840 | Recon Loss: 0.005579 | Commit Loss: 0.002522 | Perplexity: 951.755137
Trainning Epoch:  20%|█▉        | 130/665 [28:34:06<103:55:02, 699.26s/it]2025-10-01 17:24:27,204 Stage: Train 0.5 | Epoch: 130 | Iter: 98000 | Total Loss: 0.006824 | Recon Loss: 0.005577 | Commit Loss: 0.002494 | Perplexity: 950.315865
2025-10-01 17:27:30,276 Stage: Train 0.5 | Epoch: 130 | Iter: 98200 | Total Loss: 0.006874 | Recon Loss: 0.005620 | Commit Loss: 0.002507 | Perplexity: 955.377442
2025-10-01 17:30:32,847 Stage: Train 0.5 | Epoch: 130 | Iter: 98400 | Total Loss: 0.006884 | Recon Loss: 0.005614 | Commit Loss: 0.002540 | Perplexity: 954.402894
2025-10-01 17:33:36,125 Stage: Train 0.5 | Epoch: 130 | Iter: 98600 | Total Loss: 0.006768 | Recon Loss: 0.005515 | Commit Loss: 0.002507 | Perplexity: 952.073930
Trainning Epoch:  20%|█▉        | 131/665 [28:45:41<103:31:39, 697.94s/it]2025-10-01 17:36:44,685 Stage: Train 0.5 | Epoch: 131 | Iter: 98800 | Total Loss: 0.006750 | Recon Loss: 0.005494 | Commit Loss: 0.002512 | Perplexity: 949.255383
2025-10-01 17:39:46,581 Stage: Train 0.5 | Epoch: 131 | Iter: 99000 | Total Loss: 0.006806 | Recon Loss: 0.005550 | Commit Loss: 0.002512 | Perplexity: 954.692685
2025-10-01 17:42:51,816 Stage: Train 0.5 | Epoch: 131 | Iter: 99200 | Total Loss: 0.006758 | Recon Loss: 0.005497 | Commit Loss: 0.002523 | Perplexity: 954.942841
Trainning Epoch:  20%|█▉        | 132/665 [28:57:18<103:18:44, 697.80s/it]2025-10-01 17:46:02,178 Stage: Train 0.5 | Epoch: 132 | Iter: 99400 | Total Loss: 0.006865 | Recon Loss: 0.005606 | Commit Loss: 0.002519 | Perplexity: 952.715169
2025-10-01 17:49:08,572 Stage: Train 0.5 | Epoch: 132 | Iter: 99600 | Total Loss: 0.006723 | Recon Loss: 0.005472 | Commit Loss: 0.002501 | Perplexity: 957.731346
2025-10-01 17:52:12,137 Stage: Train 0.5 | Epoch: 132 | Iter: 99800 | Total Loss: 0.006916 | Recon Loss: 0.005658 | Commit Loss: 0.002516 | Perplexity: 956.203934
2025-10-01 17:55:17,218 Stage: Train 0.5 | Epoch: 132 | Iter: 100000 | Total Loss: 0.006708 | Recon Loss: 0.005452 | Commit Loss: 0.002512 | Perplexity: 951.029092
2025-10-01 17:55:17,219 Saving model at iteration 100000
2025-10-01 17:55:17,407 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000
2025-10-01 17:55:17,703 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000/model.safetensors
2025-10-01 17:55:18,094 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000/optimizer.bin
2025-10-01 17:55:18,095 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000/scheduler.bin
2025-10-01 17:55:18,095 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000/sampler.bin
2025-10-01 17:55:18,096 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_133_step_100000/random_states_0.pkl
Trainning Epoch:  20%|██        | 133/665 [29:09:02<103:23:50, 699.68s/it]2025-10-01 17:58:29,527 Stage: Train 0.5 | Epoch: 133 | Iter: 100200 | Total Loss: 0.006883 | Recon Loss: 0.005637 | Commit Loss: 0.002490 | Perplexity: 947.959455
2025-10-01 18:01:33,559 Stage: Train 0.5 | Epoch: 133 | Iter: 100400 | Total Loss: 0.006753 | Recon Loss: 0.005491 | Commit Loss: 0.002524 | Perplexity: 955.347318
2025-10-01 18:04:36,516 Stage: Train 0.5 | Epoch: 133 | Iter: 100600 | Total Loss: 0.006784 | Recon Loss: 0.005523 | Commit Loss: 0.002522 | Perplexity: 956.055547
2025-10-01 18:07:39,584 Stage: Train 0.5 | Epoch: 133 | Iter: 100800 | Total Loss: 0.006654 | Recon Loss: 0.005405 | Commit Loss: 0.002498 | Perplexity: 951.686551
Trainning Epoch:  20%|██        | 134/665 [29:20:37<102:59:42, 698.27s/it]2025-10-01 18:10:50,286 Stage: Train 0.5 | Epoch: 134 | Iter: 101000 | Total Loss: 0.006804 | Recon Loss: 0.005554 | Commit Loss: 0.002501 | Perplexity: 951.507298
2025-10-01 18:13:51,454 Stage: Train 0.5 | Epoch: 134 | Iter: 101200 | Total Loss: 0.006778 | Recon Loss: 0.005526 | Commit Loss: 0.002502 | Perplexity: 956.995454
2025-10-01 18:16:50,866 Stage: Train 0.5 | Epoch: 134 | Iter: 101400 | Total Loss: 0.006672 | Recon Loss: 0.005424 | Commit Loss: 0.002495 | Perplexity: 954.336735
2025-10-01 18:19:51,955 Stage: Train 0.5 | Epoch: 134 | Iter: 101600 | Total Loss: 0.006758 | Recon Loss: 0.005503 | Commit Loss: 0.002510 | Perplexity: 952.048908
Trainning Epoch:  20%|██        | 135/665 [29:32:06<102:23:47, 695.52s/it]2025-10-01 18:23:02,305 Stage: Train 0.5 | Epoch: 135 | Iter: 101800 | Total Loss: 0.006675 | Recon Loss: 0.005420 | Commit Loss: 0.002511 | Perplexity: 957.688407
2025-10-01 18:26:07,379 Stage: Train 0.5 | Epoch: 135 | Iter: 102000 | Total Loss: 0.006666 | Recon Loss: 0.005423 | Commit Loss: 0.002487 | Perplexity: 954.514767
2025-10-01 18:29:13,444 Stage: Train 0.5 | Epoch: 135 | Iter: 102200 | Total Loss: 0.006653 | Recon Loss: 0.005398 | Commit Loss: 0.002511 | Perplexity: 958.741563
2025-10-01 18:32:17,900 Stage: Train 0.5 | Epoch: 135 | Iter: 102400 | Total Loss: 0.006800 | Recon Loss: 0.005548 | Commit Loss: 0.002505 | Perplexity: 950.492432
Trainning Epoch:  20%|██        | 136/665 [29:43:50<102:33:40, 697.96s/it]2025-10-01 18:35:27,763 Stage: Train 0.5 | Epoch: 136 | Iter: 102600 | Total Loss: 0.006679 | Recon Loss: 0.005438 | Commit Loss: 0.002483 | Perplexity: 950.059066
2025-10-01 18:38:30,418 Stage: Train 0.5 | Epoch: 136 | Iter: 102800 | Total Loss: 0.006756 | Recon Loss: 0.005506 | Commit Loss: 0.002499 | Perplexity: 955.197526
2025-10-01 18:41:33,283 Stage: Train 0.5 | Epoch: 136 | Iter: 103000 | Total Loss: 0.006661 | Recon Loss: 0.005411 | Commit Loss: 0.002502 | Perplexity: 957.824435
Trainning Epoch:  21%|██        | 137/665 [29:55:28<102:22:48, 698.05s/it]2025-10-01 18:44:42,313 Stage: Train 0.5 | Epoch: 137 | Iter: 103200 | Total Loss: 0.006811 | Recon Loss: 0.005562 | Commit Loss: 0.002499 | Perplexity: 955.336383
2025-10-01 18:47:47,776 Stage: Train 0.5 | Epoch: 137 | Iter: 103400 | Total Loss: 0.006660 | Recon Loss: 0.005417 | Commit Loss: 0.002486 | Perplexity: 953.039754
2025-10-01 18:50:50,782 Stage: Train 0.5 | Epoch: 137 | Iter: 103600 | Total Loss: 0.006616 | Recon Loss: 0.005371 | Commit Loss: 0.002489 | Perplexity: 960.088279
2025-10-01 18:53:54,086 Stage: Train 0.5 | Epoch: 137 | Iter: 103800 | Total Loss: 0.006740 | Recon Loss: 0.005498 | Commit Loss: 0.002485 | Perplexity: 954.657464
Trainning Epoch:  21%|██        | 138/665 [30:07:05<102:08:49, 697.78s/it]2025-10-01 18:57:03,283 Stage: Train 0.5 | Epoch: 138 | Iter: 104000 | Total Loss: 0.006801 | Recon Loss: 0.005563 | Commit Loss: 0.002475 | Perplexity: 955.320672
2025-10-01 19:00:06,723 Stage: Train 0.5 | Epoch: 138 | Iter: 104200 | Total Loss: 0.006633 | Recon Loss: 0.005388 | Commit Loss: 0.002490 | Perplexity: 960.303839
2025-10-01 19:03:10,829 Stage: Train 0.5 | Epoch: 138 | Iter: 104400 | Total Loss: 0.006671 | Recon Loss: 0.005412 | Commit Loss: 0.002516 | Perplexity: 960.487311
2025-10-01 19:06:12,252 Stage: Train 0.5 | Epoch: 138 | Iter: 104600 | Total Loss: 0.006719 | Recon Loss: 0.005477 | Commit Loss: 0.002485 | Perplexity: 958.062468
Trainning Epoch:  21%|██        | 139/665 [30:18:38<101:44:48, 696.37s/it]2025-10-01 19:09:19,180 Stage: Train 0.5 | Epoch: 139 | Iter: 104800 | Total Loss: 0.006653 | Recon Loss: 0.005417 | Commit Loss: 0.002471 | Perplexity: 954.790428
2025-10-01 19:12:18,837 Stage: Train 0.5 | Epoch: 139 | Iter: 105000 | Total Loss: 0.006631 | Recon Loss: 0.005390 | Commit Loss: 0.002482 | Perplexity: 957.101110
2025-10-01 19:15:21,601 Stage: Train 0.5 | Epoch: 139 | Iter: 105200 | Total Loss: 0.006679 | Recon Loss: 0.005431 | Commit Loss: 0.002496 | Perplexity: 957.929140
2025-10-01 19:18:21,235 Stage: Train 0.5 | Epoch: 139 | Iter: 105400 | Total Loss: 0.006689 | Recon Loss: 0.005447 | Commit Loss: 0.002484 | Perplexity: 957.592201
Trainning Epoch:  21%|██        | 140/665 [30:30:04<101:06:01, 693.26s/it]2025-10-01 19:21:36,056 Stage: Train 0.5 | Epoch: 140 | Iter: 105600 | Total Loss: 0.006631 | Recon Loss: 0.005387 | Commit Loss: 0.002489 | Perplexity: 960.309485
2025-10-01 19:24:45,258 Stage: Train 0.5 | Epoch: 140 | Iter: 105800 | Total Loss: 0.006651 | Recon Loss: 0.005411 | Commit Loss: 0.002480 | Perplexity: 959.314910
2025-10-01 19:27:51,217 Stage: Train 0.5 | Epoch: 140 | Iter: 106000 | Total Loss: 0.006654 | Recon Loss: 0.005404 | Commit Loss: 0.002499 | Perplexity: 960.038843
Trainning Epoch:  21%|██        | 141/665 [30:41:55<101:40:26, 698.52s/it]2025-10-01 19:31:00,860 Stage: Train 0.5 | Epoch: 141 | Iter: 106200 | Total Loss: 0.006661 | Recon Loss: 0.005427 | Commit Loss: 0.002468 | Perplexity: 951.791614
2025-10-01 19:34:02,313 Stage: Train 0.5 | Epoch: 141 | Iter: 106400 | Total Loss: 0.006577 | Recon Loss: 0.005348 | Commit Loss: 0.002458 | Perplexity: 953.866645
2025-10-01 19:37:01,735 Stage: Train 0.5 | Epoch: 141 | Iter: 106600 | Total Loss: 0.006741 | Recon Loss: 0.005504 | Commit Loss: 0.002475 | Perplexity: 956.810527
2025-10-01 19:40:05,355 Stage: Train 0.5 | Epoch: 141 | Iter: 106800 | Total Loss: 0.006571 | Recon Loss: 0.005339 | Commit Loss: 0.002463 | Perplexity: 960.308841
Trainning Epoch:  21%|██▏       | 142/665 [30:53:27<101:11:09, 696.50s/it]2025-10-01 19:43:16,767 Stage: Train 0.5 | Epoch: 142 | Iter: 107000 | Total Loss: 0.006661 | Recon Loss: 0.005417 | Commit Loss: 0.002488 | Perplexity: 958.405539
2025-10-01 19:46:23,475 Stage: Train 0.5 | Epoch: 142 | Iter: 107200 | Total Loss: 0.006698 | Recon Loss: 0.005454 | Commit Loss: 0.002487 | Perplexity: 960.132291
2025-10-01 19:49:26,208 Stage: Train 0.5 | Epoch: 142 | Iter: 107400 | Total Loss: 0.006644 | Recon Loss: 0.005415 | Commit Loss: 0.002458 | Perplexity: 956.164464
2025-10-01 19:52:29,351 Stage: Train 0.5 | Epoch: 142 | Iter: 107600 | Total Loss: 0.006639 | Recon Loss: 0.005404 | Commit Loss: 0.002469 | Perplexity: 954.898541
Trainning Epoch:  22%|██▏       | 143/665 [31:05:07<101:07:41, 697.44s/it]2025-10-01 19:55:40,816 Stage: Train 0.5 | Epoch: 143 | Iter: 107800 | Total Loss: 0.006593 | Recon Loss: 0.005352 | Commit Loss: 0.002482 | Perplexity: 959.690034
2025-10-01 19:58:44,321 Stage: Train 0.5 | Epoch: 143 | Iter: 108000 | Total Loss: 0.006635 | Recon Loss: 0.005392 | Commit Loss: 0.002486 | Perplexity: 956.989202
2025-10-01 20:01:47,035 Stage: Train 0.5 | Epoch: 143 | Iter: 108200 | Total Loss: 0.006555 | Recon Loss: 0.005319 | Commit Loss: 0.002473 | Perplexity: 957.816696
2025-10-01 20:04:50,626 Stage: Train 0.5 | Epoch: 143 | Iter: 108400 | Total Loss: 0.006624 | Recon Loss: 0.005391 | Commit Loss: 0.002466 | Perplexity: 958.760202
Trainning Epoch:  22%|██▏       | 144/665 [31:16:45<100:57:32, 697.61s/it]2025-10-01 20:07:59,726 Stage: Train 0.5 | Epoch: 144 | Iter: 108600 | Total Loss: 0.006559 | Recon Loss: 0.005333 | Commit Loss: 0.002453 | Perplexity: 958.586694
2025-10-01 20:11:04,247 Stage: Train 0.5 | Epoch: 144 | Iter: 108800 | Total Loss: 0.006645 | Recon Loss: 0.005413 | Commit Loss: 0.002465 | Perplexity: 955.049611
2025-10-01 20:14:03,250 Stage: Train 0.5 | Epoch: 144 | Iter: 109000 | Total Loss: 0.006600 | Recon Loss: 0.005366 | Commit Loss: 0.002468 | Perplexity: 958.044019
Trainning Epoch:  22%|██▏       | 145/665 [31:28:19<100:37:29, 696.63s/it]2025-10-01 20:17:12,405 Stage: Train 0.5 | Epoch: 145 | Iter: 109200 | Total Loss: 0.006586 | Recon Loss: 0.005344 | Commit Loss: 0.002483 | Perplexity: 958.687213
2025-10-01 20:20:14,845 Stage: Train 0.5 | Epoch: 145 | Iter: 109400 | Total Loss: 0.006561 | Recon Loss: 0.005315 | Commit Loss: 0.002491 | Perplexity: 959.147694
2025-10-01 20:23:18,074 Stage: Train 0.5 | Epoch: 145 | Iter: 109600 | Total Loss: 0.006550 | Recon Loss: 0.005331 | Commit Loss: 0.002438 | Perplexity: 955.747867
2025-10-01 20:26:20,359 Stage: Train 0.5 | Epoch: 145 | Iter: 109800 | Total Loss: 0.006656 | Recon Loss: 0.005428 | Commit Loss: 0.002456 | Perplexity: 953.557510
Trainning Epoch:  22%|██▏       | 146/665 [31:39:53<100:19:25, 695.89s/it]2025-10-01 20:29:31,643 Stage: Train 0.5 | Epoch: 146 | Iter: 110000 | Total Loss: 0.006516 | Recon Loss: 0.005276 | Commit Loss: 0.002480 | Perplexity: 962.614990
2025-10-01 20:32:34,459 Stage: Train 0.5 | Epoch: 146 | Iter: 110200 | Total Loss: 0.006604 | Recon Loss: 0.005370 | Commit Loss: 0.002467 | Perplexity: 958.128030
2025-10-01 20:35:37,018 Stage: Train 0.5 | Epoch: 146 | Iter: 110400 | Total Loss: 0.006554 | Recon Loss: 0.005314 | Commit Loss: 0.002480 | Perplexity: 961.921487
2025-10-01 20:38:39,010 Stage: Train 0.5 | Epoch: 146 | Iter: 110600 | Total Loss: 0.006529 | Recon Loss: 0.005307 | Commit Loss: 0.002444 | Perplexity: 956.599973
Trainning Epoch:  22%|██▏       | 147/665 [31:51:27<100:02:43, 695.30s/it]2025-10-01 20:41:48,511 Stage: Train 0.5 | Epoch: 147 | Iter: 110800 | Total Loss: 0.006727 | Recon Loss: 0.005500 | Commit Loss: 0.002455 | Perplexity: 956.528413
2025-10-01 20:44:52,575 Stage: Train 0.5 | Epoch: 147 | Iter: 111000 | Total Loss: 0.006587 | Recon Loss: 0.005369 | Commit Loss: 0.002437 | Perplexity: 955.120685
2025-10-01 20:47:52,821 Stage: Train 0.5 | Epoch: 147 | Iter: 111200 | Total Loss: 0.006613 | Recon Loss: 0.005377 | Commit Loss: 0.002472 | Perplexity: 959.831072
2025-10-01 20:50:53,928 Stage: Train 0.5 | Epoch: 147 | Iter: 111400 | Total Loss: 0.006535 | Recon Loss: 0.005314 | Commit Loss: 0.002442 | Perplexity: 957.444246
Trainning Epoch:  22%|██▏       | 148/665 [32:03:01<99:46:51, 694.80s/it] 2025-10-01 20:54:05,866 Stage: Train 0.5 | Epoch: 148 | Iter: 111600 | Total Loss: 0.006540 | Recon Loss: 0.005312 | Commit Loss: 0.002455 | Perplexity: 962.073357
2025-10-01 20:57:04,814 Stage: Train 0.5 | Epoch: 148 | Iter: 111800 | Total Loss: 0.006534 | Recon Loss: 0.005320 | Commit Loss: 0.002429 | Perplexity: 954.517674
2025-10-01 21:00:06,893 Stage: Train 0.5 | Epoch: 148 | Iter: 112000 | Total Loss: 0.006492 | Recon Loss: 0.005256 | Commit Loss: 0.002472 | Perplexity: 958.285722
Trainning Epoch:  22%|██▏       | 149/665 [32:14:33<99:29:47, 694.16s/it]2025-10-01 21:03:16,238 Stage: Train 0.5 | Epoch: 149 | Iter: 112200 | Total Loss: 0.006723 | Recon Loss: 0.005490 | Commit Loss: 0.002466 | Perplexity: 956.885206
2025-10-01 21:06:17,770 Stage: Train 0.5 | Epoch: 149 | Iter: 112400 | Total Loss: 0.006503 | Recon Loss: 0.005285 | Commit Loss: 0.002435 | Perplexity: 955.774514
2025-10-01 21:09:21,537 Stage: Train 0.5 | Epoch: 149 | Iter: 112600 | Total Loss: 0.006608 | Recon Loss: 0.005369 | Commit Loss: 0.002479 | Perplexity: 961.292099
2025-10-01 21:12:21,989 Stage: Train 0.5 | Epoch: 149 | Iter: 112800 | Total Loss: 0.006557 | Recon Loss: 0.005334 | Commit Loss: 0.002445 | Perplexity: 956.291825
Trainning Epoch:  23%|██▎       | 150/665 [32:26:03<99:06:27, 692.79s/it]2025-10-01 21:15:29,107 Stage: Train 0.5 | Epoch: 150 | Iter: 113000 | Total Loss: 0.006570 | Recon Loss: 0.005336 | Commit Loss: 0.002468 | Perplexity: 960.147238
2025-10-01 21:18:34,231 Stage: Train 0.5 | Epoch: 150 | Iter: 113200 | Total Loss: 0.006539 | Recon Loss: 0.005323 | Commit Loss: 0.002432 | Perplexity: 958.561596
2025-10-01 21:21:34,819 Stage: Train 0.5 | Epoch: 150 | Iter: 113400 | Total Loss: 0.006522 | Recon Loss: 0.005309 | Commit Loss: 0.002426 | Perplexity: 956.457310
2025-10-01 21:24:38,308 Stage: Train 0.5 | Epoch: 150 | Iter: 113600 | Total Loss: 0.006575 | Recon Loss: 0.005335 | Commit Loss: 0.002478 | Perplexity: 959.142753
Trainning Epoch:  23%|██▎       | 151/665 [32:37:36<98:54:38, 692.76s/it]2025-10-01 21:27:45,899 Stage: Train 0.5 | Epoch: 151 | Iter: 113800 | Total Loss: 0.006515 | Recon Loss: 0.005297 | Commit Loss: 0.002436 | Perplexity: 955.032113
2025-10-01 21:30:50,078 Stage: Train 0.5 | Epoch: 151 | Iter: 114000 | Total Loss: 0.006444 | Recon Loss: 0.005229 | Commit Loss: 0.002430 | Perplexity: 958.241644
2025-10-01 21:33:50,961 Stage: Train 0.5 | Epoch: 151 | Iter: 114200 | Total Loss: 0.006496 | Recon Loss: 0.005276 | Commit Loss: 0.002440 | Perplexity: 959.854518
2025-10-01 21:36:51,936 Stage: Train 0.5 | Epoch: 151 | Iter: 114400 | Total Loss: 0.006518 | Recon Loss: 0.005274 | Commit Loss: 0.002487 | Perplexity: 962.895516
Trainning Epoch:  23%|██▎       | 152/665 [32:49:09<98:43:24, 692.80s/it]2025-10-01 21:40:02,671 Stage: Train 0.5 | Epoch: 152 | Iter: 114600 | Total Loss: 0.006557 | Recon Loss: 0.005330 | Commit Loss: 0.002454 | Perplexity: 960.017820
2025-10-01 21:43:04,900 Stage: Train 0.5 | Epoch: 152 | Iter: 114800 | Total Loss: 0.006464 | Recon Loss: 0.005246 | Commit Loss: 0.002436 | Perplexity: 961.658929
2025-10-01 21:46:07,916 Stage: Train 0.5 | Epoch: 152 | Iter: 115000 | Total Loss: 0.006536 | Recon Loss: 0.005313 | Commit Loss: 0.002446 | Perplexity: 956.267173
2025-10-01 21:49:11,441 Stage: Train 0.5 | Epoch: 152 | Iter: 115200 | Total Loss: 0.006487 | Recon Loss: 0.005258 | Commit Loss: 0.002458 | Perplexity: 960.329804
Trainning Epoch:  23%|██▎       | 153/665 [33:00:44<98:39:01, 693.64s/it]2025-10-01 21:52:19,270 Stage: Train 0.5 | Epoch: 153 | Iter: 115400 | Total Loss: 0.006592 | Recon Loss: 0.005382 | Commit Loss: 0.002420 | Perplexity: 959.573130
2025-10-01 21:55:20,957 Stage: Train 0.5 | Epoch: 153 | Iter: 115600 | Total Loss: 0.006486 | Recon Loss: 0.005256 | Commit Loss: 0.002461 | Perplexity: 964.356382
2025-10-01 21:58:26,622 Stage: Train 0.5 | Epoch: 153 | Iter: 115800 | Total Loss: 0.006504 | Recon Loss: 0.005285 | Commit Loss: 0.002439 | Perplexity: 961.439003
Trainning Epoch:  23%|██▎       | 154/665 [33:12:20<98:33:33, 694.35s/it]2025-10-01 22:01:34,358 Stage: Train 0.5 | Epoch: 154 | Iter: 116000 | Total Loss: 0.006506 | Recon Loss: 0.005287 | Commit Loss: 0.002438 | Perplexity: 960.056200
2025-10-01 22:04:37,049 Stage: Train 0.5 | Epoch: 154 | Iter: 116200 | Total Loss: 0.006481 | Recon Loss: 0.005260 | Commit Loss: 0.002440 | Perplexity: 961.810889
2025-10-01 22:07:38,665 Stage: Train 0.5 | Epoch: 154 | Iter: 116400 | Total Loss: 0.006503 | Recon Loss: 0.005280 | Commit Loss: 0.002444 | Perplexity: 963.127415
2025-10-01 22:10:45,002 Stage: Train 0.5 | Epoch: 154 | Iter: 116600 | Total Loss: 0.006426 | Recon Loss: 0.005223 | Commit Loss: 0.002405 | Perplexity: 958.185476
Trainning Epoch:  23%|██▎       | 155/665 [33:23:57<98:27:25, 694.99s/it]2025-10-01 22:13:55,936 Stage: Train 0.5 | Epoch: 155 | Iter: 116800 | Total Loss: 0.006464 | Recon Loss: 0.005265 | Commit Loss: 0.002399 | Perplexity: 961.707310
2025-10-01 22:17:01,386 Stage: Train 0.5 | Epoch: 155 | Iter: 117000 | Total Loss: 0.006418 | Recon Loss: 0.005217 | Commit Loss: 0.002402 | Perplexity: 957.782663
2025-10-01 22:20:06,843 Stage: Train 0.5 | Epoch: 155 | Iter: 117200 | Total Loss: 0.006551 | Recon Loss: 0.005330 | Commit Loss: 0.002443 | Perplexity: 962.538038
2025-10-01 22:23:06,590 Stage: Train 0.5 | Epoch: 155 | Iter: 117400 | Total Loss: 0.006487 | Recon Loss: 0.005260 | Commit Loss: 0.002454 | Perplexity: 963.324141
Trainning Epoch:  23%|██▎       | 156/665 [33:35:34<98:22:18, 695.75s/it]2025-10-01 22:26:18,317 Stage: Train 0.5 | Epoch: 156 | Iter: 117600 | Total Loss: 0.006593 | Recon Loss: 0.005363 | Commit Loss: 0.002460 | Perplexity: 962.150264
2025-10-01 22:29:23,426 Stage: Train 0.5 | Epoch: 156 | Iter: 117800 | Total Loss: 0.006389 | Recon Loss: 0.005188 | Commit Loss: 0.002403 | Perplexity: 960.984542
2025-10-01 22:32:30,433 Stage: Train 0.5 | Epoch: 156 | Iter: 118000 | Total Loss: 0.006524 | Recon Loss: 0.005303 | Commit Loss: 0.002442 | Perplexity: 963.831786
2025-10-01 22:35:33,689 Stage: Train 0.5 | Epoch: 156 | Iter: 118200 | Total Loss: 0.006474 | Recon Loss: 0.005275 | Commit Loss: 0.002400 | Perplexity: 957.891057
Trainning Epoch:  24%|██▎       | 157/665 [33:47:19<98:32:31, 698.33s/it]2025-10-01 22:38:40,873 Stage: Train 0.5 | Epoch: 157 | Iter: 118400 | Total Loss: 0.006426 | Recon Loss: 0.005221 | Commit Loss: 0.002410 | Perplexity: 963.612957
2025-10-01 22:41:43,477 Stage: Train 0.5 | Epoch: 157 | Iter: 118600 | Total Loss: 0.006409 | Recon Loss: 0.005204 | Commit Loss: 0.002410 | Perplexity: 955.946455
2025-10-01 22:44:46,043 Stage: Train 0.5 | Epoch: 157 | Iter: 118800 | Total Loss: 0.006499 | Recon Loss: 0.005283 | Commit Loss: 0.002432 | Perplexity: 962.468720
Trainning Epoch:  24%|██▍       | 158/665 [33:58:50<98:02:40, 696.17s/it]2025-10-01 22:47:53,695 Stage: Train 0.5 | Epoch: 158 | Iter: 119000 | Total Loss: 0.006422 | Recon Loss: 0.005197 | Commit Loss: 0.002449 | Perplexity: 959.539320
2025-10-01 22:50:55,892 Stage: Train 0.5 | Epoch: 158 | Iter: 119200 | Total Loss: 0.006462 | Recon Loss: 0.005249 | Commit Loss: 0.002425 | Perplexity: 961.276196
2025-10-01 22:53:59,299 Stage: Train 0.5 | Epoch: 158 | Iter: 119400 | Total Loss: 0.006397 | Recon Loss: 0.005181 | Commit Loss: 0.002431 | Perplexity: 963.925395
2025-10-01 22:57:03,561 Stage: Train 0.5 | Epoch: 158 | Iter: 119600 | Total Loss: 0.006479 | Recon Loss: 0.005262 | Commit Loss: 0.002435 | Perplexity: 961.273506
Trainning Epoch:  24%|██▍       | 159/665 [34:10:23<97:44:09, 695.35s/it]2025-10-01 23:00:11,295 Stage: Train 0.5 | Epoch: 159 | Iter: 119800 | Total Loss: 0.006393 | Recon Loss: 0.005179 | Commit Loss: 0.002428 | Perplexity: 960.002791
2025-10-01 23:03:15,638 Stage: Train 0.5 | Epoch: 159 | Iter: 120000 | Total Loss: 0.006501 | Recon Loss: 0.005281 | Commit Loss: 0.002441 | Perplexity: 963.284288
2025-10-01 23:03:15,638 Saving model at iteration 120000
2025-10-01 23:03:15,819 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000
2025-10-01 23:03:16,161 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000/model.safetensors
2025-10-01 23:03:16,586 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000/optimizer.bin
2025-10-01 23:03:16,587 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000/scheduler.bin
2025-10-01 23:03:16,587 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000/sampler.bin
2025-10-01 23:03:16,588 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_160_step_120000/random_states_0.pkl
2025-10-01 23:06:17,999 Stage: Train 0.5 | Epoch: 159 | Iter: 120200 | Total Loss: 0.006371 | Recon Loss: 0.005173 | Commit Loss: 0.002396 | Perplexity: 957.527271
2025-10-01 23:09:19,236 Stage: Train 0.5 | Epoch: 159 | Iter: 120400 | Total Loss: 0.006397 | Recon Loss: 0.005181 | Commit Loss: 0.002432 | Perplexity: 963.080479
Trainning Epoch:  24%|██▍       | 160/665 [34:21:57<97:29:57, 695.04s/it]2025-10-01 23:12:28,661 Stage: Train 0.5 | Epoch: 160 | Iter: 120600 | Total Loss: 0.006432 | Recon Loss: 0.005230 | Commit Loss: 0.002404 | Perplexity: 955.724584
2025-10-01 23:15:32,546 Stage: Train 0.5 | Epoch: 160 | Iter: 120800 | Total Loss: 0.006395 | Recon Loss: 0.005182 | Commit Loss: 0.002427 | Perplexity: 962.222468
2025-10-01 23:18:34,639 Stage: Train 0.5 | Epoch: 160 | Iter: 121000 | Total Loss: 0.006396 | Recon Loss: 0.005191 | Commit Loss: 0.002409 | Perplexity: 958.409516
2025-10-01 23:21:35,047 Stage: Train 0.5 | Epoch: 160 | Iter: 121200 | Total Loss: 0.006423 | Recon Loss: 0.005220 | Commit Loss: 0.002406 | Perplexity: 962.310184
Trainning Epoch:  24%|██▍       | 161/665 [34:33:33<97:19:46, 695.21s/it]2025-10-01 23:24:46,805 Stage: Train 0.5 | Epoch: 161 | Iter: 121400 | Total Loss: 0.006416 | Recon Loss: 0.005203 | Commit Loss: 0.002425 | Perplexity: 958.875607
2025-10-01 23:27:49,557 Stage: Train 0.5 | Epoch: 161 | Iter: 121600 | Total Loss: 0.006401 | Recon Loss: 0.005193 | Commit Loss: 0.002416 | Perplexity: 964.234911
2025-10-01 23:30:52,725 Stage: Train 0.5 | Epoch: 161 | Iter: 121800 | Total Loss: 0.006440 | Recon Loss: 0.005224 | Commit Loss: 0.002433 | Perplexity: 961.401913
Trainning Epoch:  24%|██▍       | 162/665 [34:45:07<97:06:11, 694.97s/it]2025-10-01 23:34:00,546 Stage: Train 0.5 | Epoch: 162 | Iter: 122000 | Total Loss: 0.006373 | Recon Loss: 0.005169 | Commit Loss: 0.002410 | Perplexity: 957.464745
2025-10-01 23:37:01,211 Stage: Train 0.5 | Epoch: 162 | Iter: 122200 | Total Loss: 0.006395 | Recon Loss: 0.005184 | Commit Loss: 0.002421 | Perplexity: 961.354836
2025-10-01 23:40:04,465 Stage: Train 0.5 | Epoch: 162 | Iter: 122400 | Total Loss: 0.006376 | Recon Loss: 0.005172 | Commit Loss: 0.002407 | Perplexity: 961.034175
2025-10-01 23:43:09,538 Stage: Train 0.5 | Epoch: 162 | Iter: 122600 | Total Loss: 0.006440 | Recon Loss: 0.005233 | Commit Loss: 0.002414 | Perplexity: 961.559313
Trainning Epoch:  25%|██▍       | 163/665 [34:56:42<96:53:30, 694.84s/it]2025-10-01 23:46:18,338 Stage: Train 0.5 | Epoch: 163 | Iter: 122800 | Total Loss: 0.006377 | Recon Loss: 0.005179 | Commit Loss: 0.002397 | Perplexity: 955.995892
2025-10-01 23:49:20,715 Stage: Train 0.5 | Epoch: 163 | Iter: 123000 | Total Loss: 0.006366 | Recon Loss: 0.005166 | Commit Loss: 0.002400 | Perplexity: 962.366798
2025-10-01 23:52:24,765 Stage: Train 0.5 | Epoch: 163 | Iter: 123200 | Total Loss: 0.006368 | Recon Loss: 0.005162 | Commit Loss: 0.002412 | Perplexity: 963.061653
2025-10-01 23:55:31,632 Stage: Train 0.5 | Epoch: 163 | Iter: 123400 | Total Loss: 0.006423 | Recon Loss: 0.005211 | Commit Loss: 0.002425 | Perplexity: 960.195181
Trainning Epoch:  25%|██▍       | 164/665 [35:08:23<96:56:08, 696.54s/it]2025-10-01 23:58:41,792 Stage: Train 0.5 | Epoch: 164 | Iter: 123600 | Total Loss: 0.006399 | Recon Loss: 0.005209 | Commit Loss: 0.002379 | Perplexity: 958.603943
2025-10-02 00:01:44,783 Stage: Train 0.5 | Epoch: 164 | Iter: 123800 | Total Loss: 0.006372 | Recon Loss: 0.005163 | Commit Loss: 0.002417 | Perplexity: 964.476172
2025-10-02 00:04:49,257 Stage: Train 0.5 | Epoch: 164 | Iter: 124000 | Total Loss: 0.006298 | Recon Loss: 0.005102 | Commit Loss: 0.002392 | Perplexity: 959.972790
2025-10-02 00:07:54,735 Stage: Train 0.5 | Epoch: 164 | Iter: 124200 | Total Loss: 0.006371 | Recon Loss: 0.005156 | Commit Loss: 0.002430 | Perplexity: 963.778978
Trainning Epoch:  25%|██▍       | 165/665 [35:20:01<96:50:06, 697.21s/it]2025-10-02 00:11:03,599 Stage: Train 0.5 | Epoch: 165 | Iter: 124400 | Total Loss: 0.006411 | Recon Loss: 0.005224 | Commit Loss: 0.002375 | Perplexity: 958.064870
2025-10-02 00:14:05,892 Stage: Train 0.5 | Epoch: 165 | Iter: 124600 | Total Loss: 0.006355 | Recon Loss: 0.005149 | Commit Loss: 0.002413 | Perplexity: 961.943467
2025-10-02 00:17:08,945 Stage: Train 0.5 | Epoch: 165 | Iter: 124800 | Total Loss: 0.006359 | Recon Loss: 0.005154 | Commit Loss: 0.002410 | Perplexity: 961.199203
Trainning Epoch:  25%|██▍       | 166/665 [35:31:36<96:31:52, 696.42s/it]2025-10-02 00:20:17,993 Stage: Train 0.5 | Epoch: 166 | Iter: 125000 | Total Loss: 0.006384 | Recon Loss: 0.005175 | Commit Loss: 0.002417 | Perplexity: 962.119668
2025-10-02 00:23:22,866 Stage: Train 0.5 | Epoch: 166 | Iter: 125200 | Total Loss: 0.006385 | Recon Loss: 0.005176 | Commit Loss: 0.002417 | Perplexity: 959.957625
2025-10-02 00:26:25,586 Stage: Train 0.5 | Epoch: 166 | Iter: 125400 | Total Loss: 0.006422 | Recon Loss: 0.005230 | Commit Loss: 0.002383 | Perplexity: 959.857664
2025-10-02 00:29:29,599 Stage: Train 0.5 | Epoch: 166 | Iter: 125600 | Total Loss: 0.006331 | Recon Loss: 0.005129 | Commit Loss: 0.002405 | Perplexity: 961.555565
Trainning Epoch:  25%|██▌       | 167/665 [35:43:12<96:18:54, 696.25s/it]2025-10-02 00:32:37,780 Stage: Train 0.5 | Epoch: 167 | Iter: 125800 | Total Loss: 0.006371 | Recon Loss: 0.005169 | Commit Loss: 0.002404 | Perplexity: 962.619449
2025-10-02 00:35:43,752 Stage: Train 0.5 | Epoch: 167 | Iter: 126000 | Total Loss: 0.006277 | Recon Loss: 0.005087 | Commit Loss: 0.002381 | Perplexity: 960.316908
2025-10-02 00:38:48,637 Stage: Train 0.5 | Epoch: 167 | Iter: 126200 | Total Loss: 0.006375 | Recon Loss: 0.005170 | Commit Loss: 0.002410 | Perplexity: 964.476520
2025-10-02 00:41:51,492 Stage: Train 0.5 | Epoch: 167 | Iter: 126400 | Total Loss: 0.006343 | Recon Loss: 0.005146 | Commit Loss: 0.002395 | Perplexity: 959.259639
Trainning Epoch:  25%|██▌       | 168/665 [35:54:51<96:15:52, 697.29s/it]2025-10-02 00:45:02,646 Stage: Train 0.5 | Epoch: 168 | Iter: 126600 | Total Loss: 0.006322 | Recon Loss: 0.005132 | Commit Loss: 0.002381 | Perplexity: 955.138283
2025-10-02 00:48:05,449 Stage: Train 0.5 | Epoch: 168 | Iter: 126800 | Total Loss: 0.006301 | Recon Loss: 0.005107 | Commit Loss: 0.002388 | Perplexity: 960.277072
2025-10-02 00:51:03,266 Stage: Train 0.5 | Epoch: 168 | Iter: 127000 | Total Loss: 0.006350 | Recon Loss: 0.005160 | Commit Loss: 0.002380 | Perplexity: 961.646078
2025-10-02 00:54:02,909 Stage: Train 0.5 | Epoch: 168 | Iter: 127200 | Total Loss: 0.006315 | Recon Loss: 0.005109 | Commit Loss: 0.002412 | Perplexity: 962.067558
Trainning Epoch:  25%|██▌       | 169/665 [36:06:18<95:37:29, 694.05s/it]2025-10-02 00:57:11,467 Stage: Train 0.5 | Epoch: 169 | Iter: 127400 | Total Loss: 0.006349 | Recon Loss: 0.005149 | Commit Loss: 0.002400 | Perplexity: 958.932116
2025-10-02 01:00:13,529 Stage: Train 0.5 | Epoch: 169 | Iter: 127600 | Total Loss: 0.006279 | Recon Loss: 0.005093 | Commit Loss: 0.002371 | Perplexity: 960.729170
2025-10-02 01:03:14,071 Stage: Train 0.5 | Epoch: 169 | Iter: 127800 | Total Loss: 0.006278 | Recon Loss: 0.005079 | Commit Loss: 0.002397 | Perplexity: 959.955746
2025-10-02 01:06:16,194 Stage: Train 0.5 | Epoch: 169 | Iter: 128000 | Total Loss: 0.006298 | Recon Loss: 0.005097 | Commit Loss: 0.002403 | Perplexity: 961.236880
Trainning Epoch:  26%|██▌       | 170/665 [36:17:51<95:24:41, 693.90s/it]2025-10-02 01:09:26,703 Stage: Train 0.5 | Epoch: 170 | Iter: 128200 | Total Loss: 0.006298 | Recon Loss: 0.005105 | Commit Loss: 0.002386 | Perplexity: 959.607942
2025-10-02 01:12:31,379 Stage: Train 0.5 | Epoch: 170 | Iter: 128400 | Total Loss: 0.006327 | Recon Loss: 0.005130 | Commit Loss: 0.002394 | Perplexity: 961.180190
2025-10-02 01:15:34,172 Stage: Train 0.5 | Epoch: 170 | Iter: 128600 | Total Loss: 0.006371 | Recon Loss: 0.005172 | Commit Loss: 0.002397 | Perplexity: 958.420937
Trainning Epoch:  26%|██▌       | 171/665 [36:29:29<95:22:53, 695.09s/it]2025-10-02 01:18:43,205 Stage: Train 0.5 | Epoch: 171 | Iter: 128800 | Total Loss: 0.006347 | Recon Loss: 0.005156 | Commit Loss: 0.002382 | Perplexity: 960.890476
2025-10-02 01:21:48,077 Stage: Train 0.5 | Epoch: 171 | Iter: 129000 | Total Loss: 0.006213 | Recon Loss: 0.005028 | Commit Loss: 0.002370 | Perplexity: 964.129695
2025-10-02 01:24:49,648 Stage: Train 0.5 | Epoch: 171 | Iter: 129200 | Total Loss: 0.006330 | Recon Loss: 0.005134 | Commit Loss: 0.002392 | Perplexity: 959.082436
2025-10-02 01:27:51,256 Stage: Train 0.5 | Epoch: 171 | Iter: 129400 | Total Loss: 0.006310 | Recon Loss: 0.005111 | Commit Loss: 0.002398 | Perplexity: 959.624052
Trainning Epoch:  26%|██▌       | 172/665 [36:41:02<95:06:20, 694.48s/it]2025-10-02 01:30:59,245 Stage: Train 0.5 | Epoch: 172 | Iter: 129600 | Total Loss: 0.006380 | Recon Loss: 0.005177 | Commit Loss: 0.002408 | Perplexity: 958.418471
2025-10-02 01:34:02,115 Stage: Train 0.5 | Epoch: 172 | Iter: 129800 | Total Loss: 0.006241 | Recon Loss: 0.005061 | Commit Loss: 0.002359 | Perplexity: 960.273566
2025-10-02 01:37:03,798 Stage: Train 0.5 | Epoch: 172 | Iter: 130000 | Total Loss: 0.006307 | Recon Loss: 0.005110 | Commit Loss: 0.002394 | Perplexity: 959.026479
2025-10-02 01:40:07,623 Stage: Train 0.5 | Epoch: 172 | Iter: 130200 | Total Loss: 0.006425 | Recon Loss: 0.005227 | Commit Loss: 0.002396 | Perplexity: 959.653500
Trainning Epoch:  26%|██▌       | 173/665 [36:52:34<94:48:09, 693.68s/it]2025-10-02 01:43:15,718 Stage: Train 0.5 | Epoch: 173 | Iter: 130400 | Total Loss: 0.006200 | Recon Loss: 0.005010 | Commit Loss: 0.002380 | Perplexity: 958.880836
2025-10-02 01:46:17,281 Stage: Train 0.5 | Epoch: 173 | Iter: 130600 | Total Loss: 0.006371 | Recon Loss: 0.005176 | Commit Loss: 0.002390 | Perplexity: 962.349145
2025-10-02 01:49:19,708 Stage: Train 0.5 | Epoch: 173 | Iter: 130800 | Total Loss: 0.006314 | Recon Loss: 0.005127 | Commit Loss: 0.002374 | Perplexity: 957.949357
2025-10-02 01:52:20,927 Stage: Train 0.5 | Epoch: 173 | Iter: 131000 | Total Loss: 0.006252 | Recon Loss: 0.005051 | Commit Loss: 0.002403 | Perplexity: 961.636715
Trainning Epoch:  26%|██▌       | 174/665 [37:04:06<94:31:54, 693.11s/it]2025-10-02 01:55:27,893 Stage: Train 0.5 | Epoch: 174 | Iter: 131200 | Total Loss: 0.006345 | Recon Loss: 0.005170 | Commit Loss: 0.002349 | Perplexity: 956.486653
2025-10-02 01:58:31,119 Stage: Train 0.5 | Epoch: 174 | Iter: 131400 | Total Loss: 0.006307 | Recon Loss: 0.005099 | Commit Loss: 0.002416 | Perplexity: 966.486029
2025-10-02 02:01:32,651 Stage: Train 0.5 | Epoch: 174 | Iter: 131600 | Total Loss: 0.006292 | Recon Loss: 0.005100 | Commit Loss: 0.002385 | Perplexity: 959.634150
Trainning Epoch:  26%|██▋       | 175/665 [37:15:36<94:13:31, 692.27s/it]2025-10-02 02:04:40,378 Stage: Train 0.5 | Epoch: 175 | Iter: 131800 | Total Loss: 0.006269 | Recon Loss: 0.005088 | Commit Loss: 0.002363 | Perplexity: 957.956864
2025-10-02 02:07:40,774 Stage: Train 0.5 | Epoch: 175 | Iter: 132000 | Total Loss: 0.006253 | Recon Loss: 0.005052 | Commit Loss: 0.002403 | Perplexity: 963.703433
2025-10-02 02:10:43,086 Stage: Train 0.5 | Epoch: 175 | Iter: 132200 | Total Loss: 0.006175 | Recon Loss: 0.004989 | Commit Loss: 0.002371 | Perplexity: 961.165514
2025-10-02 02:13:43,549 Stage: Train 0.5 | Epoch: 175 | Iter: 132400 | Total Loss: 0.006304 | Recon Loss: 0.005115 | Commit Loss: 0.002377 | Perplexity: 960.580003
Trainning Epoch:  26%|██▋       | 176/665 [37:27:04<93:51:44, 691.01s/it]2025-10-02 02:16:50,748 Stage: Train 0.5 | Epoch: 176 | Iter: 132600 | Total Loss: 0.006315 | Recon Loss: 0.005125 | Commit Loss: 0.002379 | Perplexity: 958.371299
2025-10-02 02:19:53,794 Stage: Train 0.5 | Epoch: 176 | Iter: 132800 | Total Loss: 0.006225 | Recon Loss: 0.005033 | Commit Loss: 0.002385 | Perplexity: 962.091675
2025-10-02 02:22:55,725 Stage: Train 0.5 | Epoch: 176 | Iter: 133000 | Total Loss: 0.006234 | Recon Loss: 0.005047 | Commit Loss: 0.002373 | Perplexity: 958.045557
2025-10-02 02:25:56,052 Stage: Train 0.5 | Epoch: 176 | Iter: 133200 | Total Loss: 0.006345 | Recon Loss: 0.005159 | Commit Loss: 0.002371 | Perplexity: 959.928662
Trainning Epoch:  27%|██▋       | 177/665 [37:38:35<93:38:54, 690.85s/it]2025-10-02 02:29:02,637 Stage: Train 0.5 | Epoch: 177 | Iter: 133400 | Total Loss: 0.006210 | Recon Loss: 0.005038 | Commit Loss: 0.002344 | Perplexity: 958.379720
2025-10-02 02:32:05,073 Stage: Train 0.5 | Epoch: 177 | Iter: 133600 | Total Loss: 0.006331 | Recon Loss: 0.005139 | Commit Loss: 0.002383 | Perplexity: 963.297060
2025-10-02 02:35:06,056 Stage: Train 0.5 | Epoch: 177 | Iter: 133800 | Total Loss: 0.006277 | Recon Loss: 0.005065 | Commit Loss: 0.002423 | Perplexity: 965.187426
2025-10-02 02:38:08,155 Stage: Train 0.5 | Epoch: 177 | Iter: 134000 | Total Loss: 0.006357 | Recon Loss: 0.005185 | Commit Loss: 0.002346 | Perplexity: 951.575145
Trainning Epoch:  27%|██▋       | 178/665 [37:50:06<93:28:27, 690.98s/it]2025-10-02 02:41:17,756 Stage: Train 0.5 | Epoch: 178 | Iter: 134200 | Total Loss: 0.006166 | Recon Loss: 0.004985 | Commit Loss: 0.002361 | Perplexity: 953.608708
2025-10-02 02:44:21,493 Stage: Train 0.5 | Epoch: 178 | Iter: 134400 | Total Loss: 0.006189 | Recon Loss: 0.005015 | Commit Loss: 0.002349 | Perplexity: 956.172053
2025-10-02 02:47:26,019 Stage: Train 0.5 | Epoch: 178 | Iter: 134600 | Total Loss: 0.006302 | Recon Loss: 0.005111 | Commit Loss: 0.002380 | Perplexity: 963.649171
Trainning Epoch:  27%|██▋       | 179/665 [38:01:44<93:34:16, 693.12s/it]2025-10-02 02:50:36,893 Stage: Train 0.5 | Epoch: 179 | Iter: 134800 | Total Loss: 0.006293 | Recon Loss: 0.005096 | Commit Loss: 0.002394 | Perplexity: 961.066047
2025-10-02 02:53:38,368 Stage: Train 0.5 | Epoch: 179 | Iter: 135000 | Total Loss: 0.006263 | Recon Loss: 0.005075 | Commit Loss: 0.002375 | Perplexity: 962.060789
2025-10-02 02:56:41,451 Stage: Train 0.5 | Epoch: 179 | Iter: 135200 | Total Loss: 0.006151 | Recon Loss: 0.004975 | Commit Loss: 0.002352 | Perplexity: 958.352529
2025-10-02 02:59:42,642 Stage: Train 0.5 | Epoch: 179 | Iter: 135400 | Total Loss: 0.006251 | Recon Loss: 0.005062 | Commit Loss: 0.002377 | Perplexity: 959.108731
Trainning Epoch:  27%|██▋       | 180/665 [38:13:18<93:24:31, 693.34s/it]2025-10-02 03:02:53,621 Stage: Train 0.5 | Epoch: 180 | Iter: 135600 | Total Loss: 0.006248 | Recon Loss: 0.005044 | Commit Loss: 0.002408 | Perplexity: 959.633088
2025-10-02 03:05:56,634 Stage: Train 0.5 | Epoch: 180 | Iter: 135800 | Total Loss: 0.006220 | Recon Loss: 0.005042 | Commit Loss: 0.002355 | Perplexity: 959.161430
2025-10-02 03:09:01,626 Stage: Train 0.5 | Epoch: 180 | Iter: 136000 | Total Loss: 0.006188 | Recon Loss: 0.005000 | Commit Loss: 0.002376 | Perplexity: 960.055133
2025-10-02 03:12:03,958 Stage: Train 0.5 | Epoch: 180 | Iter: 136200 | Total Loss: 0.006237 | Recon Loss: 0.005043 | Commit Loss: 0.002387 | Perplexity: 964.362715
Trainning Epoch:  27%|██▋       | 181/665 [38:24:57<93:25:18, 694.87s/it]2025-10-02 03:15:17,135 Stage: Train 0.5 | Epoch: 181 | Iter: 136400 | Total Loss: 0.006273 | Recon Loss: 0.005094 | Commit Loss: 0.002357 | Perplexity: 958.100721
2025-10-02 03:18:17,831 Stage: Train 0.5 | Epoch: 181 | Iter: 136600 | Total Loss: 0.006231 | Recon Loss: 0.005045 | Commit Loss: 0.002372 | Perplexity: 962.813609
2025-10-02 03:21:20,004 Stage: Train 0.5 | Epoch: 181 | Iter: 136800 | Total Loss: 0.006304 | Recon Loss: 0.005126 | Commit Loss: 0.002356 | Perplexity: 958.176528
2025-10-02 03:24:22,935 Stage: Train 0.5 | Epoch: 181 | Iter: 137000 | Total Loss: 0.006214 | Recon Loss: 0.005031 | Commit Loss: 0.002367 | Perplexity: 959.812546
Trainning Epoch:  27%|██▋       | 182/665 [38:36:30<93:10:14, 694.44s/it]2025-10-02 03:27:34,155 Stage: Train 0.5 | Epoch: 182 | Iter: 137200 | Total Loss: 0.006202 | Recon Loss: 0.005017 | Commit Loss: 0.002371 | Perplexity: 962.555684
2025-10-02 03:30:37,408 Stage: Train 0.5 | Epoch: 182 | Iter: 137400 | Total Loss: 0.006231 | Recon Loss: 0.005054 | Commit Loss: 0.002354 | Perplexity: 958.207069
2025-10-02 03:33:40,439 Stage: Train 0.5 | Epoch: 182 | Iter: 137600 | Total Loss: 0.006179 | Recon Loss: 0.005003 | Commit Loss: 0.002353 | Perplexity: 958.705759
Trainning Epoch:  28%|██▊       | 183/665 [38:48:06<93:03:15, 695.01s/it]2025-10-02 03:36:47,810 Stage: Train 0.5 | Epoch: 183 | Iter: 137800 | Total Loss: 0.006245 | Recon Loss: 0.005054 | Commit Loss: 0.002382 | Perplexity: 957.589977
2025-10-02 03:39:49,212 Stage: Train 0.5 | Epoch: 183 | Iter: 138000 | Total Loss: 0.006285 | Recon Loss: 0.005097 | Commit Loss: 0.002375 | Perplexity: 961.277365
2025-10-02 03:42:51,777 Stage: Train 0.5 | Epoch: 183 | Iter: 138200 | Total Loss: 0.006216 | Recon Loss: 0.005041 | Commit Loss: 0.002349 | Perplexity: 958.540055
2025-10-02 03:45:54,038 Stage: Train 0.5 | Epoch: 183 | Iter: 138400 | Total Loss: 0.006200 | Recon Loss: 0.005016 | Commit Loss: 0.002367 | Perplexity: 960.071757
Trainning Epoch:  28%|██▊       | 184/665 [38:59:36<92:39:43, 693.52s/it]2025-10-02 03:49:01,834 Stage: Train 0.5 | Epoch: 184 | Iter: 138600 | Total Loss: 0.006155 | Recon Loss: 0.004979 | Commit Loss: 0.002351 | Perplexity: 955.367193
2025-10-02 03:52:03,849 Stage: Train 0.5 | Epoch: 184 | Iter: 138800 | Total Loss: 0.006268 | Recon Loss: 0.005089 | Commit Loss: 0.002358 | Perplexity: 957.863734
2025-10-02 03:55:06,296 Stage: Train 0.5 | Epoch: 184 | Iter: 139000 | Total Loss: 0.006203 | Recon Loss: 0.005014 | Commit Loss: 0.002378 | Perplexity: 961.452544
2025-10-02 03:58:07,415 Stage: Train 0.5 | Epoch: 184 | Iter: 139200 | Total Loss: 0.006187 | Recon Loss: 0.004998 | Commit Loss: 0.002378 | Perplexity: 959.374778
Trainning Epoch:  28%|██▊       | 185/665 [39:11:10<92:27:42, 693.46s/it]2025-10-02 04:01:15,384 Stage: Train 0.5 | Epoch: 185 | Iter: 139400 | Total Loss: 0.006215 | Recon Loss: 0.005032 | Commit Loss: 0.002365 | Perplexity: 957.423503
2025-10-02 04:04:17,819 Stage: Train 0.5 | Epoch: 185 | Iter: 139600 | Total Loss: 0.006253 | Recon Loss: 0.005080 | Commit Loss: 0.002345 | Perplexity: 960.241990
2025-10-02 04:07:22,990 Stage: Train 0.5 | Epoch: 185 | Iter: 139800 | Total Loss: 0.006159 | Recon Loss: 0.004978 | Commit Loss: 0.002362 | Perplexity: 964.479586
2025-10-02 04:10:24,656 Stage: Train 0.5 | Epoch: 185 | Iter: 140000 | Total Loss: 0.006232 | Recon Loss: 0.005059 | Commit Loss: 0.002347 | Perplexity: 955.401571
2025-10-02 04:10:24,657 Saving model at iteration 140000
2025-10-02 04:10:24,889 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000
2025-10-02 04:10:25,193 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000/model.safetensors
2025-10-02 04:10:25,599 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000/optimizer.bin
2025-10-02 04:10:25,599 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000/scheduler.bin
2025-10-02 04:10:25,600 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000/sampler.bin
2025-10-02 04:10:25,600 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_186_step_140000/random_states_0.pkl
Trainning Epoch:  28%|██▊       | 186/665 [39:22:43<92:15:47, 693.42s/it]2025-10-02 04:13:34,069 Stage: Train 0.5 | Epoch: 186 | Iter: 140200 | Total Loss: 0.006195 | Recon Loss: 0.005017 | Commit Loss: 0.002356 | Perplexity: 956.741164
2025-10-02 04:16:37,368 Stage: Train 0.5 | Epoch: 186 | Iter: 140400 | Total Loss: 0.006167 | Recon Loss: 0.004980 | Commit Loss: 0.002374 | Perplexity: 964.967385
2025-10-02 04:19:41,334 Stage: Train 0.5 | Epoch: 186 | Iter: 140600 | Total Loss: 0.006242 | Recon Loss: 0.005060 | Commit Loss: 0.002363 | Perplexity: 959.931847
2025-10-02 04:22:42,945 Stage: Train 0.5 | Epoch: 186 | Iter: 140800 | Total Loss: 0.006158 | Recon Loss: 0.004991 | Commit Loss: 0.002334 | Perplexity: 954.946627
Trainning Epoch:  28%|██▊       | 187/665 [39:34:19<92:10:10, 694.16s/it]2025-10-02 04:25:54,790 Stage: Train 0.5 | Epoch: 187 | Iter: 141000 | Total Loss: 0.006185 | Recon Loss: 0.005002 | Commit Loss: 0.002366 | Perplexity: 960.092254
2025-10-02 04:28:55,264 Stage: Train 0.5 | Epoch: 187 | Iter: 141200 | Total Loss: 0.006161 | Recon Loss: 0.004990 | Commit Loss: 0.002341 | Perplexity: 956.521657
2025-10-02 04:31:56,427 Stage: Train 0.5 | Epoch: 187 | Iter: 141400 | Total Loss: 0.006078 | Recon Loss: 0.004896 | Commit Loss: 0.002364 | Perplexity: 963.361342
Trainning Epoch:  28%|██▊       | 188/665 [39:45:52<91:55:41, 693.80s/it]2025-10-02 04:35:05,697 Stage: Train 0.5 | Epoch: 188 | Iter: 141600 | Total Loss: 0.006288 | Recon Loss: 0.005114 | Commit Loss: 0.002348 | Perplexity: 959.027994
2025-10-02 04:38:06,247 Stage: Train 0.5 | Epoch: 188 | Iter: 141800 | Total Loss: 0.006122 | Recon Loss: 0.004945 | Commit Loss: 0.002353 | Perplexity: 964.791622
2025-10-02 04:41:09,975 Stage: Train 0.5 | Epoch: 188 | Iter: 142000 | Total Loss: 0.006113 | Recon Loss: 0.004934 | Commit Loss: 0.002356 | Perplexity: 960.873061
2025-10-02 04:44:14,665 Stage: Train 0.5 | Epoch: 188 | Iter: 142200 | Total Loss: 0.006197 | Recon Loss: 0.004995 | Commit Loss: 0.002403 | Perplexity: 960.261332
Trainning Epoch:  28%|██▊       | 189/665 [39:57:27<91:47:19, 694.20s/it]2025-10-02 04:47:24,543 Stage: Train 0.5 | Epoch: 189 | Iter: 142400 | Total Loss: 0.006142 | Recon Loss: 0.004975 | Commit Loss: 0.002334 | Perplexity: 956.014786
2025-10-02 04:50:28,410 Stage: Train 0.5 | Epoch: 189 | Iter: 142600 | Total Loss: 0.006130 | Recon Loss: 0.004949 | Commit Loss: 0.002361 | Perplexity: 960.637590
2025-10-02 04:53:29,389 Stage: Train 0.5 | Epoch: 189 | Iter: 142800 | Total Loss: 0.006204 | Recon Loss: 0.005022 | Commit Loss: 0.002364 | Perplexity: 960.038975
2025-10-02 04:56:30,838 Stage: Train 0.5 | Epoch: 189 | Iter: 143000 | Total Loss: 0.006115 | Recon Loss: 0.004938 | Commit Loss: 0.002355 | Perplexity: 959.083622
Trainning Epoch:  29%|██▊       | 190/665 [40:09:00<91:32:37, 693.81s/it]2025-10-02 04:59:36,751 Stage: Train 0.5 | Epoch: 190 | Iter: 143200 | Total Loss: 0.006166 | Recon Loss: 0.004999 | Commit Loss: 0.002334 | Perplexity: 955.613868
2025-10-02 05:02:38,120 Stage: Train 0.5 | Epoch: 190 | Iter: 143400 | Total Loss: 0.006204 | Recon Loss: 0.005033 | Commit Loss: 0.002342 | Perplexity: 961.404021
2025-10-02 05:05:40,246 Stage: Train 0.5 | Epoch: 190 | Iter: 143600 | Total Loss: 0.006142 | Recon Loss: 0.004966 | Commit Loss: 0.002351 | Perplexity: 961.119556
2025-10-02 05:08:44,493 Stage: Train 0.5 | Epoch: 190 | Iter: 143800 | Total Loss: 0.006210 | Recon Loss: 0.005039 | Commit Loss: 0.002342 | Perplexity: 957.896965
Trainning Epoch:  29%|██▊       | 191/665 [40:20:30<91:13:09, 692.81s/it]2025-10-02 05:11:52,779 Stage: Train 0.5 | Epoch: 191 | Iter: 144000 | Total Loss: 0.006087 | Recon Loss: 0.004908 | Commit Loss: 0.002359 | Perplexity: 960.310878
2025-10-02 05:14:53,658 Stage: Train 0.5 | Epoch: 191 | Iter: 144200 | Total Loss: 0.006126 | Recon Loss: 0.004965 | Commit Loss: 0.002322 | Perplexity: 958.364354
2025-10-02 05:17:54,834 Stage: Train 0.5 | Epoch: 191 | Iter: 144400 | Total Loss: 0.006228 | Recon Loss: 0.005046 | Commit Loss: 0.002364 | Perplexity: 960.793656
Trainning Epoch:  29%|██▉       | 192/665 [40:32:04<91:02:29, 692.92s/it]2025-10-02 05:21:06,550 Stage: Train 0.5 | Epoch: 192 | Iter: 144600 | Total Loss: 0.006141 | Recon Loss: 0.004970 | Commit Loss: 0.002341 | Perplexity: 956.640992
2025-10-02 05:24:09,028 Stage: Train 0.5 | Epoch: 192 | Iter: 144800 | Total Loss: 0.006101 | Recon Loss: 0.004919 | Commit Loss: 0.002364 | Perplexity: 964.408761
2025-10-02 05:27:12,234 Stage: Train 0.5 | Epoch: 192 | Iter: 145000 | Total Loss: 0.006112 | Recon Loss: 0.004939 | Commit Loss: 0.002345 | Perplexity: 954.846623
2025-10-02 05:30:14,228 Stage: Train 0.5 | Epoch: 192 | Iter: 145200 | Total Loss: 0.006175 | Recon Loss: 0.004997 | Commit Loss: 0.002355 | Perplexity: 961.656890
Trainning Epoch:  29%|██▉       | 193/665 [40:43:34<90:45:27, 692.22s/it]2025-10-02 05:33:18,333 Stage: Train 0.5 | Epoch: 193 | Iter: 145400 | Total Loss: 0.006139 | Recon Loss: 0.004976 | Commit Loss: 0.002325 | Perplexity: 957.431202
2025-10-02 05:36:21,306 Stage: Train 0.5 | Epoch: 193 | Iter: 145600 | Total Loss: 0.006134 | Recon Loss: 0.004962 | Commit Loss: 0.002345 | Perplexity: 959.732250
2025-10-02 05:39:24,498 Stage: Train 0.5 | Epoch: 193 | Iter: 145800 | Total Loss: 0.006164 | Recon Loss: 0.004994 | Commit Loss: 0.002340 | Perplexity: 956.623945
2025-10-02 05:42:27,667 Stage: Train 0.5 | Epoch: 193 | Iter: 146000 | Total Loss: 0.006141 | Recon Loss: 0.004972 | Commit Loss: 0.002338 | Perplexity: 959.399850
Trainning Epoch:  29%|██▉       | 194/665 [40:55:08<90:37:03, 692.62s/it]2025-10-02 05:45:37,446 Stage: Train 0.5 | Epoch: 194 | Iter: 146200 | Total Loss: 0.006167 | Recon Loss: 0.004985 | Commit Loss: 0.002363 | Perplexity: 960.052739
2025-10-02 05:48:40,692 Stage: Train 0.5 | Epoch: 194 | Iter: 146400 | Total Loss: 0.006080 | Recon Loss: 0.004898 | Commit Loss: 0.002366 | Perplexity: 960.938020
2025-10-02 05:51:43,382 Stage: Train 0.5 | Epoch: 194 | Iter: 146600 | Total Loss: 0.006178 | Recon Loss: 0.005028 | Commit Loss: 0.002300 | Perplexity: 956.282254
2025-10-02 05:54:44,343 Stage: Train 0.5 | Epoch: 194 | Iter: 146800 | Total Loss: 0.006131 | Recon Loss: 0.004955 | Commit Loss: 0.002353 | Perplexity: 960.953997
Trainning Epoch:  29%|██▉       | 195/665 [41:06:43<90:32:40, 693.53s/it]2025-10-02 05:57:56,710 Stage: Train 0.5 | Epoch: 195 | Iter: 147000 | Total Loss: 0.006167 | Recon Loss: 0.004994 | Commit Loss: 0.002346 | Perplexity: 959.849517
2025-10-02 06:00:59,600 Stage: Train 0.5 | Epoch: 195 | Iter: 147200 | Total Loss: 0.006192 | Recon Loss: 0.005028 | Commit Loss: 0.002327 | Perplexity: 957.601439
2025-10-02 06:04:01,667 Stage: Train 0.5 | Epoch: 195 | Iter: 147400 | Total Loss: 0.006131 | Recon Loss: 0.004952 | Commit Loss: 0.002357 | Perplexity: 961.199148
Trainning Epoch:  29%|██▉       | 196/665 [41:18:19<90:24:58, 694.03s/it]2025-10-02 06:07:09,834 Stage: Train 0.5 | Epoch: 196 | Iter: 147600 | Total Loss: 0.006101 | Recon Loss: 0.004934 | Commit Loss: 0.002333 | Perplexity: 956.242114
2025-10-02 06:10:14,991 Stage: Train 0.5 | Epoch: 196 | Iter: 147800 | Total Loss: 0.006079 | Recon Loss: 0.004911 | Commit Loss: 0.002337 | Perplexity: 959.972600
2025-10-02 06:13:24,432 Stage: Train 0.5 | Epoch: 196 | Iter: 148000 | Total Loss: 0.006102 | Recon Loss: 0.004925 | Commit Loss: 0.002354 | Perplexity: 962.355702
2025-10-02 06:16:32,271 Stage: Train 0.5 | Epoch: 196 | Iter: 148200 | Total Loss: 0.006114 | Recon Loss: 0.004952 | Commit Loss: 0.002325 | Perplexity: 958.041434
Trainning Epoch:  30%|██▉       | 197/665 [41:30:11<90:55:45, 699.46s/it]2025-10-02 06:19:44,693 Stage: Train 0.5 | Epoch: 197 | Iter: 148400 | Total Loss: 0.006162 | Recon Loss: 0.004986 | Commit Loss: 0.002351 | Perplexity: 958.201531
2025-10-02 06:22:47,091 Stage: Train 0.5 | Epoch: 197 | Iter: 148600 | Total Loss: 0.006111 | Recon Loss: 0.004939 | Commit Loss: 0.002345 | Perplexity: 959.581542
2025-10-02 06:25:47,090 Stage: Train 0.5 | Epoch: 197 | Iter: 148800 | Total Loss: 0.006019 | Recon Loss: 0.004857 | Commit Loss: 0.002325 | Perplexity: 958.906491
2025-10-02 06:28:52,396 Stage: Train 0.5 | Epoch: 197 | Iter: 149000 | Total Loss: 0.006261 | Recon Loss: 0.005081 | Commit Loss: 0.002359 | Perplexity: 960.002127
Trainning Epoch:  30%|██▉       | 198/665 [41:41:45<90:32:27, 697.96s/it]2025-10-02 06:32:06,093 Stage: Train 0.5 | Epoch: 198 | Iter: 149200 | Total Loss: 0.006085 | Recon Loss: 0.004915 | Commit Loss: 0.002341 | Perplexity: 960.673818
2025-10-02 06:35:09,807 Stage: Train 0.5 | Epoch: 198 | Iter: 149400 | Total Loss: 0.005944 | Recon Loss: 0.004778 | Commit Loss: 0.002331 | Perplexity: 960.063393
2025-10-02 06:38:14,777 Stage: Train 0.5 | Epoch: 198 | Iter: 149600 | Total Loss: 0.006145 | Recon Loss: 0.004973 | Commit Loss: 0.002344 | Perplexity: 959.704503
2025-10-02 06:41:17,425 Stage: Train 0.5 | Epoch: 198 | Iter: 149800 | Total Loss: 0.006171 | Recon Loss: 0.005001 | Commit Loss: 0.002340 | Perplexity: 959.114864
Trainning Epoch:  30%|██▉       | 199/665 [41:53:27<90:29:13, 699.04s/it]2025-10-02 06:44:27,642 Stage: Train 0.5 | Epoch: 199 | Iter: 150000 | Total Loss: 0.006068 | Recon Loss: 0.004897 | Commit Loss: 0.002343 | Perplexity: 957.277179
2025-10-02 06:47:32,198 Stage: Train 0.5 | Epoch: 199 | Iter: 150200 | Total Loss: 0.006071 | Recon Loss: 0.004902 | Commit Loss: 0.002339 | Perplexity: 960.366882
2025-10-02 06:50:37,206 Stage: Train 0.5 | Epoch: 199 | Iter: 150400 | Total Loss: 0.006094 | Recon Loss: 0.004919 | Commit Loss: 0.002349 | Perplexity: 960.335134
2025-10-02 06:53:41,191 Stage: Train 0.5 | Epoch: 199 | Iter: 150600 | Total Loss: 0.006071 | Recon Loss: 0.004905 | Commit Loss: 0.002333 | Perplexity: 959.512643
Trainning Epoch:  30%|███       | 200/665 [42:05:09<90:24:48, 699.97s/it]2025-10-02 06:56:56,284 Stage: Train 0.5 | Epoch: 200 | Iter: 150800 | Total Loss: 0.006009 | Recon Loss: 0.004845 | Commit Loss: 0.002327 | Perplexity: 961.263929
2025-10-02 07:00:06,953 Stage: Train 0.5 | Epoch: 200 | Iter: 151000 | Total Loss: 0.006102 | Recon Loss: 0.004926 | Commit Loss: 0.002351 | Perplexity: 957.591237
2025-10-02 07:03:15,682 Stage: Train 0.5 | Epoch: 200 | Iter: 151200 | Total Loss: 0.006117 | Recon Loss: 0.004956 | Commit Loss: 0.002323 | Perplexity: 958.134243
Trainning Epoch:  30%|███       | 201/665 [42:17:04<90:47:15, 704.39s/it]2025-10-02 07:06:27,441 Stage: Train 0.5 | Epoch: 201 | Iter: 151400 | Total Loss: 0.006082 | Recon Loss: 0.004909 | Commit Loss: 0.002347 | Perplexity: 958.068870
2025-10-02 07:09:35,325 Stage: Train 0.5 | Epoch: 201 | Iter: 151600 | Total Loss: 0.006093 | Recon Loss: 0.004927 | Commit Loss: 0.002331 | Perplexity: 959.173390
2025-10-02 07:12:35,712 Stage: Train 0.5 | Epoch: 201 | Iter: 151800 | Total Loss: 0.006116 | Recon Loss: 0.004951 | Commit Loss: 0.002331 | Perplexity: 958.364086
2025-10-02 07:15:38,351 Stage: Train 0.5 | Epoch: 201 | Iter: 152000 | Total Loss: 0.006028 | Recon Loss: 0.004855 | Commit Loss: 0.002346 | Perplexity: 959.280508
Trainning Epoch:  30%|███       | 202/665 [42:28:40<90:18:03, 702.12s/it]2025-10-02 07:18:46,151 Stage: Train 0.5 | Epoch: 202 | Iter: 152200 | Total Loss: 0.006056 | Recon Loss: 0.004903 | Commit Loss: 0.002305 | Perplexity: 953.276636
2025-10-02 07:21:54,101 Stage: Train 0.5 | Epoch: 202 | Iter: 152400 | Total Loss: 0.006161 | Recon Loss: 0.004994 | Commit Loss: 0.002334 | Perplexity: 962.392555
2025-10-02 07:24:57,101 Stage: Train 0.5 | Epoch: 202 | Iter: 152600 | Total Loss: 0.006091 | Recon Loss: 0.004918 | Commit Loss: 0.002346 | Perplexity: 958.768110
2025-10-02 07:28:01,860 Stage: Train 0.5 | Epoch: 202 | Iter: 152800 | Total Loss: 0.006019 | Recon Loss: 0.004862 | Commit Loss: 0.002315 | Perplexity: 958.106709
Trainning Epoch:  31%|███       | 203/665 [42:40:23<90:06:27, 702.14s/it]2025-10-02 07:31:13,153 Stage: Train 0.5 | Epoch: 203 | Iter: 153000 | Total Loss: 0.006090 | Recon Loss: 0.004921 | Commit Loss: 0.002337 | Perplexity: 957.229902
2025-10-02 07:34:12,402 Stage: Train 0.5 | Epoch: 203 | Iter: 153200 | Total Loss: 0.006046 | Recon Loss: 0.004890 | Commit Loss: 0.002312 | Perplexity: 956.489710
2025-10-02 07:37:13,612 Stage: Train 0.5 | Epoch: 203 | Iter: 153400 | Total Loss: 0.006069 | Recon Loss: 0.004904 | Commit Loss: 0.002332 | Perplexity: 959.352244
2025-10-02 07:40:15,658 Stage: Train 0.5 | Epoch: 203 | Iter: 153600 | Total Loss: 0.006182 | Recon Loss: 0.005017 | Commit Loss: 0.002331 | Perplexity: 955.159732
Trainning Epoch:  31%|███       | 204/665 [42:51:52<89:26:02, 698.40s/it]2025-10-02 07:43:25,798 Stage: Train 0.5 | Epoch: 204 | Iter: 153800 | Total Loss: 0.006057 | Recon Loss: 0.004895 | Commit Loss: 0.002324 | Perplexity: 957.285613
2025-10-02 07:46:27,206 Stage: Train 0.5 | Epoch: 204 | Iter: 154000 | Total Loss: 0.006067 | Recon Loss: 0.004903 | Commit Loss: 0.002328 | Perplexity: 959.110659
2025-10-02 07:49:31,592 Stage: Train 0.5 | Epoch: 204 | Iter: 154200 | Total Loss: 0.005988 | Recon Loss: 0.004832 | Commit Loss: 0.002311 | Perplexity: 955.590544
Trainning Epoch:  31%|███       | 205/665 [43:03:27<89:06:07, 697.32s/it]2025-10-02 07:52:39,566 Stage: Train 0.5 | Epoch: 205 | Iter: 154400 | Total Loss: 0.006099 | Recon Loss: 0.004918 | Commit Loss: 0.002363 | Perplexity: 959.631289
2025-10-02 07:55:43,542 Stage: Train 0.5 | Epoch: 205 | Iter: 154600 | Total Loss: 0.005985 | Recon Loss: 0.004819 | Commit Loss: 0.002334 | Perplexity: 962.238822
2025-10-02 07:58:45,551 Stage: Train 0.5 | Epoch: 205 | Iter: 154800 | Total Loss: 0.006048 | Recon Loss: 0.004871 | Commit Loss: 0.002353 | Perplexity: 960.546416
2025-10-02 08:01:51,421 Stage: Train 0.5 | Epoch: 205 | Iter: 155000 | Total Loss: 0.005989 | Recon Loss: 0.004828 | Commit Loss: 0.002322 | Perplexity: 957.534451
Trainning Epoch:  31%|███       | 206/665 [43:15:03<88:51:13, 696.89s/it]2025-10-02 08:05:00,341 Stage: Train 0.5 | Epoch: 206 | Iter: 155200 | Total Loss: 0.006132 | Recon Loss: 0.004969 | Commit Loss: 0.002328 | Perplexity: 956.242021
2025-10-02 08:08:04,118 Stage: Train 0.5 | Epoch: 206 | Iter: 155400 | Total Loss: 0.005980 | Recon Loss: 0.004817 | Commit Loss: 0.002325 | Perplexity: 958.806873
2025-10-02 08:11:05,254 Stage: Train 0.5 | Epoch: 206 | Iter: 155600 | Total Loss: 0.006023 | Recon Loss: 0.004853 | Commit Loss: 0.002339 | Perplexity: 957.755505
2025-10-02 08:14:04,939 Stage: Train 0.5 | Epoch: 206 | Iter: 155800 | Total Loss: 0.006092 | Recon Loss: 0.004929 | Commit Loss: 0.002328 | Perplexity: 958.232734
Trainning Epoch:  31%|███       | 207/665 [43:26:34<88:25:25, 695.03s/it]2025-10-02 08:17:12,891 Stage: Train 0.5 | Epoch: 207 | Iter: 156000 | Total Loss: 0.006062 | Recon Loss: 0.004904 | Commit Loss: 0.002315 | Perplexity: 955.934726
2025-10-02 08:20:13,318 Stage: Train 0.5 | Epoch: 207 | Iter: 156200 | Total Loss: 0.005964 | Recon Loss: 0.004801 | Commit Loss: 0.002326 | Perplexity: 957.711620
2025-10-02 08:23:15,538 Stage: Train 0.5 | Epoch: 207 | Iter: 156400 | Total Loss: 0.006047 | Recon Loss: 0.004886 | Commit Loss: 0.002323 | Perplexity: 955.734869
2025-10-02 08:26:16,971 Stage: Train 0.5 | Epoch: 207 | Iter: 156600 | Total Loss: 0.006045 | Recon Loss: 0.004867 | Commit Loss: 0.002355 | Perplexity: 959.864290
Trainning Epoch:  31%|███▏      | 208/665 [43:38:05<88:04:35, 693.82s/it]2025-10-02 08:29:25,568 Stage: Train 0.5 | Epoch: 208 | Iter: 156800 | Total Loss: 0.006022 | Recon Loss: 0.004866 | Commit Loss: 0.002313 | Perplexity: 956.676108
2025-10-02 08:32:28,017 Stage: Train 0.5 | Epoch: 208 | Iter: 157000 | Total Loss: 0.005989 | Recon Loss: 0.004827 | Commit Loss: 0.002322 | Perplexity: 959.995126
2025-10-02 08:35:33,202 Stage: Train 0.5 | Epoch: 208 | Iter: 157200 | Total Loss: 0.006058 | Recon Loss: 0.004884 | Commit Loss: 0.002347 | Perplexity: 959.938970
Trainning Epoch:  31%|███▏      | 209/665 [43:49:38<87:52:46, 693.79s/it]2025-10-02 08:38:39,244 Stage: Train 0.5 | Epoch: 209 | Iter: 157400 | Total Loss: 0.006073 | Recon Loss: 0.004907 | Commit Loss: 0.002332 | Perplexity: 957.865252
2025-10-02 08:41:42,113 Stage: Train 0.5 | Epoch: 209 | Iter: 157600 | Total Loss: 0.006033 | Recon Loss: 0.004877 | Commit Loss: 0.002313 | Perplexity: 957.750843
2025-10-02 08:44:43,657 Stage: Train 0.5 | Epoch: 209 | Iter: 157800 | Total Loss: 0.006033 | Recon Loss: 0.004862 | Commit Loss: 0.002342 | Perplexity: 959.205693
2025-10-02 08:47:44,199 Stage: Train 0.5 | Epoch: 209 | Iter: 158000 | Total Loss: 0.006030 | Recon Loss: 0.004869 | Commit Loss: 0.002321 | Perplexity: 959.789333
Trainning Epoch:  32%|███▏      | 210/665 [44:01:08<87:30:54, 692.43s/it]2025-10-02 08:50:51,041 Stage: Train 0.5 | Epoch: 210 | Iter: 158200 | Total Loss: 0.006056 | Recon Loss: 0.004887 | Commit Loss: 0.002338 | Perplexity: 959.428449
2025-10-02 08:53:55,004 Stage: Train 0.5 | Epoch: 210 | Iter: 158400 | Total Loss: 0.006040 | Recon Loss: 0.004882 | Commit Loss: 0.002316 | Perplexity: 959.512921
2025-10-02 08:56:58,101 Stage: Train 0.5 | Epoch: 210 | Iter: 158600 | Total Loss: 0.005969 | Recon Loss: 0.004811 | Commit Loss: 0.002317 | Perplexity: 955.887264
2025-10-02 08:59:59,441 Stage: Train 0.5 | Epoch: 210 | Iter: 158800 | Total Loss: 0.006146 | Recon Loss: 0.004980 | Commit Loss: 0.002333 | Perplexity: 959.712968
Trainning Epoch:  32%|███▏      | 211/665 [44:12:40<87:19:36, 692.46s/it]2025-10-02 09:03:05,320 Stage: Train 0.5 | Epoch: 211 | Iter: 159000 | Total Loss: 0.006077 | Recon Loss: 0.004910 | Commit Loss: 0.002334 | Perplexity: 959.960997
2025-10-02 09:06:08,073 Stage: Train 0.5 | Epoch: 211 | Iter: 159200 | Total Loss: 0.006059 | Recon Loss: 0.004889 | Commit Loss: 0.002340 | Perplexity: 959.581913
2025-10-02 09:09:10,101 Stage: Train 0.5 | Epoch: 211 | Iter: 159400 | Total Loss: 0.005993 | Recon Loss: 0.004820 | Commit Loss: 0.002346 | Perplexity: 959.623645
2025-10-02 09:12:11,799 Stage: Train 0.5 | Epoch: 211 | Iter: 159600 | Total Loss: 0.006105 | Recon Loss: 0.004940 | Commit Loss: 0.002331 | Perplexity: 956.595600
Trainning Epoch:  32%|███▏      | 212/665 [44:24:11<87:03:28, 691.85s/it]2025-10-02 09:15:19,980 Stage: Train 0.5 | Epoch: 212 | Iter: 159800 | Total Loss: 0.005968 | Recon Loss: 0.004809 | Commit Loss: 0.002317 | Perplexity: 955.840286
2025-10-02 09:18:23,965 Stage: Train 0.5 | Epoch: 212 | Iter: 160000 | Total Loss: 0.006081 | Recon Loss: 0.004906 | Commit Loss: 0.002349 | Perplexity: 961.676006
2025-10-02 09:18:23,966 Saving model at iteration 160000
2025-10-02 09:18:24,349 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000
2025-10-02 09:18:24,653 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000/model.safetensors
2025-10-02 09:18:25,052 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000/optimizer.bin
2025-10-02 09:18:25,053 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000/scheduler.bin
2025-10-02 09:18:25,053 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000/sampler.bin
2025-10-02 09:18:25,054 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_213_step_160000/random_states_0.pkl
2025-10-02 09:21:25,411 Stage: Train 0.5 | Epoch: 212 | Iter: 160200 | Total Loss: 0.006057 | Recon Loss: 0.004896 | Commit Loss: 0.002322 | Perplexity: 958.408924
Trainning Epoch:  32%|███▏      | 213/665 [44:35:43<86:54:19, 692.17s/it]2025-10-02 09:24:34,010 Stage: Train 0.5 | Epoch: 213 | Iter: 160400 | Total Loss: 0.005987 | Recon Loss: 0.004825 | Commit Loss: 0.002325 | Perplexity: 958.213512
2025-10-02 09:27:37,970 Stage: Train 0.5 | Epoch: 213 | Iter: 160600 | Total Loss: 0.006021 | Recon Loss: 0.004857 | Commit Loss: 0.002327 | Perplexity: 957.519417
2025-10-02 09:30:42,978 Stage: Train 0.5 | Epoch: 213 | Iter: 160800 | Total Loss: 0.005971 | Recon Loss: 0.004801 | Commit Loss: 0.002339 | Perplexity: 958.472317
2025-10-02 09:33:42,687 Stage: Train 0.5 | Epoch: 213 | Iter: 161000 | Total Loss: 0.006052 | Recon Loss: 0.004888 | Commit Loss: 0.002328 | Perplexity: 960.166344
Trainning Epoch:  32%|███▏      | 214/665 [44:47:17<86:46:48, 692.70s/it]2025-10-02 09:36:53,810 Stage: Train 0.5 | Epoch: 214 | Iter: 161200 | Total Loss: 0.006013 | Recon Loss: 0.004850 | Commit Loss: 0.002325 | Perplexity: 957.138886
2025-10-02 09:39:55,927 Stage: Train 0.5 | Epoch: 214 | Iter: 161400 | Total Loss: 0.006031 | Recon Loss: 0.004861 | Commit Loss: 0.002341 | Perplexity: 959.644224
2025-10-02 09:42:54,898 Stage: Train 0.5 | Epoch: 214 | Iter: 161600 | Total Loss: 0.006074 | Recon Loss: 0.004899 | Commit Loss: 0.002350 | Perplexity: 959.031001
2025-10-02 09:45:58,043 Stage: Train 0.5 | Epoch: 214 | Iter: 161800 | Total Loss: 0.006063 | Recon Loss: 0.004898 | Commit Loss: 0.002330 | Perplexity: 958.710977
Trainning Epoch:  32%|███▏      | 215/665 [44:58:50<86:35:15, 692.70s/it]2025-10-02 09:49:08,316 Stage: Train 0.5 | Epoch: 215 | Iter: 162000 | Total Loss: 0.006068 | Recon Loss: 0.004913 | Commit Loss: 0.002311 | Perplexity: 955.391250
2025-10-02 09:52:11,088 Stage: Train 0.5 | Epoch: 215 | Iter: 162200 | Total Loss: 0.006054 | Recon Loss: 0.004889 | Commit Loss: 0.002329 | Perplexity: 953.668853
2025-10-02 09:55:15,167 Stage: Train 0.5 | Epoch: 215 | Iter: 162400 | Total Loss: 0.006027 | Recon Loss: 0.004848 | Commit Loss: 0.002358 | Perplexity: 962.800119
2025-10-02 09:58:18,458 Stage: Train 0.5 | Epoch: 215 | Iter: 162600 | Total Loss: 0.006070 | Recon Loss: 0.004889 | Commit Loss: 0.002361 | Perplexity: 960.721826
Trainning Epoch:  32%|███▏      | 216/665 [45:10:27<86:33:33, 694.02s/it]2025-10-02 10:01:25,159 Stage: Train 0.5 | Epoch: 216 | Iter: 162800 | Total Loss: 0.006129 | Recon Loss: 0.004936 | Commit Loss: 0.002386 | Perplexity: 955.066330
2025-10-02 10:04:29,254 Stage: Train 0.5 | Epoch: 216 | Iter: 163000 | Total Loss: 0.006022 | Recon Loss: 0.004860 | Commit Loss: 0.002323 | Perplexity: 955.407116
2025-10-02 10:07:32,471 Stage: Train 0.5 | Epoch: 216 | Iter: 163200 | Total Loss: 0.006064 | Recon Loss: 0.004890 | Commit Loss: 0.002348 | Perplexity: 957.109790
2025-10-02 10:10:33,559 Stage: Train 0.5 | Epoch: 216 | Iter: 163400 | Total Loss: 0.006140 | Recon Loss: 0.004953 | Commit Loss: 0.002375 | Perplexity: 954.065684
Trainning Epoch:  33%|███▎      | 217/665 [45:22:02<86:22:45, 694.12s/it]2025-10-02 10:13:41,626 Stage: Train 0.5 | Epoch: 217 | Iter: 163600 | Total Loss: 0.006122 | Recon Loss: 0.004934 | Commit Loss: 0.002376 | Perplexity: 955.841593
2025-10-02 10:16:47,863 Stage: Train 0.5 | Epoch: 217 | Iter: 163800 | Total Loss: 0.006190 | Recon Loss: 0.005002 | Commit Loss: 0.002377 | Perplexity: 953.164711
2025-10-02 10:19:51,010 Stage: Train 0.5 | Epoch: 217 | Iter: 164000 | Total Loss: 0.006062 | Recon Loss: 0.004877 | Commit Loss: 0.002371 | Perplexity: 951.561263
Trainning Epoch:  33%|███▎      | 218/665 [45:33:38<86:16:59, 694.90s/it]2025-10-02 10:23:00,955 Stage: Train 0.5 | Epoch: 218 | Iter: 164200 | Total Loss: 0.006266 | Recon Loss: 0.005066 | Commit Loss: 0.002400 | Perplexity: 950.192441
2025-10-02 10:26:07,720 Stage: Train 0.5 | Epoch: 218 | Iter: 164400 | Total Loss: 0.006139 | Recon Loss: 0.004946 | Commit Loss: 0.002386 | Perplexity: 940.185913
2025-10-02 10:29:10,920 Stage: Train 0.5 | Epoch: 218 | Iter: 164600 | Total Loss: 0.006153 | Recon Loss: 0.004963 | Commit Loss: 0.002381 | Perplexity: 938.550879
2025-10-02 10:32:11,610 Stage: Train 0.5 | Epoch: 218 | Iter: 164800 | Total Loss: 0.006201 | Recon Loss: 0.005007 | Commit Loss: 0.002389 | Perplexity: 943.373391
Trainning Epoch:  33%|███▎      | 219/665 [45:45:15<86:09:50, 695.49s/it]2025-10-02 10:35:18,408 Stage: Train 0.5 | Epoch: 219 | Iter: 165000 | Total Loss: 0.006134 | Recon Loss: 0.004951 | Commit Loss: 0.002365 | Perplexity: 941.917296
2025-10-02 10:38:20,789 Stage: Train 0.5 | Epoch: 219 | Iter: 165200 | Total Loss: 0.006061 | Recon Loss: 0.004890 | Commit Loss: 0.002341 | Perplexity: 949.317637
2025-10-02 10:41:25,081 Stage: Train 0.5 | Epoch: 219 | Iter: 165400 | Total Loss: 0.006093 | Recon Loss: 0.004918 | Commit Loss: 0.002350 | Perplexity: 948.410180
2025-10-02 10:44:26,682 Stage: Train 0.5 | Epoch: 219 | Iter: 165600 | Total Loss: 0.006173 | Recon Loss: 0.004995 | Commit Loss: 0.002355 | Perplexity: 944.194332
Trainning Epoch:  33%|███▎      | 220/665 [45:56:46<85:48:31, 694.18s/it]2025-10-02 10:47:34,917 Stage: Train 0.5 | Epoch: 220 | Iter: 165800 | Total Loss: 0.006006 | Recon Loss: 0.004835 | Commit Loss: 0.002343 | Perplexity: 944.298405
2025-10-02 10:50:38,227 Stage: Train 0.5 | Epoch: 220 | Iter: 166000 | Total Loss: 0.006109 | Recon Loss: 0.004941 | Commit Loss: 0.002336 | Perplexity: 947.208824
2025-10-02 10:53:38,049 Stage: Train 0.5 | Epoch: 220 | Iter: 166200 | Total Loss: 0.006079 | Recon Loss: 0.004905 | Commit Loss: 0.002346 | Perplexity: 946.103072
2025-10-02 10:56:40,424 Stage: Train 0.5 | Epoch: 220 | Iter: 166400 | Total Loss: 0.006074 | Recon Loss: 0.004911 | Commit Loss: 0.002327 | Perplexity: 940.731050
Trainning Epoch:  33%|███▎      | 221/665 [46:08:18<85:30:57, 693.37s/it]2025-10-02 10:59:51,056 Stage: Train 0.5 | Epoch: 221 | Iter: 166600 | Total Loss: 0.006048 | Recon Loss: 0.004887 | Commit Loss: 0.002321 | Perplexity: 938.521750
2025-10-02 11:02:50,081 Stage: Train 0.5 | Epoch: 221 | Iter: 166800 | Total Loss: 0.006001 | Recon Loss: 0.004830 | Commit Loss: 0.002341 | Perplexity: 943.098126
2025-10-02 11:05:51,052 Stage: Train 0.5 | Epoch: 221 | Iter: 167000 | Total Loss: 0.006078 | Recon Loss: 0.004911 | Commit Loss: 0.002334 | Perplexity: 945.046278
Trainning Epoch:  33%|███▎      | 222/665 [46:19:49<85:14:55, 692.77s/it]2025-10-02 11:09:04,719 Stage: Train 0.5 | Epoch: 222 | Iter: 167200 | Total Loss: 0.006054 | Recon Loss: 0.004895 | Commit Loss: 0.002317 | Perplexity: 950.251508
2025-10-02 11:12:09,006 Stage: Train 0.5 | Epoch: 222 | Iter: 167400 | Total Loss: 0.006083 | Recon Loss: 0.004912 | Commit Loss: 0.002341 | Perplexity: 944.207640
2025-10-02 11:15:16,197 Stage: Train 0.5 | Epoch: 222 | Iter: 167600 | Total Loss: 0.006008 | Recon Loss: 0.004855 | Commit Loss: 0.002306 | Perplexity: 939.842463
2025-10-02 11:18:21,350 Stage: Train 0.5 | Epoch: 222 | Iter: 167800 | Total Loss: 0.006048 | Recon Loss: 0.004892 | Commit Loss: 0.002314 | Perplexity: 940.124128
Trainning Epoch:  34%|███▎      | 223/665 [46:31:35<85:32:52, 696.77s/it]2025-10-02 11:21:31,052 Stage: Train 0.5 | Epoch: 223 | Iter: 168000 | Total Loss: 0.006061 | Recon Loss: 0.004911 | Commit Loss: 0.002300 | Perplexity: 946.383818
2025-10-02 11:24:34,342 Stage: Train 0.5 | Epoch: 223 | Iter: 168200 | Total Loss: 0.006020 | Recon Loss: 0.004861 | Commit Loss: 0.002319 | Perplexity: 946.801777
2025-10-02 11:27:38,967 Stage: Train 0.5 | Epoch: 223 | Iter: 168400 | Total Loss: 0.006009 | Recon Loss: 0.004859 | Commit Loss: 0.002300 | Perplexity: 949.618119
2025-10-02 11:30:41,393 Stage: Train 0.5 | Epoch: 223 | Iter: 168600 | Total Loss: 0.006002 | Recon Loss: 0.004853 | Commit Loss: 0.002299 | Perplexity: 940.479570
Trainning Epoch:  34%|███▎      | 224/665 [46:43:11<85:18:44, 696.43s/it]2025-10-02 11:33:50,601 Stage: Train 0.5 | Epoch: 224 | Iter: 168800 | Total Loss: 0.006037 | Recon Loss: 0.004881 | Commit Loss: 0.002312 | Perplexity: 935.403564
2025-10-02 11:36:52,316 Stage: Train 0.5 | Epoch: 224 | Iter: 169000 | Total Loss: 0.006077 | Recon Loss: 0.004931 | Commit Loss: 0.002291 | Perplexity: 943.822977
2025-10-02 11:39:58,364 Stage: Train 0.5 | Epoch: 224 | Iter: 169200 | Total Loss: 0.005938 | Recon Loss: 0.004787 | Commit Loss: 0.002303 | Perplexity: 944.901464
2025-10-02 11:43:00,485 Stage: Train 0.5 | Epoch: 224 | Iter: 169400 | Total Loss: 0.006075 | Recon Loss: 0.004923 | Commit Loss: 0.002303 | Perplexity: 939.359633
Trainning Epoch:  34%|███▍      | 225/665 [46:54:48<85:09:31, 696.75s/it]2025-10-02 11:46:07,627 Stage: Train 0.5 | Epoch: 225 | Iter: 169600 | Total Loss: 0.005925 | Recon Loss: 0.004779 | Commit Loss: 0.002292 | Perplexity: 946.173922
2025-10-02 11:49:08,963 Stage: Train 0.5 | Epoch: 225 | Iter: 169800 | Total Loss: 0.006089 | Recon Loss: 0.004943 | Commit Loss: 0.002292 | Perplexity: 943.995754
2025-10-02 11:52:10,057 Stage: Train 0.5 | Epoch: 225 | Iter: 170000 | Total Loss: 0.005894 | Recon Loss: 0.004749 | Commit Loss: 0.002289 | Perplexity: 945.865714
Trainning Epoch:  34%|███▍      | 226/665 [47:06:16<84:37:12, 693.92s/it]2025-10-02 11:55:15,340 Stage: Train 0.5 | Epoch: 226 | Iter: 170200 | Total Loss: 0.006057 | Recon Loss: 0.004899 | Commit Loss: 0.002316 | Perplexity: 945.589358
2025-10-02 11:58:18,229 Stage: Train 0.5 | Epoch: 226 | Iter: 170400 | Total Loss: 0.005989 | Recon Loss: 0.004845 | Commit Loss: 0.002287 | Perplexity: 943.034886
2025-10-02 12:01:20,074 Stage: Train 0.5 | Epoch: 226 | Iter: 170600 | Total Loss: 0.005967 | Recon Loss: 0.004823 | Commit Loss: 0.002288 | Perplexity: 946.899233
2025-10-02 12:04:20,633 Stage: Train 0.5 | Epoch: 226 | Iter: 170800 | Total Loss: 0.005956 | Recon Loss: 0.004813 | Commit Loss: 0.002288 | Perplexity: 949.774228
Trainning Epoch:  34%|███▍      | 227/665 [47:17:46<84:17:13, 692.77s/it]2025-10-02 12:07:28,728 Stage: Train 0.5 | Epoch: 227 | Iter: 171000 | Total Loss: 0.006067 | Recon Loss: 0.004921 | Commit Loss: 0.002293 | Perplexity: 938.116689
2025-10-02 12:10:32,613 Stage: Train 0.5 | Epoch: 227 | Iter: 171200 | Total Loss: 0.005869 | Recon Loss: 0.004715 | Commit Loss: 0.002308 | Perplexity: 949.737764
2025-10-02 12:13:36,955 Stage: Train 0.5 | Epoch: 227 | Iter: 171400 | Total Loss: 0.006087 | Recon Loss: 0.004945 | Commit Loss: 0.002284 | Perplexity: 941.453483
2025-10-02 12:16:40,090 Stage: Train 0.5 | Epoch: 227 | Iter: 171600 | Total Loss: 0.006009 | Recon Loss: 0.004870 | Commit Loss: 0.002279 | Perplexity: 937.622463
Trainning Epoch:  34%|███▍      | 228/665 [47:29:21<84:11:07, 693.52s/it]2025-10-02 12:19:46,874 Stage: Train 0.5 | Epoch: 228 | Iter: 171800 | Total Loss: 0.005916 | Recon Loss: 0.004779 | Commit Loss: 0.002274 | Perplexity: 942.342123
2025-10-02 12:22:48,230 Stage: Train 0.5 | Epoch: 228 | Iter: 172000 | Total Loss: 0.005917 | Recon Loss: 0.004773 | Commit Loss: 0.002288 | Perplexity: 951.786396
2025-10-02 12:25:50,851 Stage: Train 0.5 | Epoch: 228 | Iter: 172200 | Total Loss: 0.005942 | Recon Loss: 0.004788 | Commit Loss: 0.002307 | Perplexity: 947.731997
2025-10-02 12:28:51,557 Stage: Train 0.5 | Epoch: 228 | Iter: 172400 | Total Loss: 0.005989 | Recon Loss: 0.004842 | Commit Loss: 0.002294 | Perplexity: 941.852425
Trainning Epoch:  34%|███▍      | 229/665 [47:40:50<83:49:46, 692.17s/it]2025-10-02 12:32:00,599 Stage: Train 0.5 | Epoch: 229 | Iter: 172600 | Total Loss: 0.005897 | Recon Loss: 0.004762 | Commit Loss: 0.002269 | Perplexity: 941.361061
2025-10-02 12:35:03,838 Stage: Train 0.5 | Epoch: 229 | Iter: 172800 | Total Loss: 0.005926 | Recon Loss: 0.004786 | Commit Loss: 0.002280 | Perplexity: 951.407513
2025-10-02 12:38:15,674 Stage: Train 0.5 | Epoch: 229 | Iter: 173000 | Total Loss: 0.006010 | Recon Loss: 0.004863 | Commit Loss: 0.002294 | Perplexity: 950.374184
Trainning Epoch:  35%|███▍      | 230/665 [47:52:47<84:31:37, 699.53s/it]2025-10-02 12:41:37,655 Stage: Train 0.5 | Epoch: 230 | Iter: 173200 | Total Loss: 0.005879 | Recon Loss: 0.004731 | Commit Loss: 0.002296 | Perplexity: 952.773523
2025-10-02 12:44:40,056 Stage: Train 0.5 | Epoch: 230 | Iter: 173400 | Total Loss: 0.005913 | Recon Loss: 0.004774 | Commit Loss: 0.002278 | Perplexity: 950.977530
2025-10-02 12:47:39,975 Stage: Train 0.5 | Epoch: 230 | Iter: 173600 | Total Loss: 0.005945 | Recon Loss: 0.004791 | Commit Loss: 0.002307 | Perplexity: 953.611470
2025-10-02 12:50:41,437 Stage: Train 0.5 | Epoch: 230 | Iter: 173800 | Total Loss: 0.005956 | Recon Loss: 0.004811 | Commit Loss: 0.002291 | Perplexity: 947.150685
Trainning Epoch:  35%|███▍      | 231/665 [48:04:16<83:56:52, 696.34s/it]2025-10-02 12:53:48,914 Stage: Train 0.5 | Epoch: 231 | Iter: 174000 | Total Loss: 0.005905 | Recon Loss: 0.004763 | Commit Loss: 0.002284 | Perplexity: 948.600393
2025-10-02 12:56:51,352 Stage: Train 0.5 | Epoch: 231 | Iter: 174200 | Total Loss: 0.005932 | Recon Loss: 0.004794 | Commit Loss: 0.002276 | Perplexity: 947.479682
2025-10-02 12:59:52,860 Stage: Train 0.5 | Epoch: 231 | Iter: 174400 | Total Loss: 0.005887 | Recon Loss: 0.004751 | Commit Loss: 0.002271 | Perplexity: 942.526763
2025-10-02 13:02:53,848 Stage: Train 0.5 | Epoch: 231 | Iter: 174600 | Total Loss: 0.005953 | Recon Loss: 0.004822 | Commit Loss: 0.002262 | Perplexity: 949.160447
Trainning Epoch:  35%|███▍      | 232/665 [48:15:48<83:36:18, 695.10s/it]2025-10-02 13:06:04,535 Stage: Train 0.5 | Epoch: 232 | Iter: 174800 | Total Loss: 0.005911 | Recon Loss: 0.004767 | Commit Loss: 0.002287 | Perplexity: 946.240647
2025-10-02 13:09:06,936 Stage: Train 0.5 | Epoch: 232 | Iter: 175000 | Total Loss: 0.005963 | Recon Loss: 0.004832 | Commit Loss: 0.002262 | Perplexity: 944.708751
2025-10-02 13:12:09,481 Stage: Train 0.5 | Epoch: 232 | Iter: 175200 | Total Loss: 0.005901 | Recon Loss: 0.004757 | Commit Loss: 0.002288 | Perplexity: 952.541147
2025-10-02 13:15:12,716 Stage: Train 0.5 | Epoch: 232 | Iter: 175400 | Total Loss: 0.005932 | Recon Loss: 0.004783 | Commit Loss: 0.002298 | Perplexity: 947.856367
Trainning Epoch:  35%|███▌      | 233/665 [48:27:22<83:21:54, 694.71s/it]2025-10-02 13:18:18,610 Stage: Train 0.5 | Epoch: 233 | Iter: 175600 | Total Loss: 0.005917 | Recon Loss: 0.004774 | Commit Loss: 0.002287 | Perplexity: 943.889876
2025-10-02 13:21:21,045 Stage: Train 0.5 | Epoch: 233 | Iter: 175800 | Total Loss: 0.005927 | Recon Loss: 0.004787 | Commit Loss: 0.002279 | Perplexity: 941.840200
2025-10-02 13:24:26,018 Stage: Train 0.5 | Epoch: 233 | Iter: 176000 | Total Loss: 0.005841 | Recon Loss: 0.004703 | Commit Loss: 0.002276 | Perplexity: 946.830111
2025-10-02 13:27:28,625 Stage: Train 0.5 | Epoch: 233 | Iter: 176200 | Total Loss: 0.005954 | Recon Loss: 0.004825 | Commit Loss: 0.002259 | Perplexity: 945.538674
Trainning Epoch:  35%|███▌      | 234/665 [48:38:57<83:12:05, 694.96s/it]2025-10-02 13:30:37,333 Stage: Train 0.5 | Epoch: 234 | Iter: 176400 | Total Loss: 0.005870 | Recon Loss: 0.004731 | Commit Loss: 0.002279 | Perplexity: 945.239793
2025-10-02 13:33:40,013 Stage: Train 0.5 | Epoch: 234 | Iter: 176600 | Total Loss: 0.005916 | Recon Loss: 0.004779 | Commit Loss: 0.002273 | Perplexity: 937.554803
2025-10-02 13:36:41,415 Stage: Train 0.5 | Epoch: 234 | Iter: 176800 | Total Loss: 0.005865 | Recon Loss: 0.004729 | Commit Loss: 0.002272 | Perplexity: 947.471446
Trainning Epoch:  35%|███▌      | 235/665 [48:50:27<82:49:52, 693.47s/it]2025-10-02 13:39:47,743 Stage: Train 0.5 | Epoch: 235 | Iter: 177000 | Total Loss: 0.005869 | Recon Loss: 0.004731 | Commit Loss: 0.002275 | Perplexity: 952.011884
2025-10-02 13:42:49,759 Stage: Train 0.5 | Epoch: 235 | Iter: 177200 | Total Loss: 0.005893 | Recon Loss: 0.004758 | Commit Loss: 0.002270 | Perplexity: 951.834952
2025-10-02 13:45:52,506 Stage: Train 0.5 | Epoch: 235 | Iter: 177400 | Total Loss: 0.005881 | Recon Loss: 0.004746 | Commit Loss: 0.002269 | Perplexity: 948.328514
2025-10-02 13:48:54,050 Stage: Train 0.5 | Epoch: 235 | Iter: 177600 | Total Loss: 0.005856 | Recon Loss: 0.004716 | Commit Loss: 0.002280 | Perplexity: 949.445196
Trainning Epoch:  35%|███▌      | 236/665 [49:01:59<82:34:47, 692.98s/it]2025-10-02 13:52:04,565 Stage: Train 0.5 | Epoch: 236 | Iter: 177800 | Total Loss: 0.005943 | Recon Loss: 0.004811 | Commit Loss: 0.002264 | Perplexity: 944.035775
2025-10-02 13:55:04,892 Stage: Train 0.5 | Epoch: 236 | Iter: 178000 | Total Loss: 0.005906 | Recon Loss: 0.004781 | Commit Loss: 0.002251 | Perplexity: 947.525508
2025-10-02 13:58:10,582 Stage: Train 0.5 | Epoch: 236 | Iter: 178200 | Total Loss: 0.005891 | Recon Loss: 0.004759 | Commit Loss: 0.002264 | Perplexity: 941.511879
2025-10-02 14:01:08,400 Stage: Train 0.5 | Epoch: 236 | Iter: 178400 | Total Loss: 0.005917 | Recon Loss: 0.004787 | Commit Loss: 0.002259 | Perplexity: 947.856992
Trainning Epoch:  36%|███▌      | 237/665 [49:13:30<82:18:23, 692.30s/it]2025-10-02 14:04:17,786 Stage: Train 0.5 | Epoch: 237 | Iter: 178600 | Total Loss: 0.005857 | Recon Loss: 0.004723 | Commit Loss: 0.002268 | Perplexity: 951.898949
2025-10-02 14:07:21,178 Stage: Train 0.5 | Epoch: 237 | Iter: 178800 | Total Loss: 0.005891 | Recon Loss: 0.004767 | Commit Loss: 0.002249 | Perplexity: 950.563476
2025-10-02 14:10:21,301 Stage: Train 0.5 | Epoch: 237 | Iter: 179000 | Total Loss: 0.005960 | Recon Loss: 0.004825 | Commit Loss: 0.002270 | Perplexity: 936.933166
2025-10-02 14:13:26,293 Stage: Train 0.5 | Epoch: 237 | Iter: 179200 | Total Loss: 0.005853 | Recon Loss: 0.004709 | Commit Loss: 0.002289 | Perplexity: 950.239966
Trainning Epoch:  36%|███▌      | 238/665 [49:25:04<82:11:50, 693.00s/it]2025-10-02 14:16:35,380 Stage: Train 0.5 | Epoch: 238 | Iter: 179400 | Total Loss: 0.005852 | Recon Loss: 0.004718 | Commit Loss: 0.002268 | Perplexity: 948.707279
2025-10-02 14:19:39,187 Stage: Train 0.5 | Epoch: 238 | Iter: 179600 | Total Loss: 0.005858 | Recon Loss: 0.004722 | Commit Loss: 0.002272 | Perplexity: 947.727486
2025-10-02 14:22:43,653 Stage: Train 0.5 | Epoch: 238 | Iter: 179800 | Total Loss: 0.005855 | Recon Loss: 0.004732 | Commit Loss: 0.002245 | Perplexity: 945.875847
Trainning Epoch:  36%|███▌      | 239/665 [49:36:42<82:09:59, 694.37s/it]2025-10-02 14:25:54,338 Stage: Train 0.5 | Epoch: 239 | Iter: 180000 | Total Loss: 0.005835 | Recon Loss: 0.004705 | Commit Loss: 0.002260 | Perplexity: 943.851276
2025-10-02 14:25:54,338 Saving model at iteration 180000
2025-10-02 14:25:54,566 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000
2025-10-02 14:25:54,881 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000/model.safetensors
2025-10-02 14:25:55,313 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000/optimizer.bin
2025-10-02 14:25:55,313 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000/scheduler.bin
2025-10-02 14:25:55,313 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000/sampler.bin
2025-10-02 14:25:55,314 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_240_step_180000/random_states_0.pkl
2025-10-02 14:28:59,225 Stage: Train 0.5 | Epoch: 239 | Iter: 180200 | Total Loss: 0.005917 | Recon Loss: 0.004790 | Commit Loss: 0.002253 | Perplexity: 949.865683
2025-10-02 14:32:02,738 Stage: Train 0.5 | Epoch: 239 | Iter: 180400 | Total Loss: 0.005866 | Recon Loss: 0.004738 | Commit Loss: 0.002255 | Perplexity: 945.300212
2025-10-02 14:35:06,998 Stage: Train 0.5 | Epoch: 239 | Iter: 180600 | Total Loss: 0.005899 | Recon Loss: 0.004758 | Commit Loss: 0.002283 | Perplexity: 952.663060
Trainning Epoch:  36%|███▌      | 240/665 [49:48:22<82:09:38, 695.95s/it]2025-10-02 14:38:14,980 Stage: Train 0.5 | Epoch: 240 | Iter: 180800 | Total Loss: 0.005890 | Recon Loss: 0.004757 | Commit Loss: 0.002266 | Perplexity: 955.591070
2025-10-02 14:41:18,797 Stage: Train 0.5 | Epoch: 240 | Iter: 181000 | Total Loss: 0.005927 | Recon Loss: 0.004804 | Commit Loss: 0.002246 | Perplexity: 942.984431
2025-10-02 14:44:23,630 Stage: Train 0.5 | Epoch: 240 | Iter: 181200 | Total Loss: 0.005903 | Recon Loss: 0.004768 | Commit Loss: 0.002268 | Perplexity: 947.641730
2025-10-02 14:47:27,171 Stage: Train 0.5 | Epoch: 240 | Iter: 181400 | Total Loss: 0.005859 | Recon Loss: 0.004722 | Commit Loss: 0.002274 | Perplexity: 947.726652
Trainning Epoch:  36%|███▌      | 241/665 [50:00:00<82:02:42, 696.61s/it]2025-10-02 14:50:37,441 Stage: Train 0.5 | Epoch: 241 | Iter: 181600 | Total Loss: 0.005865 | Recon Loss: 0.004741 | Commit Loss: 0.002248 | Perplexity: 943.640576
2025-10-02 14:53:43,244 Stage: Train 0.5 | Epoch: 241 | Iter: 181800 | Total Loss: 0.005846 | Recon Loss: 0.004725 | Commit Loss: 0.002242 | Perplexity: 947.082115
2025-10-02 14:56:45,063 Stage: Train 0.5 | Epoch: 241 | Iter: 182000 | Total Loss: 0.005914 | Recon Loss: 0.004779 | Commit Loss: 0.002271 | Perplexity: 951.478244
2025-10-02 14:59:47,207 Stage: Train 0.5 | Epoch: 241 | Iter: 182200 | Total Loss: 0.005781 | Recon Loss: 0.004656 | Commit Loss: 0.002250 | Perplexity: 943.947353
Trainning Epoch:  36%|███▋      | 242/665 [50:11:37<81:52:18, 696.78s/it]2025-10-02 15:02:55,445 Stage: Train 0.5 | Epoch: 242 | Iter: 182400 | Total Loss: 0.005845 | Recon Loss: 0.004721 | Commit Loss: 0.002248 | Perplexity: 940.157230
2025-10-02 15:06:00,390 Stage: Train 0.5 | Epoch: 242 | Iter: 182600 | Total Loss: 0.005826 | Recon Loss: 0.004701 | Commit Loss: 0.002250 | Perplexity: 951.370992
2025-10-02 15:09:02,427 Stage: Train 0.5 | Epoch: 242 | Iter: 182800 | Total Loss: 0.005862 | Recon Loss: 0.004731 | Commit Loss: 0.002261 | Perplexity: 951.414948
Trainning Epoch:  37%|███▋      | 243/665 [50:23:10<81:33:57, 695.82s/it]2025-10-02 15:12:10,116 Stage: Train 0.5 | Epoch: 243 | Iter: 183000 | Total Loss: 0.005850 | Recon Loss: 0.004725 | Commit Loss: 0.002250 | Perplexity: 943.423049
2025-10-02 15:15:12,843 Stage: Train 0.5 | Epoch: 243 | Iter: 183200 | Total Loss: 0.005842 | Recon Loss: 0.004713 | Commit Loss: 0.002257 | Perplexity: 946.700820
2025-10-02 15:18:16,490 Stage: Train 0.5 | Epoch: 243 | Iter: 183400 | Total Loss: 0.005851 | Recon Loss: 0.004722 | Commit Loss: 0.002258 | Perplexity: 948.780262
2025-10-02 15:21:18,697 Stage: Train 0.5 | Epoch: 243 | Iter: 183600 | Total Loss: 0.005873 | Recon Loss: 0.004753 | Commit Loss: 0.002240 | Perplexity: 946.694630
Trainning Epoch:  37%|███▋      | 244/665 [50:34:43<81:14:36, 694.72s/it]2025-10-02 15:24:24,444 Stage: Train 0.5 | Epoch: 244 | Iter: 183800 | Total Loss: 0.005823 | Recon Loss: 0.004684 | Commit Loss: 0.002278 | Perplexity: 956.032534
2025-10-02 15:27:29,671 Stage: Train 0.5 | Epoch: 244 | Iter: 184000 | Total Loss: 0.005795 | Recon Loss: 0.004664 | Commit Loss: 0.002261 | Perplexity: 951.382215
2025-10-02 15:30:34,324 Stage: Train 0.5 | Epoch: 244 | Iter: 184200 | Total Loss: 0.005787 | Recon Loss: 0.004664 | Commit Loss: 0.002246 | Perplexity: 948.127385
2025-10-02 15:33:40,057 Stage: Train 0.5 | Epoch: 244 | Iter: 184400 | Total Loss: 0.005792 | Recon Loss: 0.004664 | Commit Loss: 0.002254 | Perplexity: 950.661940
Trainning Epoch:  37%|███▋      | 245/665 [50:46:24<81:17:14, 696.75s/it]2025-10-02 15:36:50,551 Stage: Train 0.5 | Epoch: 245 | Iter: 184600 | Total Loss: 0.005861 | Recon Loss: 0.004736 | Commit Loss: 0.002250 | Perplexity: 950.216801
2025-10-02 15:39:55,022 Stage: Train 0.5 | Epoch: 245 | Iter: 184800 | Total Loss: 0.005795 | Recon Loss: 0.004672 | Commit Loss: 0.002246 | Perplexity: 948.130161
2025-10-02 15:42:56,243 Stage: Train 0.5 | Epoch: 245 | Iter: 185000 | Total Loss: 0.005879 | Recon Loss: 0.004735 | Commit Loss: 0.002289 | Perplexity: 955.428639
2025-10-02 15:45:58,543 Stage: Train 0.5 | Epoch: 245 | Iter: 185200 | Total Loss: 0.005784 | Recon Loss: 0.004673 | Commit Loss: 0.002221 | Perplexity: 943.355222
Trainning Epoch:  37%|███▋      | 246/665 [50:57:59<81:01:26, 696.15s/it]2025-10-02 15:49:09,022 Stage: Train 0.5 | Epoch: 246 | Iter: 185400 | Total Loss: 0.005799 | Recon Loss: 0.004682 | Commit Loss: 0.002233 | Perplexity: 952.409168
2025-10-02 15:52:13,687 Stage: Train 0.5 | Epoch: 246 | Iter: 185600 | Total Loss: 0.005829 | Recon Loss: 0.004700 | Commit Loss: 0.002258 | Perplexity: 950.271730
2025-10-02 15:55:13,959 Stage: Train 0.5 | Epoch: 246 | Iter: 185800 | Total Loss: 0.005833 | Recon Loss: 0.004704 | Commit Loss: 0.002258 | Perplexity: 944.705395
Trainning Epoch:  37%|███▋      | 247/665 [51:09:37<80:53:50, 696.72s/it]2025-10-02 15:58:27,130 Stage: Train 0.5 | Epoch: 247 | Iter: 186000 | Total Loss: 0.005778 | Recon Loss: 0.004657 | Commit Loss: 0.002242 | Perplexity: 948.858181
2025-10-02 16:01:32,209 Stage: Train 0.5 | Epoch: 247 | Iter: 186200 | Total Loss: 0.005837 | Recon Loss: 0.004706 | Commit Loss: 0.002263 | Perplexity: 952.734683
2025-10-02 16:04:35,723 Stage: Train 0.5 | Epoch: 247 | Iter: 186400 | Total Loss: 0.005711 | Recon Loss: 0.004580 | Commit Loss: 0.002261 | Perplexity: 953.409347
2025-10-02 16:07:37,763 Stage: Train 0.5 | Epoch: 247 | Iter: 186600 | Total Loss: 0.005799 | Recon Loss: 0.004686 | Commit Loss: 0.002226 | Perplexity: 945.344412
Trainning Epoch:  37%|███▋      | 248/665 [51:21:16<80:46:49, 697.38s/it]2025-10-02 16:10:48,454 Stage: Train 0.5 | Epoch: 248 | Iter: 186800 | Total Loss: 0.005793 | Recon Loss: 0.004669 | Commit Loss: 0.002247 | Perplexity: 939.507340
2025-10-02 16:13:51,268 Stage: Train 0.5 | Epoch: 248 | Iter: 187000 | Total Loss: 0.005787 | Recon Loss: 0.004659 | Commit Loss: 0.002255 | Perplexity: 953.038348
2025-10-02 16:16:54,450 Stage: Train 0.5 | Epoch: 248 | Iter: 187200 | Total Loss: 0.005820 | Recon Loss: 0.004700 | Commit Loss: 0.002239 | Perplexity: 941.281437
2025-10-02 16:19:59,458 Stage: Train 0.5 | Epoch: 248 | Iter: 187400 | Total Loss: 0.005838 | Recon Loss: 0.004720 | Commit Loss: 0.002237 | Perplexity: 943.731359
Trainning Epoch:  37%|███▋      | 249/665 [51:32:55<80:39:50, 698.06s/it]2025-10-02 16:23:08,702 Stage: Train 0.5 | Epoch: 249 | Iter: 187600 | Total Loss: 0.005728 | Recon Loss: 0.004607 | Commit Loss: 0.002242 | Perplexity: 938.662421
2025-10-02 16:26:13,622 Stage: Train 0.5 | Epoch: 249 | Iter: 187800 | Total Loss: 0.005784 | Recon Loss: 0.004666 | Commit Loss: 0.002237 | Perplexity: 950.608841
2025-10-02 16:29:18,491 Stage: Train 0.5 | Epoch: 249 | Iter: 188000 | Total Loss: 0.005775 | Recon Loss: 0.004650 | Commit Loss: 0.002250 | Perplexity: 945.532857
2025-10-02 16:32:23,774 Stage: Train 0.5 | Epoch: 249 | Iter: 188200 | Total Loss: 0.005822 | Recon Loss: 0.004695 | Commit Loss: 0.002252 | Perplexity: 945.483597
Trainning Epoch:  38%|███▊      | 250/665 [51:44:37<80:35:32, 699.11s/it]2025-10-02 16:35:35,619 Stage: Train 0.5 | Epoch: 250 | Iter: 188400 | Total Loss: 0.005791 | Recon Loss: 0.004671 | Commit Loss: 0.002240 | Perplexity: 944.997933
2025-10-02 16:38:40,522 Stage: Train 0.5 | Epoch: 250 | Iter: 188600 | Total Loss: 0.005752 | Recon Loss: 0.004644 | Commit Loss: 0.002216 | Perplexity: 935.040560
2025-10-02 16:41:45,688 Stage: Train 0.5 | Epoch: 250 | Iter: 188800 | Total Loss: 0.005758 | Recon Loss: 0.004627 | Commit Loss: 0.002262 | Perplexity: 942.419344
2025-10-02 16:44:51,344 Stage: Train 0.5 | Epoch: 250 | Iter: 189000 | Total Loss: 0.005781 | Recon Loss: 0.004655 | Commit Loss: 0.002253 | Perplexity: 945.746641
Trainning Epoch:  38%|███▊      | 251/665 [51:56:20<80:32:47, 700.41s/it]2025-10-02 16:48:03,043 Stage: Train 0.5 | Epoch: 251 | Iter: 189200 | Total Loss: 0.005758 | Recon Loss: 0.004643 | Commit Loss: 0.002231 | Perplexity: 944.133179
2025-10-02 16:51:10,196 Stage: Train 0.5 | Epoch: 251 | Iter: 189400 | Total Loss: 0.005777 | Recon Loss: 0.004659 | Commit Loss: 0.002235 | Perplexity: 939.929099
2025-10-02 16:54:25,475 Stage: Train 0.5 | Epoch: 251 | Iter: 189600 | Total Loss: 0.005762 | Recon Loss: 0.004648 | Commit Loss: 0.002227 | Perplexity: 941.759129
Trainning Epoch:  38%|███▊      | 252/665 [52:08:23<81:06:17, 706.97s/it]2025-10-02 16:57:45,960 Stage: Train 0.5 | Epoch: 252 | Iter: 189800 | Total Loss: 0.005817 | Recon Loss: 0.004697 | Commit Loss: 0.002238 | Perplexity: 938.354541
2025-10-02 17:00:59,445 Stage: Train 0.5 | Epoch: 252 | Iter: 190000 | Total Loss: 0.005789 | Recon Loss: 0.004658 | Commit Loss: 0.002261 | Perplexity: 941.842672
2025-10-02 17:04:06,153 Stage: Train 0.5 | Epoch: 252 | Iter: 190200 | Total Loss: 0.005747 | Recon Loss: 0.004640 | Commit Loss: 0.002215 | Perplexity: 941.273932
2025-10-02 17:07:05,957 Stage: Train 0.5 | Epoch: 252 | Iter: 190400 | Total Loss: 0.005750 | Recon Loss: 0.004630 | Commit Loss: 0.002241 | Perplexity: 941.890464
Trainning Epoch:  38%|███▊      | 253/665 [52:20:10<80:54:32, 706.97s/it]2025-10-02 17:10:01,917 Stage: Train 0.5 | Epoch: 253 | Iter: 190600 | Total Loss: 0.005807 | Recon Loss: 0.004681 | Commit Loss: 0.002253 | Perplexity: 944.124932
2025-10-02 17:12:40,933 Stage: Train 0.5 | Epoch: 253 | Iter: 190800 | Total Loss: 0.005766 | Recon Loss: 0.004650 | Commit Loss: 0.002233 | Perplexity: 946.065285
2025-10-02 17:15:37,650 Stage: Train 0.5 | Epoch: 253 | Iter: 191000 | Total Loss: 0.005796 | Recon Loss: 0.004689 | Commit Loss: 0.002215 | Perplexity: 941.747780
2025-10-02 17:18:17,362 Stage: Train 0.5 | Epoch: 253 | Iter: 191200 | Total Loss: 0.005752 | Recon Loss: 0.004644 | Commit Loss: 0.002216 | Perplexity: 942.879837
Trainning Epoch:  38%|███▊      | 254/665 [52:30:32<77:49:27, 681.67s/it]2025-10-02 17:21:00,198 Stage: Train 0.5 | Epoch: 254 | Iter: 191400 | Total Loss: 0.005779 | Recon Loss: 0.004651 | Commit Loss: 0.002256 | Perplexity: 943.773975
2025-10-02 17:23:35,769 Stage: Train 0.5 | Epoch: 254 | Iter: 191600 | Total Loss: 0.005787 | Recon Loss: 0.004652 | Commit Loss: 0.002269 | Perplexity: 943.417310
2025-10-02 17:26:14,363 Stage: Train 0.5 | Epoch: 254 | Iter: 191800 | Total Loss: 0.005716 | Recon Loss: 0.004607 | Commit Loss: 0.002217 | Perplexity: 944.375042
2025-10-02 17:28:50,250 Stage: Train 0.5 | Epoch: 254 | Iter: 192000 | Total Loss: 0.005694 | Recon Loss: 0.004585 | Commit Loss: 0.002219 | Perplexity: 945.507688
Trainning Epoch:  38%|███▊      | 255/665 [52:40:29<74:43:32, 656.13s/it]2025-10-02 17:31:36,058 Stage: Train 0.5 | Epoch: 255 | Iter: 192200 | Total Loss: 0.005794 | Recon Loss: 0.004683 | Commit Loss: 0.002221 | Perplexity: 936.887651
2025-10-02 17:34:13,880 Stage: Train 0.5 | Epoch: 255 | Iter: 192400 | Total Loss: 0.005745 | Recon Loss: 0.004618 | Commit Loss: 0.002254 | Perplexity: 946.345667
2025-10-02 17:36:48,566 Stage: Train 0.5 | Epoch: 255 | Iter: 192600 | Total Loss: 0.005758 | Recon Loss: 0.004636 | Commit Loss: 0.002243 | Perplexity: 943.651830
Trainning Epoch:  38%|███▊      | 256/665 [52:50:28<72:35:35, 638.96s/it]2025-10-02 17:39:34,624 Stage: Train 0.5 | Epoch: 256 | Iter: 192800 | Total Loss: 0.005713 | Recon Loss: 0.004616 | Commit Loss: 0.002194 | Perplexity: 938.198960
2025-10-02 17:42:12,967 Stage: Train 0.5 | Epoch: 256 | Iter: 193000 | Total Loss: 0.005737 | Recon Loss: 0.004617 | Commit Loss: 0.002241 | Perplexity: 950.819321
2025-10-02 17:44:51,387 Stage: Train 0.5 | Epoch: 256 | Iter: 193200 | Total Loss: 0.005771 | Recon Loss: 0.004664 | Commit Loss: 0.002215 | Perplexity: 938.638683
2025-10-02 17:47:31,319 Stage: Train 0.5 | Epoch: 256 | Iter: 193400 | Total Loss: 0.005661 | Recon Loss: 0.004548 | Commit Loss: 0.002226 | Perplexity: 947.633539
Trainning Epoch:  39%|███▊      | 257/665 [53:00:31<71:12:25, 628.30s/it]2025-10-02 17:50:11,777 Stage: Train 0.5 | Epoch: 257 | Iter: 193600 | Total Loss: 0.005761 | Recon Loss: 0.004636 | Commit Loss: 0.002251 | Perplexity: 947.534438
2025-10-02 17:52:49,051 Stage: Train 0.5 | Epoch: 257 | Iter: 193800 | Total Loss: 0.005727 | Recon Loss: 0.004618 | Commit Loss: 0.002218 | Perplexity: 945.081374
2025-10-02 17:55:23,925 Stage: Train 0.5 | Epoch: 257 | Iter: 194000 | Total Loss: 0.005741 | Recon Loss: 0.004628 | Commit Loss: 0.002226 | Perplexity: 943.489297
2025-10-02 17:58:02,543 Stage: Train 0.5 | Epoch: 257 | Iter: 194200 | Total Loss: 0.005731 | Recon Loss: 0.004620 | Commit Loss: 0.002222 | Perplexity: 949.009207
Trainning Epoch:  39%|███▉      | 258/665 [53:10:28<69:56:49, 618.70s/it]2025-10-02 18:00:45,903 Stage: Train 0.5 | Epoch: 258 | Iter: 194400 | Total Loss: 0.005739 | Recon Loss: 0.004616 | Commit Loss: 0.002246 | Perplexity: 945.503339
2025-10-02 18:03:21,795 Stage: Train 0.5 | Epoch: 258 | Iter: 194600 | Total Loss: 0.005701 | Recon Loss: 0.004591 | Commit Loss: 0.002221 | Perplexity: 949.841707
2025-10-02 18:05:57,789 Stage: Train 0.5 | Epoch: 258 | Iter: 194800 | Total Loss: 0.005796 | Recon Loss: 0.004672 | Commit Loss: 0.002248 | Perplexity: 943.717089
2025-10-02 18:08:34,235 Stage: Train 0.5 | Epoch: 258 | Iter: 195000 | Total Loss: 0.005713 | Recon Loss: 0.004603 | Commit Loss: 0.002220 | Perplexity: 948.493134
Trainning Epoch:  39%|███▉      | 259/665 [53:20:22<68:56:47, 611.35s/it]2025-10-02 18:11:15,851 Stage: Train 0.5 | Epoch: 259 | Iter: 195200 | Total Loss: 0.005692 | Recon Loss: 0.004583 | Commit Loss: 0.002217 | Perplexity: 944.295937
2025-10-02 18:13:50,475 Stage: Train 0.5 | Epoch: 259 | Iter: 195400 | Total Loss: 0.005831 | Recon Loss: 0.004708 | Commit Loss: 0.002245 | Perplexity: 950.108525
2025-10-02 18:16:27,971 Stage: Train 0.5 | Epoch: 259 | Iter: 195600 | Total Loss: 0.005684 | Recon Loss: 0.004578 | Commit Loss: 0.002212 | Perplexity: 948.011006
Trainning Epoch:  39%|███▉      | 260/665 [53:30:14<68:07:30, 605.56s/it]2025-10-02 18:19:09,562 Stage: Train 0.5 | Epoch: 260 | Iter: 195800 | Total Loss: 0.005719 | Recon Loss: 0.004601 | Commit Loss: 0.002235 | Perplexity: 945.772379
2025-10-02 18:21:48,838 Stage: Train 0.5 | Epoch: 260 | Iter: 196000 | Total Loss: 0.005724 | Recon Loss: 0.004613 | Commit Loss: 0.002222 | Perplexity: 952.619652
2025-10-02 18:24:24,631 Stage: Train 0.5 | Epoch: 260 | Iter: 196200 | Total Loss: 0.005657 | Recon Loss: 0.004554 | Commit Loss: 0.002206 | Perplexity: 947.741652
2025-10-02 18:26:58,994 Stage: Train 0.5 | Epoch: 260 | Iter: 196400 | Total Loss: 0.005783 | Recon Loss: 0.004666 | Commit Loss: 0.002235 | Perplexity: 956.134201
Trainning Epoch:  39%|███▉      | 261/665 [53:40:08<67:35:26, 602.29s/it]2025-10-02 18:29:41,630 Stage: Train 0.5 | Epoch: 261 | Iter: 196600 | Total Loss: 0.005737 | Recon Loss: 0.004626 | Commit Loss: 0.002222 | Perplexity: 951.495231
2025-10-02 18:32:16,853 Stage: Train 0.5 | Epoch: 261 | Iter: 196800 | Total Loss: 0.005737 | Recon Loss: 0.004624 | Commit Loss: 0.002225 | Perplexity: 953.976051
2025-10-02 18:34:54,384 Stage: Train 0.5 | Epoch: 261 | Iter: 197000 | Total Loss: 0.005700 | Recon Loss: 0.004578 | Commit Loss: 0.002243 | Perplexity: 946.295665
2025-10-02 18:37:29,248 Stage: Train 0.5 | Epoch: 261 | Iter: 197200 | Total Loss: 0.005693 | Recon Loss: 0.004583 | Commit Loss: 0.002218 | Perplexity: 942.685927
Trainning Epoch:  39%|███▉      | 262/665 [53:50:03<67:09:35, 599.94s/it]2025-10-02 18:40:11,080 Stage: Train 0.5 | Epoch: 262 | Iter: 197400 | Total Loss: 0.005749 | Recon Loss: 0.004644 | Commit Loss: 0.002210 | Perplexity: 944.110884
2025-10-02 18:42:48,120 Stage: Train 0.5 | Epoch: 262 | Iter: 197600 | Total Loss: 0.005650 | Recon Loss: 0.004548 | Commit Loss: 0.002203 | Perplexity: 951.580219
2025-10-02 18:45:26,842 Stage: Train 0.5 | Epoch: 262 | Iter: 197800 | Total Loss: 0.005776 | Recon Loss: 0.004657 | Commit Loss: 0.002238 | Perplexity: 944.528136
2025-10-02 18:48:01,519 Stage: Train 0.5 | Epoch: 262 | Iter: 198000 | Total Loss: 0.005695 | Recon Loss: 0.004584 | Commit Loss: 0.002223 | Perplexity: 949.977502
Trainning Epoch:  40%|███▉      | 263/665 [54:00:00<66:53:33, 599.04s/it]2025-10-02 18:50:45,842 Stage: Train 0.5 | Epoch: 263 | Iter: 198200 | Total Loss: 0.005718 | Recon Loss: 0.004598 | Commit Loss: 0.002241 | Perplexity: 951.274305
2025-10-02 18:53:23,372 Stage: Train 0.5 | Epoch: 263 | Iter: 198400 | Total Loss: 0.005707 | Recon Loss: 0.004603 | Commit Loss: 0.002207 | Perplexity: 944.079456
2025-10-02 18:55:59,024 Stage: Train 0.5 | Epoch: 263 | Iter: 198600 | Total Loss: 0.005740 | Recon Loss: 0.004623 | Commit Loss: 0.002234 | Perplexity: 946.114918
Trainning Epoch:  40%|███▉      | 264/665 [54:09:55<66:36:20, 597.96s/it]2025-10-02 18:58:41,940 Stage: Train 0.5 | Epoch: 264 | Iter: 198800 | Total Loss: 0.005753 | Recon Loss: 0.004648 | Commit Loss: 0.002211 | Perplexity: 951.205092
2025-10-02 19:01:16,544 Stage: Train 0.5 | Epoch: 264 | Iter: 199000 | Total Loss: 0.005681 | Recon Loss: 0.004583 | Commit Loss: 0.002197 | Perplexity: 941.439743
2025-10-02 19:03:51,467 Stage: Train 0.5 | Epoch: 264 | Iter: 199200 | Total Loss: 0.005649 | Recon Loss: 0.004536 | Commit Loss: 0.002226 | Perplexity: 954.423515
2025-10-02 19:06:28,094 Stage: Train 0.5 | Epoch: 264 | Iter: 199400 | Total Loss: 0.005708 | Recon Loss: 0.004598 | Commit Loss: 0.002221 | Perplexity: 949.385490
Trainning Epoch:  40%|███▉      | 265/665 [54:19:48<66:15:00, 596.25s/it]2025-10-02 19:09:09,774 Stage: Train 0.5 | Epoch: 265 | Iter: 199600 | Total Loss: 0.005687 | Recon Loss: 0.004593 | Commit Loss: 0.002188 | Perplexity: 942.387651
2025-10-02 19:11:46,450 Stage: Train 0.5 | Epoch: 265 | Iter: 199800 | Total Loss: 0.005706 | Recon Loss: 0.004607 | Commit Loss: 0.002197 | Perplexity: 945.820452
2025-10-02 19:14:22,882 Stage: Train 0.5 | Epoch: 265 | Iter: 200000 | Total Loss: 0.005695 | Recon Loss: 0.004590 | Commit Loss: 0.002209 | Perplexity: 943.105160
2025-10-02 19:14:22,883 Saving model at iteration 200000
2025-10-02 19:14:23,081 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000
2025-10-02 19:14:23,396 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000/model.safetensors
2025-10-02 19:14:23,787 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000/optimizer.bin
2025-10-02 19:14:23,787 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000/scheduler.bin
2025-10-02 19:14:23,787 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000/sampler.bin
2025-10-02 19:14:23,788 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_266_step_200000/random_states_0.pkl
2025-10-02 19:17:05,401 Stage: Train 0.5 | Epoch: 265 | Iter: 200200 | Total Loss: 0.005733 | Recon Loss: 0.004627 | Commit Loss: 0.002213 | Perplexity: 948.208809
Trainning Epoch:  40%|████      | 266/665 [54:29:50<66:17:04, 598.06s/it]2025-10-02 19:19:47,364 Stage: Train 0.5 | Epoch: 266 | Iter: 200400 | Total Loss: 0.005668 | Recon Loss: 0.004551 | Commit Loss: 0.002235 | Perplexity: 949.023098
2025-10-02 19:22:26,987 Stage: Train 0.5 | Epoch: 266 | Iter: 200600 | Total Loss: 0.005640 | Recon Loss: 0.004542 | Commit Loss: 0.002195 | Perplexity: 940.969326
2025-10-02 19:25:01,663 Stage: Train 0.5 | Epoch: 266 | Iter: 200800 | Total Loss: 0.005718 | Recon Loss: 0.004616 | Commit Loss: 0.002204 | Perplexity: 942.802497
2025-10-02 19:27:37,750 Stage: Train 0.5 | Epoch: 266 | Iter: 201000 | Total Loss: 0.005688 | Recon Loss: 0.004574 | Commit Loss: 0.002229 | Perplexity: 941.980784
Trainning Epoch:  40%|████      | 267/665 [54:39:43<65:58:04, 596.69s/it]2025-10-02 19:30:19,722 Stage: Train 0.5 | Epoch: 267 | Iter: 201200 | Total Loss: 0.005721 | Recon Loss: 0.004621 | Commit Loss: 0.002200 | Perplexity: 940.311054
2025-10-02 19:33:00,471 Stage: Train 0.5 | Epoch: 267 | Iter: 201400 | Total Loss: 0.005681 | Recon Loss: 0.004568 | Commit Loss: 0.002227 | Perplexity: 951.238640
2025-10-02 19:35:34,481 Stage: Train 0.5 | Epoch: 267 | Iter: 201600 | Total Loss: 0.005651 | Recon Loss: 0.004537 | Commit Loss: 0.002228 | Perplexity: 952.528253
2025-10-02 19:38:08,574 Stage: Train 0.5 | Epoch: 267 | Iter: 201800 | Total Loss: 0.005632 | Recon Loss: 0.004535 | Commit Loss: 0.002194 | Perplexity: 938.466040
Trainning Epoch:  40%|████      | 268/665 [54:49:39<65:46:58, 596.52s/it]2025-10-02 19:40:50,877 Stage: Train 0.5 | Epoch: 268 | Iter: 202000 | Total Loss: 0.005708 | Recon Loss: 0.004606 | Commit Loss: 0.002204 | Perplexity: 944.923789
2025-10-02 19:43:27,492 Stage: Train 0.5 | Epoch: 268 | Iter: 202200 | Total Loss: 0.005657 | Recon Loss: 0.004552 | Commit Loss: 0.002209 | Perplexity: 949.251668
2025-10-02 19:46:05,126 Stage: Train 0.5 | Epoch: 268 | Iter: 202400 | Total Loss: 0.005743 | Recon Loss: 0.004627 | Commit Loss: 0.002232 | Perplexity: 949.476413
Trainning Epoch:  40%|████      | 269/665 [54:59:34<65:33:54, 596.05s/it]2025-10-02 19:48:48,228 Stage: Train 0.5 | Epoch: 269 | Iter: 202600 | Total Loss: 0.005652 | Recon Loss: 0.004546 | Commit Loss: 0.002211 | Perplexity: 947.440191
2025-10-02 19:51:23,203 Stage: Train 0.5 | Epoch: 269 | Iter: 202800 | Total Loss: 0.005599 | Recon Loss: 0.004503 | Commit Loss: 0.002192 | Perplexity: 942.548591
2025-10-02 19:54:00,322 Stage: Train 0.5 | Epoch: 269 | Iter: 203000 | Total Loss: 0.005704 | Recon Loss: 0.004595 | Commit Loss: 0.002217 | Perplexity: 946.761339
2025-10-02 19:56:37,415 Stage: Train 0.5 | Epoch: 269 | Iter: 203200 | Total Loss: 0.005711 | Recon Loss: 0.004601 | Commit Loss: 0.002220 | Perplexity: 951.399576
Trainning Epoch:  41%|████      | 270/665 [55:09:29<65:21:56, 595.74s/it]2025-10-02 19:59:20,559 Stage: Train 0.5 | Epoch: 270 | Iter: 203400 | Total Loss: 0.005766 | Recon Loss: 0.004665 | Commit Loss: 0.002202 | Perplexity: 944.672575
2025-10-02 20:02:00,698 Stage: Train 0.5 | Epoch: 270 | Iter: 203600 | Total Loss: 0.005622 | Recon Loss: 0.004523 | Commit Loss: 0.002199 | Perplexity: 945.593112
2025-10-02 20:04:37,438 Stage: Train 0.5 | Epoch: 270 | Iter: 203800 | Total Loss: 0.005623 | Recon Loss: 0.004532 | Commit Loss: 0.002182 | Perplexity: 944.454379
2025-10-02 20:07:12,113 Stage: Train 0.5 | Epoch: 270 | Iter: 204000 | Total Loss: 0.005661 | Recon Loss: 0.004561 | Commit Loss: 0.002199 | Perplexity: 949.340196
Trainning Epoch:  41%|████      | 271/665 [55:19:27<65:16:02, 596.35s/it]2025-10-02 20:09:55,120 Stage: Train 0.5 | Epoch: 271 | Iter: 204200 | Total Loss: 0.005643 | Recon Loss: 0.004541 | Commit Loss: 0.002203 | Perplexity: 948.358396
2025-10-02 20:12:29,876 Stage: Train 0.5 | Epoch: 271 | Iter: 204400 | Total Loss: 0.005653 | Recon Loss: 0.004540 | Commit Loss: 0.002226 | Perplexity: 952.459299
2025-10-02 20:15:07,986 Stage: Train 0.5 | Epoch: 271 | Iter: 204600 | Total Loss: 0.005691 | Recon Loss: 0.004584 | Commit Loss: 0.002214 | Perplexity: 956.444690
2025-10-02 20:17:42,774 Stage: Train 0.5 | Epoch: 271 | Iter: 204800 | Total Loss: 0.005639 | Recon Loss: 0.004536 | Commit Loss: 0.002205 | Perplexity: 955.518653
Trainning Epoch:  41%|████      | 272/665 [55:29:22<65:03:43, 595.99s/it]2025-10-02 20:20:26,008 Stage: Train 0.5 | Epoch: 272 | Iter: 205000 | Total Loss: 0.005659 | Recon Loss: 0.004569 | Commit Loss: 0.002180 | Perplexity: 946.164220
2025-10-02 20:23:01,615 Stage: Train 0.5 | Epoch: 272 | Iter: 205200 | Total Loss: 0.005670 | Recon Loss: 0.004560 | Commit Loss: 0.002221 | Perplexity: 948.864658
2025-10-02 20:25:41,060 Stage: Train 0.5 | Epoch: 272 | Iter: 205400 | Total Loss: 0.005633 | Recon Loss: 0.004536 | Commit Loss: 0.002194 | Perplexity: 944.209458
Trainning Epoch:  41%|████      | 273/665 [55:39:20<64:56:40, 596.43s/it]2025-10-02 20:28:23,767 Stage: Train 0.5 | Epoch: 273 | Iter: 205600 | Total Loss: 0.005713 | Recon Loss: 0.004607 | Commit Loss: 0.002213 | Perplexity: 948.595840
2025-10-02 20:31:00,192 Stage: Train 0.5 | Epoch: 273 | Iter: 205800 | Total Loss: 0.005666 | Recon Loss: 0.004559 | Commit Loss: 0.002212 | Perplexity: 945.502813
2025-10-02 20:33:33,718 Stage: Train 0.5 | Epoch: 273 | Iter: 206000 | Total Loss: 0.005654 | Recon Loss: 0.004560 | Commit Loss: 0.002187 | Perplexity: 949.521642
2025-10-02 20:36:09,355 Stage: Train 0.5 | Epoch: 273 | Iter: 206200 | Total Loss: 0.005574 | Recon Loss: 0.004467 | Commit Loss: 0.002213 | Perplexity: 959.179750
Trainning Epoch:  41%|████      | 274/665 [55:49:12<64:38:30, 595.17s/it]2025-10-02 20:38:52,879 Stage: Train 0.5 | Epoch: 274 | Iter: 206400 | Total Loss: 0.005643 | Recon Loss: 0.004540 | Commit Loss: 0.002206 | Perplexity: 953.748985
2025-10-02 20:41:26,771 Stage: Train 0.5 | Epoch: 274 | Iter: 206600 | Total Loss: 0.005610 | Recon Loss: 0.004511 | Commit Loss: 0.002198 | Perplexity: 942.678760
2025-10-02 20:44:04,428 Stage: Train 0.5 | Epoch: 274 | Iter: 206800 | Total Loss: 0.005658 | Recon Loss: 0.004554 | Commit Loss: 0.002209 | Perplexity: 950.464009
2025-10-02 20:46:39,716 Stage: Train 0.5 | Epoch: 274 | Iter: 207000 | Total Loss: 0.005636 | Recon Loss: 0.004534 | Commit Loss: 0.002203 | Perplexity: 949.068060
Trainning Epoch:  41%|████▏     | 275/665 [55:59:04<64:22:17, 594.20s/it]2025-10-02 20:49:22,835 Stage: Train 0.5 | Epoch: 275 | Iter: 207200 | Total Loss: 0.005612 | Recon Loss: 0.004510 | Commit Loss: 0.002205 | Perplexity: 943.638379
2025-10-02 20:51:58,730 Stage: Train 0.5 | Epoch: 275 | Iter: 207400 | Total Loss: 0.005662 | Recon Loss: 0.004551 | Commit Loss: 0.002221 | Perplexity: 953.848943
2025-10-02 20:54:34,686 Stage: Train 0.5 | Epoch: 275 | Iter: 207600 | Total Loss: 0.005661 | Recon Loss: 0.004564 | Commit Loss: 0.002193 | Perplexity: 950.631825
2025-10-02 20:57:09,247 Stage: Train 0.5 | Epoch: 275 | Iter: 207800 | Total Loss: 0.005573 | Recon Loss: 0.004480 | Commit Loss: 0.002187 | Perplexity: 949.953698
Trainning Epoch:  42%|████▏     | 276/665 [56:08:58<64:11:34, 594.07s/it]2025-10-02 20:59:52,714 Stage: Train 0.5 | Epoch: 276 | Iter: 208000 | Total Loss: 0.005567 | Recon Loss: 0.004483 | Commit Loss: 0.002169 | Perplexity: 948.161327
2025-10-02 21:02:27,976 Stage: Train 0.5 | Epoch: 276 | Iter: 208200 | Total Loss: 0.005624 | Recon Loss: 0.004516 | Commit Loss: 0.002217 | Perplexity: 951.328165
2025-10-02 21:05:04,299 Stage: Train 0.5 | Epoch: 276 | Iter: 208400 | Total Loss: 0.005597 | Recon Loss: 0.004493 | Commit Loss: 0.002209 | Perplexity: 946.342607
Trainning Epoch:  42%|████▏     | 277/665 [56:18:50<63:59:06, 593.68s/it]2025-10-02 21:07:45,842 Stage: Train 0.5 | Epoch: 277 | Iter: 208600 | Total Loss: 0.005687 | Recon Loss: 0.004580 | Commit Loss: 0.002212 | Perplexity: 949.748712
2025-10-02 21:10:21,451 Stage: Train 0.5 | Epoch: 277 | Iter: 208800 | Total Loss: 0.005629 | Recon Loss: 0.004536 | Commit Loss: 0.002185 | Perplexity: 952.959856
2025-10-02 21:12:57,154 Stage: Train 0.5 | Epoch: 277 | Iter: 209000 | Total Loss: 0.005678 | Recon Loss: 0.004581 | Commit Loss: 0.002195 | Perplexity: 953.391522
2025-10-02 21:15:35,573 Stage: Train 0.5 | Epoch: 277 | Iter: 209200 | Total Loss: 0.005586 | Recon Loss: 0.004477 | Commit Loss: 0.002219 | Perplexity: 955.535227
Trainning Epoch:  42%|████▏     | 278/665 [56:28:44<63:48:00, 593.49s/it]2025-10-02 21:18:14,938 Stage: Train 0.5 | Epoch: 278 | Iter: 209400 | Total Loss: 0.005651 | Recon Loss: 0.004553 | Commit Loss: 0.002196 | Perplexity: 956.713687
2025-10-02 21:20:52,397 Stage: Train 0.5 | Epoch: 278 | Iter: 209600 | Total Loss: 0.005591 | Recon Loss: 0.004500 | Commit Loss: 0.002181 | Perplexity: 946.521947
2025-10-02 21:23:28,290 Stage: Train 0.5 | Epoch: 278 | Iter: 209800 | Total Loss: 0.005593 | Recon Loss: 0.004492 | Commit Loss: 0.002202 | Perplexity: 960.754362
2025-10-02 21:26:05,780 Stage: Train 0.5 | Epoch: 278 | Iter: 210000 | Total Loss: 0.005649 | Recon Loss: 0.004538 | Commit Loss: 0.002223 | Perplexity: 961.801757
Trainning Epoch:  42%|████▏     | 279/665 [56:38:39<63:41:04, 593.95s/it]2025-10-02 21:28:45,471 Stage: Train 0.5 | Epoch: 279 | Iter: 210200 | Total Loss: 0.005583 | Recon Loss: 0.004498 | Commit Loss: 0.002171 | Perplexity: 946.171817
2025-10-02 21:31:22,943 Stage: Train 0.5 | Epoch: 279 | Iter: 210400 | Total Loss: 0.005599 | Recon Loss: 0.004508 | Commit Loss: 0.002183 | Perplexity: 953.835038
2025-10-02 21:33:59,270 Stage: Train 0.5 | Epoch: 279 | Iter: 210600 | Total Loss: 0.005701 | Recon Loss: 0.004589 | Commit Loss: 0.002223 | Perplexity: 953.773107
2025-10-02 21:36:36,603 Stage: Train 0.5 | Epoch: 279 | Iter: 210800 | Total Loss: 0.005641 | Recon Loss: 0.004538 | Commit Loss: 0.002206 | Perplexity: 960.187547
Trainning Epoch:  42%|████▏     | 280/665 [56:48:34<63:34:30, 594.47s/it]2025-10-02 21:39:19,390 Stage: Train 0.5 | Epoch: 280 | Iter: 211000 | Total Loss: 0.005601 | Recon Loss: 0.004506 | Commit Loss: 0.002189 | Perplexity: 954.992490
2025-10-02 21:41:57,567 Stage: Train 0.5 | Epoch: 280 | Iter: 211200 | Total Loss: 0.005603 | Recon Loss: 0.004505 | Commit Loss: 0.002196 | Perplexity: 955.531821
2025-10-02 21:44:31,079 Stage: Train 0.5 | Epoch: 280 | Iter: 211400 | Total Loss: 0.005617 | Recon Loss: 0.004523 | Commit Loss: 0.002188 | Perplexity: 947.584562
Trainning Epoch:  42%|████▏     | 281/665 [56:58:27<63:20:30, 593.83s/it]2025-10-02 21:47:10,055 Stage: Train 0.5 | Epoch: 281 | Iter: 211600 | Total Loss: 0.005647 | Recon Loss: 0.004539 | Commit Loss: 0.002215 | Perplexity: 953.015493
2025-10-02 21:49:47,942 Stage: Train 0.5 | Epoch: 281 | Iter: 211800 | Total Loss: 0.005564 | Recon Loss: 0.004467 | Commit Loss: 0.002194 | Perplexity: 952.895046
2025-10-02 21:52:25,108 Stage: Train 0.5 | Epoch: 281 | Iter: 212000 | Total Loss: 0.005630 | Recon Loss: 0.004536 | Commit Loss: 0.002189 | Perplexity: 954.059631
2025-10-02 21:54:58,299 Stage: Train 0.5 | Epoch: 281 | Iter: 212200 | Total Loss: 0.005593 | Recon Loss: 0.004501 | Commit Loss: 0.002185 | Perplexity: 956.609050
Trainning Epoch:  42%|████▏     | 282/665 [57:08:20<63:10:42, 593.84s/it]2025-10-02 21:57:42,216 Stage: Train 0.5 | Epoch: 282 | Iter: 212400 | Total Loss: 0.005617 | Recon Loss: 0.004516 | Commit Loss: 0.002203 | Perplexity: 956.676175
2025-10-02 22:00:19,323 Stage: Train 0.5 | Epoch: 282 | Iter: 212600 | Total Loss: 0.005621 | Recon Loss: 0.004526 | Commit Loss: 0.002189 | Perplexity: 956.755991
2025-10-02 22:02:56,805 Stage: Train 0.5 | Epoch: 282 | Iter: 212800 | Total Loss: 0.005552 | Recon Loss: 0.004446 | Commit Loss: 0.002213 | Perplexity: 959.132161
2025-10-02 22:05:33,853 Stage: Train 0.5 | Epoch: 282 | Iter: 213000 | Total Loss: 0.005614 | Recon Loss: 0.004525 | Commit Loss: 0.002178 | Perplexity: 953.562103
Trainning Epoch:  43%|████▎     | 283/665 [57:18:17<63:05:44, 594.62s/it]2025-10-02 22:08:16,099 Stage: Train 0.5 | Epoch: 283 | Iter: 213200 | Total Loss: 0.005540 | Recon Loss: 0.004458 | Commit Loss: 0.002165 | Perplexity: 944.710547
2025-10-02 22:10:52,795 Stage: Train 0.5 | Epoch: 283 | Iter: 213400 | Total Loss: 0.005635 | Recon Loss: 0.004541 | Commit Loss: 0.002187 | Perplexity: 950.368665
2025-10-02 22:13:26,907 Stage: Train 0.5 | Epoch: 283 | Iter: 213600 | Total Loss: 0.005638 | Recon Loss: 0.004538 | Commit Loss: 0.002201 | Perplexity: 952.790839
2025-10-02 22:16:02,549 Stage: Train 0.5 | Epoch: 283 | Iter: 213800 | Total Loss: 0.005639 | Recon Loss: 0.004539 | Commit Loss: 0.002199 | Perplexity: 951.058072
Trainning Epoch:  43%|████▎     | 284/665 [57:28:10<62:53:07, 594.19s/it]2025-10-02 22:18:45,531 Stage: Train 0.5 | Epoch: 284 | Iter: 214000 | Total Loss: 0.005588 | Recon Loss: 0.004488 | Commit Loss: 0.002200 | Perplexity: 950.992533
2025-10-02 22:21:21,818 Stage: Train 0.5 | Epoch: 284 | Iter: 214200 | Total Loss: 0.005548 | Recon Loss: 0.004454 | Commit Loss: 0.002189 | Perplexity: 950.548993
2025-10-02 22:23:55,853 Stage: Train 0.5 | Epoch: 284 | Iter: 214400 | Total Loss: 0.005553 | Recon Loss: 0.004451 | Commit Loss: 0.002204 | Perplexity: 954.748426
2025-10-02 22:26:31,337 Stage: Train 0.5 | Epoch: 284 | Iter: 214600 | Total Loss: 0.005622 | Recon Loss: 0.004524 | Commit Loss: 0.002197 | Perplexity: 954.561666
Trainning Epoch:  43%|████▎     | 285/665 [57:38:04<62:42:48, 594.13s/it]2025-10-02 22:29:16,134 Stage: Train 0.5 | Epoch: 285 | Iter: 214800 | Total Loss: 0.005538 | Recon Loss: 0.004444 | Commit Loss: 0.002188 | Perplexity: 953.596718
2025-10-02 22:31:53,535 Stage: Train 0.5 | Epoch: 285 | Iter: 215000 | Total Loss: 0.005535 | Recon Loss: 0.004447 | Commit Loss: 0.002175 | Perplexity: 950.204736
2025-10-02 22:34:33,919 Stage: Train 0.5 | Epoch: 285 | Iter: 215200 | Total Loss: 0.005593 | Recon Loss: 0.004491 | Commit Loss: 0.002205 | Perplexity: 955.556136
Trainning Epoch:  43%|████▎     | 286/665 [57:48:04<62:44:21, 595.94s/it]2025-10-02 22:37:18,937 Stage: Train 0.5 | Epoch: 286 | Iter: 215400 | Total Loss: 0.005594 | Recon Loss: 0.004496 | Commit Loss: 0.002197 | Perplexity: 955.017073
2025-10-02 22:39:54,896 Stage: Train 0.5 | Epoch: 286 | Iter: 215600 | Total Loss: 0.005585 | Recon Loss: 0.004495 | Commit Loss: 0.002179 | Perplexity: 957.288503
2025-10-02 22:42:30,234 Stage: Train 0.5 | Epoch: 286 | Iter: 215800 | Total Loss: 0.005573 | Recon Loss: 0.004484 | Commit Loss: 0.002179 | Perplexity: 952.622603
2025-10-02 22:45:08,160 Stage: Train 0.5 | Epoch: 286 | Iter: 216000 | Total Loss: 0.005589 | Recon Loss: 0.004480 | Commit Loss: 0.002219 | Perplexity: 950.644431
Trainning Epoch:  43%|████▎     | 287/665 [57:58:03<62:40:10, 596.85s/it]2025-10-02 22:47:53,863 Stage: Train 0.5 | Epoch: 287 | Iter: 216200 | Total Loss: 0.005565 | Recon Loss: 0.004478 | Commit Loss: 0.002173 | Perplexity: 955.086160
2025-10-02 22:50:28,100 Stage: Train 0.5 | Epoch: 287 | Iter: 216400 | Total Loss: 0.005554 | Recon Loss: 0.004468 | Commit Loss: 0.002171 | Perplexity: 948.283033
2025-10-02 22:53:01,819 Stage: Train 0.5 | Epoch: 287 | Iter: 216600 | Total Loss: 0.005557 | Recon Loss: 0.004461 | Commit Loss: 0.002193 | Perplexity: 957.228240
2025-10-02 22:55:38,445 Stage: Train 0.5 | Epoch: 287 | Iter: 216800 | Total Loss: 0.005627 | Recon Loss: 0.004545 | Commit Loss: 0.002164 | Perplexity: 956.258049
Trainning Epoch:  43%|████▎     | 288/665 [58:07:53<62:17:43, 594.86s/it]2025-10-02 22:58:23,332 Stage: Train 0.5 | Epoch: 288 | Iter: 217000 | Total Loss: 0.005584 | Recon Loss: 0.004491 | Commit Loss: 0.002186 | Perplexity: 956.090647
2025-10-02 23:00:58,630 Stage: Train 0.5 | Epoch: 288 | Iter: 217200 | Total Loss: 0.005599 | Recon Loss: 0.004502 | Commit Loss: 0.002193 | Perplexity: 956.369139
2025-10-02 23:03:32,164 Stage: Train 0.5 | Epoch: 288 | Iter: 217400 | Total Loss: 0.005536 | Recon Loss: 0.004450 | Commit Loss: 0.002171 | Perplexity: 955.017689
2025-10-02 23:06:06,583 Stage: Train 0.5 | Epoch: 288 | Iter: 217600 | Total Loss: 0.005573 | Recon Loss: 0.004478 | Commit Loss: 0.002189 | Perplexity: 953.301342
Trainning Epoch:  43%|████▎     | 289/665 [58:17:46<62:03:29, 594.17s/it]2025-10-02 23:08:48,334 Stage: Train 0.5 | Epoch: 289 | Iter: 217800 | Total Loss: 0.005560 | Recon Loss: 0.004484 | Commit Loss: 0.002151 | Perplexity: 952.252413
2025-10-02 23:11:27,095 Stage: Train 0.5 | Epoch: 289 | Iter: 218000 | Total Loss: 0.005552 | Recon Loss: 0.004454 | Commit Loss: 0.002196 | Perplexity: 959.664777
2025-10-02 23:14:02,484 Stage: Train 0.5 | Epoch: 289 | Iter: 218200 | Total Loss: 0.005525 | Recon Loss: 0.004434 | Commit Loss: 0.002183 | Perplexity: 951.440594
Trainning Epoch:  44%|████▎     | 290/665 [58:27:40<61:53:46, 594.20s/it]2025-10-02 23:16:43,379 Stage: Train 0.5 | Epoch: 290 | Iter: 218400 | Total Loss: 0.005572 | Recon Loss: 0.004471 | Commit Loss: 0.002203 | Perplexity: 953.228913
2025-10-02 23:19:21,219 Stage: Train 0.5 | Epoch: 290 | Iter: 218600 | Total Loss: 0.005559 | Recon Loss: 0.004472 | Commit Loss: 0.002174 | Perplexity: 957.369055
2025-10-02 23:21:55,327 Stage: Train 0.5 | Epoch: 290 | Iter: 218800 | Total Loss: 0.005519 | Recon Loss: 0.004438 | Commit Loss: 0.002163 | Perplexity: 955.642478
2025-10-02 23:24:29,506 Stage: Train 0.5 | Epoch: 290 | Iter: 219000 | Total Loss: 0.005560 | Recon Loss: 0.004462 | Commit Loss: 0.002196 | Perplexity: 953.463414
Trainning Epoch:  44%|████▍     | 291/665 [58:37:33<61:40:59, 593.74s/it]2025-10-02 23:27:14,055 Stage: Train 0.5 | Epoch: 291 | Iter: 219200 | Total Loss: 0.005552 | Recon Loss: 0.004459 | Commit Loss: 0.002185 | Perplexity: 949.614477
2025-10-02 23:29:48,853 Stage: Train 0.5 | Epoch: 291 | Iter: 219400 | Total Loss: 0.005547 | Recon Loss: 0.004453 | Commit Loss: 0.002188 | Perplexity: 951.218939
2025-10-02 23:32:25,286 Stage: Train 0.5 | Epoch: 291 | Iter: 219600 | Total Loss: 0.005487 | Recon Loss: 0.004398 | Commit Loss: 0.002177 | Perplexity: 950.762080
2025-10-02 23:35:02,109 Stage: Train 0.5 | Epoch: 291 | Iter: 219800 | Total Loss: 0.005541 | Recon Loss: 0.004444 | Commit Loss: 0.002195 | Perplexity: 954.242527
Trainning Epoch:  44%|████▍     | 292/665 [58:47:27<61:32:18, 593.94s/it]2025-10-02 23:37:43,430 Stage: Train 0.5 | Epoch: 292 | Iter: 220000 | Total Loss: 0.005572 | Recon Loss: 0.004483 | Commit Loss: 0.002177 | Perplexity: 957.243080
2025-10-02 23:37:43,430 Saving model at iteration 220000
2025-10-02 23:37:43,627 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000
2025-10-02 23:37:43,924 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000/model.safetensors
2025-10-02 23:37:44,348 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000/optimizer.bin
2025-10-02 23:37:44,348 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000/scheduler.bin
2025-10-02 23:37:44,348 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000/sampler.bin
2025-10-02 23:37:44,349 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_293_step_220000/random_states_0.pkl
2025-10-02 23:40:21,174 Stage: Train 0.5 | Epoch: 292 | Iter: 220200 | Total Loss: 0.005535 | Recon Loss: 0.004453 | Commit Loss: 0.002165 | Perplexity: 953.345758
2025-10-02 23:43:00,681 Stage: Train 0.5 | Epoch: 292 | Iter: 220400 | Total Loss: 0.005557 | Recon Loss: 0.004475 | Commit Loss: 0.002165 | Perplexity: 955.243579
2025-10-02 23:45:37,742 Stage: Train 0.5 | Epoch: 292 | Iter: 220600 | Total Loss: 0.005524 | Recon Loss: 0.004433 | Commit Loss: 0.002181 | Perplexity: 958.953518
Trainning Epoch:  44%|████▍     | 293/665 [58:57:26<61:30:59, 595.32s/it]2025-10-02 23:48:21,535 Stage: Train 0.5 | Epoch: 293 | Iter: 220800 | Total Loss: 0.005535 | Recon Loss: 0.004451 | Commit Loss: 0.002167 | Perplexity: 951.294120
2025-10-02 23:50:58,426 Stage: Train 0.5 | Epoch: 293 | Iter: 221000 | Total Loss: 0.005510 | Recon Loss: 0.004427 | Commit Loss: 0.002166 | Perplexity: 958.393486
2025-10-02 23:53:34,079 Stage: Train 0.5 | Epoch: 293 | Iter: 221200 | Total Loss: 0.005601 | Recon Loss: 0.004505 | Commit Loss: 0.002192 | Perplexity: 951.232925
Trainning Epoch:  44%|████▍     | 294/665 [59:07:21<61:20:10, 595.18s/it]2025-10-02 23:56:16,081 Stage: Train 0.5 | Epoch: 294 | Iter: 221400 | Total Loss: 0.005537 | Recon Loss: 0.004443 | Commit Loss: 0.002188 | Perplexity: 954.898704
2025-10-02 23:58:52,476 Stage: Train 0.5 | Epoch: 294 | Iter: 221600 | Total Loss: 0.005600 | Recon Loss: 0.004513 | Commit Loss: 0.002176 | Perplexity: 952.633173
2025-10-03 00:01:24,600 Stage: Train 0.5 | Epoch: 294 | Iter: 221800 | Total Loss: 0.005477 | Recon Loss: 0.004390 | Commit Loss: 0.002173 | Perplexity: 955.266270
2025-10-03 00:03:58,904 Stage: Train 0.5 | Epoch: 294 | Iter: 222000 | Total Loss: 0.005580 | Recon Loss: 0.004495 | Commit Loss: 0.002169 | Perplexity: 953.913252
Trainning Epoch:  44%|████▍     | 295/665 [59:17:11<61:01:38, 593.78s/it]2025-10-03 00:06:41,868 Stage: Train 0.5 | Epoch: 295 | Iter: 222200 | Total Loss: 0.005530 | Recon Loss: 0.004440 | Commit Loss: 0.002181 | Perplexity: 951.770287
2025-10-03 00:09:18,778 Stage: Train 0.5 | Epoch: 295 | Iter: 222400 | Total Loss: 0.005552 | Recon Loss: 0.004467 | Commit Loss: 0.002171 | Perplexity: 956.614683
2025-10-03 00:11:54,471 Stage: Train 0.5 | Epoch: 295 | Iter: 222600 | Total Loss: 0.005540 | Recon Loss: 0.004458 | Commit Loss: 0.002165 | Perplexity: 945.196375
2025-10-03 00:14:29,972 Stage: Train 0.5 | Epoch: 295 | Iter: 222800 | Total Loss: 0.005560 | Recon Loss: 0.004465 | Commit Loss: 0.002190 | Perplexity: 962.825910
Trainning Epoch:  45%|████▍     | 296/665 [59:27:05<60:52:16, 593.87s/it]2025-10-03 00:17:13,313 Stage: Train 0.5 | Epoch: 296 | Iter: 223000 | Total Loss: 0.005599 | Recon Loss: 0.004520 | Commit Loss: 0.002158 | Perplexity: 953.926822
2025-10-03 00:19:48,345 Stage: Train 0.5 | Epoch: 296 | Iter: 223200 | Total Loss: 0.005463 | Recon Loss: 0.004385 | Commit Loss: 0.002155 | Perplexity: 957.002770
2025-10-03 00:22:23,065 Stage: Train 0.5 | Epoch: 296 | Iter: 223400 | Total Loss: 0.005550 | Recon Loss: 0.004468 | Commit Loss: 0.002165 | Perplexity: 947.456298
2025-10-03 00:25:00,184 Stage: Train 0.5 | Epoch: 296 | Iter: 223600 | Total Loss: 0.005520 | Recon Loss: 0.004432 | Commit Loss: 0.002176 | Perplexity: 951.375941
Trainning Epoch:  45%|████▍     | 297/665 [59:36:59<60:41:31, 593.73s/it]2025-10-03 00:27:42,915 Stage: Train 0.5 | Epoch: 297 | Iter: 223800 | Total Loss: 0.005542 | Recon Loss: 0.004452 | Commit Loss: 0.002180 | Perplexity: 954.102723
2025-10-03 00:30:18,686 Stage: Train 0.5 | Epoch: 297 | Iter: 224000 | Total Loss: 0.005548 | Recon Loss: 0.004461 | Commit Loss: 0.002174 | Perplexity: 959.823562
2025-10-03 00:32:54,265 Stage: Train 0.5 | Epoch: 297 | Iter: 224200 | Total Loss: 0.005497 | Recon Loss: 0.004410 | Commit Loss: 0.002174 | Perplexity: 951.567807
Trainning Epoch:  45%|████▍     | 298/665 [59:46:51<60:29:39, 593.41s/it]2025-10-03 00:35:34,747 Stage: Train 0.5 | Epoch: 298 | Iter: 224400 | Total Loss: 0.005517 | Recon Loss: 0.004443 | Commit Loss: 0.002148 | Perplexity: 949.510373
2025-10-03 00:38:14,426 Stage: Train 0.5 | Epoch: 298 | Iter: 224600 | Total Loss: 0.005550 | Recon Loss: 0.004462 | Commit Loss: 0.002176 | Perplexity: 945.251047
2025-10-03 00:40:49,518 Stage: Train 0.5 | Epoch: 298 | Iter: 224800 | Total Loss: 0.005507 | Recon Loss: 0.004426 | Commit Loss: 0.002162 | Perplexity: 957.614908
2025-10-03 00:43:26,643 Stage: Train 0.5 | Epoch: 298 | Iter: 225000 | Total Loss: 0.005521 | Recon Loss: 0.004434 | Commit Loss: 0.002175 | Perplexity: 957.317656
Trainning Epoch:  45%|████▍     | 299/665 [59:56:45<60:20:31, 593.53s/it]2025-10-03 00:46:06,017 Stage: Train 0.5 | Epoch: 299 | Iter: 225200 | Total Loss: 0.005500 | Recon Loss: 0.004419 | Commit Loss: 0.002162 | Perplexity: 957.481465
2025-10-03 00:48:42,149 Stage: Train 0.5 | Epoch: 299 | Iter: 225400 | Total Loss: 0.005529 | Recon Loss: 0.004453 | Commit Loss: 0.002152 | Perplexity: 953.316475
2025-10-03 00:51:16,449 Stage: Train 0.5 | Epoch: 299 | Iter: 225600 | Total Loss: 0.005505 | Recon Loss: 0.004426 | Commit Loss: 0.002160 | Perplexity: 959.195780
2025-10-03 00:53:51,996 Stage: Train 0.5 | Epoch: 299 | Iter: 225800 | Total Loss: 0.005567 | Recon Loss: 0.004477 | Commit Loss: 0.002180 | Perplexity: 949.151483
Trainning Epoch:  45%|████▌     | 300/665 [60:06:36<60:06:32, 592.86s/it]2025-10-03 00:56:34,863 Stage: Train 0.5 | Epoch: 300 | Iter: 226000 | Total Loss: 0.005482 | Recon Loss: 0.004401 | Commit Loss: 0.002163 | Perplexity: 954.606799
2025-10-03 00:59:10,344 Stage: Train 0.5 | Epoch: 300 | Iter: 226200 | Total Loss: 0.005464 | Recon Loss: 0.004376 | Commit Loss: 0.002177 | Perplexity: 953.534841
2025-10-03 01:01:48,419 Stage: Train 0.5 | Epoch: 300 | Iter: 226400 | Total Loss: 0.005514 | Recon Loss: 0.004427 | Commit Loss: 0.002175 | Perplexity: 956.696673
2025-10-03 01:04:22,336 Stage: Train 0.5 | Epoch: 300 | Iter: 226600 | Total Loss: 0.005499 | Recon Loss: 0.004418 | Commit Loss: 0.002163 | Perplexity: 951.728101
Trainning Epoch:  45%|████▌     | 301/665 [60:16:29<59:56:10, 592.77s/it]2025-10-03 01:07:03,727 Stage: Train 0.5 | Epoch: 301 | Iter: 226800 | Total Loss: 0.005533 | Recon Loss: 0.004442 | Commit Loss: 0.002182 | Perplexity: 956.656110
2025-10-03 01:09:36,918 Stage: Train 0.5 | Epoch: 301 | Iter: 227000 | Total Loss: 0.005467 | Recon Loss: 0.004386 | Commit Loss: 0.002163 | Perplexity: 953.769111
2025-10-03 01:12:13,283 Stage: Train 0.5 | Epoch: 301 | Iter: 227200 | Total Loss: 0.005517 | Recon Loss: 0.004439 | Commit Loss: 0.002156 | Perplexity: 950.506029
2025-10-03 01:14:48,648 Stage: Train 0.5 | Epoch: 301 | Iter: 227400 | Total Loss: 0.005425 | Recon Loss: 0.004342 | Commit Loss: 0.002167 | Perplexity: 956.452648
Trainning Epoch:  45%|████▌     | 302/665 [60:26:19<59:42:01, 592.07s/it]2025-10-03 01:17:30,350 Stage: Train 0.5 | Epoch: 302 | Iter: 227600 | Total Loss: 0.005537 | Recon Loss: 0.004452 | Commit Loss: 0.002170 | Perplexity: 954.699349
2025-10-03 01:20:06,992 Stage: Train 0.5 | Epoch: 302 | Iter: 227800 | Total Loss: 0.005437 | Recon Loss: 0.004360 | Commit Loss: 0.002154 | Perplexity: 955.035612
2025-10-03 01:22:42,949 Stage: Train 0.5 | Epoch: 302 | Iter: 228000 | Total Loss: 0.005531 | Recon Loss: 0.004454 | Commit Loss: 0.002153 | Perplexity: 957.980591
Trainning Epoch:  46%|████▌     | 303/665 [60:36:14<59:36:13, 592.75s/it]2025-10-03 01:25:28,025 Stage: Train 0.5 | Epoch: 303 | Iter: 228200 | Total Loss: 0.005511 | Recon Loss: 0.004429 | Commit Loss: 0.002164 | Perplexity: 950.467196
2025-10-03 01:28:01,859 Stage: Train 0.5 | Epoch: 303 | Iter: 228400 | Total Loss: 0.005494 | Recon Loss: 0.004405 | Commit Loss: 0.002179 | Perplexity: 959.342920
2025-10-03 01:30:35,765 Stage: Train 0.5 | Epoch: 303 | Iter: 228600 | Total Loss: 0.005483 | Recon Loss: 0.004403 | Commit Loss: 0.002161 | Perplexity: 958.198774
2025-10-03 01:33:12,667 Stage: Train 0.5 | Epoch: 303 | Iter: 228800 | Total Loss: 0.005523 | Recon Loss: 0.004438 | Commit Loss: 0.002171 | Perplexity: 953.033640
Trainning Epoch:  46%|████▌     | 304/665 [60:46:08<59:29:35, 593.29s/it]2025-10-03 01:35:56,819 Stage: Train 0.5 | Epoch: 304 | Iter: 229000 | Total Loss: 0.005574 | Recon Loss: 0.004494 | Commit Loss: 0.002160 | Perplexity: 950.989428
2025-10-03 01:38:31,875 Stage: Train 0.5 | Epoch: 304 | Iter: 229200 | Total Loss: 0.005448 | Recon Loss: 0.004365 | Commit Loss: 0.002166 | Perplexity: 953.579699
2025-10-03 01:41:08,653 Stage: Train 0.5 | Epoch: 304 | Iter: 229400 | Total Loss: 0.005511 | Recon Loss: 0.004432 | Commit Loss: 0.002158 | Perplexity: 961.118929
2025-10-03 01:43:41,333 Stage: Train 0.5 | Epoch: 304 | Iter: 229600 | Total Loss: 0.005554 | Recon Loss: 0.004471 | Commit Loss: 0.002165 | Perplexity: 957.139520
Trainning Epoch:  46%|████▌     | 305/665 [60:55:58<59:14:09, 592.36s/it]2025-10-03 01:46:24,568 Stage: Train 0.5 | Epoch: 305 | Iter: 229800 | Total Loss: 0.005461 | Recon Loss: 0.004387 | Commit Loss: 0.002147 | Perplexity: 955.144762
2025-10-03 01:49:02,038 Stage: Train 0.5 | Epoch: 305 | Iter: 230000 | Total Loss: 0.005452 | Recon Loss: 0.004380 | Commit Loss: 0.002145 | Perplexity: 949.743793
2025-10-03 01:51:38,352 Stage: Train 0.5 | Epoch: 305 | Iter: 230200 | Total Loss: 0.005479 | Recon Loss: 0.004391 | Commit Loss: 0.002175 | Perplexity: 953.974124
2025-10-03 01:54:17,630 Stage: Train 0.5 | Epoch: 305 | Iter: 230400 | Total Loss: 0.005531 | Recon Loss: 0.004448 | Commit Loss: 0.002166 | Perplexity: 954.969721
Trainning Epoch:  46%|████▌     | 306/665 [61:05:57<59:15:56, 594.31s/it]2025-10-03 01:57:00,767 Stage: Train 0.5 | Epoch: 306 | Iter: 230600 | Total Loss: 0.005466 | Recon Loss: 0.004378 | Commit Loss: 0.002175 | Perplexity: 956.427231
2025-10-03 01:59:38,509 Stage: Train 0.5 | Epoch: 306 | Iter: 230800 | Total Loss: 0.005501 | Recon Loss: 0.004422 | Commit Loss: 0.002157 | Perplexity: 955.921362
2025-10-03 02:02:17,719 Stage: Train 0.5 | Epoch: 306 | Iter: 231000 | Total Loss: 0.005468 | Recon Loss: 0.004393 | Commit Loss: 0.002149 | Perplexity: 952.739866
Trainning Epoch:  46%|████▌     | 307/665 [61:15:57<59:15:43, 595.93s/it]2025-10-03 02:04:59,659 Stage: Train 0.5 | Epoch: 307 | Iter: 231200 | Total Loss: 0.005508 | Recon Loss: 0.004426 | Commit Loss: 0.002164 | Perplexity: 957.094576
2025-10-03 02:07:34,513 Stage: Train 0.5 | Epoch: 307 | Iter: 231400 | Total Loss: 0.005485 | Recon Loss: 0.004408 | Commit Loss: 0.002155 | Perplexity: 959.783401
2025-10-03 02:10:10,467 Stage: Train 0.5 | Epoch: 307 | Iter: 231600 | Total Loss: 0.005502 | Recon Loss: 0.004428 | Commit Loss: 0.002148 | Perplexity: 955.176920
2025-10-03 02:12:46,245 Stage: Train 0.5 | Epoch: 307 | Iter: 231800 | Total Loss: 0.005456 | Recon Loss: 0.004374 | Commit Loss: 0.002166 | Perplexity: 961.496140
Trainning Epoch:  46%|████▋     | 308/665 [61:25:49<58:58:11, 594.65s/it]2025-10-03 02:15:28,471 Stage: Train 0.5 | Epoch: 308 | Iter: 232000 | Total Loss: 0.005483 | Recon Loss: 0.004408 | Commit Loss: 0.002150 | Perplexity: 953.863072
2025-10-03 02:18:05,855 Stage: Train 0.5 | Epoch: 308 | Iter: 232200 | Total Loss: 0.005432 | Recon Loss: 0.004357 | Commit Loss: 0.002149 | Perplexity: 957.729071
2025-10-03 02:20:42,371 Stage: Train 0.5 | Epoch: 308 | Iter: 232400 | Total Loss: 0.005416 | Recon Loss: 0.004333 | Commit Loss: 0.002167 | Perplexity: 955.221283
2025-10-03 02:23:17,074 Stage: Train 0.5 | Epoch: 308 | Iter: 232600 | Total Loss: 0.005500 | Recon Loss: 0.004418 | Commit Loss: 0.002165 | Perplexity: 954.384449
Trainning Epoch:  46%|████▋     | 309/665 [61:35:42<58:46:36, 594.37s/it]2025-10-03 02:25:58,685 Stage: Train 0.5 | Epoch: 309 | Iter: 232800 | Total Loss: 0.005495 | Recon Loss: 0.004418 | Commit Loss: 0.002155 | Perplexity: 958.384068
2025-10-03 02:28:33,971 Stage: Train 0.5 | Epoch: 309 | Iter: 233000 | Total Loss: 0.005504 | Recon Loss: 0.004425 | Commit Loss: 0.002158 | Perplexity: 959.467160
2025-10-03 02:31:10,859 Stage: Train 0.5 | Epoch: 309 | Iter: 233200 | Total Loss: 0.005507 | Recon Loss: 0.004432 | Commit Loss: 0.002150 | Perplexity: 944.840160
2025-10-03 02:33:46,723 Stage: Train 0.5 | Epoch: 309 | Iter: 233400 | Total Loss: 0.005482 | Recon Loss: 0.004408 | Commit Loss: 0.002148 | Perplexity: 952.177475
Trainning Epoch:  47%|████▋     | 310/665 [61:45:38<58:38:16, 594.64s/it]2025-10-03 02:36:30,497 Stage: Train 0.5 | Epoch: 310 | Iter: 233600 | Total Loss: 0.005439 | Recon Loss: 0.004367 | Commit Loss: 0.002145 | Perplexity: 953.837083
2025-10-03 02:39:03,238 Stage: Train 0.5 | Epoch: 310 | Iter: 233800 | Total Loss: 0.005472 | Recon Loss: 0.004395 | Commit Loss: 0.002154 | Perplexity: 959.614855
2025-10-03 02:41:41,133 Stage: Train 0.5 | Epoch: 310 | Iter: 234000 | Total Loss: 0.005425 | Recon Loss: 0.004352 | Commit Loss: 0.002147 | Perplexity: 952.042448
Trainning Epoch:  47%|████▋     | 311/665 [61:55:30<58:23:24, 593.80s/it]2025-10-03 02:44:23,333 Stage: Train 0.5 | Epoch: 311 | Iter: 234200 | Total Loss: 0.005504 | Recon Loss: 0.004421 | Commit Loss: 0.002167 | Perplexity: 958.537010
2025-10-03 02:46:58,555 Stage: Train 0.5 | Epoch: 311 | Iter: 234400 | Total Loss: 0.005377 | Recon Loss: 0.004302 | Commit Loss: 0.002150 | Perplexity: 961.136957
2025-10-03 02:49:33,670 Stage: Train 0.5 | Epoch: 311 | Iter: 234600 | Total Loss: 0.005396 | Recon Loss: 0.004318 | Commit Loss: 0.002155 | Perplexity: 961.555826
2025-10-03 02:52:10,992 Stage: Train 0.5 | Epoch: 311 | Iter: 234800 | Total Loss: 0.005475 | Recon Loss: 0.004404 | Commit Loss: 0.002142 | Perplexity: 954.278580
Trainning Epoch:  47%|████▋     | 312/665 [62:05:22<58:11:07, 593.39s/it]2025-10-03 02:54:51,682 Stage: Train 0.5 | Epoch: 312 | Iter: 235000 | Total Loss: 0.005471 | Recon Loss: 0.004391 | Commit Loss: 0.002158 | Perplexity: 955.021493
2025-10-03 02:57:26,718 Stage: Train 0.5 | Epoch: 312 | Iter: 235200 | Total Loss: 0.005451 | Recon Loss: 0.004373 | Commit Loss: 0.002156 | Perplexity: 953.881328
2025-10-03 03:00:03,432 Stage: Train 0.5 | Epoch: 312 | Iter: 235400 | Total Loss: 0.005459 | Recon Loss: 0.004375 | Commit Loss: 0.002169 | Perplexity: 959.679485
2025-10-03 03:02:36,305 Stage: Train 0.5 | Epoch: 312 | Iter: 235600 | Total Loss: 0.005523 | Recon Loss: 0.004461 | Commit Loss: 0.002124 | Perplexity: 949.388155
Trainning Epoch:  47%|████▋     | 313/665 [62:15:13<57:56:44, 592.63s/it]2025-10-03 03:05:19,723 Stage: Train 0.5 | Epoch: 313 | Iter: 235800 | Total Loss: 0.005431 | Recon Loss: 0.004362 | Commit Loss: 0.002137 | Perplexity: 956.695465
2025-10-03 03:07:55,822 Stage: Train 0.5 | Epoch: 313 | Iter: 236000 | Total Loss: 0.005412 | Recon Loss: 0.004350 | Commit Loss: 0.002124 | Perplexity: 955.637172
2025-10-03 03:10:30,891 Stage: Train 0.5 | Epoch: 313 | Iter: 236200 | Total Loss: 0.005421 | Recon Loss: 0.004345 | Commit Loss: 0.002151 | Perplexity: 960.264962
2025-10-03 03:13:09,558 Stage: Train 0.5 | Epoch: 313 | Iter: 236400 | Total Loss: 0.005461 | Recon Loss: 0.004376 | Commit Loss: 0.002171 | Perplexity: 958.611779
Trainning Epoch:  47%|████▋     | 314/665 [62:25:08<57:50:50, 593.31s/it]2025-10-03 03:15:51,457 Stage: Train 0.5 | Epoch: 314 | Iter: 236600 | Total Loss: 0.005482 | Recon Loss: 0.004404 | Commit Loss: 0.002155 | Perplexity: 959.333058
2025-10-03 03:18:27,188 Stage: Train 0.5 | Epoch: 314 | Iter: 236800 | Total Loss: 0.005415 | Recon Loss: 0.004335 | Commit Loss: 0.002160 | Perplexity: 957.473923
2025-10-03 03:21:03,458 Stage: Train 0.5 | Epoch: 314 | Iter: 237000 | Total Loss: 0.005425 | Recon Loss: 0.004357 | Commit Loss: 0.002136 | Perplexity: 954.406959
Trainning Epoch:  47%|████▋     | 315/665 [62:35:04<57:45:24, 594.07s/it]2025-10-03 03:23:46,064 Stage: Train 0.5 | Epoch: 315 | Iter: 237200 | Total Loss: 0.005416 | Recon Loss: 0.004342 | Commit Loss: 0.002146 | Perplexity: 959.725245
2025-10-03 03:26:24,203 Stage: Train 0.5 | Epoch: 315 | Iter: 237400 | Total Loss: 0.005531 | Recon Loss: 0.004458 | Commit Loss: 0.002147 | Perplexity: 956.959570
2025-10-03 03:29:00,805 Stage: Train 0.5 | Epoch: 315 | Iter: 237600 | Total Loss: 0.005421 | Recon Loss: 0.004349 | Commit Loss: 0.002144 | Perplexity: 955.398192
2025-10-03 03:31:36,780 Stage: Train 0.5 | Epoch: 315 | Iter: 237800 | Total Loss: 0.005410 | Recon Loss: 0.004338 | Commit Loss: 0.002144 | Perplexity: 957.617627
Trainning Epoch:  48%|████▊     | 316/665 [62:45:01<57:40:50, 594.99s/it]2025-10-03 03:34:21,325 Stage: Train 0.5 | Epoch: 316 | Iter: 238000 | Total Loss: 0.005417 | Recon Loss: 0.004345 | Commit Loss: 0.002145 | Perplexity: 956.985473
2025-10-03 03:37:00,823 Stage: Train 0.5 | Epoch: 316 | Iter: 238200 | Total Loss: 0.005393 | Recon Loss: 0.004325 | Commit Loss: 0.002137 | Perplexity: 959.426903
2025-10-03 03:39:34,987 Stage: Train 0.5 | Epoch: 316 | Iter: 238400 | Total Loss: 0.005507 | Recon Loss: 0.004431 | Commit Loss: 0.002151 | Perplexity: 951.753554
2025-10-03 03:42:12,609 Stage: Train 0.5 | Epoch: 316 | Iter: 238600 | Total Loss: 0.005441 | Recon Loss: 0.004358 | Commit Loss: 0.002166 | Perplexity: 962.643311
Trainning Epoch:  48%|████▊     | 317/665 [62:54:56<57:31:47, 595.14s/it]2025-10-03 03:44:52,512 Stage: Train 0.5 | Epoch: 317 | Iter: 238800 | Total Loss: 0.005393 | Recon Loss: 0.004330 | Commit Loss: 0.002126 | Perplexity: 952.911793
2025-10-03 03:47:27,734 Stage: Train 0.5 | Epoch: 317 | Iter: 239000 | Total Loss: 0.005411 | Recon Loss: 0.004340 | Commit Loss: 0.002142 | Perplexity: 955.437140
2025-10-03 03:50:03,219 Stage: Train 0.5 | Epoch: 317 | Iter: 239200 | Total Loss: 0.005443 | Recon Loss: 0.004363 | Commit Loss: 0.002159 | Perplexity: 956.236444
2025-10-03 03:52:38,459 Stage: Train 0.5 | Epoch: 317 | Iter: 239400 | Total Loss: 0.005443 | Recon Loss: 0.004377 | Commit Loss: 0.002132 | Perplexity: 954.733697
Trainning Epoch:  48%|████▊     | 318/665 [63:04:49<57:18:19, 594.52s/it]2025-10-03 03:55:25,903 Stage: Train 0.5 | Epoch: 318 | Iter: 239600 | Total Loss: 0.005397 | Recon Loss: 0.004314 | Commit Loss: 0.002165 | Perplexity: 957.054172
2025-10-03 03:58:00,979 Stage: Train 0.5 | Epoch: 318 | Iter: 239800 | Total Loss: 0.005459 | Recon Loss: 0.004375 | Commit Loss: 0.002169 | Perplexity: 959.927022
2025-10-03 04:00:32,043 Stage: Train 0.5 | Epoch: 318 | Iter: 240000 | Total Loss: 0.005379 | Recon Loss: 0.004313 | Commit Loss: 0.002130 | Perplexity: 956.358566
2025-10-03 04:00:32,044 Saving model at iteration 240000
2025-10-03 04:00:32,241 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000
2025-10-03 04:00:32,539 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000/model.safetensors
2025-10-03 04:00:32,916 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000/optimizer.bin
2025-10-03 04:00:32,916 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000/scheduler.bin
2025-10-03 04:00:32,916 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000/sampler.bin
2025-10-03 04:00:32,917 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_319_step_240000/random_states_0.pkl
2025-10-03 04:03:03,268 Stage: Train 0.5 | Epoch: 318 | Iter: 240200 | Total Loss: 0.005398 | Recon Loss: 0.004335 | Commit Loss: 0.002127 | Perplexity: 952.031471
Trainning Epoch:  48%|████▊     | 319/665 [63:14:35<56:53:40, 591.97s/it]2025-10-03 04:05:42,401 Stage: Train 0.5 | Epoch: 319 | Iter: 240400 | Total Loss: 0.005476 | Recon Loss: 0.004396 | Commit Loss: 0.002159 | Perplexity: 961.831302
2025-10-03 04:08:12,359 Stage: Train 0.5 | Epoch: 319 | Iter: 240600 | Total Loss: 0.005429 | Recon Loss: 0.004360 | Commit Loss: 0.002138 | Perplexity: 958.114658
2025-10-03 04:10:44,126 Stage: Train 0.5 | Epoch: 319 | Iter: 240800 | Total Loss: 0.005426 | Recon Loss: 0.004351 | Commit Loss: 0.002150 | Perplexity: 962.012164
Trainning Epoch:  48%|████▊     | 320/665 [63:24:11<56:16:15, 587.17s/it]2025-10-03 04:13:23,100 Stage: Train 0.5 | Epoch: 320 | Iter: 241000 | Total Loss: 0.005410 | Recon Loss: 0.004346 | Commit Loss: 0.002129 | Perplexity: 957.194581
2025-10-03 04:15:53,366 Stage: Train 0.5 | Epoch: 320 | Iter: 241200 | Total Loss: 0.005411 | Recon Loss: 0.004340 | Commit Loss: 0.002142 | Perplexity: 960.910492
2025-10-03 04:18:22,720 Stage: Train 0.5 | Epoch: 320 | Iter: 241400 | Total Loss: 0.005348 | Recon Loss: 0.004281 | Commit Loss: 0.002134 | Perplexity: 960.991203
2025-10-03 04:20:52,605 Stage: Train 0.5 | Epoch: 320 | Iter: 241600 | Total Loss: 0.005451 | Recon Loss: 0.004375 | Commit Loss: 0.002152 | Perplexity: 957.619779
Trainning Epoch:  48%|████▊     | 321/665 [63:33:44<55:41:50, 582.88s/it]2025-10-03 04:23:31,070 Stage: Train 0.5 | Epoch: 321 | Iter: 241800 | Total Loss: 0.005495 | Recon Loss: 0.004412 | Commit Loss: 0.002167 | Perplexity: 959.701117
2025-10-03 04:26:08,180 Stage: Train 0.5 | Epoch: 321 | Iter: 242000 | Total Loss: 0.005370 | Recon Loss: 0.004312 | Commit Loss: 0.002116 | Perplexity: 957.335730
2025-10-03 04:28:45,619 Stage: Train 0.5 | Epoch: 321 | Iter: 242200 | Total Loss: 0.005389 | Recon Loss: 0.004315 | Commit Loss: 0.002147 | Perplexity: 958.617901
2025-10-03 04:31:24,057 Stage: Train 0.5 | Epoch: 321 | Iter: 242400 | Total Loss: 0.005435 | Recon Loss: 0.004369 | Commit Loss: 0.002133 | Perplexity: 957.733971
Trainning Epoch:  48%|████▊     | 322/665 [63:43:42<55:58:08, 587.43s/it]2025-10-03 04:34:06,873 Stage: Train 0.5 | Epoch: 322 | Iter: 242600 | Total Loss: 0.005489 | Recon Loss: 0.004413 | Commit Loss: 0.002151 | Perplexity: 952.856948
2025-10-03 04:36:44,213 Stage: Train 0.5 | Epoch: 322 | Iter: 242800 | Total Loss: 0.005350 | Recon Loss: 0.004288 | Commit Loss: 0.002123 | Perplexity: 957.995157
2025-10-03 04:39:19,570 Stage: Train 0.5 | Epoch: 322 | Iter: 243000 | Total Loss: 0.005411 | Recon Loss: 0.004340 | Commit Loss: 0.002141 | Perplexity: 956.854507
2025-10-03 04:41:52,987 Stage: Train 0.5 | Epoch: 322 | Iter: 243200 | Total Loss: 0.005448 | Recon Loss: 0.004372 | Commit Loss: 0.002150 | Perplexity: 960.570569
Trainning Epoch:  49%|████▊     | 323/665 [63:53:35<55:56:53, 588.93s/it]2025-10-03 04:44:37,010 Stage: Train 0.5 | Epoch: 323 | Iter: 243400 | Total Loss: 0.005431 | Recon Loss: 0.004363 | Commit Loss: 0.002136 | Perplexity: 956.363695
2025-10-03 04:47:11,876 Stage: Train 0.5 | Epoch: 323 | Iter: 243600 | Total Loss: 0.005413 | Recon Loss: 0.004350 | Commit Loss: 0.002127 | Perplexity: 960.681800
2025-10-03 04:49:47,521 Stage: Train 0.5 | Epoch: 323 | Iter: 243800 | Total Loss: 0.005475 | Recon Loss: 0.004396 | Commit Loss: 0.002160 | Perplexity: 963.231926
Trainning Epoch:  49%|████▊     | 324/665 [64:03:26<55:50:44, 589.57s/it]2025-10-03 04:52:27,150 Stage: Train 0.5 | Epoch: 324 | Iter: 244000 | Total Loss: 0.005355 | Recon Loss: 0.004290 | Commit Loss: 0.002129 | Perplexity: 954.678253
2025-10-03 04:55:04,337 Stage: Train 0.5 | Epoch: 324 | Iter: 244200 | Total Loss: 0.005367 | Recon Loss: 0.004289 | Commit Loss: 0.002155 | Perplexity: 957.732921
2025-10-03 04:57:40,150 Stage: Train 0.5 | Epoch: 324 | Iter: 244400 | Total Loss: 0.005405 | Recon Loss: 0.004343 | Commit Loss: 0.002123 | Perplexity: 960.536758
2025-10-03 05:00:15,050 Stage: Train 0.5 | Epoch: 324 | Iter: 244600 | Total Loss: 0.005364 | Recon Loss: 0.004298 | Commit Loss: 0.002132 | Perplexity: 962.855321
Trainning Epoch:  49%|████▉     | 325/665 [64:13:18<55:44:53, 590.28s/it]2025-10-03 05:02:55,745 Stage: Train 0.5 | Epoch: 325 | Iter: 244800 | Total Loss: 0.005375 | Recon Loss: 0.004305 | Commit Loss: 0.002140 | Perplexity: 960.404910
2025-10-03 05:05:32,274 Stage: Train 0.5 | Epoch: 325 | Iter: 245000 | Total Loss: 0.005445 | Recon Loss: 0.004376 | Commit Loss: 0.002138 | Perplexity: 951.812180
2025-10-03 05:08:08,469 Stage: Train 0.5 | Epoch: 325 | Iter: 245200 | Total Loss: 0.005377 | Recon Loss: 0.004312 | Commit Loss: 0.002129 | Perplexity: 958.879005
2025-10-03 05:10:44,367 Stage: Train 0.5 | Epoch: 325 | Iter: 245400 | Total Loss: 0.005418 | Recon Loss: 0.004341 | Commit Loss: 0.002153 | Perplexity: 951.785112
Trainning Epoch:  49%|████▉     | 326/665 [64:23:10<55:38:53, 590.95s/it]2025-10-03 05:13:26,889 Stage: Train 0.5 | Epoch: 326 | Iter: 245600 | Total Loss: 0.005370 | Recon Loss: 0.004306 | Commit Loss: 0.002127 | Perplexity: 954.197574
2025-10-03 05:16:02,096 Stage: Train 0.5 | Epoch: 326 | Iter: 245800 | Total Loss: 0.005363 | Recon Loss: 0.004294 | Commit Loss: 0.002138 | Perplexity: 959.067263
2025-10-03 05:18:39,245 Stage: Train 0.5 | Epoch: 326 | Iter: 246000 | Total Loss: 0.005418 | Recon Loss: 0.004347 | Commit Loss: 0.002142 | Perplexity: 961.887154
2025-10-03 05:21:12,952 Stage: Train 0.5 | Epoch: 326 | Iter: 246200 | Total Loss: 0.005353 | Recon Loss: 0.004282 | Commit Loss: 0.002142 | Perplexity: 959.302994
Trainning Epoch:  49%|████▉     | 327/665 [64:33:03<55:32:12, 591.52s/it]2025-10-03 05:23:58,111 Stage: Train 0.5 | Epoch: 327 | Iter: 246400 | Total Loss: 0.005393 | Recon Loss: 0.004320 | Commit Loss: 0.002145 | Perplexity: 964.012678
2025-10-03 05:26:34,527 Stage: Train 0.5 | Epoch: 327 | Iter: 246600 | Total Loss: 0.005376 | Recon Loss: 0.004313 | Commit Loss: 0.002126 | Perplexity: 958.504263
2025-10-03 05:29:14,830 Stage: Train 0.5 | Epoch: 327 | Iter: 246800 | Total Loss: 0.005362 | Recon Loss: 0.004290 | Commit Loss: 0.002146 | Perplexity: 961.683264
Trainning Epoch:  49%|████▉     | 328/665 [64:43:06<55:40:56, 594.83s/it]2025-10-03 05:31:59,618 Stage: Train 0.5 | Epoch: 328 | Iter: 247000 | Total Loss: 0.005413 | Recon Loss: 0.004353 | Commit Loss: 0.002120 | Perplexity: 948.599746
2025-10-03 05:34:35,548 Stage: Train 0.5 | Epoch: 328 | Iter: 247200 | Total Loss: 0.005344 | Recon Loss: 0.004283 | Commit Loss: 0.002123 | Perplexity: 954.909619
2025-10-03 05:37:09,727 Stage: Train 0.5 | Epoch: 328 | Iter: 247400 | Total Loss: 0.005360 | Recon Loss: 0.004297 | Commit Loss: 0.002125 | Perplexity: 951.559193
2025-10-03 05:39:44,759 Stage: Train 0.5 | Epoch: 328 | Iter: 247600 | Total Loss: 0.005360 | Recon Loss: 0.004288 | Commit Loss: 0.002144 | Perplexity: 955.783226
Trainning Epoch:  49%|████▉     | 329/665 [64:52:57<55:26:02, 593.94s/it]2025-10-03 05:42:25,695 Stage: Train 0.5 | Epoch: 329 | Iter: 247800 | Total Loss: 0.005372 | Recon Loss: 0.004304 | Commit Loss: 0.002136 | Perplexity: 957.032347
2025-10-03 05:45:02,283 Stage: Train 0.5 | Epoch: 329 | Iter: 248000 | Total Loss: 0.005335 | Recon Loss: 0.004274 | Commit Loss: 0.002121 | Perplexity: 953.360248
2025-10-03 05:47:39,563 Stage: Train 0.5 | Epoch: 329 | Iter: 248200 | Total Loss: 0.005395 | Recon Loss: 0.004328 | Commit Loss: 0.002135 | Perplexity: 959.613645
2025-10-03 05:50:15,082 Stage: Train 0.5 | Epoch: 329 | Iter: 248400 | Total Loss: 0.005442 | Recon Loss: 0.004367 | Commit Loss: 0.002150 | Perplexity: 961.610927
Trainning Epoch:  50%|████▉     | 330/665 [65:02:52<55:16:33, 594.01s/it]2025-10-03 05:52:57,061 Stage: Train 0.5 | Epoch: 330 | Iter: 248600 | Total Loss: 0.005455 | Recon Loss: 0.004384 | Commit Loss: 0.002143 | Perplexity: 954.142587
2025-10-03 05:55:31,582 Stage: Train 0.5 | Epoch: 330 | Iter: 248800 | Total Loss: 0.005320 | Recon Loss: 0.004251 | Commit Loss: 0.002139 | Perplexity: 959.966263
2025-10-03 05:58:02,480 Stage: Train 0.5 | Epoch: 330 | Iter: 249000 | Total Loss: 0.005353 | Recon Loss: 0.004293 | Commit Loss: 0.002120 | Perplexity: 957.989483
2025-10-03 06:00:37,227 Stage: Train 0.5 | Epoch: 330 | Iter: 249200 | Total Loss: 0.005351 | Recon Loss: 0.004278 | Commit Loss: 0.002146 | Perplexity: 959.686581
Trainning Epoch:  50%|████▉     | 331/665 [65:12:37<54:52:53, 591.54s/it]2025-10-03 06:03:19,387 Stage: Train 0.5 | Epoch: 331 | Iter: 249400 | Total Loss: 0.005456 | Recon Loss: 0.004386 | Commit Loss: 0.002138 | Perplexity: 959.508250
2025-10-03 06:05:57,278 Stage: Train 0.5 | Epoch: 331 | Iter: 249600 | Total Loss: 0.005305 | Recon Loss: 0.004239 | Commit Loss: 0.002130 | Perplexity: 959.364138
2025-10-03 06:08:34,366 Stage: Train 0.5 | Epoch: 331 | Iter: 249800 | Total Loss: 0.005335 | Recon Loss: 0.004268 | Commit Loss: 0.002134 | Perplexity: 958.616929
Trainning Epoch:  50%|████▉     | 332/665 [65:22:42<55:04:52, 595.47s/it]2025-10-03 06:11:25,132 Stage: Train 0.5 | Epoch: 332 | Iter: 250000 | Total Loss: 0.005371 | Recon Loss: 0.004306 | Commit Loss: 0.002130 | Perplexity: 957.227164
2025-10-03 06:14:10,053 Stage: Train 0.5 | Epoch: 332 | Iter: 250200 | Total Loss: 0.005353 | Recon Loss: 0.004283 | Commit Loss: 0.002140 | Perplexity: 962.427964
2025-10-03 06:16:53,454 Stage: Train 0.5 | Epoch: 332 | Iter: 250400 | Total Loss: 0.005340 | Recon Loss: 0.004269 | Commit Loss: 0.002142 | Perplexity: 959.942840
2025-10-03 06:19:29,767 Stage: Train 0.5 | Epoch: 332 | Iter: 250600 | Total Loss: 0.005407 | Recon Loss: 0.004345 | Commit Loss: 0.002125 | Perplexity: 957.335874
Trainning Epoch:  50%|█████     | 333/665 [65:32:51<55:17:51, 599.61s/it]2025-10-03 06:22:11,083 Stage: Train 0.5 | Epoch: 333 | Iter: 250800 | Total Loss: 0.005343 | Recon Loss: 0.004281 | Commit Loss: 0.002123 | Perplexity: 957.397825
2025-10-03 06:24:54,917 Stage: Train 0.5 | Epoch: 333 | Iter: 251000 | Total Loss: 0.005378 | Recon Loss: 0.004315 | Commit Loss: 0.002125 | Perplexity: 962.616233
2025-10-03 06:27:32,070 Stage: Train 0.5 | Epoch: 333 | Iter: 251200 | Total Loss: 0.005356 | Recon Loss: 0.004292 | Commit Loss: 0.002127 | Perplexity: 959.031512
2025-10-03 06:30:11,936 Stage: Train 0.5 | Epoch: 333 | Iter: 251400 | Total Loss: 0.005370 | Recon Loss: 0.004300 | Commit Loss: 0.002141 | Perplexity: 953.785650
Trainning Epoch:  50%|█████     | 334/665 [65:43:03<55:27:10, 603.11s/it]2025-10-03 06:32:58,492 Stage: Train 0.5 | Epoch: 334 | Iter: 251600 | Total Loss: 0.005377 | Recon Loss: 0.004315 | Commit Loss: 0.002125 | Perplexity: 954.260863
2025-10-03 06:35:36,173 Stage: Train 0.5 | Epoch: 334 | Iter: 251800 | Total Loss: 0.005361 | Recon Loss: 0.004294 | Commit Loss: 0.002135 | Perplexity: 957.133400
2025-10-03 06:38:14,760 Stage: Train 0.5 | Epoch: 334 | Iter: 252000 | Total Loss: 0.005355 | Recon Loss: 0.004288 | Commit Loss: 0.002133 | Perplexity: 958.169083
2025-10-03 06:40:51,015 Stage: Train 0.5 | Epoch: 334 | Iter: 252200 | Total Loss: 0.005312 | Recon Loss: 0.004251 | Commit Loss: 0.002122 | Perplexity: 962.275344
Trainning Epoch:  50%|█████     | 335/665 [65:53:00<55:07:08, 601.30s/it]2025-10-03 06:43:32,467 Stage: Train 0.5 | Epoch: 335 | Iter: 252400 | Total Loss: 0.005312 | Recon Loss: 0.004247 | Commit Loss: 0.002131 | Perplexity: 959.266365
2025-10-03 06:46:09,122 Stage: Train 0.5 | Epoch: 335 | Iter: 252600 | Total Loss: 0.005369 | Recon Loss: 0.004297 | Commit Loss: 0.002143 | Perplexity: 962.461525
2025-10-03 06:48:45,402 Stage: Train 0.5 | Epoch: 335 | Iter: 252800 | Total Loss: 0.005330 | Recon Loss: 0.004260 | Commit Loss: 0.002139 | Perplexity: 962.201640
2025-10-03 06:51:26,342 Stage: Train 0.5 | Epoch: 335 | Iter: 253000 | Total Loss: 0.005372 | Recon Loss: 0.004304 | Commit Loss: 0.002137 | Perplexity: 959.008192
Trainning Epoch:  51%|█████     | 336/665 [66:02:59<54:54:46, 600.87s/it]2025-10-03 06:54:11,879 Stage: Train 0.5 | Epoch: 336 | Iter: 253200 | Total Loss: 0.005412 | Recon Loss: 0.004345 | Commit Loss: 0.002133 | Perplexity: 958.011463
2025-10-03 06:56:48,571 Stage: Train 0.5 | Epoch: 336 | Iter: 253400 | Total Loss: 0.005349 | Recon Loss: 0.004279 | Commit Loss: 0.002139 | Perplexity: 955.413367
2025-10-03 06:59:24,331 Stage: Train 0.5 | Epoch: 336 | Iter: 253600 | Total Loss: 0.005296 | Recon Loss: 0.004232 | Commit Loss: 0.002127 | Perplexity: 958.910045
Trainning Epoch:  51%|█████     | 337/665 [66:12:55<54:35:59, 599.27s/it]2025-10-03 07:02:04,654 Stage: Train 0.5 | Epoch: 337 | Iter: 253800 | Total Loss: 0.005356 | Recon Loss: 0.004291 | Commit Loss: 0.002130 | Perplexity: 962.041198
2025-10-03 07:04:46,884 Stage: Train 0.5 | Epoch: 337 | Iter: 254000 | Total Loss: 0.005290 | Recon Loss: 0.004235 | Commit Loss: 0.002111 | Perplexity: 958.676874
2025-10-03 07:07:24,000 Stage: Train 0.5 | Epoch: 337 | Iter: 254200 | Total Loss: 0.005408 | Recon Loss: 0.004343 | Commit Loss: 0.002130 | Perplexity: 956.733551
2025-10-03 07:10:01,830 Stage: Train 0.5 | Epoch: 337 | Iter: 254400 | Total Loss: 0.005271 | Recon Loss: 0.004211 | Commit Loss: 0.002120 | Perplexity: 963.941044
Trainning Epoch:  51%|█████     | 338/665 [66:22:59<54:34:28, 600.82s/it]2025-10-03 07:12:48,229 Stage: Train 0.5 | Epoch: 338 | Iter: 254600 | Total Loss: 0.005429 | Recon Loss: 0.004357 | Commit Loss: 0.002144 | Perplexity: 959.689938
2025-10-03 07:15:22,811 Stage: Train 0.5 | Epoch: 338 | Iter: 254800 | Total Loss: 0.005352 | Recon Loss: 0.004297 | Commit Loss: 0.002111 | Perplexity: 961.718558
2025-10-03 07:17:59,513 Stage: Train 0.5 | Epoch: 338 | Iter: 255000 | Total Loss: 0.005317 | Recon Loss: 0.004251 | Commit Loss: 0.002133 | Perplexity: 961.641742
2025-10-03 07:20:34,543 Stage: Train 0.5 | Epoch: 338 | Iter: 255200 | Total Loss: 0.005340 | Recon Loss: 0.004276 | Commit Loss: 0.002129 | Perplexity: 956.850836
Trainning Epoch:  51%|█████     | 339/665 [66:32:54<54:13:38, 598.83s/it]2025-10-03 07:23:17,480 Stage: Train 0.5 | Epoch: 339 | Iter: 255400 | Total Loss: 0.005363 | Recon Loss: 0.004306 | Commit Loss: 0.002114 | Perplexity: 955.915591
2025-10-03 07:26:00,006 Stage: Train 0.5 | Epoch: 339 | Iter: 255600 | Total Loss: 0.005331 | Recon Loss: 0.004267 | Commit Loss: 0.002128 | Perplexity: 961.130994
2025-10-03 07:28:35,234 Stage: Train 0.5 | Epoch: 339 | Iter: 255800 | Total Loss: 0.005314 | Recon Loss: 0.004257 | Commit Loss: 0.002114 | Perplexity: 957.838055
2025-10-03 07:31:13,079 Stage: Train 0.5 | Epoch: 339 | Iter: 256000 | Total Loss: 0.005442 | Recon Loss: 0.004371 | Commit Loss: 0.002143 | Perplexity: 958.655693
Trainning Epoch:  51%|█████     | 340/665 [66:42:56<54:08:43, 599.76s/it]2025-10-03 07:33:56,795 Stage: Train 0.5 | Epoch: 340 | Iter: 256200 | Total Loss: 0.005274 | Recon Loss: 0.004220 | Commit Loss: 0.002109 | Perplexity: 957.106708
2025-10-03 07:36:35,168 Stage: Train 0.5 | Epoch: 340 | Iter: 256400 | Total Loss: 0.005325 | Recon Loss: 0.004269 | Commit Loss: 0.002111 | Perplexity: 959.330773
2025-10-03 07:39:10,653 Stage: Train 0.5 | Epoch: 340 | Iter: 256600 | Total Loss: 0.005386 | Recon Loss: 0.004322 | Commit Loss: 0.002129 | Perplexity: 959.407330
Trainning Epoch:  51%|█████▏    | 341/665 [66:52:53<53:54:26, 598.97s/it]2025-10-03 07:41:54,840 Stage: Train 0.5 | Epoch: 341 | Iter: 256800 | Total Loss: 0.005323 | Recon Loss: 0.004257 | Commit Loss: 0.002132 | Perplexity: 959.973390
2025-10-03 07:44:35,512 Stage: Train 0.5 | Epoch: 341 | Iter: 257000 | Total Loss: 0.005289 | Recon Loss: 0.004234 | Commit Loss: 0.002110 | Perplexity: 959.667282
2025-10-03 07:47:11,354 Stage: Train 0.5 | Epoch: 341 | Iter: 257200 | Total Loss: 0.005326 | Recon Loss: 0.004259 | Commit Loss: 0.002135 | Perplexity: 962.488630
2025-10-03 07:49:51,037 Stage: Train 0.5 | Epoch: 341 | Iter: 257400 | Total Loss: 0.005352 | Recon Loss: 0.004287 | Commit Loss: 0.002131 | Perplexity: 962.846237
Trainning Epoch:  51%|█████▏    | 342/665 [67:02:56<53:51:26, 600.27s/it]2025-10-03 07:52:34,235 Stage: Train 0.5 | Epoch: 342 | Iter: 257600 | Total Loss: 0.005364 | Recon Loss: 0.004310 | Commit Loss: 0.002108 | Perplexity: 961.287888
2025-10-03 07:55:11,787 Stage: Train 0.5 | Epoch: 342 | Iter: 257800 | Total Loss: 0.005275 | Recon Loss: 0.004216 | Commit Loss: 0.002119 | Perplexity: 958.566472
2025-10-03 07:57:51,838 Stage: Train 0.5 | Epoch: 342 | Iter: 258000 | Total Loss: 0.005343 | Recon Loss: 0.004292 | Commit Loss: 0.002102 | Perplexity: 957.752723
2025-10-03 08:00:29,115 Stage: Train 0.5 | Epoch: 342 | Iter: 258200 | Total Loss: 0.005365 | Recon Loss: 0.004294 | Commit Loss: 0.002142 | Perplexity: 954.004909
Trainning Epoch:  52%|█████▏    | 343/665 [67:12:58<53:43:34, 600.67s/it]2025-10-03 08:03:15,071 Stage: Train 0.5 | Epoch: 343 | Iter: 258400 | Total Loss: 0.005346 | Recon Loss: 0.004296 | Commit Loss: 0.002100 | Perplexity: 957.322777
2025-10-03 08:05:53,097 Stage: Train 0.5 | Epoch: 343 | Iter: 258600 | Total Loss: 0.005272 | Recon Loss: 0.004213 | Commit Loss: 0.002119 | Perplexity: 959.014674
2025-10-03 08:08:32,353 Stage: Train 0.5 | Epoch: 343 | Iter: 258800 | Total Loss: 0.005334 | Recon Loss: 0.004275 | Commit Loss: 0.002117 | Perplexity: 957.386570
2025-10-03 08:11:16,759 Stage: Train 0.5 | Epoch: 343 | Iter: 259000 | Total Loss: 0.005373 | Recon Loss: 0.004303 | Commit Loss: 0.002141 | Perplexity: 964.626897
Trainning Epoch:  52%|█████▏    | 344/665 [67:23:08<53:48:36, 603.48s/it]2025-10-03 08:14:00,558 Stage: Train 0.5 | Epoch: 344 | Iter: 259200 | Total Loss: 0.005303 | Recon Loss: 0.004241 | Commit Loss: 0.002125 | Perplexity: 958.187768
2025-10-03 08:16:41,909 Stage: Train 0.5 | Epoch: 344 | Iter: 259400 | Total Loss: 0.005402 | Recon Loss: 0.004331 | Commit Loss: 0.002142 | Perplexity: 960.209864
2025-10-03 08:19:20,599 Stage: Train 0.5 | Epoch: 344 | Iter: 259600 | Total Loss: 0.005314 | Recon Loss: 0.004259 | Commit Loss: 0.002110 | Perplexity: 956.529444
Trainning Epoch:  52%|█████▏    | 345/665 [67:33:13<53:40:56, 603.93s/it]2025-10-03 08:22:04,570 Stage: Train 0.5 | Epoch: 345 | Iter: 259800 | Total Loss: 0.005313 | Recon Loss: 0.004253 | Commit Loss: 0.002120 | Perplexity: 960.967437
2025-10-03 08:24:47,607 Stage: Train 0.5 | Epoch: 345 | Iter: 260000 | Total Loss: 0.005347 | Recon Loss: 0.004290 | Commit Loss: 0.002114 | Perplexity: 963.222052
2025-10-03 08:24:47,608 Saving model at iteration 260000
2025-10-03 08:24:47,808 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000
2025-10-03 08:24:48,143 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000/model.safetensors
2025-10-03 08:24:48,567 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000/optimizer.bin
2025-10-03 08:24:48,568 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000/scheduler.bin
2025-10-03 08:24:48,568 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000/sampler.bin
2025-10-03 08:24:48,569 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_346_step_260000/random_states_0.pkl
2025-10-03 08:27:24,321 Stage: Train 0.5 | Epoch: 345 | Iter: 260200 | Total Loss: 0.005371 | Recon Loss: 0.004308 | Commit Loss: 0.002127 | Perplexity: 960.926052
2025-10-03 08:30:02,305 Stage: Train 0.5 | Epoch: 345 | Iter: 260400 | Total Loss: 0.005214 | Recon Loss: 0.004164 | Commit Loss: 0.002101 | Perplexity: 957.813556
Trainning Epoch:  52%|█████▏    | 346/665 [67:43:17<53:31:10, 603.98s/it]2025-10-03 08:32:45,755 Stage: Train 0.5 | Epoch: 346 | Iter: 260600 | Total Loss: 0.005356 | Recon Loss: 0.004300 | Commit Loss: 0.002112 | Perplexity: 955.675684
2025-10-03 08:35:24,728 Stage: Train 0.5 | Epoch: 346 | Iter: 260800 | Total Loss: 0.005265 | Recon Loss: 0.004215 | Commit Loss: 0.002099 | Perplexity: 957.196893
2025-10-03 08:38:03,528 Stage: Train 0.5 | Epoch: 346 | Iter: 261000 | Total Loss: 0.005316 | Recon Loss: 0.004256 | Commit Loss: 0.002122 | Perplexity: 960.719499
2025-10-03 08:40:53,396 Stage: Train 0.5 | Epoch: 346 | Iter: 261200 | Total Loss: 0.005302 | Recon Loss: 0.004240 | Commit Loss: 0.002125 | Perplexity: 959.226063
Trainning Epoch:  52%|█████▏    | 347/665 [67:53:36<53:45:49, 608.65s/it]2025-10-03 08:43:48,722 Stage: Train 0.5 | Epoch: 347 | Iter: 261400 | Total Loss: 0.005354 | Recon Loss: 0.004300 | Commit Loss: 0.002107 | Perplexity: 952.646827
2025-10-03 08:46:31,053 Stage: Train 0.5 | Epoch: 347 | Iter: 261600 | Total Loss: 0.005310 | Recon Loss: 0.004247 | Commit Loss: 0.002126 | Perplexity: 961.772888
2025-10-03 08:49:09,543 Stage: Train 0.5 | Epoch: 347 | Iter: 261800 | Total Loss: 0.005318 | Recon Loss: 0.004269 | Commit Loss: 0.002098 | Perplexity: 958.964461
2025-10-03 08:51:49,598 Stage: Train 0.5 | Epoch: 347 | Iter: 262000 | Total Loss: 0.005324 | Recon Loss: 0.004259 | Commit Loss: 0.002131 | Perplexity: 961.248318
Trainning Epoch:  52%|█████▏    | 348/665 [68:03:51<53:45:22, 610.48s/it]2025-10-03 08:54:36,038 Stage: Train 0.5 | Epoch: 348 | Iter: 262200 | Total Loss: 0.005319 | Recon Loss: 0.004263 | Commit Loss: 0.002112 | Perplexity: 956.622663
2025-10-03 08:57:12,239 Stage: Train 0.5 | Epoch: 348 | Iter: 262400 | Total Loss: 0.005257 | Recon Loss: 0.004207 | Commit Loss: 0.002100 | Perplexity: 959.466693
2025-10-03 08:59:52,830 Stage: Train 0.5 | Epoch: 348 | Iter: 262600 | Total Loss: 0.005291 | Recon Loss: 0.004234 | Commit Loss: 0.002115 | Perplexity: 963.420800
Trainning Epoch:  52%|█████▏    | 349/665 [68:13:55<53:24:25, 608.43s/it]2025-10-03 09:02:36,883 Stage: Train 0.5 | Epoch: 349 | Iter: 262800 | Total Loss: 0.005296 | Recon Loss: 0.004234 | Commit Loss: 0.002125 | Perplexity: 957.307779
2025-10-03 09:05:19,541 Stage: Train 0.5 | Epoch: 349 | Iter: 263000 | Total Loss: 0.005313 | Recon Loss: 0.004260 | Commit Loss: 0.002107 | Perplexity: 960.254867
2025-10-03 09:08:09,059 Stage: Train 0.5 | Epoch: 349 | Iter: 263200 | Total Loss: 0.005284 | Recon Loss: 0.004230 | Commit Loss: 0.002107 | Perplexity: 957.541067
2025-10-03 09:10:46,759 Stage: Train 0.5 | Epoch: 349 | Iter: 263400 | Total Loss: 0.005351 | Recon Loss: 0.004292 | Commit Loss: 0.002117 | Perplexity: 960.916768
Trainning Epoch:  53%|█████▎    | 350/665 [68:24:11<53:27:11, 610.89s/it]2025-10-03 09:13:31,907 Stage: Train 0.5 | Epoch: 350 | Iter: 263600 | Total Loss: 0.005322 | Recon Loss: 0.004266 | Commit Loss: 0.002113 | Perplexity: 959.214892
2025-10-03 09:16:09,326 Stage: Train 0.5 | Epoch: 350 | Iter: 263800 | Total Loss: 0.005271 | Recon Loss: 0.004219 | Commit Loss: 0.002103 | Perplexity: 960.889893
2025-10-03 09:18:45,330 Stage: Train 0.5 | Epoch: 350 | Iter: 264000 | Total Loss: 0.005276 | Recon Loss: 0.004216 | Commit Loss: 0.002120 | Perplexity: 961.852917
2025-10-03 09:21:25,891 Stage: Train 0.5 | Epoch: 350 | Iter: 264200 | Total Loss: 0.005348 | Recon Loss: 0.004284 | Commit Loss: 0.002128 | Perplexity: 961.401744
Trainning Epoch:  53%|█████▎    | 351/665 [68:34:12<53:01:15, 607.88s/it]2025-10-03 09:24:08,877 Stage: Train 0.5 | Epoch: 351 | Iter: 264400 | Total Loss: 0.005334 | Recon Loss: 0.004285 | Commit Loss: 0.002098 | Perplexity: 958.061446
2025-10-03 09:26:49,916 Stage: Train 0.5 | Epoch: 351 | Iter: 264600 | Total Loss: 0.005266 | Recon Loss: 0.004212 | Commit Loss: 0.002108 | Perplexity: 957.308502
2025-10-03 09:29:37,083 Stage: Train 0.5 | Epoch: 351 | Iter: 264800 | Total Loss: 0.005253 | Recon Loss: 0.004196 | Commit Loss: 0.002115 | Perplexity: 960.990619
2025-10-03 09:32:19,520 Stage: Train 0.5 | Epoch: 351 | Iter: 265000 | Total Loss: 0.005314 | Recon Loss: 0.004247 | Commit Loss: 0.002134 | Perplexity: 961.709882
Trainning Epoch:  53%|█████▎    | 352/665 [68:44:31<53:08:23, 611.19s/it]2025-10-03 09:35:08,890 Stage: Train 0.5 | Epoch: 352 | Iter: 265200 | Total Loss: 0.005311 | Recon Loss: 0.004258 | Commit Loss: 0.002106 | Perplexity: 960.303821
2025-10-03 09:37:53,348 Stage: Train 0.5 | Epoch: 352 | Iter: 265400 | Total Loss: 0.005270 | Recon Loss: 0.004223 | Commit Loss: 0.002092 | Perplexity: 956.250495
2025-10-03 09:40:37,496 Stage: Train 0.5 | Epoch: 352 | Iter: 265600 | Total Loss: 0.005322 | Recon Loss: 0.004253 | Commit Loss: 0.002138 | Perplexity: 960.270708
2025-10-03 09:43:14,015 Stage: Train 0.5 | Epoch: 352 | Iter: 265800 | Total Loss: 0.005352 | Recon Loss: 0.004301 | Commit Loss: 0.002102 | Perplexity: 953.105171
Trainning Epoch:  53%|█████▎    | 353/665 [68:54:48<53:07:10, 612.92s/it]2025-10-03 09:46:01,554 Stage: Train 0.5 | Epoch: 353 | Iter: 266000 | Total Loss: 0.005242 | Recon Loss: 0.004192 | Commit Loss: 0.002101 | Perplexity: 956.168879
2025-10-03 09:48:39,819 Stage: Train 0.5 | Epoch: 353 | Iter: 266200 | Total Loss: 0.005328 | Recon Loss: 0.004274 | Commit Loss: 0.002108 | Perplexity: 960.367678
2025-10-03 09:51:14,267 Stage: Train 0.5 | Epoch: 353 | Iter: 266400 | Total Loss: 0.005266 | Recon Loss: 0.004205 | Commit Loss: 0.002122 | Perplexity: 965.323095
Trainning Epoch:  53%|█████▎    | 354/665 [69:04:47<52:36:01, 608.88s/it]2025-10-03 09:53:55,883 Stage: Train 0.5 | Epoch: 354 | Iter: 266600 | Total Loss: 0.005281 | Recon Loss: 0.004226 | Commit Loss: 0.002109 | Perplexity: 961.828422
2025-10-03 09:56:36,725 Stage: Train 0.5 | Epoch: 354 | Iter: 266800 | Total Loss: 0.005240 | Recon Loss: 0.004195 | Commit Loss: 0.002088 | Perplexity: 958.933454
2025-10-03 09:59:16,872 Stage: Train 0.5 | Epoch: 354 | Iter: 267000 | Total Loss: 0.005273 | Recon Loss: 0.004219 | Commit Loss: 0.002107 | Perplexity: 962.517330
2025-10-03 10:01:50,270 Stage: Train 0.5 | Epoch: 354 | Iter: 267200 | Total Loss: 0.005273 | Recon Loss: 0.004220 | Commit Loss: 0.002106 | Perplexity: 956.958620
Trainning Epoch:  53%|█████▎    | 355/665 [69:14:46<52:10:17, 605.86s/it]2025-10-03 10:04:35,729 Stage: Train 0.5 | Epoch: 355 | Iter: 267400 | Total Loss: 0.005234 | Recon Loss: 0.004178 | Commit Loss: 0.002112 | Perplexity: 963.254523
2025-10-03 10:07:24,004 Stage: Train 0.5 | Epoch: 355 | Iter: 267600 | Total Loss: 0.005328 | Recon Loss: 0.004267 | Commit Loss: 0.002123 | Perplexity: 964.649396
2025-10-03 10:10:13,807 Stage: Train 0.5 | Epoch: 355 | Iter: 267800 | Total Loss: 0.005299 | Recon Loss: 0.004241 | Commit Loss: 0.002115 | Perplexity: 957.952341
2025-10-03 10:12:54,544 Stage: Train 0.5 | Epoch: 355 | Iter: 268000 | Total Loss: 0.005258 | Recon Loss: 0.004208 | Commit Loss: 0.002100 | Perplexity: 956.708179
Trainning Epoch:  54%|█████▎    | 356/665 [69:25:12<52:31:01, 611.85s/it]2025-10-03 10:15:34,174 Stage: Train 0.5 | Epoch: 356 | Iter: 268200 | Total Loss: 0.005268 | Recon Loss: 0.004212 | Commit Loss: 0.002111 | Perplexity: 960.899361
2025-10-03 10:18:07,649 Stage: Train 0.5 | Epoch: 356 | Iter: 268400 | Total Loss: 0.005326 | Recon Loss: 0.004274 | Commit Loss: 0.002103 | Perplexity: 961.145129
2025-10-03 10:20:41,375 Stage: Train 0.5 | Epoch: 356 | Iter: 268600 | Total Loss: 0.005301 | Recon Loss: 0.004237 | Commit Loss: 0.002129 | Perplexity: 963.880679
2025-10-03 10:23:14,160 Stage: Train 0.5 | Epoch: 356 | Iter: 268800 | Total Loss: 0.005243 | Recon Loss: 0.004203 | Commit Loss: 0.002081 | Perplexity: 957.002456
Trainning Epoch:  54%|█████▎    | 357/665 [69:34:56<51:37:45, 603.46s/it]2025-10-03 10:25:55,991 Stage: Train 0.5 | Epoch: 357 | Iter: 269000 | Total Loss: 0.005256 | Recon Loss: 0.004210 | Commit Loss: 0.002091 | Perplexity: 959.076785
2025-10-03 10:28:32,031 Stage: Train 0.5 | Epoch: 357 | Iter: 269200 | Total Loss: 0.005249 | Recon Loss: 0.004198 | Commit Loss: 0.002101 | Perplexity: 955.940714
2025-10-03 10:31:05,902 Stage: Train 0.5 | Epoch: 357 | Iter: 269400 | Total Loss: 0.005316 | Recon Loss: 0.004260 | Commit Loss: 0.002111 | Perplexity: 961.572395
Trainning Epoch:  54%|█████▍    | 358/665 [69:44:47<51:08:52, 599.78s/it]2025-10-03 10:33:48,055 Stage: Train 0.5 | Epoch: 358 | Iter: 269600 | Total Loss: 0.005299 | Recon Loss: 0.004236 | Commit Loss: 0.002126 | Perplexity: 956.976091
2025-10-03 10:36:22,906 Stage: Train 0.5 | Epoch: 358 | Iter: 269800 | Total Loss: 0.005266 | Recon Loss: 0.004220 | Commit Loss: 0.002091 | Perplexity: 957.262548
2025-10-03 10:38:59,194 Stage: Train 0.5 | Epoch: 358 | Iter: 270000 | Total Loss: 0.005259 | Recon Loss: 0.004203 | Commit Loss: 0.002113 | Perplexity: 964.533686
2025-10-03 10:41:32,532 Stage: Train 0.5 | Epoch: 358 | Iter: 270200 | Total Loss: 0.005233 | Recon Loss: 0.004174 | Commit Loss: 0.002118 | Perplexity: 967.191905
Trainning Epoch:  54%|█████▍    | 359/665 [69:54:35<50:39:55, 596.06s/it]2025-10-03 10:44:11,957 Stage: Train 0.5 | Epoch: 359 | Iter: 270400 | Total Loss: 0.005303 | Recon Loss: 0.004259 | Commit Loss: 0.002087 | Perplexity: 959.014569
2025-10-03 10:46:46,098 Stage: Train 0.5 | Epoch: 359 | Iter: 270600 | Total Loss: 0.005229 | Recon Loss: 0.004180 | Commit Loss: 0.002097 | Perplexity: 960.422477
2025-10-03 10:49:20,747 Stage: Train 0.5 | Epoch: 359 | Iter: 270800 | Total Loss: 0.005252 | Recon Loss: 0.004198 | Commit Loss: 0.002108 | Perplexity: 961.552021
2025-10-03 10:51:56,371 Stage: Train 0.5 | Epoch: 359 | Iter: 271000 | Total Loss: 0.005284 | Recon Loss: 0.004232 | Commit Loss: 0.002102 | Perplexity: 961.387571
Trainning Epoch:  54%|█████▍    | 360/665 [70:04:26<50:22:52, 594.67s/it]2025-10-03 10:54:39,598 Stage: Train 0.5 | Epoch: 360 | Iter: 271200 | Total Loss: 0.005290 | Recon Loss: 0.004233 | Commit Loss: 0.002114 | Perplexity: 964.496217
2025-10-03 10:57:20,351 Stage: Train 0.5 | Epoch: 360 | Iter: 271400 | Total Loss: 0.005249 | Recon Loss: 0.004205 | Commit Loss: 0.002089 | Perplexity: 958.456644
2025-10-03 10:59:59,546 Stage: Train 0.5 | Epoch: 360 | Iter: 271600 | Total Loss: 0.005220 | Recon Loss: 0.004161 | Commit Loss: 0.002117 | Perplexity: 964.050455
2025-10-03 11:02:34,759 Stage: Train 0.5 | Epoch: 360 | Iter: 271800 | Total Loss: 0.005271 | Recon Loss: 0.004216 | Commit Loss: 0.002110 | Perplexity: 961.166273
Trainning Epoch:  54%|█████▍    | 361/665 [70:14:27<50:22:20, 596.52s/it]2025-10-03 11:05:19,586 Stage: Train 0.5 | Epoch: 361 | Iter: 272000 | Total Loss: 0.005245 | Recon Loss: 0.004197 | Commit Loss: 0.002095 | Perplexity: 953.258304
2025-10-03 11:07:54,710 Stage: Train 0.5 | Epoch: 361 | Iter: 272200 | Total Loss: 0.005232 | Recon Loss: 0.004179 | Commit Loss: 0.002107 | Perplexity: 963.663596
2025-10-03 11:10:35,771 Stage: Train 0.5 | Epoch: 361 | Iter: 272400 | Total Loss: 0.005257 | Recon Loss: 0.004206 | Commit Loss: 0.002102 | Perplexity: 962.601120
Trainning Epoch:  54%|█████▍    | 362/665 [70:24:32<50:25:16, 599.07s/it]2025-10-03 11:13:22,733 Stage: Train 0.5 | Epoch: 362 | Iter: 272600 | Total Loss: 0.005238 | Recon Loss: 0.004185 | Commit Loss: 0.002105 | Perplexity: 961.784109
2025-10-03 11:16:02,093 Stage: Train 0.5 | Epoch: 362 | Iter: 272800 | Total Loss: 0.005231 | Recon Loss: 0.004181 | Commit Loss: 0.002100 | Perplexity: 961.466895
2025-10-03 11:18:41,192 Stage: Train 0.5 | Epoch: 362 | Iter: 273000 | Total Loss: 0.005300 | Recon Loss: 0.004243 | Commit Loss: 0.002113 | Perplexity: 963.415597
2025-10-03 11:21:18,713 Stage: Train 0.5 | Epoch: 362 | Iter: 273200 | Total Loss: 0.005264 | Recon Loss: 0.004224 | Commit Loss: 0.002080 | Perplexity: 956.611505
Trainning Epoch:  55%|█████▍    | 363/665 [70:34:34<50:20:28, 600.09s/it]2025-10-03 11:24:03,412 Stage: Train 0.5 | Epoch: 363 | Iter: 273400 | Total Loss: 0.005252 | Recon Loss: 0.004198 | Commit Loss: 0.002107 | Perplexity: 960.414632
2025-10-03 11:26:43,077 Stage: Train 0.5 | Epoch: 363 | Iter: 273600 | Total Loss: 0.005310 | Recon Loss: 0.004265 | Commit Loss: 0.002089 | Perplexity: 962.540962
2025-10-03 11:29:25,291 Stage: Train 0.5 | Epoch: 363 | Iter: 273800 | Total Loss: 0.005221 | Recon Loss: 0.004179 | Commit Loss: 0.002084 | Perplexity: 958.058369
2025-10-03 11:32:02,515 Stage: Train 0.5 | Epoch: 363 | Iter: 274000 | Total Loss: 0.005273 | Recon Loss: 0.004215 | Commit Loss: 0.002115 | Perplexity: 958.833916
Trainning Epoch:  55%|█████▍    | 364/665 [70:44:41<50:20:57, 602.19s/it]2025-10-03 11:34:46,115 Stage: Train 0.5 | Epoch: 364 | Iter: 274200 | Total Loss: 0.005270 | Recon Loss: 0.004221 | Commit Loss: 0.002098 | Perplexity: 957.201857
2025-10-03 11:37:27,195 Stage: Train 0.5 | Epoch: 364 | Iter: 274400 | Total Loss: 0.005257 | Recon Loss: 0.004204 | Commit Loss: 0.002108 | Perplexity: 962.842860
2025-10-03 11:40:03,607 Stage: Train 0.5 | Epoch: 364 | Iter: 274600 | Total Loss: 0.005264 | Recon Loss: 0.004213 | Commit Loss: 0.002102 | Perplexity: 962.120370
2025-10-03 11:42:40,402 Stage: Train 0.5 | Epoch: 364 | Iter: 274800 | Total Loss: 0.005248 | Recon Loss: 0.004207 | Commit Loss: 0.002082 | Perplexity: 957.558564
Trainning Epoch:  55%|█████▍    | 365/665 [70:54:43<50:10:39, 602.13s/it]2025-10-03 11:45:27,513 Stage: Train 0.5 | Epoch: 365 | Iter: 275000 | Total Loss: 0.005229 | Recon Loss: 0.004178 | Commit Loss: 0.002102 | Perplexity: 959.804006
2025-10-03 11:48:07,502 Stage: Train 0.5 | Epoch: 365 | Iter: 275200 | Total Loss: 0.005209 | Recon Loss: 0.004163 | Commit Loss: 0.002091 | Perplexity: 961.735918
2025-10-03 11:50:43,547 Stage: Train 0.5 | Epoch: 365 | Iter: 275400 | Total Loss: 0.005194 | Recon Loss: 0.004147 | Commit Loss: 0.002093 | Perplexity: 958.435467
Trainning Epoch:  55%|█████▌    | 366/665 [71:04:45<50:00:32, 602.11s/it]2025-10-03 11:53:26,471 Stage: Train 0.5 | Epoch: 366 | Iter: 275600 | Total Loss: 0.005315 | Recon Loss: 0.004261 | Commit Loss: 0.002106 | Perplexity: 959.918452
2025-10-03 11:56:04,059 Stage: Train 0.5 | Epoch: 366 | Iter: 275800 | Total Loss: 0.005300 | Recon Loss: 0.004261 | Commit Loss: 0.002077 | Perplexity: 958.997374
2025-10-03 11:58:40,413 Stage: Train 0.5 | Epoch: 366 | Iter: 276000 | Total Loss: 0.005178 | Recon Loss: 0.004136 | Commit Loss: 0.002083 | Perplexity: 960.062582
2025-10-03 12:01:19,307 Stage: Train 0.5 | Epoch: 366 | Iter: 276200 | Total Loss: 0.005272 | Recon Loss: 0.004220 | Commit Loss: 0.002103 | Perplexity: 958.108109
Trainning Epoch:  55%|█████▌    | 367/665 [71:14:46<49:47:28, 601.51s/it]2025-10-03 12:04:04,722 Stage: Train 0.5 | Epoch: 367 | Iter: 276400 | Total Loss: 0.005268 | Recon Loss: 0.004205 | Commit Loss: 0.002126 | Perplexity: 961.440706
2025-10-03 12:06:44,202 Stage: Train 0.5 | Epoch: 367 | Iter: 276600 | Total Loss: 0.005276 | Recon Loss: 0.004228 | Commit Loss: 0.002095 | Perplexity: 961.093759
2025-10-03 12:09:21,525 Stage: Train 0.5 | Epoch: 367 | Iter: 276800 | Total Loss: 0.005263 | Recon Loss: 0.004213 | Commit Loss: 0.002100 | Perplexity: 959.548900
2025-10-03 12:11:55,973 Stage: Train 0.5 | Epoch: 367 | Iter: 277000 | Total Loss: 0.005213 | Recon Loss: 0.004175 | Commit Loss: 0.002076 | Perplexity: 957.762925
Trainning Epoch:  55%|█████▌    | 368/665 [71:24:46<49:35:34, 601.13s/it]2025-10-03 12:14:41,841 Stage: Train 0.5 | Epoch: 368 | Iter: 277200 | Total Loss: 0.005301 | Recon Loss: 0.004247 | Commit Loss: 0.002109 | Perplexity: 954.596784
2025-10-03 12:17:20,965 Stage: Train 0.5 | Epoch: 368 | Iter: 277400 | Total Loss: 0.005235 | Recon Loss: 0.004184 | Commit Loss: 0.002102 | Perplexity: 961.421534
2025-10-03 12:19:58,501 Stage: Train 0.5 | Epoch: 368 | Iter: 277600 | Total Loss: 0.005219 | Recon Loss: 0.004177 | Commit Loss: 0.002082 | Perplexity: 950.682683
2025-10-03 12:22:36,179 Stage: Train 0.5 | Epoch: 368 | Iter: 277800 | Total Loss: 0.005287 | Recon Loss: 0.004238 | Commit Loss: 0.002097 | Perplexity: 963.722687
Trainning Epoch:  55%|█████▌    | 369/665 [71:34:46<49:23:52, 600.79s/it]2025-10-03 12:25:20,019 Stage: Train 0.5 | Epoch: 369 | Iter: 278000 | Total Loss: 0.005198 | Recon Loss: 0.004150 | Commit Loss: 0.002097 | Perplexity: 956.699642
2025-10-03 12:28:00,371 Stage: Train 0.5 | Epoch: 369 | Iter: 278200 | Total Loss: 0.005276 | Recon Loss: 0.004229 | Commit Loss: 0.002092 | Perplexity: 963.081931
2025-10-03 12:30:39,263 Stage: Train 0.5 | Epoch: 369 | Iter: 278400 | Total Loss: 0.005245 | Recon Loss: 0.004193 | Commit Loss: 0.002105 | Perplexity: 959.999074
2025-10-03 12:33:22,089 Stage: Train 0.5 | Epoch: 369 | Iter: 278600 | Total Loss: 0.005274 | Recon Loss: 0.004230 | Commit Loss: 0.002089 | Perplexity: 953.847286
Trainning Epoch:  56%|█████▌    | 370/665 [71:44:56<49:28:14, 603.71s/it]2025-10-03 12:36:05,959 Stage: Train 0.5 | Epoch: 370 | Iter: 278800 | Total Loss: 0.005184 | Recon Loss: 0.004140 | Commit Loss: 0.002090 | Perplexity: 959.464678
2025-10-03 12:38:44,709 Stage: Train 0.5 | Epoch: 370 | Iter: 279000 | Total Loss: 0.005211 | Recon Loss: 0.004169 | Commit Loss: 0.002083 | Perplexity: 958.588195
2025-10-03 12:41:23,699 Stage: Train 0.5 | Epoch: 370 | Iter: 279200 | Total Loss: 0.005268 | Recon Loss: 0.004214 | Commit Loss: 0.002107 | Perplexity: 962.477239
Trainning Epoch:  56%|█████▌    | 371/665 [71:55:00<49:18:04, 603.69s/it]2025-10-03 12:44:09,227 Stage: Train 0.5 | Epoch: 371 | Iter: 279400 | Total Loss: 0.005287 | Recon Loss: 0.004245 | Commit Loss: 0.002085 | Perplexity: 956.808817
2025-10-03 12:46:47,522 Stage: Train 0.5 | Epoch: 371 | Iter: 279600 | Total Loss: 0.005211 | Recon Loss: 0.004170 | Commit Loss: 0.002082 | Perplexity: 959.196650
2025-10-03 12:49:28,102 Stage: Train 0.5 | Epoch: 371 | Iter: 279800 | Total Loss: 0.005275 | Recon Loss: 0.004229 | Commit Loss: 0.002093 | Perplexity: 960.733900
2025-10-03 12:52:03,903 Stage: Train 0.5 | Epoch: 371 | Iter: 280000 | Total Loss: 0.005227 | Recon Loss: 0.004177 | Commit Loss: 0.002099 | Perplexity: 955.961748
2025-10-03 12:52:03,903 Saving model at iteration 280000
2025-10-03 12:52:04,097 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000
2025-10-03 12:52:04,407 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000/model.safetensors
2025-10-03 12:52:04,826 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000/optimizer.bin
2025-10-03 12:52:04,827 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000/scheduler.bin
2025-10-03 12:52:04,827 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000/sampler.bin
2025-10-03 12:52:04,828 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_372_step_280000/random_states_0.pkl
Trainning Epoch:  56%|█████▌    | 372/665 [72:05:00<49:03:02, 602.67s/it]2025-10-03 12:54:48,261 Stage: Train 0.5 | Epoch: 372 | Iter: 280200 | Total Loss: 0.005229 | Recon Loss: 0.004180 | Commit Loss: 0.002099 | Perplexity: 959.910630
2025-10-03 12:57:24,819 Stage: Train 0.5 | Epoch: 372 | Iter: 280400 | Total Loss: 0.005219 | Recon Loss: 0.004177 | Commit Loss: 0.002083 | Perplexity: 959.102674
2025-10-03 13:00:01,297 Stage: Train 0.5 | Epoch: 372 | Iter: 280600 | Total Loss: 0.005236 | Recon Loss: 0.004188 | Commit Loss: 0.002095 | Perplexity: 959.779387
2025-10-03 13:02:39,427 Stage: Train 0.5 | Epoch: 372 | Iter: 280800 | Total Loss: 0.005276 | Recon Loss: 0.004231 | Commit Loss: 0.002091 | Perplexity: 955.432945
Trainning Epoch:  56%|█████▌    | 373/665 [72:15:03<48:52:54, 602.65s/it]2025-10-03 13:05:25,283 Stage: Train 0.5 | Epoch: 373 | Iter: 281000 | Total Loss: 0.005212 | Recon Loss: 0.004170 | Commit Loss: 0.002083 | Perplexity: 962.078601
2025-10-03 13:08:02,081 Stage: Train 0.5 | Epoch: 373 | Iter: 281200 | Total Loss: 0.005214 | Recon Loss: 0.004166 | Commit Loss: 0.002096 | Perplexity: 961.805498
2025-10-03 13:10:42,218 Stage: Train 0.5 | Epoch: 373 | Iter: 281400 | Total Loss: 0.005222 | Recon Loss: 0.004172 | Commit Loss: 0.002099 | Perplexity: 960.844634
2025-10-03 13:13:20,346 Stage: Train 0.5 | Epoch: 373 | Iter: 281600 | Total Loss: 0.005257 | Recon Loss: 0.004218 | Commit Loss: 0.002079 | Perplexity: 961.474500
Trainning Epoch:  56%|█████▌    | 374/665 [72:25:05<48:42:07, 602.50s/it]2025-10-03 13:16:06,214 Stage: Train 0.5 | Epoch: 374 | Iter: 281800 | Total Loss: 0.005213 | Recon Loss: 0.004168 | Commit Loss: 0.002090 | Perplexity: 961.881101
2025-10-03 13:18:45,170 Stage: Train 0.5 | Epoch: 374 | Iter: 282000 | Total Loss: 0.005228 | Recon Loss: 0.004178 | Commit Loss: 0.002102 | Perplexity: 959.446370
2025-10-03 13:21:24,202 Stage: Train 0.5 | Epoch: 374 | Iter: 282200 | Total Loss: 0.005240 | Recon Loss: 0.004198 | Commit Loss: 0.002084 | Perplexity: 962.640229
Trainning Epoch:  56%|█████▋    | 375/665 [72:35:08<48:32:13, 602.53s/it]2025-10-03 13:24:07,788 Stage: Train 0.5 | Epoch: 375 | Iter: 282400 | Total Loss: 0.005248 | Recon Loss: 0.004205 | Commit Loss: 0.002086 | Perplexity: 952.551628
2025-10-03 13:26:44,361 Stage: Train 0.5 | Epoch: 375 | Iter: 282600 | Total Loss: 0.005199 | Recon Loss: 0.004155 | Commit Loss: 0.002088 | Perplexity: 959.234236
2025-10-03 13:29:23,675 Stage: Train 0.5 | Epoch: 375 | Iter: 282800 | Total Loss: 0.005242 | Recon Loss: 0.004189 | Commit Loss: 0.002107 | Perplexity: 961.138367
2025-10-03 13:32:02,828 Stage: Train 0.5 | Epoch: 375 | Iter: 283000 | Total Loss: 0.005259 | Recon Loss: 0.004217 | Commit Loss: 0.002083 | Perplexity: 954.282738
Trainning Epoch:  57%|█████▋    | 376/665 [72:45:08<48:18:27, 601.76s/it]2025-10-03 13:34:46,543 Stage: Train 0.5 | Epoch: 376 | Iter: 283200 | Total Loss: 0.005203 | Recon Loss: 0.004168 | Commit Loss: 0.002070 | Perplexity: 953.236932
2025-10-03 13:37:20,528 Stage: Train 0.5 | Epoch: 376 | Iter: 283400 | Total Loss: 0.005155 | Recon Loss: 0.004126 | Commit Loss: 0.002058 | Perplexity: 954.053385
2025-10-03 13:39:56,790 Stage: Train 0.5 | Epoch: 376 | Iter: 283600 | Total Loss: 0.005291 | Recon Loss: 0.004236 | Commit Loss: 0.002110 | Perplexity: 963.546028
2025-10-03 13:42:35,130 Stage: Train 0.5 | Epoch: 376 | Iter: 283800 | Total Loss: 0.005177 | Recon Loss: 0.004138 | Commit Loss: 0.002077 | Perplexity: 957.532062
Trainning Epoch:  57%|█████▋    | 377/665 [72:55:06<48:03:42, 600.77s/it]2025-10-03 13:45:19,207 Stage: Train 0.5 | Epoch: 377 | Iter: 284000 | Total Loss: 0.005212 | Recon Loss: 0.004169 | Commit Loss: 0.002086 | Perplexity: 958.201639
2025-10-03 13:48:00,827 Stage: Train 0.5 | Epoch: 377 | Iter: 284200 | Total Loss: 0.005172 | Recon Loss: 0.004127 | Commit Loss: 0.002089 | Perplexity: 961.281296
2025-10-03 13:50:36,638 Stage: Train 0.5 | Epoch: 377 | Iter: 284400 | Total Loss: 0.005231 | Recon Loss: 0.004186 | Commit Loss: 0.002090 | Perplexity: 959.253496
2025-10-03 13:53:11,944 Stage: Train 0.5 | Epoch: 377 | Iter: 284600 | Total Loss: 0.005252 | Recon Loss: 0.004209 | Commit Loss: 0.002085 | Perplexity: 958.531530
Trainning Epoch:  57%|█████▋    | 378/665 [73:05:05<47:50:32, 600.11s/it]2025-10-03 13:55:57,694 Stage: Train 0.5 | Epoch: 378 | Iter: 284800 | Total Loss: 0.005153 | Recon Loss: 0.004107 | Commit Loss: 0.002092 | Perplexity: 961.931118
2025-10-03 13:58:33,095 Stage: Train 0.5 | Epoch: 378 | Iter: 285000 | Total Loss: 0.005261 | Recon Loss: 0.004208 | Commit Loss: 0.002105 | Perplexity: 957.235543
2025-10-03 14:01:10,922 Stage: Train 0.5 | Epoch: 378 | Iter: 285200 | Total Loss: 0.005184 | Recon Loss: 0.004148 | Commit Loss: 0.002071 | Perplexity: 958.485335
Trainning Epoch:  57%|█████▋    | 379/665 [73:15:04<47:40:07, 600.03s/it]2025-10-03 14:03:54,735 Stage: Train 0.5 | Epoch: 379 | Iter: 285400 | Total Loss: 0.005219 | Recon Loss: 0.004175 | Commit Loss: 0.002089 | Perplexity: 956.634636
2025-10-03 14:06:32,462 Stage: Train 0.5 | Epoch: 379 | Iter: 285600 | Total Loss: 0.005192 | Recon Loss: 0.004150 | Commit Loss: 0.002083 | Perplexity: 960.066161
2025-10-03 14:09:07,330 Stage: Train 0.5 | Epoch: 379 | Iter: 285800 | Total Loss: 0.005166 | Recon Loss: 0.004121 | Commit Loss: 0.002090 | Perplexity: 960.947928
2025-10-03 14:11:46,273 Stage: Train 0.5 | Epoch: 379 | Iter: 286000 | Total Loss: 0.005236 | Recon Loss: 0.004190 | Commit Loss: 0.002093 | Perplexity: 964.733307
Trainning Epoch:  57%|█████▋    | 380/665 [73:25:00<47:23:46, 598.69s/it]2025-10-03 14:14:28,234 Stage: Train 0.5 | Epoch: 380 | Iter: 286200 | Total Loss: 0.005220 | Recon Loss: 0.004180 | Commit Loss: 0.002080 | Perplexity: 958.545081
2025-10-03 14:17:08,181 Stage: Train 0.5 | Epoch: 380 | Iter: 286400 | Total Loss: 0.005167 | Recon Loss: 0.004125 | Commit Loss: 0.002083 | Perplexity: 959.941570
2025-10-03 14:19:42,888 Stage: Train 0.5 | Epoch: 380 | Iter: 286600 | Total Loss: 0.005226 | Recon Loss: 0.004181 | Commit Loss: 0.002090 | Perplexity: 958.016265
2025-10-03 14:22:20,313 Stage: Train 0.5 | Epoch: 380 | Iter: 286800 | Total Loss: 0.005162 | Recon Loss: 0.004117 | Commit Loss: 0.002090 | Perplexity: 962.027395
Trainning Epoch:  57%|█████▋    | 381/665 [73:35:00<47:15:02, 598.95s/it]2025-10-03 14:25:05,118 Stage: Train 0.5 | Epoch: 381 | Iter: 287000 | Total Loss: 0.005191 | Recon Loss: 0.004144 | Commit Loss: 0.002094 | Perplexity: 958.456556
2025-10-03 14:27:43,318 Stage: Train 0.5 | Epoch: 381 | Iter: 287200 | Total Loss: 0.005196 | Recon Loss: 0.004160 | Commit Loss: 0.002071 | Perplexity: 956.244028
2025-10-03 14:30:23,871 Stage: Train 0.5 | Epoch: 381 | Iter: 287400 | Total Loss: 0.005175 | Recon Loss: 0.004127 | Commit Loss: 0.002096 | Perplexity: 961.958357
2025-10-03 14:33:01,532 Stage: Train 0.5 | Epoch: 381 | Iter: 287600 | Total Loss: 0.005231 | Recon Loss: 0.004187 | Commit Loss: 0.002089 | Perplexity: 957.967376
Trainning Epoch:  57%|█████▋    | 382/665 [73:45:03<47:10:48, 600.17s/it]2025-10-03 14:35:43,963 Stage: Train 0.5 | Epoch: 382 | Iter: 287800 | Total Loss: 0.005223 | Recon Loss: 0.004173 | Commit Loss: 0.002100 | Perplexity: 960.115957
2025-10-03 14:38:21,775 Stage: Train 0.5 | Epoch: 382 | Iter: 288000 | Total Loss: 0.005238 | Recon Loss: 0.004192 | Commit Loss: 0.002093 | Perplexity: 954.303784
2025-10-03 14:40:59,151 Stage: Train 0.5 | Epoch: 382 | Iter: 288200 | Total Loss: 0.005148 | Recon Loss: 0.004107 | Commit Loss: 0.002082 | Perplexity: 961.645276
Trainning Epoch:  58%|█████▊    | 383/665 [73:55:02<47:00:08, 600.03s/it]2025-10-03 14:43:43,055 Stage: Train 0.5 | Epoch: 383 | Iter: 288400 | Total Loss: 0.005131 | Recon Loss: 0.004089 | Commit Loss: 0.002084 | Perplexity: 958.542029
2025-10-03 14:46:22,815 Stage: Train 0.5 | Epoch: 383 | Iter: 288600 | Total Loss: 0.005166 | Recon Loss: 0.004137 | Commit Loss: 0.002058 | Perplexity: 958.544677
2025-10-03 14:49:02,175 Stage: Train 0.5 | Epoch: 383 | Iter: 288800 | Total Loss: 0.005253 | Recon Loss: 0.004211 | Commit Loss: 0.002084 | Perplexity: 961.722198
2025-10-03 14:51:44,502 Stage: Train 0.5 | Epoch: 383 | Iter: 289000 | Total Loss: 0.005189 | Recon Loss: 0.004138 | Commit Loss: 0.002101 | Perplexity: 964.869766
Trainning Epoch:  58%|█████▊    | 384/665 [74:05:14<47:05:54, 603.40s/it]2025-10-03 14:54:32,275 Stage: Train 0.5 | Epoch: 384 | Iter: 289200 | Total Loss: 0.005185 | Recon Loss: 0.004152 | Commit Loss: 0.002065 | Perplexity: 954.087640
2025-10-03 14:57:12,878 Stage: Train 0.5 | Epoch: 384 | Iter: 289400 | Total Loss: 0.005252 | Recon Loss: 0.004208 | Commit Loss: 0.002089 | Perplexity: 962.089129
2025-10-03 14:59:53,024 Stage: Train 0.5 | Epoch: 384 | Iter: 289600 | Total Loss: 0.005179 | Recon Loss: 0.004139 | Commit Loss: 0.002080 | Perplexity: 960.994220
2025-10-03 15:02:28,287 Stage: Train 0.5 | Epoch: 384 | Iter: 289800 | Total Loss: 0.005153 | Recon Loss: 0.004116 | Commit Loss: 0.002074 | Perplexity: 958.572398
Trainning Epoch:  58%|█████▊    | 385/665 [74:15:14<46:52:06, 602.59s/it]2025-10-03 15:05:06,112 Stage: Train 0.5 | Epoch: 385 | Iter: 290000 | Total Loss: 0.005199 | Recon Loss: 0.004160 | Commit Loss: 0.002077 | Perplexity: 958.701687
2025-10-03 15:07:39,410 Stage: Train 0.5 | Epoch: 385 | Iter: 290200 | Total Loss: 0.005161 | Recon Loss: 0.004130 | Commit Loss: 0.002061 | Perplexity: 957.547752
2025-10-03 15:10:12,644 Stage: Train 0.5 | Epoch: 385 | Iter: 290400 | Total Loss: 0.005188 | Recon Loss: 0.004141 | Commit Loss: 0.002093 | Perplexity: 963.500740
2025-10-03 15:12:45,970 Stage: Train 0.5 | Epoch: 385 | Iter: 290600 | Total Loss: 0.005199 | Recon Loss: 0.004157 | Commit Loss: 0.002085 | Perplexity: 961.218464
Trainning Epoch:  58%|█████▊    | 386/665 [74:24:59<46:17:17, 597.27s/it]2025-10-03 15:15:31,502 Stage: Train 0.5 | Epoch: 386 | Iter: 290800 | Total Loss: 0.005264 | Recon Loss: 0.004221 | Commit Loss: 0.002086 | Perplexity: 957.673582
2025-10-03 15:18:06,995 Stage: Train 0.5 | Epoch: 386 | Iter: 291000 | Total Loss: 0.005146 | Recon Loss: 0.004116 | Commit Loss: 0.002060 | Perplexity: 954.838105
2025-10-03 15:20:46,418 Stage: Train 0.5 | Epoch: 386 | Iter: 291200 | Total Loss: 0.005194 | Recon Loss: 0.004155 | Commit Loss: 0.002079 | Perplexity: 953.485847
2025-10-03 15:23:22,637 Stage: Train 0.5 | Epoch: 386 | Iter: 291400 | Total Loss: 0.005192 | Recon Loss: 0.004155 | Commit Loss: 0.002074 | Perplexity: 960.432033
Trainning Epoch:  58%|█████▊    | 387/665 [74:34:57<46:08:48, 597.58s/it]2025-10-03 15:26:06,597 Stage: Train 0.5 | Epoch: 387 | Iter: 291600 | Total Loss: 0.005135 | Recon Loss: 0.004102 | Commit Loss: 0.002067 | Perplexity: 955.234548
2025-10-03 15:28:44,055 Stage: Train 0.5 | Epoch: 387 | Iter: 291800 | Total Loss: 0.005159 | Recon Loss: 0.004119 | Commit Loss: 0.002079 | Perplexity: 960.643241
2025-10-03 15:31:24,948 Stage: Train 0.5 | Epoch: 387 | Iter: 292000 | Total Loss: 0.005211 | Recon Loss: 0.004164 | Commit Loss: 0.002094 | Perplexity: 962.561772
Trainning Epoch:  58%|█████▊    | 388/665 [74:44:59<46:04:59, 598.92s/it]2025-10-03 15:34:08,333 Stage: Train 0.5 | Epoch: 388 | Iter: 292200 | Total Loss: 0.005127 | Recon Loss: 0.004089 | Commit Loss: 0.002077 | Perplexity: 956.880462
2025-10-03 15:36:44,336 Stage: Train 0.5 | Epoch: 388 | Iter: 292400 | Total Loss: 0.005230 | Recon Loss: 0.004190 | Commit Loss: 0.002080 | Perplexity: 959.168804
2025-10-03 15:39:24,159 Stage: Train 0.5 | Epoch: 388 | Iter: 292600 | Total Loss: 0.005153 | Recon Loss: 0.004109 | Commit Loss: 0.002089 | Perplexity: 964.512281
2025-10-03 15:42:05,734 Stage: Train 0.5 | Epoch: 388 | Iter: 292800 | Total Loss: 0.005159 | Recon Loss: 0.004122 | Commit Loss: 0.002074 | Perplexity: 957.286358
Trainning Epoch:  58%|█████▊    | 389/665 [74:55:06<46:05:25, 601.18s/it]2025-10-03 15:44:53,496 Stage: Train 0.5 | Epoch: 389 | Iter: 293000 | Total Loss: 0.005123 | Recon Loss: 0.004090 | Commit Loss: 0.002065 | Perplexity: 957.726046
2025-10-03 15:47:35,986 Stage: Train 0.5 | Epoch: 389 | Iter: 293200 | Total Loss: 0.005200 | Recon Loss: 0.004165 | Commit Loss: 0.002070 | Perplexity: 959.494405
2025-10-03 15:50:16,676 Stage: Train 0.5 | Epoch: 389 | Iter: 293400 | Total Loss: 0.005189 | Recon Loss: 0.004151 | Commit Loss: 0.002076 | Perplexity: 958.326624
2025-10-03 15:52:57,242 Stage: Train 0.5 | Epoch: 389 | Iter: 293600 | Total Loss: 0.005230 | Recon Loss: 0.004188 | Commit Loss: 0.002084 | Perplexity: 963.790757
Trainning Epoch:  59%|█████▊    | 390/665 [75:05:17<46:08:57, 604.13s/it]2025-10-03 15:55:44,516 Stage: Train 0.5 | Epoch: 390 | Iter: 293800 | Total Loss: 0.005176 | Recon Loss: 0.004129 | Commit Loss: 0.002094 | Perplexity: 964.086080
2025-10-03 15:58:21,512 Stage: Train 0.5 | Epoch: 390 | Iter: 294000 | Total Loss: 0.005173 | Recon Loss: 0.004148 | Commit Loss: 0.002049 | Perplexity: 954.852547
2025-10-03 16:01:02,103 Stage: Train 0.5 | Epoch: 390 | Iter: 294200 | Total Loss: 0.005132 | Recon Loss: 0.004094 | Commit Loss: 0.002077 | Perplexity: 961.733681
2025-10-03 16:03:38,132 Stage: Train 0.5 | Epoch: 390 | Iter: 294400 | Total Loss: 0.005134 | Recon Loss: 0.004089 | Commit Loss: 0.002090 | Perplexity: 962.720702
Trainning Epoch:  59%|█████▉    | 391/665 [75:15:21<45:59:20, 604.24s/it]2025-10-03 16:06:17,579 Stage: Train 0.5 | Epoch: 391 | Iter: 294600 | Total Loss: 0.005239 | Recon Loss: 0.004205 | Commit Loss: 0.002068 | Perplexity: 956.532418
2025-10-03 16:08:49,669 Stage: Train 0.5 | Epoch: 391 | Iter: 294800 | Total Loss: 0.005133 | Recon Loss: 0.004095 | Commit Loss: 0.002077 | Perplexity: 961.636463
2025-10-03 16:11:26,884 Stage: Train 0.5 | Epoch: 391 | Iter: 295000 | Total Loss: 0.005171 | Recon Loss: 0.004136 | Commit Loss: 0.002069 | Perplexity: 960.358437
Trainning Epoch:  59%|█████▉    | 392/665 [75:25:08<45:25:17, 598.97s/it]2025-10-03 16:14:08,140 Stage: Train 0.5 | Epoch: 392 | Iter: 295200 | Total Loss: 0.005116 | Recon Loss: 0.004072 | Commit Loss: 0.002089 | Perplexity: 960.639236
2025-10-03 16:16:47,729 Stage: Train 0.5 | Epoch: 392 | Iter: 295400 | Total Loss: 0.005155 | Recon Loss: 0.004123 | Commit Loss: 0.002064 | Perplexity: 958.999773
2025-10-03 16:19:29,990 Stage: Train 0.5 | Epoch: 392 | Iter: 295600 | Total Loss: 0.005133 | Recon Loss: 0.004101 | Commit Loss: 0.002064 | Perplexity: 960.175671
2025-10-03 16:22:12,346 Stage: Train 0.5 | Epoch: 392 | Iter: 295800 | Total Loss: 0.005199 | Recon Loss: 0.004152 | Commit Loss: 0.002094 | Perplexity: 961.184194
Trainning Epoch:  59%|█████▉    | 393/665 [75:35:20<45:33:04, 602.89s/it]2025-10-03 16:24:55,531 Stage: Train 0.5 | Epoch: 393 | Iter: 296000 | Total Loss: 0.005189 | Recon Loss: 0.004144 | Commit Loss: 0.002091 | Perplexity: 960.261194
2025-10-03 16:27:40,193 Stage: Train 0.5 | Epoch: 393 | Iter: 296200 | Total Loss: 0.005095 | Recon Loss: 0.004059 | Commit Loss: 0.002071 | Perplexity: 959.382047
2025-10-03 16:30:17,370 Stage: Train 0.5 | Epoch: 393 | Iter: 296400 | Total Loss: 0.005141 | Recon Loss: 0.004097 | Commit Loss: 0.002087 | Perplexity: 963.936440
2025-10-03 16:32:57,213 Stage: Train 0.5 | Epoch: 393 | Iter: 296600 | Total Loss: 0.005179 | Recon Loss: 0.004131 | Commit Loss: 0.002096 | Perplexity: 965.688716
Trainning Epoch:  59%|█████▉    | 394/665 [75:45:28<45:30:29, 604.54s/it]2025-10-03 16:35:47,387 Stage: Train 0.5 | Epoch: 394 | Iter: 296800 | Total Loss: 0.005123 | Recon Loss: 0.004094 | Commit Loss: 0.002057 | Perplexity: 959.658917
2025-10-03 16:38:25,951 Stage: Train 0.5 | Epoch: 394 | Iter: 297000 | Total Loss: 0.005146 | Recon Loss: 0.004106 | Commit Loss: 0.002080 | Perplexity: 961.646956
2025-10-03 16:41:06,141 Stage: Train 0.5 | Epoch: 394 | Iter: 297200 | Total Loss: 0.005190 | Recon Loss: 0.004147 | Commit Loss: 0.002087 | Perplexity: 960.196814
2025-10-03 16:43:42,463 Stage: Train 0.5 | Epoch: 394 | Iter: 297400 | Total Loss: 0.005149 | Recon Loss: 0.004110 | Commit Loss: 0.002079 | Perplexity: 961.745595
Trainning Epoch:  59%|█████▉    | 395/665 [75:55:37<45:25:37, 605.69s/it]2025-10-03 16:46:34,072 Stage: Train 0.5 | Epoch: 395 | Iter: 297600 | Total Loss: 0.005177 | Recon Loss: 0.004140 | Commit Loss: 0.002075 | Perplexity: 962.827508
2025-10-03 16:49:12,083 Stage: Train 0.5 | Epoch: 395 | Iter: 297800 | Total Loss: 0.005132 | Recon Loss: 0.004097 | Commit Loss: 0.002070 | Perplexity: 958.467432
2025-10-03 16:51:51,839 Stage: Train 0.5 | Epoch: 395 | Iter: 298000 | Total Loss: 0.005154 | Recon Loss: 0.004119 | Commit Loss: 0.002070 | Perplexity: 963.177436
Trainning Epoch:  60%|█████▉    | 396/665 [76:05:47<45:21:40, 607.07s/it]2025-10-03 16:54:35,327 Stage: Train 0.5 | Epoch: 396 | Iter: 298200 | Total Loss: 0.005193 | Recon Loss: 0.004159 | Commit Loss: 0.002068 | Perplexity: 955.870948
2025-10-03 16:57:11,217 Stage: Train 0.5 | Epoch: 396 | Iter: 298400 | Total Loss: 0.005135 | Recon Loss: 0.004103 | Commit Loss: 0.002064 | Perplexity: 960.277041
2025-10-03 16:59:48,663 Stage: Train 0.5 | Epoch: 396 | Iter: 298600 | Total Loss: 0.005168 | Recon Loss: 0.004133 | Commit Loss: 0.002071 | Perplexity: 961.248532
2025-10-03 17:02:25,937 Stage: Train 0.5 | Epoch: 396 | Iter: 298800 | Total Loss: 0.005159 | Recon Loss: 0.004119 | Commit Loss: 0.002079 | Perplexity: 961.218946
Trainning Epoch:  60%|█████▉    | 397/665 [76:15:47<45:01:21, 604.78s/it]2025-10-03 17:05:12,181 Stage: Train 0.5 | Epoch: 397 | Iter: 299000 | Total Loss: 0.005186 | Recon Loss: 0.004153 | Commit Loss: 0.002067 | Perplexity: 956.796673
2025-10-03 17:07:52,373 Stage: Train 0.5 | Epoch: 397 | Iter: 299200 | Total Loss: 0.005065 | Recon Loss: 0.004030 | Commit Loss: 0.002069 | Perplexity: 959.897671
2025-10-03 17:10:33,323 Stage: Train 0.5 | Epoch: 397 | Iter: 299400 | Total Loss: 0.005127 | Recon Loss: 0.004088 | Commit Loss: 0.002078 | Perplexity: 958.986644
2025-10-03 17:13:12,623 Stage: Train 0.5 | Epoch: 397 | Iter: 299600 | Total Loss: 0.005188 | Recon Loss: 0.004145 | Commit Loss: 0.002086 | Perplexity: 965.223771
Trainning Epoch:  60%|█████▉    | 398/665 [76:25:53<44:53:28, 605.27s/it]2025-10-03 17:15:57,632 Stage: Train 0.5 | Epoch: 398 | Iter: 299800 | Total Loss: 0.005160 | Recon Loss: 0.004115 | Commit Loss: 0.002089 | Perplexity: 966.125565
2025-10-03 17:18:36,991 Stage: Train 0.5 | Epoch: 398 | Iter: 300000 | Total Loss: 0.005121 | Recon Loss: 0.004096 | Commit Loss: 0.002051 | Perplexity: 958.830187
2025-10-03 17:18:36,991 Saving model at iteration 300000
2025-10-03 17:18:37,185 Saving current state to vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000
2025-10-03 17:18:37,483 Model weights saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000/model.safetensors
2025-10-03 17:18:37,885 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000/optimizer.bin
2025-10-03 17:18:37,886 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000/scheduler.bin
2025-10-03 17:18:37,886 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000/sampler.bin
2025-10-03 17:18:37,887 Random states saved in vqvae_experiment/joint_only/joint3d_image_affined_192x256/f64s1d64_cb4096x2048_mpjpe/models/checkpoint_epoch_399_step_300000/random_states_0.pkl
2025-10-03 17:21:13,622 Stage: Train 0.5 | Epoch: 398 | Iter: 300200 | Total Loss: 0.005164 | Recon Loss: 0.004141 | Commit Loss: 0.002047 | Perplexity: 955.697818
2025-10-03 17:23:50,252 Stage: Train 0.5 | Epoch: 398 | Iter: 300400 | Total Loss: 0.005169 | Recon Loss: 0.004124 | Commit Loss: 0.002091 | Perplexity: 962.181887
Trainning Epoch:  60%|██████    | 399/665 [76:35:52<44:34:51, 603.35s/it]2025-10-03 17:26:36,950 Stage: Train 0.5 | Epoch: 399 | Iter: 300600 | Total Loss: 0.005111 | Recon Loss: 0.004070 | Commit Loss: 0.002081 | Perplexity: 961.387808
2025-10-03 17:29:15,991 Stage: Train 0.5 | Epoch: 399 | Iter: 300800 | Total Loss: 0.005111 | Recon Loss: 0.004081 | Commit Loss: 0.002061 | Perplexity: 961.412412
2025-10-03 17:31:53,051 Stage: Train 0.5 | Epoch: 399 | Iter: 301000 | Total Loss: 0.005141 | Recon Loss: 0.004096 | Commit Loss: 0.002091 | Perplexity: 961.753087
2025-10-03 17:34:31,403 Stage: Train 0.5 | Epoch: 399 | Iter: 301200 | Total Loss: 0.005106 | Recon Loss: 0.004067 | Commit Loss: 0.002078 | Perplexity: 959.401968
Trainning Epoch:  60%|██████    | 400/665 [76:45:59<44:29:51, 604.50s/it]2025-10-03 17:37:18,584 Stage: Train 0.5 | Epoch: 400 | Iter: 301400 | Total Loss: 0.005141 | Recon Loss: 0.004110 | Commit Loss: 0.002062 | Perplexity: 953.477229
2025-10-03 17:39:56,561 Stage: Train 0.5 | Epoch: 400 | Iter: 301600 | Total Loss: 0.005093 | Recon Loss: 0.004055 | Commit Loss: 0.002077 | Perplexity: 961.115964
2025-10-03 17:42:34,286 Stage: Train 0.5 | Epoch: 400 | Iter: 301800 | Total Loss: 0.005167 | Recon Loss: 0.004125 | Commit Loss: 0.002085 | Perplexity: 961.257964
Trainning Epoch:  60%|██████    | 401/665 [76:56:00<44:14:31, 603.30s/it]2025-10-03 17:45:15,939 Stage: Train 0.5 | Epoch: 401 | Iter: 302000 | Total Loss: 0.005134 | Recon Loss: 0.004098 | Commit Loss: 0.002072 | Perplexity: 961.440518
2025-10-03 17:47:52,590 Stage: Train 0.5 | Epoch: 401 | Iter: 302200 | Total Loss: 0.005116 | Recon Loss: 0.004082 | Commit Loss: 0.002068 | Perplexity: 962.547509
2025-10-03 17:50:36,117 Stage: Train 0.5 | Epoch: 401 | Iter: 302400 | Total Loss: 0.005182 | Recon Loss: 0.004139 | Commit Loss: 0.002087 | Perplexity: 962.906713
2025-10-03 17:53:17,448 Stage: Train 0.5 | Epoch: 401 | Iter: 302600 | Total Loss: 0.005180 | Recon Loss: 0.004148 | Commit Loss: 0.002064 | Perplexity: 958.440888
Trainning Epoch:  60%|██████    | 402/665 [77:06:08<44:11:05, 604.81s/it]2025-10-03 17:56:01,771 Stage: Train 0.5 | Epoch: 402 | Iter: 302800 | Total Loss: 0.005107 | Recon Loss: 0.004076 | Commit Loss: 0.002062 | Perplexity: 955.394770
2025-10-03 17:58:44,246 Stage: Train 0.5 | Epoch: 402 | Iter: 303000 | Total Loss: 0.005116 | Recon Loss: 0.004084 | Commit Loss: 0.002065 | Perplexity: 961.101915
2025-10-03 18:01:29,702 Stage: Train 0.5 | Epoch: 402 | Iter: 303200 | Total Loss: 0.005188 | Recon Loss: 0.004141 | Commit Loss: 0.002095 | Perplexity: 960.331137
2025-10-03 18:04:09,509 Stage: Train 0.5 | Epoch: 402 | Iter: 303400 | Total Loss: 0.005099 | Recon Loss: 0.004072 | Commit Loss: 0.002053 | Perplexity: 952.948097
Trainning Epoch:  61%|██████    | 403/665 [77:16:22<44:13:28, 607.66s/it]2025-10-03 18:06:56,766 Stage: Train 0.5 | Epoch: 403 | Iter: 303600 | Total Loss: 0.005137 | Recon Loss: 0.004113 | Commit Loss: 0.002047 | Perplexity: 954.085848
2025-10-03 18:09:38,527 Stage: Train 0.5 | Epoch: 403 | Iter: 303800 | Total Loss: 0.005122 | Recon Loss: 0.004093 | Commit Loss: 0.002058 | Perplexity: 953.550128
2025-10-03 18:12:19,434 Stage: Train 0.5 | Epoch: 403 | Iter: 304000 | Total Loss: 0.005169 | Recon Loss: 0.004128 | Commit Loss: 0.002083 | Perplexity: 963.812194
2025-10-03 18:15:04,466 Stage: Train 0.5 | Epoch: 403 | Iter: 304200 | Total Loss: 0.005161 | Recon Loss: 0.004121 | Commit Loss: 0.002080 | Perplexity: 963.622304
Trainning Epoch:  61%|██████    | 404/665 [77:26:40<44:16:32, 610.70s/it]2025-10-03 18:17:54,900 Stage: Train 0.5 | Epoch: 404 | Iter: 304400 | Total Loss: 0.005189 | Recon Loss: 0.004154 | Commit Loss: 0.002071 | Perplexity: 954.572338
2025-10-03 18:20:41,051 Stage: Train 0.5 | Epoch: 404 | Iter: 304600 | Total Loss: 0.005149 | Recon Loss: 0.004120 | Commit Loss: 0.002059 | Perplexity: 956.665791
2025-10-03 18:23:26,883 Stage: Train 0.5 | Epoch: 404 | Iter: 304800 | Total Loss: 0.005072 | Recon Loss: 0.004043 | Commit Loss: 0.002058 | Perplexity: 958.742470
Trainning Epoch:  61%|██████    | 405/665 [77:37:11<44:32:51, 616.81s/it]2025-10-03 18:26:20,998 Stage: Train 0.5 | Epoch: 405 | Iter: 305000 | Total Loss: 0.005110 | Recon Loss: 0.004069 | Commit Loss: 0.002083 | Perplexity: 963.144604
2025-10-03 18:29:11,560 Stage: Train 0.5 | Epoch: 405 | Iter: 305200 | Total Loss: 0.005118 | Recon Loss: 0.004085 | Commit Loss: 0.002066 | Perplexity: 963.656490
2025-10-03 18:32:01,408 Stage: Train 0.5 | Epoch: 405 | Iter: 305400 | Total Loss: 0.005124 | Recon Loss: 0.004091 | Commit Loss: 0.002066 | Perplexity: 958.054560
2025-10-03 18:35:10,196 Stage: Train 0.5 | Epoch: 405 | Iter: 305600 | Total Loss: 0.005141 | Recon Loss: 0.004109 | Commit Loss: 0.002066 | Perplexity: 961.738388
Trainning Epoch:  61%|██████    | 406/665 [77:48:21<45:31:04, 632.68s/it]2025-10-03 18:38:10,758 Stage: Train 0.5 | Epoch: 406 | Iter: 305800 | Total Loss: 0.005089 | Recon Loss: 0.004052 | Commit Loss: 0.002074 | Perplexity: 963.093566
2025-10-03 18:40:59,650 Stage: Train 0.5 | Epoch: 406 | Iter: 306000 | Total Loss: 0.005083 | Recon Loss: 0.004058 | Commit Loss: 0.002051 | Perplexity: 958.047449
2025-10-03 18:43:44,863 Stage: Train 0.5 | Epoch: 406 | Iter: 306200 | Total Loss: 0.005142 | Recon Loss: 0.004105 | Commit Loss: 0.002073 | Perplexity: 965.832781
