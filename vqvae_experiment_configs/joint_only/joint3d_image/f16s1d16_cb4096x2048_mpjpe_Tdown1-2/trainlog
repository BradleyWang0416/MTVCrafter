The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-26 14:12:22,475 
python train_vqvae_new.py --batch_size 64 --config vqvae_experiment_configs/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/config.yaml --data_mode null --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2 --not_find_unused_parameters --nb_code 4096 --codebook_dim 2048 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level 3 --vision_guidance_ratio 0 --fix_weights --resume_pth  --joint_data_type joint3d_image_normed --downsample_time [1,2] --frame_upsample_rate [2.0,1.0]
2025-09-26 14:12:32,083 Data loaded with 97196 samples
2025-09-26 14:12:32,495 Trainable parameters: 38,884,867
2025-09-26 14:12:32,495 Non-trainable parameters: 0
2025-09-26 14:12:33,800 Number of trainable parameters: 38.884867 M
2025-09-26 14:12:33,801 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'null', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [[]], 'normalize': 'isotropic', 'filter_invalid_images': False, 'processed_image_shape': None, 'backbone': 'hrnet_32', 'get_item_list': ['factor_2_5d', 'joint3d_image_normed', 'joint3d_image_scale', 'joint3d_image_transl', 'joint_2_5d_image'], 'config': 'vqvae_experiment_configs/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/config.yaml', 'resume_pth': '', 'batch_size': 64, 'commit_ratio': 0.5, 'nb_code': 4096, 'codebook_dim': 2048, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_image_normed', 'hrnet_output_level': 3, 'fix_weights': True, 'vision_guidance_ratio': 0.0, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0]}
Trainning Epoch:   0%|          | 0/330 [00:00<?, ?it/s]2025-09-26 14:13:42,120 current_lr 0.000008 at iteration 200
2025-09-26 14:13:42,543 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.178013 | Recon Loss: 0.174108 | Commit Loss: 0.007809 | Perplexity: 1524.927103
2025-09-26 14:15:12,831 current_lr 0.000016 at iteration 400
2025-09-26 14:15:13,264 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.100771 | Recon Loss: 0.084636 | Commit Loss: 0.032271 | Perplexity: 1344.877741
2025-09-26 14:16:43,551 current_lr 0.000024 at iteration 600
2025-09-26 14:16:43,985 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.102690 | Recon Loss: 0.068504 | Commit Loss: 0.068372 | Perplexity: 1406.460355
2025-09-26 14:18:14,010 current_lr 0.000032 at iteration 800
2025-09-26 14:18:14,428 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.100207 | Recon Loss: 0.059406 | Commit Loss: 0.081603 | Perplexity: 1476.420356
2025-09-26 14:19:44,587 current_lr 0.000040 at iteration 1000
2025-09-26 14:19:45,011 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.094574 | Recon Loss: 0.054543 | Commit Loss: 0.080061 | Perplexity: 1501.924360
2025-09-26 14:21:15,221 current_lr 0.000048 at iteration 1200
2025-09-26 14:21:15,650 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.087315 | Recon Loss: 0.051255 | Commit Loss: 0.072120 | Perplexity: 1508.769741
2025-09-26 14:22:45,744 current_lr 0.000056 at iteration 1400
2025-09-26 14:22:46,173 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.077464 | Recon Loss: 0.046397 | Commit Loss: 0.062133 | Perplexity: 1517.323661
Trainning Epoch:   0%|          | 1/330 [11:06<60:52:55, 666.19s/it]2025-09-26 14:24:16,480 current_lr 0.000064 at iteration 1600
2025-09-26 14:24:16,916 Stage: Warm Up | Epoch: 1 | Iter: 1600 | Total Loss: 0.069549 | Recon Loss: 0.043763 | Commit Loss: 0.051571 | Perplexity: 1521.873880
2025-09-26 14:25:47,038 current_lr 0.000072 at iteration 1800
2025-09-26 14:25:47,460 Stage: Warm Up | Epoch: 1 | Iter: 1800 | Total Loss: 0.061637 | Recon Loss: 0.039524 | Commit Loss: 0.044226 | Perplexity: 1532.717258
2025-09-26 14:27:17,521 current_lr 0.000080 at iteration 2000
2025-09-26 14:27:17,951 Stage: Warm Up | Epoch: 1 | Iter: 2000 | Total Loss: 0.055554 | Recon Loss: 0.037532 | Commit Loss: 0.036045 | Perplexity: 1526.817674
2025-09-26 14:28:48,035 current_lr 0.000088 at iteration 2200
2025-09-26 14:28:48,459 Stage: Warm Up | Epoch: 1 | Iter: 2200 | Total Loss: 0.050112 | Recon Loss: 0.035274 | Commit Loss: 0.029677 | Perplexity: 1526.069578
2025-09-26 14:30:18,558 current_lr 0.000096 at iteration 2400
2025-09-26 14:30:18,989 Stage: Warm Up | Epoch: 1 | Iter: 2400 | Total Loss: 0.044913 | Recon Loss: 0.032588 | Commit Loss: 0.024652 | Perplexity: 1513.188358
2025-09-26 14:31:48,795 current_lr 0.000104 at iteration 2600
2025-09-26 14:31:49,233 Stage: Warm Up | Epoch: 1 | Iter: 2600 | Total Loss: 0.041222 | Recon Loss: 0.031429 | Commit Loss: 0.019586 | Perplexity: 1500.608102
2025-09-26 14:33:00,057 current_lr 0.000112 at iteration 2800
2025-09-26 14:33:00,392 Stage: Warm Up | Epoch: 1 | Iter: 2800 | Total Loss: 0.037476 | Recon Loss: 0.029651 | Commit Loss: 0.015649 | Perplexity: 1487.565944
2025-09-26 14:34:06,340 current_lr 0.000120 at iteration 3000
2025-09-26 14:34:06,655 Stage: Warm Up | Epoch: 1 | Iter: 3000 | Total Loss: 0.034074 | Recon Loss: 0.027673 | Commit Loss: 0.012803 | Perplexity: 1467.896122
Trainning Epoch:   1%|          | 2/330 [21:45<59:15:11, 650.34s/it]2025-09-26 14:35:12,982 current_lr 0.000128 at iteration 3200
2025-09-26 14:35:13,309 Stage: Warm Up | Epoch: 2 | Iter: 3200 | Total Loss: 0.031005 | Recon Loss: 0.025794 | Commit Loss: 0.010422 | Perplexity: 1449.654385
2025-09-26 14:36:19,402 current_lr 0.000136 at iteration 3400
2025-09-26 14:36:19,713 Stage: Warm Up | Epoch: 2 | Iter: 3400 | Total Loss: 0.029076 | Recon Loss: 0.024828 | Commit Loss: 0.008495 | Perplexity: 1426.458206
2025-09-26 14:37:25,827 current_lr 0.000144 at iteration 3600
2025-09-26 14:37:26,139 Stage: Warm Up | Epoch: 2 | Iter: 3600 | Total Loss: 0.028422 | Recon Loss: 0.024750 | Commit Loss: 0.007344 | Perplexity: 1408.008063
2025-09-26 14:38:32,282 current_lr 0.000152 at iteration 3800
2025-09-26 14:38:32,599 Stage: Warm Up | Epoch: 2 | Iter: 3800 | Total Loss: 0.026012 | Recon Loss: 0.022695 | Commit Loss: 0.006634 | Perplexity: 1419.089608
2025-09-26 14:39:38,917 current_lr 0.000160 at iteration 4000
2025-09-26 14:39:39,231 Stage: Warm Up | Epoch: 2 | Iter: 4000 | Total Loss: 0.025888 | Recon Loss: 0.022922 | Commit Loss: 0.005934 | Perplexity: 1420.834955
2025-09-26 14:40:45,557 current_lr 0.000168 at iteration 4200
2025-09-26 14:40:45,879 Stage: Warm Up | Epoch: 2 | Iter: 4200 | Total Loss: 0.025415 | Recon Loss: 0.022703 | Commit Loss: 0.005423 | Perplexity: 1424.362208
2025-09-26 14:41:52,001 current_lr 0.000176 at iteration 4400
2025-09-26 14:41:52,329 Stage: Warm Up | Epoch: 2 | Iter: 4400 | Total Loss: 0.024705 | Recon Loss: 0.022161 | Commit Loss: 0.005088 | Perplexity: 1440.071594
Trainning Epoch:   1%|          | 3/330 [30:10<53:03:17, 584.09s/it]2025-09-26 14:42:58,603 current_lr 0.000184 at iteration 4600
2025-09-26 14:42:58,915 Stage: Warm Up | Epoch: 3 | Iter: 4600 | Total Loss: 0.025074 | Recon Loss: 0.022711 | Commit Loss: 0.004726 | Perplexity: 1441.801903
2025-09-26 14:44:04,692 current_lr 0.000192 at iteration 4800
2025-09-26 14:44:05,022 Stage: Warm Up | Epoch: 3 | Iter: 4800 | Total Loss: 0.023972 | Recon Loss: 0.021717 | Commit Loss: 0.004509 | Perplexity: 1439.142596
2025-09-26 14:45:11,315 current_lr 0.000200 at iteration 5000
2025-09-26 14:45:11,639 Stage: Warm Up | Epoch: 3 | Iter: 5000 | Total Loss: 0.023289 | Recon Loss: 0.021146 | Commit Loss: 0.004286 | Perplexity: 1445.730968
2025-09-26 14:46:18,129 Stage: Train 0.5 | Epoch: 3 | Iter: 5200 | Total Loss: 0.022747 | Recon Loss: 0.020665 | Commit Loss: 0.004163 | Perplexity: 1448.205575
2025-09-26 14:47:18,768 Stage: Train 0.5 | Epoch: 3 | Iter: 5400 | Total Loss: 0.021527 | Recon Loss: 0.019501 | Commit Loss: 0.004051 | Perplexity: 1448.319656
2025-09-26 14:48:02,369 Stage: Train 0.5 | Epoch: 3 | Iter: 5600 | Total Loss: 0.020527 | Recon Loss: 0.018575 | Commit Loss: 0.003903 | Perplexity: 1453.138733
2025-09-26 14:48:45,972 Stage: Train 0.5 | Epoch: 3 | Iter: 5800 | Total Loss: 0.020772 | Recon Loss: 0.018896 | Commit Loss: 0.003753 | Perplexity: 1448.703437
2025-09-26 14:49:29,558 Stage: Train 0.5 | Epoch: 3 | Iter: 6000 | Total Loss: 0.020021 | Recon Loss: 0.018178 | Commit Loss: 0.003687 | Perplexity: 1451.096115
Trainning Epoch:   1%|          | 4/330 [37:12<47:05:04, 519.95s/it]2025-09-26 14:50:13,472 Stage: Train 0.5 | Epoch: 4 | Iter: 6200 | Total Loss: 0.019197 | Recon Loss: 0.017396 | Commit Loss: 0.003603 | Perplexity: 1450.738679
2025-09-26 14:50:57,395 Stage: Train 0.5 | Epoch: 4 | Iter: 6400 | Total Loss: 0.019121 | Recon Loss: 0.017437 | Commit Loss: 0.003369 | Perplexity: 1427.969188
2025-09-26 14:51:40,838 Stage: Train 0.5 | Epoch: 4 | Iter: 6600 | Total Loss: 0.017854 | Recon Loss: 0.016218 | Commit Loss: 0.003272 | Perplexity: 1418.870311
2025-09-26 14:52:24,895 Stage: Train 0.5 | Epoch: 4 | Iter: 6800 | Total Loss: 0.017992 | Recon Loss: 0.016432 | Commit Loss: 0.003120 | Perplexity: 1391.705820
2025-09-26 14:53:07,443 Stage: Train 0.5 | Epoch: 4 | Iter: 7000 | Total Loss: 0.016933 | Recon Loss: 0.015485 | Commit Loss: 0.002896 | Perplexity: 1380.430668
2025-09-26 14:53:51,225 Stage: Train 0.5 | Epoch: 4 | Iter: 7200 | Total Loss: 0.015765 | Recon Loss: 0.014397 | Commit Loss: 0.002735 | Perplexity: 1375.639178
2025-09-26 14:54:34,880 Stage: Train 0.5 | Epoch: 4 | Iter: 7400 | Total Loss: 0.017239 | Recon Loss: 0.016003 | Commit Loss: 0.002471 | Perplexity: 1373.910357
Trainning Epoch:   2%|▏         | 5/330 [42:43<40:48:17, 451.99s/it]2025-09-26 14:55:18,913 Stage: Train 0.5 | Epoch: 5 | Iter: 7600 | Total Loss: 0.015057 | Recon Loss: 0.013918 | Commit Loss: 0.002277 | Perplexity: 1388.072557
2025-09-26 14:56:02,693 Stage: Train 0.5 | Epoch: 5 | Iter: 7800 | Total Loss: 0.014282 | Recon Loss: 0.013190 | Commit Loss: 0.002184 | Perplexity: 1416.856155
2025-09-26 14:56:46,693 Stage: Train 0.5 | Epoch: 5 | Iter: 8000 | Total Loss: 0.014611 | Recon Loss: 0.013576 | Commit Loss: 0.002071 | Perplexity: 1441.551569
2025-09-26 14:57:30,511 Stage: Train 0.5 | Epoch: 5 | Iter: 8200 | Total Loss: 0.013860 | Recon Loss: 0.012869 | Commit Loss: 0.001983 | Perplexity: 1457.117955
2025-09-26 14:58:14,201 Stage: Train 0.5 | Epoch: 5 | Iter: 8400 | Total Loss: 0.014139 | Recon Loss: 0.013183 | Commit Loss: 0.001913 | Perplexity: 1466.283806
2025-09-26 14:58:57,664 Stage: Train 0.5 | Epoch: 5 | Iter: 8600 | Total Loss: 0.014183 | Recon Loss: 0.013246 | Commit Loss: 0.001873 | Perplexity: 1468.715576
2025-09-26 14:59:41,614 Stage: Train 0.5 | Epoch: 5 | Iter: 8800 | Total Loss: 0.013320 | Recon Loss: 0.012398 | Commit Loss: 0.001844 | Perplexity: 1488.324512
2025-09-26 15:00:25,792 Stage: Train 0.5 | Epoch: 5 | Iter: 9000 | Total Loss: 0.013081 | Recon Loss: 0.012178 | Commit Loss: 0.001806 | Perplexity: 1493.510075
Trainning Epoch:   2%|▏         | 6/330 [48:17<37:02:42, 411.61s/it]2025-09-26 15:01:10,008 Stage: Train 0.5 | Epoch: 6 | Iter: 9200 | Total Loss: 0.012924 | Recon Loss: 0.012030 | Commit Loss: 0.001787 | Perplexity: 1496.928766
2025-09-26 15:01:53,623 Stage: Train 0.5 | Epoch: 6 | Iter: 9400 | Total Loss: 0.012993 | Recon Loss: 0.012104 | Commit Loss: 0.001776 | Perplexity: 1498.698349
2025-09-26 15:02:37,555 Stage: Train 0.5 | Epoch: 6 | Iter: 9600 | Total Loss: 0.012793 | Recon Loss: 0.011909 | Commit Loss: 0.001768 | Perplexity: 1505.264229
2025-09-26 15:03:21,581 Stage: Train 0.5 | Epoch: 6 | Iter: 9800 | Total Loss: 0.013083 | Recon Loss: 0.012218 | Commit Loss: 0.001730 | Perplexity: 1503.684289
2025-09-26 15:04:05,621 Stage: Train 0.5 | Epoch: 6 | Iter: 10000 | Total Loss: 0.012839 | Recon Loss: 0.011983 | Commit Loss: 0.001711 | Perplexity: 1506.615335
2025-09-26 15:04:49,782 Stage: Train 0.5 | Epoch: 6 | Iter: 10200 | Total Loss: 0.012195 | Recon Loss: 0.011338 | Commit Loss: 0.001713 | Perplexity: 1500.600764
2025-09-26 15:05:33,616 Stage: Train 0.5 | Epoch: 6 | Iter: 10400 | Total Loss: 0.012303 | Recon Loss: 0.011454 | Commit Loss: 0.001699 | Perplexity: 1500.250953
2025-09-26 15:06:17,747 Stage: Train 0.5 | Epoch: 6 | Iter: 10600 | Total Loss: 0.011890 | Recon Loss: 0.011022 | Commit Loss: 0.001736 | Perplexity: 1501.915482
Trainning Epoch:   2%|▏         | 7/330 [53:51<34:39:26, 386.27s/it]2025-09-26 15:07:01,853 Stage: Train 0.5 | Epoch: 7 | Iter: 10800 | Total Loss: 0.011799 | Recon Loss: 0.010943 | Commit Loss: 0.001711 | Perplexity: 1497.987617
2025-09-26 15:07:45,884 Stage: Train 0.5 | Epoch: 7 | Iter: 11000 | Total Loss: 0.012104 | Recon Loss: 0.011284 | Commit Loss: 0.001640 | Perplexity: 1490.787811
2025-09-26 15:08:29,618 Stage: Train 0.5 | Epoch: 7 | Iter: 11200 | Total Loss: 0.011578 | Recon Loss: 0.010758 | Commit Loss: 0.001640 | Perplexity: 1510.794897
2025-09-26 15:09:13,675 Stage: Train 0.5 | Epoch: 7 | Iter: 11400 | Total Loss: 0.011710 | Recon Loss: 0.010894 | Commit Loss: 0.001633 | Perplexity: 1522.213831
2025-09-26 15:09:57,499 Stage: Train 0.5 | Epoch: 7 | Iter: 11600 | Total Loss: 0.011401 | Recon Loss: 0.010576 | Commit Loss: 0.001650 | Perplexity: 1524.111357
2025-09-26 15:10:41,480 Stage: Train 0.5 | Epoch: 7 | Iter: 11800 | Total Loss: 0.011426 | Recon Loss: 0.010620 | Commit Loss: 0.001613 | Perplexity: 1527.044955
2025-09-26 15:11:25,426 Stage: Train 0.5 | Epoch: 7 | Iter: 12000 | Total Loss: 0.011240 | Recon Loss: 0.010441 | Commit Loss: 0.001599 | Perplexity: 1526.133662
Trainning Epoch:   2%|▏         | 8/330 [59:24<33:03:04, 369.52s/it]2025-09-26 15:12:09,373 Stage: Train 0.5 | Epoch: 8 | Iter: 12200 | Total Loss: 0.011168 | Recon Loss: 0.010362 | Commit Loss: 0.001612 | Perplexity: 1530.716666
2025-09-26 15:12:53,412 Stage: Train 0.5 | Epoch: 8 | Iter: 12400 | Total Loss: 0.011354 | Recon Loss: 0.010568 | Commit Loss: 0.001573 | Perplexity: 1533.123299
2025-09-26 15:13:37,288 Stage: Train 0.5 | Epoch: 8 | Iter: 12600 | Total Loss: 0.010886 | Recon Loss: 0.010085 | Commit Loss: 0.001602 | Perplexity: 1552.403812
2025-09-26 15:14:21,246 Stage: Train 0.5 | Epoch: 8 | Iter: 12800 | Total Loss: 0.011257 | Recon Loss: 0.010475 | Commit Loss: 0.001563 | Perplexity: 1556.166080
2025-09-26 15:15:05,239 Stage: Train 0.5 | Epoch: 8 | Iter: 13000 | Total Loss: 0.010819 | Recon Loss: 0.010009 | Commit Loss: 0.001620 | Perplexity: 1557.835632
2025-09-26 15:15:49,253 Stage: Train 0.5 | Epoch: 8 | Iter: 13200 | Total Loss: 0.010811 | Recon Loss: 0.010024 | Commit Loss: 0.001573 | Perplexity: 1556.392699
2025-09-26 15:16:33,246 Stage: Train 0.5 | Epoch: 8 | Iter: 13400 | Total Loss: 0.011175 | Recon Loss: 0.010389 | Commit Loss: 0.001571 | Perplexity: 1554.032259
2025-09-26 15:17:17,326 Stage: Train 0.5 | Epoch: 8 | Iter: 13600 | Total Loss: 0.010280 | Recon Loss: 0.009469 | Commit Loss: 0.001621 | Perplexity: 1554.691544
Trainning Epoch:   3%|▎         | 9/330 [1:04:59<31:58:11, 358.54s/it]2025-09-26 15:18:01,647 Stage: Train 0.5 | Epoch: 9 | Iter: 13800 | Total Loss: 0.010796 | Recon Loss: 0.009988 | Commit Loss: 0.001616 | Perplexity: 1548.287917
2025-09-26 15:18:45,327 Stage: Train 0.5 | Epoch: 9 | Iter: 14000 | Total Loss: 0.010319 | Recon Loss: 0.009509 | Commit Loss: 0.001621 | Perplexity: 1556.805573
2025-09-26 15:19:29,396 Stage: Train 0.5 | Epoch: 9 | Iter: 14200 | Total Loss: 0.010246 | Recon Loss: 0.009413 | Commit Loss: 0.001666 | Perplexity: 1555.208705
2025-09-26 15:20:13,010 Stage: Train 0.5 | Epoch: 9 | Iter: 14400 | Total Loss: 0.009984 | Recon Loss: 0.009149 | Commit Loss: 0.001669 | Perplexity: 1549.481913
2025-09-26 15:20:56,548 Stage: Train 0.5 | Epoch: 9 | Iter: 14600 | Total Loss: 0.010750 | Recon Loss: 0.009940 | Commit Loss: 0.001620 | Perplexity: 1542.582045
2025-09-26 15:21:40,165 Stage: Train 0.5 | Epoch: 9 | Iter: 14800 | Total Loss: 0.009980 | Recon Loss: 0.009148 | Commit Loss: 0.001663 | Perplexity: 1544.627722
2025-09-26 15:22:23,520 Stage: Train 0.5 | Epoch: 9 | Iter: 15000 | Total Loss: 0.010675 | Recon Loss: 0.009841 | Commit Loss: 0.001669 | Perplexity: 1540.723886
Trainning Epoch:   3%|▎         | 10/330 [1:10:31<31:08:44, 350.39s/it]2025-09-26 15:23:07,544 Stage: Train 0.5 | Epoch: 10 | Iter: 15200 | Total Loss: 0.009842 | Recon Loss: 0.008998 | Commit Loss: 0.001689 | Perplexity: 1542.464526
2025-09-26 15:23:51,267 Stage: Train 0.5 | Epoch: 10 | Iter: 15400 | Total Loss: 0.010231 | Recon Loss: 0.009406 | Commit Loss: 0.001650 | Perplexity: 1539.667817
2025-09-26 15:24:34,974 Stage: Train 0.5 | Epoch: 10 | Iter: 15600 | Total Loss: 0.010005 | Recon Loss: 0.009174 | Commit Loss: 0.001663 | Perplexity: 1541.759182
2025-09-26 15:25:18,676 Stage: Train 0.5 | Epoch: 10 | Iter: 15800 | Total Loss: 0.010165 | Recon Loss: 0.009323 | Commit Loss: 0.001685 | Perplexity: 1537.632683
2025-09-26 15:26:02,217 Stage: Train 0.5 | Epoch: 10 | Iter: 16000 | Total Loss: 0.009782 | Recon Loss: 0.008963 | Commit Loss: 0.001638 | Perplexity: 1536.068916
2025-09-26 15:26:45,878 Stage: Train 0.5 | Epoch: 10 | Iter: 16200 | Total Loss: 0.009745 | Recon Loss: 0.008905 | Commit Loss: 0.001681 | Perplexity: 1527.211249
2025-09-26 15:27:29,747 Stage: Train 0.5 | Epoch: 10 | Iter: 16400 | Total Loss: 0.009325 | Recon Loss: 0.008466 | Commit Loss: 0.001718 | Perplexity: 1529.303374
2025-09-26 15:28:13,524 Stage: Train 0.5 | Epoch: 10 | Iter: 16600 | Total Loss: 0.009878 | Recon Loss: 0.009043 | Commit Loss: 0.001670 | Perplexity: 1522.223438
Trainning Epoch:   3%|▎         | 11/330 [1:16:03<30:33:04, 344.78s/it]2025-09-26 15:28:57,091 Stage: Train 0.5 | Epoch: 11 | Iter: 16800 | Total Loss: 0.009568 | Recon Loss: 0.008712 | Commit Loss: 0.001712 | Perplexity: 1534.431277
2025-09-26 15:29:40,637 Stage: Train 0.5 | Epoch: 11 | Iter: 17000 | Total Loss: 0.009874 | Recon Loss: 0.009047 | Commit Loss: 0.001653 | Perplexity: 1533.792064
2025-09-26 15:30:24,380 Stage: Train 0.5 | Epoch: 11 | Iter: 17200 | Total Loss: 0.009430 | Recon Loss: 0.008586 | Commit Loss: 0.001688 | Perplexity: 1537.540042
2025-09-26 15:31:08,234 Stage: Train 0.5 | Epoch: 11 | Iter: 17400 | Total Loss: 0.011095 | Recon Loss: 0.010286 | Commit Loss: 0.001617 | Perplexity: 1529.529330
2025-09-26 15:31:51,961 Stage: Train 0.5 | Epoch: 11 | Iter: 17600 | Total Loss: 0.009410 | Recon Loss: 0.008627 | Commit Loss: 0.001567 | Perplexity: 1538.278604
2025-09-26 15:32:35,411 Stage: Train 0.5 | Epoch: 11 | Iter: 17800 | Total Loss: 0.009604 | Recon Loss: 0.008804 | Commit Loss: 0.001600 | Perplexity: 1538.017467
2025-09-26 15:33:19,215 Stage: Train 0.5 | Epoch: 11 | Iter: 18000 | Total Loss: 0.009715 | Recon Loss: 0.008904 | Commit Loss: 0.001623 | Perplexity: 1534.042078
2025-09-26 15:34:03,355 Stage: Train 0.5 | Epoch: 11 | Iter: 18200 | Total Loss: 0.009151 | Recon Loss: 0.008320 | Commit Loss: 0.001661 | Perplexity: 1534.253693
Trainning Epoch:   4%|▎         | 12/330 [1:21:35<30:07:15, 340.99s/it]2025-09-26 15:34:47,512 Stage: Train 0.5 | Epoch: 12 | Iter: 18400 | Total Loss: 0.009641 | Recon Loss: 0.008831 | Commit Loss: 0.001619 | Perplexity: 1534.208196
2025-09-26 15:35:31,451 Stage: Train 0.5 | Epoch: 12 | Iter: 18600 | Total Loss: 0.009448 | Recon Loss: 0.008631 | Commit Loss: 0.001633 | Perplexity: 1534.117987
2025-09-26 15:36:15,070 Stage: Train 0.5 | Epoch: 12 | Iter: 18800 | Total Loss: 0.009038 | Recon Loss: 0.008198 | Commit Loss: 0.001681 | Perplexity: 1532.524797
2025-09-26 15:36:59,398 Stage: Train 0.5 | Epoch: 12 | Iter: 19000 | Total Loss: 0.009397 | Recon Loss: 0.008572 | Commit Loss: 0.001648 | Perplexity: 1529.069526
2025-09-26 15:37:42,235 Stage: Train 0.5 | Epoch: 12 | Iter: 19200 | Total Loss: 0.009065 | Recon Loss: 0.008236 | Commit Loss: 0.001659 | Perplexity: 1530.109400
2025-09-26 15:38:26,046 Stage: Train 0.5 | Epoch: 12 | Iter: 19400 | Total Loss: 0.009658 | Recon Loss: 0.008847 | Commit Loss: 0.001623 | Perplexity: 1529.035714
2025-09-26 15:39:10,040 Stage: Train 0.5 | Epoch: 12 | Iter: 19600 | Total Loss: 0.008954 | Recon Loss: 0.008128 | Commit Loss: 0.001652 | Perplexity: 1528.760423
Trainning Epoch:   4%|▍         | 13/330 [1:27:07<29:47:26, 338.32s/it]2025-09-26 15:39:53,434 Stage: Train 0.5 | Epoch: 13 | Iter: 19800 | Total Loss: 0.009409 | Recon Loss: 0.008596 | Commit Loss: 0.001625 | Perplexity: 1525.638739
2025-09-26 15:40:37,547 Stage: Train 0.5 | Epoch: 13 | Iter: 20000 | Total Loss: 0.009020 | Recon Loss: 0.008188 | Commit Loss: 0.001664 | Perplexity: 1529.941752
2025-09-26 15:40:37,547 Saving model at iteration 20000
2025-09-26 15:40:38,036 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000
2025-09-26 15:40:38,333 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/model.safetensors
2025-09-26 15:40:38,733 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/optimizer.bin
2025-09-26 15:40:38,733 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/scheduler.bin
2025-09-26 15:40:38,733 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/sampler.bin
2025-09-26 15:40:38,734 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/random_states_0.pkl
2025-09-26 15:41:22,894 Stage: Train 0.5 | Epoch: 13 | Iter: 20200 | Total Loss: 0.008783 | Recon Loss: 0.007966 | Commit Loss: 0.001633 | Perplexity: 1526.749802
2025-09-26 15:42:06,857 Stage: Train 0.5 | Epoch: 13 | Iter: 20400 | Total Loss: 0.008852 | Recon Loss: 0.008037 | Commit Loss: 0.001630 | Perplexity: 1521.659427
2025-09-26 15:42:50,367 Stage: Train 0.5 | Epoch: 13 | Iter: 20600 | Total Loss: 0.008877 | Recon Loss: 0.008068 | Commit Loss: 0.001618 | Perplexity: 1525.499396
2025-09-26 15:43:34,408 Stage: Train 0.5 | Epoch: 13 | Iter: 20800 | Total Loss: 0.009027 | Recon Loss: 0.008230 | Commit Loss: 0.001594 | Perplexity: 1528.614092
2025-09-26 15:44:18,663 Stage: Train 0.5 | Epoch: 13 | Iter: 21000 | Total Loss: 0.008650 | Recon Loss: 0.007839 | Commit Loss: 0.001622 | Perplexity: 1532.866173
2025-09-26 15:45:02,393 Stage: Train 0.5 | Epoch: 13 | Iter: 21200 | Total Loss: 0.008822 | Recon Loss: 0.008018 | Commit Loss: 0.001607 | Perplexity: 1537.676271
Trainning Epoch:   4%|▍         | 14/330 [1:32:43<29:36:57, 337.40s/it]2025-09-26 15:45:46,572 Stage: Train 0.5 | Epoch: 14 | Iter: 21400 | Total Loss: 0.008668 | Recon Loss: 0.007864 | Commit Loss: 0.001607 | Perplexity: 1531.792506
2025-09-26 15:46:29,960 Stage: Train 0.5 | Epoch: 14 | Iter: 21600 | Total Loss: 0.008887 | Recon Loss: 0.008098 | Commit Loss: 0.001580 | Perplexity: 1531.990594
2025-09-26 15:47:14,206 Stage: Train 0.5 | Epoch: 14 | Iter: 21800 | Total Loss: 0.008736 | Recon Loss: 0.007936 | Commit Loss: 0.001599 | Perplexity: 1534.642870
2025-09-26 15:47:58,412 Stage: Train 0.5 | Epoch: 14 | Iter: 22000 | Total Loss: 0.008726 | Recon Loss: 0.007917 | Commit Loss: 0.001616 | Perplexity: 1530.038322
2025-09-26 15:48:42,097 Stage: Train 0.5 | Epoch: 14 | Iter: 22200 | Total Loss: 0.009141 | Recon Loss: 0.008350 | Commit Loss: 0.001582 | Perplexity: 1532.949998
2025-09-26 15:49:25,645 Stage: Train 0.5 | Epoch: 14 | Iter: 22400 | Total Loss: 0.008397 | Recon Loss: 0.007598 | Commit Loss: 0.001598 | Perplexity: 1537.579000
2025-09-26 15:50:09,729 Stage: Train 0.5 | Epoch: 14 | Iter: 22600 | Total Loss: 0.008731 | Recon Loss: 0.007931 | Commit Loss: 0.001600 | Perplexity: 1535.278218
Trainning Epoch:   5%|▍         | 15/330 [1:38:16<29:25:04, 336.21s/it]2025-09-26 15:50:53,947 Stage: Train 0.5 | Epoch: 15 | Iter: 22800 | Total Loss: 0.008559 | Recon Loss: 0.007762 | Commit Loss: 0.001594 | Perplexity: 1533.576047
2025-09-26 15:51:38,351 Stage: Train 0.5 | Epoch: 15 | Iter: 23000 | Total Loss: 0.008254 | Recon Loss: 0.007453 | Commit Loss: 0.001601 | Perplexity: 1538.938279
2025-09-26 15:52:22,309 Stage: Train 0.5 | Epoch: 15 | Iter: 23200 | Total Loss: 0.008565 | Recon Loss: 0.007761 | Commit Loss: 0.001607 | Perplexity: 1530.322451
2025-09-26 15:53:05,995 Stage: Train 0.5 | Epoch: 15 | Iter: 23400 | Total Loss: 0.008642 | Recon Loss: 0.007858 | Commit Loss: 0.001568 | Perplexity: 1531.833133
2025-09-26 15:53:50,014 Stage: Train 0.5 | Epoch: 15 | Iter: 23600 | Total Loss: 0.008351 | Recon Loss: 0.007560 | Commit Loss: 0.001582 | Perplexity: 1538.276890
2025-09-26 15:54:34,031 Stage: Train 0.5 | Epoch: 15 | Iter: 23800 | Total Loss: 0.008727 | Recon Loss: 0.007952 | Commit Loss: 0.001549 | Perplexity: 1528.274424
2025-09-26 15:55:18,120 Stage: Train 0.5 | Epoch: 15 | Iter: 24000 | Total Loss: 0.008557 | Recon Loss: 0.007769 | Commit Loss: 0.001575 | Perplexity: 1534.162517
2025-09-26 15:56:01,898 Stage: Train 0.5 | Epoch: 15 | Iter: 24200 | Total Loss: 0.008473 | Recon Loss: 0.007693 | Commit Loss: 0.001560 | Perplexity: 1530.103824
Trainning Epoch:   5%|▍         | 16/330 [1:43:50<29:16:13, 335.58s/it]2025-09-26 15:56:46,032 Stage: Train 0.5 | Epoch: 16 | Iter: 24400 | Total Loss: 0.008198 | Recon Loss: 0.007401 | Commit Loss: 0.001594 | Perplexity: 1537.100363
2025-09-26 15:57:29,915 Stage: Train 0.5 | Epoch: 16 | Iter: 24600 | Total Loss: 0.008392 | Recon Loss: 0.007613 | Commit Loss: 0.001559 | Perplexity: 1535.518487
2025-09-26 15:58:13,884 Stage: Train 0.5 | Epoch: 16 | Iter: 24800 | Total Loss: 0.008402 | Recon Loss: 0.007637 | Commit Loss: 0.001531 | Perplexity: 1533.128301
2025-09-26 15:58:57,921 Stage: Train 0.5 | Epoch: 16 | Iter: 25000 | Total Loss: 0.008455 | Recon Loss: 0.007683 | Commit Loss: 0.001544 | Perplexity: 1536.822919
2025-09-26 15:59:41,741 Stage: Train 0.5 | Epoch: 16 | Iter: 25200 | Total Loss: 0.008056 | Recon Loss: 0.007286 | Commit Loss: 0.001540 | Perplexity: 1536.527702
2025-09-26 16:00:25,826 Stage: Train 0.5 | Epoch: 16 | Iter: 25400 | Total Loss: 0.008634 | Recon Loss: 0.007874 | Commit Loss: 0.001519 | Perplexity: 1534.741791
2025-09-26 16:01:09,981 Stage: Train 0.5 | Epoch: 16 | Iter: 25600 | Total Loss: 0.008082 | Recon Loss: 0.007313 | Commit Loss: 0.001538 | Perplexity: 1535.219066
2025-09-26 16:01:53,932 Stage: Train 0.5 | Epoch: 16 | Iter: 25800 | Total Loss: 0.008382 | Recon Loss: 0.007627 | Commit Loss: 0.001511 | Perplexity: 1533.992276
Trainning Epoch:   5%|▌         | 17/330 [1:49:25<29:08:47, 335.23s/it]2025-09-26 16:02:38,026 Stage: Train 0.5 | Epoch: 17 | Iter: 26000 | Total Loss: 0.007904 | Recon Loss: 0.007146 | Commit Loss: 0.001516 | Perplexity: 1540.708660
2025-09-26 16:03:21,835 Stage: Train 0.5 | Epoch: 17 | Iter: 26200 | Total Loss: 0.007921 | Recon Loss: 0.007161 | Commit Loss: 0.001519 | Perplexity: 1539.475479
2025-09-26 16:04:05,806 Stage: Train 0.5 | Epoch: 17 | Iter: 26400 | Total Loss: 0.008053 | Recon Loss: 0.007301 | Commit Loss: 0.001505 | Perplexity: 1537.703146
2025-09-26 16:04:49,854 Stage: Train 0.5 | Epoch: 17 | Iter: 26600 | Total Loss: 0.008268 | Recon Loss: 0.007517 | Commit Loss: 0.001503 | Perplexity: 1542.666863
2025-09-26 16:05:33,808 Stage: Train 0.5 | Epoch: 17 | Iter: 26800 | Total Loss: 0.007734 | Recon Loss: 0.006982 | Commit Loss: 0.001504 | Perplexity: 1545.517510
2025-09-26 16:06:17,901 Stage: Train 0.5 | Epoch: 17 | Iter: 27000 | Total Loss: 0.008091 | Recon Loss: 0.007341 | Commit Loss: 0.001500 | Perplexity: 1540.157924
2025-09-26 16:07:01,700 Stage: Train 0.5 | Epoch: 17 | Iter: 27200 | Total Loss: 0.007811 | Recon Loss: 0.007062 | Commit Loss: 0.001498 | Perplexity: 1541.502270
Trainning Epoch:   5%|▌         | 18/330 [1:54:59<29:01:11, 334.85s/it]2025-09-26 16:07:45,960 Stage: Train 0.5 | Epoch: 18 | Iter: 27400 | Total Loss: 0.008351 | Recon Loss: 0.007609 | Commit Loss: 0.001485 | Perplexity: 1540.937337
2025-09-26 16:08:29,959 Stage: Train 0.5 | Epoch: 18 | Iter: 27600 | Total Loss: 0.007756 | Recon Loss: 0.007016 | Commit Loss: 0.001479 | Perplexity: 1546.693287
2025-09-26 16:09:14,041 Stage: Train 0.5 | Epoch: 18 | Iter: 27800 | Total Loss: 0.007977 | Recon Loss: 0.007244 | Commit Loss: 0.001465 | Perplexity: 1549.370778
2025-09-26 16:09:57,643 Stage: Train 0.5 | Epoch: 18 | Iter: 28000 | Total Loss: 0.007668 | Recon Loss: 0.006936 | Commit Loss: 0.001464 | Perplexity: 1548.797382
2025-09-26 16:10:41,550 Stage: Train 0.5 | Epoch: 18 | Iter: 28200 | Total Loss: 0.008298 | Recon Loss: 0.007580 | Commit Loss: 0.001436 | Perplexity: 1551.024977
2025-09-26 16:11:25,704 Stage: Train 0.5 | Epoch: 18 | Iter: 28400 | Total Loss: 0.007762 | Recon Loss: 0.007027 | Commit Loss: 0.001469 | Perplexity: 1556.263812
2025-09-26 16:12:09,815 Stage: Train 0.5 | Epoch: 18 | Iter: 28600 | Total Loss: 0.007871 | Recon Loss: 0.007140 | Commit Loss: 0.001461 | Perplexity: 1554.767166
2025-09-26 16:12:53,561 Stage: Train 0.5 | Epoch: 18 | Iter: 28800 | Total Loss: 0.007538 | Recon Loss: 0.006788 | Commit Loss: 0.001500 | Perplexity: 1559.154661
Trainning Epoch:   6%|▌         | 19/330 [2:00:33<28:54:17, 334.59s/it]2025-09-26 16:13:37,362 Stage: Train 0.5 | Epoch: 19 | Iter: 29000 | Total Loss: 0.007762 | Recon Loss: 0.007031 | Commit Loss: 0.001462 | Perplexity: 1556.081094
2025-09-26 16:14:21,344 Stage: Train 0.5 | Epoch: 19 | Iter: 29200 | Total Loss: 0.008005 | Recon Loss: 0.007279 | Commit Loss: 0.001453 | Perplexity: 1559.780604
2025-09-26 16:15:05,319 Stage: Train 0.5 | Epoch: 19 | Iter: 29400 | Total Loss: 0.007646 | Recon Loss: 0.006910 | Commit Loss: 0.001470 | Perplexity: 1569.309975
2025-09-26 16:15:49,272 Stage: Train 0.5 | Epoch: 19 | Iter: 29600 | Total Loss: 0.007731 | Recon Loss: 0.007003 | Commit Loss: 0.001458 | Perplexity: 1565.889526
2025-09-26 16:16:32,940 Stage: Train 0.5 | Epoch: 19 | Iter: 29800 | Total Loss: 0.007510 | Recon Loss: 0.006787 | Commit Loss: 0.001445 | Perplexity: 1563.013973
2025-09-26 16:17:17,005 Stage: Train 0.5 | Epoch: 19 | Iter: 30000 | Total Loss: 0.007512 | Recon Loss: 0.006780 | Commit Loss: 0.001465 | Perplexity: 1568.200125
2025-09-26 16:18:00,779 Stage: Train 0.5 | Epoch: 19 | Iter: 30200 | Total Loss: 0.007827 | Recon Loss: 0.007112 | Commit Loss: 0.001431 | Perplexity: 1564.885846
Trainning Epoch:   6%|▌         | 20/330 [2:06:06<28:46:58, 334.25s/it]2025-09-26 16:18:45,002 Stage: Train 0.5 | Epoch: 20 | Iter: 30400 | Total Loss: 0.007633 | Recon Loss: 0.006904 | Commit Loss: 0.001459 | Perplexity: 1565.174812
2025-09-26 16:19:29,092 Stage: Train 0.5 | Epoch: 20 | Iter: 30600 | Total Loss: 0.007615 | Recon Loss: 0.006901 | Commit Loss: 0.001427 | Perplexity: 1569.281909
2025-09-26 16:20:12,582 Stage: Train 0.5 | Epoch: 20 | Iter: 30800 | Total Loss: 0.007909 | Recon Loss: 0.007194 | Commit Loss: 0.001430 | Perplexity: 1567.877339
2025-09-26 16:20:56,487 Stage: Train 0.5 | Epoch: 20 | Iter: 31000 | Total Loss: 0.007244 | Recon Loss: 0.006515 | Commit Loss: 0.001459 | Perplexity: 1574.031055
2025-09-26 16:21:40,506 Stage: Train 0.5 | Epoch: 20 | Iter: 31200 | Total Loss: 0.007602 | Recon Loss: 0.006884 | Commit Loss: 0.001436 | Perplexity: 1572.664484
2025-09-26 16:22:23,364 Stage: Train 0.5 | Epoch: 20 | Iter: 31400 | Total Loss: 0.007509 | Recon Loss: 0.006788 | Commit Loss: 0.001442 | Perplexity: 1574.605784
2025-09-26 16:23:07,470 Stage: Train 0.5 | Epoch: 20 | Iter: 31600 | Total Loss: 0.007602 | Recon Loss: 0.006890 | Commit Loss: 0.001424 | Perplexity: 1566.821143
2025-09-26 16:23:51,230 Stage: Train 0.5 | Epoch: 20 | Iter: 31800 | Total Loss: 0.007419 | Recon Loss: 0.006713 | Commit Loss: 0.001412 | Perplexity: 1566.187983
Trainning Epoch:   6%|▋         | 21/330 [2:11:39<28:39:07, 333.81s/it]2025-09-26 16:24:35,491 Stage: Train 0.5 | Epoch: 21 | Iter: 32000 | Total Loss: 0.007629 | Recon Loss: 0.006912 | Commit Loss: 0.001434 | Perplexity: 1572.896024
2025-09-26 16:25:19,301 Stage: Train 0.5 | Epoch: 21 | Iter: 32200 | Total Loss: 0.007423 | Recon Loss: 0.006712 | Commit Loss: 0.001424 | Perplexity: 1572.148702
2025-09-26 16:26:03,241 Stage: Train 0.5 | Epoch: 21 | Iter: 32400 | Total Loss: 0.007540 | Recon Loss: 0.006833 | Commit Loss: 0.001415 | Perplexity: 1571.497754
2025-09-26 16:26:46,892 Stage: Train 0.5 | Epoch: 21 | Iter: 32600 | Total Loss: 0.007183 | Recon Loss: 0.006471 | Commit Loss: 0.001423 | Perplexity: 1574.569712
2025-09-26 16:27:30,828 Stage: Train 0.5 | Epoch: 21 | Iter: 32800 | Total Loss: 0.007482 | Recon Loss: 0.006769 | Commit Loss: 0.001426 | Perplexity: 1574.259431
2025-09-26 16:28:14,582 Stage: Train 0.5 | Epoch: 21 | Iter: 33000 | Total Loss: 0.007155 | Recon Loss: 0.006433 | Commit Loss: 0.001444 | Perplexity: 1574.782284
2025-09-26 16:28:58,769 Stage: Train 0.5 | Epoch: 21 | Iter: 33200 | Total Loss: 0.007230 | Recon Loss: 0.006511 | Commit Loss: 0.001438 | Perplexity: 1578.276055
2025-09-26 16:29:42,742 Stage: Train 0.5 | Epoch: 21 | Iter: 33400 | Total Loss: 0.007271 | Recon Loss: 0.006545 | Commit Loss: 0.001453 | Perplexity: 1570.985095
Trainning Epoch:   7%|▋         | 22/330 [2:17:12<28:33:07, 333.73s/it]2025-09-26 16:30:26,414 Stage: Train 0.5 | Epoch: 22 | Iter: 33600 | Total Loss: 0.007475 | Recon Loss: 0.006764 | Commit Loss: 0.001421 | Perplexity: 1568.923343
2025-09-26 16:31:10,354 Stage: Train 0.5 | Epoch: 22 | Iter: 33800 | Total Loss: 0.007223 | Recon Loss: 0.006511 | Commit Loss: 0.001422 | Perplexity: 1575.888768
2025-09-26 16:31:54,358 Stage: Train 0.5 | Epoch: 22 | Iter: 34000 | Total Loss: 0.007158 | Recon Loss: 0.006432 | Commit Loss: 0.001453 | Perplexity: 1575.216068
2025-09-26 16:32:38,505 Stage: Train 0.5 | Epoch: 22 | Iter: 34200 | Total Loss: 0.007201 | Recon Loss: 0.006477 | Commit Loss: 0.001449 | Perplexity: 1576.242460
2025-09-26 16:33:22,457 Stage: Train 0.5 | Epoch: 22 | Iter: 34400 | Total Loss: 0.007231 | Recon Loss: 0.006515 | Commit Loss: 0.001433 | Perplexity: 1575.791178
2025-09-26 16:34:06,039 Stage: Train 0.5 | Epoch: 22 | Iter: 34600 | Total Loss: 0.007320 | Recon Loss: 0.006609 | Commit Loss: 0.001421 | Perplexity: 1574.001655
2025-09-26 16:34:49,989 Stage: Train 0.5 | Epoch: 22 | Iter: 34800 | Total Loss: 0.007097 | Recon Loss: 0.006372 | Commit Loss: 0.001449 | Perplexity: 1581.721183
Trainning Epoch:   7%|▋         | 23/330 [2:22:46<28:26:50, 333.59s/it]2025-09-26 16:35:33,967 Stage: Train 0.5 | Epoch: 23 | Iter: 35000 | Total Loss: 0.007255 | Recon Loss: 0.006538 | Commit Loss: 0.001434 | Perplexity: 1572.208580
2025-09-26 16:36:17,876 Stage: Train 0.5 | Epoch: 23 | Iter: 35200 | Total Loss: 0.007183 | Recon Loss: 0.006463 | Commit Loss: 0.001440 | Perplexity: 1579.711049
2025-09-26 16:37:01,474 Stage: Train 0.5 | Epoch: 23 | Iter: 35400 | Total Loss: 0.006927 | Recon Loss: 0.006208 | Commit Loss: 0.001439 | Perplexity: 1577.228141
2025-09-26 16:37:45,310 Stage: Train 0.5 | Epoch: 23 | Iter: 35600 | Total Loss: 0.007136 | Recon Loss: 0.006423 | Commit Loss: 0.001427 | Perplexity: 1572.527756
2025-09-26 16:38:29,225 Stage: Train 0.5 | Epoch: 23 | Iter: 35800 | Total Loss: 0.007563 | Recon Loss: 0.006855 | Commit Loss: 0.001416 | Perplexity: 1574.748367
2025-09-26 16:39:13,203 Stage: Train 0.5 | Epoch: 23 | Iter: 36000 | Total Loss: 0.007123 | Recon Loss: 0.006403 | Commit Loss: 0.001440 | Perplexity: 1572.625642
2025-09-26 16:39:57,146 Stage: Train 0.5 | Epoch: 23 | Iter: 36200 | Total Loss: 0.007156 | Recon Loss: 0.006443 | Commit Loss: 0.001426 | Perplexity: 1572.416161
2025-09-26 16:40:40,673 Stage: Train 0.5 | Epoch: 23 | Iter: 36400 | Total Loss: 0.007229 | Recon Loss: 0.006511 | Commit Loss: 0.001437 | Perplexity: 1575.401121
Trainning Epoch:   7%|▋         | 24/330 [2:28:19<28:20:21, 333.40s/it]2025-09-26 16:41:24,737 Stage: Train 0.5 | Epoch: 24 | Iter: 36600 | Total Loss: 0.006798 | Recon Loss: 0.006078 | Commit Loss: 0.001441 | Perplexity: 1578.212625
2025-09-26 16:42:08,471 Stage: Train 0.5 | Epoch: 24 | Iter: 36800 | Total Loss: 0.007088 | Recon Loss: 0.006371 | Commit Loss: 0.001433 | Perplexity: 1574.115450
2025-09-26 16:42:52,634 Stage: Train 0.5 | Epoch: 24 | Iter: 37000 | Total Loss: 0.007089 | Recon Loss: 0.006370 | Commit Loss: 0.001438 | Perplexity: 1573.902642
2025-09-26 16:43:36,402 Stage: Train 0.5 | Epoch: 24 | Iter: 37200 | Total Loss: 0.006967 | Recon Loss: 0.006245 | Commit Loss: 0.001444 | Perplexity: 1574.008476
2025-09-26 16:44:19,896 Stage: Train 0.5 | Epoch: 24 | Iter: 37400 | Total Loss: 0.006952 | Recon Loss: 0.006230 | Commit Loss: 0.001445 | Perplexity: 1572.809229
2025-09-26 16:45:03,795 Stage: Train 0.5 | Epoch: 24 | Iter: 37600 | Total Loss: 0.007105 | Recon Loss: 0.006396 | Commit Loss: 0.001419 | Perplexity: 1577.004232
2025-09-26 16:45:47,682 Stage: Train 0.5 | Epoch: 24 | Iter: 37800 | Total Loss: 0.006912 | Recon Loss: 0.006192 | Commit Loss: 0.001441 | Perplexity: 1578.559692
Trainning Epoch:   8%|▊         | 25/330 [2:33:52<28:14:10, 333.28s/it]2025-09-26 16:46:31,645 Stage: Train 0.5 | Epoch: 25 | Iter: 38000 | Total Loss: 0.007195 | Recon Loss: 0.006492 | Commit Loss: 0.001405 | Perplexity: 1570.990117
2025-09-26 16:47:15,108 Stage: Train 0.5 | Epoch: 25 | Iter: 38200 | Total Loss: 0.006888 | Recon Loss: 0.006171 | Commit Loss: 0.001434 | Perplexity: 1579.635126
2025-09-26 16:47:58,785 Stage: Train 0.5 | Epoch: 25 | Iter: 38400 | Total Loss: 0.006958 | Recon Loss: 0.006237 | Commit Loss: 0.001442 | Perplexity: 1576.725725
2025-09-26 16:48:42,471 Stage: Train 0.5 | Epoch: 25 | Iter: 38600 | Total Loss: 0.007160 | Recon Loss: 0.006451 | Commit Loss: 0.001418 | Perplexity: 1571.943868
2025-09-26 16:49:26,360 Stage: Train 0.5 | Epoch: 25 | Iter: 38800 | Total Loss: 0.006795 | Recon Loss: 0.006085 | Commit Loss: 0.001420 | Perplexity: 1570.244423
2025-09-26 16:50:10,119 Stage: Train 0.5 | Epoch: 25 | Iter: 39000 | Total Loss: 0.007346 | Recon Loss: 0.006640 | Commit Loss: 0.001411 | Perplexity: 1572.836802
2025-09-26 16:50:53,812 Stage: Train 0.5 | Epoch: 25 | Iter: 39200 | Total Loss: 0.006775 | Recon Loss: 0.006060 | Commit Loss: 0.001430 | Perplexity: 1575.744792
2025-09-26 16:51:37,931 Stage: Train 0.5 | Epoch: 25 | Iter: 39400 | Total Loss: 0.006954 | Recon Loss: 0.006244 | Commit Loss: 0.001420 | Perplexity: 1577.178213
Trainning Epoch:   8%|▊         | 26/330 [2:39:24<28:07:50, 333.13s/it]2025-09-26 16:52:22,178 Stage: Train 0.5 | Epoch: 26 | Iter: 39600 | Total Loss: 0.006851 | Recon Loss: 0.006141 | Commit Loss: 0.001420 | Perplexity: 1571.321621
2025-09-26 16:53:05,997 Stage: Train 0.5 | Epoch: 26 | Iter: 39800 | Total Loss: 0.006992 | Recon Loss: 0.006270 | Commit Loss: 0.001443 | Perplexity: 1572.984526
2025-09-26 16:53:49,883 Stage: Train 0.5 | Epoch: 26 | Iter: 40000 | Total Loss: 0.006825 | Recon Loss: 0.006116 | Commit Loss: 0.001418 | Perplexity: 1574.048488
2025-09-26 16:53:49,883 Saving model at iteration 40000
2025-09-26 16:53:50,373 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000
2025-09-26 16:53:50,642 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/model.safetensors
2025-09-26 16:53:50,985 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/optimizer.bin
2025-09-26 16:53:50,986 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/scheduler.bin
2025-09-26 16:53:50,986 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/sampler.bin
2025-09-26 16:53:50,987 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/random_states_0.pkl
2025-09-26 16:54:35,036 Stage: Train 0.5 | Epoch: 26 | Iter: 40200 | Total Loss: 0.007184 | Recon Loss: 0.006477 | Commit Loss: 0.001415 | Perplexity: 1574.933656
2025-09-26 16:55:19,086 Stage: Train 0.5 | Epoch: 26 | Iter: 40400 | Total Loss: 0.007007 | Recon Loss: 0.006300 | Commit Loss: 0.001413 | Perplexity: 1566.877714
2025-09-26 16:56:03,013 Stage: Train 0.5 | Epoch: 26 | Iter: 40600 | Total Loss: 0.006734 | Recon Loss: 0.006024 | Commit Loss: 0.001419 | Perplexity: 1570.304471
2025-09-26 16:56:46,885 Stage: Train 0.5 | Epoch: 26 | Iter: 40800 | Total Loss: 0.006892 | Recon Loss: 0.006182 | Commit Loss: 0.001420 | Perplexity: 1573.205858
2025-09-26 16:57:30,499 Stage: Train 0.5 | Epoch: 26 | Iter: 41000 | Total Loss: 0.006629 | Recon Loss: 0.005917 | Commit Loss: 0.001426 | Perplexity: 1571.217943
Trainning Epoch:   8%|▊         | 27/330 [2:44:59<28:04:35, 333.58s/it]2025-09-26 16:58:14,201 Stage: Train 0.5 | Epoch: 27 | Iter: 41200 | Total Loss: 0.007102 | Recon Loss: 0.006384 | Commit Loss: 0.001436 | Perplexity: 1575.692629
2025-09-26 16:58:58,061 Stage: Train 0.5 | Epoch: 27 | Iter: 41400 | Total Loss: 0.006604 | Recon Loss: 0.005898 | Commit Loss: 0.001413 | Perplexity: 1574.628107
2025-09-26 16:59:41,840 Stage: Train 0.5 | Epoch: 27 | Iter: 41600 | Total Loss: 0.006878 | Recon Loss: 0.006158 | Commit Loss: 0.001439 | Perplexity: 1576.986607
2025-09-26 17:00:25,637 Stage: Train 0.5 | Epoch: 27 | Iter: 41800 | Total Loss: 0.006765 | Recon Loss: 0.006051 | Commit Loss: 0.001429 | Perplexity: 1569.175098
2025-09-26 17:01:08,978 Stage: Train 0.5 | Epoch: 27 | Iter: 42000 | Total Loss: 0.006710 | Recon Loss: 0.005989 | Commit Loss: 0.001442 | Perplexity: 1570.817747
2025-09-26 17:01:52,799 Stage: Train 0.5 | Epoch: 27 | Iter: 42200 | Total Loss: 0.006838 | Recon Loss: 0.006130 | Commit Loss: 0.001415 | Perplexity: 1571.742531
2025-09-26 17:02:36,519 Stage: Train 0.5 | Epoch: 27 | Iter: 42400 | Total Loss: 0.007160 | Recon Loss: 0.006453 | Commit Loss: 0.001414 | Perplexity: 1569.006333
Trainning Epoch:   8%|▊         | 28/330 [2:50:31<27:57:06, 333.20s/it]2025-09-26 17:03:20,888 Stage: Train 0.5 | Epoch: 28 | Iter: 42600 | Total Loss: 0.006686 | Recon Loss: 0.005981 | Commit Loss: 0.001411 | Perplexity: 1572.783395
2025-09-26 17:04:04,533 Stage: Train 0.5 | Epoch: 28 | Iter: 42800 | Total Loss: 0.006643 | Recon Loss: 0.005936 | Commit Loss: 0.001414 | Perplexity: 1574.208840
2025-09-26 17:04:48,450 Stage: Train 0.5 | Epoch: 28 | Iter: 43000 | Total Loss: 0.006730 | Recon Loss: 0.006018 | Commit Loss: 0.001423 | Perplexity: 1575.424945
2025-09-26 17:05:32,285 Stage: Train 0.5 | Epoch: 28 | Iter: 43200 | Total Loss: 0.006650 | Recon Loss: 0.005938 | Commit Loss: 0.001422 | Perplexity: 1572.504290
2025-09-26 17:06:15,755 Stage: Train 0.5 | Epoch: 28 | Iter: 43400 | Total Loss: 0.006908 | Recon Loss: 0.006200 | Commit Loss: 0.001415 | Perplexity: 1574.595504
2025-09-26 17:06:58,544 Stage: Train 0.5 | Epoch: 28 | Iter: 43600 | Total Loss: 0.006773 | Recon Loss: 0.006053 | Commit Loss: 0.001440 | Perplexity: 1571.194858
2025-09-26 17:07:41,835 Stage: Train 0.5 | Epoch: 28 | Iter: 43800 | Total Loss: 0.006588 | Recon Loss: 0.005876 | Commit Loss: 0.001424 | Perplexity: 1572.806626
2025-09-26 17:08:25,559 Stage: Train 0.5 | Epoch: 28 | Iter: 44000 | Total Loss: 0.006598 | Recon Loss: 0.005888 | Commit Loss: 0.001421 | Perplexity: 1569.444681
Trainning Epoch:   9%|▉         | 29/330 [2:56:02<27:48:20, 332.56s/it]2025-09-26 17:09:09,585 Stage: Train 0.5 | Epoch: 29 | Iter: 44200 | Total Loss: 0.006794 | Recon Loss: 0.006084 | Commit Loss: 0.001420 | Perplexity: 1569.261929
2025-09-26 17:09:53,240 Stage: Train 0.5 | Epoch: 29 | Iter: 44400 | Total Loss: 0.006750 | Recon Loss: 0.006044 | Commit Loss: 0.001411 | Perplexity: 1575.178126
2025-09-26 17:10:37,013 Stage: Train 0.5 | Epoch: 29 | Iter: 44600 | Total Loss: 0.006686 | Recon Loss: 0.005976 | Commit Loss: 0.001420 | Perplexity: 1566.353328
2025-09-26 17:11:20,362 Stage: Train 0.5 | Epoch: 29 | Iter: 44800 | Total Loss: 0.006663 | Recon Loss: 0.005951 | Commit Loss: 0.001424 | Perplexity: 1572.340822
2025-09-26 17:12:04,300 Stage: Train 0.5 | Epoch: 29 | Iter: 45000 | Total Loss: 0.006590 | Recon Loss: 0.005891 | Commit Loss: 0.001397 | Perplexity: 1569.876326
2025-09-26 17:12:48,218 Stage: Train 0.5 | Epoch: 29 | Iter: 45200 | Total Loss: 0.006727 | Recon Loss: 0.006009 | Commit Loss: 0.001437 | Perplexity: 1569.281323
2025-09-26 17:13:32,299 Stage: Train 0.5 | Epoch: 29 | Iter: 45400 | Total Loss: 0.006487 | Recon Loss: 0.005777 | Commit Loss: 0.001420 | Perplexity: 1570.013239
Trainning Epoch:   9%|▉         | 30/330 [3:01:35<27:43:23, 332.68s/it]2025-09-26 17:14:16,487 Stage: Train 0.5 | Epoch: 30 | Iter: 45600 | Total Loss: 0.006627 | Recon Loss: 0.005918 | Commit Loss: 0.001420 | Perplexity: 1561.518152
2025-09-26 17:15:00,095 Stage: Train 0.5 | Epoch: 30 | Iter: 45800 | Total Loss: 0.006491 | Recon Loss: 0.005777 | Commit Loss: 0.001430 | Perplexity: 1566.242398
2025-09-26 17:15:43,935 Stage: Train 0.5 | Epoch: 30 | Iter: 46000 | Total Loss: 0.006714 | Recon Loss: 0.006017 | Commit Loss: 0.001394 | Perplexity: 1566.844246
2025-09-26 17:16:27,920 Stage: Train 0.5 | Epoch: 30 | Iter: 46200 | Total Loss: 0.006406 | Recon Loss: 0.005690 | Commit Loss: 0.001432 | Perplexity: 1568.254042
2025-09-26 17:17:11,850 Stage: Train 0.5 | Epoch: 30 | Iter: 46400 | Total Loss: 0.006748 | Recon Loss: 0.006034 | Commit Loss: 0.001429 | Perplexity: 1567.989250
2025-09-26 17:17:55,241 Stage: Train 0.5 | Epoch: 30 | Iter: 46600 | Total Loss: 0.006413 | Recon Loss: 0.005691 | Commit Loss: 0.001445 | Perplexity: 1571.355832
2025-09-26 17:18:39,176 Stage: Train 0.5 | Epoch: 30 | Iter: 46800 | Total Loss: 0.006575 | Recon Loss: 0.005866 | Commit Loss: 0.001418 | Perplexity: 1570.493094
2025-09-26 17:19:23,153 Stage: Train 0.5 | Epoch: 30 | Iter: 47000 | Total Loss: 0.006774 | Recon Loss: 0.006058 | Commit Loss: 0.001432 | Perplexity: 1569.804712
Trainning Epoch:   9%|▉         | 31/330 [3:07:08<27:38:33, 332.82s/it]2025-09-26 17:20:07,237 Stage: Train 0.5 | Epoch: 31 | Iter: 47200 | Total Loss: 0.006473 | Recon Loss: 0.005760 | Commit Loss: 0.001425 | Perplexity: 1567.281257
2025-09-26 17:20:51,168 Stage: Train 0.5 | Epoch: 31 | Iter: 47400 | Total Loss: 0.006941 | Recon Loss: 0.006239 | Commit Loss: 0.001404 | Perplexity: 1563.694676
2025-09-26 17:21:34,778 Stage: Train 0.5 | Epoch: 31 | Iter: 47600 | Total Loss: 0.006307 | Recon Loss: 0.005589 | Commit Loss: 0.001436 | Perplexity: 1572.251563
2025-09-26 17:22:18,587 Stage: Train 0.5 | Epoch: 31 | Iter: 47800 | Total Loss: 0.006524 | Recon Loss: 0.005825 | Commit Loss: 0.001397 | Perplexity: 1565.526750
2025-09-26 17:23:02,223 Stage: Train 0.5 | Epoch: 31 | Iter: 48000 | Total Loss: 0.006367 | Recon Loss: 0.005649 | Commit Loss: 0.001437 | Perplexity: 1563.829164
2025-09-26 17:23:46,118 Stage: Train 0.5 | Epoch: 31 | Iter: 48200 | Total Loss: 0.006471 | Recon Loss: 0.005763 | Commit Loss: 0.001416 | Perplexity: 1565.729512
2025-09-26 17:24:29,745 Stage: Train 0.5 | Epoch: 31 | Iter: 48400 | Total Loss: 0.006429 | Recon Loss: 0.005720 | Commit Loss: 0.001419 | Perplexity: 1563.033701
2025-09-26 17:25:13,466 Stage: Train 0.5 | Epoch: 31 | Iter: 48600 | Total Loss: 0.006636 | Recon Loss: 0.005917 | Commit Loss: 0.001439 | Perplexity: 1569.269949
Trainning Epoch:  10%|▉         | 32/330 [3:12:41<27:32:26, 332.71s/it]2025-09-26 17:25:57,529 Stage: Train 0.5 | Epoch: 32 | Iter: 48800 | Total Loss: 0.006500 | Recon Loss: 0.005790 | Commit Loss: 0.001421 | Perplexity: 1563.516690
2025-09-26 17:26:41,580 Stage: Train 0.5 | Epoch: 32 | Iter: 49000 | Total Loss: 0.006445 | Recon Loss: 0.005738 | Commit Loss: 0.001415 | Perplexity: 1562.466218
2025-09-26 17:27:25,597 Stage: Train 0.5 | Epoch: 32 | Iter: 49200 | Total Loss: 0.006570 | Recon Loss: 0.005868 | Commit Loss: 0.001403 | Perplexity: 1559.085903
2025-09-26 17:28:09,403 Stage: Train 0.5 | Epoch: 32 | Iter: 49400 | Total Loss: 0.006547 | Recon Loss: 0.005840 | Commit Loss: 0.001413 | Perplexity: 1563.521752
2025-09-26 17:28:53,589 Stage: Train 0.5 | Epoch: 32 | Iter: 49600 | Total Loss: 0.006338 | Recon Loss: 0.005630 | Commit Loss: 0.001415 | Perplexity: 1568.216838
2025-09-26 17:29:37,515 Stage: Train 0.5 | Epoch: 32 | Iter: 49800 | Total Loss: 0.006856 | Recon Loss: 0.006147 | Commit Loss: 0.001417 | Perplexity: 1566.254021
2025-09-26 17:30:21,537 Stage: Train 0.5 | Epoch: 32 | Iter: 50000 | Total Loss: 0.006045 | Recon Loss: 0.005338 | Commit Loss: 0.001414 | Perplexity: 1564.096477
Trainning Epoch:  10%|█         | 33/330 [3:18:15<27:28:56, 333.12s/it]2025-09-26 17:31:05,521 Stage: Train 0.5 | Epoch: 33 | Iter: 50200 | Total Loss: 0.006564 | Recon Loss: 0.005854 | Commit Loss: 0.001419 | Perplexity: 1569.549524
2025-09-26 17:31:49,133 Stage: Train 0.5 | Epoch: 33 | Iter: 50400 | Total Loss: 0.006383 | Recon Loss: 0.005675 | Commit Loss: 0.001418 | Perplexity: 1562.614384
2025-09-26 17:32:33,074 Stage: Train 0.5 | Epoch: 33 | Iter: 50600 | Total Loss: 0.006574 | Recon Loss: 0.005868 | Commit Loss: 0.001411 | Perplexity: 1565.316045
2025-09-26 17:33:16,843 Stage: Train 0.5 | Epoch: 33 | Iter: 50800 | Total Loss: 0.006596 | Recon Loss: 0.005889 | Commit Loss: 0.001414 | Perplexity: 1563.742541
2025-09-26 17:34:00,990 Stage: Train 0.5 | Epoch: 33 | Iter: 51000 | Total Loss: 0.006279 | Recon Loss: 0.005575 | Commit Loss: 0.001409 | Perplexity: 1563.766038
2025-09-26 17:34:44,536 Stage: Train 0.5 | Epoch: 33 | Iter: 51200 | Total Loss: 0.006574 | Recon Loss: 0.005865 | Commit Loss: 0.001419 | Perplexity: 1566.845623
2025-09-26 17:35:28,497 Stage: Train 0.5 | Epoch: 33 | Iter: 51400 | Total Loss: 0.006328 | Recon Loss: 0.005623 | Commit Loss: 0.001411 | Perplexity: 1561.732467
2025-09-26 17:36:12,251 Stage: Train 0.5 | Epoch: 33 | Iter: 51600 | Total Loss: 0.006190 | Recon Loss: 0.005488 | Commit Loss: 0.001404 | Perplexity: 1556.053614
Trainning Epoch:  10%|█         | 34/330 [3:23:48<27:23:15, 333.09s/it]2025-09-26 17:36:56,863 Stage: Train 0.5 | Epoch: 34 | Iter: 51800 | Total Loss: 0.006391 | Recon Loss: 0.005685 | Commit Loss: 0.001413 | Perplexity: 1556.190265
2025-09-26 17:37:40,521 Stage: Train 0.5 | Epoch: 34 | Iter: 52000 | Total Loss: 0.006386 | Recon Loss: 0.005685 | Commit Loss: 0.001402 | Perplexity: 1557.455682
2025-09-26 17:38:24,040 Stage: Train 0.5 | Epoch: 34 | Iter: 52200 | Total Loss: 0.006274 | Recon Loss: 0.005568 | Commit Loss: 0.001413 | Perplexity: 1560.176916
2025-09-26 17:39:08,015 Stage: Train 0.5 | Epoch: 34 | Iter: 52400 | Total Loss: 0.006535 | Recon Loss: 0.005835 | Commit Loss: 0.001400 | Perplexity: 1555.097007
2025-09-26 17:39:52,058 Stage: Train 0.5 | Epoch: 34 | Iter: 52600 | Total Loss: 0.006307 | Recon Loss: 0.005598 | Commit Loss: 0.001418 | Perplexity: 1562.160410
2025-09-26 17:40:36,050 Stage: Train 0.5 | Epoch: 34 | Iter: 52800 | Total Loss: 0.006389 | Recon Loss: 0.005678 | Commit Loss: 0.001422 | Perplexity: 1557.634052
2025-09-26 17:41:19,943 Stage: Train 0.5 | Epoch: 34 | Iter: 53000 | Total Loss: 0.006232 | Recon Loss: 0.005519 | Commit Loss: 0.001425 | Perplexity: 1562.327974
Trainning Epoch:  11%|█         | 35/330 [3:29:21<27:18:07, 333.18s/it]2025-09-26 17:42:03,627 Stage: Train 0.5 | Epoch: 35 | Iter: 53200 | Total Loss: 0.006337 | Recon Loss: 0.005631 | Commit Loss: 0.001413 | Perplexity: 1556.825123
2025-09-26 17:42:47,410 Stage: Train 0.5 | Epoch: 35 | Iter: 53400 | Total Loss: 0.006273 | Recon Loss: 0.005567 | Commit Loss: 0.001411 | Perplexity: 1562.358223
2025-09-26 17:43:31,343 Stage: Train 0.5 | Epoch: 35 | Iter: 53600 | Total Loss: 0.006371 | Recon Loss: 0.005665 | Commit Loss: 0.001412 | Perplexity: 1556.854910
2025-09-26 17:44:15,065 Stage: Train 0.5 | Epoch: 35 | Iter: 53800 | Total Loss: 0.006174 | Recon Loss: 0.005462 | Commit Loss: 0.001423 | Perplexity: 1559.170316
2025-09-26 17:44:58,769 Stage: Train 0.5 | Epoch: 35 | Iter: 54000 | Total Loss: 0.006158 | Recon Loss: 0.005444 | Commit Loss: 0.001429 | Perplexity: 1563.981249
2025-09-26 17:45:42,626 Stage: Train 0.5 | Epoch: 35 | Iter: 54200 | Total Loss: 0.006320 | Recon Loss: 0.005615 | Commit Loss: 0.001410 | Perplexity: 1561.797050
2025-09-26 17:46:26,272 Stage: Train 0.5 | Epoch: 35 | Iter: 54400 | Total Loss: 0.006307 | Recon Loss: 0.005595 | Commit Loss: 0.001423 | Perplexity: 1555.321738
2025-09-26 17:47:10,200 Stage: Train 0.5 | Epoch: 35 | Iter: 54600 | Total Loss: 0.006666 | Recon Loss: 0.005948 | Commit Loss: 0.001436 | Perplexity: 1559.437760
Trainning Epoch:  11%|█         | 36/330 [3:34:54<27:12:18, 333.12s/it]2025-09-26 17:47:54,275 Stage: Train 0.5 | Epoch: 36 | Iter: 54800 | Total Loss: 0.006250 | Recon Loss: 0.005538 | Commit Loss: 0.001424 | Perplexity: 1561.591193
2025-09-26 17:48:37,905 Stage: Train 0.5 | Epoch: 36 | Iter: 55000 | Total Loss: 0.006360 | Recon Loss: 0.005652 | Commit Loss: 0.001417 | Perplexity: 1560.079902
2025-09-26 17:49:21,804 Stage: Train 0.5 | Epoch: 36 | Iter: 55200 | Total Loss: 0.006146 | Recon Loss: 0.005443 | Commit Loss: 0.001405 | Perplexity: 1557.956132
2025-09-26 17:50:05,663 Stage: Train 0.5 | Epoch: 36 | Iter: 55400 | Total Loss: 0.006398 | Recon Loss: 0.005691 | Commit Loss: 0.001414 | Perplexity: 1559.332990
2025-09-26 17:50:49,740 Stage: Train 0.5 | Epoch: 36 | Iter: 55600 | Total Loss: 0.006158 | Recon Loss: 0.005453 | Commit Loss: 0.001411 | Perplexity: 1559.115598
2025-09-26 17:51:33,640 Stage: Train 0.5 | Epoch: 36 | Iter: 55800 | Total Loss: 0.006141 | Recon Loss: 0.005435 | Commit Loss: 0.001411 | Perplexity: 1560.101531
2025-09-26 17:52:16,341 Stage: Train 0.5 | Epoch: 36 | Iter: 56000 | Total Loss: 0.006390 | Recon Loss: 0.005690 | Commit Loss: 0.001402 | Perplexity: 1556.778810
2025-09-26 17:53:00,317 Stage: Train 0.5 | Epoch: 36 | Iter: 56200 | Total Loss: 0.006462 | Recon Loss: 0.005763 | Commit Loss: 0.001399 | Perplexity: 1557.013007
Trainning Epoch:  11%|█         | 37/330 [3:40:27<27:05:29, 332.87s/it]2025-09-26 17:53:44,493 Stage: Train 0.5 | Epoch: 37 | Iter: 56400 | Total Loss: 0.006066 | Recon Loss: 0.005367 | Commit Loss: 0.001398 | Perplexity: 1554.757326
2025-09-26 17:54:28,608 Stage: Train 0.5 | Epoch: 37 | Iter: 56600 | Total Loss: 0.006187 | Recon Loss: 0.005478 | Commit Loss: 0.001419 | Perplexity: 1562.727011
2025-09-26 17:55:12,294 Stage: Train 0.5 | Epoch: 37 | Iter: 56800 | Total Loss: 0.006234 | Recon Loss: 0.005523 | Commit Loss: 0.001423 | Perplexity: 1554.682168
2025-09-26 17:55:56,176 Stage: Train 0.5 | Epoch: 37 | Iter: 57000 | Total Loss: 0.006271 | Recon Loss: 0.005569 | Commit Loss: 0.001405 | Perplexity: 1557.221479
2025-09-26 17:56:40,258 Stage: Train 0.5 | Epoch: 37 | Iter: 57200 | Total Loss: 0.006085 | Recon Loss: 0.005373 | Commit Loss: 0.001423 | Perplexity: 1555.884147
2025-09-26 17:57:24,258 Stage: Train 0.5 | Epoch: 37 | Iter: 57400 | Total Loss: 0.006162 | Recon Loss: 0.005452 | Commit Loss: 0.001420 | Perplexity: 1559.502310
2025-09-26 17:58:08,178 Stage: Train 0.5 | Epoch: 37 | Iter: 57600 | Total Loss: 0.006127 | Recon Loss: 0.005420 | Commit Loss: 0.001415 | Perplexity: 1550.515328
Trainning Epoch:  12%|█▏        | 38/330 [3:46:00<27:01:17, 333.14s/it]2025-09-26 17:58:52,103 Stage: Train 0.5 | Epoch: 38 | Iter: 57800 | Total Loss: 0.006181 | Recon Loss: 0.005464 | Commit Loss: 0.001434 | Perplexity: 1558.392341
2025-09-26 17:59:35,953 Stage: Train 0.5 | Epoch: 38 | Iter: 58000 | Total Loss: 0.006012 | Recon Loss: 0.005300 | Commit Loss: 0.001424 | Perplexity: 1554.225741
2025-09-26 18:00:20,042 Stage: Train 0.5 | Epoch: 38 | Iter: 58200 | Total Loss: 0.006247 | Recon Loss: 0.005541 | Commit Loss: 0.001412 | Perplexity: 1559.544930
2025-09-26 18:01:04,090 Stage: Train 0.5 | Epoch: 38 | Iter: 58400 | Total Loss: 0.006126 | Recon Loss: 0.005413 | Commit Loss: 0.001426 | Perplexity: 1556.132040
2025-09-26 18:01:47,425 Stage: Train 0.5 | Epoch: 38 | Iter: 58600 | Total Loss: 0.006280 | Recon Loss: 0.005573 | Commit Loss: 0.001415 | Perplexity: 1554.091124
2025-09-26 18:02:31,215 Stage: Train 0.5 | Epoch: 38 | Iter: 58800 | Total Loss: 0.005904 | Recon Loss: 0.005185 | Commit Loss: 0.001437 | Perplexity: 1564.726616
2025-09-26 18:03:14,792 Stage: Train 0.5 | Epoch: 38 | Iter: 59000 | Total Loss: 0.006257 | Recon Loss: 0.005554 | Commit Loss: 0.001407 | Perplexity: 1554.860500
2025-09-26 18:03:58,561 Stage: Train 0.5 | Epoch: 38 | Iter: 59200 | Total Loss: 0.005949 | Recon Loss: 0.005244 | Commit Loss: 0.001410 | Perplexity: 1552.848900
Trainning Epoch:  12%|█▏        | 39/330 [3:51:33<26:55:14, 333.04s/it]2025-09-26 18:04:42,535 Stage: Train 0.5 | Epoch: 39 | Iter: 59400 | Total Loss: 0.006241 | Recon Loss: 0.005532 | Commit Loss: 0.001419 | Perplexity: 1554.455895
2025-09-26 18:05:26,282 Stage: Train 0.5 | Epoch: 39 | Iter: 59600 | Total Loss: 0.005942 | Recon Loss: 0.005239 | Commit Loss: 0.001407 | Perplexity: 1551.578263
2025-09-26 18:06:10,116 Stage: Train 0.5 | Epoch: 39 | Iter: 59800 | Total Loss: 0.006141 | Recon Loss: 0.005433 | Commit Loss: 0.001415 | Perplexity: 1556.694125
2025-09-26 18:06:54,142 Stage: Train 0.5 | Epoch: 39 | Iter: 60000 | Total Loss: 0.005974 | Recon Loss: 0.005262 | Commit Loss: 0.001423 | Perplexity: 1552.895175
2025-09-26 18:06:54,142 Saving model at iteration 60000
2025-09-26 18:06:54,694 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000
2025-09-26 18:06:54,991 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/model.safetensors
2025-09-26 18:06:55,398 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/optimizer.bin
2025-09-26 18:06:55,399 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/scheduler.bin
2025-09-26 18:06:55,399 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/sampler.bin
2025-09-26 18:06:55,400 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/random_states_0.pkl
2025-09-26 18:07:39,398 Stage: Train 0.5 | Epoch: 39 | Iter: 60200 | Total Loss: 0.006119 | Recon Loss: 0.005408 | Commit Loss: 0.001423 | Perplexity: 1557.379346
2025-09-26 18:08:23,010 Stage: Train 0.5 | Epoch: 39 | Iter: 60400 | Total Loss: 0.006081 | Recon Loss: 0.005373 | Commit Loss: 0.001417 | Perplexity: 1556.262520
2025-09-26 18:09:06,609 Stage: Train 0.5 | Epoch: 39 | Iter: 60600 | Total Loss: 0.006143 | Recon Loss: 0.005429 | Commit Loss: 0.001428 | Perplexity: 1552.789822
Trainning Epoch:  12%|█▏        | 40/330 [3:57:07<26:51:00, 333.31s/it]2025-09-26 18:09:50,616 Stage: Train 0.5 | Epoch: 40 | Iter: 60800 | Total Loss: 0.006319 | Recon Loss: 0.005608 | Commit Loss: 0.001421 | Perplexity: 1552.317795
2025-09-26 18:10:34,665 Stage: Train 0.5 | Epoch: 40 | Iter: 61000 | Total Loss: 0.005855 | Recon Loss: 0.005153 | Commit Loss: 0.001403 | Perplexity: 1552.715719
2025-09-26 18:11:18,584 Stage: Train 0.5 | Epoch: 40 | Iter: 61200 | Total Loss: 0.006075 | Recon Loss: 0.005362 | Commit Loss: 0.001425 | Perplexity: 1559.357756
2025-09-26 18:12:02,228 Stage: Train 0.5 | Epoch: 40 | Iter: 61400 | Total Loss: 0.005988 | Recon Loss: 0.005281 | Commit Loss: 0.001415 | Perplexity: 1553.907601
2025-09-26 18:12:46,044 Stage: Train 0.5 | Epoch: 40 | Iter: 61600 | Total Loss: 0.006061 | Recon Loss: 0.005349 | Commit Loss: 0.001424 | Perplexity: 1556.503297
2025-09-26 18:13:29,800 Stage: Train 0.5 | Epoch: 40 | Iter: 61800 | Total Loss: 0.006020 | Recon Loss: 0.005308 | Commit Loss: 0.001424 | Perplexity: 1553.083795
2025-09-26 18:14:13,640 Stage: Train 0.5 | Epoch: 40 | Iter: 62000 | Total Loss: 0.006130 | Recon Loss: 0.005423 | Commit Loss: 0.001413 | Perplexity: 1551.372998
2025-09-26 18:14:57,552 Stage: Train 0.5 | Epoch: 40 | Iter: 62200 | Total Loss: 0.006006 | Recon Loss: 0.005294 | Commit Loss: 0.001424 | Perplexity: 1549.870093
Trainning Epoch:  12%|█▏        | 41/330 [4:02:40<26:45:24, 333.30s/it]2025-09-26 18:15:41,167 Stage: Train 0.5 | Epoch: 41 | Iter: 62400 | Total Loss: 0.006245 | Recon Loss: 0.005535 | Commit Loss: 0.001420 | Perplexity: 1550.148651
2025-09-26 18:16:24,964 Stage: Train 0.5 | Epoch: 41 | Iter: 62600 | Total Loss: 0.005860 | Recon Loss: 0.005150 | Commit Loss: 0.001420 | Perplexity: 1554.217297
2025-09-26 18:17:08,916 Stage: Train 0.5 | Epoch: 41 | Iter: 62800 | Total Loss: 0.005928 | Recon Loss: 0.005212 | Commit Loss: 0.001431 | Perplexity: 1550.940292
2025-09-26 18:17:52,585 Stage: Train 0.5 | Epoch: 41 | Iter: 63000 | Total Loss: 0.005997 | Recon Loss: 0.005291 | Commit Loss: 0.001413 | Perplexity: 1550.550568
2025-09-26 18:18:36,260 Stage: Train 0.5 | Epoch: 41 | Iter: 63200 | Total Loss: 0.005975 | Recon Loss: 0.005262 | Commit Loss: 0.001427 | Perplexity: 1552.365173
2025-09-26 18:19:19,855 Stage: Train 0.5 | Epoch: 41 | Iter: 63400 | Total Loss: 0.005943 | Recon Loss: 0.005225 | Commit Loss: 0.001437 | Perplexity: 1556.077979
2025-09-26 18:20:03,957 Stage: Train 0.5 | Epoch: 41 | Iter: 63600 | Total Loss: 0.006001 | Recon Loss: 0.005290 | Commit Loss: 0.001422 | Perplexity: 1556.315623
Trainning Epoch:  13%|█▎        | 42/330 [4:08:13<26:38:49, 333.09s/it]2025-09-26 18:20:48,084 Stage: Train 0.5 | Epoch: 42 | Iter: 63800 | Total Loss: 0.005967 | Recon Loss: 0.005255 | Commit Loss: 0.001424 | Perplexity: 1553.498982
2025-09-26 18:21:32,190 Stage: Train 0.5 | Epoch: 42 | Iter: 64000 | Total Loss: 0.005974 | Recon Loss: 0.005262 | Commit Loss: 0.001426 | Perplexity: 1551.326758
2025-09-26 18:22:15,878 Stage: Train 0.5 | Epoch: 42 | Iter: 64200 | Total Loss: 0.005730 | Recon Loss: 0.005029 | Commit Loss: 0.001403 | Perplexity: 1548.128374
2025-09-26 18:22:59,717 Stage: Train 0.5 | Epoch: 42 | Iter: 64400 | Total Loss: 0.006153 | Recon Loss: 0.005441 | Commit Loss: 0.001424 | Perplexity: 1549.613521
2025-09-26 18:23:43,472 Stage: Train 0.5 | Epoch: 42 | Iter: 64600 | Total Loss: 0.005885 | Recon Loss: 0.005174 | Commit Loss: 0.001422 | Perplexity: 1553.692532
2025-09-26 18:24:27,446 Stage: Train 0.5 | Epoch: 42 | Iter: 64800 | Total Loss: 0.006043 | Recon Loss: 0.005331 | Commit Loss: 0.001423 | Perplexity: 1549.521228
2025-09-26 18:25:11,381 Stage: Train 0.5 | Epoch: 42 | Iter: 65000 | Total Loss: 0.005955 | Recon Loss: 0.005251 | Commit Loss: 0.001408 | Perplexity: 1548.378852
2025-09-26 18:25:54,947 Stage: Train 0.5 | Epoch: 42 | Iter: 65200 | Total Loss: 0.005925 | Recon Loss: 0.005211 | Commit Loss: 0.001428 | Perplexity: 1553.179653
Trainning Epoch:  13%|█▎        | 43/330 [4:13:46<26:33:22, 333.11s/it]2025-09-26 18:26:38,944 Stage: Train 0.5 | Epoch: 43 | Iter: 65400 | Total Loss: 0.005978 | Recon Loss: 0.005270 | Commit Loss: 0.001416 | Perplexity: 1555.475095
2025-09-26 18:27:22,746 Stage: Train 0.5 | Epoch: 43 | Iter: 65600 | Total Loss: 0.005876 | Recon Loss: 0.005167 | Commit Loss: 0.001419 | Perplexity: 1549.771769
2025-09-26 18:28:06,355 Stage: Train 0.5 | Epoch: 43 | Iter: 65800 | Total Loss: 0.005998 | Recon Loss: 0.005286 | Commit Loss: 0.001426 | Perplexity: 1550.760287
2025-09-26 18:28:49,936 Stage: Train 0.5 | Epoch: 43 | Iter: 66000 | Total Loss: 0.006278 | Recon Loss: 0.005571 | Commit Loss: 0.001415 | Perplexity: 1549.130419
2025-09-26 18:29:33,060 Stage: Train 0.5 | Epoch: 43 | Iter: 66200 | Total Loss: 0.005764 | Recon Loss: 0.005053 | Commit Loss: 0.001421 | Perplexity: 1549.144333
2025-09-26 18:30:16,773 Stage: Train 0.5 | Epoch: 43 | Iter: 66400 | Total Loss: 0.005870 | Recon Loss: 0.005165 | Commit Loss: 0.001410 | Perplexity: 1548.717260
2025-09-26 18:31:00,736 Stage: Train 0.5 | Epoch: 43 | Iter: 66600 | Total Loss: 0.005819 | Recon Loss: 0.005105 | Commit Loss: 0.001428 | Perplexity: 1551.063869
2025-09-26 18:31:44,822 Stage: Train 0.5 | Epoch: 43 | Iter: 66800 | Total Loss: 0.005967 | Recon Loss: 0.005253 | Commit Loss: 0.001429 | Perplexity: 1554.707690
Trainning Epoch:  13%|█▎        | 44/330 [4:19:18<26:26:30, 332.83s/it]2025-09-26 18:32:28,469 Stage: Train 0.5 | Epoch: 44 | Iter: 67000 | Total Loss: 0.005801 | Recon Loss: 0.005092 | Commit Loss: 0.001419 | Perplexity: 1551.353552
2025-09-26 18:33:12,179 Stage: Train 0.5 | Epoch: 44 | Iter: 67200 | Total Loss: 0.005796 | Recon Loss: 0.005089 | Commit Loss: 0.001414 | Perplexity: 1547.339107
2025-09-26 18:33:56,027 Stage: Train 0.5 | Epoch: 44 | Iter: 67400 | Total Loss: 0.005828 | Recon Loss: 0.005120 | Commit Loss: 0.001418 | Perplexity: 1547.557321
2025-09-26 18:34:40,071 Stage: Train 0.5 | Epoch: 44 | Iter: 67600 | Total Loss: 0.005841 | Recon Loss: 0.005125 | Commit Loss: 0.001431 | Perplexity: 1551.292523
2025-09-26 18:35:24,091 Stage: Train 0.5 | Epoch: 44 | Iter: 67800 | Total Loss: 0.005809 | Recon Loss: 0.005096 | Commit Loss: 0.001426 | Perplexity: 1549.795898
2025-09-26 18:36:07,378 Stage: Train 0.5 | Epoch: 44 | Iter: 68000 | Total Loss: 0.005928 | Recon Loss: 0.005215 | Commit Loss: 0.001426 | Perplexity: 1556.068752
2025-09-26 18:36:49,985 Stage: Train 0.5 | Epoch: 44 | Iter: 68200 | Total Loss: 0.005746 | Recon Loss: 0.005040 | Commit Loss: 0.001412 | Perplexity: 1548.965095
Trainning Epoch:  14%|█▎        | 45/330 [4:24:50<26:19:03, 332.43s/it]2025-09-26 18:37:34,468 Stage: Train 0.5 | Epoch: 45 | Iter: 68400 | Total Loss: 0.006237 | Recon Loss: 0.005524 | Commit Loss: 0.001426 | Perplexity: 1546.514720
2025-09-26 18:38:18,521 Stage: Train 0.5 | Epoch: 45 | Iter: 68600 | Total Loss: 0.006043 | Recon Loss: 0.005349 | Commit Loss: 0.001389 | Perplexity: 1538.934766
2025-09-26 18:39:02,715 Stage: Train 0.5 | Epoch: 45 | Iter: 68800 | Total Loss: 0.005805 | Recon Loss: 0.005102 | Commit Loss: 0.001406 | Perplexity: 1546.174050
2025-09-26 18:39:46,345 Stage: Train 0.5 | Epoch: 45 | Iter: 69000 | Total Loss: 0.005906 | Recon Loss: 0.005192 | Commit Loss: 0.001428 | Perplexity: 1547.968099
2025-09-26 18:40:30,002 Stage: Train 0.5 | Epoch: 45 | Iter: 69200 | Total Loss: 0.005946 | Recon Loss: 0.005243 | Commit Loss: 0.001407 | Perplexity: 1547.127439
2025-09-26 18:41:13,703 Stage: Train 0.5 | Epoch: 45 | Iter: 69400 | Total Loss: 0.005777 | Recon Loss: 0.005074 | Commit Loss: 0.001407 | Perplexity: 1551.099448
2025-09-26 18:41:57,372 Stage: Train 0.5 | Epoch: 45 | Iter: 69600 | Total Loss: 0.005946 | Recon Loss: 0.005240 | Commit Loss: 0.001412 | Perplexity: 1546.433482
2025-09-26 18:42:40,819 Stage: Train 0.5 | Epoch: 45 | Iter: 69800 | Total Loss: 0.005775 | Recon Loss: 0.005058 | Commit Loss: 0.001433 | Perplexity: 1548.068881
Trainning Epoch:  14%|█▍        | 46/330 [4:30:23<26:13:57, 332.53s/it]2025-09-26 18:43:24,791 Stage: Train 0.5 | Epoch: 46 | Iter: 70000 | Total Loss: 0.005862 | Recon Loss: 0.005152 | Commit Loss: 0.001421 | Perplexity: 1545.188568
2025-09-26 18:44:08,486 Stage: Train 0.5 | Epoch: 46 | Iter: 70200 | Total Loss: 0.005889 | Recon Loss: 0.005185 | Commit Loss: 0.001408 | Perplexity: 1543.389631
2025-09-26 18:44:52,454 Stage: Train 0.5 | Epoch: 46 | Iter: 70400 | Total Loss: 0.005748 | Recon Loss: 0.005040 | Commit Loss: 0.001417 | Perplexity: 1543.550204
2025-09-26 18:45:36,472 Stage: Train 0.5 | Epoch: 46 | Iter: 70600 | Total Loss: 0.005770 | Recon Loss: 0.005060 | Commit Loss: 0.001419 | Perplexity: 1542.629151
2025-09-26 18:46:20,282 Stage: Train 0.5 | Epoch: 46 | Iter: 70800 | Total Loss: 0.005883 | Recon Loss: 0.005178 | Commit Loss: 0.001411 | Perplexity: 1549.612086
2025-09-26 18:47:04,309 Stage: Train 0.5 | Epoch: 46 | Iter: 71000 | Total Loss: 0.005964 | Recon Loss: 0.005257 | Commit Loss: 0.001414 | Perplexity: 1544.450265
2025-09-26 18:47:48,347 Stage: Train 0.5 | Epoch: 46 | Iter: 71200 | Total Loss: 0.005762 | Recon Loss: 0.005056 | Commit Loss: 0.001411 | Perplexity: 1548.183466
Trainning Epoch:  14%|█▍        | 47/330 [4:35:56<26:10:11, 332.90s/it]2025-09-26 18:48:32,551 Stage: Train 0.5 | Epoch: 47 | Iter: 71400 | Total Loss: 0.005755 | Recon Loss: 0.005046 | Commit Loss: 0.001418 | Perplexity: 1545.862112
2025-09-26 18:49:16,383 Stage: Train 0.5 | Epoch: 47 | Iter: 71600 | Total Loss: 0.006035 | Recon Loss: 0.005334 | Commit Loss: 0.001402 | Perplexity: 1542.516559
2025-09-26 18:50:00,079 Stage: Train 0.5 | Epoch: 47 | Iter: 71800 | Total Loss: 0.005573 | Recon Loss: 0.004863 | Commit Loss: 0.001420 | Perplexity: 1548.308965
2025-09-26 18:50:43,851 Stage: Train 0.5 | Epoch: 47 | Iter: 72000 | Total Loss: 0.005808 | Recon Loss: 0.005100 | Commit Loss: 0.001417 | Perplexity: 1548.296421
2025-09-26 18:51:27,568 Stage: Train 0.5 | Epoch: 47 | Iter: 72200 | Total Loss: 0.005857 | Recon Loss: 0.005146 | Commit Loss: 0.001420 | Perplexity: 1542.620059
2025-09-26 18:52:11,365 Stage: Train 0.5 | Epoch: 47 | Iter: 72400 | Total Loss: 0.005815 | Recon Loss: 0.005121 | Commit Loss: 0.001390 | Perplexity: 1541.040985
2025-09-26 18:52:55,012 Stage: Train 0.5 | Epoch: 47 | Iter: 72600 | Total Loss: 0.005764 | Recon Loss: 0.005052 | Commit Loss: 0.001424 | Perplexity: 1547.863409
2025-09-26 18:53:38,782 Stage: Train 0.5 | Epoch: 47 | Iter: 72800 | Total Loss: 0.005662 | Recon Loss: 0.004952 | Commit Loss: 0.001421 | Perplexity: 1548.499501
Trainning Epoch:  15%|█▍        | 48/330 [4:41:29<26:04:31, 332.88s/it]2025-09-26 18:54:23,156 Stage: Train 0.5 | Epoch: 48 | Iter: 73000 | Total Loss: 0.005712 | Recon Loss: 0.005006 | Commit Loss: 0.001413 | Perplexity: 1544.942204
2025-09-26 18:55:07,040 Stage: Train 0.5 | Epoch: 48 | Iter: 73200 | Total Loss: 0.005765 | Recon Loss: 0.005067 | Commit Loss: 0.001397 | Perplexity: 1538.270422
2025-09-26 18:55:50,705 Stage: Train 0.5 | Epoch: 48 | Iter: 73400 | Total Loss: 0.005672 | Recon Loss: 0.004954 | Commit Loss: 0.001437 | Perplexity: 1548.906991
2025-09-26 18:56:34,395 Stage: Train 0.5 | Epoch: 48 | Iter: 73600 | Total Loss: 0.005739 | Recon Loss: 0.005029 | Commit Loss: 0.001419 | Perplexity: 1540.262579
2025-09-26 18:57:18,441 Stage: Train 0.5 | Epoch: 48 | Iter: 73800 | Total Loss: 0.005649 | Recon Loss: 0.004947 | Commit Loss: 0.001404 | Perplexity: 1546.609072
2025-09-26 18:58:02,448 Stage: Train 0.5 | Epoch: 48 | Iter: 74000 | Total Loss: 0.005707 | Recon Loss: 0.004992 | Commit Loss: 0.001429 | Perplexity: 1546.436559
2025-09-26 18:58:46,391 Stage: Train 0.5 | Epoch: 48 | Iter: 74200 | Total Loss: 0.005669 | Recon Loss: 0.004966 | Commit Loss: 0.001407 | Perplexity: 1539.348652
2025-09-26 18:59:29,996 Stage: Train 0.5 | Epoch: 48 | Iter: 74400 | Total Loss: 0.005957 | Recon Loss: 0.005247 | Commit Loss: 0.001419 | Perplexity: 1544.523256
Trainning Epoch:  15%|█▍        | 49/330 [4:47:02<25:59:06, 332.91s/it]2025-09-26 19:00:14,226 Stage: Train 0.5 | Epoch: 49 | Iter: 74600 | Total Loss: 0.005681 | Recon Loss: 0.004972 | Commit Loss: 0.001418 | Perplexity: 1544.468281
2025-09-26 19:00:58,139 Stage: Train 0.5 | Epoch: 49 | Iter: 74800 | Total Loss: 0.005585 | Recon Loss: 0.004877 | Commit Loss: 0.001416 | Perplexity: 1547.104532
2025-09-26 19:01:41,766 Stage: Train 0.5 | Epoch: 49 | Iter: 75000 | Total Loss: 0.005792 | Recon Loss: 0.005086 | Commit Loss: 0.001413 | Perplexity: 1541.506617
2025-09-26 19:02:25,877 Stage: Train 0.5 | Epoch: 49 | Iter: 75200 | Total Loss: 0.005818 | Recon Loss: 0.005113 | Commit Loss: 0.001410 | Perplexity: 1538.342329
2025-09-26 19:03:09,633 Stage: Train 0.5 | Epoch: 49 | Iter: 75400 | Total Loss: 0.005569 | Recon Loss: 0.004855 | Commit Loss: 0.001428 | Perplexity: 1541.635977
2025-09-26 19:03:53,489 Stage: Train 0.5 | Epoch: 49 | Iter: 75600 | Total Loss: 0.005657 | Recon Loss: 0.004951 | Commit Loss: 0.001412 | Perplexity: 1542.413108
2025-09-26 19:04:37,280 Stage: Train 0.5 | Epoch: 49 | Iter: 75800 | Total Loss: 0.005763 | Recon Loss: 0.005042 | Commit Loss: 0.001441 | Perplexity: 1539.867476
Trainning Epoch:  15%|█▌        | 50/330 [4:52:36<25:54:35, 333.13s/it]2025-09-26 19:05:21,458 Stage: Train 0.5 | Epoch: 50 | Iter: 76000 | Total Loss: 0.005666 | Recon Loss: 0.004969 | Commit Loss: 0.001394 | Perplexity: 1539.321876
2025-09-26 19:06:05,218 Stage: Train 0.5 | Epoch: 50 | Iter: 76200 | Total Loss: 0.005615 | Recon Loss: 0.004912 | Commit Loss: 0.001405 | Perplexity: 1541.484150
2025-09-26 19:06:48,652 Stage: Train 0.5 | Epoch: 50 | Iter: 76400 | Total Loss: 0.005785 | Recon Loss: 0.005077 | Commit Loss: 0.001417 | Perplexity: 1540.623057
2025-09-26 19:07:32,502 Stage: Train 0.5 | Epoch: 50 | Iter: 76600 | Total Loss: 0.006064 | Recon Loss: 0.005357 | Commit Loss: 0.001414 | Perplexity: 1538.315936
2025-09-26 19:08:16,506 Stage: Train 0.5 | Epoch: 50 | Iter: 76800 | Total Loss: 0.005579 | Recon Loss: 0.004875 | Commit Loss: 0.001408 | Perplexity: 1536.378491
2025-09-26 19:09:00,701 Stage: Train 0.5 | Epoch: 50 | Iter: 77000 | Total Loss: 0.005653 | Recon Loss: 0.004936 | Commit Loss: 0.001434 | Perplexity: 1547.279902
2025-09-26 19:09:44,192 Stage: Train 0.5 | Epoch: 50 | Iter: 77200 | Total Loss: 0.005548 | Recon Loss: 0.004841 | Commit Loss: 0.001413 | Perplexity: 1539.588237
2025-09-26 19:10:28,155 Stage: Train 0.5 | Epoch: 50 | Iter: 77400 | Total Loss: 0.005716 | Recon Loss: 0.005011 | Commit Loss: 0.001412 | Perplexity: 1543.702363
Trainning Epoch:  15%|█▌        | 51/330 [4:58:09<25:48:52, 333.09s/it]2025-09-26 19:11:12,163 Stage: Train 0.5 | Epoch: 51 | Iter: 77600 | Total Loss: 0.005661 | Recon Loss: 0.004956 | Commit Loss: 0.001411 | Perplexity: 1535.066694
2025-09-26 19:11:56,431 Stage: Train 0.5 | Epoch: 51 | Iter: 77800 | Total Loss: 0.005650 | Recon Loss: 0.004942 | Commit Loss: 0.001415 | Perplexity: 1539.800846
2025-09-26 19:12:40,299 Stage: Train 0.5 | Epoch: 51 | Iter: 78000 | Total Loss: 0.005522 | Recon Loss: 0.004818 | Commit Loss: 0.001407 | Perplexity: 1540.119416
2025-09-26 19:13:23,541 Stage: Train 0.5 | Epoch: 51 | Iter: 78200 | Total Loss: 0.005617 | Recon Loss: 0.004905 | Commit Loss: 0.001423 | Perplexity: 1539.075493
2025-09-26 19:14:07,525 Stage: Train 0.5 | Epoch: 51 | Iter: 78400 | Total Loss: 0.005520 | Recon Loss: 0.004810 | Commit Loss: 0.001419 | Perplexity: 1540.019995
2025-09-26 19:14:51,391 Stage: Train 0.5 | Epoch: 51 | Iter: 78600 | Total Loss: 0.005706 | Recon Loss: 0.005000 | Commit Loss: 0.001412 | Perplexity: 1537.071739
2025-09-26 19:15:35,246 Stage: Train 0.5 | Epoch: 51 | Iter: 78800 | Total Loss: 0.005449 | Recon Loss: 0.004738 | Commit Loss: 0.001422 | Perplexity: 1539.898319
Trainning Epoch:  16%|█▌        | 52/330 [5:03:42<25:43:41, 333.17s/it]2025-09-26 19:16:19,472 Stage: Train 0.5 | Epoch: 52 | Iter: 79000 | Total Loss: 0.005804 | Recon Loss: 0.005090 | Commit Loss: 0.001427 | Perplexity: 1537.118455
2025-09-26 19:17:02,884 Stage: Train 0.5 | Epoch: 52 | Iter: 79200 | Total Loss: 0.005620 | Recon Loss: 0.004911 | Commit Loss: 0.001418 | Perplexity: 1539.801746
2025-09-26 19:17:46,900 Stage: Train 0.5 | Epoch: 52 | Iter: 79400 | Total Loss: 0.005603 | Recon Loss: 0.004898 | Commit Loss: 0.001411 | Perplexity: 1535.722671
2025-09-26 19:18:30,846 Stage: Train 0.5 | Epoch: 52 | Iter: 79600 | Total Loss: 0.005557 | Recon Loss: 0.004855 | Commit Loss: 0.001404 | Perplexity: 1537.976661
2025-09-26 19:19:14,757 Stage: Train 0.5 | Epoch: 52 | Iter: 79800 | Total Loss: 0.005702 | Recon Loss: 0.004991 | Commit Loss: 0.001421 | Perplexity: 1540.948052
2025-09-26 19:19:58,549 Stage: Train 0.5 | Epoch: 52 | Iter: 80000 | Total Loss: 0.005492 | Recon Loss: 0.004778 | Commit Loss: 0.001429 | Perplexity: 1542.679604
2025-09-26 19:19:58,549 Saving model at iteration 80000
2025-09-26 19:19:58,756 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000
2025-09-26 19:19:59,074 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/model.safetensors
2025-09-26 19:19:59,524 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/optimizer.bin
2025-09-26 19:19:59,525 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/scheduler.bin
2025-09-26 19:19:59,525 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/sampler.bin
2025-09-26 19:19:59,526 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/random_states_0.pkl
2025-09-26 19:20:43,475 Stage: Train 0.5 | Epoch: 52 | Iter: 80200 | Total Loss: 0.005619 | Recon Loss: 0.004908 | Commit Loss: 0.001422 | Perplexity: 1538.731124
2025-09-26 19:21:26,019 Stage: Train 0.5 | Epoch: 52 | Iter: 80400 | Total Loss: 0.005652 | Recon Loss: 0.004941 | Commit Loss: 0.001421 | Perplexity: 1540.863949
Trainning Epoch:  16%|█▌        | 53/330 [5:09:15<25:38:00, 333.14s/it]2025-09-26 19:22:10,323 Stage: Train 0.5 | Epoch: 53 | Iter: 80600 | Total Loss: 0.005571 | Recon Loss: 0.004858 | Commit Loss: 0.001425 | Perplexity: 1540.126673
2025-09-26 19:22:54,316 Stage: Train 0.5 | Epoch: 53 | Iter: 80800 | Total Loss: 0.005553 | Recon Loss: 0.004849 | Commit Loss: 0.001408 | Perplexity: 1538.124155
2025-09-26 19:23:37,883 Stage: Train 0.5 | Epoch: 53 | Iter: 81000 | Total Loss: 0.005735 | Recon Loss: 0.005033 | Commit Loss: 0.001405 | Perplexity: 1535.610127
2025-09-26 19:24:21,388 Stage: Train 0.5 | Epoch: 53 | Iter: 81200 | Total Loss: 0.005559 | Recon Loss: 0.004848 | Commit Loss: 0.001422 | Perplexity: 1542.136510
2025-09-26 19:25:05,140 Stage: Train 0.5 | Epoch: 53 | Iter: 81400 | Total Loss: 0.005591 | Recon Loss: 0.004883 | Commit Loss: 0.001416 | Perplexity: 1545.997943
2025-09-26 19:25:49,476 Stage: Train 0.5 | Epoch: 53 | Iter: 81600 | Total Loss: 0.005570 | Recon Loss: 0.004861 | Commit Loss: 0.001417 | Perplexity: 1539.842297
2025-09-26 19:26:33,030 Stage: Train 0.5 | Epoch: 53 | Iter: 81800 | Total Loss: 0.005530 | Recon Loss: 0.004824 | Commit Loss: 0.001413 | Perplexity: 1536.474146
2025-09-26 19:27:16,701 Stage: Train 0.5 | Epoch: 53 | Iter: 82000 | Total Loss: 0.005626 | Recon Loss: 0.004928 | Commit Loss: 0.001396 | Perplexity: 1534.003342
Trainning Epoch:  16%|█▋        | 54/330 [5:14:48<25:31:58, 333.04s/it]2025-09-26 19:28:00,773 Stage: Train 0.5 | Epoch: 54 | Iter: 82200 | Total Loss: 0.005460 | Recon Loss: 0.004757 | Commit Loss: 0.001407 | Perplexity: 1539.867420
2025-09-26 19:28:44,751 Stage: Train 0.5 | Epoch: 54 | Iter: 82400 | Total Loss: 0.005776 | Recon Loss: 0.005071 | Commit Loss: 0.001410 | Perplexity: 1533.376575
2025-09-26 19:29:28,428 Stage: Train 0.5 | Epoch: 54 | Iter: 82600 | Total Loss: 0.005586 | Recon Loss: 0.004879 | Commit Loss: 0.001414 | Perplexity: 1534.427219
2025-09-26 19:30:11,896 Stage: Train 0.5 | Epoch: 54 | Iter: 82800 | Total Loss: 0.005472 | Recon Loss: 0.004757 | Commit Loss: 0.001430 | Perplexity: 1538.021025
2025-09-26 19:30:55,896 Stage: Train 0.5 | Epoch: 54 | Iter: 83000 | Total Loss: 0.005587 | Recon Loss: 0.004880 | Commit Loss: 0.001414 | Perplexity: 1538.044393
2025-09-26 19:31:39,974 Stage: Train 0.5 | Epoch: 54 | Iter: 83200 | Total Loss: 0.005399 | Recon Loss: 0.004695 | Commit Loss: 0.001409 | Perplexity: 1540.314075
2025-09-26 19:32:23,950 Stage: Train 0.5 | Epoch: 54 | Iter: 83400 | Total Loss: 0.005564 | Recon Loss: 0.004863 | Commit Loss: 0.001403 | Perplexity: 1539.743378
Trainning Epoch:  17%|█▋        | 55/330 [5:20:21<25:26:43, 333.10s/it]2025-09-26 19:33:08,054 Stage: Train 0.5 | Epoch: 55 | Iter: 83600 | Total Loss: 0.005601 | Recon Loss: 0.004895 | Commit Loss: 0.001412 | Perplexity: 1532.800252
2025-09-26 19:33:51,690 Stage: Train 0.5 | Epoch: 55 | Iter: 83800 | Total Loss: 0.005425 | Recon Loss: 0.004721 | Commit Loss: 0.001407 | Perplexity: 1533.954314
2025-09-26 19:34:35,413 Stage: Train 0.5 | Epoch: 55 | Iter: 84000 | Total Loss: 0.005416 | Recon Loss: 0.004709 | Commit Loss: 0.001414 | Perplexity: 1535.757221
2025-09-26 19:35:19,310 Stage: Train 0.5 | Epoch: 55 | Iter: 84200 | Total Loss: 0.005499 | Recon Loss: 0.004793 | Commit Loss: 0.001413 | Perplexity: 1542.272988
2025-09-26 19:36:03,200 Stage: Train 0.5 | Epoch: 55 | Iter: 84400 | Total Loss: 0.005478 | Recon Loss: 0.004764 | Commit Loss: 0.001428 | Perplexity: 1541.317784
2025-09-26 19:36:47,234 Stage: Train 0.5 | Epoch: 55 | Iter: 84600 | Total Loss: 0.005630 | Recon Loss: 0.004925 | Commit Loss: 0.001409 | Perplexity: 1535.392347
2025-09-26 19:37:30,800 Stage: Train 0.5 | Epoch: 55 | Iter: 84800 | Total Loss: 0.005477 | Recon Loss: 0.004770 | Commit Loss: 0.001414 | Perplexity: 1538.373968
2025-09-26 19:38:14,689 Stage: Train 0.5 | Epoch: 55 | Iter: 85000 | Total Loss: 0.005459 | Recon Loss: 0.004751 | Commit Loss: 0.001417 | Perplexity: 1537.698315
Trainning Epoch:  17%|█▋        | 56/330 [5:25:54<25:21:09, 333.10s/it]2025-09-26 19:38:58,733 Stage: Train 0.5 | Epoch: 56 | Iter: 85200 | Total Loss: 0.005383 | Recon Loss: 0.004670 | Commit Loss: 0.001425 | Perplexity: 1540.942161
2025-09-26 19:39:42,594 Stage: Train 0.5 | Epoch: 56 | Iter: 85400 | Total Loss: 0.005606 | Recon Loss: 0.004895 | Commit Loss: 0.001423 | Perplexity: 1538.678570
2025-09-26 19:40:25,961 Stage: Train 0.5 | Epoch: 56 | Iter: 85600 | Total Loss: 0.005438 | Recon Loss: 0.004735 | Commit Loss: 0.001407 | Perplexity: 1534.946486
2025-09-26 19:41:09,826 Stage: Train 0.5 | Epoch: 56 | Iter: 85800 | Total Loss: 0.005404 | Recon Loss: 0.004692 | Commit Loss: 0.001423 | Perplexity: 1538.940583
2025-09-26 19:41:53,717 Stage: Train 0.5 | Epoch: 56 | Iter: 86000 | Total Loss: 0.005440 | Recon Loss: 0.004739 | Commit Loss: 0.001403 | Perplexity: 1537.937561
2025-09-26 19:42:37,376 Stage: Train 0.5 | Epoch: 56 | Iter: 86200 | Total Loss: 0.005428 | Recon Loss: 0.004718 | Commit Loss: 0.001420 | Perplexity: 1540.138763
2025-09-26 19:43:21,078 Stage: Train 0.5 | Epoch: 56 | Iter: 86400 | Total Loss: 0.005417 | Recon Loss: 0.004712 | Commit Loss: 0.001410 | Perplexity: 1536.168928
Trainning Epoch:  17%|█▋        | 57/330 [5:31:27<25:14:23, 332.83s/it]2025-09-26 19:44:04,983 Stage: Train 0.5 | Epoch: 57 | Iter: 86600 | Total Loss: 0.005347 | Recon Loss: 0.004638 | Commit Loss: 0.001417 | Perplexity: 1538.350980
2025-09-26 19:44:48,907 Stage: Train 0.5 | Epoch: 57 | Iter: 86800 | Total Loss: 0.006571 | Recon Loss: 0.005864 | Commit Loss: 0.001414 | Perplexity: 1537.267537
2025-09-26 19:45:32,870 Stage: Train 0.5 | Epoch: 57 | Iter: 87000 | Total Loss: 0.005195 | Recon Loss: 0.004510 | Commit Loss: 0.001371 | Perplexity: 1534.883923
2025-09-26 19:46:16,617 Stage: Train 0.5 | Epoch: 57 | Iter: 87200 | Total Loss: 0.005392 | Recon Loss: 0.004685 | Commit Loss: 0.001414 | Perplexity: 1539.461061
2025-09-26 19:47:00,256 Stage: Train 0.5 | Epoch: 57 | Iter: 87400 | Total Loss: 0.005350 | Recon Loss: 0.004642 | Commit Loss: 0.001415 | Perplexity: 1537.067660
2025-09-26 19:47:43,846 Stage: Train 0.5 | Epoch: 57 | Iter: 87600 | Total Loss: 0.005474 | Recon Loss: 0.004766 | Commit Loss: 0.001416 | Perplexity: 1536.597522
2025-09-26 19:48:27,834 Stage: Train 0.5 | Epoch: 57 | Iter: 87800 | Total Loss: 0.005478 | Recon Loss: 0.004773 | Commit Loss: 0.001409 | Perplexity: 1537.199868
2025-09-26 19:49:11,477 Stage: Train 0.5 | Epoch: 57 | Iter: 88000 | Total Loss: 0.005389 | Recon Loss: 0.004686 | Commit Loss: 0.001406 | Perplexity: 1534.659371
Trainning Epoch:  18%|█▊        | 58/330 [5:36:59<25:08:45, 332.81s/it]2025-09-26 19:49:55,544 Stage: Train 0.5 | Epoch: 58 | Iter: 88200 | Total Loss: 0.005526 | Recon Loss: 0.004834 | Commit Loss: 0.001384 | Perplexity: 1529.575007
2025-09-26 19:50:38,881 Stage: Train 0.5 | Epoch: 58 | Iter: 88400 | Total Loss: 0.005322 | Recon Loss: 0.004617 | Commit Loss: 0.001410 | Perplexity: 1537.472560
2025-09-26 19:51:22,526 Stage: Train 0.5 | Epoch: 58 | Iter: 88600 | Total Loss: 0.005631 | Recon Loss: 0.004924 | Commit Loss: 0.001415 | Perplexity: 1537.694587
2025-09-26 19:52:06,591 Stage: Train 0.5 | Epoch: 58 | Iter: 88800 | Total Loss: 0.005412 | Recon Loss: 0.004703 | Commit Loss: 0.001419 | Perplexity: 1540.651144
2025-09-26 19:52:50,475 Stage: Train 0.5 | Epoch: 58 | Iter: 89000 | Total Loss: 0.005562 | Recon Loss: 0.004862 | Commit Loss: 0.001400 | Perplexity: 1530.222274
2025-09-26 19:53:34,398 Stage: Train 0.5 | Epoch: 58 | Iter: 89200 | Total Loss: 0.005299 | Recon Loss: 0.004600 | Commit Loss: 0.001397 | Perplexity: 1535.267711
2025-09-26 19:54:17,909 Stage: Train 0.5 | Epoch: 58 | Iter: 89400 | Total Loss: 0.005442 | Recon Loss: 0.004742 | Commit Loss: 0.001401 | Perplexity: 1534.931430
2025-09-26 19:55:01,753 Stage: Train 0.5 | Epoch: 58 | Iter: 89600 | Total Loss: 0.005409 | Recon Loss: 0.004702 | Commit Loss: 0.001413 | Perplexity: 1536.341768
Trainning Epoch:  18%|█▊        | 59/330 [5:42:32<25:02:56, 332.76s/it]2025-09-26 19:55:45,948 Stage: Train 0.5 | Epoch: 59 | Iter: 89800 | Total Loss: 0.005484 | Recon Loss: 0.004786 | Commit Loss: 0.001397 | Perplexity: 1530.866079
2025-09-26 19:56:29,723 Stage: Train 0.5 | Epoch: 59 | Iter: 90000 | Total Loss: 0.005220 | Recon Loss: 0.004519 | Commit Loss: 0.001402 | Perplexity: 1533.170311
2025-09-26 19:57:13,441 Stage: Train 0.5 | Epoch: 59 | Iter: 90200 | Total Loss: 0.005536 | Recon Loss: 0.004837 | Commit Loss: 0.001399 | Perplexity: 1529.315566
2025-09-26 19:57:57,036 Stage: Train 0.5 | Epoch: 59 | Iter: 90400 | Total Loss: 0.005352 | Recon Loss: 0.004652 | Commit Loss: 0.001399 | Perplexity: 1534.961385
2025-09-26 19:58:40,888 Stage: Train 0.5 | Epoch: 59 | Iter: 90600 | Total Loss: 0.005373 | Recon Loss: 0.004664 | Commit Loss: 0.001417 | Perplexity: 1533.822066
2025-09-26 19:59:24,822 Stage: Train 0.5 | Epoch: 59 | Iter: 90800 | Total Loss: 0.005454 | Recon Loss: 0.004744 | Commit Loss: 0.001419 | Perplexity: 1536.760411
2025-09-26 20:00:08,582 Stage: Train 0.5 | Epoch: 59 | Iter: 91000 | Total Loss: 0.005398 | Recon Loss: 0.004693 | Commit Loss: 0.001410 | Perplexity: 1532.274860
Trainning Epoch:  18%|█▊        | 60/330 [5:48:05<24:57:43, 332.83s/it]2025-09-26 20:00:52,456 Stage: Train 0.5 | Epoch: 60 | Iter: 91200 | Total Loss: 0.005410 | Recon Loss: 0.004707 | Commit Loss: 0.001406 | Perplexity: 1532.584935
2025-09-26 20:01:36,211 Stage: Train 0.5 | Epoch: 60 | Iter: 91400 | Total Loss: 0.005389 | Recon Loss: 0.004693 | Commit Loss: 0.001393 | Perplexity: 1528.189319
2025-09-26 20:02:20,122 Stage: Train 0.5 | Epoch: 60 | Iter: 91600 | Total Loss: 0.005320 | Recon Loss: 0.004615 | Commit Loss: 0.001409 | Perplexity: 1531.690156
2025-09-26 20:03:03,875 Stage: Train 0.5 | Epoch: 60 | Iter: 91800 | Total Loss: 0.005353 | Recon Loss: 0.004654 | Commit Loss: 0.001400 | Perplexity: 1532.776547
2025-09-26 20:03:47,843 Stage: Train 0.5 | Epoch: 60 | Iter: 92000 | Total Loss: 0.005553 | Recon Loss: 0.004851 | Commit Loss: 0.001403 | Perplexity: 1533.847253
2025-09-26 20:04:31,462 Stage: Train 0.5 | Epoch: 60 | Iter: 92200 | Total Loss: 0.005232 | Recon Loss: 0.004533 | Commit Loss: 0.001399 | Perplexity: 1529.861428
2025-09-26 20:05:15,338 Stage: Train 0.5 | Epoch: 60 | Iter: 92400 | Total Loss: 0.005450 | Recon Loss: 0.004744 | Commit Loss: 0.001412 | Perplexity: 1536.626212
2025-09-26 20:05:57,622 Stage: Train 0.5 | Epoch: 60 | Iter: 92600 | Total Loss: 0.005355 | Recon Loss: 0.004652 | Commit Loss: 0.001405 | Perplexity: 1535.063826
Trainning Epoch:  18%|█▊        | 61/330 [5:53:36<24:49:52, 332.32s/it]2025-09-26 20:06:41,521 Stage: Train 0.5 | Epoch: 61 | Iter: 92800 | Total Loss: 0.005341 | Recon Loss: 0.004638 | Commit Loss: 0.001406 | Perplexity: 1534.164688
2025-09-26 20:07:25,256 Stage: Train 0.5 | Epoch: 61 | Iter: 93000 | Total Loss: 0.005526 | Recon Loss: 0.004820 | Commit Loss: 0.001413 | Perplexity: 1530.753365
2025-09-26 20:08:08,726 Stage: Train 0.5 | Epoch: 61 | Iter: 93200 | Total Loss: 0.005515 | Recon Loss: 0.004820 | Commit Loss: 0.001391 | Perplexity: 1531.523995
2025-09-26 20:08:52,332 Stage: Train 0.5 | Epoch: 61 | Iter: 93400 | Total Loss: 0.005298 | Recon Loss: 0.004595 | Commit Loss: 0.001407 | Perplexity: 1530.555898
2025-09-26 20:09:36,154 Stage: Train 0.5 | Epoch: 61 | Iter: 93600 | Total Loss: 0.005449 | Recon Loss: 0.004749 | Commit Loss: 0.001399 | Perplexity: 1526.867241
2025-09-26 20:10:19,769 Stage: Train 0.5 | Epoch: 61 | Iter: 93800 | Total Loss: 0.005204 | Recon Loss: 0.004503 | Commit Loss: 0.001402 | Perplexity: 1532.941481
2025-09-26 20:11:03,491 Stage: Train 0.5 | Epoch: 61 | Iter: 94000 | Total Loss: 0.005302 | Recon Loss: 0.004598 | Commit Loss: 0.001407 | Perplexity: 1528.214130
Trainning Epoch:  19%|█▉        | 62/330 [5:59:08<24:43:51, 332.21s/it]2025-09-26 20:11:47,495 Stage: Train 0.5 | Epoch: 62 | Iter: 94200 | Total Loss: 0.005419 | Recon Loss: 0.004711 | Commit Loss: 0.001416 | Perplexity: 1531.109006
2025-09-26 20:12:31,399 Stage: Train 0.5 | Epoch: 62 | Iter: 94400 | Total Loss: 0.005376 | Recon Loss: 0.004674 | Commit Loss: 0.001403 | Perplexity: 1533.468770
2025-09-26 20:13:14,789 Stage: Train 0.5 | Epoch: 62 | Iter: 94600 | Total Loss: 0.005269 | Recon Loss: 0.004573 | Commit Loss: 0.001392 | Perplexity: 1527.331863
2025-09-26 20:13:58,727 Stage: Train 0.5 | Epoch: 62 | Iter: 94800 | Total Loss: 0.005384 | Recon Loss: 0.004685 | Commit Loss: 0.001399 | Perplexity: 1530.626625
2025-09-26 20:14:42,237 Stage: Train 0.5 | Epoch: 62 | Iter: 95000 | Total Loss: 0.005242 | Recon Loss: 0.004544 | Commit Loss: 0.001397 | Perplexity: 1526.484564
2025-09-26 20:15:25,870 Stage: Train 0.5 | Epoch: 62 | Iter: 95200 | Total Loss: 0.005366 | Recon Loss: 0.004666 | Commit Loss: 0.001402 | Perplexity: 1528.145714
2025-09-26 20:16:09,669 Stage: Train 0.5 | Epoch: 62 | Iter: 95400 | Total Loss: 0.005408 | Recon Loss: 0.004705 | Commit Loss: 0.001405 | Perplexity: 1529.170603
2025-09-26 20:16:53,338 Stage: Train 0.5 | Epoch: 62 | Iter: 95600 | Total Loss: 0.005406 | Recon Loss: 0.004708 | Commit Loss: 0.001395 | Perplexity: 1531.977471
Trainning Epoch:  19%|█▉        | 63/330 [6:04:40<24:38:11, 332.18s/it]2025-09-26 20:17:37,258 Stage: Train 0.5 | Epoch: 63 | Iter: 95800 | Total Loss: 0.005163 | Recon Loss: 0.004464 | Commit Loss: 0.001398 | Perplexity: 1530.501392
2025-09-26 20:18:20,631 Stage: Train 0.5 | Epoch: 63 | Iter: 96000 | Total Loss: 0.005340 | Recon Loss: 0.004639 | Commit Loss: 0.001404 | Perplexity: 1529.275719
2025-09-26 20:19:04,639 Stage: Train 0.5 | Epoch: 63 | Iter: 96200 | Total Loss: 0.005342 | Recon Loss: 0.004638 | Commit Loss: 0.001407 | Perplexity: 1530.984216
2025-09-26 20:19:48,566 Stage: Train 0.5 | Epoch: 63 | Iter: 96400 | Total Loss: 0.005285 | Recon Loss: 0.004589 | Commit Loss: 0.001391 | Perplexity: 1529.269831
2025-09-26 20:20:32,504 Stage: Train 0.5 | Epoch: 63 | Iter: 96600 | Total Loss: 0.005162 | Recon Loss: 0.004457 | Commit Loss: 0.001409 | Perplexity: 1531.902626
2025-09-26 20:21:16,256 Stage: Train 0.5 | Epoch: 63 | Iter: 96800 | Total Loss: 0.005491 | Recon Loss: 0.004788 | Commit Loss: 0.001405 | Perplexity: 1531.675050
2025-09-26 20:22:00,076 Stage: Train 0.5 | Epoch: 63 | Iter: 97000 | Total Loss: 0.005430 | Recon Loss: 0.004738 | Commit Loss: 0.001383 | Perplexity: 1522.564177
2025-09-26 20:22:43,575 Stage: Train 0.5 | Epoch: 63 | Iter: 97200 | Total Loss: 0.005137 | Recon Loss: 0.004437 | Commit Loss: 0.001399 | Perplexity: 1532.467306
Trainning Epoch:  19%|█▉        | 64/330 [6:10:13<24:33:03, 332.27s/it]2025-09-26 20:23:27,320 Stage: Train 0.5 | Epoch: 64 | Iter: 97400 | Total Loss: 0.005161 | Recon Loss: 0.004458 | Commit Loss: 0.001406 | Perplexity: 1532.647623
2025-09-26 20:24:11,042 Stage: Train 0.5 | Epoch: 64 | Iter: 97600 | Total Loss: 0.005376 | Recon Loss: 0.004677 | Commit Loss: 0.001398 | Perplexity: 1527.234133
2025-09-26 20:24:54,614 Stage: Train 0.5 | Epoch: 64 | Iter: 97800 | Total Loss: 0.005259 | Recon Loss: 0.004559 | Commit Loss: 0.001400 | Perplexity: 1525.352031
2025-09-26 20:25:38,529 Stage: Train 0.5 | Epoch: 64 | Iter: 98000 | Total Loss: 0.005422 | Recon Loss: 0.004719 | Commit Loss: 0.001407 | Perplexity: 1525.663732
2025-09-26 20:26:22,283 Stage: Train 0.5 | Epoch: 64 | Iter: 98200 | Total Loss: 0.005153 | Recon Loss: 0.004455 | Commit Loss: 0.001396 | Perplexity: 1524.673654
2025-09-26 20:27:06,367 Stage: Train 0.5 | Epoch: 64 | Iter: 98400 | Total Loss: 0.005361 | Recon Loss: 0.004671 | Commit Loss: 0.001381 | Perplexity: 1523.281733
2025-09-26 20:27:50,234 Stage: Train 0.5 | Epoch: 64 | Iter: 98600 | Total Loss: 0.005319 | Recon Loss: 0.004619 | Commit Loss: 0.001400 | Perplexity: 1527.295837
Trainning Epoch:  20%|█▉        | 65/330 [6:15:45<24:27:43, 332.32s/it]2025-09-26 20:28:33,848 Stage: Train 0.5 | Epoch: 65 | Iter: 98800 | Total Loss: 0.005209 | Recon Loss: 0.004514 | Commit Loss: 0.001389 | Perplexity: 1525.028633
2025-09-26 20:29:17,511 Stage: Train 0.5 | Epoch: 65 | Iter: 99000 | Total Loss: 0.005365 | Recon Loss: 0.004668 | Commit Loss: 0.001393 | Perplexity: 1528.376065
2025-09-26 20:30:01,553 Stage: Train 0.5 | Epoch: 65 | Iter: 99200 | Total Loss: 0.005257 | Recon Loss: 0.004556 | Commit Loss: 0.001402 | Perplexity: 1532.316135
2025-09-26 20:30:45,524 Stage: Train 0.5 | Epoch: 65 | Iter: 99400 | Total Loss: 0.005101 | Recon Loss: 0.004406 | Commit Loss: 0.001390 | Perplexity: 1525.764183
2025-09-26 20:31:28,995 Stage: Train 0.5 | Epoch: 65 | Iter: 99600 | Total Loss: 0.005377 | Recon Loss: 0.004674 | Commit Loss: 0.001405 | Perplexity: 1531.697404
2025-09-26 20:32:12,860 Stage: Train 0.5 | Epoch: 65 | Iter: 99800 | Total Loss: 0.005198 | Recon Loss: 0.004501 | Commit Loss: 0.001394 | Perplexity: 1521.829686
2025-09-26 20:32:56,917 Stage: Train 0.5 | Epoch: 65 | Iter: 100000 | Total Loss: 0.005307 | Recon Loss: 0.004612 | Commit Loss: 0.001390 | Perplexity: 1526.642037
2025-09-26 20:32:56,917 Saving model at iteration 100000
2025-09-26 20:32:57,160 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000
2025-09-26 20:32:57,451 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/model.safetensors
2025-09-26 20:32:57,898 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/optimizer.bin
2025-09-26 20:32:57,899 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/scheduler.bin
2025-09-26 20:32:57,899 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/sampler.bin
2025-09-26 20:32:57,900 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/random_states_0.pkl
2025-09-26 20:33:41,941 Stage: Train 0.5 | Epoch: 65 | Iter: 100200 | Total Loss: 0.005194 | Recon Loss: 0.004496 | Commit Loss: 0.001395 | Perplexity: 1524.968042
Trainning Epoch:  20%|██        | 66/330 [6:21:20<24:24:53, 332.93s/it]2025-09-26 20:34:26,061 Stage: Train 0.5 | Epoch: 66 | Iter: 100400 | Total Loss: 0.005237 | Recon Loss: 0.004541 | Commit Loss: 0.001392 | Perplexity: 1523.737416
2025-09-26 20:35:09,391 Stage: Train 0.5 | Epoch: 66 | Iter: 100600 | Total Loss: 0.005348 | Recon Loss: 0.004647 | Commit Loss: 0.001401 | Perplexity: 1530.374042
2025-09-26 20:35:53,123 Stage: Train 0.5 | Epoch: 66 | Iter: 100800 | Total Loss: 0.005290 | Recon Loss: 0.004595 | Commit Loss: 0.001389 | Perplexity: 1525.561799
2025-09-26 20:36:36,924 Stage: Train 0.5 | Epoch: 66 | Iter: 101000 | Total Loss: 0.005240 | Recon Loss: 0.004546 | Commit Loss: 0.001388 | Perplexity: 1527.576961
2025-09-26 20:37:20,960 Stage: Train 0.5 | Epoch: 66 | Iter: 101200 | Total Loss: 0.005181 | Recon Loss: 0.004487 | Commit Loss: 0.001388 | Perplexity: 1526.671391
2025-09-26 20:38:04,907 Stage: Train 0.5 | Epoch: 66 | Iter: 101400 | Total Loss: 0.005176 | Recon Loss: 0.004476 | Commit Loss: 0.001400 | Perplexity: 1528.643569
2025-09-26 20:38:48,430 Stage: Train 0.5 | Epoch: 66 | Iter: 101600 | Total Loss: 0.005301 | Recon Loss: 0.004604 | Commit Loss: 0.001394 | Perplexity: 1520.640552
Trainning Epoch:  20%|██        | 67/330 [6:26:52<24:18:49, 332.81s/it]2025-09-26 20:39:32,540 Stage: Train 0.5 | Epoch: 67 | Iter: 101800 | Total Loss: 0.005151 | Recon Loss: 0.004460 | Commit Loss: 0.001382 | Perplexity: 1518.616028
2025-09-26 20:40:16,316 Stage: Train 0.5 | Epoch: 67 | Iter: 102000 | Total Loss: 0.005253 | Recon Loss: 0.004558 | Commit Loss: 0.001390 | Perplexity: 1526.591062
2025-09-26 20:40:59,977 Stage: Train 0.5 | Epoch: 67 | Iter: 102200 | Total Loss: 0.005270 | Recon Loss: 0.004575 | Commit Loss: 0.001390 | Perplexity: 1523.453387
2025-09-26 20:41:43,676 Stage: Train 0.5 | Epoch: 67 | Iter: 102400 | Total Loss: 0.005335 | Recon Loss: 0.004648 | Commit Loss: 0.001374 | Perplexity: 1522.820720
2025-09-26 20:42:27,437 Stage: Train 0.5 | Epoch: 67 | Iter: 102600 | Total Loss: 0.005089 | Recon Loss: 0.004393 | Commit Loss: 0.001393 | Perplexity: 1526.515600
2025-09-26 20:43:10,996 Stage: Train 0.5 | Epoch: 67 | Iter: 102800 | Total Loss: 0.005289 | Recon Loss: 0.004594 | Commit Loss: 0.001390 | Perplexity: 1526.295128
2025-09-26 20:43:54,795 Stage: Train 0.5 | Epoch: 67 | Iter: 103000 | Total Loss: 0.005328 | Recon Loss: 0.004630 | Commit Loss: 0.001397 | Perplexity: 1522.709950
2025-09-26 20:44:38,421 Stage: Train 0.5 | Epoch: 67 | Iter: 103200 | Total Loss: 0.005074 | Recon Loss: 0.004370 | Commit Loss: 0.001408 | Perplexity: 1527.626218
Trainning Epoch:  21%|██        | 68/330 [6:32:24<24:12:03, 332.53s/it]2025-09-26 20:45:22,082 Stage: Train 0.5 | Epoch: 68 | Iter: 103400 | Total Loss: 0.005282 | Recon Loss: 0.004584 | Commit Loss: 0.001396 | Perplexity: 1519.429315
2025-09-26 20:46:05,946 Stage: Train 0.5 | Epoch: 68 | Iter: 103600 | Total Loss: 0.005112 | Recon Loss: 0.004419 | Commit Loss: 0.001385 | Perplexity: 1526.539836
2025-09-26 20:46:49,729 Stage: Train 0.5 | Epoch: 68 | Iter: 103800 | Total Loss: 0.005236 | Recon Loss: 0.004544 | Commit Loss: 0.001383 | Perplexity: 1520.249844
2025-09-26 20:47:33,456 Stage: Train 0.5 | Epoch: 68 | Iter: 104000 | Total Loss: 0.005062 | Recon Loss: 0.004372 | Commit Loss: 0.001381 | Perplexity: 1521.403207
2025-09-26 20:48:17,224 Stage: Train 0.5 | Epoch: 68 | Iter: 104200 | Total Loss: 0.005336 | Recon Loss: 0.004638 | Commit Loss: 0.001396 | Perplexity: 1524.997833
2025-09-26 20:49:00,682 Stage: Train 0.5 | Epoch: 68 | Iter: 104400 | Total Loss: 0.005239 | Recon Loss: 0.004547 | Commit Loss: 0.001385 | Perplexity: 1524.360828
2025-09-26 20:49:44,426 Stage: Train 0.5 | Epoch: 68 | Iter: 104600 | Total Loss: 0.005176 | Recon Loss: 0.004477 | Commit Loss: 0.001399 | Perplexity: 1528.810121
2025-09-26 20:50:27,970 Stage: Train 0.5 | Epoch: 68 | Iter: 104800 | Total Loss: 0.005208 | Recon Loss: 0.004507 | Commit Loss: 0.001403 | Perplexity: 1531.065065
Trainning Epoch:  21%|██        | 69/330 [6:37:56<24:05:57, 332.40s/it]2025-09-26 20:51:10,288 Stage: Train 0.5 | Epoch: 69 | Iter: 105000 | Total Loss: 0.005141 | Recon Loss: 0.004445 | Commit Loss: 0.001391 | Perplexity: 1524.900041
2025-09-26 20:51:53,268 Stage: Train 0.5 | Epoch: 69 | Iter: 105200 | Total Loss: 0.005175 | Recon Loss: 0.004477 | Commit Loss: 0.001396 | Perplexity: 1527.118613
2025-09-26 20:52:37,030 Stage: Train 0.5 | Epoch: 69 | Iter: 105400 | Total Loss: 0.005050 | Recon Loss: 0.004356 | Commit Loss: 0.001388 | Perplexity: 1527.212935
2025-09-26 20:53:21,026 Stage: Train 0.5 | Epoch: 69 | Iter: 105600 | Total Loss: 0.005302 | Recon Loss: 0.004610 | Commit Loss: 0.001385 | Perplexity: 1523.935388
2025-09-26 20:54:04,797 Stage: Train 0.5 | Epoch: 69 | Iter: 105800 | Total Loss: 0.005174 | Recon Loss: 0.004480 | Commit Loss: 0.001388 | Perplexity: 1521.277937
2025-09-26 20:54:48,613 Stage: Train 0.5 | Epoch: 69 | Iter: 106000 | Total Loss: 0.005154 | Recon Loss: 0.004458 | Commit Loss: 0.001393 | Perplexity: 1520.358053
2025-09-26 20:55:32,238 Stage: Train 0.5 | Epoch: 69 | Iter: 106200 | Total Loss: 0.005203 | Recon Loss: 0.004517 | Commit Loss: 0.001372 | Perplexity: 1521.143092
Trainning Epoch:  21%|██        | 70/330 [6:43:26<23:57:38, 331.76s/it]2025-09-26 20:56:16,021 Stage: Train 0.5 | Epoch: 70 | Iter: 106400 | Total Loss: 0.005131 | Recon Loss: 0.004439 | Commit Loss: 0.001384 | Perplexity: 1521.415536
2025-09-26 20:56:59,820 Stage: Train 0.5 | Epoch: 70 | Iter: 106600 | Total Loss: 0.005067 | Recon Loss: 0.004375 | Commit Loss: 0.001384 | Perplexity: 1522.104214
2025-09-26 20:57:43,575 Stage: Train 0.5 | Epoch: 70 | Iter: 106800 | Total Loss: 0.005176 | Recon Loss: 0.004481 | Commit Loss: 0.001391 | Perplexity: 1525.819301
2025-09-26 20:58:27,470 Stage: Train 0.5 | Epoch: 70 | Iter: 107000 | Total Loss: 0.005098 | Recon Loss: 0.004407 | Commit Loss: 0.001382 | Perplexity: 1525.958883
2025-09-26 20:59:11,005 Stage: Train 0.5 | Epoch: 70 | Iter: 107200 | Total Loss: 0.005150 | Recon Loss: 0.004451 | Commit Loss: 0.001396 | Perplexity: 1525.800655
2025-09-26 20:59:54,938 Stage: Train 0.5 | Epoch: 70 | Iter: 107400 | Total Loss: 0.005133 | Recon Loss: 0.004441 | Commit Loss: 0.001384 | Perplexity: 1520.514825
2025-09-26 21:00:38,812 Stage: Train 0.5 | Epoch: 70 | Iter: 107600 | Total Loss: 0.005197 | Recon Loss: 0.004505 | Commit Loss: 0.001383 | Perplexity: 1524.506359
2025-09-26 21:01:22,643 Stage: Train 0.5 | Epoch: 70 | Iter: 107800 | Total Loss: 0.005064 | Recon Loss: 0.004369 | Commit Loss: 0.001390 | Perplexity: 1526.494019
Trainning Epoch:  22%|██▏       | 71/330 [6:48:59<23:53:17, 332.03s/it]2025-09-26 21:02:06,402 Stage: Train 0.5 | Epoch: 71 | Iter: 108000 | Total Loss: 0.005252 | Recon Loss: 0.004558 | Commit Loss: 0.001388 | Perplexity: 1522.878010
2025-09-26 21:02:50,356 Stage: Train 0.5 | Epoch: 71 | Iter: 108200 | Total Loss: 0.005074 | Recon Loss: 0.004387 | Commit Loss: 0.001375 | Perplexity: 1523.046207
2025-09-26 21:03:34,336 Stage: Train 0.5 | Epoch: 71 | Iter: 108400 | Total Loss: 0.005118 | Recon Loss: 0.004430 | Commit Loss: 0.001376 | Perplexity: 1524.988682
2025-09-26 21:04:18,318 Stage: Train 0.5 | Epoch: 71 | Iter: 108600 | Total Loss: 0.005315 | Recon Loss: 0.004623 | Commit Loss: 0.001384 | Perplexity: 1525.882690
2025-09-26 21:05:02,285 Stage: Train 0.5 | Epoch: 71 | Iter: 108800 | Total Loss: 0.005093 | Recon Loss: 0.004404 | Commit Loss: 0.001379 | Perplexity: 1519.719776
2025-09-26 21:05:45,742 Stage: Train 0.5 | Epoch: 71 | Iter: 109000 | Total Loss: 0.005126 | Recon Loss: 0.004432 | Commit Loss: 0.001388 | Perplexity: 1523.025074
2025-09-26 21:06:29,704 Stage: Train 0.5 | Epoch: 71 | Iter: 109200 | Total Loss: 0.005104 | Recon Loss: 0.004409 | Commit Loss: 0.001389 | Perplexity: 1524.980750
Trainning Epoch:  22%|██▏       | 72/330 [6:54:32<23:49:07, 332.35s/it]2025-09-26 21:07:13,605 Stage: Train 0.5 | Epoch: 72 | Iter: 109400 | Total Loss: 0.005021 | Recon Loss: 0.004327 | Commit Loss: 0.001387 | Perplexity: 1525.440980
2025-09-26 21:07:57,565 Stage: Train 0.5 | Epoch: 72 | Iter: 109600 | Total Loss: 0.005181 | Recon Loss: 0.004490 | Commit Loss: 0.001383 | Perplexity: 1520.971462
2025-09-26 21:08:41,231 Stage: Train 0.5 | Epoch: 72 | Iter: 109800 | Total Loss: 0.005136 | Recon Loss: 0.004445 | Commit Loss: 0.001381 | Perplexity: 1519.737177
2025-09-26 21:09:24,716 Stage: Train 0.5 | Epoch: 72 | Iter: 110000 | Total Loss: 0.004990 | Recon Loss: 0.004300 | Commit Loss: 0.001382 | Perplexity: 1524.446582
2025-09-26 21:10:08,542 Stage: Train 0.5 | Epoch: 72 | Iter: 110200 | Total Loss: 0.005211 | Recon Loss: 0.004526 | Commit Loss: 0.001369 | Perplexity: 1519.809619
2025-09-26 21:10:52,286 Stage: Train 0.5 | Epoch: 72 | Iter: 110400 | Total Loss: 0.005119 | Recon Loss: 0.004428 | Commit Loss: 0.001384 | Perplexity: 1521.627778
2025-09-26 21:11:36,011 Stage: Train 0.5 | Epoch: 72 | Iter: 110600 | Total Loss: 0.004961 | Recon Loss: 0.004264 | Commit Loss: 0.001393 | Perplexity: 1526.033545
2025-09-26 21:12:19,561 Stage: Train 0.5 | Epoch: 72 | Iter: 110800 | Total Loss: 0.005025 | Recon Loss: 0.004333 | Commit Loss: 0.001385 | Perplexity: 1521.527593
Trainning Epoch:  22%|██▏       | 73/330 [7:00:04<23:43:16, 332.28s/it]2025-09-26 21:13:03,315 Stage: Train 0.5 | Epoch: 73 | Iter: 111000 | Total Loss: 0.005026 | Recon Loss: 0.004334 | Commit Loss: 0.001385 | Perplexity: 1527.032326
2025-09-26 21:13:47,154 Stage: Train 0.5 | Epoch: 73 | Iter: 111200 | Total Loss: 0.005204 | Recon Loss: 0.004513 | Commit Loss: 0.001382 | Perplexity: 1520.744303
2025-09-26 21:14:30,859 Stage: Train 0.5 | Epoch: 73 | Iter: 111400 | Total Loss: 0.005083 | Recon Loss: 0.004396 | Commit Loss: 0.001374 | Perplexity: 1519.434039
2025-09-26 21:15:14,229 Stage: Train 0.5 | Epoch: 73 | Iter: 111600 | Total Loss: 0.005121 | Recon Loss: 0.004436 | Commit Loss: 0.001370 | Perplexity: 1517.264438
2025-09-26 21:15:57,848 Stage: Train 0.5 | Epoch: 73 | Iter: 111800 | Total Loss: 0.005059 | Recon Loss: 0.004377 | Commit Loss: 0.001365 | Perplexity: 1522.377193
2025-09-26 21:16:41,656 Stage: Train 0.5 | Epoch: 73 | Iter: 112000 | Total Loss: 0.005150 | Recon Loss: 0.004460 | Commit Loss: 0.001379 | Perplexity: 1518.897555
2025-09-26 21:17:25,498 Stage: Train 0.5 | Epoch: 73 | Iter: 112200 | Total Loss: 0.005206 | Recon Loss: 0.004514 | Commit Loss: 0.001386 | Perplexity: 1518.740069
2025-09-26 21:18:09,211 Stage: Train 0.5 | Epoch: 73 | Iter: 112400 | Total Loss: 0.005098 | Recon Loss: 0.004409 | Commit Loss: 0.001378 | Perplexity: 1526.773340
Trainning Epoch:  22%|██▏       | 74/330 [7:05:36<23:37:25, 332.21s/it]2025-09-26 21:18:53,189 Stage: Train 0.5 | Epoch: 74 | Iter: 112600 | Total Loss: 0.005014 | Recon Loss: 0.004325 | Commit Loss: 0.001377 | Perplexity: 1520.912136
2025-09-26 21:19:36,733 Stage: Train 0.5 | Epoch: 74 | Iter: 112800 | Total Loss: 0.004992 | Recon Loss: 0.004297 | Commit Loss: 0.001389 | Perplexity: 1525.634803
2025-09-26 21:20:20,326 Stage: Train 0.5 | Epoch: 74 | Iter: 113000 | Total Loss: 0.005053 | Recon Loss: 0.004366 | Commit Loss: 0.001374 | Perplexity: 1515.212734
2025-09-26 21:21:04,129 Stage: Train 0.5 | Epoch: 74 | Iter: 113200 | Total Loss: 0.005139 | Recon Loss: 0.004453 | Commit Loss: 0.001372 | Perplexity: 1520.846642
2025-09-26 21:21:47,816 Stage: Train 0.5 | Epoch: 74 | Iter: 113400 | Total Loss: 0.004995 | Recon Loss: 0.004309 | Commit Loss: 0.001373 | Perplexity: 1516.749068
2025-09-26 21:22:31,229 Stage: Train 0.5 | Epoch: 74 | Iter: 113600 | Total Loss: 0.005078 | Recon Loss: 0.004389 | Commit Loss: 0.001378 | Perplexity: 1522.614930
2025-09-26 21:23:14,901 Stage: Train 0.5 | Epoch: 74 | Iter: 113800 | Total Loss: 0.005049 | Recon Loss: 0.004361 | Commit Loss: 0.001376 | Perplexity: 1517.190286
Trainning Epoch:  23%|██▎       | 75/330 [7:11:08<23:31:29, 332.11s/it]2025-09-26 21:23:59,108 Stage: Train 0.5 | Epoch: 75 | Iter: 114000 | Total Loss: 0.005156 | Recon Loss: 0.004463 | Commit Loss: 0.001386 | Perplexity: 1521.565271
2025-09-26 21:24:43,078 Stage: Train 0.5 | Epoch: 75 | Iter: 114200 | Total Loss: 0.005094 | Recon Loss: 0.004404 | Commit Loss: 0.001379 | Perplexity: 1518.330795
2025-09-26 21:25:26,987 Stage: Train 0.5 | Epoch: 75 | Iter: 114400 | Total Loss: 0.005139 | Recon Loss: 0.004453 | Commit Loss: 0.001370 | Perplexity: 1515.945682
2025-09-26 21:26:10,541 Stage: Train 0.5 | Epoch: 75 | Iter: 114600 | Total Loss: 0.005391 | Recon Loss: 0.004704 | Commit Loss: 0.001373 | Perplexity: 1516.330367
2025-09-26 21:26:54,305 Stage: Train 0.5 | Epoch: 75 | Iter: 114800 | Total Loss: 0.004906 | Recon Loss: 0.004222 | Commit Loss: 0.001368 | Perplexity: 1520.508149
2025-09-26 21:27:38,077 Stage: Train 0.5 | Epoch: 75 | Iter: 115000 | Total Loss: 0.005112 | Recon Loss: 0.004428 | Commit Loss: 0.001366 | Perplexity: 1519.206659
2025-09-26 21:28:21,746 Stage: Train 0.5 | Epoch: 75 | Iter: 115200 | Total Loss: 0.004988 | Recon Loss: 0.004308 | Commit Loss: 0.001361 | Perplexity: 1515.648672
2025-09-26 21:29:05,447 Stage: Train 0.5 | Epoch: 75 | Iter: 115400 | Total Loss: 0.005137 | Recon Loss: 0.004451 | Commit Loss: 0.001373 | Perplexity: 1520.116741
Trainning Epoch:  23%|██▎       | 76/330 [7:16:40<23:26:16, 332.19s/it]2025-09-26 21:29:49,167 Stage: Train 0.5 | Epoch: 76 | Iter: 115600 | Total Loss: 0.005004 | Recon Loss: 0.004319 | Commit Loss: 0.001371 | Perplexity: 1515.877231
2025-09-26 21:30:32,816 Stage: Train 0.5 | Epoch: 76 | Iter: 115800 | Total Loss: 0.005122 | Recon Loss: 0.004435 | Commit Loss: 0.001374 | Perplexity: 1522.851674
2025-09-26 21:31:16,632 Stage: Train 0.5 | Epoch: 76 | Iter: 116000 | Total Loss: 0.004916 | Recon Loss: 0.004233 | Commit Loss: 0.001366 | Perplexity: 1520.200423
2025-09-26 21:32:00,389 Stage: Train 0.5 | Epoch: 76 | Iter: 116200 | Total Loss: 0.005085 | Recon Loss: 0.004403 | Commit Loss: 0.001363 | Perplexity: 1517.702095
2025-09-26 21:32:43,765 Stage: Train 0.5 | Epoch: 76 | Iter: 116400 | Total Loss: 0.004948 | Recon Loss: 0.004266 | Commit Loss: 0.001363 | Perplexity: 1520.598818
2025-09-26 21:33:27,381 Stage: Train 0.5 | Epoch: 76 | Iter: 116600 | Total Loss: 0.005076 | Recon Loss: 0.004392 | Commit Loss: 0.001368 | Perplexity: 1519.875075
2025-09-26 21:34:11,119 Stage: Train 0.5 | Epoch: 76 | Iter: 116800 | Total Loss: 0.004958 | Recon Loss: 0.004266 | Commit Loss: 0.001384 | Perplexity: 1523.540594
Trainning Epoch:  23%|██▎       | 77/330 [7:22:13<23:20:34, 332.15s/it]2025-09-26 21:34:55,192 Stage: Train 0.5 | Epoch: 77 | Iter: 117000 | Total Loss: 0.005069 | Recon Loss: 0.004379 | Commit Loss: 0.001380 | Perplexity: 1518.881517
2025-09-26 21:35:37,566 Stage: Train 0.5 | Epoch: 77 | Iter: 117200 | Total Loss: 0.005029 | Recon Loss: 0.004350 | Commit Loss: 0.001360 | Perplexity: 1516.359928
2025-09-26 21:36:20,785 Stage: Train 0.5 | Epoch: 77 | Iter: 117400 | Total Loss: 0.004926 | Recon Loss: 0.004238 | Commit Loss: 0.001376 | Perplexity: 1519.118384
2025-09-26 21:37:04,824 Stage: Train 0.5 | Epoch: 77 | Iter: 117600 | Total Loss: 0.005075 | Recon Loss: 0.004395 | Commit Loss: 0.001359 | Perplexity: 1516.030396
2025-09-26 21:37:48,860 Stage: Train 0.5 | Epoch: 77 | Iter: 117800 | Total Loss: 0.005066 | Recon Loss: 0.004381 | Commit Loss: 0.001370 | Perplexity: 1519.052355
2025-09-26 21:38:32,766 Stage: Train 0.5 | Epoch: 77 | Iter: 118000 | Total Loss: 0.004870 | Recon Loss: 0.004188 | Commit Loss: 0.001365 | Perplexity: 1518.005560
2025-09-26 21:39:16,660 Stage: Train 0.5 | Epoch: 77 | Iter: 118200 | Total Loss: 0.005087 | Recon Loss: 0.004399 | Commit Loss: 0.001376 | Perplexity: 1521.093215
2025-09-26 21:40:00,413 Stage: Train 0.5 | Epoch: 77 | Iter: 118400 | Total Loss: 0.004938 | Recon Loss: 0.004248 | Commit Loss: 0.001380 | Perplexity: 1523.109604
Trainning Epoch:  24%|██▎       | 78/330 [7:27:44<23:14:10, 331.95s/it]2025-09-26 21:40:44,439 Stage: Train 0.5 | Epoch: 78 | Iter: 118600 | Total Loss: 0.005113 | Recon Loss: 0.004430 | Commit Loss: 0.001366 | Perplexity: 1521.188991
2025-09-26 21:41:28,193 Stage: Train 0.5 | Epoch: 78 | Iter: 118800 | Total Loss: 0.004883 | Recon Loss: 0.004196 | Commit Loss: 0.001373 | Perplexity: 1522.799097
2025-09-26 21:42:11,777 Stage: Train 0.5 | Epoch: 78 | Iter: 119000 | Total Loss: 0.005193 | Recon Loss: 0.004510 | Commit Loss: 0.001365 | Perplexity: 1518.591620
2025-09-26 21:42:55,172 Stage: Train 0.5 | Epoch: 78 | Iter: 119200 | Total Loss: 0.004850 | Recon Loss: 0.004173 | Commit Loss: 0.001354 | Perplexity: 1515.575370
2025-09-26 21:43:38,995 Stage: Train 0.5 | Epoch: 78 | Iter: 119400 | Total Loss: 0.004995 | Recon Loss: 0.004308 | Commit Loss: 0.001375 | Perplexity: 1516.257875
2025-09-26 21:44:22,691 Stage: Train 0.5 | Epoch: 78 | Iter: 119600 | Total Loss: 0.004937 | Recon Loss: 0.004252 | Commit Loss: 0.001369 | Perplexity: 1517.263032
2025-09-26 21:45:06,437 Stage: Train 0.5 | Epoch: 78 | Iter: 119800 | Total Loss: 0.005000 | Recon Loss: 0.004315 | Commit Loss: 0.001371 | Perplexity: 1520.493872
2025-09-26 21:45:50,116 Stage: Train 0.5 | Epoch: 78 | Iter: 120000 | Total Loss: 0.004955 | Recon Loss: 0.004271 | Commit Loss: 0.001369 | Perplexity: 1520.256448
2025-09-26 21:45:50,116 Saving model at iteration 120000
2025-09-26 21:45:50,655 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000
2025-09-26 21:45:50,954 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/model.safetensors
2025-09-26 21:45:51,345 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/optimizer.bin
2025-09-26 21:45:51,345 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/scheduler.bin
2025-09-26 21:45:51,345 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/sampler.bin
2025-09-26 21:45:51,346 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/random_states_0.pkl
Trainning Epoch:  24%|██▍       | 79/330 [7:33:17<23:10:33, 332.40s/it]2025-09-26 21:46:35,324 Stage: Train 0.5 | Epoch: 79 | Iter: 120200 | Total Loss: 0.005044 | Recon Loss: 0.004358 | Commit Loss: 0.001373 | Perplexity: 1517.935823
2025-09-26 21:47:18,866 Stage: Train 0.5 | Epoch: 79 | Iter: 120400 | Total Loss: 0.004942 | Recon Loss: 0.004264 | Commit Loss: 0.001355 | Perplexity: 1514.601835
2025-09-26 21:48:02,524 Stage: Train 0.5 | Epoch: 79 | Iter: 120600 | Total Loss: 0.004917 | Recon Loss: 0.004232 | Commit Loss: 0.001369 | Perplexity: 1514.204026
2025-09-26 21:48:45,960 Stage: Train 0.5 | Epoch: 79 | Iter: 120800 | Total Loss: 0.005038 | Recon Loss: 0.004358 | Commit Loss: 0.001361 | Perplexity: 1513.639044
2025-09-26 21:49:29,600 Stage: Train 0.5 | Epoch: 79 | Iter: 121000 | Total Loss: 0.004825 | Recon Loss: 0.004144 | Commit Loss: 0.001363 | Perplexity: 1519.480504
2025-09-26 21:50:13,039 Stage: Train 0.5 | Epoch: 79 | Iter: 121200 | Total Loss: 0.005001 | Recon Loss: 0.004322 | Commit Loss: 0.001358 | Perplexity: 1522.431693
2025-09-26 21:50:57,041 Stage: Train 0.5 | Epoch: 79 | Iter: 121400 | Total Loss: 0.005079 | Recon Loss: 0.004401 | Commit Loss: 0.001358 | Perplexity: 1514.926650
Trainning Epoch:  24%|██▍       | 80/330 [7:38:49<23:03:55, 332.14s/it]2025-09-26 21:51:40,953 Stage: Train 0.5 | Epoch: 80 | Iter: 121600 | Total Loss: 0.004875 | Recon Loss: 0.004190 | Commit Loss: 0.001370 | Perplexity: 1522.287063
2025-09-26 21:52:24,673 Stage: Train 0.5 | Epoch: 80 | Iter: 121800 | Total Loss: 0.005029 | Recon Loss: 0.004355 | Commit Loss: 0.001350 | Perplexity: 1514.031818
2025-09-26 21:53:08,101 Stage: Train 0.5 | Epoch: 80 | Iter: 122000 | Total Loss: 0.004916 | Recon Loss: 0.004228 | Commit Loss: 0.001376 | Perplexity: 1518.623036
2025-09-26 21:53:51,865 Stage: Train 0.5 | Epoch: 80 | Iter: 122200 | Total Loss: 0.004898 | Recon Loss: 0.004211 | Commit Loss: 0.001375 | Perplexity: 1524.044553
2025-09-26 21:54:35,600 Stage: Train 0.5 | Epoch: 80 | Iter: 122400 | Total Loss: 0.005042 | Recon Loss: 0.004366 | Commit Loss: 0.001353 | Perplexity: 1512.847126
2025-09-26 21:55:19,680 Stage: Train 0.5 | Epoch: 80 | Iter: 122600 | Total Loss: 0.004985 | Recon Loss: 0.004304 | Commit Loss: 0.001362 | Perplexity: 1522.085122
2025-09-26 21:56:03,860 Stage: Train 0.5 | Epoch: 80 | Iter: 122800 | Total Loss: 0.004843 | Recon Loss: 0.004161 | Commit Loss: 0.001364 | Perplexity: 1514.894084
2025-09-26 21:56:47,008 Stage: Train 0.5 | Epoch: 80 | Iter: 123000 | Total Loss: 0.004835 | Recon Loss: 0.004158 | Commit Loss: 0.001354 | Perplexity: 1517.954813
Trainning Epoch:  25%|██▍       | 81/330 [7:44:21<22:58:35, 332.19s/it]2025-09-26 21:57:31,293 Stage: Train 0.5 | Epoch: 81 | Iter: 123200 | Total Loss: 0.004974 | Recon Loss: 0.004295 | Commit Loss: 0.001358 | Perplexity: 1514.254966
2025-09-26 21:58:15,098 Stage: Train 0.5 | Epoch: 81 | Iter: 123400 | Total Loss: 0.004900 | Recon Loss: 0.004222 | Commit Loss: 0.001356 | Perplexity: 1516.753085
2025-09-26 21:58:58,810 Stage: Train 0.5 | Epoch: 81 | Iter: 123600 | Total Loss: 0.004943 | Recon Loss: 0.004268 | Commit Loss: 0.001348 | Perplexity: 1513.677794
2025-09-26 21:59:42,619 Stage: Train 0.5 | Epoch: 81 | Iter: 123800 | Total Loss: 0.004824 | Recon Loss: 0.004144 | Commit Loss: 0.001360 | Perplexity: 1517.296564
2025-09-26 22:00:26,225 Stage: Train 0.5 | Epoch: 81 | Iter: 124000 | Total Loss: 0.005007 | Recon Loss: 0.004329 | Commit Loss: 0.001357 | Perplexity: 1514.687833
2025-09-26 22:01:10,157 Stage: Train 0.5 | Epoch: 81 | Iter: 124200 | Total Loss: 0.004879 | Recon Loss: 0.004199 | Commit Loss: 0.001359 | Perplexity: 1517.580225
2025-09-26 22:01:53,977 Stage: Train 0.5 | Epoch: 81 | Iter: 124400 | Total Loss: 0.005054 | Recon Loss: 0.004374 | Commit Loss: 0.001359 | Perplexity: 1512.568731
Trainning Epoch:  25%|██▍       | 82/330 [7:49:54<22:54:09, 332.46s/it]2025-09-26 22:02:38,130 Stage: Train 0.5 | Epoch: 82 | Iter: 124600 | Total Loss: 0.004915 | Recon Loss: 0.004238 | Commit Loss: 0.001355 | Perplexity: 1517.866883
2025-09-26 22:03:21,683 Stage: Train 0.5 | Epoch: 82 | Iter: 124800 | Total Loss: 0.004903 | Recon Loss: 0.004225 | Commit Loss: 0.001357 | Perplexity: 1516.620543
2025-09-26 22:04:05,422 Stage: Train 0.5 | Epoch: 82 | Iter: 125000 | Total Loss: 0.004927 | Recon Loss: 0.004257 | Commit Loss: 0.001342 | Perplexity: 1510.293795
2025-09-26 22:04:49,176 Stage: Train 0.5 | Epoch: 82 | Iter: 125200 | Total Loss: 0.004906 | Recon Loss: 0.004229 | Commit Loss: 0.001353 | Perplexity: 1517.276437
2025-09-26 22:05:32,939 Stage: Train 0.5 | Epoch: 82 | Iter: 125400 | Total Loss: 0.004905 | Recon Loss: 0.004230 | Commit Loss: 0.001349 | Perplexity: 1518.973303
2025-09-26 22:06:16,620 Stage: Train 0.5 | Epoch: 82 | Iter: 125600 | Total Loss: 0.004849 | Recon Loss: 0.004173 | Commit Loss: 0.001352 | Perplexity: 1522.256165
2025-09-26 22:07:00,086 Stage: Train 0.5 | Epoch: 82 | Iter: 125800 | Total Loss: 0.004920 | Recon Loss: 0.004240 | Commit Loss: 0.001360 | Perplexity: 1517.325497
2025-09-26 22:07:44,073 Stage: Train 0.5 | Epoch: 82 | Iter: 126000 | Total Loss: 0.005075 | Recon Loss: 0.004391 | Commit Loss: 0.001367 | Perplexity: 1516.535220
Trainning Epoch:  25%|██▌       | 83/330 [7:55:27<22:48:19, 332.39s/it]2025-09-26 22:08:28,225 Stage: Train 0.5 | Epoch: 83 | Iter: 126200 | Total Loss: 0.004877 | Recon Loss: 0.004203 | Commit Loss: 0.001349 | Perplexity: 1516.700002
2025-09-26 22:09:12,178 Stage: Train 0.5 | Epoch: 83 | Iter: 126400 | Total Loss: 0.004982 | Recon Loss: 0.004305 | Commit Loss: 0.001354 | Perplexity: 1516.666700
2025-09-26 22:09:55,940 Stage: Train 0.5 | Epoch: 83 | Iter: 126600 | Total Loss: 0.004830 | Recon Loss: 0.004158 | Commit Loss: 0.001343 | Perplexity: 1512.793455
2025-09-26 22:10:39,313 Stage: Train 0.5 | Epoch: 83 | Iter: 126800 | Total Loss: 0.004965 | Recon Loss: 0.004288 | Commit Loss: 0.001353 | Perplexity: 1518.195674
2025-09-26 22:11:23,153 Stage: Train 0.5 | Epoch: 83 | Iter: 127000 | Total Loss: 0.004863 | Recon Loss: 0.004179 | Commit Loss: 0.001368 | Perplexity: 1516.656484
2025-09-26 22:12:07,161 Stage: Train 0.5 | Epoch: 83 | Iter: 127200 | Total Loss: 0.004862 | Recon Loss: 0.004182 | Commit Loss: 0.001360 | Perplexity: 1512.249789
2025-09-26 22:12:51,056 Stage: Train 0.5 | Epoch: 83 | Iter: 127400 | Total Loss: 0.004871 | Recon Loss: 0.004189 | Commit Loss: 0.001362 | Perplexity: 1523.424952
Trainning Epoch:  25%|██▌       | 84/330 [8:00:59<22:42:59, 332.44s/it]2025-09-26 22:13:34,600 Stage: Train 0.5 | Epoch: 84 | Iter: 127600 | Total Loss: 0.004904 | Recon Loss: 0.004225 | Commit Loss: 0.001357 | Perplexity: 1513.946022
2025-09-26 22:14:18,088 Stage: Train 0.5 | Epoch: 84 | Iter: 127800 | Total Loss: 0.004875 | Recon Loss: 0.004200 | Commit Loss: 0.001350 | Perplexity: 1516.929481
2025-09-26 22:15:01,854 Stage: Train 0.5 | Epoch: 84 | Iter: 128000 | Total Loss: 0.004868 | Recon Loss: 0.004199 | Commit Loss: 0.001338 | Perplexity: 1507.572120
2025-09-26 22:15:45,634 Stage: Train 0.5 | Epoch: 84 | Iter: 128200 | Total Loss: 0.004834 | Recon Loss: 0.004156 | Commit Loss: 0.001356 | Perplexity: 1520.346284
2025-09-26 22:16:29,321 Stage: Train 0.5 | Epoch: 84 | Iter: 128400 | Total Loss: 0.005033 | Recon Loss: 0.004355 | Commit Loss: 0.001355 | Perplexity: 1515.963925
2025-09-26 22:17:13,071 Stage: Train 0.5 | Epoch: 84 | Iter: 128600 | Total Loss: 0.004777 | Recon Loss: 0.004104 | Commit Loss: 0.001345 | Perplexity: 1514.033536
2025-09-26 22:17:56,778 Stage: Train 0.5 | Epoch: 84 | Iter: 128800 | Total Loss: 0.004937 | Recon Loss: 0.004259 | Commit Loss: 0.001356 | Perplexity: 1514.830765
2025-09-26 22:18:40,625 Stage: Train 0.5 | Epoch: 84 | Iter: 129000 | Total Loss: 0.004851 | Recon Loss: 0.004177 | Commit Loss: 0.001347 | Perplexity: 1516.616458
Trainning Epoch:  26%|██▌       | 85/330 [8:06:32<22:37:26, 332.43s/it]2025-09-26 22:19:24,859 Stage: Train 0.5 | Epoch: 85 | Iter: 129200 | Total Loss: 0.004817 | Recon Loss: 0.004139 | Commit Loss: 0.001357 | Perplexity: 1518.619524
2025-09-26 22:20:08,516 Stage: Train 0.5 | Epoch: 85 | Iter: 129400 | Total Loss: 0.004874 | Recon Loss: 0.004200 | Commit Loss: 0.001348 | Perplexity: 1517.156320
2025-09-26 22:20:50,863 Stage: Train 0.5 | Epoch: 85 | Iter: 129600 | Total Loss: 0.004859 | Recon Loss: 0.004183 | Commit Loss: 0.001352 | Perplexity: 1516.070064
2025-09-26 22:21:34,696 Stage: Train 0.5 | Epoch: 85 | Iter: 129800 | Total Loss: 0.004949 | Recon Loss: 0.004271 | Commit Loss: 0.001356 | Perplexity: 1512.840858
2025-09-26 22:22:18,692 Stage: Train 0.5 | Epoch: 85 | Iter: 130000 | Total Loss: 0.004978 | Recon Loss: 0.004304 | Commit Loss: 0.001349 | Perplexity: 1514.378000
2025-09-26 22:23:02,671 Stage: Train 0.5 | Epoch: 85 | Iter: 130200 | Total Loss: 0.004849 | Recon Loss: 0.004179 | Commit Loss: 0.001340 | Perplexity: 1515.334157
2025-09-26 22:23:46,323 Stage: Train 0.5 | Epoch: 85 | Iter: 130400 | Total Loss: 0.004923 | Recon Loss: 0.004251 | Commit Loss: 0.001344 | Perplexity: 1514.618118
2025-09-26 22:24:30,359 Stage: Train 0.5 | Epoch: 85 | Iter: 130600 | Total Loss: 0.004987 | Recon Loss: 0.004309 | Commit Loss: 0.001355 | Perplexity: 1514.971477
Trainning Epoch:  26%|██▌       | 86/330 [8:12:04<22:31:17, 332.29s/it]2025-09-26 22:25:14,621 Stage: Train 0.5 | Epoch: 86 | Iter: 130800 | Total Loss: 0.004656 | Recon Loss: 0.003993 | Commit Loss: 0.001327 | Perplexity: 1510.094904
2025-09-26 22:25:58,541 Stage: Train 0.5 | Epoch: 86 | Iter: 131000 | Total Loss: 0.004872 | Recon Loss: 0.004198 | Commit Loss: 0.001348 | Perplexity: 1518.630446
2025-09-26 22:26:42,710 Stage: Train 0.5 | Epoch: 86 | Iter: 131200 | Total Loss: 0.004968 | Recon Loss: 0.004296 | Commit Loss: 0.001346 | Perplexity: 1514.342893
2025-09-26 22:27:26,304 Stage: Train 0.5 | Epoch: 86 | Iter: 131400 | Total Loss: 0.004742 | Recon Loss: 0.004066 | Commit Loss: 0.001352 | Perplexity: 1517.727299
2025-09-26 22:28:10,241 Stage: Train 0.5 | Epoch: 86 | Iter: 131600 | Total Loss: 0.004931 | Recon Loss: 0.004257 | Commit Loss: 0.001348 | Perplexity: 1514.695250
2025-09-26 22:28:54,229 Stage: Train 0.5 | Epoch: 86 | Iter: 131800 | Total Loss: 0.004836 | Recon Loss: 0.004162 | Commit Loss: 0.001347 | Perplexity: 1514.698625
2025-09-26 22:29:38,184 Stage: Train 0.5 | Epoch: 86 | Iter: 132000 | Total Loss: 0.004812 | Recon Loss: 0.004142 | Commit Loss: 0.001340 | Perplexity: 1513.822184
Trainning Epoch:  26%|██▋       | 87/330 [8:17:38<22:27:57, 332.83s/it]2025-09-26 22:30:22,527 Stage: Train 0.5 | Epoch: 87 | Iter: 132200 | Total Loss: 0.004749 | Recon Loss: 0.004078 | Commit Loss: 0.001343 | Perplexity: 1514.512059
2025-09-26 22:31:06,375 Stage: Train 0.5 | Epoch: 87 | Iter: 132400 | Total Loss: 0.004961 | Recon Loss: 0.004289 | Commit Loss: 0.001345 | Perplexity: 1514.618216
2025-09-26 22:31:50,346 Stage: Train 0.5 | Epoch: 87 | Iter: 132600 | Total Loss: 0.004816 | Recon Loss: 0.004144 | Commit Loss: 0.001344 | Perplexity: 1511.245984
2025-09-26 22:32:34,697 Stage: Train 0.5 | Epoch: 87 | Iter: 132800 | Total Loss: 0.004794 | Recon Loss: 0.004128 | Commit Loss: 0.001331 | Perplexity: 1514.535443
2025-09-26 22:33:18,644 Stage: Train 0.5 | Epoch: 87 | Iter: 133000 | Total Loss: 0.004850 | Recon Loss: 0.004176 | Commit Loss: 0.001348 | Perplexity: 1514.590740
2025-09-26 22:34:02,560 Stage: Train 0.5 | Epoch: 87 | Iter: 133200 | Total Loss: 0.004819 | Recon Loss: 0.004149 | Commit Loss: 0.001341 | Perplexity: 1513.580239
2025-09-26 22:34:46,547 Stage: Train 0.5 | Epoch: 87 | Iter: 133400 | Total Loss: 0.005000 | Recon Loss: 0.004329 | Commit Loss: 0.001341 | Perplexity: 1514.753629
2025-09-26 22:35:30,564 Stage: Train 0.5 | Epoch: 87 | Iter: 133600 | Total Loss: 0.004721 | Recon Loss: 0.004044 | Commit Loss: 0.001353 | Perplexity: 1517.487341
Trainning Epoch:  27%|██▋       | 88/330 [8:23:12<22:24:20, 333.31s/it]2025-09-26 22:36:14,746 Stage: Train 0.5 | Epoch: 88 | Iter: 133800 | Total Loss: 0.004690 | Recon Loss: 0.004014 | Commit Loss: 0.001351 | Perplexity: 1519.511730
2025-09-26 22:36:58,751 Stage: Train 0.5 | Epoch: 88 | Iter: 134000 | Total Loss: 0.004848 | Recon Loss: 0.004180 | Commit Loss: 0.001336 | Perplexity: 1511.162526
2025-09-26 22:37:42,275 Stage: Train 0.5 | Epoch: 88 | Iter: 134200 | Total Loss: 0.004793 | Recon Loss: 0.004126 | Commit Loss: 0.001334 | Perplexity: 1516.452705
2025-09-26 22:38:26,187 Stage: Train 0.5 | Epoch: 88 | Iter: 134400 | Total Loss: 0.004711 | Recon Loss: 0.004042 | Commit Loss: 0.001340 | Perplexity: 1519.275083
2025-09-26 22:39:10,068 Stage: Train 0.5 | Epoch: 88 | Iter: 134600 | Total Loss: 0.004868 | Recon Loss: 0.004203 | Commit Loss: 0.001331 | Perplexity: 1508.480790
2025-09-26 22:39:53,909 Stage: Train 0.5 | Epoch: 88 | Iter: 134800 | Total Loss: 0.004696 | Recon Loss: 0.004025 | Commit Loss: 0.001343 | Perplexity: 1514.550448
2025-09-26 22:40:37,768 Stage: Train 0.5 | Epoch: 88 | Iter: 135000 | Total Loss: 0.004842 | Recon Loss: 0.004165 | Commit Loss: 0.001354 | Perplexity: 1518.567081
Trainning Epoch:  27%|██▋       | 89/330 [8:28:45<22:18:55, 333.34s/it]2025-09-26 22:41:22,053 Stage: Train 0.5 | Epoch: 89 | Iter: 135200 | Total Loss: 0.004822 | Recon Loss: 0.004152 | Commit Loss: 0.001341 | Perplexity: 1514.841382
2025-09-26 22:42:06,091 Stage: Train 0.5 | Epoch: 89 | Iter: 135400 | Total Loss: 0.004784 | Recon Loss: 0.004115 | Commit Loss: 0.001338 | Perplexity: 1512.425252
2025-09-26 22:42:49,983 Stage: Train 0.5 | Epoch: 89 | Iter: 135600 | Total Loss: 0.005047 | Recon Loss: 0.004387 | Commit Loss: 0.001320 | Perplexity: 1507.619476
2025-09-26 22:43:33,903 Stage: Train 0.5 | Epoch: 89 | Iter: 135800 | Total Loss: 0.004777 | Recon Loss: 0.004104 | Commit Loss: 0.001345 | Perplexity: 1518.540986
2025-09-26 22:44:17,737 Stage: Train 0.5 | Epoch: 89 | Iter: 136000 | Total Loss: 0.004887 | Recon Loss: 0.004214 | Commit Loss: 0.001345 | Perplexity: 1517.286980
2025-09-26 22:45:01,629 Stage: Train 0.5 | Epoch: 89 | Iter: 136200 | Total Loss: 0.004758 | Recon Loss: 0.004089 | Commit Loss: 0.001339 | Perplexity: 1517.592800
2025-09-26 22:45:45,540 Stage: Train 0.5 | Epoch: 89 | Iter: 136400 | Total Loss: 0.004831 | Recon Loss: 0.004162 | Commit Loss: 0.001337 | Perplexity: 1515.259087
2025-09-26 22:46:29,524 Stage: Train 0.5 | Epoch: 89 | Iter: 136600 | Total Loss: 0.004878 | Recon Loss: 0.004213 | Commit Loss: 0.001331 | Perplexity: 1514.397921
Trainning Epoch:  27%|██▋       | 90/330 [8:34:19<22:14:01, 333.51s/it]2025-09-26 22:47:13,703 Stage: Train 0.5 | Epoch: 90 | Iter: 136800 | Total Loss: 0.004794 | Recon Loss: 0.004130 | Commit Loss: 0.001327 | Perplexity: 1512.644182
2025-09-26 22:47:57,490 Stage: Train 0.5 | Epoch: 90 | Iter: 137000 | Total Loss: 0.004692 | Recon Loss: 0.004025 | Commit Loss: 0.001334 | Perplexity: 1512.360363
2025-09-26 22:48:41,614 Stage: Train 0.5 | Epoch: 90 | Iter: 137200 | Total Loss: 0.004768 | Recon Loss: 0.004097 | Commit Loss: 0.001341 | Perplexity: 1517.234006
2025-09-26 22:49:25,538 Stage: Train 0.5 | Epoch: 90 | Iter: 137400 | Total Loss: 0.004830 | Recon Loss: 0.004167 | Commit Loss: 0.001325 | Perplexity: 1512.952847
2025-09-26 22:50:09,702 Stage: Train 0.5 | Epoch: 90 | Iter: 137600 | Total Loss: 0.004951 | Recon Loss: 0.004286 | Commit Loss: 0.001329 | Perplexity: 1514.242925
2025-09-26 22:50:53,502 Stage: Train 0.5 | Epoch: 90 | Iter: 137800 | Total Loss: 0.004688 | Recon Loss: 0.004024 | Commit Loss: 0.001326 | Perplexity: 1515.175946
2025-09-26 22:51:37,683 Stage: Train 0.5 | Epoch: 90 | Iter: 138000 | Total Loss: 0.004851 | Recon Loss: 0.004183 | Commit Loss: 0.001336 | Perplexity: 1513.964248
2025-09-26 22:52:21,439 Stage: Train 0.5 | Epoch: 90 | Iter: 138200 | Total Loss: 0.004805 | Recon Loss: 0.004142 | Commit Loss: 0.001327 | Perplexity: 1513.978442
Trainning Epoch:  28%|██▊       | 91/330 [8:39:53<22:09:08, 333.68s/it]2025-09-26 22:53:05,765 Stage: Train 0.5 | Epoch: 91 | Iter: 138400 | Total Loss: 0.004795 | Recon Loss: 0.004133 | Commit Loss: 0.001323 | Perplexity: 1515.565446
2025-09-26 22:53:49,686 Stage: Train 0.5 | Epoch: 91 | Iter: 138600 | Total Loss: 0.004742 | Recon Loss: 0.004074 | Commit Loss: 0.001336 | Perplexity: 1516.615827
2025-09-26 22:54:33,448 Stage: Train 0.5 | Epoch: 91 | Iter: 138800 | Total Loss: 0.004924 | Recon Loss: 0.004262 | Commit Loss: 0.001324 | Perplexity: 1509.470547
2025-09-26 22:55:17,511 Stage: Train 0.5 | Epoch: 91 | Iter: 139000 | Total Loss: 0.004685 | Recon Loss: 0.004023 | Commit Loss: 0.001324 | Perplexity: 1511.580278
2025-09-26 22:56:01,299 Stage: Train 0.5 | Epoch: 91 | Iter: 139200 | Total Loss: 0.004748 | Recon Loss: 0.004073 | Commit Loss: 0.001349 | Perplexity: 1522.346721
2025-09-26 22:56:45,434 Stage: Train 0.5 | Epoch: 91 | Iter: 139400 | Total Loss: 0.004757 | Recon Loss: 0.004089 | Commit Loss: 0.001335 | Perplexity: 1517.767690
2025-09-26 22:57:29,303 Stage: Train 0.5 | Epoch: 91 | Iter: 139600 | Total Loss: 0.004678 | Recon Loss: 0.004016 | Commit Loss: 0.001324 | Perplexity: 1515.890027
Trainning Epoch:  28%|██▊       | 92/330 [8:45:27<22:03:46, 333.73s/it]2025-09-26 22:58:13,256 Stage: Train 0.5 | Epoch: 92 | Iter: 139800 | Total Loss: 0.004744 | Recon Loss: 0.004081 | Commit Loss: 0.001325 | Perplexity: 1511.913120
2025-09-26 22:58:57,246 Stage: Train 0.5 | Epoch: 92 | Iter: 140000 | Total Loss: 0.004672 | Recon Loss: 0.004014 | Commit Loss: 0.001316 | Perplexity: 1509.305055
2025-09-26 22:58:57,246 Saving model at iteration 140000
2025-09-26 22:58:57,431 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000
2025-09-26 22:58:57,732 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/model.safetensors
2025-09-26 22:58:58,135 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/optimizer.bin
2025-09-26 22:58:58,136 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/scheduler.bin
2025-09-26 22:58:58,136 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/sampler.bin
2025-09-26 22:58:58,137 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/random_states_0.pkl
2025-09-26 22:59:42,116 Stage: Train 0.5 | Epoch: 92 | Iter: 140200 | Total Loss: 0.004771 | Recon Loss: 0.004109 | Commit Loss: 0.001324 | Perplexity: 1516.098904
2025-09-26 23:00:26,004 Stage: Train 0.5 | Epoch: 92 | Iter: 140400 | Total Loss: 0.004899 | Recon Loss: 0.004236 | Commit Loss: 0.001327 | Perplexity: 1519.552980
2025-09-26 23:01:09,859 Stage: Train 0.5 | Epoch: 92 | Iter: 140600 | Total Loss: 0.004715 | Recon Loss: 0.004049 | Commit Loss: 0.001333 | Perplexity: 1518.447107
2025-09-26 23:01:53,882 Stage: Train 0.5 | Epoch: 92 | Iter: 140800 | Total Loss: 0.004790 | Recon Loss: 0.004125 | Commit Loss: 0.001329 | Perplexity: 1515.425955
2025-09-26 23:02:37,961 Stage: Train 0.5 | Epoch: 92 | Iter: 141000 | Total Loss: 0.004767 | Recon Loss: 0.004101 | Commit Loss: 0.001331 | Perplexity: 1511.666863
2025-09-26 23:03:22,083 Stage: Train 0.5 | Epoch: 92 | Iter: 141200 | Total Loss: 0.004793 | Recon Loss: 0.004130 | Commit Loss: 0.001326 | Perplexity: 1512.978877
Trainning Epoch:  28%|██▊       | 93/330 [8:51:03<21:59:58, 334.17s/it]2025-09-26 23:04:06,298 Stage: Train 0.5 | Epoch: 93 | Iter: 141400 | Total Loss: 0.004756 | Recon Loss: 0.004098 | Commit Loss: 0.001316 | Perplexity: 1516.350225
2025-09-26 23:04:49,223 Stage: Train 0.5 | Epoch: 93 | Iter: 141600 | Total Loss: 0.004737 | Recon Loss: 0.004078 | Commit Loss: 0.001317 | Perplexity: 1513.363724
2025-09-26 23:05:33,517 Stage: Train 0.5 | Epoch: 93 | Iter: 141800 | Total Loss: 0.004736 | Recon Loss: 0.004072 | Commit Loss: 0.001328 | Perplexity: 1516.018947
2025-09-26 23:06:17,490 Stage: Train 0.5 | Epoch: 93 | Iter: 142000 | Total Loss: 0.004783 | Recon Loss: 0.004118 | Commit Loss: 0.001330 | Perplexity: 1514.961767
2025-09-26 23:07:01,565 Stage: Train 0.5 | Epoch: 93 | Iter: 142200 | Total Loss: 0.004711 | Recon Loss: 0.004050 | Commit Loss: 0.001323 | Perplexity: 1515.194404
2025-09-26 23:07:45,387 Stage: Train 0.5 | Epoch: 93 | Iter: 142400 | Total Loss: 0.004723 | Recon Loss: 0.004060 | Commit Loss: 0.001326 | Perplexity: 1518.436757
2025-09-26 23:08:29,508 Stage: Train 0.5 | Epoch: 93 | Iter: 142600 | Total Loss: 0.004687 | Recon Loss: 0.004025 | Commit Loss: 0.001325 | Perplexity: 1515.714103
Trainning Epoch:  28%|██▊       | 94/330 [8:56:36<21:53:53, 334.04s/it]2025-09-26 23:09:13,849 Stage: Train 0.5 | Epoch: 94 | Iter: 142800 | Total Loss: 0.004889 | Recon Loss: 0.004220 | Commit Loss: 0.001337 | Perplexity: 1514.766373
2025-09-26 23:09:57,844 Stage: Train 0.5 | Epoch: 94 | Iter: 143000 | Total Loss: 0.004898 | Recon Loss: 0.004241 | Commit Loss: 0.001314 | Perplexity: 1512.645463
2025-09-26 23:10:41,822 Stage: Train 0.5 | Epoch: 94 | Iter: 143200 | Total Loss: 0.004742 | Recon Loss: 0.004076 | Commit Loss: 0.001332 | Perplexity: 1520.803376
2025-09-26 23:11:25,775 Stage: Train 0.5 | Epoch: 94 | Iter: 143400 | Total Loss: 0.004711 | Recon Loss: 0.004055 | Commit Loss: 0.001312 | Perplexity: 1513.174246
2025-09-26 23:12:09,928 Stage: Train 0.5 | Epoch: 94 | Iter: 143600 | Total Loss: 0.004720 | Recon Loss: 0.004062 | Commit Loss: 0.001317 | Perplexity: 1515.089339
2025-09-26 23:12:54,009 Stage: Train 0.5 | Epoch: 94 | Iter: 143800 | Total Loss: 0.004699 | Recon Loss: 0.004033 | Commit Loss: 0.001331 | Perplexity: 1513.340734
2025-09-26 23:13:37,969 Stage: Train 0.5 | Epoch: 94 | Iter: 144000 | Total Loss: 0.004646 | Recon Loss: 0.003989 | Commit Loss: 0.001314 | Perplexity: 1511.538162
2025-09-26 23:14:22,181 Stage: Train 0.5 | Epoch: 94 | Iter: 144200 | Total Loss: 0.004714 | Recon Loss: 0.004048 | Commit Loss: 0.001331 | Perplexity: 1513.374852
Trainning Epoch:  29%|██▉       | 95/330 [9:02:11<21:48:58, 334.21s/it]2025-09-26 23:15:06,176 Stage: Train 0.5 | Epoch: 95 | Iter: 144400 | Total Loss: 0.004687 | Recon Loss: 0.004027 | Commit Loss: 0.001321 | Perplexity: 1514.556731
2025-09-26 23:15:50,248 Stage: Train 0.5 | Epoch: 95 | Iter: 144600 | Total Loss: 0.004957 | Recon Loss: 0.004296 | Commit Loss: 0.001322 | Perplexity: 1515.762833
2025-09-26 23:16:34,062 Stage: Train 0.5 | Epoch: 95 | Iter: 144800 | Total Loss: 0.004713 | Recon Loss: 0.004061 | Commit Loss: 0.001305 | Perplexity: 1509.177601
2025-09-26 23:17:17,877 Stage: Train 0.5 | Epoch: 95 | Iter: 145000 | Total Loss: 0.004769 | Recon Loss: 0.004109 | Commit Loss: 0.001320 | Perplexity: 1510.893264
2025-09-26 23:18:01,649 Stage: Train 0.5 | Epoch: 95 | Iter: 145200 | Total Loss: 0.004601 | Recon Loss: 0.003941 | Commit Loss: 0.001320 | Perplexity: 1514.535291
2025-09-26 23:18:45,794 Stage: Train 0.5 | Epoch: 95 | Iter: 145400 | Total Loss: 0.004585 | Recon Loss: 0.003928 | Commit Loss: 0.001314 | Perplexity: 1512.450099
2025-09-26 23:19:29,664 Stage: Train 0.5 | Epoch: 95 | Iter: 145600 | Total Loss: 0.004715 | Recon Loss: 0.004054 | Commit Loss: 0.001323 | Perplexity: 1518.236024
2025-09-26 23:20:13,661 Stage: Train 0.5 | Epoch: 95 | Iter: 145800 | Total Loss: 0.004596 | Recon Loss: 0.003934 | Commit Loss: 0.001324 | Perplexity: 1514.845508
Trainning Epoch:  29%|██▉       | 96/330 [9:07:45<21:42:54, 334.08s/it]2025-09-26 23:20:57,670 Stage: Train 0.5 | Epoch: 96 | Iter: 146000 | Total Loss: 0.004628 | Recon Loss: 0.003971 | Commit Loss: 0.001315 | Perplexity: 1514.641948
2025-09-26 23:21:41,405 Stage: Train 0.5 | Epoch: 96 | Iter: 146200 | Total Loss: 0.004649 | Recon Loss: 0.003991 | Commit Loss: 0.001317 | Perplexity: 1519.008547
2025-09-26 23:22:25,368 Stage: Train 0.5 | Epoch: 96 | Iter: 146400 | Total Loss: 0.004669 | Recon Loss: 0.004012 | Commit Loss: 0.001314 | Perplexity: 1514.373584
2025-09-26 23:23:09,434 Stage: Train 0.5 | Epoch: 96 | Iter: 146600 | Total Loss: 0.004591 | Recon Loss: 0.003932 | Commit Loss: 0.001319 | Perplexity: 1511.607527
2025-09-26 23:23:53,360 Stage: Train 0.5 | Epoch: 96 | Iter: 146800 | Total Loss: 0.004845 | Recon Loss: 0.004185 | Commit Loss: 0.001321 | Perplexity: 1511.640590
2025-09-26 23:24:37,379 Stage: Train 0.5 | Epoch: 96 | Iter: 147000 | Total Loss: 0.004600 | Recon Loss: 0.003939 | Commit Loss: 0.001322 | Perplexity: 1513.349532
2025-09-26 23:25:21,166 Stage: Train 0.5 | Epoch: 96 | Iter: 147200 | Total Loss: 0.004704 | Recon Loss: 0.004046 | Commit Loss: 0.001316 | Perplexity: 1512.794398
Trainning Epoch:  29%|██▉       | 97/330 [9:13:18<21:37:03, 334.01s/it]2025-09-26 23:26:05,538 Stage: Train 0.5 | Epoch: 97 | Iter: 147400 | Total Loss: 0.004703 | Recon Loss: 0.004043 | Commit Loss: 0.001319 | Perplexity: 1515.101868
2025-09-26 23:26:49,639 Stage: Train 0.5 | Epoch: 97 | Iter: 147600 | Total Loss: 0.004687 | Recon Loss: 0.004034 | Commit Loss: 0.001304 | Perplexity: 1511.576375
2025-09-26 23:27:33,607 Stage: Train 0.5 | Epoch: 97 | Iter: 147800 | Total Loss: 0.004688 | Recon Loss: 0.004032 | Commit Loss: 0.001312 | Perplexity: 1516.654447
2025-09-26 23:28:17,360 Stage: Train 0.5 | Epoch: 97 | Iter: 148000 | Total Loss: 0.004600 | Recon Loss: 0.003943 | Commit Loss: 0.001313 | Perplexity: 1515.239255
2025-09-26 23:29:01,251 Stage: Train 0.5 | Epoch: 97 | Iter: 148200 | Total Loss: 0.004640 | Recon Loss: 0.003982 | Commit Loss: 0.001317 | Perplexity: 1513.945887
2025-09-26 23:29:45,101 Stage: Train 0.5 | Epoch: 97 | Iter: 148400 | Total Loss: 0.004728 | Recon Loss: 0.004068 | Commit Loss: 0.001322 | Perplexity: 1514.688663
2025-09-26 23:30:29,156 Stage: Train 0.5 | Epoch: 97 | Iter: 148600 | Total Loss: 0.004527 | Recon Loss: 0.003868 | Commit Loss: 0.001317 | Perplexity: 1513.337169
2025-09-26 23:31:13,253 Stage: Train 0.5 | Epoch: 97 | Iter: 148800 | Total Loss: 0.004848 | Recon Loss: 0.004191 | Commit Loss: 0.001314 | Perplexity: 1509.049534
Trainning Epoch:  30%|██▉       | 98/330 [9:18:52<21:31:18, 333.96s/it]2025-09-26 23:31:57,096 Stage: Train 0.5 | Epoch: 98 | Iter: 149000 | Total Loss: 0.004596 | Recon Loss: 0.003938 | Commit Loss: 0.001317 | Perplexity: 1516.386197
2025-09-26 23:32:41,084 Stage: Train 0.5 | Epoch: 98 | Iter: 149200 | Total Loss: 0.004678 | Recon Loss: 0.004020 | Commit Loss: 0.001315 | Perplexity: 1515.796569
2025-09-26 23:33:25,245 Stage: Train 0.5 | Epoch: 98 | Iter: 149400 | Total Loss: 0.004718 | Recon Loss: 0.004066 | Commit Loss: 0.001304 | Perplexity: 1514.232717
2025-09-26 23:34:09,333 Stage: Train 0.5 | Epoch: 98 | Iter: 149600 | Total Loss: 0.004856 | Recon Loss: 0.004202 | Commit Loss: 0.001308 | Perplexity: 1514.343068
2025-09-26 23:34:53,212 Stage: Train 0.5 | Epoch: 98 | Iter: 149800 | Total Loss: 0.004544 | Recon Loss: 0.003891 | Commit Loss: 0.001307 | Perplexity: 1517.394101
2025-09-26 23:35:37,185 Stage: Train 0.5 | Epoch: 98 | Iter: 150000 | Total Loss: 0.004584 | Recon Loss: 0.003922 | Commit Loss: 0.001322 | Perplexity: 1518.501696
2025-09-26 23:36:21,156 Stage: Train 0.5 | Epoch: 98 | Iter: 150200 | Total Loss: 0.004590 | Recon Loss: 0.003938 | Commit Loss: 0.001304 | Perplexity: 1512.132830
Trainning Epoch:  30%|███       | 99/330 [9:24:27<21:26:11, 334.08s/it]2025-09-26 23:37:05,403 Stage: Train 0.5 | Epoch: 99 | Iter: 150400 | Total Loss: 0.004733 | Recon Loss: 0.004077 | Commit Loss: 0.001313 | Perplexity: 1512.161445
2025-09-26 23:37:49,278 Stage: Train 0.5 | Epoch: 99 | Iter: 150600 | Total Loss: 0.004681 | Recon Loss: 0.004029 | Commit Loss: 0.001304 | Perplexity: 1510.148591
2025-09-26 23:38:33,109 Stage: Train 0.5 | Epoch: 99 | Iter: 150800 | Total Loss: 0.004689 | Recon Loss: 0.004031 | Commit Loss: 0.001316 | Perplexity: 1516.785114
2025-09-26 23:39:17,058 Stage: Train 0.5 | Epoch: 99 | Iter: 151000 | Total Loss: 0.004595 | Recon Loss: 0.003941 | Commit Loss: 0.001307 | Perplexity: 1514.295593
2025-09-26 23:40:01,060 Stage: Train 0.5 | Epoch: 99 | Iter: 151200 | Total Loss: 0.004691 | Recon Loss: 0.004036 | Commit Loss: 0.001310 | Perplexity: 1512.926481
2025-09-26 23:40:44,820 Stage: Train 0.5 | Epoch: 99 | Iter: 151400 | Total Loss: 0.004780 | Recon Loss: 0.004127 | Commit Loss: 0.001308 | Perplexity: 1510.528855
2025-09-26 23:41:28,854 Stage: Train 0.5 | Epoch: 99 | Iter: 151600 | Total Loss: 0.004597 | Recon Loss: 0.003948 | Commit Loss: 0.001298 | Perplexity: 1513.065554
2025-09-26 23:42:12,641 Stage: Train 0.5 | Epoch: 99 | Iter: 151800 | Total Loss: 0.004587 | Recon Loss: 0.003931 | Commit Loss: 0.001312 | Perplexity: 1514.578065
Trainning Epoch:  30%|███       | 100/330 [9:30:00<21:20:14, 333.98s/it]2025-09-26 23:42:56,808 Stage: Train 0.5 | Epoch: 100 | Iter: 152000 | Total Loss: 0.004671 | Recon Loss: 0.004015 | Commit Loss: 0.001313 | Perplexity: 1513.606524
2025-09-26 23:43:40,754 Stage: Train 0.5 | Epoch: 100 | Iter: 152200 | Total Loss: 0.004620 | Recon Loss: 0.003974 | Commit Loss: 0.001294 | Perplexity: 1510.915278
2025-09-26 23:44:24,781 Stage: Train 0.5 | Epoch: 100 | Iter: 152400 | Total Loss: 0.004644 | Recon Loss: 0.003988 | Commit Loss: 0.001312 | Perplexity: 1510.327626
2025-09-26 23:45:08,568 Stage: Train 0.5 | Epoch: 100 | Iter: 152600 | Total Loss: 0.004666 | Recon Loss: 0.004016 | Commit Loss: 0.001301 | Perplexity: 1512.523856
2025-09-26 23:45:52,631 Stage: Train 0.5 | Epoch: 100 | Iter: 152800 | Total Loss: 0.004699 | Recon Loss: 0.004047 | Commit Loss: 0.001304 | Perplexity: 1515.034785
2025-09-26 23:46:36,642 Stage: Train 0.5 | Epoch: 100 | Iter: 153000 | Total Loss: 0.004550 | Recon Loss: 0.003899 | Commit Loss: 0.001302 | Perplexity: 1511.587812
2025-09-26 23:47:20,711 Stage: Train 0.5 | Epoch: 100 | Iter: 153200 | Total Loss: 0.004497 | Recon Loss: 0.003845 | Commit Loss: 0.001303 | Perplexity: 1512.865224
2025-09-26 23:48:04,766 Stage: Train 0.5 | Epoch: 100 | Iter: 153400 | Total Loss: 0.004587 | Recon Loss: 0.003931 | Commit Loss: 0.001312 | Perplexity: 1510.226076
Trainning Epoch:  31%|███       | 101/330 [9:35:35<21:15:02, 334.07s/it]2025-09-26 23:48:48,795 Stage: Train 0.5 | Epoch: 101 | Iter: 153600 | Total Loss: 0.004835 | Recon Loss: 0.004186 | Commit Loss: 0.001299 | Perplexity: 1509.353939
2025-09-26 23:49:31,754 Stage: Train 0.5 | Epoch: 101 | Iter: 153800 | Total Loss: 0.004626 | Recon Loss: 0.003975 | Commit Loss: 0.001302 | Perplexity: 1513.555935
2025-09-26 23:50:15,608 Stage: Train 0.5 | Epoch: 101 | Iter: 154000 | Total Loss: 0.004685 | Recon Loss: 0.004033 | Commit Loss: 0.001304 | Perplexity: 1511.039592
2025-09-26 23:50:59,473 Stage: Train 0.5 | Epoch: 101 | Iter: 154200 | Total Loss: 0.004541 | Recon Loss: 0.003888 | Commit Loss: 0.001306 | Perplexity: 1512.367496
2025-09-26 23:51:43,359 Stage: Train 0.5 | Epoch: 101 | Iter: 154400 | Total Loss: 0.004612 | Recon Loss: 0.003963 | Commit Loss: 0.001298 | Perplexity: 1518.047410
2025-09-26 23:52:26,869 Stage: Train 0.5 | Epoch: 101 | Iter: 154600 | Total Loss: 0.004631 | Recon Loss: 0.003976 | Commit Loss: 0.001309 | Perplexity: 1517.952339
2025-09-26 23:53:10,727 Stage: Train 0.5 | Epoch: 101 | Iter: 154800 | Total Loss: 0.004606 | Recon Loss: 0.003960 | Commit Loss: 0.001294 | Perplexity: 1507.050449
Trainning Epoch:  31%|███       | 102/330 [9:41:07<21:07:08, 333.46s/it]2025-09-26 23:53:54,788 Stage: Train 0.5 | Epoch: 102 | Iter: 155000 | Total Loss: 0.004575 | Recon Loss: 0.003924 | Commit Loss: 0.001301 | Perplexity: 1511.078890
2025-09-26 23:54:38,734 Stage: Train 0.5 | Epoch: 102 | Iter: 155200 | Total Loss: 0.004589 | Recon Loss: 0.003938 | Commit Loss: 0.001302 | Perplexity: 1514.820962
2025-09-26 23:55:22,226 Stage: Train 0.5 | Epoch: 102 | Iter: 155400 | Total Loss: 0.004560 | Recon Loss: 0.003912 | Commit Loss: 0.001295 | Perplexity: 1511.218707
2025-09-26 23:56:06,018 Stage: Train 0.5 | Epoch: 102 | Iter: 155600 | Total Loss: 0.004655 | Recon Loss: 0.004008 | Commit Loss: 0.001295 | Perplexity: 1512.318450
2025-09-26 23:56:50,086 Stage: Train 0.5 | Epoch: 102 | Iter: 155800 | Total Loss: 0.004623 | Recon Loss: 0.003973 | Commit Loss: 0.001299 | Perplexity: 1509.315738
2025-09-26 23:57:34,061 Stage: Train 0.5 | Epoch: 102 | Iter: 156000 | Total Loss: 0.004526 | Recon Loss: 0.003880 | Commit Loss: 0.001291 | Perplexity: 1505.563225
2025-09-26 23:58:18,016 Stage: Train 0.5 | Epoch: 102 | Iter: 156200 | Total Loss: 0.004616 | Recon Loss: 0.003967 | Commit Loss: 0.001299 | Perplexity: 1511.311904
2025-09-26 23:59:01,614 Stage: Train 0.5 | Epoch: 102 | Iter: 156400 | Total Loss: 0.004654 | Recon Loss: 0.003998 | Commit Loss: 0.001313 | Perplexity: 1514.677339
Trainning Epoch:  31%|███       | 103/330 [9:46:40<21:01:12, 333.36s/it]2025-09-26 23:59:45,965 Stage: Train 0.5 | Epoch: 103 | Iter: 156600 | Total Loss: 0.004597 | Recon Loss: 0.003946 | Commit Loss: 0.001302 | Perplexity: 1512.241769
2025-09-27 00:00:29,755 Stage: Train 0.5 | Epoch: 103 | Iter: 156800 | Total Loss: 0.004481 | Recon Loss: 0.003836 | Commit Loss: 0.001290 | Perplexity: 1512.854769
2025-09-27 00:01:13,657 Stage: Train 0.5 | Epoch: 103 | Iter: 157000 | Total Loss: 0.004597 | Recon Loss: 0.003948 | Commit Loss: 0.001298 | Perplexity: 1512.432250
2025-09-27 00:01:57,312 Stage: Train 0.5 | Epoch: 103 | Iter: 157200 | Total Loss: 0.004646 | Recon Loss: 0.003993 | Commit Loss: 0.001306 | Perplexity: 1514.747872
2025-09-27 00:02:41,321 Stage: Train 0.5 | Epoch: 103 | Iter: 157400 | Total Loss: 0.004488 | Recon Loss: 0.003837 | Commit Loss: 0.001303 | Perplexity: 1515.096979
2025-09-27 00:03:25,272 Stage: Train 0.5 | Epoch: 103 | Iter: 157600 | Total Loss: 0.004590 | Recon Loss: 0.003938 | Commit Loss: 0.001304 | Perplexity: 1507.370698
2025-09-27 00:04:09,226 Stage: Train 0.5 | Epoch: 103 | Iter: 157800 | Total Loss: 0.004535 | Recon Loss: 0.003888 | Commit Loss: 0.001296 | Perplexity: 1509.664998
Trainning Epoch:  32%|███▏      | 104/330 [9:52:14<20:56:13, 333.51s/it]2025-09-27 00:04:53,568 Stage: Train 0.5 | Epoch: 104 | Iter: 158000 | Total Loss: 0.004629 | Recon Loss: 0.003976 | Commit Loss: 0.001305 | Perplexity: 1514.090050
2025-09-27 00:05:37,382 Stage: Train 0.5 | Epoch: 104 | Iter: 158200 | Total Loss: 0.004620 | Recon Loss: 0.003969 | Commit Loss: 0.001302 | Perplexity: 1515.756677
2025-09-27 00:06:21,247 Stage: Train 0.5 | Epoch: 104 | Iter: 158400 | Total Loss: 0.004455 | Recon Loss: 0.003805 | Commit Loss: 0.001300 | Perplexity: 1514.823145
2025-09-27 00:07:05,325 Stage: Train 0.5 | Epoch: 104 | Iter: 158600 | Total Loss: 0.004662 | Recon Loss: 0.004011 | Commit Loss: 0.001301 | Perplexity: 1510.801501
2025-09-27 00:07:49,577 Stage: Train 0.5 | Epoch: 104 | Iter: 158800 | Total Loss: 0.004485 | Recon Loss: 0.003838 | Commit Loss: 0.001294 | Perplexity: 1512.732927
2025-09-27 00:08:33,579 Stage: Train 0.5 | Epoch: 104 | Iter: 159000 | Total Loss: 0.004531 | Recon Loss: 0.003882 | Commit Loss: 0.001299 | Perplexity: 1512.696133
2025-09-27 00:09:17,213 Stage: Train 0.5 | Epoch: 104 | Iter: 159200 | Total Loss: 0.004570 | Recon Loss: 0.003923 | Commit Loss: 0.001294 | Perplexity: 1509.842812
2025-09-27 00:10:01,116 Stage: Train 0.5 | Epoch: 104 | Iter: 159400 | Total Loss: 0.004653 | Recon Loss: 0.004006 | Commit Loss: 0.001295 | Perplexity: 1506.591042
Trainning Epoch:  32%|███▏      | 105/330 [9:57:48<20:51:13, 333.66s/it]2025-09-27 00:10:45,321 Stage: Train 0.5 | Epoch: 105 | Iter: 159600 | Total Loss: 0.004467 | Recon Loss: 0.003818 | Commit Loss: 0.001299 | Perplexity: 1509.516729
2025-09-27 00:11:29,183 Stage: Train 0.5 | Epoch: 105 | Iter: 159800 | Total Loss: 0.004561 | Recon Loss: 0.003915 | Commit Loss: 0.001292 | Perplexity: 1510.794270
2025-09-27 00:12:13,026 Stage: Train 0.5 | Epoch: 105 | Iter: 160000 | Total Loss: 0.004707 | Recon Loss: 0.004064 | Commit Loss: 0.001286 | Perplexity: 1510.315372
2025-09-27 00:12:13,026 Saving model at iteration 160000
2025-09-27 00:12:13,229 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000
2025-09-27 00:12:13,542 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/model.safetensors
2025-09-27 00:12:13,940 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/optimizer.bin
2025-09-27 00:12:13,940 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/scheduler.bin
2025-09-27 00:12:13,941 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/sampler.bin
2025-09-27 00:12:13,942 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/random_states_0.pkl
2025-09-27 00:12:58,086 Stage: Train 0.5 | Epoch: 105 | Iter: 160200 | Total Loss: 0.004511 | Recon Loss: 0.003868 | Commit Loss: 0.001287 | Perplexity: 1510.617812
2025-09-27 00:13:41,939 Stage: Train 0.5 | Epoch: 105 | Iter: 160400 | Total Loss: 0.004523 | Recon Loss: 0.003874 | Commit Loss: 0.001299 | Perplexity: 1513.978634
2025-09-27 00:14:25,870 Stage: Train 0.5 | Epoch: 105 | Iter: 160600 | Total Loss: 0.004600 | Recon Loss: 0.003952 | Commit Loss: 0.001297 | Perplexity: 1515.971942
2025-09-27 00:15:09,843 Stage: Train 0.5 | Epoch: 105 | Iter: 160800 | Total Loss: 0.004434 | Recon Loss: 0.003785 | Commit Loss: 0.001299 | Perplexity: 1512.597515
2025-09-27 00:15:53,604 Stage: Train 0.5 | Epoch: 105 | Iter: 161000 | Total Loss: 0.004679 | Recon Loss: 0.004027 | Commit Loss: 0.001304 | Perplexity: 1514.578478
Trainning Epoch:  32%|███▏      | 106/330 [10:03:22<20:46:44, 333.95s/it]2025-09-27 00:16:37,894 Stage: Train 0.5 | Epoch: 106 | Iter: 161200 | Total Loss: 0.004497 | Recon Loss: 0.003853 | Commit Loss: 0.001288 | Perplexity: 1509.873624
2025-09-27 00:17:21,917 Stage: Train 0.5 | Epoch: 106 | Iter: 161400 | Total Loss: 0.004539 | Recon Loss: 0.003892 | Commit Loss: 0.001294 | Perplexity: 1513.064764
2025-09-27 00:18:05,973 Stage: Train 0.5 | Epoch: 106 | Iter: 161600 | Total Loss: 0.004614 | Recon Loss: 0.003967 | Commit Loss: 0.001294 | Perplexity: 1512.843896
2025-09-27 00:18:49,705 Stage: Train 0.5 | Epoch: 106 | Iter: 161800 | Total Loss: 0.004526 | Recon Loss: 0.003882 | Commit Loss: 0.001288 | Perplexity: 1512.364689
2025-09-27 00:19:33,698 Stage: Train 0.5 | Epoch: 106 | Iter: 162000 | Total Loss: 0.004568 | Recon Loss: 0.003922 | Commit Loss: 0.001293 | Perplexity: 1509.991431
2025-09-27 00:20:17,679 Stage: Train 0.5 | Epoch: 106 | Iter: 162200 | Total Loss: 0.004573 | Recon Loss: 0.003929 | Commit Loss: 0.001290 | Perplexity: 1509.279601
2025-09-27 00:21:01,726 Stage: Train 0.5 | Epoch: 106 | Iter: 162400 | Total Loss: 0.004583 | Recon Loss: 0.003942 | Commit Loss: 0.001282 | Perplexity: 1507.407475
Trainning Epoch:  32%|███▏      | 107/330 [10:08:57<20:41:38, 334.08s/it]2025-09-27 00:21:46,061 Stage: Train 0.5 | Epoch: 107 | Iter: 162600 | Total Loss: 0.004476 | Recon Loss: 0.003833 | Commit Loss: 0.001285 | Perplexity: 1512.683630
2025-09-27 00:22:29,695 Stage: Train 0.5 | Epoch: 107 | Iter: 162800 | Total Loss: 0.004482 | Recon Loss: 0.003835 | Commit Loss: 0.001295 | Perplexity: 1514.412509
2025-09-27 00:23:13,593 Stage: Train 0.5 | Epoch: 107 | Iter: 163000 | Total Loss: 0.004509 | Recon Loss: 0.003865 | Commit Loss: 0.001286 | Perplexity: 1506.428917
2025-09-27 00:23:57,443 Stage: Train 0.5 | Epoch: 107 | Iter: 163200 | Total Loss: 0.004519 | Recon Loss: 0.003878 | Commit Loss: 0.001281 | Perplexity: 1511.978282
2025-09-27 00:24:41,248 Stage: Train 0.5 | Epoch: 107 | Iter: 163400 | Total Loss: 0.004503 | Recon Loss: 0.003859 | Commit Loss: 0.001288 | Perplexity: 1511.726205
2025-09-27 00:25:25,292 Stage: Train 0.5 | Epoch: 107 | Iter: 163600 | Total Loss: 0.004521 | Recon Loss: 0.003878 | Commit Loss: 0.001286 | Perplexity: 1509.848993
2025-09-27 00:26:09,232 Stage: Train 0.5 | Epoch: 107 | Iter: 163800 | Total Loss: 0.004428 | Recon Loss: 0.003778 | Commit Loss: 0.001300 | Perplexity: 1513.335403
2025-09-27 00:26:53,327 Stage: Train 0.5 | Epoch: 107 | Iter: 164000 | Total Loss: 0.004785 | Recon Loss: 0.004137 | Commit Loss: 0.001296 | Perplexity: 1512.712000
Trainning Epoch:  33%|███▎      | 108/330 [10:14:31<20:35:48, 334.00s/it]2025-09-27 00:27:37,796 Stage: Train 0.5 | Epoch: 108 | Iter: 164200 | Total Loss: 0.004506 | Recon Loss: 0.003865 | Commit Loss: 0.001283 | Perplexity: 1510.664496
2025-09-27 00:28:21,708 Stage: Train 0.5 | Epoch: 108 | Iter: 164400 | Total Loss: 0.004611 | Recon Loss: 0.003966 | Commit Loss: 0.001291 | Perplexity: 1509.554457
2025-09-27 00:29:05,429 Stage: Train 0.5 | Epoch: 108 | Iter: 164600 | Total Loss: 0.004514 | Recon Loss: 0.003875 | Commit Loss: 0.001277 | Perplexity: 1508.693898
2025-09-27 00:29:49,259 Stage: Train 0.5 | Epoch: 108 | Iter: 164800 | Total Loss: 0.004493 | Recon Loss: 0.003842 | Commit Loss: 0.001302 | Perplexity: 1519.313264
2025-09-27 00:30:33,101 Stage: Train 0.5 | Epoch: 108 | Iter: 165000 | Total Loss: 0.004514 | Recon Loss: 0.003873 | Commit Loss: 0.001283 | Perplexity: 1510.731642
2025-09-27 00:31:16,934 Stage: Train 0.5 | Epoch: 108 | Iter: 165200 | Total Loss: 0.004639 | Recon Loss: 0.003995 | Commit Loss: 0.001287 | Perplexity: 1510.992574
2025-09-27 00:32:00,768 Stage: Train 0.5 | Epoch: 108 | Iter: 165400 | Total Loss: 0.004524 | Recon Loss: 0.003879 | Commit Loss: 0.001289 | Perplexity: 1512.323547
Trainning Epoch:  33%|███▎      | 109/330 [10:20:04<20:29:16, 333.74s/it]2025-09-27 00:32:44,678 Stage: Train 0.5 | Epoch: 109 | Iter: 165600 | Total Loss: 0.004510 | Recon Loss: 0.003874 | Commit Loss: 0.001273 | Perplexity: 1505.337926
2025-09-27 00:33:28,519 Stage: Train 0.5 | Epoch: 109 | Iter: 165800 | Total Loss: 0.004472 | Recon Loss: 0.003832 | Commit Loss: 0.001281 | Perplexity: 1510.456703
2025-09-27 00:34:11,276 Stage: Train 0.5 | Epoch: 109 | Iter: 166000 | Total Loss: 0.004449 | Recon Loss: 0.003809 | Commit Loss: 0.001280 | Perplexity: 1508.586904
2025-09-27 00:34:55,091 Stage: Train 0.5 | Epoch: 109 | Iter: 166200 | Total Loss: 0.004616 | Recon Loss: 0.003969 | Commit Loss: 0.001294 | Perplexity: 1511.313640
2025-09-27 00:35:39,080 Stage: Train 0.5 | Epoch: 109 | Iter: 166400 | Total Loss: 0.004657 | Recon Loss: 0.004014 | Commit Loss: 0.001285 | Perplexity: 1515.603004
2025-09-27 00:36:22,906 Stage: Train 0.5 | Epoch: 109 | Iter: 166600 | Total Loss: 0.004354 | Recon Loss: 0.003711 | Commit Loss: 0.001285 | Perplexity: 1515.569225
2025-09-27 00:37:06,764 Stage: Train 0.5 | Epoch: 109 | Iter: 166800 | Total Loss: 0.004445 | Recon Loss: 0.003805 | Commit Loss: 0.001281 | Perplexity: 1507.239771
2025-09-27 00:37:50,657 Stage: Train 0.5 | Epoch: 109 | Iter: 167000 | Total Loss: 0.004529 | Recon Loss: 0.003883 | Commit Loss: 0.001292 | Perplexity: 1517.680128
Trainning Epoch:  33%|███▎      | 110/330 [10:25:36<20:22:11, 333.33s/it]2025-09-27 00:38:34,839 Stage: Train 0.5 | Epoch: 110 | Iter: 167200 | Total Loss: 0.004546 | Recon Loss: 0.003908 | Commit Loss: 0.001275 | Perplexity: 1509.684459
2025-09-27 00:39:18,740 Stage: Train 0.5 | Epoch: 110 | Iter: 167400 | Total Loss: 0.004471 | Recon Loss: 0.003832 | Commit Loss: 0.001278 | Perplexity: 1508.147930
2025-09-27 00:40:02,618 Stage: Train 0.5 | Epoch: 110 | Iter: 167600 | Total Loss: 0.004525 | Recon Loss: 0.003886 | Commit Loss: 0.001278 | Perplexity: 1509.458768
2025-09-27 00:40:46,457 Stage: Train 0.5 | Epoch: 110 | Iter: 167800 | Total Loss: 0.004382 | Recon Loss: 0.003746 | Commit Loss: 0.001273 | Perplexity: 1509.106406
2025-09-27 00:41:30,478 Stage: Train 0.5 | Epoch: 110 | Iter: 168000 | Total Loss: 0.004476 | Recon Loss: 0.003836 | Commit Loss: 0.001281 | Perplexity: 1511.061182
2025-09-27 00:42:14,420 Stage: Train 0.5 | Epoch: 110 | Iter: 168200 | Total Loss: 0.004510 | Recon Loss: 0.003868 | Commit Loss: 0.001283 | Perplexity: 1514.638606
2025-09-27 00:42:58,025 Stage: Train 0.5 | Epoch: 110 | Iter: 168400 | Total Loss: 0.004418 | Recon Loss: 0.003773 | Commit Loss: 0.001291 | Perplexity: 1515.839631
2025-09-27 00:43:42,101 Stage: Train 0.5 | Epoch: 110 | Iter: 168600 | Total Loss: 0.004644 | Recon Loss: 0.004001 | Commit Loss: 0.001287 | Perplexity: 1513.577130
Trainning Epoch:  34%|███▎      | 111/330 [10:31:10<20:17:05, 333.45s/it]2025-09-27 00:44:26,324 Stage: Train 0.5 | Epoch: 111 | Iter: 168800 | Total Loss: 0.004357 | Recon Loss: 0.003718 | Commit Loss: 0.001278 | Perplexity: 1510.977427
2025-09-27 00:45:10,173 Stage: Train 0.5 | Epoch: 111 | Iter: 169000 | Total Loss: 0.004611 | Recon Loss: 0.003970 | Commit Loss: 0.001282 | Perplexity: 1512.804082
2025-09-27 00:45:53,824 Stage: Train 0.5 | Epoch: 111 | Iter: 169200 | Total Loss: 0.004443 | Recon Loss: 0.003803 | Commit Loss: 0.001281 | Perplexity: 1512.537698
2025-09-27 00:46:37,764 Stage: Train 0.5 | Epoch: 111 | Iter: 169400 | Total Loss: 0.004515 | Recon Loss: 0.003874 | Commit Loss: 0.001281 | Perplexity: 1511.242050
2025-09-27 00:47:21,324 Stage: Train 0.5 | Epoch: 111 | Iter: 169600 | Total Loss: 0.004417 | Recon Loss: 0.003778 | Commit Loss: 0.001279 | Perplexity: 1513.126312
2025-09-27 00:48:05,302 Stage: Train 0.5 | Epoch: 111 | Iter: 169800 | Total Loss: 0.004489 | Recon Loss: 0.003852 | Commit Loss: 0.001275 | Perplexity: 1514.206398
2025-09-27 00:48:49,205 Stage: Train 0.5 | Epoch: 111 | Iter: 170000 | Total Loss: 0.004456 | Recon Loss: 0.003818 | Commit Loss: 0.001276 | Perplexity: 1508.225544
Trainning Epoch:  34%|███▍      | 112/330 [10:36:43<20:11:10, 333.35s/it]2025-09-27 00:49:33,270 Stage: Train 0.5 | Epoch: 112 | Iter: 170200 | Total Loss: 0.004608 | Recon Loss: 0.003967 | Commit Loss: 0.001282 | Perplexity: 1515.736071
2025-09-27 00:50:17,183 Stage: Train 0.5 | Epoch: 112 | Iter: 170400 | Total Loss: 0.004385 | Recon Loss: 0.003751 | Commit Loss: 0.001267 | Perplexity: 1510.683528
2025-09-27 00:51:01,058 Stage: Train 0.5 | Epoch: 112 | Iter: 170600 | Total Loss: 0.004445 | Recon Loss: 0.003807 | Commit Loss: 0.001276 | Perplexity: 1512.461785
2025-09-27 00:51:45,172 Stage: Train 0.5 | Epoch: 112 | Iter: 170800 | Total Loss: 0.004493 | Recon Loss: 0.003854 | Commit Loss: 0.001278 | Perplexity: 1512.222672
2025-09-27 00:52:29,294 Stage: Train 0.5 | Epoch: 112 | Iter: 171000 | Total Loss: 0.004462 | Recon Loss: 0.003826 | Commit Loss: 0.001271 | Perplexity: 1511.207650
2025-09-27 00:53:12,946 Stage: Train 0.5 | Epoch: 112 | Iter: 171200 | Total Loss: 0.004647 | Recon Loss: 0.004008 | Commit Loss: 0.001277 | Perplexity: 1505.514853
2025-09-27 00:53:57,003 Stage: Train 0.5 | Epoch: 112 | Iter: 171400 | Total Loss: 0.004418 | Recon Loss: 0.003780 | Commit Loss: 0.001277 | Perplexity: 1512.819548
2025-09-27 00:54:40,918 Stage: Train 0.5 | Epoch: 112 | Iter: 171600 | Total Loss: 0.004434 | Recon Loss: 0.003796 | Commit Loss: 0.001275 | Perplexity: 1512.374857
Trainning Epoch:  34%|███▍      | 113/330 [10:42:17<20:06:22, 333.56s/it]2025-09-27 00:55:25,135 Stage: Train 0.5 | Epoch: 113 | Iter: 171800 | Total Loss: 0.004381 | Recon Loss: 0.003745 | Commit Loss: 0.001271 | Perplexity: 1510.884860
2025-09-27 00:56:08,726 Stage: Train 0.5 | Epoch: 113 | Iter: 172000 | Total Loss: 0.004437 | Recon Loss: 0.003799 | Commit Loss: 0.001276 | Perplexity: 1515.358288
2025-09-27 00:56:52,580 Stage: Train 0.5 | Epoch: 113 | Iter: 172200 | Total Loss: 0.004519 | Recon Loss: 0.003883 | Commit Loss: 0.001271 | Perplexity: 1512.260004
2025-09-27 00:57:36,469 Stage: Train 0.5 | Epoch: 113 | Iter: 172400 | Total Loss: 0.004452 | Recon Loss: 0.003814 | Commit Loss: 0.001275 | Perplexity: 1514.365203
2025-09-27 00:58:20,325 Stage: Train 0.5 | Epoch: 113 | Iter: 172600 | Total Loss: 0.004515 | Recon Loss: 0.003879 | Commit Loss: 0.001271 | Perplexity: 1513.986105
2025-09-27 00:59:04,360 Stage: Train 0.5 | Epoch: 113 | Iter: 172800 | Total Loss: 0.004374 | Recon Loss: 0.003733 | Commit Loss: 0.001281 | Perplexity: 1513.475772
2025-09-27 00:59:47,986 Stage: Train 0.5 | Epoch: 113 | Iter: 173000 | Total Loss: 0.004489 | Recon Loss: 0.003851 | Commit Loss: 0.001276 | Perplexity: 1513.020618
Trainning Epoch:  35%|███▍      | 114/330 [10:47:50<20:00:18, 333.42s/it]2025-09-27 01:00:32,120 Stage: Train 0.5 | Epoch: 114 | Iter: 173200 | Total Loss: 0.004442 | Recon Loss: 0.003803 | Commit Loss: 0.001278 | Perplexity: 1511.521540
2025-09-27 01:01:16,137 Stage: Train 0.5 | Epoch: 114 | Iter: 173400 | Total Loss: 0.004482 | Recon Loss: 0.003852 | Commit Loss: 0.001259 | Perplexity: 1507.391821
2025-09-27 01:02:00,033 Stage: Train 0.5 | Epoch: 114 | Iter: 173600 | Total Loss: 0.004473 | Recon Loss: 0.003834 | Commit Loss: 0.001278 | Perplexity: 1516.075466
2025-09-27 01:02:43,926 Stage: Train 0.5 | Epoch: 114 | Iter: 173800 | Total Loss: 0.004513 | Recon Loss: 0.003879 | Commit Loss: 0.001268 | Perplexity: 1508.751929
2025-09-27 01:03:27,617 Stage: Train 0.5 | Epoch: 114 | Iter: 174000 | Total Loss: 0.004574 | Recon Loss: 0.003936 | Commit Loss: 0.001276 | Perplexity: 1513.874960
2025-09-27 01:04:11,476 Stage: Train 0.5 | Epoch: 114 | Iter: 174200 | Total Loss: 0.004320 | Recon Loss: 0.003686 | Commit Loss: 0.001269 | Perplexity: 1511.619109
2025-09-27 01:04:55,330 Stage: Train 0.5 | Epoch: 114 | Iter: 174400 | Total Loss: 0.004438 | Recon Loss: 0.003802 | Commit Loss: 0.001272 | Perplexity: 1513.713926
2025-09-27 01:05:39,365 Stage: Train 0.5 | Epoch: 114 | Iter: 174600 | Total Loss: 0.004363 | Recon Loss: 0.003724 | Commit Loss: 0.001279 | Perplexity: 1514.714382
Trainning Epoch:  35%|███▍      | 115/330 [10:53:24<19:54:58, 333.48s/it]2025-09-27 01:06:23,331 Stage: Train 0.5 | Epoch: 115 | Iter: 174800 | Total Loss: 0.004414 | Recon Loss: 0.003775 | Commit Loss: 0.001276 | Perplexity: 1512.066735
2025-09-27 01:07:07,135 Stage: Train 0.5 | Epoch: 115 | Iter: 175000 | Total Loss: 0.004482 | Recon Loss: 0.003848 | Commit Loss: 0.001269 | Perplexity: 1509.282092
2025-09-27 01:07:51,119 Stage: Train 0.5 | Epoch: 115 | Iter: 175200 | Total Loss: 0.004428 | Recon Loss: 0.003792 | Commit Loss: 0.001271 | Perplexity: 1511.243955
2025-09-27 01:08:35,129 Stage: Train 0.5 | Epoch: 115 | Iter: 175400 | Total Loss: 0.004541 | Recon Loss: 0.003908 | Commit Loss: 0.001267 | Perplexity: 1508.796593
2025-09-27 01:09:19,124 Stage: Train 0.5 | Epoch: 115 | Iter: 175600 | Total Loss: 0.004418 | Recon Loss: 0.003781 | Commit Loss: 0.001275 | Perplexity: 1511.914826
2025-09-27 01:10:02,892 Stage: Train 0.5 | Epoch: 115 | Iter: 175800 | Total Loss: 0.004479 | Recon Loss: 0.003845 | Commit Loss: 0.001269 | Perplexity: 1511.153951
2025-09-27 01:10:46,776 Stage: Train 0.5 | Epoch: 115 | Iter: 176000 | Total Loss: 0.004399 | Recon Loss: 0.003768 | Commit Loss: 0.001261 | Perplexity: 1509.622659
2025-09-27 01:11:30,615 Stage: Train 0.5 | Epoch: 115 | Iter: 176200 | Total Loss: 0.004426 | Recon Loss: 0.003794 | Commit Loss: 0.001263 | Perplexity: 1512.062553
Trainning Epoch:  35%|███▌      | 116/330 [10:58:57<19:49:27, 333.49s/it]2025-09-27 01:12:14,695 Stage: Train 0.5 | Epoch: 116 | Iter: 176400 | Total Loss: 0.004477 | Recon Loss: 0.003849 | Commit Loss: 0.001256 | Perplexity: 1510.968644
2025-09-27 01:12:58,421 Stage: Train 0.5 | Epoch: 116 | Iter: 176600 | Total Loss: 0.004401 | Recon Loss: 0.003767 | Commit Loss: 0.001268 | Perplexity: 1511.007877
2025-09-27 01:13:42,314 Stage: Train 0.5 | Epoch: 116 | Iter: 176800 | Total Loss: 0.004466 | Recon Loss: 0.003832 | Commit Loss: 0.001267 | Perplexity: 1508.492964
2025-09-27 01:14:26,121 Stage: Train 0.5 | Epoch: 116 | Iter: 177000 | Total Loss: 0.004425 | Recon Loss: 0.003790 | Commit Loss: 0.001270 | Perplexity: 1512.553582
2025-09-27 01:15:10,160 Stage: Train 0.5 | Epoch: 116 | Iter: 177200 | Total Loss: 0.004610 | Recon Loss: 0.003976 | Commit Loss: 0.001268 | Perplexity: 1507.195618
2025-09-27 01:15:54,103 Stage: Train 0.5 | Epoch: 116 | Iter: 177400 | Total Loss: 0.004320 | Recon Loss: 0.003686 | Commit Loss: 0.001268 | Perplexity: 1508.652906
2025-09-27 01:16:37,971 Stage: Train 0.5 | Epoch: 116 | Iter: 177600 | Total Loss: 0.004486 | Recon Loss: 0.003849 | Commit Loss: 0.001275 | Perplexity: 1512.853699
Trainning Epoch:  35%|███▌      | 117/330 [11:04:31<19:43:48, 333.47s/it]2025-09-27 01:17:22,014 Stage: Train 0.5 | Epoch: 117 | Iter: 177800 | Total Loss: 0.004439 | Recon Loss: 0.003812 | Commit Loss: 0.001255 | Perplexity: 1506.019293
2025-09-27 01:18:05,962 Stage: Train 0.5 | Epoch: 117 | Iter: 178000 | Total Loss: 0.004377 | Recon Loss: 0.003744 | Commit Loss: 0.001267 | Perplexity: 1513.791643
2025-09-27 01:18:48,609 Stage: Train 0.5 | Epoch: 117 | Iter: 178200 | Total Loss: 0.004335 | Recon Loss: 0.003702 | Commit Loss: 0.001266 | Perplexity: 1510.761548
2025-09-27 01:19:32,510 Stage: Train 0.5 | Epoch: 117 | Iter: 178400 | Total Loss: 0.004393 | Recon Loss: 0.003759 | Commit Loss: 0.001267 | Perplexity: 1511.675554
2025-09-27 01:20:16,027 Stage: Train 0.5 | Epoch: 117 | Iter: 178600 | Total Loss: 0.004388 | Recon Loss: 0.003756 | Commit Loss: 0.001264 | Perplexity: 1511.474390
2025-09-27 01:20:59,898 Stage: Train 0.5 | Epoch: 117 | Iter: 178800 | Total Loss: 0.004386 | Recon Loss: 0.003753 | Commit Loss: 0.001266 | Perplexity: 1512.505260
2025-09-27 01:21:43,471 Stage: Train 0.5 | Epoch: 117 | Iter: 179000 | Total Loss: 0.004464 | Recon Loss: 0.003829 | Commit Loss: 0.001270 | Perplexity: 1513.908405
2025-09-27 01:22:27,512 Stage: Train 0.5 | Epoch: 117 | Iter: 179200 | Total Loss: 0.004443 | Recon Loss: 0.003806 | Commit Loss: 0.001274 | Perplexity: 1516.013640
Trainning Epoch:  36%|███▌      | 118/330 [11:10:02<19:36:30, 332.97s/it]2025-09-27 01:23:11,494 Stage: Train 0.5 | Epoch: 118 | Iter: 179400 | Total Loss: 0.004604 | Recon Loss: 0.003975 | Commit Loss: 0.001259 | Perplexity: 1511.181172
2025-09-27 01:23:55,329 Stage: Train 0.5 | Epoch: 118 | Iter: 179600 | Total Loss: 0.004286 | Recon Loss: 0.003655 | Commit Loss: 0.001263 | Perplexity: 1513.688631
2025-09-27 01:24:39,001 Stage: Train 0.5 | Epoch: 118 | Iter: 179800 | Total Loss: 0.004452 | Recon Loss: 0.003824 | Commit Loss: 0.001257 | Perplexity: 1510.974252
2025-09-27 01:25:22,962 Stage: Train 0.5 | Epoch: 118 | Iter: 180000 | Total Loss: 0.004409 | Recon Loss: 0.003781 | Commit Loss: 0.001256 | Perplexity: 1509.758264
2025-09-27 01:25:22,962 Saving model at iteration 180000
2025-09-27 01:25:23,152 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000
2025-09-27 01:25:23,428 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/model.safetensors
2025-09-27 01:25:23,784 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/optimizer.bin
2025-09-27 01:25:23,785 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/scheduler.bin
2025-09-27 01:25:23,785 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/sampler.bin
2025-09-27 01:25:23,785 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/random_states_0.pkl
2025-09-27 01:26:08,228 Stage: Train 0.5 | Epoch: 118 | Iter: 180200 | Total Loss: 0.004523 | Recon Loss: 0.003891 | Commit Loss: 0.001262 | Perplexity: 1513.274973
2025-09-27 01:26:52,080 Stage: Train 0.5 | Epoch: 118 | Iter: 180400 | Total Loss: 0.004375 | Recon Loss: 0.003741 | Commit Loss: 0.001269 | Perplexity: 1513.229363
2025-09-27 01:27:36,067 Stage: Train 0.5 | Epoch: 118 | Iter: 180600 | Total Loss: 0.004378 | Recon Loss: 0.003746 | Commit Loss: 0.001265 | Perplexity: 1515.599622
Trainning Epoch:  36%|███▌      | 119/330 [11:15:37<19:32:42, 333.47s/it]2025-09-27 01:28:20,163 Stage: Train 0.5 | Epoch: 119 | Iter: 180800 | Total Loss: 0.004391 | Recon Loss: 0.003765 | Commit Loss: 0.001250 | Perplexity: 1508.419800
2025-09-27 01:29:04,106 Stage: Train 0.5 | Epoch: 119 | Iter: 181000 | Total Loss: 0.004501 | Recon Loss: 0.003872 | Commit Loss: 0.001258 | Perplexity: 1511.756729
2025-09-27 01:29:48,128 Stage: Train 0.5 | Epoch: 119 | Iter: 181200 | Total Loss: 0.004383 | Recon Loss: 0.003750 | Commit Loss: 0.001266 | Perplexity: 1516.066606
2025-09-27 01:30:31,911 Stage: Train 0.5 | Epoch: 119 | Iter: 181400 | Total Loss: 0.004468 | Recon Loss: 0.003839 | Commit Loss: 0.001257 | Perplexity: 1510.511537
2025-09-27 01:31:15,881 Stage: Train 0.5 | Epoch: 119 | Iter: 181600 | Total Loss: 0.004336 | Recon Loss: 0.003710 | Commit Loss: 0.001251 | Perplexity: 1509.253585
2025-09-27 01:31:59,857 Stage: Train 0.5 | Epoch: 119 | Iter: 181800 | Total Loss: 0.004418 | Recon Loss: 0.003790 | Commit Loss: 0.001257 | Perplexity: 1506.135285
2025-09-27 01:32:43,971 Stage: Train 0.5 | Epoch: 119 | Iter: 182000 | Total Loss: 0.004363 | Recon Loss: 0.003734 | Commit Loss: 0.001259 | Perplexity: 1513.215913
2025-09-27 01:33:27,664 Stage: Train 0.5 | Epoch: 119 | Iter: 182200 | Total Loss: 0.004513 | Recon Loss: 0.003885 | Commit Loss: 0.001256 | Perplexity: 1510.032766
Trainning Epoch:  36%|███▋      | 120/330 [11:21:11<19:27:44, 333.64s/it]2025-09-27 01:34:12,012 Stage: Train 0.5 | Epoch: 120 | Iter: 182400 | Total Loss: 0.004239 | Recon Loss: 0.003613 | Commit Loss: 0.001251 | Perplexity: 1511.414531
2025-09-27 01:34:56,144 Stage: Train 0.5 | Epoch: 120 | Iter: 182600 | Total Loss: 0.004379 | Recon Loss: 0.003750 | Commit Loss: 0.001258 | Perplexity: 1511.193615
2025-09-27 01:35:39,895 Stage: Train 0.5 | Epoch: 120 | Iter: 182800 | Total Loss: 0.004390 | Recon Loss: 0.003764 | Commit Loss: 0.001252 | Perplexity: 1515.358268
2025-09-27 01:36:23,740 Stage: Train 0.5 | Epoch: 120 | Iter: 183000 | Total Loss: 0.004362 | Recon Loss: 0.003730 | Commit Loss: 0.001263 | Perplexity: 1511.964068
2025-09-27 01:37:07,386 Stage: Train 0.5 | Epoch: 120 | Iter: 183200 | Total Loss: 0.004428 | Recon Loss: 0.003802 | Commit Loss: 0.001252 | Perplexity: 1508.825284
2025-09-27 01:37:51,380 Stage: Train 0.5 | Epoch: 120 | Iter: 183400 | Total Loss: 0.004373 | Recon Loss: 0.003742 | Commit Loss: 0.001262 | Perplexity: 1512.486921
2025-09-27 01:38:35,195 Stage: Train 0.5 | Epoch: 120 | Iter: 183600 | Total Loss: 0.004335 | Recon Loss: 0.003704 | Commit Loss: 0.001263 | Perplexity: 1512.885483
Trainning Epoch:  37%|███▋      | 121/330 [11:26:45<19:22:17, 333.67s/it]2025-09-27 01:39:19,618 Stage: Train 0.5 | Epoch: 121 | Iter: 183800 | Total Loss: 0.004493 | Recon Loss: 0.003865 | Commit Loss: 0.001255 | Perplexity: 1508.120219
2025-09-27 01:40:03,185 Stage: Train 0.5 | Epoch: 121 | Iter: 184000 | Total Loss: 0.004397 | Recon Loss: 0.003769 | Commit Loss: 0.001258 | Perplexity: 1516.973906
2025-09-27 01:40:47,255 Stage: Train 0.5 | Epoch: 121 | Iter: 184200 | Total Loss: 0.004400 | Recon Loss: 0.003773 | Commit Loss: 0.001253 | Perplexity: 1511.789789
2025-09-27 01:41:31,273 Stage: Train 0.5 | Epoch: 121 | Iter: 184400 | Total Loss: 0.004323 | Recon Loss: 0.003698 | Commit Loss: 0.001251 | Perplexity: 1512.163067
2025-09-27 01:42:15,290 Stage: Train 0.5 | Epoch: 121 | Iter: 184600 | Total Loss: 0.004384 | Recon Loss: 0.003754 | Commit Loss: 0.001259 | Perplexity: 1512.295756
2025-09-27 01:42:59,127 Stage: Train 0.5 | Epoch: 121 | Iter: 184800 | Total Loss: 0.004446 | Recon Loss: 0.003827 | Commit Loss: 0.001239 | Perplexity: 1502.094431
2025-09-27 01:43:42,763 Stage: Train 0.5 | Epoch: 121 | Iter: 185000 | Total Loss: 0.004301 | Recon Loss: 0.003672 | Commit Loss: 0.001258 | Perplexity: 1512.708836
2025-09-27 01:44:26,738 Stage: Train 0.5 | Epoch: 121 | Iter: 185200 | Total Loss: 0.004365 | Recon Loss: 0.003738 | Commit Loss: 0.001255 | Perplexity: 1510.407484
Trainning Epoch:  37%|███▋      | 122/330 [11:32:18<19:16:34, 333.63s/it]2025-09-27 01:45:10,944 Stage: Train 0.5 | Epoch: 122 | Iter: 185400 | Total Loss: 0.004531 | Recon Loss: 0.003907 | Commit Loss: 0.001247 | Perplexity: 1511.700376
2025-09-27 01:45:54,832 Stage: Train 0.5 | Epoch: 122 | Iter: 185600 | Total Loss: 0.004252 | Recon Loss: 0.003624 | Commit Loss: 0.001256 | Perplexity: 1516.028975
2025-09-27 01:46:38,700 Stage: Train 0.5 | Epoch: 122 | Iter: 185800 | Total Loss: 0.004409 | Recon Loss: 0.003779 | Commit Loss: 0.001259 | Perplexity: 1514.745510
2025-09-27 01:47:22,290 Stage: Train 0.5 | Epoch: 122 | Iter: 186000 | Total Loss: 0.004348 | Recon Loss: 0.003723 | Commit Loss: 0.001250 | Perplexity: 1510.177613
2025-09-27 01:48:06,151 Stage: Train 0.5 | Epoch: 122 | Iter: 186200 | Total Loss: 0.004387 | Recon Loss: 0.003762 | Commit Loss: 0.001250 | Perplexity: 1510.578297
2025-09-27 01:48:50,163 Stage: Train 0.5 | Epoch: 122 | Iter: 186400 | Total Loss: 0.004343 | Recon Loss: 0.003716 | Commit Loss: 0.001254 | Perplexity: 1512.584693
2025-09-27 01:49:33,919 Stage: Train 0.5 | Epoch: 122 | Iter: 186600 | Total Loss: 0.004391 | Recon Loss: 0.003768 | Commit Loss: 0.001245 | Perplexity: 1508.996695
2025-09-27 01:50:17,640 Stage: Train 0.5 | Epoch: 122 | Iter: 186800 | Total Loss: 0.004399 | Recon Loss: 0.003774 | Commit Loss: 0.001251 | Perplexity: 1510.282682
Trainning Epoch:  37%|███▋      | 123/330 [11:37:51<19:10:29, 333.48s/it]2025-09-27 01:51:01,831 Stage: Train 0.5 | Epoch: 123 | Iter: 187000 | Total Loss: 0.004276 | Recon Loss: 0.003653 | Commit Loss: 0.001246 | Perplexity: 1510.873261
2025-09-27 01:51:45,906 Stage: Train 0.5 | Epoch: 123 | Iter: 187200 | Total Loss: 0.004355 | Recon Loss: 0.003730 | Commit Loss: 0.001250 | Perplexity: 1512.221923
2025-09-27 01:52:29,871 Stage: Train 0.5 | Epoch: 123 | Iter: 187400 | Total Loss: 0.004355 | Recon Loss: 0.003730 | Commit Loss: 0.001249 | Perplexity: 1509.777916
2025-09-27 01:53:13,597 Stage: Train 0.5 | Epoch: 123 | Iter: 187600 | Total Loss: 0.004340 | Recon Loss: 0.003717 | Commit Loss: 0.001246 | Perplexity: 1511.814559
2025-09-27 01:53:57,374 Stage: Train 0.5 | Epoch: 123 | Iter: 187800 | Total Loss: 0.004364 | Recon Loss: 0.003740 | Commit Loss: 0.001247 | Perplexity: 1511.683560
2025-09-27 01:54:41,234 Stage: Train 0.5 | Epoch: 123 | Iter: 188000 | Total Loss: 0.004505 | Recon Loss: 0.003879 | Commit Loss: 0.001251 | Perplexity: 1514.116205
2025-09-27 01:55:25,133 Stage: Train 0.5 | Epoch: 123 | Iter: 188200 | Total Loss: 0.004284 | Recon Loss: 0.003663 | Commit Loss: 0.001243 | Perplexity: 1510.673297
Trainning Epoch:  38%|███▊      | 124/330 [11:43:25<19:05:05, 333.52s/it]2025-09-27 01:56:09,353 Stage: Train 0.5 | Epoch: 124 | Iter: 188400 | Total Loss: 0.004310 | Recon Loss: 0.003685 | Commit Loss: 0.001249 | Perplexity: 1510.797435
2025-09-27 01:56:53,216 Stage: Train 0.5 | Epoch: 124 | Iter: 188600 | Total Loss: 0.004316 | Recon Loss: 0.003693 | Commit Loss: 0.001246 | Perplexity: 1512.414116
2025-09-27 01:57:36,814 Stage: Train 0.5 | Epoch: 124 | Iter: 188800 | Total Loss: 0.004371 | Recon Loss: 0.003749 | Commit Loss: 0.001244 | Perplexity: 1511.137579
2025-09-27 01:58:20,772 Stage: Train 0.5 | Epoch: 124 | Iter: 189000 | Total Loss: 0.004370 | Recon Loss: 0.003750 | Commit Loss: 0.001241 | Perplexity: 1508.749107
2025-09-27 01:59:04,599 Stage: Train 0.5 | Epoch: 124 | Iter: 189200 | Total Loss: 0.004289 | Recon Loss: 0.003660 | Commit Loss: 0.001258 | Perplexity: 1514.998450
2025-09-27 01:59:48,524 Stage: Train 0.5 | Epoch: 124 | Iter: 189400 | Total Loss: 0.004452 | Recon Loss: 0.003829 | Commit Loss: 0.001247 | Perplexity: 1511.309233
2025-09-27 02:00:32,051 Stage: Train 0.5 | Epoch: 124 | Iter: 189600 | Total Loss: 0.004309 | Recon Loss: 0.003685 | Commit Loss: 0.001248 | Perplexity: 1510.809691
2025-09-27 02:01:16,066 Stage: Train 0.5 | Epoch: 124 | Iter: 189800 | Total Loss: 0.004387 | Recon Loss: 0.003764 | Commit Loss: 0.001246 | Perplexity: 1511.294641
Trainning Epoch:  38%|███▊      | 125/330 [11:48:58<18:59:01, 333.37s/it]2025-09-27 02:02:00,336 Stage: Train 0.5 | Epoch: 125 | Iter: 190000 | Total Loss: 0.004316 | Recon Loss: 0.003691 | Commit Loss: 0.001250 | Perplexity: 1513.247486
2025-09-27 02:02:44,277 Stage: Train 0.5 | Epoch: 125 | Iter: 190200 | Total Loss: 0.004345 | Recon Loss: 0.003722 | Commit Loss: 0.001247 | Perplexity: 1511.827163
2025-09-27 02:03:26,956 Stage: Train 0.5 | Epoch: 125 | Iter: 190400 | Total Loss: 0.004331 | Recon Loss: 0.003710 | Commit Loss: 0.001241 | Perplexity: 1513.004780
2025-09-27 02:04:10,602 Stage: Train 0.5 | Epoch: 125 | Iter: 190600 | Total Loss: 0.004354 | Recon Loss: 0.003734 | Commit Loss: 0.001240 | Perplexity: 1509.775012
2025-09-27 02:04:54,561 Stage: Train 0.5 | Epoch: 125 | Iter: 190800 | Total Loss: 0.004314 | Recon Loss: 0.003690 | Commit Loss: 0.001247 | Perplexity: 1513.587653
2025-09-27 02:05:38,573 Stage: Train 0.5 | Epoch: 125 | Iter: 191000 | Total Loss: 0.004390 | Recon Loss: 0.003764 | Commit Loss: 0.001251 | Perplexity: 1513.509117
2025-09-27 02:06:22,540 Stage: Train 0.5 | Epoch: 125 | Iter: 191200 | Total Loss: 0.004263 | Recon Loss: 0.003642 | Commit Loss: 0.001241 | Perplexity: 1509.291430
Trainning Epoch:  38%|███▊      | 126/330 [11:54:31<18:52:57, 333.22s/it]2025-09-27 02:07:06,865 Stage: Train 0.5 | Epoch: 126 | Iter: 191400 | Total Loss: 0.004330 | Recon Loss: 0.003709 | Commit Loss: 0.001241 | Perplexity: 1511.579795
2025-09-27 02:07:50,660 Stage: Train 0.5 | Epoch: 126 | Iter: 191600 | Total Loss: 0.004207 | Recon Loss: 0.003587 | Commit Loss: 0.001240 | Perplexity: 1513.287201
2025-09-27 02:08:34,687 Stage: Train 0.5 | Epoch: 126 | Iter: 191800 | Total Loss: 0.004368 | Recon Loss: 0.003748 | Commit Loss: 0.001241 | Perplexity: 1512.447761
2025-09-27 02:09:18,490 Stage: Train 0.5 | Epoch: 126 | Iter: 192000 | Total Loss: 0.004235 | Recon Loss: 0.003611 | Commit Loss: 0.001248 | Perplexity: 1511.768915
2025-09-27 02:10:02,422 Stage: Train 0.5 | Epoch: 126 | Iter: 192200 | Total Loss: 0.004313 | Recon Loss: 0.003695 | Commit Loss: 0.001237 | Perplexity: 1505.762042
2025-09-27 02:10:46,187 Stage: Train 0.5 | Epoch: 126 | Iter: 192400 | Total Loss: 0.004339 | Recon Loss: 0.003713 | Commit Loss: 0.001251 | Perplexity: 1512.511897
2025-09-27 02:11:30,259 Stage: Train 0.5 | Epoch: 126 | Iter: 192600 | Total Loss: 0.004325 | Recon Loss: 0.003702 | Commit Loss: 0.001245 | Perplexity: 1513.544500
2025-09-27 02:12:14,222 Stage: Train 0.5 | Epoch: 126 | Iter: 192800 | Total Loss: 0.004360 | Recon Loss: 0.003739 | Commit Loss: 0.001242 | Perplexity: 1509.507337
Trainning Epoch:  38%|███▊      | 127/330 [12:00:05<18:48:03, 333.42s/it]2025-09-27 02:12:58,543 Stage: Train 0.5 | Epoch: 127 | Iter: 193000 | Total Loss: 0.004251 | Recon Loss: 0.003630 | Commit Loss: 0.001243 | Perplexity: 1513.704904
2025-09-27 02:13:42,667 Stage: Train 0.5 | Epoch: 127 | Iter: 193200 | Total Loss: 0.004420 | Recon Loss: 0.003796 | Commit Loss: 0.001247 | Perplexity: 1511.478185
2025-09-27 02:14:26,416 Stage: Train 0.5 | Epoch: 127 | Iter: 193400 | Total Loss: 0.004205 | Recon Loss: 0.003585 | Commit Loss: 0.001240 | Perplexity: 1511.955352
2025-09-27 02:15:10,122 Stage: Train 0.5 | Epoch: 127 | Iter: 193600 | Total Loss: 0.004526 | Recon Loss: 0.003908 | Commit Loss: 0.001235 | Perplexity: 1512.552590
2025-09-27 02:15:53,930 Stage: Train 0.5 | Epoch: 127 | Iter: 193800 | Total Loss: 0.004180 | Recon Loss: 0.003555 | Commit Loss: 0.001252 | Perplexity: 1510.959526
2025-09-27 02:16:37,917 Stage: Train 0.5 | Epoch: 127 | Iter: 194000 | Total Loss: 0.004333 | Recon Loss: 0.003717 | Commit Loss: 0.001234 | Perplexity: 1504.255898
2025-09-27 02:17:21,833 Stage: Train 0.5 | Epoch: 127 | Iter: 194200 | Total Loss: 0.004326 | Recon Loss: 0.003704 | Commit Loss: 0.001244 | Perplexity: 1511.202188
2025-09-27 02:18:05,585 Stage: Train 0.5 | Epoch: 127 | Iter: 194400 | Total Loss: 0.004263 | Recon Loss: 0.003641 | Commit Loss: 0.001243 | Perplexity: 1515.860104
Trainning Epoch:  39%|███▉      | 128/330 [12:05:38<18:42:28, 333.41s/it]2025-09-27 02:18:49,604 Stage: Train 0.5 | Epoch: 128 | Iter: 194600 | Total Loss: 0.004356 | Recon Loss: 0.003736 | Commit Loss: 0.001239 | Perplexity: 1507.798211
2025-09-27 02:19:33,687 Stage: Train 0.5 | Epoch: 128 | Iter: 194800 | Total Loss: 0.004271 | Recon Loss: 0.003653 | Commit Loss: 0.001236 | Perplexity: 1517.197987
2025-09-27 02:20:17,908 Stage: Train 0.5 | Epoch: 128 | Iter: 195000 | Total Loss: 0.004332 | Recon Loss: 0.003715 | Commit Loss: 0.001233 | Perplexity: 1506.294106
2025-09-27 02:21:01,764 Stage: Train 0.5 | Epoch: 128 | Iter: 195200 | Total Loss: 0.004280 | Recon Loss: 0.003660 | Commit Loss: 0.001239 | Perplexity: 1508.283015
2025-09-27 02:21:45,694 Stage: Train 0.5 | Epoch: 128 | Iter: 195400 | Total Loss: 0.004288 | Recon Loss: 0.003666 | Commit Loss: 0.001245 | Perplexity: 1511.339200
2025-09-27 02:22:29,689 Stage: Train 0.5 | Epoch: 128 | Iter: 195600 | Total Loss: 0.004256 | Recon Loss: 0.003633 | Commit Loss: 0.001245 | Perplexity: 1511.851435
2025-09-27 02:23:13,600 Stage: Train 0.5 | Epoch: 128 | Iter: 195800 | Total Loss: 0.004343 | Recon Loss: 0.003719 | Commit Loss: 0.001248 | Perplexity: 1516.666325
Trainning Epoch:  39%|███▉      | 129/330 [12:11:12<18:37:35, 333.61s/it]2025-09-27 02:23:57,692 Stage: Train 0.5 | Epoch: 129 | Iter: 196000 | Total Loss: 0.004223 | Recon Loss: 0.003601 | Commit Loss: 0.001244 | Perplexity: 1514.920087
2025-09-27 02:24:41,446 Stage: Train 0.5 | Epoch: 129 | Iter: 196200 | Total Loss: 0.004215 | Recon Loss: 0.003597 | Commit Loss: 0.001235 | Perplexity: 1512.226299
2025-09-27 02:25:25,436 Stage: Train 0.5 | Epoch: 129 | Iter: 196400 | Total Loss: 0.004305 | Recon Loss: 0.003691 | Commit Loss: 0.001228 | Perplexity: 1513.012255
2025-09-27 02:26:09,309 Stage: Train 0.5 | Epoch: 129 | Iter: 196600 | Total Loss: 0.004292 | Recon Loss: 0.003668 | Commit Loss: 0.001249 | Perplexity: 1520.136586
2025-09-27 02:26:53,330 Stage: Train 0.5 | Epoch: 129 | Iter: 196800 | Total Loss: 0.004303 | Recon Loss: 0.003683 | Commit Loss: 0.001241 | Perplexity: 1513.067250
2025-09-27 02:27:36,878 Stage: Train 0.5 | Epoch: 129 | Iter: 197000 | Total Loss: 0.004311 | Recon Loss: 0.003690 | Commit Loss: 0.001241 | Perplexity: 1512.105922
2025-09-27 02:28:20,859 Stage: Train 0.5 | Epoch: 129 | Iter: 197200 | Total Loss: 0.004250 | Recon Loss: 0.003633 | Commit Loss: 0.001235 | Perplexity: 1510.922502
2025-09-27 02:29:04,699 Stage: Train 0.5 | Epoch: 129 | Iter: 197400 | Total Loss: 0.004354 | Recon Loss: 0.003736 | Commit Loss: 0.001235 | Perplexity: 1509.281132
Trainning Epoch:  39%|███▉      | 130/330 [12:16:46<18:31:50, 333.55s/it]2025-09-27 02:29:49,012 Stage: Train 0.5 | Epoch: 130 | Iter: 197600 | Total Loss: 0.004294 | Recon Loss: 0.003679 | Commit Loss: 0.001231 | Perplexity: 1508.288926
2025-09-27 02:30:32,948 Stage: Train 0.5 | Epoch: 130 | Iter: 197800 | Total Loss: 0.004280 | Recon Loss: 0.003659 | Commit Loss: 0.001242 | Perplexity: 1515.816750
2025-09-27 02:31:16,549 Stage: Train 0.5 | Epoch: 130 | Iter: 198000 | Total Loss: 0.004290 | Recon Loss: 0.003670 | Commit Loss: 0.001240 | Perplexity: 1514.048611
2025-09-27 02:32:00,411 Stage: Train 0.5 | Epoch: 130 | Iter: 198200 | Total Loss: 0.004298 | Recon Loss: 0.003679 | Commit Loss: 0.001236 | Perplexity: 1509.906271
2025-09-27 02:32:44,567 Stage: Train 0.5 | Epoch: 130 | Iter: 198400 | Total Loss: 0.004255 | Recon Loss: 0.003638 | Commit Loss: 0.001234 | Perplexity: 1509.972471
2025-09-27 02:33:28,530 Stage: Train 0.5 | Epoch: 130 | Iter: 198600 | Total Loss: 0.004239 | Recon Loss: 0.003617 | Commit Loss: 0.001244 | Perplexity: 1512.746904
2025-09-27 02:34:12,473 Stage: Train 0.5 | Epoch: 130 | Iter: 198800 | Total Loss: 0.004429 | Recon Loss: 0.003813 | Commit Loss: 0.001231 | Perplexity: 1510.106360
Trainning Epoch:  40%|███▉      | 131/330 [12:22:20<18:26:33, 333.63s/it]2025-09-27 02:34:56,567 Stage: Train 0.5 | Epoch: 131 | Iter: 199000 | Total Loss: 0.004204 | Recon Loss: 0.003584 | Commit Loss: 0.001240 | Perplexity: 1510.507734
2025-09-27 02:35:40,356 Stage: Train 0.5 | Epoch: 131 | Iter: 199200 | Total Loss: 0.004331 | Recon Loss: 0.003715 | Commit Loss: 0.001232 | Perplexity: 1508.982735
2025-09-27 02:36:24,224 Stage: Train 0.5 | Epoch: 131 | Iter: 199400 | Total Loss: 0.004319 | Recon Loss: 0.003700 | Commit Loss: 0.001240 | Perplexity: 1515.005687
2025-09-27 02:37:08,283 Stage: Train 0.5 | Epoch: 131 | Iter: 199600 | Total Loss: 0.004277 | Recon Loss: 0.003665 | Commit Loss: 0.001225 | Perplexity: 1509.862619
2025-09-27 02:37:52,047 Stage: Train 0.5 | Epoch: 131 | Iter: 199800 | Total Loss: 0.004286 | Recon Loss: 0.003668 | Commit Loss: 0.001237 | Perplexity: 1512.209172
2025-09-27 02:38:35,922 Stage: Train 0.5 | Epoch: 131 | Iter: 200000 | Total Loss: 0.004293 | Recon Loss: 0.003679 | Commit Loss: 0.001229 | Perplexity: 1510.451224
2025-09-27 02:38:35,922 Saving model at iteration 200000
2025-09-27 02:38:36,371 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000
2025-09-27 02:38:36,665 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/model.safetensors
2025-09-27 02:38:37,064 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/optimizer.bin
2025-09-27 02:38:37,065 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/scheduler.bin
2025-09-27 02:38:37,065 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/sampler.bin
2025-09-27 02:38:37,066 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/random_states_0.pkl
2025-09-27 02:39:21,297 Stage: Train 0.5 | Epoch: 131 | Iter: 200200 | Total Loss: 0.004343 | Recon Loss: 0.003726 | Commit Loss: 0.001234 | Perplexity: 1511.345519
2025-09-27 02:40:05,214 Stage: Train 0.5 | Epoch: 131 | Iter: 200400 | Total Loss: 0.004256 | Recon Loss: 0.003643 | Commit Loss: 0.001227 | Perplexity: 1508.916355
Trainning Epoch:  40%|████      | 132/330 [12:27:55<18:22:29, 334.09s/it]2025-09-27 02:40:49,686 Stage: Train 0.5 | Epoch: 132 | Iter: 200600 | Total Loss: 0.004305 | Recon Loss: 0.003690 | Commit Loss: 0.001229 | Perplexity: 1507.770773
2025-09-27 02:41:33,485 Stage: Train 0.5 | Epoch: 132 | Iter: 200800 | Total Loss: 0.004192 | Recon Loss: 0.003578 | Commit Loss: 0.001229 | Perplexity: 1511.679761
2025-09-27 02:42:17,578 Stage: Train 0.5 | Epoch: 132 | Iter: 201000 | Total Loss: 0.004367 | Recon Loss: 0.003752 | Commit Loss: 0.001231 | Perplexity: 1509.693179
2025-09-27 02:43:01,563 Stage: Train 0.5 | Epoch: 132 | Iter: 201200 | Total Loss: 0.004261 | Recon Loss: 0.003646 | Commit Loss: 0.001230 | Perplexity: 1511.206528
2025-09-27 02:43:45,631 Stage: Train 0.5 | Epoch: 132 | Iter: 201400 | Total Loss: 0.004161 | Recon Loss: 0.003546 | Commit Loss: 0.001230 | Perplexity: 1514.652699
2025-09-27 02:44:29,310 Stage: Train 0.5 | Epoch: 132 | Iter: 201600 | Total Loss: 0.004295 | Recon Loss: 0.003675 | Commit Loss: 0.001241 | Perplexity: 1512.564772
2025-09-27 02:45:13,194 Stage: Train 0.5 | Epoch: 132 | Iter: 201800 | Total Loss: 0.004260 | Recon Loss: 0.003647 | Commit Loss: 0.001227 | Perplexity: 1509.166553
2025-09-27 02:45:57,097 Stage: Train 0.5 | Epoch: 132 | Iter: 202000 | Total Loss: 0.004342 | Recon Loss: 0.003726 | Commit Loss: 0.001232 | Perplexity: 1514.527503
Trainning Epoch:  40%|████      | 133/330 [12:33:29<18:16:52, 334.07s/it]2025-09-27 02:46:41,395 Stage: Train 0.5 | Epoch: 133 | Iter: 202200 | Total Loss: 0.004164 | Recon Loss: 0.003554 | Commit Loss: 0.001220 | Perplexity: 1510.515630
2025-09-27 02:47:25,437 Stage: Train 0.5 | Epoch: 133 | Iter: 202400 | Total Loss: 0.004189 | Recon Loss: 0.003574 | Commit Loss: 0.001230 | Perplexity: 1513.381040
2025-09-27 02:48:07,410 Stage: Train 0.5 | Epoch: 133 | Iter: 202600 | Total Loss: 0.004302 | Recon Loss: 0.003682 | Commit Loss: 0.001239 | Perplexity: 1511.610876
2025-09-27 02:48:51,340 Stage: Train 0.5 | Epoch: 133 | Iter: 202800 | Total Loss: 0.004326 | Recon Loss: 0.003713 | Commit Loss: 0.001227 | Perplexity: 1510.585984
2025-09-27 02:49:35,380 Stage: Train 0.5 | Epoch: 133 | Iter: 203000 | Total Loss: 0.004233 | Recon Loss: 0.003615 | Commit Loss: 0.001236 | Perplexity: 1514.969197
2025-09-27 02:50:19,136 Stage: Train 0.5 | Epoch: 133 | Iter: 203200 | Total Loss: 0.004391 | Recon Loss: 0.003775 | Commit Loss: 0.001232 | Perplexity: 1509.078262
2025-09-27 02:51:03,084 Stage: Train 0.5 | Epoch: 133 | Iter: 203400 | Total Loss: 0.004190 | Recon Loss: 0.003572 | Commit Loss: 0.001236 | Perplexity: 1515.496835
Trainning Epoch:  41%|████      | 134/330 [12:39:01<18:09:09, 333.42s/it]2025-09-27 02:51:47,093 Stage: Train 0.5 | Epoch: 134 | Iter: 203600 | Total Loss: 0.004282 | Recon Loss: 0.003666 | Commit Loss: 0.001230 | Perplexity: 1512.039254
2025-09-27 02:52:31,396 Stage: Train 0.5 | Epoch: 134 | Iter: 203800 | Total Loss: 0.004345 | Recon Loss: 0.003732 | Commit Loss: 0.001226 | Perplexity: 1509.154846
2025-09-27 02:53:15,366 Stage: Train 0.5 | Epoch: 134 | Iter: 204000 | Total Loss: 0.004335 | Recon Loss: 0.003722 | Commit Loss: 0.001227 | Perplexity: 1511.400901
2025-09-27 02:53:59,123 Stage: Train 0.5 | Epoch: 134 | Iter: 204200 | Total Loss: 0.004139 | Recon Loss: 0.003527 | Commit Loss: 0.001225 | Perplexity: 1511.707643
2025-09-27 02:54:42,724 Stage: Train 0.5 | Epoch: 134 | Iter: 204400 | Total Loss: 0.004364 | Recon Loss: 0.003744 | Commit Loss: 0.001239 | Perplexity: 1517.032809
2025-09-27 02:55:26,717 Stage: Train 0.5 | Epoch: 134 | Iter: 204600 | Total Loss: 0.004161 | Recon Loss: 0.003552 | Commit Loss: 0.001219 | Perplexity: 1509.412601
2025-09-27 02:56:10,789 Stage: Train 0.5 | Epoch: 134 | Iter: 204800 | Total Loss: 0.004273 | Recon Loss: 0.003661 | Commit Loss: 0.001224 | Perplexity: 1509.008117
2025-09-27 02:56:54,899 Stage: Train 0.5 | Epoch: 134 | Iter: 205000 | Total Loss: 0.004235 | Recon Loss: 0.003618 | Commit Loss: 0.001233 | Perplexity: 1509.703273
Trainning Epoch:  41%|████      | 135/330 [12:44:35<18:04:24, 333.66s/it]2025-09-27 02:57:39,137 Stage: Train 0.5 | Epoch: 135 | Iter: 205200 | Total Loss: 0.004199 | Recon Loss: 0.003592 | Commit Loss: 0.001213 | Perplexity: 1507.249210
2025-09-27 02:58:22,869 Stage: Train 0.5 | Epoch: 135 | Iter: 205400 | Total Loss: 0.004370 | Recon Loss: 0.003759 | Commit Loss: 0.001223 | Perplexity: 1510.182128
2025-09-27 02:59:06,924 Stage: Train 0.5 | Epoch: 135 | Iter: 205600 | Total Loss: 0.004203 | Recon Loss: 0.003590 | Commit Loss: 0.001226 | Perplexity: 1522.052647
2025-09-27 02:59:50,673 Stage: Train 0.5 | Epoch: 135 | Iter: 205800 | Total Loss: 0.004167 | Recon Loss: 0.003550 | Commit Loss: 0.001234 | Perplexity: 1513.482568
2025-09-27 03:00:34,570 Stage: Train 0.5 | Epoch: 135 | Iter: 206000 | Total Loss: 0.004228 | Recon Loss: 0.003613 | Commit Loss: 0.001229 | Perplexity: 1511.957001
2025-09-27 03:01:18,643 Stage: Train 0.5 | Epoch: 135 | Iter: 206200 | Total Loss: 0.004256 | Recon Loss: 0.003644 | Commit Loss: 0.001224 | Perplexity: 1513.267366
2025-09-27 03:02:02,273 Stage: Train 0.5 | Epoch: 135 | Iter: 206400 | Total Loss: 0.004081 | Recon Loss: 0.003467 | Commit Loss: 0.001228 | Perplexity: 1512.005072
Trainning Epoch:  41%|████      | 136/330 [12:50:08<17:58:35, 333.58s/it]2025-09-27 03:02:46,359 Stage: Train 0.5 | Epoch: 136 | Iter: 206600 | Total Loss: 0.004272 | Recon Loss: 0.003657 | Commit Loss: 0.001231 | Perplexity: 1511.274989
2025-09-27 03:03:30,329 Stage: Train 0.5 | Epoch: 136 | Iter: 206800 | Total Loss: 0.004209 | Recon Loss: 0.003597 | Commit Loss: 0.001222 | Perplexity: 1511.215663
2025-09-27 03:04:14,261 Stage: Train 0.5 | Epoch: 136 | Iter: 207000 | Total Loss: 0.004251 | Recon Loss: 0.003638 | Commit Loss: 0.001226 | Perplexity: 1511.977316
2025-09-27 03:04:57,721 Stage: Train 0.5 | Epoch: 136 | Iter: 207200 | Total Loss: 0.004265 | Recon Loss: 0.003656 | Commit Loss: 0.001218 | Perplexity: 1507.687368
2025-09-27 03:05:41,536 Stage: Train 0.5 | Epoch: 136 | Iter: 207400 | Total Loss: 0.004112 | Recon Loss: 0.003498 | Commit Loss: 0.001228 | Perplexity: 1514.414659
2025-09-27 03:06:25,494 Stage: Train 0.5 | Epoch: 136 | Iter: 207600 | Total Loss: 0.004214 | Recon Loss: 0.003598 | Commit Loss: 0.001232 | Perplexity: 1517.895139
2025-09-27 03:07:09,336 Stage: Train 0.5 | Epoch: 136 | Iter: 207800 | Total Loss: 0.004377 | Recon Loss: 0.003763 | Commit Loss: 0.001227 | Perplexity: 1516.162655
2025-09-27 03:07:53,168 Stage: Train 0.5 | Epoch: 136 | Iter: 208000 | Total Loss: 0.004130 | Recon Loss: 0.003517 | Commit Loss: 0.001226 | Perplexity: 1516.936962
Trainning Epoch:  42%|████▏     | 137/330 [12:55:41<17:52:33, 333.44s/it]2025-09-27 03:08:37,217 Stage: Train 0.5 | Epoch: 137 | Iter: 208200 | Total Loss: 0.004221 | Recon Loss: 0.003610 | Commit Loss: 0.001223 | Perplexity: 1512.050690
2025-09-27 03:09:21,402 Stage: Train 0.5 | Epoch: 137 | Iter: 208400 | Total Loss: 0.004259 | Recon Loss: 0.003648 | Commit Loss: 0.001221 | Perplexity: 1514.726744
2025-09-27 03:10:05,368 Stage: Train 0.5 | Epoch: 137 | Iter: 208600 | Total Loss: 0.004294 | Recon Loss: 0.003684 | Commit Loss: 0.001220 | Perplexity: 1510.169332
2025-09-27 03:10:49,055 Stage: Train 0.5 | Epoch: 137 | Iter: 208800 | Total Loss: 0.004075 | Recon Loss: 0.003463 | Commit Loss: 0.001223 | Perplexity: 1513.033337
2025-09-27 03:11:32,659 Stage: Train 0.5 | Epoch: 137 | Iter: 209000 | Total Loss: 0.004265 | Recon Loss: 0.003651 | Commit Loss: 0.001228 | Perplexity: 1514.624635
2025-09-27 03:12:16,634 Stage: Train 0.5 | Epoch: 137 | Iter: 209200 | Total Loss: 0.004209 | Recon Loss: 0.003593 | Commit Loss: 0.001231 | Perplexity: 1514.242331
2025-09-27 03:13:00,710 Stage: Train 0.5 | Epoch: 137 | Iter: 209400 | Total Loss: 0.004208 | Recon Loss: 0.003596 | Commit Loss: 0.001222 | Perplexity: 1512.395637
2025-09-27 03:13:44,688 Stage: Train 0.5 | Epoch: 137 | Iter: 209600 | Total Loss: 0.004215 | Recon Loss: 0.003607 | Commit Loss: 0.001217 | Perplexity: 1512.098959
Trainning Epoch:  42%|████▏     | 138/330 [13:01:15<17:47:22, 333.55s/it]2025-09-27 03:14:28,774 Stage: Train 0.5 | Epoch: 138 | Iter: 209800 | Total Loss: 0.004252 | Recon Loss: 0.003646 | Commit Loss: 0.001213 | Perplexity: 1508.944485
2025-09-27 03:15:12,612 Stage: Train 0.5 | Epoch: 138 | Iter: 210000 | Total Loss: 0.004187 | Recon Loss: 0.003578 | Commit Loss: 0.001218 | Perplexity: 1513.254850
2025-09-27 03:15:56,527 Stage: Train 0.5 | Epoch: 138 | Iter: 210200 | Total Loss: 0.004142 | Recon Loss: 0.003531 | Commit Loss: 0.001221 | Perplexity: 1513.870936
2025-09-27 03:16:40,415 Stage: Train 0.5 | Epoch: 138 | Iter: 210400 | Total Loss: 0.004304 | Recon Loss: 0.003696 | Commit Loss: 0.001217 | Perplexity: 1512.622438
2025-09-27 03:17:24,362 Stage: Train 0.5 | Epoch: 138 | Iter: 210600 | Total Loss: 0.004142 | Recon Loss: 0.003527 | Commit Loss: 0.001228 | Perplexity: 1516.836129
2025-09-27 03:18:08,220 Stage: Train 0.5 | Epoch: 138 | Iter: 210800 | Total Loss: 0.004300 | Recon Loss: 0.003691 | Commit Loss: 0.001219 | Perplexity: 1514.856573
2025-09-27 03:18:51,818 Stage: Train 0.5 | Epoch: 138 | Iter: 211000 | Total Loss: 0.004144 | Recon Loss: 0.003527 | Commit Loss: 0.001234 | Perplexity: 1521.345212
Trainning Epoch:  42%|████▏     | 139/330 [13:06:48<17:41:34, 333.48s/it]2025-09-27 03:19:36,033 Stage: Train 0.5 | Epoch: 139 | Iter: 211200 | Total Loss: 0.004223 | Recon Loss: 0.003613 | Commit Loss: 0.001221 | Perplexity: 1514.333992
2025-09-27 03:20:20,009 Stage: Train 0.5 | Epoch: 139 | Iter: 211400 | Total Loss: 0.004233 | Recon Loss: 0.003629 | Commit Loss: 0.001210 | Perplexity: 1511.217568
2025-09-27 03:21:03,784 Stage: Train 0.5 | Epoch: 139 | Iter: 211600 | Total Loss: 0.004163 | Recon Loss: 0.003555 | Commit Loss: 0.001216 | Perplexity: 1512.971301
2025-09-27 03:21:47,356 Stage: Train 0.5 | Epoch: 139 | Iter: 211800 | Total Loss: 0.004255 | Recon Loss: 0.003646 | Commit Loss: 0.001219 | Perplexity: 1516.941832
2025-09-27 03:22:31,383 Stage: Train 0.5 | Epoch: 139 | Iter: 212000 | Total Loss: 0.004156 | Recon Loss: 0.003549 | Commit Loss: 0.001214 | Perplexity: 1507.156092
2025-09-27 03:23:15,270 Stage: Train 0.5 | Epoch: 139 | Iter: 212200 | Total Loss: 0.004279 | Recon Loss: 0.003670 | Commit Loss: 0.001218 | Perplexity: 1509.678214
2025-09-27 03:23:59,254 Stage: Train 0.5 | Epoch: 139 | Iter: 212400 | Total Loss: 0.004270 | Recon Loss: 0.003655 | Commit Loss: 0.001230 | Perplexity: 1516.600211
2025-09-27 03:24:43,223 Stage: Train 0.5 | Epoch: 139 | Iter: 212600 | Total Loss: 0.004232 | Recon Loss: 0.003626 | Commit Loss: 0.001212 | Perplexity: 1508.458345
Trainning Epoch:  42%|████▏     | 140/330 [13:12:22<17:36:04, 333.50s/it]2025-09-27 03:25:27,193 Stage: Train 0.5 | Epoch: 140 | Iter: 212800 | Total Loss: 0.004155 | Recon Loss: 0.003551 | Commit Loss: 0.001208 | Perplexity: 1510.295373
2025-09-27 03:26:11,155 Stage: Train 0.5 | Epoch: 140 | Iter: 213000 | Total Loss: 0.004198 | Recon Loss: 0.003587 | Commit Loss: 0.001221 | Perplexity: 1512.256158
2025-09-27 03:26:55,096 Stage: Train 0.5 | Epoch: 140 | Iter: 213200 | Total Loss: 0.004272 | Recon Loss: 0.003664 | Commit Loss: 0.001216 | Perplexity: 1514.474609
2025-09-27 03:27:38,934 Stage: Train 0.5 | Epoch: 140 | Iter: 213400 | Total Loss: 0.004096 | Recon Loss: 0.003489 | Commit Loss: 0.001214 | Perplexity: 1509.255904
2025-09-27 03:28:22,892 Stage: Train 0.5 | Epoch: 140 | Iter: 213600 | Total Loss: 0.004194 | Recon Loss: 0.003586 | Commit Loss: 0.001215 | Perplexity: 1514.112568
2025-09-27 03:29:06,612 Stage: Train 0.5 | Epoch: 140 | Iter: 213800 | Total Loss: 0.004155 | Recon Loss: 0.003548 | Commit Loss: 0.001214 | Perplexity: 1515.565616
2025-09-27 03:29:50,563 Stage: Train 0.5 | Epoch: 140 | Iter: 214000 | Total Loss: 0.004204 | Recon Loss: 0.003593 | Commit Loss: 0.001221 | Perplexity: 1516.204601
Trainning Epoch:  43%|████▎     | 141/330 [13:17:56<17:30:31, 333.50s/it]2025-09-27 03:30:34,666 Stage: Train 0.5 | Epoch: 141 | Iter: 214200 | Total Loss: 0.004286 | Recon Loss: 0.003675 | Commit Loss: 0.001221 | Perplexity: 1511.459008
2025-09-27 03:31:18,677 Stage: Train 0.5 | Epoch: 141 | Iter: 214400 | Total Loss: 0.004090 | Recon Loss: 0.003483 | Commit Loss: 0.001214 | Perplexity: 1514.242861
2025-09-27 03:32:02,492 Stage: Train 0.5 | Epoch: 141 | Iter: 214600 | Total Loss: 0.004192 | Recon Loss: 0.003587 | Commit Loss: 0.001209 | Perplexity: 1509.782843
2025-09-27 03:32:45,436 Stage: Train 0.5 | Epoch: 141 | Iter: 214800 | Total Loss: 0.004141 | Recon Loss: 0.003535 | Commit Loss: 0.001212 | Perplexity: 1509.122736
2025-09-27 03:33:29,350 Stage: Train 0.5 | Epoch: 141 | Iter: 215000 | Total Loss: 0.004150 | Recon Loss: 0.003542 | Commit Loss: 0.001216 | Perplexity: 1509.568472
2025-09-27 03:34:13,330 Stage: Train 0.5 | Epoch: 141 | Iter: 215200 | Total Loss: 0.004228 | Recon Loss: 0.003622 | Commit Loss: 0.001212 | Perplexity: 1512.788567
2025-09-27 03:34:57,339 Stage: Train 0.5 | Epoch: 141 | Iter: 215400 | Total Loss: 0.004219 | Recon Loss: 0.003607 | Commit Loss: 0.001223 | Perplexity: 1513.161627
2025-09-27 03:35:41,056 Stage: Train 0.5 | Epoch: 141 | Iter: 215600 | Total Loss: 0.004185 | Recon Loss: 0.003576 | Commit Loss: 0.001219 | Perplexity: 1518.453124
Trainning Epoch:  43%|████▎     | 142/330 [13:23:28<17:24:22, 333.31s/it]2025-09-27 03:36:25,395 Stage: Train 0.5 | Epoch: 142 | Iter: 215800 | Total Loss: 0.004144 | Recon Loss: 0.003532 | Commit Loss: 0.001223 | Perplexity: 1518.631547
2025-09-27 03:37:09,359 Stage: Train 0.5 | Epoch: 142 | Iter: 216000 | Total Loss: 0.004170 | Recon Loss: 0.003563 | Commit Loss: 0.001214 | Perplexity: 1513.153931
2025-09-27 03:37:53,475 Stage: Train 0.5 | Epoch: 142 | Iter: 216200 | Total Loss: 0.004152 | Recon Loss: 0.003549 | Commit Loss: 0.001206 | Perplexity: 1508.826953
2025-09-27 03:38:37,679 Stage: Train 0.5 | Epoch: 142 | Iter: 216400 | Total Loss: 0.004340 | Recon Loss: 0.003732 | Commit Loss: 0.001216 | Perplexity: 1514.006672
2025-09-27 03:39:21,317 Stage: Train 0.5 | Epoch: 142 | Iter: 216600 | Total Loss: 0.004108 | Recon Loss: 0.003503 | Commit Loss: 0.001210 | Perplexity: 1511.510323
2025-09-27 03:40:05,149 Stage: Train 0.5 | Epoch: 142 | Iter: 216800 | Total Loss: 0.004117 | Recon Loss: 0.003510 | Commit Loss: 0.001215 | Perplexity: 1512.661050
2025-09-27 03:40:49,369 Stage: Train 0.5 | Epoch: 142 | Iter: 217000 | Total Loss: 0.004155 | Recon Loss: 0.003546 | Commit Loss: 0.001219 | Perplexity: 1515.981490
2025-09-27 03:41:33,275 Stage: Train 0.5 | Epoch: 142 | Iter: 217200 | Total Loss: 0.004216 | Recon Loss: 0.003611 | Commit Loss: 0.001211 | Perplexity: 1512.839376
Trainning Epoch:  43%|████▎     | 143/330 [13:29:03<17:19:41, 333.59s/it]2025-09-27 03:42:17,241 Stage: Train 0.5 | Epoch: 143 | Iter: 217400 | Total Loss: 0.004158 | Recon Loss: 0.003551 | Commit Loss: 0.001215 | Perplexity: 1513.564865
2025-09-27 03:43:01,339 Stage: Train 0.5 | Epoch: 143 | Iter: 217600 | Total Loss: 0.004126 | Recon Loss: 0.003521 | Commit Loss: 0.001210 | Perplexity: 1512.206469
2025-09-27 03:43:45,101 Stage: Train 0.5 | Epoch: 143 | Iter: 217800 | Total Loss: 0.004171 | Recon Loss: 0.003566 | Commit Loss: 0.001211 | Perplexity: 1517.090491
2025-09-27 03:44:29,113 Stage: Train 0.5 | Epoch: 143 | Iter: 218000 | Total Loss: 0.004189 | Recon Loss: 0.003587 | Commit Loss: 0.001204 | Perplexity: 1510.424013
2025-09-27 03:45:13,105 Stage: Train 0.5 | Epoch: 143 | Iter: 218200 | Total Loss: 0.004164 | Recon Loss: 0.003555 | Commit Loss: 0.001218 | Perplexity: 1510.216199
2025-09-27 03:45:56,785 Stage: Train 0.5 | Epoch: 143 | Iter: 218400 | Total Loss: 0.004269 | Recon Loss: 0.003664 | Commit Loss: 0.001210 | Perplexity: 1510.467960
2025-09-27 03:46:40,807 Stage: Train 0.5 | Epoch: 143 | Iter: 218600 | Total Loss: 0.004181 | Recon Loss: 0.003574 | Commit Loss: 0.001215 | Perplexity: 1516.756237
Trainning Epoch:  44%|████▎     | 144/330 [13:34:36<17:14:19, 333.65s/it]2025-09-27 03:47:25,077 Stage: Train 0.5 | Epoch: 144 | Iter: 218800 | Total Loss: 0.004090 | Recon Loss: 0.003485 | Commit Loss: 0.001210 | Perplexity: 1506.498165
2025-09-27 03:48:09,106 Stage: Train 0.5 | Epoch: 144 | Iter: 219000 | Total Loss: 0.004155 | Recon Loss: 0.003550 | Commit Loss: 0.001211 | Perplexity: 1513.884017
2025-09-27 03:48:52,829 Stage: Train 0.5 | Epoch: 144 | Iter: 219200 | Total Loss: 0.004090 | Recon Loss: 0.003485 | Commit Loss: 0.001211 | Perplexity: 1514.676385
2025-09-27 03:49:36,851 Stage: Train 0.5 | Epoch: 144 | Iter: 219400 | Total Loss: 0.004151 | Recon Loss: 0.003547 | Commit Loss: 0.001207 | Perplexity: 1512.054417
2025-09-27 03:50:20,690 Stage: Train 0.5 | Epoch: 144 | Iter: 219600 | Total Loss: 0.004379 | Recon Loss: 0.003778 | Commit Loss: 0.001201 | Perplexity: 1509.695373
2025-09-27 03:51:04,654 Stage: Train 0.5 | Epoch: 144 | Iter: 219800 | Total Loss: 0.004077 | Recon Loss: 0.003470 | Commit Loss: 0.001214 | Perplexity: 1512.329318
2025-09-27 03:51:48,619 Stage: Train 0.5 | Epoch: 144 | Iter: 220000 | Total Loss: 0.004212 | Recon Loss: 0.003610 | Commit Loss: 0.001205 | Perplexity: 1513.402680
2025-09-27 03:51:48,619 Saving model at iteration 220000
2025-09-27 03:51:49,027 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000
2025-09-27 03:51:49,340 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/model.safetensors
2025-09-27 03:51:49,741 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/optimizer.bin
2025-09-27 03:51:49,742 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/scheduler.bin
2025-09-27 03:51:49,742 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/sampler.bin
2025-09-27 03:51:49,743 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/random_states_0.pkl
2025-09-27 03:52:33,830 Stage: Train 0.5 | Epoch: 144 | Iter: 220200 | Total Loss: 0.004118 | Recon Loss: 0.003509 | Commit Loss: 0.001217 | Perplexity: 1515.885390
Trainning Epoch:  44%|████▍     | 145/330 [13:40:12<17:10:11, 334.12s/it]2025-09-27 03:53:18,138 Stage: Train 0.5 | Epoch: 145 | Iter: 220400 | Total Loss: 0.004041 | Recon Loss: 0.003433 | Commit Loss: 0.001216 | Perplexity: 1516.975013
2025-09-27 03:54:02,081 Stage: Train 0.5 | Epoch: 145 | Iter: 220600 | Total Loss: 0.004425 | Recon Loss: 0.003822 | Commit Loss: 0.001206 | Perplexity: 1513.029930
2025-09-27 03:54:46,195 Stage: Train 0.5 | Epoch: 145 | Iter: 220800 | Total Loss: 0.003996 | Recon Loss: 0.003394 | Commit Loss: 0.001203 | Perplexity: 1514.138165
2025-09-27 03:55:30,201 Stage: Train 0.5 | Epoch: 145 | Iter: 221000 | Total Loss: 0.004087 | Recon Loss: 0.003476 | Commit Loss: 0.001222 | Perplexity: 1520.027684
2025-09-27 03:56:13,839 Stage: Train 0.5 | Epoch: 145 | Iter: 221200 | Total Loss: 0.004256 | Recon Loss: 0.003651 | Commit Loss: 0.001209 | Perplexity: 1511.810040
2025-09-27 03:56:57,923 Stage: Train 0.5 | Epoch: 145 | Iter: 221400 | Total Loss: 0.004131 | Recon Loss: 0.003526 | Commit Loss: 0.001210 | Perplexity: 1511.198016
2025-09-27 03:57:41,888 Stage: Train 0.5 | Epoch: 145 | Iter: 221600 | Total Loss: 0.004116 | Recon Loss: 0.003515 | Commit Loss: 0.001202 | Perplexity: 1513.037125
Trainning Epoch:  44%|████▍     | 146/330 [13:45:46<17:04:40, 334.13s/it]2025-09-27 03:58:26,042 Stage: Train 0.5 | Epoch: 146 | Iter: 221800 | Total Loss: 0.004089 | Recon Loss: 0.003488 | Commit Loss: 0.001202 | Perplexity: 1511.802350
2025-09-27 03:59:09,899 Stage: Train 0.5 | Epoch: 146 | Iter: 222000 | Total Loss: 0.004078 | Recon Loss: 0.003474 | Commit Loss: 0.001208 | Perplexity: 1512.418732
2025-09-27 03:59:53,879 Stage: Train 0.5 | Epoch: 146 | Iter: 222200 | Total Loss: 0.004092 | Recon Loss: 0.003490 | Commit Loss: 0.001203 | Perplexity: 1509.143111
2025-09-27 04:00:37,879 Stage: Train 0.5 | Epoch: 146 | Iter: 222400 | Total Loss: 0.004121 | Recon Loss: 0.003514 | Commit Loss: 0.001213 | Perplexity: 1512.892452
2025-09-27 04:01:21,983 Stage: Train 0.5 | Epoch: 146 | Iter: 222600 | Total Loss: 0.004111 | Recon Loss: 0.003509 | Commit Loss: 0.001204 | Perplexity: 1511.931777
2025-09-27 04:02:06,064 Stage: Train 0.5 | Epoch: 146 | Iter: 222800 | Total Loss: 0.004106 | Recon Loss: 0.003506 | Commit Loss: 0.001200 | Perplexity: 1514.137280
2025-09-27 04:02:49,801 Stage: Train 0.5 | Epoch: 146 | Iter: 223000 | Total Loss: 0.004164 | Recon Loss: 0.003554 | Commit Loss: 0.001219 | Perplexity: 1522.569760
2025-09-27 04:03:33,612 Stage: Train 0.5 | Epoch: 146 | Iter: 223200 | Total Loss: 0.004115 | Recon Loss: 0.003511 | Commit Loss: 0.001207 | Perplexity: 1514.534487
Trainning Epoch:  45%|████▍     | 147/330 [13:51:20<16:58:56, 334.08s/it]2025-09-27 04:04:17,802 Stage: Train 0.5 | Epoch: 147 | Iter: 223400 | Total Loss: 0.004060 | Recon Loss: 0.003454 | Commit Loss: 0.001213 | Perplexity: 1519.969083
2025-09-27 04:05:01,606 Stage: Train 0.5 | Epoch: 147 | Iter: 223600 | Total Loss: 0.004128 | Recon Loss: 0.003526 | Commit Loss: 0.001204 | Perplexity: 1514.350142
2025-09-27 04:05:45,355 Stage: Train 0.5 | Epoch: 147 | Iter: 223800 | Total Loss: 0.004185 | Recon Loss: 0.003582 | Commit Loss: 0.001206 | Perplexity: 1517.856685
2025-09-27 04:06:29,520 Stage: Train 0.5 | Epoch: 147 | Iter: 224000 | Total Loss: 0.004122 | Recon Loss: 0.003519 | Commit Loss: 0.001206 | Perplexity: 1515.360563
2025-09-27 04:07:13,472 Stage: Train 0.5 | Epoch: 147 | Iter: 224200 | Total Loss: 0.004213 | Recon Loss: 0.003610 | Commit Loss: 0.001207 | Perplexity: 1513.422557
2025-09-27 04:07:57,423 Stage: Train 0.5 | Epoch: 147 | Iter: 224400 | Total Loss: 0.004101 | Recon Loss: 0.003496 | Commit Loss: 0.001210 | Perplexity: 1517.450338
2025-09-27 04:08:41,354 Stage: Train 0.5 | Epoch: 147 | Iter: 224600 | Total Loss: 0.004085 | Recon Loss: 0.003483 | Commit Loss: 0.001203 | Perplexity: 1514.959813
2025-09-27 04:09:25,125 Stage: Train 0.5 | Epoch: 147 | Iter: 224800 | Total Loss: 0.004104 | Recon Loss: 0.003501 | Commit Loss: 0.001207 | Perplexity: 1516.508165
Trainning Epoch:  45%|████▍     | 148/330 [13:56:53<16:53:01, 333.96s/it]2025-09-27 04:10:09,570 Stage: Train 0.5 | Epoch: 148 | Iter: 225000 | Total Loss: 0.004168 | Recon Loss: 0.003566 | Commit Loss: 0.001204 | Perplexity: 1515.879167
2025-09-27 04:10:53,718 Stage: Train 0.5 | Epoch: 148 | Iter: 225200 | Total Loss: 0.004142 | Recon Loss: 0.003543 | Commit Loss: 0.001197 | Perplexity: 1514.114858
2025-09-27 04:11:37,821 Stage: Train 0.5 | Epoch: 148 | Iter: 225400 | Total Loss: 0.004143 | Recon Loss: 0.003536 | Commit Loss: 0.001213 | Perplexity: 1518.901213
2025-09-27 04:12:21,675 Stage: Train 0.5 | Epoch: 148 | Iter: 225600 | Total Loss: 0.004027 | Recon Loss: 0.003426 | Commit Loss: 0.001201 | Perplexity: 1518.409593
2025-09-27 04:13:05,803 Stage: Train 0.5 | Epoch: 148 | Iter: 225800 | Total Loss: 0.004224 | Recon Loss: 0.003626 | Commit Loss: 0.001195 | Perplexity: 1509.072982
2025-09-27 04:13:49,953 Stage: Train 0.5 | Epoch: 148 | Iter: 226000 | Total Loss: 0.004135 | Recon Loss: 0.003534 | Commit Loss: 0.001204 | Perplexity: 1516.128948
2025-09-27 04:14:34,029 Stage: Train 0.5 | Epoch: 148 | Iter: 226200 | Total Loss: 0.004151 | Recon Loss: 0.003552 | Commit Loss: 0.001198 | Perplexity: 1515.089134
Trainning Epoch:  45%|████▌     | 149/330 [14:02:29<16:48:32, 334.33s/it]2025-09-27 04:15:18,491 Stage: Train 0.5 | Epoch: 149 | Iter: 226400 | Total Loss: 0.004077 | Recon Loss: 0.003475 | Commit Loss: 0.001205 | Perplexity: 1515.675325
2025-09-27 04:16:02,318 Stage: Train 0.5 | Epoch: 149 | Iter: 226600 | Total Loss: 0.004226 | Recon Loss: 0.003628 | Commit Loss: 0.001196 | Perplexity: 1516.422267
2025-09-27 04:16:46,558 Stage: Train 0.5 | Epoch: 149 | Iter: 226800 | Total Loss: 0.004110 | Recon Loss: 0.003509 | Commit Loss: 0.001200 | Perplexity: 1512.791375
2025-09-27 04:17:29,190 Stage: Train 0.5 | Epoch: 149 | Iter: 227000 | Total Loss: 0.004102 | Recon Loss: 0.003500 | Commit Loss: 0.001204 | Perplexity: 1514.922451
2025-09-27 04:18:13,243 Stage: Train 0.5 | Epoch: 149 | Iter: 227200 | Total Loss: 0.004088 | Recon Loss: 0.003486 | Commit Loss: 0.001203 | Perplexity: 1517.165078
2025-09-27 04:18:57,454 Stage: Train 0.5 | Epoch: 149 | Iter: 227400 | Total Loss: 0.004210 | Recon Loss: 0.003607 | Commit Loss: 0.001205 | Perplexity: 1513.705212
2025-09-27 04:19:41,312 Stage: Train 0.5 | Epoch: 149 | Iter: 227600 | Total Loss: 0.004049 | Recon Loss: 0.003449 | Commit Loss: 0.001199 | Perplexity: 1514.123514
2025-09-27 04:20:25,363 Stage: Train 0.5 | Epoch: 149 | Iter: 227800 | Total Loss: 0.004105 | Recon Loss: 0.003506 | Commit Loss: 0.001199 | Perplexity: 1506.770246
Trainning Epoch:  45%|████▌     | 150/330 [14:08:02<16:42:09, 334.05s/it]2025-09-27 04:21:09,660 Stage: Train 0.5 | Epoch: 150 | Iter: 228000 | Total Loss: 0.004118 | Recon Loss: 0.003524 | Commit Loss: 0.001187 | Perplexity: 1507.084265
2025-09-27 04:21:53,603 Stage: Train 0.5 | Epoch: 150 | Iter: 228200 | Total Loss: 0.004078 | Recon Loss: 0.003475 | Commit Loss: 0.001205 | Perplexity: 1515.465793
2025-09-27 04:22:37,648 Stage: Train 0.5 | Epoch: 150 | Iter: 228400 | Total Loss: 0.004174 | Recon Loss: 0.003575 | Commit Loss: 0.001198 | Perplexity: 1515.549367
2025-09-27 04:23:21,300 Stage: Train 0.5 | Epoch: 150 | Iter: 228600 | Total Loss: 0.004010 | Recon Loss: 0.003412 | Commit Loss: 0.001196 | Perplexity: 1515.223161
2025-09-27 04:24:05,246 Stage: Train 0.5 | Epoch: 150 | Iter: 228800 | Total Loss: 0.004209 | Recon Loss: 0.003607 | Commit Loss: 0.001203 | Perplexity: 1513.905316
2025-09-27 04:24:49,145 Stage: Train 0.5 | Epoch: 150 | Iter: 229000 | Total Loss: 0.004062 | Recon Loss: 0.003462 | Commit Loss: 0.001200 | Perplexity: 1513.069305
2025-09-27 04:25:33,141 Stage: Train 0.5 | Epoch: 150 | Iter: 229200 | Total Loss: 0.004185 | Recon Loss: 0.003585 | Commit Loss: 0.001200 | Perplexity: 1512.291134
Trainning Epoch:  46%|████▌     | 151/330 [14:13:36<16:36:11, 333.92s/it]2025-09-27 04:26:16,968 Stage: Train 0.5 | Epoch: 151 | Iter: 229400 | Total Loss: 0.004009 | Recon Loss: 0.003407 | Commit Loss: 0.001204 | Perplexity: 1518.293202
2025-09-27 04:27:00,840 Stage: Train 0.5 | Epoch: 151 | Iter: 229600 | Total Loss: 0.004021 | Recon Loss: 0.003425 | Commit Loss: 0.001193 | Perplexity: 1514.307443
2025-09-27 04:27:44,828 Stage: Train 0.5 | Epoch: 151 | Iter: 229800 | Total Loss: 0.004100 | Recon Loss: 0.003499 | Commit Loss: 0.001203 | Perplexity: 1515.716890
2025-09-27 04:28:29,039 Stage: Train 0.5 | Epoch: 151 | Iter: 230000 | Total Loss: 0.004033 | Recon Loss: 0.003434 | Commit Loss: 0.001198 | Perplexity: 1514.107144
2025-09-27 04:29:12,870 Stage: Train 0.5 | Epoch: 151 | Iter: 230200 | Total Loss: 0.004196 | Recon Loss: 0.003593 | Commit Loss: 0.001208 | Perplexity: 1516.668796
2025-09-27 04:29:56,416 Stage: Train 0.5 | Epoch: 151 | Iter: 230400 | Total Loss: 0.004110 | Recon Loss: 0.003513 | Commit Loss: 0.001195 | Perplexity: 1516.866108
2025-09-27 04:30:40,169 Stage: Train 0.5 | Epoch: 151 | Iter: 230600 | Total Loss: 0.004041 | Recon Loss: 0.003443 | Commit Loss: 0.001195 | Perplexity: 1515.613643
2025-09-27 04:31:24,120 Stage: Train 0.5 | Epoch: 151 | Iter: 230800 | Total Loss: 0.004041 | Recon Loss: 0.003440 | Commit Loss: 0.001203 | Perplexity: 1519.089368
Trainning Epoch:  46%|████▌     | 152/330 [14:19:09<16:30:15, 333.80s/it]2025-09-27 04:32:08,380 Stage: Train 0.5 | Epoch: 152 | Iter: 231000 | Total Loss: 0.004048 | Recon Loss: 0.003448 | Commit Loss: 0.001201 | Perplexity: 1519.620874
2025-09-27 04:32:52,191 Stage: Train 0.5 | Epoch: 152 | Iter: 231200 | Total Loss: 0.004171 | Recon Loss: 0.003574 | Commit Loss: 0.001193 | Perplexity: 1517.273093
2025-09-27 04:33:36,115 Stage: Train 0.5 | Epoch: 152 | Iter: 231400 | Total Loss: 0.004054 | Recon Loss: 0.003460 | Commit Loss: 0.001188 | Perplexity: 1512.884270
2025-09-27 04:34:19,983 Stage: Train 0.5 | Epoch: 152 | Iter: 231600 | Total Loss: 0.004071 | Recon Loss: 0.003474 | Commit Loss: 0.001193 | Perplexity: 1514.852920
2025-09-27 04:35:03,819 Stage: Train 0.5 | Epoch: 152 | Iter: 231800 | Total Loss: 0.004129 | Recon Loss: 0.003532 | Commit Loss: 0.001195 | Perplexity: 1512.778564
2025-09-27 04:35:47,708 Stage: Train 0.5 | Epoch: 152 | Iter: 232000 | Total Loss: 0.004069 | Recon Loss: 0.003469 | Commit Loss: 0.001199 | Perplexity: 1516.186119
2025-09-27 04:36:31,518 Stage: Train 0.5 | Epoch: 152 | Iter: 232200 | Total Loss: 0.004142 | Recon Loss: 0.003541 | Commit Loss: 0.001202 | Perplexity: 1515.189666
2025-09-27 04:37:15,491 Stage: Train 0.5 | Epoch: 152 | Iter: 232400 | Total Loss: 0.004063 | Recon Loss: 0.003468 | Commit Loss: 0.001190 | Perplexity: 1512.780513
Trainning Epoch:  46%|████▋     | 153/330 [14:24:43<16:24:28, 333.72s/it]2025-09-27 04:37:59,636 Stage: Train 0.5 | Epoch: 153 | Iter: 232600 | Total Loss: 0.004066 | Recon Loss: 0.003468 | Commit Loss: 0.001196 | Perplexity: 1518.353321
2025-09-27 04:38:43,775 Stage: Train 0.5 | Epoch: 153 | Iter: 232800 | Total Loss: 0.004146 | Recon Loss: 0.003548 | Commit Loss: 0.001195 | Perplexity: 1516.731738
2025-09-27 04:39:27,777 Stage: Train 0.5 | Epoch: 153 | Iter: 233000 | Total Loss: 0.004015 | Recon Loss: 0.003417 | Commit Loss: 0.001196 | Perplexity: 1516.297044
2025-09-27 04:40:11,427 Stage: Train 0.5 | Epoch: 153 | Iter: 233200 | Total Loss: 0.004108 | Recon Loss: 0.003516 | Commit Loss: 0.001185 | Perplexity: 1512.885748
2025-09-27 04:40:55,244 Stage: Train 0.5 | Epoch: 153 | Iter: 233400 | Total Loss: 0.004059 | Recon Loss: 0.003459 | Commit Loss: 0.001201 | Perplexity: 1516.342880
2025-09-27 04:41:39,088 Stage: Train 0.5 | Epoch: 153 | Iter: 233600 | Total Loss: 0.004015 | Recon Loss: 0.003423 | Commit Loss: 0.001183 | Perplexity: 1515.710351
2025-09-27 04:42:22,975 Stage: Train 0.5 | Epoch: 153 | Iter: 233800 | Total Loss: 0.004221 | Recon Loss: 0.003626 | Commit Loss: 0.001191 | Perplexity: 1514.811115
Trainning Epoch:  47%|████▋     | 154/330 [14:30:16<16:18:48, 333.69s/it]2025-09-27 04:43:06,910 Stage: Train 0.5 | Epoch: 154 | Iter: 234000 | Total Loss: 0.004015 | Recon Loss: 0.003420 | Commit Loss: 0.001190 | Perplexity: 1516.575506
2025-09-27 04:43:50,923 Stage: Train 0.5 | Epoch: 154 | Iter: 234200 | Total Loss: 0.003972 | Recon Loss: 0.003379 | Commit Loss: 0.001187 | Perplexity: 1512.971619
2025-09-27 04:44:34,814 Stage: Train 0.5 | Epoch: 154 | Iter: 234400 | Total Loss: 0.004208 | Recon Loss: 0.003611 | Commit Loss: 0.001194 | Perplexity: 1515.348741
2025-09-27 04:45:18,807 Stage: Train 0.5 | Epoch: 154 | Iter: 234600 | Total Loss: 0.004016 | Recon Loss: 0.003418 | Commit Loss: 0.001196 | Perplexity: 1522.679348
2025-09-27 04:46:02,760 Stage: Train 0.5 | Epoch: 154 | Iter: 234800 | Total Loss: 0.004117 | Recon Loss: 0.003520 | Commit Loss: 0.001193 | Perplexity: 1518.865508
2025-09-27 04:46:46,597 Stage: Train 0.5 | Epoch: 154 | Iter: 235000 | Total Loss: 0.004026 | Recon Loss: 0.003431 | Commit Loss: 0.001189 | Perplexity: 1516.203972
2025-09-27 04:47:30,523 Stage: Train 0.5 | Epoch: 154 | Iter: 235200 | Total Loss: 0.004050 | Recon Loss: 0.003453 | Commit Loss: 0.001194 | Perplexity: 1516.820276
2025-09-27 04:48:14,338 Stage: Train 0.5 | Epoch: 154 | Iter: 235400 | Total Loss: 0.004029 | Recon Loss: 0.003432 | Commit Loss: 0.001196 | Perplexity: 1518.799041
Trainning Epoch:  47%|████▋     | 155/330 [14:35:50<16:13:09, 333.65s/it]2025-09-27 04:48:58,458 Stage: Train 0.5 | Epoch: 155 | Iter: 235600 | Total Loss: 0.004098 | Recon Loss: 0.003499 | Commit Loss: 0.001197 | Perplexity: 1514.857842
2025-09-27 04:49:42,377 Stage: Train 0.5 | Epoch: 155 | Iter: 235800 | Total Loss: 0.004055 | Recon Loss: 0.003458 | Commit Loss: 0.001193 | Perplexity: 1516.424522
2025-09-27 04:50:26,146 Stage: Train 0.5 | Epoch: 155 | Iter: 236000 | Total Loss: 0.004165 | Recon Loss: 0.003569 | Commit Loss: 0.001193 | Perplexity: 1519.406863
2025-09-27 04:51:10,200 Stage: Train 0.5 | Epoch: 155 | Iter: 236200 | Total Loss: 0.004047 | Recon Loss: 0.003453 | Commit Loss: 0.001186 | Perplexity: 1514.094121
2025-09-27 04:51:54,226 Stage: Train 0.5 | Epoch: 155 | Iter: 236400 | Total Loss: 0.004045 | Recon Loss: 0.003453 | Commit Loss: 0.001184 | Perplexity: 1518.638415
2025-09-27 04:52:38,398 Stage: Train 0.5 | Epoch: 155 | Iter: 236600 | Total Loss: 0.004056 | Recon Loss: 0.003461 | Commit Loss: 0.001189 | Perplexity: 1513.580005
2025-09-27 04:53:22,168 Stage: Train 0.5 | Epoch: 155 | Iter: 236800 | Total Loss: 0.004090 | Recon Loss: 0.003498 | Commit Loss: 0.001184 | Perplexity: 1517.335804
Trainning Epoch:  47%|████▋     | 156/330 [14:41:24<16:08:00, 333.80s/it]2025-09-27 04:54:06,530 Stage: Train 0.5 | Epoch: 156 | Iter: 237000 | Total Loss: 0.004075 | Recon Loss: 0.003480 | Commit Loss: 0.001189 | Perplexity: 1514.810435
2025-09-27 04:54:50,607 Stage: Train 0.5 | Epoch: 156 | Iter: 237200 | Total Loss: 0.004003 | Recon Loss: 0.003410 | Commit Loss: 0.001187 | Perplexity: 1519.504199
2025-09-27 04:55:34,531 Stage: Train 0.5 | Epoch: 156 | Iter: 237400 | Total Loss: 0.004060 | Recon Loss: 0.003465 | Commit Loss: 0.001189 | Perplexity: 1517.848628
2025-09-27 04:56:18,410 Stage: Train 0.5 | Epoch: 156 | Iter: 237600 | Total Loss: 0.004019 | Recon Loss: 0.003429 | Commit Loss: 0.001181 | Perplexity: 1520.020659
2025-09-27 04:57:02,268 Stage: Train 0.5 | Epoch: 156 | Iter: 237800 | Total Loss: 0.004010 | Recon Loss: 0.003417 | Commit Loss: 0.001186 | Perplexity: 1516.908446
2025-09-27 04:57:46,345 Stage: Train 0.5 | Epoch: 156 | Iter: 238000 | Total Loss: 0.004150 | Recon Loss: 0.003555 | Commit Loss: 0.001190 | Perplexity: 1515.433159
2025-09-27 04:58:30,354 Stage: Train 0.5 | Epoch: 156 | Iter: 238200 | Total Loss: 0.004045 | Recon Loss: 0.003452 | Commit Loss: 0.001186 | Perplexity: 1518.121022
2025-09-27 04:59:14,233 Stage: Train 0.5 | Epoch: 156 | Iter: 238400 | Total Loss: 0.004086 | Recon Loss: 0.003495 | Commit Loss: 0.001181 | Perplexity: 1512.803911
Trainning Epoch:  48%|████▊     | 157/330 [14:46:58<16:02:38, 333.87s/it]2025-09-27 04:59:58,260 Stage: Train 0.5 | Epoch: 157 | Iter: 238600 | Total Loss: 0.004148 | Recon Loss: 0.003559 | Commit Loss: 0.001178 | Perplexity: 1514.029974
2025-09-27 05:00:42,195 Stage: Train 0.5 | Epoch: 157 | Iter: 238800 | Total Loss: 0.004054 | Recon Loss: 0.003462 | Commit Loss: 0.001183 | Perplexity: 1517.355018
2025-09-27 05:01:26,022 Stage: Train 0.5 | Epoch: 157 | Iter: 239000 | Total Loss: 0.003992 | Recon Loss: 0.003398 | Commit Loss: 0.001188 | Perplexity: 1519.038281
2025-09-27 05:02:09,218 Stage: Train 0.5 | Epoch: 157 | Iter: 239200 | Total Loss: 0.004004 | Recon Loss: 0.003412 | Commit Loss: 0.001184 | Perplexity: 1516.847007
2025-09-27 05:02:53,401 Stage: Train 0.5 | Epoch: 157 | Iter: 239400 | Total Loss: 0.004092 | Recon Loss: 0.003500 | Commit Loss: 0.001184 | Perplexity: 1516.608521
2025-09-27 05:03:37,119 Stage: Train 0.5 | Epoch: 157 | Iter: 239600 | Total Loss: 0.004060 | Recon Loss: 0.003466 | Commit Loss: 0.001189 | Perplexity: 1519.406799
2025-09-27 05:04:21,046 Stage: Train 0.5 | Epoch: 157 | Iter: 239800 | Total Loss: 0.003980 | Recon Loss: 0.003392 | Commit Loss: 0.001176 | Perplexity: 1515.713505
2025-09-27 05:05:05,076 Stage: Train 0.5 | Epoch: 157 | Iter: 240000 | Total Loss: 0.004094 | Recon Loss: 0.003500 | Commit Loss: 0.001187 | Perplexity: 1516.066931
2025-09-27 05:05:05,076 Saving model at iteration 240000
2025-09-27 05:05:05,587 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000
2025-09-27 05:05:05,837 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/model.safetensors
2025-09-27 05:05:06,215 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/optimizer.bin
2025-09-27 05:05:06,216 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/scheduler.bin
2025-09-27 05:05:06,216 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/sampler.bin
2025-09-27 05:05:06,217 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 158/330 [14:52:33<15:57:44, 334.09s/it]2025-09-27 05:05:51,061 Stage: Train 0.5 | Epoch: 158 | Iter: 240200 | Total Loss: 0.004060 | Recon Loss: 0.003468 | Commit Loss: 0.001184 | Perplexity: 1519.058680
2025-09-27 05:06:34,919 Stage: Train 0.5 | Epoch: 158 | Iter: 240400 | Total Loss: 0.004037 | Recon Loss: 0.003448 | Commit Loss: 0.001178 | Perplexity: 1514.882046
2025-09-27 05:07:18,714 Stage: Train 0.5 | Epoch: 158 | Iter: 240600 | Total Loss: 0.004013 | Recon Loss: 0.003419 | Commit Loss: 0.001188 | Perplexity: 1523.255346
2025-09-27 05:08:02,697 Stage: Train 0.5 | Epoch: 158 | Iter: 240800 | Total Loss: 0.003978 | Recon Loss: 0.003389 | Commit Loss: 0.001177 | Perplexity: 1512.554928
2025-09-27 05:08:46,351 Stage: Train 0.5 | Epoch: 158 | Iter: 241000 | Total Loss: 0.004095 | Recon Loss: 0.003505 | Commit Loss: 0.001180 | Perplexity: 1513.478128
2025-09-27 05:09:30,250 Stage: Train 0.5 | Epoch: 158 | Iter: 241200 | Total Loss: 0.004036 | Recon Loss: 0.003448 | Commit Loss: 0.001178 | Perplexity: 1512.133654
2025-09-27 05:10:13,962 Stage: Train 0.5 | Epoch: 158 | Iter: 241400 | Total Loss: 0.003983 | Recon Loss: 0.003389 | Commit Loss: 0.001187 | Perplexity: 1519.368965
Trainning Epoch:  48%|████▊     | 159/330 [14:58:06<15:51:41, 333.93s/it]2025-09-27 05:10:58,236 Stage: Train 0.5 | Epoch: 159 | Iter: 241600 | Total Loss: 0.004018 | Recon Loss: 0.003426 | Commit Loss: 0.001184 | Perplexity: 1518.263255
2025-09-27 05:11:42,170 Stage: Train 0.5 | Epoch: 159 | Iter: 241800 | Total Loss: 0.004082 | Recon Loss: 0.003496 | Commit Loss: 0.001172 | Perplexity: 1514.953129
2025-09-27 05:12:25,874 Stage: Train 0.5 | Epoch: 159 | Iter: 242000 | Total Loss: 0.004008 | Recon Loss: 0.003421 | Commit Loss: 0.001175 | Perplexity: 1516.491561
2025-09-27 05:13:09,684 Stage: Train 0.5 | Epoch: 159 | Iter: 242200 | Total Loss: 0.004064 | Recon Loss: 0.003474 | Commit Loss: 0.001181 | Perplexity: 1520.330558
2025-09-27 05:13:53,422 Stage: Train 0.5 | Epoch: 159 | Iter: 242400 | Total Loss: 0.004051 | Recon Loss: 0.003462 | Commit Loss: 0.001180 | Perplexity: 1514.511821
2025-09-27 05:14:37,432 Stage: Train 0.5 | Epoch: 159 | Iter: 242600 | Total Loss: 0.004036 | Recon Loss: 0.003445 | Commit Loss: 0.001183 | Perplexity: 1515.087537
2025-09-27 05:15:21,502 Stage: Train 0.5 | Epoch: 159 | Iter: 242800 | Total Loss: 0.003965 | Recon Loss: 0.003374 | Commit Loss: 0.001183 | Perplexity: 1522.696437
2025-09-27 05:16:05,437 Stage: Train 0.5 | Epoch: 159 | Iter: 243000 | Total Loss: 0.004008 | Recon Loss: 0.003415 | Commit Loss: 0.001187 | Perplexity: 1523.165911
Trainning Epoch:  48%|████▊     | 160/330 [15:03:40<15:45:57, 333.87s/it]2025-09-27 05:16:49,599 Stage: Train 0.5 | Epoch: 160 | Iter: 243200 | Total Loss: 0.004035 | Recon Loss: 0.003445 | Commit Loss: 0.001178 | Perplexity: 1516.024145
2025-09-27 05:17:33,419 Stage: Train 0.5 | Epoch: 160 | Iter: 243400 | Total Loss: 0.003982 | Recon Loss: 0.003396 | Commit Loss: 0.001172 | Perplexity: 1514.133701
2025-09-27 05:18:17,331 Stage: Train 0.5 | Epoch: 160 | Iter: 243600 | Total Loss: 0.004107 | Recon Loss: 0.003517 | Commit Loss: 0.001181 | Perplexity: 1516.501235
2025-09-27 05:19:01,454 Stage: Train 0.5 | Epoch: 160 | Iter: 243800 | Total Loss: 0.004001 | Recon Loss: 0.003409 | Commit Loss: 0.001183 | Perplexity: 1522.506946
2025-09-27 05:19:45,617 Stage: Train 0.5 | Epoch: 160 | Iter: 244000 | Total Loss: 0.004081 | Recon Loss: 0.003493 | Commit Loss: 0.001176 | Perplexity: 1517.569152
2025-09-27 05:20:29,520 Stage: Train 0.5 | Epoch: 160 | Iter: 244200 | Total Loss: 0.004057 | Recon Loss: 0.003465 | Commit Loss: 0.001184 | Perplexity: 1521.870970
2025-09-27 05:21:13,549 Stage: Train 0.5 | Epoch: 160 | Iter: 244400 | Total Loss: 0.004019 | Recon Loss: 0.003429 | Commit Loss: 0.001180 | Perplexity: 1520.477498
Trainning Epoch:  49%|████▉     | 161/330 [15:09:14<15:40:40, 333.96s/it]2025-09-27 05:21:57,778 Stage: Train 0.5 | Epoch: 161 | Iter: 244600 | Total Loss: 0.004029 | Recon Loss: 0.003439 | Commit Loss: 0.001179 | Perplexity: 1515.728920
2025-09-27 05:22:41,780 Stage: Train 0.5 | Epoch: 161 | Iter: 244800 | Total Loss: 0.004114 | Recon Loss: 0.003522 | Commit Loss: 0.001184 | Perplexity: 1519.284863
2025-09-27 05:23:25,821 Stage: Train 0.5 | Epoch: 161 | Iter: 245000 | Total Loss: 0.003919 | Recon Loss: 0.003332 | Commit Loss: 0.001173 | Perplexity: 1518.222132
2025-09-27 05:24:09,502 Stage: Train 0.5 | Epoch: 161 | Iter: 245200 | Total Loss: 0.004063 | Recon Loss: 0.003477 | Commit Loss: 0.001172 | Perplexity: 1517.196486
2025-09-27 05:24:53,525 Stage: Train 0.5 | Epoch: 161 | Iter: 245400 | Total Loss: 0.003970 | Recon Loss: 0.003382 | Commit Loss: 0.001176 | Perplexity: 1521.009951
2025-09-27 05:25:37,385 Stage: Train 0.5 | Epoch: 161 | Iter: 245600 | Total Loss: 0.004052 | Recon Loss: 0.003465 | Commit Loss: 0.001175 | Perplexity: 1516.991411
2025-09-27 05:26:21,232 Stage: Train 0.5 | Epoch: 161 | Iter: 245800 | Total Loss: 0.003987 | Recon Loss: 0.003401 | Commit Loss: 0.001171 | Perplexity: 1512.002501
2025-09-27 05:27:04,808 Stage: Train 0.5 | Epoch: 161 | Iter: 246000 | Total Loss: 0.003945 | Recon Loss: 0.003354 | Commit Loss: 0.001181 | Perplexity: 1522.894990
Trainning Epoch:  49%|████▉     | 162/330 [15:14:48<15:34:48, 333.86s/it]2025-09-27 05:27:49,071 Stage: Train 0.5 | Epoch: 162 | Iter: 246200 | Total Loss: 0.004044 | Recon Loss: 0.003458 | Commit Loss: 0.001172 | Perplexity: 1520.286272
2025-09-27 05:28:33,081 Stage: Train 0.5 | Epoch: 162 | Iter: 246400 | Total Loss: 0.004021 | Recon Loss: 0.003427 | Commit Loss: 0.001188 | Perplexity: 1521.612102
2025-09-27 05:29:16,815 Stage: Train 0.5 | Epoch: 162 | Iter: 246600 | Total Loss: 0.003955 | Recon Loss: 0.003370 | Commit Loss: 0.001170 | Perplexity: 1514.170803
2025-09-27 05:30:00,807 Stage: Train 0.5 | Epoch: 162 | Iter: 246800 | Total Loss: 0.004002 | Recon Loss: 0.003415 | Commit Loss: 0.001173 | Perplexity: 1518.832132
2025-09-27 05:30:44,620 Stage: Train 0.5 | Epoch: 162 | Iter: 247000 | Total Loss: 0.004022 | Recon Loss: 0.003438 | Commit Loss: 0.001168 | Perplexity: 1517.628704
2025-09-27 05:31:28,509 Stage: Train 0.5 | Epoch: 162 | Iter: 247200 | Total Loss: 0.004010 | Recon Loss: 0.003418 | Commit Loss: 0.001183 | Perplexity: 1514.310089
2025-09-27 05:32:12,398 Stage: Train 0.5 | Epoch: 162 | Iter: 247400 | Total Loss: 0.003998 | Recon Loss: 0.003411 | Commit Loss: 0.001174 | Perplexity: 1517.336800
Trainning Epoch:  49%|████▉     | 163/330 [15:20:21<15:28:58, 333.76s/it]2025-09-27 05:32:56,536 Stage: Train 0.5 | Epoch: 163 | Iter: 247600 | Total Loss: 0.003980 | Recon Loss: 0.003387 | Commit Loss: 0.001185 | Perplexity: 1522.769681
2025-09-27 05:33:40,437 Stage: Train 0.5 | Epoch: 163 | Iter: 247800 | Total Loss: 0.004009 | Recon Loss: 0.003425 | Commit Loss: 0.001168 | Perplexity: 1516.883365
2025-09-27 05:34:24,349 Stage: Train 0.5 | Epoch: 163 | Iter: 248000 | Total Loss: 0.004019 | Recon Loss: 0.003431 | Commit Loss: 0.001176 | Perplexity: 1519.214279
2025-09-27 05:35:08,095 Stage: Train 0.5 | Epoch: 163 | Iter: 248200 | Total Loss: 0.004006 | Recon Loss: 0.003420 | Commit Loss: 0.001173 | Perplexity: 1518.465863
2025-09-27 05:35:52,246 Stage: Train 0.5 | Epoch: 163 | Iter: 248400 | Total Loss: 0.004015 | Recon Loss: 0.003428 | Commit Loss: 0.001174 | Perplexity: 1521.632988
2025-09-27 05:36:36,285 Stage: Train 0.5 | Epoch: 163 | Iter: 248600 | Total Loss: 0.004027 | Recon Loss: 0.003435 | Commit Loss: 0.001183 | Perplexity: 1521.009506
2025-09-27 05:37:20,136 Stage: Train 0.5 | Epoch: 163 | Iter: 248800 | Total Loss: 0.004044 | Recon Loss: 0.003462 | Commit Loss: 0.001166 | Perplexity: 1516.377630
2025-09-27 05:38:04,235 Stage: Train 0.5 | Epoch: 163 | Iter: 249000 | Total Loss: 0.004011 | Recon Loss: 0.003424 | Commit Loss: 0.001173 | Perplexity: 1519.819020
Trainning Epoch:  50%|████▉     | 164/330 [15:25:55<15:23:40, 333.86s/it]2025-09-27 05:38:48,465 Stage: Train 0.5 | Epoch: 164 | Iter: 249200 | Total Loss: 0.004011 | Recon Loss: 0.003428 | Commit Loss: 0.001166 | Perplexity: 1515.321620
2025-09-27 05:39:32,314 Stage: Train 0.5 | Epoch: 164 | Iter: 249400 | Total Loss: 0.003985 | Recon Loss: 0.003402 | Commit Loss: 0.001166 | Perplexity: 1514.603403
2025-09-27 05:40:16,215 Stage: Train 0.5 | Epoch: 164 | Iter: 249600 | Total Loss: 0.004023 | Recon Loss: 0.003439 | Commit Loss: 0.001169 | Perplexity: 1519.374804
2025-09-27 05:40:59,794 Stage: Train 0.5 | Epoch: 164 | Iter: 249800 | Total Loss: 0.003955 | Recon Loss: 0.003368 | Commit Loss: 0.001173 | Perplexity: 1515.481947
2025-09-27 05:41:43,800 Stage: Train 0.5 | Epoch: 164 | Iter: 250000 | Total Loss: 0.004032 | Recon Loss: 0.003446 | Commit Loss: 0.001173 | Perplexity: 1522.388445
2025-09-27 05:42:27,838 Stage: Train 0.5 | Epoch: 164 | Iter: 250200 | Total Loss: 0.003937 | Recon Loss: 0.003349 | Commit Loss: 0.001176 | Perplexity: 1525.936788
2025-09-27 05:43:11,680 Stage: Train 0.5 | Epoch: 164 | Iter: 250400 | Total Loss: 0.004004 | Recon Loss: 0.003418 | Commit Loss: 0.001173 | Perplexity: 1519.416080
2025-09-27 05:43:55,607 Stage: Train 0.5 | Epoch: 164 | Iter: 250600 | Total Loss: 0.004037 | Recon Loss: 0.003456 | Commit Loss: 0.001161 | Perplexity: 1514.318407
Trainning Epoch:  50%|█████     | 165/330 [15:31:29<15:17:45, 333.73s/it]2025-09-27 05:44:39,484 Stage: Train 0.5 | Epoch: 165 | Iter: 250800 | Total Loss: 0.003984 | Recon Loss: 0.003395 | Commit Loss: 0.001176 | Perplexity: 1524.894150
2025-09-27 05:45:23,408 Stage: Train 0.5 | Epoch: 165 | Iter: 251000 | Total Loss: 0.004000 | Recon Loss: 0.003420 | Commit Loss: 0.001161 | Perplexity: 1516.023148
2025-09-27 05:46:07,536 Stage: Train 0.5 | Epoch: 165 | Iter: 251200 | Total Loss: 0.004065 | Recon Loss: 0.003483 | Commit Loss: 0.001165 | Perplexity: 1517.183694
2025-09-27 05:46:50,449 Stage: Train 0.5 | Epoch: 165 | Iter: 251400 | Total Loss: 0.003917 | Recon Loss: 0.003332 | Commit Loss: 0.001170 | Perplexity: 1518.342719
2025-09-27 05:47:34,099 Stage: Train 0.5 | Epoch: 165 | Iter: 251600 | Total Loss: 0.003990 | Recon Loss: 0.003404 | Commit Loss: 0.001173 | Perplexity: 1517.532906
2025-09-27 05:48:18,059 Stage: Train 0.5 | Epoch: 165 | Iter: 251800 | Total Loss: 0.004065 | Recon Loss: 0.003482 | Commit Loss: 0.001167 | Perplexity: 1518.407867
2025-09-27 05:49:02,131 Stage: Train 0.5 | Epoch: 165 | Iter: 252000 | Total Loss: 0.003904 | Recon Loss: 0.003322 | Commit Loss: 0.001165 | Perplexity: 1518.719986
Trainning Epoch:  50%|█████     | 166/330 [15:37:02<15:11:30, 333.48s/it]2025-09-27 05:49:46,325 Stage: Train 0.5 | Epoch: 166 | Iter: 252200 | Total Loss: 0.004033 | Recon Loss: 0.003451 | Commit Loss: 0.001165 | Perplexity: 1512.378420
2025-09-27 05:50:30,294 Stage: Train 0.5 | Epoch: 166 | Iter: 252400 | Total Loss: 0.004072 | Recon Loss: 0.003490 | Commit Loss: 0.001164 | Perplexity: 1517.659033
2025-09-27 05:51:13,983 Stage: Train 0.5 | Epoch: 166 | Iter: 252600 | Total Loss: 0.003962 | Recon Loss: 0.003378 | Commit Loss: 0.001166 | Perplexity: 1517.806106
2025-09-27 05:51:57,993 Stage: Train 0.5 | Epoch: 166 | Iter: 252800 | Total Loss: 0.004001 | Recon Loss: 0.003417 | Commit Loss: 0.001169 | Perplexity: 1519.136509
2025-09-27 05:52:41,963 Stage: Train 0.5 | Epoch: 166 | Iter: 253000 | Total Loss: 0.003933 | Recon Loss: 0.003346 | Commit Loss: 0.001174 | Perplexity: 1524.083710
2025-09-27 05:53:25,835 Stage: Train 0.5 | Epoch: 166 | Iter: 253200 | Total Loss: 0.003972 | Recon Loss: 0.003390 | Commit Loss: 0.001163 | Perplexity: 1518.519315
2025-09-27 05:54:09,785 Stage: Train 0.5 | Epoch: 166 | Iter: 253400 | Total Loss: 0.003992 | Recon Loss: 0.003409 | Commit Loss: 0.001165 | Perplexity: 1519.486722
2025-09-27 05:54:53,469 Stage: Train 0.5 | Epoch: 166 | Iter: 253600 | Total Loss: 0.003972 | Recon Loss: 0.003389 | Commit Loss: 0.001166 | Perplexity: 1516.796801
Trainning Epoch:  51%|█████     | 167/330 [15:42:35<15:05:55, 333.47s/it]2025-09-27 05:55:37,578 Stage: Train 0.5 | Epoch: 167 | Iter: 253800 | Total Loss: 0.003906 | Recon Loss: 0.003323 | Commit Loss: 0.001165 | Perplexity: 1517.721509
2025-09-27 05:56:21,534 Stage: Train 0.5 | Epoch: 167 | Iter: 254000 | Total Loss: 0.003927 | Recon Loss: 0.003342 | Commit Loss: 0.001170 | Perplexity: 1520.368645
2025-09-27 05:57:05,486 Stage: Train 0.5 | Epoch: 167 | Iter: 254200 | Total Loss: 0.004026 | Recon Loss: 0.003441 | Commit Loss: 0.001170 | Perplexity: 1522.687658
2025-09-27 05:57:49,117 Stage: Train 0.5 | Epoch: 167 | Iter: 254400 | Total Loss: 0.003952 | Recon Loss: 0.003373 | Commit Loss: 0.001158 | Perplexity: 1519.743869
2025-09-27 05:58:33,063 Stage: Train 0.5 | Epoch: 167 | Iter: 254600 | Total Loss: 0.003939 | Recon Loss: 0.003358 | Commit Loss: 0.001162 | Perplexity: 1516.021426
2025-09-27 05:59:16,995 Stage: Train 0.5 | Epoch: 167 | Iter: 254800 | Total Loss: 0.003926 | Recon Loss: 0.003341 | Commit Loss: 0.001169 | Perplexity: 1517.739435
2025-09-27 06:00:00,890 Stage: Train 0.5 | Epoch: 167 | Iter: 255000 | Total Loss: 0.003941 | Recon Loss: 0.003356 | Commit Loss: 0.001169 | Perplexity: 1519.615432
Trainning Epoch:  51%|█████     | 168/330 [15:48:09<15:00:21, 333.47s/it]2025-09-27 06:00:44,933 Stage: Train 0.5 | Epoch: 168 | Iter: 255200 | Total Loss: 0.004113 | Recon Loss: 0.003527 | Commit Loss: 0.001171 | Perplexity: 1521.519784
2025-09-27 06:01:28,348 Stage: Train 0.5 | Epoch: 168 | Iter: 255400 | Total Loss: 0.003926 | Recon Loss: 0.003347 | Commit Loss: 0.001158 | Perplexity: 1518.489407
2025-09-27 06:02:12,156 Stage: Train 0.5 | Epoch: 168 | Iter: 255600 | Total Loss: 0.004028 | Recon Loss: 0.003445 | Commit Loss: 0.001166 | Perplexity: 1515.382758
2025-09-27 06:02:56,096 Stage: Train 0.5 | Epoch: 168 | Iter: 255800 | Total Loss: 0.003896 | Recon Loss: 0.003313 | Commit Loss: 0.001166 | Perplexity: 1520.556525
2025-09-27 06:03:39,852 Stage: Train 0.5 | Epoch: 168 | Iter: 256000 | Total Loss: 0.003967 | Recon Loss: 0.003383 | Commit Loss: 0.001167 | Perplexity: 1519.830299
2025-09-27 06:04:23,385 Stage: Train 0.5 | Epoch: 168 | Iter: 256200 | Total Loss: 0.003976 | Recon Loss: 0.003392 | Commit Loss: 0.001168 | Perplexity: 1517.884456
2025-09-27 06:05:07,256 Stage: Train 0.5 | Epoch: 168 | Iter: 256400 | Total Loss: 0.003907 | Recon Loss: 0.003323 | Commit Loss: 0.001168 | Perplexity: 1518.383846
2025-09-27 06:05:51,062 Stage: Train 0.5 | Epoch: 168 | Iter: 256600 | Total Loss: 0.004021 | Recon Loss: 0.003441 | Commit Loss: 0.001159 | Perplexity: 1516.958487
Trainning Epoch:  51%|█████     | 169/330 [15:53:41<14:54:02, 333.18s/it]2025-09-27 06:06:35,108 Stage: Train 0.5 | Epoch: 169 | Iter: 256800 | Total Loss: 0.003885 | Recon Loss: 0.003304 | Commit Loss: 0.001162 | Perplexity: 1513.392047
2025-09-27 06:07:19,065 Stage: Train 0.5 | Epoch: 169 | Iter: 257000 | Total Loss: 0.003922 | Recon Loss: 0.003340 | Commit Loss: 0.001165 | Perplexity: 1520.468005
2025-09-27 06:08:02,787 Stage: Train 0.5 | Epoch: 169 | Iter: 257200 | Total Loss: 0.003926 | Recon Loss: 0.003343 | Commit Loss: 0.001165 | Perplexity: 1518.803595
2025-09-27 06:08:46,716 Stage: Train 0.5 | Epoch: 169 | Iter: 257400 | Total Loss: 0.003895 | Recon Loss: 0.003316 | Commit Loss: 0.001158 | Perplexity: 1517.892372
2025-09-27 06:09:30,806 Stage: Train 0.5 | Epoch: 169 | Iter: 257600 | Total Loss: 0.003960 | Recon Loss: 0.003379 | Commit Loss: 0.001163 | Perplexity: 1518.505955
2025-09-27 06:10:14,939 Stage: Train 0.5 | Epoch: 169 | Iter: 257800 | Total Loss: 0.003993 | Recon Loss: 0.003412 | Commit Loss: 0.001163 | Perplexity: 1516.698517
2025-09-27 06:10:58,778 Stage: Train 0.5 | Epoch: 169 | Iter: 258000 | Total Loss: 0.003937 | Recon Loss: 0.003354 | Commit Loss: 0.001167 | Perplexity: 1521.318177
2025-09-27 06:11:42,501 Stage: Train 0.5 | Epoch: 169 | Iter: 258200 | Total Loss: 0.003910 | Recon Loss: 0.003324 | Commit Loss: 0.001173 | Perplexity: 1523.831163
Trainning Epoch:  52%|█████▏    | 170/330 [15:59:15<14:48:50, 333.32s/it]2025-09-27 06:12:26,561 Stage: Train 0.5 | Epoch: 170 | Iter: 258400 | Total Loss: 0.004009 | Recon Loss: 0.003426 | Commit Loss: 0.001166 | Perplexity: 1518.627758
2025-09-27 06:13:10,533 Stage: Train 0.5 | Epoch: 170 | Iter: 258600 | Total Loss: 0.003980 | Recon Loss: 0.003401 | Commit Loss: 0.001158 | Perplexity: 1516.269424
2025-09-27 06:13:54,452 Stage: Train 0.5 | Epoch: 170 | Iter: 258800 | Total Loss: 0.003855 | Recon Loss: 0.003275 | Commit Loss: 0.001159 | Perplexity: 1518.305072
2025-09-27 06:14:38,217 Stage: Train 0.5 | Epoch: 170 | Iter: 259000 | Total Loss: 0.003950 | Recon Loss: 0.003367 | Commit Loss: 0.001165 | Perplexity: 1514.417367
2025-09-27 06:15:22,234 Stage: Train 0.5 | Epoch: 170 | Iter: 259200 | Total Loss: 0.003975 | Recon Loss: 0.003392 | Commit Loss: 0.001165 | Perplexity: 1518.024921
2025-09-27 06:16:06,198 Stage: Train 0.5 | Epoch: 170 | Iter: 259400 | Total Loss: 0.003920 | Recon Loss: 0.003342 | Commit Loss: 0.001156 | Perplexity: 1521.244338
2025-09-27 06:16:50,069 Stage: Train 0.5 | Epoch: 170 | Iter: 259600 | Total Loss: 0.003956 | Recon Loss: 0.003369 | Commit Loss: 0.001175 | Perplexity: 1522.100012
Trainning Epoch:  52%|█████▏    | 171/330 [16:04:48<14:43:28, 333.39s/it]2025-09-27 06:17:34,184 Stage: Train 0.5 | Epoch: 171 | Iter: 259800 | Total Loss: 0.003941 | Recon Loss: 0.003363 | Commit Loss: 0.001156 | Perplexity: 1516.943110
2025-09-27 06:18:17,969 Stage: Train 0.5 | Epoch: 171 | Iter: 260000 | Total Loss: 0.003988 | Recon Loss: 0.003407 | Commit Loss: 0.001161 | Perplexity: 1519.580743
2025-09-27 06:18:17,969 Saving model at iteration 260000
2025-09-27 06:18:18,405 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000
2025-09-27 06:18:18,696 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/model.safetensors
2025-09-27 06:18:19,095 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/optimizer.bin
2025-09-27 06:18:19,096 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/scheduler.bin
2025-09-27 06:18:19,096 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/sampler.bin
2025-09-27 06:18:19,097 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/random_states_0.pkl
2025-09-27 06:19:03,352 Stage: Train 0.5 | Epoch: 171 | Iter: 260200 | Total Loss: 0.003868 | Recon Loss: 0.003290 | Commit Loss: 0.001156 | Perplexity: 1519.039490
2025-09-27 06:19:47,299 Stage: Train 0.5 | Epoch: 171 | Iter: 260400 | Total Loss: 0.004058 | Recon Loss: 0.003476 | Commit Loss: 0.001163 | Perplexity: 1516.695908
2025-09-27 06:20:31,453 Stage: Train 0.5 | Epoch: 171 | Iter: 260600 | Total Loss: 0.003992 | Recon Loss: 0.003409 | Commit Loss: 0.001167 | Perplexity: 1519.809442
2025-09-27 06:21:15,398 Stage: Train 0.5 | Epoch: 171 | Iter: 260800 | Total Loss: 0.003981 | Recon Loss: 0.003404 | Commit Loss: 0.001155 | Perplexity: 1515.371569
2025-09-27 06:21:59,140 Stage: Train 0.5 | Epoch: 171 | Iter: 261000 | Total Loss: 0.003899 | Recon Loss: 0.003320 | Commit Loss: 0.001158 | Perplexity: 1519.904666
2025-09-27 06:22:43,142 Stage: Train 0.5 | Epoch: 171 | Iter: 261200 | Total Loss: 0.003892 | Recon Loss: 0.003311 | Commit Loss: 0.001162 | Perplexity: 1524.326922
Trainning Epoch:  52%|█████▏    | 172/330 [16:10:24<14:39:35, 334.02s/it]2025-09-27 06:23:27,354 Stage: Train 0.5 | Epoch: 172 | Iter: 261400 | Total Loss: 0.004112 | Recon Loss: 0.003531 | Commit Loss: 0.001161 | Perplexity: 1523.459433
2025-09-27 06:24:11,576 Stage: Train 0.5 | Epoch: 172 | Iter: 261600 | Total Loss: 0.003819 | Recon Loss: 0.003242 | Commit Loss: 0.001153 | Perplexity: 1518.920754
2025-09-27 06:24:55,275 Stage: Train 0.5 | Epoch: 172 | Iter: 261800 | Total Loss: 0.003980 | Recon Loss: 0.003401 | Commit Loss: 0.001159 | Perplexity: 1516.800125
2025-09-27 06:25:39,367 Stage: Train 0.5 | Epoch: 172 | Iter: 262000 | Total Loss: 0.003961 | Recon Loss: 0.003385 | Commit Loss: 0.001152 | Perplexity: 1520.102894
2025-09-27 06:26:23,174 Stage: Train 0.5 | Epoch: 172 | Iter: 262200 | Total Loss: 0.003925 | Recon Loss: 0.003347 | Commit Loss: 0.001155 | Perplexity: 1517.667495
2025-09-27 06:26:54,578 Stage: Train 0.5 | Epoch: 172 | Iter: 262400 | Total Loss: 0.003892 | Recon Loss: 0.003310 | Commit Loss: 0.001164 | Perplexity: 1518.528311
2025-09-27 06:27:15,380 Stage: Train 0.5 | Epoch: 172 | Iter: 262600 | Total Loss: 0.004051 | Recon Loss: 0.003472 | Commit Loss: 0.001159 | Perplexity: 1516.945336
Trainning Epoch:  52%|█████▏    | 173/330 [16:15:01<13:49:12, 316.90s/it]2025-09-27 06:27:36,659 Stage: Train 0.5 | Epoch: 173 | Iter: 262800 | Total Loss: 0.003974 | Recon Loss: 0.003399 | Commit Loss: 0.001150 | Perplexity: 1518.203859
2025-09-27 06:27:57,323 Stage: Train 0.5 | Epoch: 173 | Iter: 263000 | Total Loss: 0.004007 | Recon Loss: 0.003430 | Commit Loss: 0.001154 | Perplexity: 1521.094902
2025-09-27 06:28:18,438 Stage: Train 0.5 | Epoch: 173 | Iter: 263200 | Total Loss: 0.003858 | Recon Loss: 0.003279 | Commit Loss: 0.001158 | Perplexity: 1518.446124
2025-09-27 06:28:39,246 Stage: Train 0.5 | Epoch: 173 | Iter: 263400 | Total Loss: 0.003892 | Recon Loss: 0.003315 | Commit Loss: 0.001155 | Perplexity: 1518.561563
2025-09-27 06:29:00,081 Stage: Train 0.5 | Epoch: 173 | Iter: 263600 | Total Loss: 0.003925 | Recon Loss: 0.003345 | Commit Loss: 0.001161 | Perplexity: 1524.481519
2025-09-27 06:29:21,036 Stage: Train 0.5 | Epoch: 173 | Iter: 263800 | Total Loss: 0.003923 | Recon Loss: 0.003342 | Commit Loss: 0.001161 | Perplexity: 1522.386758
2025-09-27 06:29:42,065 Stage: Train 0.5 | Epoch: 173 | Iter: 264000 | Total Loss: 0.003915 | Recon Loss: 0.003337 | Commit Loss: 0.001156 | Perplexity: 1515.149423
2025-09-27 06:30:02,958 Stage: Train 0.5 | Epoch: 173 | Iter: 264200 | Total Loss: 0.003977 | Recon Loss: 0.003399 | Commit Loss: 0.001155 | Perplexity: 1518.277220
Trainning Epoch:  53%|█████▎    | 174/330 [16:17:40<11:40:55, 269.58s/it]2025-09-27 06:30:24,416 Stage: Train 0.5 | Epoch: 174 | Iter: 264400 | Total Loss: 0.003889 | Recon Loss: 0.003312 | Commit Loss: 0.001153 | Perplexity: 1515.697574
2025-09-27 06:30:45,105 Stage: Train 0.5 | Epoch: 174 | Iter: 264600 | Total Loss: 0.003952 | Recon Loss: 0.003375 | Commit Loss: 0.001152 | Perplexity: 1522.414070
2025-09-27 06:31:06,166 Stage: Train 0.5 | Epoch: 174 | Iter: 264800 | Total Loss: 0.003899 | Recon Loss: 0.003319 | Commit Loss: 0.001160 | Perplexity: 1519.124966
2025-09-27 06:31:27,169 Stage: Train 0.5 | Epoch: 174 | Iter: 265000 | Total Loss: 0.003941 | Recon Loss: 0.003362 | Commit Loss: 0.001158 | Perplexity: 1524.030474
2025-09-27 06:31:48,391 Stage: Train 0.5 | Epoch: 174 | Iter: 265200 | Total Loss: 0.003867 | Recon Loss: 0.003290 | Commit Loss: 0.001155 | Perplexity: 1519.244952
2025-09-27 06:32:09,668 Stage: Train 0.5 | Epoch: 174 | Iter: 265400 | Total Loss: 0.003936 | Recon Loss: 0.003357 | Commit Loss: 0.001157 | Perplexity: 1520.984900
2025-09-27 06:32:30,911 Stage: Train 0.5 | Epoch: 174 | Iter: 265600 | Total Loss: 0.003943 | Recon Loss: 0.003365 | Commit Loss: 0.001157 | Perplexity: 1520.090271
2025-09-27 06:32:52,135 Stage: Train 0.5 | Epoch: 174 | Iter: 265800 | Total Loss: 0.003990 | Recon Loss: 0.003415 | Commit Loss: 0.001149 | Perplexity: 1513.268506
Trainning Epoch:  53%|█████▎    | 175/330 [16:20:20<10:11:53, 236.86s/it]2025-09-27 06:33:13,347 Stage: Train 0.5 | Epoch: 175 | Iter: 266000 | Total Loss: 0.003867 | Recon Loss: 0.003294 | Commit Loss: 0.001146 | Perplexity: 1520.682875
2025-09-27 06:33:34,637 Stage: Train 0.5 | Epoch: 175 | Iter: 266200 | Total Loss: 0.003888 | Recon Loss: 0.003315 | Commit Loss: 0.001148 | Perplexity: 1522.650699
2025-09-27 06:33:55,537 Stage: Train 0.5 | Epoch: 175 | Iter: 266400 | Total Loss: 0.003951 | Recon Loss: 0.003374 | Commit Loss: 0.001154 | Perplexity: 1518.614264
2025-09-27 06:34:16,777 Stage: Train 0.5 | Epoch: 175 | Iter: 266600 | Total Loss: 0.003854 | Recon Loss: 0.003275 | Commit Loss: 0.001157 | Perplexity: 1521.782142
2025-09-27 06:34:37,972 Stage: Train 0.5 | Epoch: 175 | Iter: 266800 | Total Loss: 0.003954 | Recon Loss: 0.003377 | Commit Loss: 0.001154 | Perplexity: 1516.331930
2025-09-27 06:34:59,160 Stage: Train 0.5 | Epoch: 175 | Iter: 267000 | Total Loss: 0.003840 | Recon Loss: 0.003264 | Commit Loss: 0.001151 | Perplexity: 1521.670333
2025-09-27 06:35:20,321 Stage: Train 0.5 | Epoch: 175 | Iter: 267200 | Total Loss: 0.003992 | Recon Loss: 0.003412 | Commit Loss: 0.001159 | Perplexity: 1520.847327
Trainning Epoch:  53%|█████▎    | 176/330 [16:23:01<9:09:23, 214.05s/it] 2025-09-27 06:35:41,736 Stage: Train 0.5 | Epoch: 176 | Iter: 267400 | Total Loss: 0.003952 | Recon Loss: 0.003373 | Commit Loss: 0.001158 | Perplexity: 1516.938074
2025-09-27 06:36:02,831 Stage: Train 0.5 | Epoch: 176 | Iter: 267600 | Total Loss: 0.003919 | Recon Loss: 0.003345 | Commit Loss: 0.001147 | Perplexity: 1518.911497
2025-09-27 06:36:24,056 Stage: Train 0.5 | Epoch: 176 | Iter: 267800 | Total Loss: 0.003887 | Recon Loss: 0.003309 | Commit Loss: 0.001157 | Perplexity: 1520.719648
2025-09-27 06:36:45,272 Stage: Train 0.5 | Epoch: 176 | Iter: 268000 | Total Loss: 0.003897 | Recon Loss: 0.003321 | Commit Loss: 0.001152 | Perplexity: 1518.260089
2025-09-27 06:37:06,399 Stage: Train 0.5 | Epoch: 176 | Iter: 268200 | Total Loss: 0.004009 | Recon Loss: 0.003433 | Commit Loss: 0.001152 | Perplexity: 1516.528421
2025-09-27 06:37:27,597 Stage: Train 0.5 | Epoch: 176 | Iter: 268400 | Total Loss: 0.003814 | Recon Loss: 0.003239 | Commit Loss: 0.001150 | Perplexity: 1519.941442
2025-09-27 06:37:48,742 Stage: Train 0.5 | Epoch: 176 | Iter: 268600 | Total Loss: 0.003859 | Recon Loss: 0.003278 | Commit Loss: 0.001162 | Perplexity: 1523.582897
2025-09-27 06:38:09,869 Stage: Train 0.5 | Epoch: 176 | Iter: 268800 | Total Loss: 0.003966 | Recon Loss: 0.003387 | Commit Loss: 0.001156 | Perplexity: 1518.849348
Trainning Epoch:  54%|█████▎    | 177/330 [16:25:42<8:25:10, 198.11s/it]2025-09-27 06:38:31,192 Stage: Train 0.5 | Epoch: 177 | Iter: 269000 | Total Loss: 0.003900 | Recon Loss: 0.003325 | Commit Loss: 0.001150 | Perplexity: 1521.910555
2025-09-27 06:38:52,204 Stage: Train 0.5 | Epoch: 177 | Iter: 269200 | Total Loss: 0.003841 | Recon Loss: 0.003266 | Commit Loss: 0.001150 | Perplexity: 1517.907862
2025-09-27 06:39:13,084 Stage: Train 0.5 | Epoch: 177 | Iter: 269400 | Total Loss: 0.003971 | Recon Loss: 0.003395 | Commit Loss: 0.001153 | Perplexity: 1520.705386
2025-09-27 06:39:34,106 Stage: Train 0.5 | Epoch: 177 | Iter: 269600 | Total Loss: 0.004020 | Recon Loss: 0.003446 | Commit Loss: 0.001148 | Perplexity: 1515.391661
2025-09-27 06:39:55,119 Stage: Train 0.5 | Epoch: 177 | Iter: 269800 | Total Loss: 0.003818 | Recon Loss: 0.003238 | Commit Loss: 0.001160 | Perplexity: 1523.618882
2025-09-27 06:40:16,484 Stage: Train 0.5 | Epoch: 177 | Iter: 270000 | Total Loss: 0.003886 | Recon Loss: 0.003316 | Commit Loss: 0.001141 | Perplexity: 1518.386245
2025-09-27 06:40:37,497 Stage: Train 0.5 | Epoch: 177 | Iter: 270200 | Total Loss: 0.003888 | Recon Loss: 0.003311 | Commit Loss: 0.001154 | Perplexity: 1522.653668
Trainning Epoch:  54%|█████▍    | 178/330 [16:28:22<7:52:53, 186.67s/it]2025-09-27 06:40:58,560 Stage: Train 0.5 | Epoch: 178 | Iter: 270400 | Total Loss: 0.003920 | Recon Loss: 0.003348 | Commit Loss: 0.001145 | Perplexity: 1519.620828
2025-09-27 06:41:19,541 Stage: Train 0.5 | Epoch: 178 | Iter: 270600 | Total Loss: 0.003845 | Recon Loss: 0.003273 | Commit Loss: 0.001144 | Perplexity: 1520.658369
2025-09-27 06:41:40,557 Stage: Train 0.5 | Epoch: 178 | Iter: 270800 | Total Loss: 0.003892 | Recon Loss: 0.003316 | Commit Loss: 0.001151 | Perplexity: 1518.289188
2025-09-27 06:42:01,543 Stage: Train 0.5 | Epoch: 178 | Iter: 271000 | Total Loss: 0.003908 | Recon Loss: 0.003334 | Commit Loss: 0.001149 | Perplexity: 1523.252039
2025-09-27 06:42:22,283 Stage: Train 0.5 | Epoch: 178 | Iter: 271200 | Total Loss: 0.003982 | Recon Loss: 0.003407 | Commit Loss: 0.001150 | Perplexity: 1522.259437
2025-09-27 06:42:43,339 Stage: Train 0.5 | Epoch: 178 | Iter: 271400 | Total Loss: 0.003873 | Recon Loss: 0.003302 | Commit Loss: 0.001142 | Perplexity: 1516.405404
2025-09-27 06:43:04,634 Stage: Train 0.5 | Epoch: 178 | Iter: 271600 | Total Loss: 0.003859 | Recon Loss: 0.003283 | Commit Loss: 0.001152 | Perplexity: 1523.729338
2025-09-27 06:43:25,483 Stage: Train 0.5 | Epoch: 178 | Iter: 271800 | Total Loss: 0.003911 | Recon Loss: 0.003336 | Commit Loss: 0.001151 | Perplexity: 1518.624545
Trainning Epoch:  54%|█████▍    | 179/330 [16:31:02<7:29:22, 178.56s/it]2025-09-27 06:43:46,612 Stage: Train 0.5 | Epoch: 179 | Iter: 272000 | Total Loss: 0.003857 | Recon Loss: 0.003285 | Commit Loss: 0.001144 | Perplexity: 1517.114623
2025-09-27 06:44:07,453 Stage: Train 0.5 | Epoch: 179 | Iter: 272200 | Total Loss: 0.003930 | Recon Loss: 0.003358 | Commit Loss: 0.001145 | Perplexity: 1515.675377
2025-09-27 06:44:28,068 Stage: Train 0.5 | Epoch: 179 | Iter: 272400 | Total Loss: 0.003883 | Recon Loss: 0.003310 | Commit Loss: 0.001146 | Perplexity: 1520.991217
2025-09-27 06:44:48,936 Stage: Train 0.5 | Epoch: 179 | Iter: 272600 | Total Loss: 0.003860 | Recon Loss: 0.003283 | Commit Loss: 0.001155 | Perplexity: 1525.931707
2025-09-27 06:45:09,720 Stage: Train 0.5 | Epoch: 179 | Iter: 272800 | Total Loss: 0.004017 | Recon Loss: 0.003442 | Commit Loss: 0.001150 | Perplexity: 1522.676025
2025-09-27 06:45:30,466 Stage: Train 0.5 | Epoch: 179 | Iter: 273000 | Total Loss: 0.003844 | Recon Loss: 0.003271 | Commit Loss: 0.001145 | Perplexity: 1517.290984
2025-09-27 06:45:51,482 Stage: Train 0.5 | Epoch: 179 | Iter: 273200 | Total Loss: 0.003917 | Recon Loss: 0.003343 | Commit Loss: 0.001146 | Perplexity: 1520.814906
2025-09-27 06:46:12,399 Stage: Train 0.5 | Epoch: 179 | Iter: 273400 | Total Loss: 0.003860 | Recon Loss: 0.003286 | Commit Loss: 0.001147 | Perplexity: 1521.849648
Trainning Epoch:  55%|█████▍    | 180/330 [16:33:40<7:11:18, 172.53s/it]2025-09-27 06:46:33,753 Stage: Train 0.5 | Epoch: 180 | Iter: 273600 | Total Loss: 0.003919 | Recon Loss: 0.003344 | Commit Loss: 0.001151 | Perplexity: 1518.198930
2025-09-27 06:46:54,563 Stage: Train 0.5 | Epoch: 180 | Iter: 273800 | Total Loss: 0.003918 | Recon Loss: 0.003346 | Commit Loss: 0.001143 | Perplexity: 1520.662742
2025-09-27 06:47:15,532 Stage: Train 0.5 | Epoch: 180 | Iter: 274000 | Total Loss: 0.003861 | Recon Loss: 0.003289 | Commit Loss: 0.001144 | Perplexity: 1522.578375
2025-09-27 06:47:36,199 Stage: Train 0.5 | Epoch: 180 | Iter: 274200 | Total Loss: 0.003883 | Recon Loss: 0.003308 | Commit Loss: 0.001149 | Perplexity: 1520.653318
2025-09-27 06:47:57,211 Stage: Train 0.5 | Epoch: 180 | Iter: 274400 | Total Loss: 0.003917 | Recon Loss: 0.003344 | Commit Loss: 0.001146 | Perplexity: 1522.826315
2025-09-27 06:48:18,121 Stage: Train 0.5 | Epoch: 180 | Iter: 274600 | Total Loss: 0.003812 | Recon Loss: 0.003240 | Commit Loss: 0.001146 | Perplexity: 1523.078047
2025-09-27 06:48:39,372 Stage: Train 0.5 | Epoch: 180 | Iter: 274800 | Total Loss: 0.003905 | Recon Loss: 0.003331 | Commit Loss: 0.001148 | Perplexity: 1519.094656
Trainning Epoch:  55%|█████▍    | 181/330 [16:36:20<6:58:43, 168.61s/it]2025-09-27 06:49:05,793 Stage: Train 0.5 | Epoch: 181 | Iter: 275000 | Total Loss: 0.003900 | Recon Loss: 0.003327 | Commit Loss: 0.001146 | Perplexity: 1520.754107
2025-09-27 06:49:51,865 Stage: Train 0.5 | Epoch: 181 | Iter: 275200 | Total Loss: 0.003840 | Recon Loss: 0.003271 | Commit Loss: 0.001138 | Perplexity: 1516.927864
2025-09-27 06:50:37,914 Stage: Train 0.5 | Epoch: 181 | Iter: 275400 | Total Loss: 0.003918 | Recon Loss: 0.003346 | Commit Loss: 0.001145 | Perplexity: 1520.327083
2025-09-27 06:51:24,050 Stage: Train 0.5 | Epoch: 181 | Iter: 275600 | Total Loss: 0.003831 | Recon Loss: 0.003258 | Commit Loss: 0.001146 | Perplexity: 1522.833445
2025-09-27 06:52:10,174 Stage: Train 0.5 | Epoch: 181 | Iter: 275800 | Total Loss: 0.003905 | Recon Loss: 0.003336 | Commit Loss: 0.001138 | Perplexity: 1517.897408
2025-09-27 06:52:56,217 Stage: Train 0.5 | Epoch: 181 | Iter: 276000 | Total Loss: 0.003833 | Recon Loss: 0.003259 | Commit Loss: 0.001149 | Perplexity: 1520.521882
2025-09-27 06:53:42,332 Stage: Train 0.5 | Epoch: 181 | Iter: 276200 | Total Loss: 0.003871 | Recon Loss: 0.003300 | Commit Loss: 0.001143 | Perplexity: 1521.393317
2025-09-27 06:54:28,436 Stage: Train 0.5 | Epoch: 181 | Iter: 276400 | Total Loss: 0.003900 | Recon Loss: 0.003322 | Commit Loss: 0.001155 | Perplexity: 1524.387514
Trainning Epoch:  55%|█████▌    | 182/330 [16:42:07<9:08:27, 222.35s/it]2025-09-27 06:55:14,716 Stage: Train 0.5 | Epoch: 182 | Iter: 276600 | Total Loss: 0.003854 | Recon Loss: 0.003283 | Commit Loss: 0.001143 | Perplexity: 1517.738930
2025-09-27 06:56:00,788 Stage: Train 0.5 | Epoch: 182 | Iter: 276800 | Total Loss: 0.004017 | Recon Loss: 0.003447 | Commit Loss: 0.001140 | Perplexity: 1521.414920
2025-09-27 06:56:46,599 Stage: Train 0.5 | Epoch: 182 | Iter: 277000 | Total Loss: 0.003852 | Recon Loss: 0.003285 | Commit Loss: 0.001134 | Perplexity: 1518.519594
2025-09-27 06:57:32,680 Stage: Train 0.5 | Epoch: 182 | Iter: 277200 | Total Loss: 0.003855 | Recon Loss: 0.003285 | Commit Loss: 0.001141 | Perplexity: 1523.218286
2025-09-27 06:58:18,795 Stage: Train 0.5 | Epoch: 182 | Iter: 277400 | Total Loss: 0.003883 | Recon Loss: 0.003310 | Commit Loss: 0.001146 | Perplexity: 1523.699360
2025-09-27 06:59:04,967 Stage: Train 0.5 | Epoch: 182 | Iter: 277600 | Total Loss: 0.003840 | Recon Loss: 0.003271 | Commit Loss: 0.001138 | Perplexity: 1523.182332
2025-09-27 06:59:51,032 Stage: Train 0.5 | Epoch: 182 | Iter: 277800 | Total Loss: 0.003840 | Recon Loss: 0.003266 | Commit Loss: 0.001149 | Perplexity: 1524.205552
Trainning Epoch:  55%|█████▌    | 183/330 [16:47:57<10:38:26, 260.59s/it]2025-09-27 07:00:37,141 Stage: Train 0.5 | Epoch: 183 | Iter: 278000 | Total Loss: 0.003946 | Recon Loss: 0.003375 | Commit Loss: 0.001142 | Perplexity: 1518.665413
2025-09-27 07:01:23,128 Stage: Train 0.5 | Epoch: 183 | Iter: 278200 | Total Loss: 0.003821 | Recon Loss: 0.003255 | Commit Loss: 0.001131 | Perplexity: 1520.738354
2025-09-27 07:02:09,229 Stage: Train 0.5 | Epoch: 183 | Iter: 278400 | Total Loss: 0.003837 | Recon Loss: 0.003265 | Commit Loss: 0.001144 | Perplexity: 1525.322753
2025-09-27 07:02:55,317 Stage: Train 0.5 | Epoch: 183 | Iter: 278600 | Total Loss: 0.003809 | Recon Loss: 0.003238 | Commit Loss: 0.001142 | Perplexity: 1521.369267
2025-09-27 07:03:41,223 Stage: Train 0.5 | Epoch: 183 | Iter: 278800 | Total Loss: 0.003890 | Recon Loss: 0.003315 | Commit Loss: 0.001150 | Perplexity: 1525.967866
2025-09-27 07:04:27,212 Stage: Train 0.5 | Epoch: 183 | Iter: 279000 | Total Loss: 0.003937 | Recon Loss: 0.003368 | Commit Loss: 0.001138 | Perplexity: 1521.305765
2025-09-27 07:05:13,297 Stage: Train 0.5 | Epoch: 183 | Iter: 279200 | Total Loss: 0.003824 | Recon Loss: 0.003251 | Commit Loss: 0.001145 | Perplexity: 1520.062565
2025-09-27 07:05:59,319 Stage: Train 0.5 | Epoch: 183 | Iter: 279400 | Total Loss: 0.003879 | Recon Loss: 0.003309 | Commit Loss: 0.001140 | Perplexity: 1520.535381
Trainning Epoch:  56%|█████▌    | 184/330 [16:53:47<11:39:15, 287.37s/it]2025-09-27 07:06:45,725 Stage: Train 0.5 | Epoch: 184 | Iter: 279600 | Total Loss: 0.003893 | Recon Loss: 0.003325 | Commit Loss: 0.001136 | Perplexity: 1512.339501
2025-09-27 07:07:31,807 Stage: Train 0.5 | Epoch: 184 | Iter: 279800 | Total Loss: 0.003846 | Recon Loss: 0.003278 | Commit Loss: 0.001136 | Perplexity: 1522.751714
2025-09-27 07:08:17,934 Stage: Train 0.5 | Epoch: 184 | Iter: 280000 | Total Loss: 0.003842 | Recon Loss: 0.003270 | Commit Loss: 0.001143 | Perplexity: 1519.230442
2025-09-27 07:08:17,934 Saving model at iteration 280000
2025-09-27 07:08:18,134 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000
2025-09-27 07:08:18,422 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/model.safetensors
2025-09-27 07:08:18,804 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/optimizer.bin
2025-09-27 07:08:18,805 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/scheduler.bin
2025-09-27 07:08:18,805 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/sampler.bin
2025-09-27 07:08:18,806 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/random_states_0.pkl
2025-09-27 07:09:05,124 Stage: Train 0.5 | Epoch: 184 | Iter: 280200 | Total Loss: 0.003985 | Recon Loss: 0.003413 | Commit Loss: 0.001145 | Perplexity: 1523.494156
2025-09-27 07:09:51,042 Stage: Train 0.5 | Epoch: 184 | Iter: 280400 | Total Loss: 0.003792 | Recon Loss: 0.003223 | Commit Loss: 0.001139 | Perplexity: 1525.032916
2025-09-27 07:10:36,592 Stage: Train 0.5 | Epoch: 184 | Iter: 280600 | Total Loss: 0.003868 | Recon Loss: 0.003297 | Commit Loss: 0.001142 | Perplexity: 1525.628007
2025-09-27 07:11:22,609 Stage: Train 0.5 | Epoch: 184 | Iter: 280800 | Total Loss: 0.003828 | Recon Loss: 0.003256 | Commit Loss: 0.001145 | Perplexity: 1523.544938
2025-09-27 07:12:08,746 Stage: Train 0.5 | Epoch: 184 | Iter: 281000 | Total Loss: 0.003897 | Recon Loss: 0.003324 | Commit Loss: 0.001146 | Perplexity: 1522.022468
Trainning Epoch:  56%|█████▌    | 185/330 [16:59:38<12:20:25, 306.38s/it]2025-09-27 07:12:55,053 Stage: Train 0.5 | Epoch: 185 | Iter: 281200 | Total Loss: 0.003848 | Recon Loss: 0.003280 | Commit Loss: 0.001136 | Perplexity: 1519.380027
2025-09-27 07:13:41,249 Stage: Train 0.5 | Epoch: 185 | Iter: 281400 | Total Loss: 0.003841 | Recon Loss: 0.003274 | Commit Loss: 0.001134 | Perplexity: 1519.612405
2025-09-27 07:14:27,369 Stage: Train 0.5 | Epoch: 185 | Iter: 281600 | Total Loss: 0.003829 | Recon Loss: 0.003260 | Commit Loss: 0.001136 | Perplexity: 1522.747232
2025-09-27 07:15:13,390 Stage: Train 0.5 | Epoch: 185 | Iter: 281800 | Total Loss: 0.003862 | Recon Loss: 0.003290 | Commit Loss: 0.001145 | Perplexity: 1525.244979
2025-09-27 07:15:59,372 Stage: Train 0.5 | Epoch: 185 | Iter: 282000 | Total Loss: 0.003932 | Recon Loss: 0.003361 | Commit Loss: 0.001142 | Perplexity: 1522.096490
2025-09-27 07:16:45,306 Stage: Train 0.5 | Epoch: 185 | Iter: 282200 | Total Loss: 0.003809 | Recon Loss: 0.003241 | Commit Loss: 0.001135 | Perplexity: 1522.682738
2025-09-27 07:17:31,183 Stage: Train 0.5 | Epoch: 185 | Iter: 282400 | Total Loss: 0.003928 | Recon Loss: 0.003357 | Commit Loss: 0.001144 | Perplexity: 1522.626776
Trainning Epoch:  56%|█████▋    | 186/330 [17:05:28<12:46:40, 319.45s/it]2025-09-27 07:18:17,664 Stage: Train 0.5 | Epoch: 186 | Iter: 282600 | Total Loss: 0.003919 | Recon Loss: 0.003354 | Commit Loss: 0.001132 | Perplexity: 1520.164907
2025-09-27 07:19:03,844 Stage: Train 0.5 | Epoch: 186 | Iter: 282800 | Total Loss: 0.003825 | Recon Loss: 0.003252 | Commit Loss: 0.001144 | Perplexity: 1525.639554
2025-09-27 07:19:49,865 Stage: Train 0.5 | Epoch: 186 | Iter: 283000 | Total Loss: 0.003749 | Recon Loss: 0.003180 | Commit Loss: 0.001137 | Perplexity: 1520.443166
2025-09-27 07:20:35,812 Stage: Train 0.5 | Epoch: 186 | Iter: 283200 | Total Loss: 0.003846 | Recon Loss: 0.003275 | Commit Loss: 0.001142 | Perplexity: 1527.008437
2025-09-27 07:21:21,763 Stage: Train 0.5 | Epoch: 186 | Iter: 283400 | Total Loss: 0.003832 | Recon Loss: 0.003266 | Commit Loss: 0.001131 | Perplexity: 1521.638561
2025-09-27 07:22:07,751 Stage: Train 0.5 | Epoch: 186 | Iter: 283600 | Total Loss: 0.003818 | Recon Loss: 0.003246 | Commit Loss: 0.001143 | Perplexity: 1524.523217
2025-09-27 07:22:53,939 Stage: Train 0.5 | Epoch: 186 | Iter: 283800 | Total Loss: 0.003821 | Recon Loss: 0.003250 | Commit Loss: 0.001141 | Perplexity: 1523.942772
2025-09-27 07:23:39,940 Stage: Train 0.5 | Epoch: 186 | Iter: 284000 | Total Loss: 0.003953 | Recon Loss: 0.003386 | Commit Loss: 0.001133 | Perplexity: 1517.156418
Trainning Epoch:  57%|█████▋    | 187/330 [17:11:18<13:03:12, 328.62s/it]2025-09-27 07:24:26,201 Stage: Train 0.5 | Epoch: 187 | Iter: 284200 | Total Loss: 0.003821 | Recon Loss: 0.003256 | Commit Loss: 0.001128 | Perplexity: 1519.987507
2025-09-27 07:25:12,072 Stage: Train 0.5 | Epoch: 187 | Iter: 284400 | Total Loss: 0.003881 | Recon Loss: 0.003310 | Commit Loss: 0.001141 | Perplexity: 1526.115197
2025-09-27 07:25:58,079 Stage: Train 0.5 | Epoch: 187 | Iter: 284600 | Total Loss: 0.003765 | Recon Loss: 0.003198 | Commit Loss: 0.001134 | Perplexity: 1522.528365
2025-09-27 07:26:44,125 Stage: Train 0.5 | Epoch: 187 | Iter: 284800 | Total Loss: 0.003888 | Recon Loss: 0.003318 | Commit Loss: 0.001139 | Perplexity: 1526.659083
2025-09-27 07:27:30,155 Stage: Train 0.5 | Epoch: 187 | Iter: 285000 | Total Loss: 0.003878 | Recon Loss: 0.003312 | Commit Loss: 0.001131 | Perplexity: 1521.096447
2025-09-27 07:28:16,180 Stage: Train 0.5 | Epoch: 187 | Iter: 285200 | Total Loss: 0.003767 | Recon Loss: 0.003199 | Commit Loss: 0.001136 | Perplexity: 1521.570577
2025-09-27 07:29:02,170 Stage: Train 0.5 | Epoch: 187 | Iter: 285400 | Total Loss: 0.003882 | Recon Loss: 0.003310 | Commit Loss: 0.001144 | Perplexity: 1522.985282
Trainning Epoch:  57%|█████▋    | 188/330 [17:17:07<13:12:33, 334.89s/it]2025-09-27 07:29:48,342 Stage: Train 0.5 | Epoch: 188 | Iter: 285600 | Total Loss: 0.003848 | Recon Loss: 0.003280 | Commit Loss: 0.001135 | Perplexity: 1518.969441
2025-09-27 07:30:34,416 Stage: Train 0.5 | Epoch: 188 | Iter: 285800 | Total Loss: 0.003822 | Recon Loss: 0.003251 | Commit Loss: 0.001142 | Perplexity: 1527.192557
2025-09-27 07:31:20,667 Stage: Train 0.5 | Epoch: 188 | Iter: 286000 | Total Loss: 0.003791 | Recon Loss: 0.003225 | Commit Loss: 0.001131 | Perplexity: 1521.679823
2025-09-27 07:32:06,392 Stage: Train 0.5 | Epoch: 188 | Iter: 286200 | Total Loss: 0.003852 | Recon Loss: 0.003288 | Commit Loss: 0.001128 | Perplexity: 1520.897873
2025-09-27 07:32:52,301 Stage: Train 0.5 | Epoch: 188 | Iter: 286400 | Total Loss: 0.003878 | Recon Loss: 0.003310 | Commit Loss: 0.001136 | Perplexity: 1522.738235
2025-09-27 07:33:38,372 Stage: Train 0.5 | Epoch: 188 | Iter: 286600 | Total Loss: 0.003861 | Recon Loss: 0.003292 | Commit Loss: 0.001137 | Perplexity: 1523.501600
2025-09-27 07:34:24,429 Stage: Train 0.5 | Epoch: 188 | Iter: 286800 | Total Loss: 0.003854 | Recon Loss: 0.003289 | Commit Loss: 0.001130 | Perplexity: 1520.744847
2025-09-27 07:35:10,588 Stage: Train 0.5 | Epoch: 188 | Iter: 287000 | Total Loss: 0.003829 | Recon Loss: 0.003258 | Commit Loss: 0.001142 | Perplexity: 1525.652391
Trainning Epoch:  57%|█████▋    | 189/330 [17:22:57<13:17:33, 339.38s/it]2025-09-27 07:35:56,773 Stage: Train 0.5 | Epoch: 189 | Iter: 287200 | Total Loss: 0.003870 | Recon Loss: 0.003304 | Commit Loss: 0.001133 | Perplexity: 1520.864503
2025-09-27 07:36:42,854 Stage: Train 0.5 | Epoch: 189 | Iter: 287400 | Total Loss: 0.003820 | Recon Loss: 0.003253 | Commit Loss: 0.001133 | Perplexity: 1520.918661
2025-09-27 07:37:28,957 Stage: Train 0.5 | Epoch: 189 | Iter: 287600 | Total Loss: 0.003886 | Recon Loss: 0.003322 | Commit Loss: 0.001128 | Perplexity: 1521.345878
2025-09-27 07:38:14,873 Stage: Train 0.5 | Epoch: 189 | Iter: 287800 | Total Loss: 0.003812 | Recon Loss: 0.003247 | Commit Loss: 0.001129 | Perplexity: 1521.528850
2025-09-27 07:39:00,519 Stage: Train 0.5 | Epoch: 189 | Iter: 288000 | Total Loss: 0.003809 | Recon Loss: 0.003243 | Commit Loss: 0.001132 | Perplexity: 1524.542458
2025-09-27 07:39:46,516 Stage: Train 0.5 | Epoch: 189 | Iter: 288200 | Total Loss: 0.003794 | Recon Loss: 0.003227 | Commit Loss: 0.001134 | Perplexity: 1526.300070
2025-09-27 07:40:32,489 Stage: Train 0.5 | Epoch: 189 | Iter: 288400 | Total Loss: 0.003873 | Recon Loss: 0.003309 | Commit Loss: 0.001129 | Perplexity: 1521.620624
2025-09-27 07:41:18,535 Stage: Train 0.5 | Epoch: 189 | Iter: 288600 | Total Loss: 0.003887 | Recon Loss: 0.003314 | Commit Loss: 0.001147 | Perplexity: 1526.335999
Trainning Epoch:  58%|█████▊    | 190/330 [17:28:46<13:18:49, 342.35s/it]2025-09-27 07:42:04,757 Stage: Train 0.5 | Epoch: 190 | Iter: 288800 | Total Loss: 0.003821 | Recon Loss: 0.003256 | Commit Loss: 0.001130 | Perplexity: 1524.677692
2025-09-27 07:42:50,855 Stage: Train 0.5 | Epoch: 190 | Iter: 289000 | Total Loss: 0.003906 | Recon Loss: 0.003343 | Commit Loss: 0.001126 | Perplexity: 1520.420391
2025-09-27 07:43:37,052 Stage: Train 0.5 | Epoch: 190 | Iter: 289200 | Total Loss: 0.003808 | Recon Loss: 0.003244 | Commit Loss: 0.001127 | Perplexity: 1523.206742
2025-09-27 07:44:23,000 Stage: Train 0.5 | Epoch: 190 | Iter: 289400 | Total Loss: 0.003781 | Recon Loss: 0.003214 | Commit Loss: 0.001135 | Perplexity: 1526.639537
2025-09-27 07:45:09,046 Stage: Train 0.5 | Epoch: 190 | Iter: 289600 | Total Loss: 0.003832 | Recon Loss: 0.003265 | Commit Loss: 0.001133 | Perplexity: 1521.967723
2025-09-27 07:45:55,058 Stage: Train 0.5 | Epoch: 190 | Iter: 289800 | Total Loss: 0.003849 | Recon Loss: 0.003281 | Commit Loss: 0.001136 | Perplexity: 1524.395948
2025-09-27 07:46:40,894 Stage: Train 0.5 | Epoch: 190 | Iter: 290000 | Total Loss: 0.003884 | Recon Loss: 0.003315 | Commit Loss: 0.001138 | Perplexity: 1526.709976
Trainning Epoch:  58%|█████▊    | 191/330 [17:34:36<13:18:12, 344.55s/it]2025-09-27 07:47:27,103 Stage: Train 0.5 | Epoch: 191 | Iter: 290200 | Total Loss: 0.003892 | Recon Loss: 0.003330 | Commit Loss: 0.001125 | Perplexity: 1518.489711
2025-09-27 07:48:13,256 Stage: Train 0.5 | Epoch: 191 | Iter: 290400 | Total Loss: 0.003837 | Recon Loss: 0.003271 | Commit Loss: 0.001132 | Perplexity: 1521.458692
2025-09-27 07:48:59,293 Stage: Train 0.5 | Epoch: 191 | Iter: 290600 | Total Loss: 0.003851 | Recon Loss: 0.003288 | Commit Loss: 0.001126 | Perplexity: 1522.469722
2025-09-27 07:49:45,359 Stage: Train 0.5 | Epoch: 191 | Iter: 290800 | Total Loss: 0.003785 | Recon Loss: 0.003221 | Commit Loss: 0.001127 | Perplexity: 1525.477863
2025-09-27 07:50:31,318 Stage: Train 0.5 | Epoch: 191 | Iter: 291000 | Total Loss: 0.003966 | Recon Loss: 0.003401 | Commit Loss: 0.001129 | Perplexity: 1524.196407
2025-09-27 07:51:17,377 Stage: Train 0.5 | Epoch: 191 | Iter: 291200 | Total Loss: 0.003712 | Recon Loss: 0.003146 | Commit Loss: 0.001133 | Perplexity: 1523.510554
2025-09-27 07:52:03,486 Stage: Train 0.5 | Epoch: 191 | Iter: 291400 | Total Loss: 0.003834 | Recon Loss: 0.003266 | Commit Loss: 0.001137 | Perplexity: 1528.021841
2025-09-27 07:52:49,281 Stage: Train 0.5 | Epoch: 191 | Iter: 291600 | Total Loss: 0.003812 | Recon Loss: 0.003244 | Commit Loss: 0.001135 | Perplexity: 1521.304124
Trainning Epoch:  58%|█████▊    | 192/330 [17:40:26<13:16:04, 346.12s/it]2025-09-27 07:53:35,279 Stage: Train 0.5 | Epoch: 192 | Iter: 291800 | Total Loss: 0.003832 | Recon Loss: 0.003269 | Commit Loss: 0.001125 | Perplexity: 1521.442496
2025-09-27 07:54:21,286 Stage: Train 0.5 | Epoch: 192 | Iter: 292000 | Total Loss: 0.003757 | Recon Loss: 0.003197 | Commit Loss: 0.001121 | Perplexity: 1520.620955
2025-09-27 07:55:07,248 Stage: Train 0.5 | Epoch: 192 | Iter: 292200 | Total Loss: 0.003805 | Recon Loss: 0.003240 | Commit Loss: 0.001129 | Perplexity: 1527.745925
2025-09-27 07:55:53,425 Stage: Train 0.5 | Epoch: 192 | Iter: 292400 | Total Loss: 0.003764 | Recon Loss: 0.003200 | Commit Loss: 0.001128 | Perplexity: 1527.109288
2025-09-27 07:56:39,575 Stage: Train 0.5 | Epoch: 192 | Iter: 292600 | Total Loss: 0.003873 | Recon Loss: 0.003305 | Commit Loss: 0.001135 | Perplexity: 1525.660764
2025-09-27 07:57:25,670 Stage: Train 0.5 | Epoch: 192 | Iter: 292800 | Total Loss: 0.003824 | Recon Loss: 0.003259 | Commit Loss: 0.001130 | Perplexity: 1523.122109
2025-09-27 07:58:11,769 Stage: Train 0.5 | Epoch: 192 | Iter: 293000 | Total Loss: 0.003778 | Recon Loss: 0.003209 | Commit Loss: 0.001137 | Perplexity: 1529.109996
Trainning Epoch:  58%|█████▊    | 193/330 [17:46:16<13:12:58, 347.29s/it]2025-09-27 07:58:58,113 Stage: Train 0.5 | Epoch: 193 | Iter: 293200 | Total Loss: 0.003797 | Recon Loss: 0.003232 | Commit Loss: 0.001130 | Perplexity: 1525.818879
2025-09-27 07:59:44,134 Stage: Train 0.5 | Epoch: 193 | Iter: 293400 | Total Loss: 0.003773 | Recon Loss: 0.003210 | Commit Loss: 0.001125 | Perplexity: 1525.532900
2025-09-27 08:00:30,251 Stage: Train 0.5 | Epoch: 193 | Iter: 293600 | Total Loss: 0.003887 | Recon Loss: 0.003320 | Commit Loss: 0.001133 | Perplexity: 1526.700366
2025-09-27 08:01:16,053 Stage: Train 0.5 | Epoch: 193 | Iter: 293800 | Total Loss: 0.003751 | Recon Loss: 0.003189 | Commit Loss: 0.001125 | Perplexity: 1525.540826
2025-09-27 08:02:01,999 Stage: Train 0.5 | Epoch: 193 | Iter: 294000 | Total Loss: 0.003802 | Recon Loss: 0.003238 | Commit Loss: 0.001129 | Perplexity: 1526.327404
2025-09-27 08:02:48,059 Stage: Train 0.5 | Epoch: 193 | Iter: 294200 | Total Loss: 0.003847 | Recon Loss: 0.003286 | Commit Loss: 0.001122 | Perplexity: 1523.919380
2025-09-27 08:03:34,079 Stage: Train 0.5 | Epoch: 193 | Iter: 294400 | Total Loss: 0.003741 | Recon Loss: 0.003180 | Commit Loss: 0.001123 | Perplexity: 1523.087974
2025-09-27 08:04:20,004 Stage: Train 0.5 | Epoch: 193 | Iter: 294600 | Total Loss: 0.003915 | Recon Loss: 0.003353 | Commit Loss: 0.001125 | Perplexity: 1523.338558
Trainning Epoch:  59%|█████▉    | 194/330 [17:52:05<13:08:42, 347.96s/it]2025-09-27 08:05:06,245 Stage: Train 0.5 | Epoch: 194 | Iter: 294800 | Total Loss: 0.003804 | Recon Loss: 0.003243 | Commit Loss: 0.001121 | Perplexity: 1520.224300
2025-09-27 08:05:52,312 Stage: Train 0.5 | Epoch: 194 | Iter: 295000 | Total Loss: 0.003752 | Recon Loss: 0.003189 | Commit Loss: 0.001125 | Perplexity: 1531.213154
2025-09-27 08:06:38,446 Stage: Train 0.5 | Epoch: 194 | Iter: 295200 | Total Loss: 0.003817 | Recon Loss: 0.003254 | Commit Loss: 0.001126 | Perplexity: 1523.783488
2025-09-27 08:07:24,449 Stage: Train 0.5 | Epoch: 194 | Iter: 295400 | Total Loss: 0.003786 | Recon Loss: 0.003221 | Commit Loss: 0.001130 | Perplexity: 1530.279185
2025-09-27 08:08:10,251 Stage: Train 0.5 | Epoch: 194 | Iter: 295600 | Total Loss: 0.003737 | Recon Loss: 0.003175 | Commit Loss: 0.001125 | Perplexity: 1527.474844
2025-09-27 08:08:56,379 Stage: Train 0.5 | Epoch: 194 | Iter: 295800 | Total Loss: 0.003929 | Recon Loss: 0.003368 | Commit Loss: 0.001122 | Perplexity: 1525.507519
2025-09-27 08:09:42,263 Stage: Train 0.5 | Epoch: 194 | Iter: 296000 | Total Loss: 0.003709 | Recon Loss: 0.003147 | Commit Loss: 0.001123 | Perplexity: 1525.890756
2025-09-27 08:10:28,281 Stage: Train 0.5 | Epoch: 194 | Iter: 296200 | Total Loss: 0.003778 | Recon Loss: 0.003215 | Commit Loss: 0.001127 | Perplexity: 1524.221702
Trainning Epoch:  59%|█████▉    | 195/330 [17:57:55<13:04:02, 348.46s/it]2025-09-27 08:11:14,622 Stage: Train 0.5 | Epoch: 195 | Iter: 296400 | Total Loss: 0.003833 | Recon Loss: 0.003271 | Commit Loss: 0.001123 | Perplexity: 1522.983900
2025-09-27 08:12:00,697 Stage: Train 0.5 | Epoch: 195 | Iter: 296600 | Total Loss: 0.003795 | Recon Loss: 0.003236 | Commit Loss: 0.001118 | Perplexity: 1522.860672
2025-09-27 08:12:46,807 Stage: Train 0.5 | Epoch: 195 | Iter: 296800 | Total Loss: 0.003789 | Recon Loss: 0.003225 | Commit Loss: 0.001128 | Perplexity: 1525.736442
2025-09-27 08:13:32,954 Stage: Train 0.5 | Epoch: 195 | Iter: 297000 | Total Loss: 0.003872 | Recon Loss: 0.003309 | Commit Loss: 0.001125 | Perplexity: 1529.789008
2025-09-27 08:14:18,904 Stage: Train 0.5 | Epoch: 195 | Iter: 297200 | Total Loss: 0.003776 | Recon Loss: 0.003215 | Commit Loss: 0.001121 | Perplexity: 1526.613858
2025-09-27 08:15:04,791 Stage: Train 0.5 | Epoch: 195 | Iter: 297400 | Total Loss: 0.003803 | Recon Loss: 0.003242 | Commit Loss: 0.001122 | Perplexity: 1528.886443
2025-09-27 08:15:50,839 Stage: Train 0.5 | Epoch: 195 | Iter: 297600 | Total Loss: 0.003888 | Recon Loss: 0.003325 | Commit Loss: 0.001125 | Perplexity: 1526.835625
Trainning Epoch:  59%|█████▉    | 196/330 [18:03:45<12:59:15, 348.92s/it]2025-09-27 08:16:37,258 Stage: Train 0.5 | Epoch: 196 | Iter: 297800 | Total Loss: 0.003717 | Recon Loss: 0.003156 | Commit Loss: 0.001122 | Perplexity: 1526.720768
2025-09-27 08:17:23,245 Stage: Train 0.5 | Epoch: 196 | Iter: 298000 | Total Loss: 0.003752 | Recon Loss: 0.003190 | Commit Loss: 0.001123 | Perplexity: 1529.457054
2025-09-27 08:18:09,262 Stage: Train 0.5 | Epoch: 196 | Iter: 298200 | Total Loss: 0.003879 | Recon Loss: 0.003316 | Commit Loss: 0.001127 | Perplexity: 1525.116588
2025-09-27 08:18:55,285 Stage: Train 0.5 | Epoch: 196 | Iter: 298400 | Total Loss: 0.003808 | Recon Loss: 0.003246 | Commit Loss: 0.001124 | Perplexity: 1525.470652
2025-09-27 08:19:41,291 Stage: Train 0.5 | Epoch: 196 | Iter: 298600 | Total Loss: 0.003850 | Recon Loss: 0.003289 | Commit Loss: 0.001122 | Perplexity: 1530.598875
2025-09-27 08:20:27,277 Stage: Train 0.5 | Epoch: 196 | Iter: 298800 | Total Loss: 0.003792 | Recon Loss: 0.003235 | Commit Loss: 0.001115 | Perplexity: 1521.356450
2025-09-27 08:21:13,379 Stage: Train 0.5 | Epoch: 196 | Iter: 299000 | Total Loss: 0.003733 | Recon Loss: 0.003173 | Commit Loss: 0.001120 | Perplexity: 1527.044706
2025-09-27 08:21:59,217 Stage: Train 0.5 | Epoch: 196 | Iter: 299200 | Total Loss: 0.003798 | Recon Loss: 0.003235 | Commit Loss: 0.001125 | Perplexity: 1527.658945
Trainning Epoch:  60%|█████▉    | 197/330 [18:09:35<12:53:56, 349.15s/it]2025-09-27 08:22:45,417 Stage: Train 0.5 | Epoch: 197 | Iter: 299400 | Total Loss: 0.003784 | Recon Loss: 0.003224 | Commit Loss: 0.001118 | Perplexity: 1527.805209
2025-09-27 08:23:29,829 Stage: Train 0.5 | Epoch: 197 | Iter: 299600 | Total Loss: 0.003727 | Recon Loss: 0.003167 | Commit Loss: 0.001120 | Perplexity: 1531.053225
2025-09-27 08:24:15,761 Stage: Train 0.5 | Epoch: 197 | Iter: 299800 | Total Loss: 0.003861 | Recon Loss: 0.003300 | Commit Loss: 0.001121 | Perplexity: 1524.480230
2025-09-27 08:25:01,672 Stage: Train 0.5 | Epoch: 197 | Iter: 300000 | Total Loss: 0.003762 | Recon Loss: 0.003202 | Commit Loss: 0.001121 | Perplexity: 1525.894113
2025-09-27 08:25:01,672 Saving model at iteration 300000
2025-09-27 08:25:01,910 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000
2025-09-27 08:25:02,244 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/model.safetensors
2025-09-27 08:25:02,679 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/optimizer.bin
2025-09-27 08:25:02,679 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/scheduler.bin
2025-09-27 08:25:02,679 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/sampler.bin
2025-09-27 08:25:02,680 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/random_states_0.pkl
2025-09-27 08:25:48,929 Stage: Train 0.5 | Epoch: 197 | Iter: 300200 | Total Loss: 0.003803 | Recon Loss: 0.003247 | Commit Loss: 0.001112 | Perplexity: 1526.287000
2025-09-27 08:26:34,830 Stage: Train 0.5 | Epoch: 197 | Iter: 300400 | Total Loss: 0.003721 | Recon Loss: 0.003160 | Commit Loss: 0.001122 | Perplexity: 1531.957150
2025-09-27 08:27:20,978 Stage: Train 0.5 | Epoch: 197 | Iter: 300600 | Total Loss: 0.003839 | Recon Loss: 0.003279 | Commit Loss: 0.001120 | Perplexity: 1523.498344
Trainning Epoch:  60%|██████    | 198/330 [18:15:24<12:48:09, 349.16s/it]2025-09-27 08:28:07,377 Stage: Train 0.5 | Epoch: 198 | Iter: 300800 | Total Loss: 0.003743 | Recon Loss: 0.003182 | Commit Loss: 0.001122 | Perplexity: 1528.433444
2025-09-27 08:28:53,500 Stage: Train 0.5 | Epoch: 198 | Iter: 301000 | Total Loss: 0.003879 | Recon Loss: 0.003317 | Commit Loss: 0.001123 | Perplexity: 1526.024651
2025-09-27 08:29:39,382 Stage: Train 0.5 | Epoch: 198 | Iter: 301200 | Total Loss: 0.003777 | Recon Loss: 0.003218 | Commit Loss: 0.001119 | Perplexity: 1529.707192
2025-09-27 08:30:25,558 Stage: Train 0.5 | Epoch: 198 | Iter: 301400 | Total Loss: 0.003675 | Recon Loss: 0.003118 | Commit Loss: 0.001115 | Perplexity: 1526.486548
2025-09-27 08:31:11,764 Stage: Train 0.5 | Epoch: 198 | Iter: 301600 | Total Loss: 0.003911 | Recon Loss: 0.003355 | Commit Loss: 0.001111 | Perplexity: 1521.812631
2025-09-27 08:31:58,024 Stage: Train 0.5 | Epoch: 198 | Iter: 301800 | Total Loss: 0.003749 | Recon Loss: 0.003193 | Commit Loss: 0.001111 | Perplexity: 1524.884303
2025-09-27 08:32:44,159 Stage: Train 0.5 | Epoch: 198 | Iter: 302000 | Total Loss: 0.003798 | Recon Loss: 0.003233 | Commit Loss: 0.001131 | Perplexity: 1526.052076
2025-09-27 08:33:30,269 Stage: Train 0.5 | Epoch: 198 | Iter: 302200 | Total Loss: 0.003758 | Recon Loss: 0.003193 | Commit Loss: 0.001131 | Perplexity: 1532.730952
Trainning Epoch:  60%|██████    | 199/330 [18:21:15<12:43:18, 349.61s/it]2025-09-27 08:34:16,627 Stage: Train 0.5 | Epoch: 199 | Iter: 302400 | Total Loss: 0.003713 | Recon Loss: 0.003158 | Commit Loss: 0.001111 | Perplexity: 1522.939216
2025-09-27 08:35:02,480 Stage: Train 0.5 | Epoch: 199 | Iter: 302600 | Total Loss: 0.003796 | Recon Loss: 0.003234 | Commit Loss: 0.001124 | Perplexity: 1528.278386
2025-09-27 08:35:48,660 Stage: Train 0.5 | Epoch: 199 | Iter: 302800 | Total Loss: 0.003742 | Recon Loss: 0.003185 | Commit Loss: 0.001114 | Perplexity: 1529.958845
2025-09-27 08:36:34,536 Stage: Train 0.5 | Epoch: 199 | Iter: 303000 | Total Loss: 0.003864 | Recon Loss: 0.003301 | Commit Loss: 0.001126 | Perplexity: 1529.050759
2025-09-27 08:37:20,664 Stage: Train 0.5 | Epoch: 199 | Iter: 303200 | Total Loss: 0.003762 | Recon Loss: 0.003205 | Commit Loss: 0.001115 | Perplexity: 1529.097418
2025-09-27 08:38:06,819 Stage: Train 0.5 | Epoch: 199 | Iter: 303400 | Total Loss: 0.003863 | Recon Loss: 0.003302 | Commit Loss: 0.001121 | Perplexity: 1528.752785
2025-09-27 08:38:52,982 Stage: Train 0.5 | Epoch: 199 | Iter: 303600 | Total Loss: 0.003771 | Recon Loss: 0.003209 | Commit Loss: 0.001123 | Perplexity: 1528.250803
2025-09-27 08:39:39,017 Stage: Train 0.5 | Epoch: 199 | Iter: 303800 | Total Loss: 0.003745 | Recon Loss: 0.003191 | Commit Loss: 0.001108 | Perplexity: 1525.589886
Trainning Epoch:  61%|██████    | 200/330 [18:27:05<12:37:47, 349.75s/it]2025-09-27 08:40:25,529 Stage: Train 0.5 | Epoch: 200 | Iter: 304000 | Total Loss: 0.003751 | Recon Loss: 0.003190 | Commit Loss: 0.001122 | Perplexity: 1530.058133
2025-09-27 08:41:11,576 Stage: Train 0.5 | Epoch: 200 | Iter: 304200 | Total Loss: 0.003804 | Recon Loss: 0.003247 | Commit Loss: 0.001114 | Perplexity: 1521.638876
2025-09-27 08:41:57,644 Stage: Train 0.5 | Epoch: 200 | Iter: 304400 | Total Loss: 0.003723 | Recon Loss: 0.003165 | Commit Loss: 0.001115 | Perplexity: 1527.962453
2025-09-27 08:42:43,638 Stage: Train 0.5 | Epoch: 200 | Iter: 304600 | Total Loss: 0.003768 | Recon Loss: 0.003210 | Commit Loss: 0.001116 | Perplexity: 1526.658437
2025-09-27 08:43:29,398 Stage: Train 0.5 | Epoch: 200 | Iter: 304800 | Total Loss: 0.003715 | Recon Loss: 0.003154 | Commit Loss: 0.001123 | Perplexity: 1530.993531
2025-09-27 08:44:15,423 Stage: Train 0.5 | Epoch: 200 | Iter: 305000 | Total Loss: 0.003715 | Recon Loss: 0.003157 | Commit Loss: 0.001115 | Perplexity: 1529.415337
2025-09-27 08:45:01,569 Stage: Train 0.5 | Epoch: 200 | Iter: 305200 | Total Loss: 0.003777 | Recon Loss: 0.003215 | Commit Loss: 0.001123 | Perplexity: 1530.671707
Trainning Epoch:  61%|██████    | 201/330 [18:32:55<12:32:05, 349.81s/it]2025-09-27 08:45:47,916 Stage: Train 0.5 | Epoch: 201 | Iter: 305400 | Total Loss: 0.003703 | Recon Loss: 0.003144 | Commit Loss: 0.001117 | Perplexity: 1526.297009
2025-09-27 08:46:33,858 Stage: Train 0.5 | Epoch: 201 | Iter: 305600 | Total Loss: 0.003799 | Recon Loss: 0.003242 | Commit Loss: 0.001114 | Perplexity: 1529.774618
2025-09-27 08:47:20,034 Stage: Train 0.5 | Epoch: 201 | Iter: 305800 | Total Loss: 0.003764 | Recon Loss: 0.003204 | Commit Loss: 0.001119 | Perplexity: 1528.166399
2025-09-27 08:48:06,030 Stage: Train 0.5 | Epoch: 201 | Iter: 306000 | Total Loss: 0.003694 | Recon Loss: 0.003135 | Commit Loss: 0.001119 | Perplexity: 1528.673394
2025-09-27 08:48:52,142 Stage: Train 0.5 | Epoch: 201 | Iter: 306200 | Total Loss: 0.003754 | Recon Loss: 0.003189 | Commit Loss: 0.001130 | Perplexity: 1532.796870
2025-09-27 08:49:38,123 Stage: Train 0.5 | Epoch: 201 | Iter: 306400 | Total Loss: 0.003784 | Recon Loss: 0.003224 | Commit Loss: 0.001120 | Perplexity: 1531.699053
2025-09-27 08:50:23,767 Stage: Train 0.5 | Epoch: 201 | Iter: 306600 | Total Loss: 0.003796 | Recon Loss: 0.003241 | Commit Loss: 0.001110 | Perplexity: 1520.546212
2025-09-27 08:51:09,729 Stage: Train 0.5 | Epoch: 201 | Iter: 306800 | Total Loss: 0.003701 | Recon Loss: 0.003146 | Commit Loss: 0.001111 | Perplexity: 1524.660678
Trainning Epoch:  61%|██████    | 202/330 [18:38:44<12:26:04, 349.72s/it]2025-09-27 08:51:55,959 Stage: Train 0.5 | Epoch: 202 | Iter: 307000 | Total Loss: 0.003698 | Recon Loss: 0.003141 | Commit Loss: 0.001115 | Perplexity: 1526.743795
2025-09-27 08:52:41,930 Stage: Train 0.5 | Epoch: 202 | Iter: 307200 | Total Loss: 0.003799 | Recon Loss: 0.003244 | Commit Loss: 0.001111 | Perplexity: 1526.755433
2025-09-27 08:53:27,954 Stage: Train 0.5 | Epoch: 202 | Iter: 307400 | Total Loss: 0.003722 | Recon Loss: 0.003164 | Commit Loss: 0.001115 | Perplexity: 1529.352186
2025-09-27 08:54:13,970 Stage: Train 0.5 | Epoch: 202 | Iter: 307600 | Total Loss: 0.003819 | Recon Loss: 0.003260 | Commit Loss: 0.001118 | Perplexity: 1526.967258
2025-09-27 08:55:00,117 Stage: Train 0.5 | Epoch: 202 | Iter: 307800 | Total Loss: 0.003732 | Recon Loss: 0.003176 | Commit Loss: 0.001112 | Perplexity: 1525.862307
2025-09-27 08:55:46,405 Stage: Train 0.5 | Epoch: 202 | Iter: 308000 | Total Loss: 0.003727 | Recon Loss: 0.003169 | Commit Loss: 0.001116 | Perplexity: 1530.872862
2025-09-27 08:56:32,471 Stage: Train 0.5 | Epoch: 202 | Iter: 308200 | Total Loss: 0.003730 | Recon Loss: 0.003172 | Commit Loss: 0.001115 | Perplexity: 1532.084693
Trainning Epoch:  62%|██████▏   | 203/330 [18:44:34<12:20:29, 349.84s/it]2025-09-27 08:57:18,702 Stage: Train 0.5 | Epoch: 203 | Iter: 308400 | Total Loss: 0.003992 | Recon Loss: 0.003436 | Commit Loss: 0.001112 | Perplexity: 1521.955455
2025-09-27 08:58:04,424 Stage: Train 0.5 | Epoch: 203 | Iter: 308600 | Total Loss: 0.003701 | Recon Loss: 0.003147 | Commit Loss: 0.001108 | Perplexity: 1530.681294
2025-09-27 08:58:50,274 Stage: Train 0.5 | Epoch: 203 | Iter: 308800 | Total Loss: 0.003775 | Recon Loss: 0.003221 | Commit Loss: 0.001108 | Perplexity: 1524.694199
2025-09-27 08:59:36,361 Stage: Train 0.5 | Epoch: 203 | Iter: 309000 | Total Loss: 0.003758 | Recon Loss: 0.003197 | Commit Loss: 0.001121 | Perplexity: 1532.218151
2025-09-27 09:00:22,483 Stage: Train 0.5 | Epoch: 203 | Iter: 309200 | Total Loss: 0.003768 | Recon Loss: 0.003211 | Commit Loss: 0.001114 | Perplexity: 1530.178243
2025-09-27 09:01:08,493 Stage: Train 0.5 | Epoch: 203 | Iter: 309400 | Total Loss: 0.003796 | Recon Loss: 0.003237 | Commit Loss: 0.001117 | Perplexity: 1529.933104
2025-09-27 09:01:54,360 Stage: Train 0.5 | Epoch: 203 | Iter: 309600 | Total Loss: 0.003744 | Recon Loss: 0.003187 | Commit Loss: 0.001113 | Perplexity: 1526.524991
2025-09-27 09:02:40,310 Stage: Train 0.5 | Epoch: 203 | Iter: 309800 | Total Loss: 0.003737 | Recon Loss: 0.003177 | Commit Loss: 0.001119 | Perplexity: 1531.623577
Trainning Epoch:  62%|██████▏   | 204/330 [18:50:24<12:14:18, 349.67s/it]2025-09-27 09:03:26,763 Stage: Train 0.5 | Epoch: 204 | Iter: 310000 | Total Loss: 0.003797 | Recon Loss: 0.003243 | Commit Loss: 0.001108 | Perplexity: 1524.725479
2025-09-27 09:04:12,907 Stage: Train 0.5 | Epoch: 204 | Iter: 310200 | Total Loss: 0.003829 | Recon Loss: 0.003273 | Commit Loss: 0.001112 | Perplexity: 1528.358899
2025-09-27 09:04:58,679 Stage: Train 0.5 | Epoch: 204 | Iter: 310400 | Total Loss: 0.003650 | Recon Loss: 0.003093 | Commit Loss: 0.001115 | Perplexity: 1527.237513
2025-09-27 09:05:44,531 Stage: Train 0.5 | Epoch: 204 | Iter: 310600 | Total Loss: 0.003744 | Recon Loss: 0.003191 | Commit Loss: 0.001106 | Perplexity: 1524.498202
2025-09-27 09:06:30,599 Stage: Train 0.5 | Epoch: 204 | Iter: 310800 | Total Loss: 0.003778 | Recon Loss: 0.003221 | Commit Loss: 0.001115 | Perplexity: 1531.379557
2025-09-27 09:07:16,678 Stage: Train 0.5 | Epoch: 204 | Iter: 311000 | Total Loss: 0.003757 | Recon Loss: 0.003202 | Commit Loss: 0.001110 | Perplexity: 1530.272511
2025-09-27 09:08:02,720 Stage: Train 0.5 | Epoch: 204 | Iter: 311200 | Total Loss: 0.003767 | Recon Loss: 0.003208 | Commit Loss: 0.001119 | Perplexity: 1529.375617
Trainning Epoch:  62%|██████▏   | 205/330 [18:56:13<12:08:33, 349.71s/it]2025-09-27 09:08:49,071 Stage: Train 0.5 | Epoch: 205 | Iter: 311400 | Total Loss: 0.003787 | Recon Loss: 0.003233 | Commit Loss: 0.001109 | Perplexity: 1527.507358
2025-09-27 09:09:35,277 Stage: Train 0.5 | Epoch: 205 | Iter: 311600 | Total Loss: 0.003663 | Recon Loss: 0.003109 | Commit Loss: 0.001108 | Perplexity: 1530.422834
2025-09-27 09:10:21,389 Stage: Train 0.5 | Epoch: 205 | Iter: 311800 | Total Loss: 0.003709 | Recon Loss: 0.003155 | Commit Loss: 0.001110 | Perplexity: 1527.261386
2025-09-27 09:11:07,643 Stage: Train 0.5 | Epoch: 205 | Iter: 312000 | Total Loss: 0.003819 | Recon Loss: 0.003264 | Commit Loss: 0.001111 | Perplexity: 1526.960291
2025-09-27 09:11:53,418 Stage: Train 0.5 | Epoch: 205 | Iter: 312200 | Total Loss: 0.003742 | Recon Loss: 0.003190 | Commit Loss: 0.001104 | Perplexity: 1525.410374
2025-09-27 09:12:39,435 Stage: Train 0.5 | Epoch: 205 | Iter: 312400 | Total Loss: 0.003712 | Recon Loss: 0.003155 | Commit Loss: 0.001114 | Perplexity: 1529.485243
2025-09-27 09:13:25,369 Stage: Train 0.5 | Epoch: 205 | Iter: 312600 | Total Loss: 0.003721 | Recon Loss: 0.003167 | Commit Loss: 0.001106 | Perplexity: 1527.356342
2025-09-27 09:14:11,473 Stage: Train 0.5 | Epoch: 205 | Iter: 312800 | Total Loss: 0.003754 | Recon Loss: 0.003197 | Commit Loss: 0.001113 | Perplexity: 1529.347935
Trainning Epoch:  62%|██████▏   | 206/330 [19:02:03<12:02:55, 349.80s/it]2025-09-27 09:14:57,644 Stage: Train 0.5 | Epoch: 206 | Iter: 313000 | Total Loss: 0.003736 | Recon Loss: 0.003179 | Commit Loss: 0.001113 | Perplexity: 1532.578601
2025-09-27 09:15:43,711 Stage: Train 0.5 | Epoch: 206 | Iter: 313200 | Total Loss: 0.003742 | Recon Loss: 0.003188 | Commit Loss: 0.001107 | Perplexity: 1529.429039
2025-09-27 09:16:29,775 Stage: Train 0.5 | Epoch: 206 | Iter: 313400 | Total Loss: 0.003711 | Recon Loss: 0.003155 | Commit Loss: 0.001111 | Perplexity: 1531.677196
2025-09-27 09:17:15,904 Stage: Train 0.5 | Epoch: 206 | Iter: 313600 | Total Loss: 0.003684 | Recon Loss: 0.003129 | Commit Loss: 0.001110 | Perplexity: 1525.637994
2025-09-27 09:18:01,816 Stage: Train 0.5 | Epoch: 206 | Iter: 313800 | Total Loss: 0.003738 | Recon Loss: 0.003186 | Commit Loss: 0.001103 | Perplexity: 1523.854301
2025-09-27 09:18:47,756 Stage: Train 0.5 | Epoch: 206 | Iter: 314000 | Total Loss: 0.003727 | Recon Loss: 0.003171 | Commit Loss: 0.001111 | Perplexity: 1529.966804
2025-09-27 09:19:33,624 Stage: Train 0.5 | Epoch: 206 | Iter: 314200 | Total Loss: 0.003745 | Recon Loss: 0.003189 | Commit Loss: 0.001112 | Perplexity: 1526.973477
2025-09-27 09:20:19,675 Stage: Train 0.5 | Epoch: 206 | Iter: 314400 | Total Loss: 0.003722 | Recon Loss: 0.003164 | Commit Loss: 0.001114 | Perplexity: 1533.100732
Trainning Epoch:  63%|██████▎   | 207/330 [19:07:53<11:56:57, 349.73s/it]2025-09-27 09:21:06,022 Stage: Train 0.5 | Epoch: 207 | Iter: 314600 | Total Loss: 0.003691 | Recon Loss: 0.003137 | Commit Loss: 0.001107 | Perplexity: 1527.574310
2025-09-27 09:21:51,992 Stage: Train 0.5 | Epoch: 207 | Iter: 314800 | Total Loss: 0.003765 | Recon Loss: 0.003211 | Commit Loss: 0.001108 | Perplexity: 1530.921008
2025-09-27 09:22:38,213 Stage: Train 0.5 | Epoch: 207 | Iter: 315000 | Total Loss: 0.003680 | Recon Loss: 0.003127 | Commit Loss: 0.001107 | Perplexity: 1527.614356
2025-09-27 09:23:24,115 Stage: Train 0.5 | Epoch: 207 | Iter: 315200 | Total Loss: 0.003777 | Recon Loss: 0.003219 | Commit Loss: 0.001114 | Perplexity: 1531.155607
2025-09-27 09:24:10,024 Stage: Train 0.5 | Epoch: 207 | Iter: 315400 | Total Loss: 0.003777 | Recon Loss: 0.003221 | Commit Loss: 0.001112 | Perplexity: 1530.317921
2025-09-27 09:24:56,170 Stage: Train 0.5 | Epoch: 207 | Iter: 315600 | Total Loss: 0.003627 | Recon Loss: 0.003074 | Commit Loss: 0.001106 | Perplexity: 1529.091458
2025-09-27 09:25:42,135 Stage: Train 0.5 | Epoch: 207 | Iter: 315800 | Total Loss: 0.003780 | Recon Loss: 0.003223 | Commit Loss: 0.001114 | Perplexity: 1532.654366
Trainning Epoch:  63%|██████▎   | 208/330 [19:13:43<11:51:07, 349.73s/it]2025-09-27 09:26:28,290 Stage: Train 0.5 | Epoch: 208 | Iter: 316000 | Total Loss: 0.003657 | Recon Loss: 0.003102 | Commit Loss: 0.001110 | Perplexity: 1531.860715
2025-09-27 09:27:14,483 Stage: Train 0.5 | Epoch: 208 | Iter: 316200 | Total Loss: 0.003774 | Recon Loss: 0.003218 | Commit Loss: 0.001112 | Perplexity: 1530.869669
2025-09-27 09:28:00,456 Stage: Train 0.5 | Epoch: 208 | Iter: 316400 | Total Loss: 0.003767 | Recon Loss: 0.003214 | Commit Loss: 0.001106 | Perplexity: 1528.114571
2025-09-27 09:28:46,560 Stage: Train 0.5 | Epoch: 208 | Iter: 316600 | Total Loss: 0.003705 | Recon Loss: 0.003152 | Commit Loss: 0.001107 | Perplexity: 1529.194315
2025-09-27 09:29:32,682 Stage: Train 0.5 | Epoch: 208 | Iter: 316800 | Total Loss: 0.003686 | Recon Loss: 0.003134 | Commit Loss: 0.001105 | Perplexity: 1529.825706
2025-09-27 09:30:18,788 Stage: Train 0.5 | Epoch: 208 | Iter: 317000 | Total Loss: 0.003683 | Recon Loss: 0.003130 | Commit Loss: 0.001106 | Perplexity: 1526.445004
2025-09-27 09:31:04,956 Stage: Train 0.5 | Epoch: 208 | Iter: 317200 | Total Loss: 0.003876 | Recon Loss: 0.003323 | Commit Loss: 0.001106 | Perplexity: 1532.147906
2025-09-27 09:31:51,195 Stage: Train 0.5 | Epoch: 208 | Iter: 317400 | Total Loss: 0.003696 | Recon Loss: 0.003140 | Commit Loss: 0.001112 | Perplexity: 1531.998149
Trainning Epoch:  63%|██████▎   | 209/330 [19:19:33<11:45:47, 349.98s/it]2025-09-27 09:32:37,419 Stage: Train 0.5 | Epoch: 209 | Iter: 317600 | Total Loss: 0.003904 | Recon Loss: 0.003346 | Commit Loss: 0.001115 | Perplexity: 1532.450184
2025-09-27 09:33:22,917 Stage: Train 0.5 | Epoch: 209 | Iter: 317800 | Total Loss: 0.003578 | Recon Loss: 0.003023 | Commit Loss: 0.001110 | Perplexity: 1534.991514
2025-09-27 09:34:08,904 Stage: Train 0.5 | Epoch: 209 | Iter: 318000 | Total Loss: 0.003725 | Recon Loss: 0.003170 | Commit Loss: 0.001110 | Perplexity: 1532.370723
2025-09-27 09:34:54,700 Stage: Train 0.5 | Epoch: 209 | Iter: 318200 | Total Loss: 0.003753 | Recon Loss: 0.003204 | Commit Loss: 0.001097 | Perplexity: 1528.404744
2025-09-27 09:35:40,861 Stage: Train 0.5 | Epoch: 209 | Iter: 318400 | Total Loss: 0.003797 | Recon Loss: 0.003248 | Commit Loss: 0.001097 | Perplexity: 1525.991163
2025-09-27 09:36:26,905 Stage: Train 0.5 | Epoch: 209 | Iter: 318600 | Total Loss: 0.003744 | Recon Loss: 0.003189 | Commit Loss: 0.001109 | Perplexity: 1530.491310
2025-09-27 09:37:12,884 Stage: Train 0.5 | Epoch: 209 | Iter: 318800 | Total Loss: 0.003727 | Recon Loss: 0.003167 | Commit Loss: 0.001121 | Perplexity: 1536.051613
Trainning Epoch:  64%|██████▎   | 210/330 [19:25:22<11:39:23, 349.70s/it]2025-09-27 09:37:59,167 Stage: Train 0.5 | Epoch: 210 | Iter: 319000 | Total Loss: 0.003675 | Recon Loss: 0.003123 | Commit Loss: 0.001102 | Perplexity: 1532.340596
2025-09-27 09:38:45,325 Stage: Train 0.5 | Epoch: 210 | Iter: 319200 | Total Loss: 0.003716 | Recon Loss: 0.003166 | Commit Loss: 0.001101 | Perplexity: 1535.463665
2025-09-27 09:39:31,527 Stage: Train 0.5 | Epoch: 210 | Iter: 319400 | Total Loss: 0.003728 | Recon Loss: 0.003176 | Commit Loss: 0.001103 | Perplexity: 1530.621408
2025-09-27 09:40:17,221 Stage: Train 0.5 | Epoch: 210 | Iter: 319600 | Total Loss: 0.003780 | Recon Loss: 0.003227 | Commit Loss: 0.001105 | Perplexity: 1528.539544
2025-09-27 09:41:03,353 Stage: Train 0.5 | Epoch: 210 | Iter: 319800 | Total Loss: 0.003693 | Recon Loss: 0.003139 | Commit Loss: 0.001109 | Perplexity: 1532.995720
2025-09-27 09:41:49,420 Stage: Train 0.5 | Epoch: 210 | Iter: 320000 | Total Loss: 0.003748 | Recon Loss: 0.003193 | Commit Loss: 0.001109 | Perplexity: 1533.498990
2025-09-27 09:41:49,420 Saving model at iteration 320000
2025-09-27 09:41:49,725 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000
2025-09-27 09:41:50,002 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/model.safetensors
2025-09-27 09:41:50,422 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/optimizer.bin
2025-09-27 09:41:50,422 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/scheduler.bin
2025-09-27 09:41:50,422 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/sampler.bin
2025-09-27 09:41:50,423 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/random_states_0.pkl
2025-09-27 09:42:36,903 Stage: Train 0.5 | Epoch: 210 | Iter: 320200 | Total Loss: 0.003643 | Recon Loss: 0.003094 | Commit Loss: 0.001099 | Perplexity: 1529.896659
2025-09-27 09:43:22,898 Stage: Train 0.5 | Epoch: 210 | Iter: 320400 | Total Loss: 0.003721 | Recon Loss: 0.003167 | Commit Loss: 0.001107 | Perplexity: 1529.781239
Trainning Epoch:  64%|██████▍   | 211/330 [19:31:14<11:34:34, 350.21s/it]2025-09-27 09:44:09,267 Stage: Train 0.5 | Epoch: 211 | Iter: 320600 | Total Loss: 0.003750 | Recon Loss: 0.003203 | Commit Loss: 0.001094 | Perplexity: 1524.608879
2025-09-27 09:44:55,335 Stage: Train 0.5 | Epoch: 211 | Iter: 320800 | Total Loss: 0.003726 | Recon Loss: 0.003174 | Commit Loss: 0.001103 | Perplexity: 1531.684495
2025-09-27 09:45:41,493 Stage: Train 0.5 | Epoch: 211 | Iter: 321000 | Total Loss: 0.003702 | Recon Loss: 0.003152 | Commit Loss: 0.001101 | Perplexity: 1528.351465
2025-09-27 09:46:27,610 Stage: Train 0.5 | Epoch: 211 | Iter: 321200 | Total Loss: 0.003610 | Recon Loss: 0.003056 | Commit Loss: 0.001107 | Perplexity: 1531.165111
2025-09-27 09:47:13,541 Stage: Train 0.5 | Epoch: 211 | Iter: 321400 | Total Loss: 0.003715 | Recon Loss: 0.003161 | Commit Loss: 0.001108 | Perplexity: 1528.396311
2025-09-27 09:47:59,292 Stage: Train 0.5 | Epoch: 211 | Iter: 321600 | Total Loss: 0.003732 | Recon Loss: 0.003180 | Commit Loss: 0.001104 | Perplexity: 1530.746973
2025-09-27 09:48:45,249 Stage: Train 0.5 | Epoch: 211 | Iter: 321800 | Total Loss: 0.003783 | Recon Loss: 0.003231 | Commit Loss: 0.001104 | Perplexity: 1529.094415
2025-09-27 09:49:31,363 Stage: Train 0.5 | Epoch: 211 | Iter: 322000 | Total Loss: 0.003657 | Recon Loss: 0.003102 | Commit Loss: 0.001111 | Perplexity: 1534.694332
Trainning Epoch:  64%|██████▍   | 212/330 [19:37:03<11:28:30, 350.09s/it]2025-09-27 09:50:17,640 Stage: Train 0.5 | Epoch: 212 | Iter: 322200 | Total Loss: 0.003663 | Recon Loss: 0.003112 | Commit Loss: 0.001103 | Perplexity: 1529.155254
2025-09-27 09:51:03,764 Stage: Train 0.5 | Epoch: 212 | Iter: 322400 | Total Loss: 0.003737 | Recon Loss: 0.003189 | Commit Loss: 0.001097 | Perplexity: 1529.729969
2025-09-27 09:51:49,646 Stage: Train 0.5 | Epoch: 212 | Iter: 322600 | Total Loss: 0.003696 | Recon Loss: 0.003146 | Commit Loss: 0.001101 | Perplexity: 1531.440909
2025-09-27 09:52:35,627 Stage: Train 0.5 | Epoch: 212 | Iter: 322800 | Total Loss: 0.003720 | Recon Loss: 0.003165 | Commit Loss: 0.001111 | Perplexity: 1537.278808
2025-09-27 09:53:21,649 Stage: Train 0.5 | Epoch: 212 | Iter: 323000 | Total Loss: 0.003726 | Recon Loss: 0.003177 | Commit Loss: 0.001098 | Perplexity: 1527.687381
2025-09-27 09:54:07,676 Stage: Train 0.5 | Epoch: 212 | Iter: 323200 | Total Loss: 0.003671 | Recon Loss: 0.003118 | Commit Loss: 0.001106 | Perplexity: 1533.566175
2025-09-27 09:54:53,368 Stage: Train 0.5 | Epoch: 212 | Iter: 323400 | Total Loss: 0.003770 | Recon Loss: 0.003214 | Commit Loss: 0.001111 | Perplexity: 1536.127401
Trainning Epoch:  65%|██████▍   | 213/330 [19:42:53<11:22:18, 349.90s/it]2025-09-27 09:55:39,722 Stage: Train 0.5 | Epoch: 213 | Iter: 323600 | Total Loss: 0.003670 | Recon Loss: 0.003119 | Commit Loss: 0.001101 | Perplexity: 1533.669919
2025-09-27 09:56:25,666 Stage: Train 0.5 | Epoch: 213 | Iter: 323800 | Total Loss: 0.003699 | Recon Loss: 0.003150 | Commit Loss: 0.001098 | Perplexity: 1531.957160
2025-09-27 09:57:09,866 Stage: Train 0.5 | Epoch: 213 | Iter: 324000 | Total Loss: 0.003720 | Recon Loss: 0.003172 | Commit Loss: 0.001096 | Perplexity: 1529.441755
2025-09-27 09:57:55,598 Stage: Train 0.5 | Epoch: 213 | Iter: 324200 | Total Loss: 0.003617 | Recon Loss: 0.003066 | Commit Loss: 0.001103 | Perplexity: 1532.939272
2025-09-27 09:58:41,659 Stage: Train 0.5 | Epoch: 213 | Iter: 324400 | Total Loss: 0.003745 | Recon Loss: 0.003190 | Commit Loss: 0.001109 | Perplexity: 1530.979355
2025-09-27 09:59:27,870 Stage: Train 0.5 | Epoch: 213 | Iter: 324600 | Total Loss: 0.003676 | Recon Loss: 0.003123 | Commit Loss: 0.001106 | Perplexity: 1534.727863
2025-09-27 10:00:14,010 Stage: Train 0.5 | Epoch: 213 | Iter: 324800 | Total Loss: 0.003651 | Recon Loss: 0.003101 | Commit Loss: 0.001100 | Perplexity: 1533.851092
2025-09-27 10:01:00,028 Stage: Train 0.5 | Epoch: 213 | Iter: 325000 | Total Loss: 0.003675 | Recon Loss: 0.003123 | Commit Loss: 0.001104 | Perplexity: 1527.905381
Trainning Epoch:  65%|██████▍   | 214/330 [19:48:41<11:15:20, 349.31s/it]2025-09-27 10:01:46,108 Stage: Train 0.5 | Epoch: 214 | Iter: 325200 | Total Loss: 0.003698 | Recon Loss: 0.003150 | Commit Loss: 0.001097 | Perplexity: 1529.088281
2025-09-27 10:02:32,203 Stage: Train 0.5 | Epoch: 214 | Iter: 325400 | Total Loss: 0.003737 | Recon Loss: 0.003188 | Commit Loss: 0.001098 | Perplexity: 1531.822939
2025-09-27 10:03:18,469 Stage: Train 0.5 | Epoch: 214 | Iter: 325600 | Total Loss: 0.003698 | Recon Loss: 0.003151 | Commit Loss: 0.001094 | Perplexity: 1527.306572
2025-09-27 10:04:04,524 Stage: Train 0.5 | Epoch: 214 | Iter: 325800 | Total Loss: 0.003652 | Recon Loss: 0.003101 | Commit Loss: 0.001101 | Perplexity: 1528.838160
2025-09-27 10:04:50,597 Stage: Train 0.5 | Epoch: 214 | Iter: 326000 | Total Loss: 0.003707 | Recon Loss: 0.003158 | Commit Loss: 0.001099 | Perplexity: 1531.837403
2025-09-27 10:05:36,631 Stage: Train 0.5 | Epoch: 214 | Iter: 326200 | Total Loss: 0.003728 | Recon Loss: 0.003177 | Commit Loss: 0.001101 | Perplexity: 1534.502904
2025-09-27 10:06:22,707 Stage: Train 0.5 | Epoch: 214 | Iter: 326400 | Total Loss: 0.003617 | Recon Loss: 0.003062 | Commit Loss: 0.001109 | Perplexity: 1533.168427
Trainning Epoch:  65%|██████▌   | 215/330 [19:54:31<11:09:57, 349.54s/it]2025-09-27 10:07:08,969 Stage: Train 0.5 | Epoch: 215 | Iter: 326600 | Total Loss: 0.003724 | Recon Loss: 0.003171 | Commit Loss: 0.001106 | Perplexity: 1536.491713
2025-09-27 10:07:55,151 Stage: Train 0.5 | Epoch: 215 | Iter: 326800 | Total Loss: 0.003683 | Recon Loss: 0.003133 | Commit Loss: 0.001100 | Perplexity: 1535.921537
2025-09-27 10:08:41,391 Stage: Train 0.5 | Epoch: 215 | Iter: 327000 | Total Loss: 0.003691 | Recon Loss: 0.003140 | Commit Loss: 0.001100 | Perplexity: 1529.665375
2025-09-27 10:09:27,187 Stage: Train 0.5 | Epoch: 215 | Iter: 327200 | Total Loss: 0.003786 | Recon Loss: 0.003231 | Commit Loss: 0.001109 | Perplexity: 1533.853568
2025-09-27 10:10:13,472 Stage: Train 0.5 | Epoch: 215 | Iter: 327400 | Total Loss: 0.003656 | Recon Loss: 0.003107 | Commit Loss: 0.001097 | Perplexity: 1534.421552
2025-09-27 10:10:59,738 Stage: Train 0.5 | Epoch: 215 | Iter: 327600 | Total Loss: 0.003707 | Recon Loss: 0.003157 | Commit Loss: 0.001101 | Perplexity: 1533.350585
2025-09-27 10:11:45,746 Stage: Train 0.5 | Epoch: 215 | Iter: 327800 | Total Loss: 0.003748 | Recon Loss: 0.003199 | Commit Loss: 0.001098 | Perplexity: 1531.279854
2025-09-27 10:12:31,884 Stage: Train 0.5 | Epoch: 215 | Iter: 328000 | Total Loss: 0.003656 | Recon Loss: 0.003107 | Commit Loss: 0.001097 | Perplexity: 1531.526735
Trainning Epoch:  65%|██████▌   | 216/330 [20:00:22<11:04:47, 349.89s/it]2025-09-27 10:13:18,351 Stage: Train 0.5 | Epoch: 216 | Iter: 328200 | Total Loss: 0.003693 | Recon Loss: 0.003144 | Commit Loss: 0.001097 | Perplexity: 1527.668325
2025-09-27 10:14:04,350 Stage: Train 0.5 | Epoch: 216 | Iter: 328400 | Total Loss: 0.003669 | Recon Loss: 0.003121 | Commit Loss: 0.001095 | Perplexity: 1531.342254
2025-09-27 10:14:50,454 Stage: Train 0.5 | Epoch: 216 | Iter: 328600 | Total Loss: 0.003785 | Recon Loss: 0.003236 | Commit Loss: 0.001098 | Perplexity: 1534.812443
2025-09-27 10:15:36,612 Stage: Train 0.5 | Epoch: 216 | Iter: 328800 | Total Loss: 0.003736 | Recon Loss: 0.003188 | Commit Loss: 0.001097 | Perplexity: 1535.240694
2025-09-27 10:16:22,345 Stage: Train 0.5 | Epoch: 216 | Iter: 329000 | Total Loss: 0.003615 | Recon Loss: 0.003065 | Commit Loss: 0.001100 | Perplexity: 1536.914298
2025-09-27 10:17:08,446 Stage: Train 0.5 | Epoch: 216 | Iter: 329200 | Total Loss: 0.003634 | Recon Loss: 0.003083 | Commit Loss: 0.001102 | Perplexity: 1536.959150
2025-09-27 10:17:54,424 Stage: Train 0.5 | Epoch: 216 | Iter: 329400 | Total Loss: 0.003677 | Recon Loss: 0.003127 | Commit Loss: 0.001099 | Perplexity: 1531.334845
2025-09-27 10:18:40,503 Stage: Train 0.5 | Epoch: 216 | Iter: 329600 | Total Loss: 0.003744 | Recon Loss: 0.003196 | Commit Loss: 0.001097 | Perplexity: 1530.866069
Trainning Epoch:  66%|██████▌   | 217/330 [20:06:11<10:58:54, 349.86s/it]2025-09-27 10:19:26,630 Stage: Train 0.5 | Epoch: 217 | Iter: 329800 | Total Loss: 0.003620 | Recon Loss: 0.003074 | Commit Loss: 0.001092 | Perplexity: 1530.527676
2025-09-27 10:20:12,632 Stage: Train 0.5 | Epoch: 217 | Iter: 330000 | Total Loss: 0.003678 | Recon Loss: 0.003129 | Commit Loss: 0.001097 | Perplexity: 1529.003141
2025-09-27 10:20:58,651 Stage: Train 0.5 | Epoch: 217 | Iter: 330200 | Total Loss: 0.003619 | Recon Loss: 0.003070 | Commit Loss: 0.001099 | Perplexity: 1534.760798
2025-09-27 10:21:44,618 Stage: Train 0.5 | Epoch: 217 | Iter: 330400 | Total Loss: 0.003749 | Recon Loss: 0.003199 | Commit Loss: 0.001101 | Perplexity: 1536.837468
2025-09-27 10:22:30,588 Stage: Train 0.5 | Epoch: 217 | Iter: 330600 | Total Loss: 0.003641 | Recon Loss: 0.003092 | Commit Loss: 0.001097 | Perplexity: 1532.271443
2025-09-27 10:23:16,257 Stage: Train 0.5 | Epoch: 217 | Iter: 330800 | Total Loss: 0.003638 | Recon Loss: 0.003090 | Commit Loss: 0.001097 | Perplexity: 1534.111151
2025-09-27 10:24:02,145 Stage: Train 0.5 | Epoch: 217 | Iter: 331000 | Total Loss: 0.003684 | Recon Loss: 0.003131 | Commit Loss: 0.001104 | Perplexity: 1534.110217
Trainning Epoch:  66%|██████▌   | 218/330 [20:12:01<10:52:39, 349.64s/it]2025-09-27 10:24:48,651 Stage: Train 0.5 | Epoch: 218 | Iter: 331200 | Total Loss: 0.003662 | Recon Loss: 0.003116 | Commit Loss: 0.001092 | Perplexity: 1528.852422
2025-09-27 10:25:34,660 Stage: Train 0.5 | Epoch: 218 | Iter: 331400 | Total Loss: 0.003653 | Recon Loss: 0.003102 | Commit Loss: 0.001102 | Perplexity: 1536.445613
2025-09-27 10:26:20,908 Stage: Train 0.5 | Epoch: 218 | Iter: 331600 | Total Loss: 0.003689 | Recon Loss: 0.003140 | Commit Loss: 0.001098 | Perplexity: 1532.619166
2025-09-27 10:27:06,795 Stage: Train 0.5 | Epoch: 218 | Iter: 331800 | Total Loss: 0.003667 | Recon Loss: 0.003121 | Commit Loss: 0.001093 | Perplexity: 1528.529019
2025-09-27 10:27:52,946 Stage: Train 0.5 | Epoch: 218 | Iter: 332000 | Total Loss: 0.003716 | Recon Loss: 0.003167 | Commit Loss: 0.001100 | Perplexity: 1527.926064
2025-09-27 10:28:39,206 Stage: Train 0.5 | Epoch: 218 | Iter: 332200 | Total Loss: 0.003699 | Recon Loss: 0.003149 | Commit Loss: 0.001101 | Perplexity: 1533.978734
2025-09-27 10:29:25,471 Stage: Train 0.5 | Epoch: 218 | Iter: 332400 | Total Loss: 0.003653 | Recon Loss: 0.003108 | Commit Loss: 0.001090 | Perplexity: 1529.011402
2025-09-27 10:30:11,476 Stage: Train 0.5 | Epoch: 218 | Iter: 332600 | Total Loss: 0.003731 | Recon Loss: 0.003181 | Commit Loss: 0.001098 | Perplexity: 1535.551913
Trainning Epoch:  66%|██████▋   | 219/330 [20:17:51<10:47:15, 349.87s/it]2025-09-27 10:30:57,564 Stage: Train 0.5 | Epoch: 219 | Iter: 332800 | Total Loss: 0.003837 | Recon Loss: 0.003287 | Commit Loss: 0.001099 | Perplexity: 1535.904563
2025-09-27 10:31:43,730 Stage: Train 0.5 | Epoch: 219 | Iter: 333000 | Total Loss: 0.003610 | Recon Loss: 0.003068 | Commit Loss: 0.001084 | Perplexity: 1527.073932
2025-09-27 10:32:29,745 Stage: Train 0.5 | Epoch: 219 | Iter: 333200 | Total Loss: 0.003616 | Recon Loss: 0.003071 | Commit Loss: 0.001091 | Perplexity: 1531.158312
2025-09-27 10:33:16,230 Stage: Train 0.5 | Epoch: 219 | Iter: 333400 | Total Loss: 0.003643 | Recon Loss: 0.003093 | Commit Loss: 0.001101 | Perplexity: 1534.070703
2025-09-27 10:34:02,314 Stage: Train 0.5 | Epoch: 219 | Iter: 333600 | Total Loss: 0.003718 | Recon Loss: 0.003169 | Commit Loss: 0.001098 | Perplexity: 1531.259579
2025-09-27 10:34:48,394 Stage: Train 0.5 | Epoch: 219 | Iter: 333800 | Total Loss: 0.003650 | Recon Loss: 0.003102 | Commit Loss: 0.001095 | Perplexity: 1534.619076
2025-09-27 10:35:34,432 Stage: Train 0.5 | Epoch: 219 | Iter: 334000 | Total Loss: 0.003695 | Recon Loss: 0.003144 | Commit Loss: 0.001102 | Perplexity: 1536.447668
Trainning Epoch:  67%|██████▋   | 220/330 [20:23:42<10:41:47, 350.07s/it]2025-09-27 10:36:20,667 Stage: Train 0.5 | Epoch: 220 | Iter: 334200 | Total Loss: 0.003633 | Recon Loss: 0.003083 | Commit Loss: 0.001101 | Perplexity: 1536.701918
2025-09-27 10:37:06,672 Stage: Train 0.5 | Epoch: 220 | Iter: 334400 | Total Loss: 0.003652 | Recon Loss: 0.003105 | Commit Loss: 0.001094 | Perplexity: 1531.374625
2025-09-27 10:37:52,256 Stage: Train 0.5 | Epoch: 220 | Iter: 334600 | Total Loss: 0.003714 | Recon Loss: 0.003170 | Commit Loss: 0.001086 | Perplexity: 1529.860615
2025-09-27 10:38:38,403 Stage: Train 0.5 | Epoch: 220 | Iter: 334800 | Total Loss: 0.003646 | Recon Loss: 0.003098 | Commit Loss: 0.001095 | Perplexity: 1536.393589
2025-09-27 10:39:24,472 Stage: Train 0.5 | Epoch: 220 | Iter: 335000 | Total Loss: 0.003630 | Recon Loss: 0.003080 | Commit Loss: 0.001099 | Perplexity: 1537.880236
2025-09-27 10:40:10,388 Stage: Train 0.5 | Epoch: 220 | Iter: 335200 | Total Loss: 0.003705 | Recon Loss: 0.003159 | Commit Loss: 0.001091 | Perplexity: 1528.866582
2025-09-27 10:40:56,595 Stage: Train 0.5 | Epoch: 220 | Iter: 335400 | Total Loss: 0.003642 | Recon Loss: 0.003095 | Commit Loss: 0.001093 | Perplexity: 1532.582224
2025-09-27 10:41:42,645 Stage: Train 0.5 | Epoch: 220 | Iter: 335600 | Total Loss: 0.003699 | Recon Loss: 0.003153 | Commit Loss: 0.001093 | Perplexity: 1529.993023
Trainning Epoch:  67%|██████▋   | 221/330 [20:29:31<10:35:42, 349.93s/it]2025-09-27 10:42:28,979 Stage: Train 0.5 | Epoch: 221 | Iter: 335800 | Total Loss: 0.003653 | Recon Loss: 0.003108 | Commit Loss: 0.001090 | Perplexity: 1531.285888
2025-09-27 10:43:14,920 Stage: Train 0.5 | Epoch: 221 | Iter: 336000 | Total Loss: 0.003622 | Recon Loss: 0.003076 | Commit Loss: 0.001092 | Perplexity: 1533.071081
2025-09-27 10:44:01,096 Stage: Train 0.5 | Epoch: 221 | Iter: 336200 | Total Loss: 0.003646 | Recon Loss: 0.003100 | Commit Loss: 0.001092 | Perplexity: 1533.278867
2025-09-27 10:44:46,933 Stage: Train 0.5 | Epoch: 221 | Iter: 336400 | Total Loss: 0.003726 | Recon Loss: 0.003183 | Commit Loss: 0.001086 | Perplexity: 1533.612948
2025-09-27 10:45:33,151 Stage: Train 0.5 | Epoch: 221 | Iter: 336600 | Total Loss: 0.003653 | Recon Loss: 0.003108 | Commit Loss: 0.001092 | Perplexity: 1527.368335
2025-09-27 10:46:19,267 Stage: Train 0.5 | Epoch: 221 | Iter: 336800 | Total Loss: 0.003904 | Recon Loss: 0.003354 | Commit Loss: 0.001099 | Perplexity: 1532.383919
2025-09-27 10:47:05,402 Stage: Train 0.5 | Epoch: 221 | Iter: 337000 | Total Loss: 0.003540 | Recon Loss: 0.002990 | Commit Loss: 0.001100 | Perplexity: 1537.940036
2025-09-27 10:47:51,518 Stage: Train 0.5 | Epoch: 221 | Iter: 337200 | Total Loss: 0.003664 | Recon Loss: 0.003117 | Commit Loss: 0.001093 | Perplexity: 1530.883243
Trainning Epoch:  67%|██████▋   | 222/330 [20:35:21<10:30:01, 350.02s/it]2025-09-27 10:48:37,719 Stage: Train 0.5 | Epoch: 222 | Iter: 337400 | Total Loss: 0.003691 | Recon Loss: 0.003148 | Commit Loss: 0.001087 | Perplexity: 1528.021588
2025-09-27 10:49:23,757 Stage: Train 0.5 | Epoch: 222 | Iter: 337600 | Total Loss: 0.003634 | Recon Loss: 0.003089 | Commit Loss: 0.001090 | Perplexity: 1532.494350
2025-09-27 10:50:09,695 Stage: Train 0.5 | Epoch: 222 | Iter: 337800 | Total Loss: 0.003663 | Recon Loss: 0.003116 | Commit Loss: 0.001094 | Perplexity: 1534.007018
2025-09-27 10:50:55,796 Stage: Train 0.5 | Epoch: 222 | Iter: 338000 | Total Loss: 0.003683 | Recon Loss: 0.003139 | Commit Loss: 0.001087 | Perplexity: 1536.112443
2025-09-27 10:51:41,585 Stage: Train 0.5 | Epoch: 222 | Iter: 338200 | Total Loss: 0.003671 | Recon Loss: 0.003125 | Commit Loss: 0.001093 | Perplexity: 1535.186942
2025-09-27 10:52:27,803 Stage: Train 0.5 | Epoch: 222 | Iter: 338400 | Total Loss: 0.003732 | Recon Loss: 0.003186 | Commit Loss: 0.001091 | Perplexity: 1532.397873
2025-09-27 10:53:13,816 Stage: Train 0.5 | Epoch: 222 | Iter: 338600 | Total Loss: 0.003570 | Recon Loss: 0.003023 | Commit Loss: 0.001093 | Perplexity: 1535.607169
Trainning Epoch:  68%|██████▊   | 223/330 [20:41:11<10:23:57, 349.88s/it]2025-09-27 10:53:59,960 Stage: Train 0.5 | Epoch: 223 | Iter: 338800 | Total Loss: 0.003696 | Recon Loss: 0.003152 | Commit Loss: 0.001089 | Perplexity: 1533.551788
2025-09-27 10:54:46,082 Stage: Train 0.5 | Epoch: 223 | Iter: 339000 | Total Loss: 0.003622 | Recon Loss: 0.003076 | Commit Loss: 0.001092 | Perplexity: 1532.612424
2025-09-27 10:55:32,111 Stage: Train 0.5 | Epoch: 223 | Iter: 339200 | Total Loss: 0.003715 | Recon Loss: 0.003169 | Commit Loss: 0.001091 | Perplexity: 1535.186327
2025-09-27 10:56:18,123 Stage: Train 0.5 | Epoch: 223 | Iter: 339400 | Total Loss: 0.003660 | Recon Loss: 0.003112 | Commit Loss: 0.001097 | Perplexity: 1536.474721
2025-09-27 10:57:04,223 Stage: Train 0.5 | Epoch: 223 | Iter: 339600 | Total Loss: 0.003705 | Recon Loss: 0.003161 | Commit Loss: 0.001088 | Perplexity: 1535.815159
2025-09-27 10:57:50,374 Stage: Train 0.5 | Epoch: 223 | Iter: 339800 | Total Loss: 0.003591 | Recon Loss: 0.003051 | Commit Loss: 0.001080 | Perplexity: 1529.707982
2025-09-27 10:58:36,385 Stage: Train 0.5 | Epoch: 223 | Iter: 340000 | Total Loss: 0.003720 | Recon Loss: 0.003175 | Commit Loss: 0.001090 | Perplexity: 1536.082981
2025-09-27 10:58:36,385 Saving model at iteration 340000
2025-09-27 10:58:36,601 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000
2025-09-27 10:58:36,894 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/model.safetensors
2025-09-27 10:58:37,288 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/optimizer.bin
2025-09-27 10:58:37,289 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/scheduler.bin
2025-09-27 10:58:37,289 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/sampler.bin
2025-09-27 10:58:37,290 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/random_states_0.pkl
2025-09-27 10:59:23,301 Stage: Train 0.5 | Epoch: 223 | Iter: 340200 | Total Loss: 0.003626 | Recon Loss: 0.003078 | Commit Loss: 0.001096 | Perplexity: 1534.006077
Trainning Epoch:  68%|██████▊   | 224/330 [20:47:02<10:18:41, 350.21s/it]2025-09-27 11:00:09,500 Stage: Train 0.5 | Epoch: 224 | Iter: 340400 | Total Loss: 0.003576 | Recon Loss: 0.003034 | Commit Loss: 0.001085 | Perplexity: 1535.122319
2025-09-27 11:00:55,532 Stage: Train 0.5 | Epoch: 224 | Iter: 340600 | Total Loss: 0.003686 | Recon Loss: 0.003140 | Commit Loss: 0.001093 | Perplexity: 1535.891963
2025-09-27 11:01:41,610 Stage: Train 0.5 | Epoch: 224 | Iter: 340800 | Total Loss: 0.003647 | Recon Loss: 0.003103 | Commit Loss: 0.001088 | Perplexity: 1533.714256
2025-09-27 11:02:27,725 Stage: Train 0.5 | Epoch: 224 | Iter: 341000 | Total Loss: 0.003627 | Recon Loss: 0.003083 | Commit Loss: 0.001088 | Perplexity: 1535.667427
2025-09-27 11:03:13,882 Stage: Train 0.5 | Epoch: 224 | Iter: 341200 | Total Loss: 0.003618 | Recon Loss: 0.003074 | Commit Loss: 0.001089 | Perplexity: 1530.040933
2025-09-27 11:03:59,918 Stage: Train 0.5 | Epoch: 224 | Iter: 341400 | Total Loss: 0.003677 | Recon Loss: 0.003132 | Commit Loss: 0.001090 | Perplexity: 1532.921393
2025-09-27 11:04:46,061 Stage: Train 0.5 | Epoch: 224 | Iter: 341600 | Total Loss: 0.003639 | Recon Loss: 0.003092 | Commit Loss: 0.001094 | Perplexity: 1534.454433
Trainning Epoch:  68%|██████▊   | 225/330 [20:52:52<10:12:48, 350.18s/it]2025-09-27 11:05:32,305 Stage: Train 0.5 | Epoch: 225 | Iter: 341800 | Total Loss: 0.003638 | Recon Loss: 0.003094 | Commit Loss: 0.001088 | Perplexity: 1534.497386
2025-09-27 11:06:18,294 Stage: Train 0.5 | Epoch: 225 | Iter: 342000 | Total Loss: 0.003629 | Recon Loss: 0.003084 | Commit Loss: 0.001090 | Perplexity: 1533.445670
2025-09-27 11:07:04,389 Stage: Train 0.5 | Epoch: 225 | Iter: 342200 | Total Loss: 0.003625 | Recon Loss: 0.003083 | Commit Loss: 0.001085 | Perplexity: 1533.616614
2025-09-27 11:07:50,431 Stage: Train 0.5 | Epoch: 225 | Iter: 342400 | Total Loss: 0.003617 | Recon Loss: 0.003075 | Commit Loss: 0.001084 | Perplexity: 1532.952355
2025-09-27 11:08:36,558 Stage: Train 0.5 | Epoch: 225 | Iter: 342600 | Total Loss: 0.003608 | Recon Loss: 0.003062 | Commit Loss: 0.001093 | Perplexity: 1538.623862
2025-09-27 11:09:22,728 Stage: Train 0.5 | Epoch: 225 | Iter: 342800 | Total Loss: 0.003671 | Recon Loss: 0.003125 | Commit Loss: 0.001093 | Perplexity: 1537.155409
2025-09-27 11:10:08,827 Stage: Train 0.5 | Epoch: 225 | Iter: 343000 | Total Loss: 0.003654 | Recon Loss: 0.003110 | Commit Loss: 0.001089 | Perplexity: 1532.368186
2025-09-27 11:10:54,855 Stage: Train 0.5 | Epoch: 225 | Iter: 343200 | Total Loss: 0.003616 | Recon Loss: 0.003069 | Commit Loss: 0.001094 | Perplexity: 1539.729382
Trainning Epoch:  68%|██████▊   | 226/330 [20:58:42<10:06:57, 350.16s/it]2025-09-27 11:11:41,138 Stage: Train 0.5 | Epoch: 226 | Iter: 343400 | Total Loss: 0.003642 | Recon Loss: 0.003095 | Commit Loss: 0.001093 | Perplexity: 1534.274253
2025-09-27 11:12:27,277 Stage: Train 0.5 | Epoch: 226 | Iter: 343600 | Total Loss: 0.003625 | Recon Loss: 0.003084 | Commit Loss: 0.001081 | Perplexity: 1531.025125
2025-09-27 11:13:13,138 Stage: Train 0.5 | Epoch: 226 | Iter: 343800 | Total Loss: 0.003728 | Recon Loss: 0.003187 | Commit Loss: 0.001081 | Perplexity: 1533.736548
2025-09-27 11:13:59,273 Stage: Train 0.5 | Epoch: 226 | Iter: 344000 | Total Loss: 0.003576 | Recon Loss: 0.003030 | Commit Loss: 0.001092 | Perplexity: 1538.221869
2025-09-27 11:14:45,614 Stage: Train 0.5 | Epoch: 226 | Iter: 344200 | Total Loss: 0.003630 | Recon Loss: 0.003087 | Commit Loss: 0.001087 | Perplexity: 1535.241852
2025-09-27 11:15:31,908 Stage: Train 0.5 | Epoch: 226 | Iter: 344400 | Total Loss: 0.003653 | Recon Loss: 0.003109 | Commit Loss: 0.001088 | Perplexity: 1533.398568
2025-09-27 11:16:18,008 Stage: Train 0.5 | Epoch: 226 | Iter: 344600 | Total Loss: 0.003630 | Recon Loss: 0.003085 | Commit Loss: 0.001089 | Perplexity: 1531.084067
2025-09-27 11:17:04,220 Stage: Train 0.5 | Epoch: 226 | Iter: 344800 | Total Loss: 0.003775 | Recon Loss: 0.003230 | Commit Loss: 0.001091 | Perplexity: 1533.196850
Trainning Epoch:  69%|██████▉   | 227/330 [21:04:33<10:01:27, 350.36s/it]2025-09-27 11:17:50,755 Stage: Train 0.5 | Epoch: 227 | Iter: 345000 | Total Loss: 0.003553 | Recon Loss: 0.003012 | Commit Loss: 0.001082 | Perplexity: 1537.020896
2025-09-27 11:18:37,061 Stage: Train 0.5 | Epoch: 227 | Iter: 345200 | Total Loss: 0.003673 | Recon Loss: 0.003132 | Commit Loss: 0.001081 | Perplexity: 1532.462839
2025-09-27 11:19:23,343 Stage: Train 0.5 | Epoch: 227 | Iter: 345400 | Total Loss: 0.003717 | Recon Loss: 0.003174 | Commit Loss: 0.001085 | Perplexity: 1532.903829
2025-09-27 11:20:09,322 Stage: Train 0.5 | Epoch: 227 | Iter: 345600 | Total Loss: 0.003658 | Recon Loss: 0.003113 | Commit Loss: 0.001091 | Perplexity: 1536.938296
2025-09-27 11:20:55,476 Stage: Train 0.5 | Epoch: 227 | Iter: 345800 | Total Loss: 0.003564 | Recon Loss: 0.003019 | Commit Loss: 0.001091 | Perplexity: 1533.152809
2025-09-27 11:21:41,668 Stage: Train 0.5 | Epoch: 227 | Iter: 346000 | Total Loss: 0.003659 | Recon Loss: 0.003116 | Commit Loss: 0.001087 | Perplexity: 1534.544632
2025-09-27 11:22:27,850 Stage: Train 0.5 | Epoch: 227 | Iter: 346200 | Total Loss: 0.003668 | Recon Loss: 0.003125 | Commit Loss: 0.001088 | Perplexity: 1536.826378
Trainning Epoch:  69%|██████▉   | 228/330 [21:10:24<9:56:00, 350.59s/it] 2025-09-27 11:23:14,417 Stage: Train 0.5 | Epoch: 228 | Iter: 346400 | Total Loss: 0.003570 | Recon Loss: 0.003028 | Commit Loss: 0.001083 | Perplexity: 1534.299005
2025-09-27 11:24:00,586 Stage: Train 0.5 | Epoch: 228 | Iter: 346600 | Total Loss: 0.003641 | Recon Loss: 0.003098 | Commit Loss: 0.001087 | Perplexity: 1539.381482
2025-09-27 11:24:46,711 Stage: Train 0.5 | Epoch: 228 | Iter: 346800 | Total Loss: 0.003544 | Recon Loss: 0.002999 | Commit Loss: 0.001089 | Perplexity: 1535.732203
2025-09-27 11:25:32,848 Stage: Train 0.5 | Epoch: 228 | Iter: 347000 | Total Loss: 0.003604 | Recon Loss: 0.003061 | Commit Loss: 0.001087 | Perplexity: 1536.418112
2025-09-27 11:26:19,113 Stage: Train 0.5 | Epoch: 228 | Iter: 347200 | Total Loss: 0.003675 | Recon Loss: 0.003128 | Commit Loss: 0.001093 | Perplexity: 1537.015522
2025-09-27 11:27:05,314 Stage: Train 0.5 | Epoch: 228 | Iter: 347400 | Total Loss: 0.003656 | Recon Loss: 0.003113 | Commit Loss: 0.001086 | Perplexity: 1536.120022
2025-09-27 11:27:51,296 Stage: Train 0.5 | Epoch: 228 | Iter: 347600 | Total Loss: 0.003579 | Recon Loss: 0.003035 | Commit Loss: 0.001088 | Perplexity: 1533.869227
2025-09-27 11:28:37,643 Stage: Train 0.5 | Epoch: 228 | Iter: 347800 | Total Loss: 0.003693 | Recon Loss: 0.003152 | Commit Loss: 0.001082 | Perplexity: 1533.777935
Trainning Epoch:  69%|██████▉   | 229/330 [21:16:15<9:50:21, 350.70s/it]2025-09-27 11:29:23,901 Stage: Train 0.5 | Epoch: 229 | Iter: 348000 | Total Loss: 0.003531 | Recon Loss: 0.002990 | Commit Loss: 0.001083 | Perplexity: 1534.796069
2025-09-27 11:30:10,557 Stage: Train 0.5 | Epoch: 229 | Iter: 348200 | Total Loss: 0.003731 | Recon Loss: 0.003187 | Commit Loss: 0.001088 | Perplexity: 1535.908228
2025-09-27 11:30:55,293 Stage: Train 0.5 | Epoch: 229 | Iter: 348400 | Total Loss: 0.003550 | Recon Loss: 0.003011 | Commit Loss: 0.001079 | Perplexity: 1534.045230
2025-09-27 11:31:41,505 Stage: Train 0.5 | Epoch: 229 | Iter: 348600 | Total Loss: 0.003714 | Recon Loss: 0.003171 | Commit Loss: 0.001088 | Perplexity: 1539.074120
2025-09-27 11:32:27,733 Stage: Train 0.5 | Epoch: 229 | Iter: 348800 | Total Loss: 0.003631 | Recon Loss: 0.003089 | Commit Loss: 0.001083 | Perplexity: 1532.603676
2025-09-27 11:33:13,834 Stage: Train 0.5 | Epoch: 229 | Iter: 349000 | Total Loss: 0.003550 | Recon Loss: 0.003009 | Commit Loss: 0.001081 | Perplexity: 1537.875485
2025-09-27 11:34:00,017 Stage: Train 0.5 | Epoch: 229 | Iter: 349200 | Total Loss: 0.003684 | Recon Loss: 0.003143 | Commit Loss: 0.001083 | Perplexity: 1532.839338
Trainning Epoch:  70%|██████▉   | 230/330 [21:22:05<9:43:57, 350.37s/it]2025-09-27 11:34:46,123 Stage: Train 0.5 | Epoch: 230 | Iter: 349400 | Total Loss: 0.003665 | Recon Loss: 0.003121 | Commit Loss: 0.001088 | Perplexity: 1537.997984
2025-09-27 11:35:32,299 Stage: Train 0.5 | Epoch: 230 | Iter: 349600 | Total Loss: 0.003546 | Recon Loss: 0.003006 | Commit Loss: 0.001081 | Perplexity: 1533.020839
2025-09-27 11:36:18,548 Stage: Train 0.5 | Epoch: 230 | Iter: 349800 | Total Loss: 0.003565 | Recon Loss: 0.003025 | Commit Loss: 0.001080 | Perplexity: 1534.312986
2025-09-27 11:37:04,711 Stage: Train 0.5 | Epoch: 230 | Iter: 350000 | Total Loss: 0.003752 | Recon Loss: 0.003208 | Commit Loss: 0.001088 | Perplexity: 1539.021522
2025-09-27 11:37:51,017 Stage: Train 0.5 | Epoch: 230 | Iter: 350200 | Total Loss: 0.003609 | Recon Loss: 0.003065 | Commit Loss: 0.001087 | Perplexity: 1538.425126
2025-09-27 11:38:37,231 Stage: Train 0.5 | Epoch: 230 | Iter: 350400 | Total Loss: 0.003668 | Recon Loss: 0.003126 | Commit Loss: 0.001084 | Perplexity: 1537.975905
2025-09-27 11:39:23,476 Stage: Train 0.5 | Epoch: 230 | Iter: 350600 | Total Loss: 0.003594 | Recon Loss: 0.003054 | Commit Loss: 0.001080 | Perplexity: 1531.593795
2025-09-27 11:40:09,599 Stage: Train 0.5 | Epoch: 230 | Iter: 350800 | Total Loss: 0.003586 | Recon Loss: 0.003043 | Commit Loss: 0.001086 | Perplexity: 1536.744752
Trainning Epoch:  70%|███████   | 231/330 [21:27:56<9:38:32, 350.64s/it]2025-09-27 11:40:56,084 Stage: Train 0.5 | Epoch: 231 | Iter: 351000 | Total Loss: 0.003591 | Recon Loss: 0.003046 | Commit Loss: 0.001090 | Perplexity: 1532.610424
2025-09-27 11:41:41,982 Stage: Train 0.5 | Epoch: 231 | Iter: 351200 | Total Loss: 0.003644 | Recon Loss: 0.003103 | Commit Loss: 0.001082 | Perplexity: 1534.619836
2025-09-27 11:42:28,275 Stage: Train 0.5 | Epoch: 231 | Iter: 351400 | Total Loss: 0.003603 | Recon Loss: 0.003063 | Commit Loss: 0.001080 | Perplexity: 1535.169470
2025-09-27 11:43:14,642 Stage: Train 0.5 | Epoch: 231 | Iter: 351600 | Total Loss: 0.003598 | Recon Loss: 0.003055 | Commit Loss: 0.001086 | Perplexity: 1535.829303
2025-09-27 11:44:01,011 Stage: Train 0.5 | Epoch: 231 | Iter: 351800 | Total Loss: 0.003654 | Recon Loss: 0.003112 | Commit Loss: 0.001083 | Perplexity: 1537.993618
2025-09-27 11:44:47,115 Stage: Train 0.5 | Epoch: 231 | Iter: 352000 | Total Loss: 0.003674 | Recon Loss: 0.003136 | Commit Loss: 0.001076 | Perplexity: 1534.033223
2025-09-27 11:45:33,407 Stage: Train 0.5 | Epoch: 231 | Iter: 352200 | Total Loss: 0.003504 | Recon Loss: 0.002960 | Commit Loss: 0.001088 | Perplexity: 1540.160687
2025-09-27 11:46:19,777 Stage: Train 0.5 | Epoch: 231 | Iter: 352400 | Total Loss: 0.003614 | Recon Loss: 0.003073 | Commit Loss: 0.001082 | Perplexity: 1533.456260
Trainning Epoch:  70%|███████   | 232/330 [21:33:47<9:33:05, 350.87s/it]2025-09-27 11:47:06,330 Stage: Train 0.5 | Epoch: 232 | Iter: 352600 | Total Loss: 0.003643 | Recon Loss: 0.003103 | Commit Loss: 0.001080 | Perplexity: 1537.283825
2025-09-27 11:47:52,503 Stage: Train 0.5 | Epoch: 232 | Iter: 352800 | Total Loss: 0.003555 | Recon Loss: 0.003016 | Commit Loss: 0.001078 | Perplexity: 1533.811307
2025-09-27 11:48:38,650 Stage: Train 0.5 | Epoch: 232 | Iter: 353000 | Total Loss: 0.003654 | Recon Loss: 0.003115 | Commit Loss: 0.001078 | Perplexity: 1534.353840
2025-09-27 11:49:24,697 Stage: Train 0.5 | Epoch: 232 | Iter: 353200 | Total Loss: 0.003608 | Recon Loss: 0.003069 | Commit Loss: 0.001077 | Perplexity: 1532.163101
2025-09-27 11:50:10,862 Stage: Train 0.5 | Epoch: 232 | Iter: 353400 | Total Loss: 0.003615 | Recon Loss: 0.003075 | Commit Loss: 0.001081 | Perplexity: 1537.949457
2025-09-27 11:50:57,005 Stage: Train 0.5 | Epoch: 232 | Iter: 353600 | Total Loss: 0.003587 | Recon Loss: 0.003046 | Commit Loss: 0.001083 | Perplexity: 1537.706265
2025-09-27 11:51:43,043 Stage: Train 0.5 | Epoch: 232 | Iter: 353800 | Total Loss: 0.003579 | Recon Loss: 0.003034 | Commit Loss: 0.001089 | Perplexity: 1541.315252
Trainning Epoch:  71%|███████   | 233/330 [21:39:38<9:27:06, 350.79s/it]2025-09-27 11:52:29,338 Stage: Train 0.5 | Epoch: 233 | Iter: 354000 | Total Loss: 0.003688 | Recon Loss: 0.003146 | Commit Loss: 0.001083 | Perplexity: 1535.008309
2025-09-27 11:53:15,431 Stage: Train 0.5 | Epoch: 233 | Iter: 354200 | Total Loss: 0.003616 | Recon Loss: 0.003072 | Commit Loss: 0.001089 | Perplexity: 1543.104160
2025-09-27 11:54:01,596 Stage: Train 0.5 | Epoch: 233 | Iter: 354400 | Total Loss: 0.003550 | Recon Loss: 0.003014 | Commit Loss: 0.001073 | Perplexity: 1535.734565
2025-09-27 11:54:47,732 Stage: Train 0.5 | Epoch: 233 | Iter: 354600 | Total Loss: 0.003589 | Recon Loss: 0.003046 | Commit Loss: 0.001085 | Perplexity: 1537.709896
2025-09-27 11:55:33,635 Stage: Train 0.5 | Epoch: 233 | Iter: 354800 | Total Loss: 0.003608 | Recon Loss: 0.003067 | Commit Loss: 0.001080 | Perplexity: 1534.308825
2025-09-27 11:56:19,514 Stage: Train 0.5 | Epoch: 233 | Iter: 355000 | Total Loss: 0.003584 | Recon Loss: 0.003044 | Commit Loss: 0.001080 | Perplexity: 1537.670637
2025-09-27 11:57:05,627 Stage: Train 0.5 | Epoch: 233 | Iter: 355200 | Total Loss: 0.003595 | Recon Loss: 0.003052 | Commit Loss: 0.001086 | Perplexity: 1539.135906
2025-09-27 11:57:51,715 Stage: Train 0.5 | Epoch: 233 | Iter: 355400 | Total Loss: 0.003556 | Recon Loss: 0.003016 | Commit Loss: 0.001080 | Perplexity: 1534.927243
Trainning Epoch:  71%|███████   | 234/330 [21:45:28<9:20:58, 350.60s/it]2025-09-27 11:58:38,206 Stage: Train 0.5 | Epoch: 234 | Iter: 355600 | Total Loss: 0.003743 | Recon Loss: 0.003207 | Commit Loss: 0.001073 | Perplexity: 1533.895782
2025-09-27 11:59:24,305 Stage: Train 0.5 | Epoch: 234 | Iter: 355800 | Total Loss: 0.003595 | Recon Loss: 0.003057 | Commit Loss: 0.001075 | Perplexity: 1534.755930
2025-09-27 12:00:10,580 Stage: Train 0.5 | Epoch: 234 | Iter: 356000 | Total Loss: 0.003599 | Recon Loss: 0.003055 | Commit Loss: 0.001087 | Perplexity: 1536.664155
2025-09-27 12:00:56,784 Stage: Train 0.5 | Epoch: 234 | Iter: 356200 | Total Loss: 0.003598 | Recon Loss: 0.003057 | Commit Loss: 0.001082 | Perplexity: 1540.488398
2025-09-27 12:01:43,038 Stage: Train 0.5 | Epoch: 234 | Iter: 356400 | Total Loss: 0.003585 | Recon Loss: 0.003050 | Commit Loss: 0.001071 | Perplexity: 1534.543102
2025-09-27 12:02:29,047 Stage: Train 0.5 | Epoch: 234 | Iter: 356600 | Total Loss: 0.003551 | Recon Loss: 0.003009 | Commit Loss: 0.001082 | Perplexity: 1538.542834
2025-09-27 12:03:14,925 Stage: Train 0.5 | Epoch: 234 | Iter: 356800 | Total Loss: 0.003605 | Recon Loss: 0.003064 | Commit Loss: 0.001081 | Perplexity: 1539.099584
Trainning Epoch:  71%|███████   | 235/330 [21:51:19<9:15:04, 350.58s/it]2025-09-27 12:04:01,292 Stage: Train 0.5 | Epoch: 235 | Iter: 357000 | Total Loss: 0.003713 | Recon Loss: 0.003171 | Commit Loss: 0.001084 | Perplexity: 1535.737525
2025-09-27 12:04:47,509 Stage: Train 0.5 | Epoch: 235 | Iter: 357200 | Total Loss: 0.003569 | Recon Loss: 0.003033 | Commit Loss: 0.001071 | Perplexity: 1536.800316
2025-09-27 12:05:33,636 Stage: Train 0.5 | Epoch: 235 | Iter: 357400 | Total Loss: 0.003650 | Recon Loss: 0.003111 | Commit Loss: 0.001078 | Perplexity: 1538.640421
2025-09-27 12:06:19,911 Stage: Train 0.5 | Epoch: 235 | Iter: 357600 | Total Loss: 0.003578 | Recon Loss: 0.003039 | Commit Loss: 0.001078 | Perplexity: 1536.515579
2025-09-27 12:07:06,004 Stage: Train 0.5 | Epoch: 235 | Iter: 357800 | Total Loss: 0.003561 | Recon Loss: 0.003022 | Commit Loss: 0.001080 | Perplexity: 1535.193920
2025-09-27 12:07:51,948 Stage: Train 0.5 | Epoch: 235 | Iter: 358000 | Total Loss: 0.003631 | Recon Loss: 0.003092 | Commit Loss: 0.001078 | Perplexity: 1536.195242
2025-09-27 12:08:38,181 Stage: Train 0.5 | Epoch: 235 | Iter: 358200 | Total Loss: 0.003578 | Recon Loss: 0.003040 | Commit Loss: 0.001076 | Perplexity: 1536.241211
2025-09-27 12:09:24,304 Stage: Train 0.5 | Epoch: 235 | Iter: 358400 | Total Loss: 0.003583 | Recon Loss: 0.003044 | Commit Loss: 0.001078 | Perplexity: 1536.453374
Trainning Epoch:  72%|███████▏  | 236/330 [21:57:09<9:09:17, 350.62s/it]2025-09-27 12:10:10,357 Stage: Train 0.5 | Epoch: 236 | Iter: 358600 | Total Loss: 0.003548 | Recon Loss: 0.003009 | Commit Loss: 0.001078 | Perplexity: 1538.836937
2025-09-27 12:10:56,598 Stage: Train 0.5 | Epoch: 236 | Iter: 358800 | Total Loss: 0.003560 | Recon Loss: 0.003022 | Commit Loss: 0.001076 | Perplexity: 1534.850364
2025-09-27 12:11:42,792 Stage: Train 0.5 | Epoch: 236 | Iter: 359000 | Total Loss: 0.003568 | Recon Loss: 0.003028 | Commit Loss: 0.001080 | Perplexity: 1539.688417
2025-09-27 12:12:28,909 Stage: Train 0.5 | Epoch: 236 | Iter: 359200 | Total Loss: 0.003610 | Recon Loss: 0.003067 | Commit Loss: 0.001085 | Perplexity: 1538.451795
2025-09-27 12:13:15,092 Stage: Train 0.5 | Epoch: 236 | Iter: 359400 | Total Loss: 0.003664 | Recon Loss: 0.003121 | Commit Loss: 0.001086 | Perplexity: 1544.059523
2025-09-27 12:14:01,186 Stage: Train 0.5 | Epoch: 236 | Iter: 359600 | Total Loss: 0.003640 | Recon Loss: 0.003101 | Commit Loss: 0.001080 | Perplexity: 1538.563200
2025-09-27 12:14:47,312 Stage: Train 0.5 | Epoch: 236 | Iter: 359800 | Total Loss: 0.003589 | Recon Loss: 0.003051 | Commit Loss: 0.001075 | Perplexity: 1533.486957
2025-09-27 12:15:33,452 Stage: Train 0.5 | Epoch: 236 | Iter: 360000 | Total Loss: 0.003533 | Recon Loss: 0.002998 | Commit Loss: 0.001070 | Perplexity: 1535.665013
2025-09-27 12:15:33,453 Saving model at iteration 360000
2025-09-27 12:15:33,681 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000
2025-09-27 12:15:33,981 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/model.safetensors
2025-09-27 12:15:34,360 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/optimizer.bin
2025-09-27 12:15:34,360 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/scheduler.bin
2025-09-27 12:15:34,361 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/sampler.bin
2025-09-27 12:15:34,361 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/random_states_0.pkl
Trainning Epoch:  72%|███████▏  | 237/330 [22:03:01<9:03:49, 350.86s/it]2025-09-27 12:16:20,841 Stage: Train 0.5 | Epoch: 237 | Iter: 360200 | Total Loss: 0.003583 | Recon Loss: 0.003047 | Commit Loss: 0.001072 | Perplexity: 1539.320137
2025-09-27 12:17:07,054 Stage: Train 0.5 | Epoch: 237 | Iter: 360400 | Total Loss: 0.003606 | Recon Loss: 0.003067 | Commit Loss: 0.001079 | Perplexity: 1540.210173
2025-09-27 12:17:52,935 Stage: Train 0.5 | Epoch: 237 | Iter: 360600 | Total Loss: 0.003569 | Recon Loss: 0.003029 | Commit Loss: 0.001080 | Perplexity: 1540.965191
2025-09-27 12:18:39,108 Stage: Train 0.5 | Epoch: 237 | Iter: 360800 | Total Loss: 0.003600 | Recon Loss: 0.003062 | Commit Loss: 0.001076 | Perplexity: 1536.255698
2025-09-27 12:19:25,267 Stage: Train 0.5 | Epoch: 237 | Iter: 361000 | Total Loss: 0.003624 | Recon Loss: 0.003086 | Commit Loss: 0.001075 | Perplexity: 1538.767952
2025-09-27 12:20:11,583 Stage: Train 0.5 | Epoch: 237 | Iter: 361200 | Total Loss: 0.003613 | Recon Loss: 0.003074 | Commit Loss: 0.001077 | Perplexity: 1537.304064
2025-09-27 12:20:57,828 Stage: Train 0.5 | Epoch: 237 | Iter: 361400 | Total Loss: 0.003566 | Recon Loss: 0.003031 | Commit Loss: 0.001070 | Perplexity: 1534.792523
Trainning Epoch:  72%|███████▏  | 238/330 [22:08:52<8:58:04, 350.92s/it]2025-09-27 12:21:44,312 Stage: Train 0.5 | Epoch: 238 | Iter: 361600 | Total Loss: 0.003542 | Recon Loss: 0.003006 | Commit Loss: 0.001072 | Perplexity: 1538.863223
2025-09-27 12:22:30,389 Stage: Train 0.5 | Epoch: 238 | Iter: 361800 | Total Loss: 0.003565 | Recon Loss: 0.003029 | Commit Loss: 0.001072 | Perplexity: 1538.032758
2025-09-27 12:23:16,579 Stage: Train 0.5 | Epoch: 238 | Iter: 362000 | Total Loss: 0.003617 | Recon Loss: 0.003079 | Commit Loss: 0.001076 | Perplexity: 1535.036450
2025-09-27 12:24:02,596 Stage: Train 0.5 | Epoch: 238 | Iter: 362200 | Total Loss: 0.003584 | Recon Loss: 0.003045 | Commit Loss: 0.001077 | Perplexity: 1541.514547
2025-09-27 12:24:48,495 Stage: Train 0.5 | Epoch: 238 | Iter: 362400 | Total Loss: 0.003536 | Recon Loss: 0.003000 | Commit Loss: 0.001071 | Perplexity: 1536.546642
2025-09-27 12:25:34,528 Stage: Train 0.5 | Epoch: 238 | Iter: 362600 | Total Loss: 0.003608 | Recon Loss: 0.003070 | Commit Loss: 0.001075 | Perplexity: 1538.032560
2025-09-27 12:26:20,741 Stage: Train 0.5 | Epoch: 238 | Iter: 362800 | Total Loss: 0.003618 | Recon Loss: 0.003081 | Commit Loss: 0.001073 | Perplexity: 1538.001522
2025-09-27 12:27:06,931 Stage: Train 0.5 | Epoch: 238 | Iter: 363000 | Total Loss: 0.003618 | Recon Loss: 0.003079 | Commit Loss: 0.001077 | Perplexity: 1539.426635
Trainning Epoch:  72%|███████▏  | 239/330 [22:14:42<8:51:56, 350.73s/it]2025-09-27 12:27:53,273 Stage: Train 0.5 | Epoch: 239 | Iter: 363200 | Total Loss: 0.003578 | Recon Loss: 0.003044 | Commit Loss: 0.001069 | Perplexity: 1536.850638
2025-09-27 12:28:39,413 Stage: Train 0.5 | Epoch: 239 | Iter: 363400 | Total Loss: 0.003572 | Recon Loss: 0.003037 | Commit Loss: 0.001071 | Perplexity: 1539.067046
2025-09-27 12:29:25,590 Stage: Train 0.5 | Epoch: 239 | Iter: 363600 | Total Loss: 0.003597 | Recon Loss: 0.003061 | Commit Loss: 0.001072 | Perplexity: 1534.031136
2025-09-27 12:30:11,641 Stage: Train 0.5 | Epoch: 239 | Iter: 363800 | Total Loss: 0.003576 | Recon Loss: 0.003037 | Commit Loss: 0.001078 | Perplexity: 1543.896584
2025-09-27 12:30:57,724 Stage: Train 0.5 | Epoch: 239 | Iter: 364000 | Total Loss: 0.003517 | Recon Loss: 0.002979 | Commit Loss: 0.001076 | Perplexity: 1538.534017
2025-09-27 12:31:43,622 Stage: Train 0.5 | Epoch: 239 | Iter: 364200 | Total Loss: 0.003594 | Recon Loss: 0.003054 | Commit Loss: 0.001080 | Perplexity: 1539.164400
2025-09-27 12:32:29,639 Stage: Train 0.5 | Epoch: 239 | Iter: 364400 | Total Loss: 0.003597 | Recon Loss: 0.003060 | Commit Loss: 0.001075 | Perplexity: 1540.319735
Trainning Epoch:  73%|███████▎  | 240/330 [22:20:32<8:45:50, 350.56s/it]2025-09-27 12:33:16,121 Stage: Train 0.5 | Epoch: 240 | Iter: 364600 | Total Loss: 0.003581 | Recon Loss: 0.003040 | Commit Loss: 0.001082 | Perplexity: 1540.423777
2025-09-27 12:34:02,270 Stage: Train 0.5 | Epoch: 240 | Iter: 364800 | Total Loss: 0.003589 | Recon Loss: 0.003056 | Commit Loss: 0.001066 | Perplexity: 1535.348193
2025-09-27 12:34:48,432 Stage: Train 0.5 | Epoch: 240 | Iter: 365000 | Total Loss: 0.003591 | Recon Loss: 0.003055 | Commit Loss: 0.001073 | Perplexity: 1542.229653
2025-09-27 12:35:34,562 Stage: Train 0.5 | Epoch: 240 | Iter: 365200 | Total Loss: 0.003551 | Recon Loss: 0.003013 | Commit Loss: 0.001075 | Perplexity: 1540.302648
2025-09-27 12:36:20,710 Stage: Train 0.5 | Epoch: 240 | Iter: 365400 | Total Loss: 0.003605 | Recon Loss: 0.003068 | Commit Loss: 0.001073 | Perplexity: 1535.307988
2025-09-27 12:37:06,854 Stage: Train 0.5 | Epoch: 240 | Iter: 365600 | Total Loss: 0.003575 | Recon Loss: 0.003036 | Commit Loss: 0.001078 | Perplexity: 1542.441299
2025-09-27 12:37:53,049 Stage: Train 0.5 | Epoch: 240 | Iter: 365800 | Total Loss: 0.003621 | Recon Loss: 0.003083 | Commit Loss: 0.001076 | Perplexity: 1540.175314
2025-09-27 12:38:38,920 Stage: Train 0.5 | Epoch: 240 | Iter: 366000 | Total Loss: 0.003520 | Recon Loss: 0.002986 | Commit Loss: 0.001070 | Perplexity: 1539.889464
Trainning Epoch:  73%|███████▎  | 241/330 [22:26:23<8:39:59, 350.55s/it]2025-09-27 12:39:25,059 Stage: Train 0.5 | Epoch: 241 | Iter: 366200 | Total Loss: 0.003554 | Recon Loss: 0.003017 | Commit Loss: 0.001074 | Perplexity: 1539.265917
2025-09-27 12:40:11,354 Stage: Train 0.5 | Epoch: 241 | Iter: 366400 | Total Loss: 0.003539 | Recon Loss: 0.003002 | Commit Loss: 0.001072 | Perplexity: 1537.649995
2025-09-27 12:40:57,679 Stage: Train 0.5 | Epoch: 241 | Iter: 366600 | Total Loss: 0.003600 | Recon Loss: 0.003065 | Commit Loss: 0.001070 | Perplexity: 1541.138998
2025-09-27 12:41:44,041 Stage: Train 0.5 | Epoch: 241 | Iter: 366800 | Total Loss: 0.003572 | Recon Loss: 0.003038 | Commit Loss: 0.001069 | Perplexity: 1541.145907
2025-09-27 12:42:30,352 Stage: Train 0.5 | Epoch: 241 | Iter: 367000 | Total Loss: 0.003591 | Recon Loss: 0.003054 | Commit Loss: 0.001072 | Perplexity: 1542.783057
2025-09-27 12:43:16,504 Stage: Train 0.5 | Epoch: 241 | Iter: 367200 | Total Loss: 0.003535 | Recon Loss: 0.003000 | Commit Loss: 0.001068 | Perplexity: 1539.761685
2025-09-27 12:44:02,751 Stage: Train 0.5 | Epoch: 241 | Iter: 367400 | Total Loss: 0.003604 | Recon Loss: 0.003067 | Commit Loss: 0.001075 | Perplexity: 1539.545272
Trainning Epoch:  73%|███████▎  | 242/330 [22:32:14<8:34:32, 350.82s/it]2025-09-27 12:44:49,282 Stage: Train 0.5 | Epoch: 242 | Iter: 367600 | Total Loss: 0.003660 | Recon Loss: 0.003122 | Commit Loss: 0.001075 | Perplexity: 1538.758156
2025-09-27 12:45:35,527 Stage: Train 0.5 | Epoch: 242 | Iter: 367800 | Total Loss: 0.003554 | Recon Loss: 0.003019 | Commit Loss: 0.001069 | Perplexity: 1538.955356
2025-09-27 12:46:21,354 Stage: Train 0.5 | Epoch: 242 | Iter: 368000 | Total Loss: 0.003597 | Recon Loss: 0.003063 | Commit Loss: 0.001066 | Perplexity: 1539.528322
2025-09-27 12:47:07,589 Stage: Train 0.5 | Epoch: 242 | Iter: 368200 | Total Loss: 0.003589 | Recon Loss: 0.003052 | Commit Loss: 0.001074 | Perplexity: 1539.883770
2025-09-27 12:47:53,795 Stage: Train 0.5 | Epoch: 242 | Iter: 368400 | Total Loss: 0.003550 | Recon Loss: 0.003019 | Commit Loss: 0.001063 | Perplexity: 1541.624294
2025-09-27 12:48:40,013 Stage: Train 0.5 | Epoch: 242 | Iter: 368600 | Total Loss: 0.003606 | Recon Loss: 0.003072 | Commit Loss: 0.001069 | Perplexity: 1540.534719
2025-09-27 12:49:26,085 Stage: Train 0.5 | Epoch: 242 | Iter: 368800 | Total Loss: 0.003654 | Recon Loss: 0.003120 | Commit Loss: 0.001068 | Perplexity: 1537.543661
2025-09-27 12:50:12,269 Stage: Train 0.5 | Epoch: 242 | Iter: 369000 | Total Loss: 0.003654 | Recon Loss: 0.003116 | Commit Loss: 0.001075 | Perplexity: 1540.160317
Trainning Epoch:  74%|███████▎  | 243/330 [22:38:05<8:28:41, 350.83s/it]2025-09-27 12:50:58,778 Stage: Train 0.5 | Epoch: 243 | Iter: 369200 | Total Loss: 0.003499 | Recon Loss: 0.002968 | Commit Loss: 0.001063 | Perplexity: 1536.162981
2025-09-27 12:51:44,957 Stage: Train 0.5 | Epoch: 243 | Iter: 369400 | Total Loss: 0.003695 | Recon Loss: 0.003163 | Commit Loss: 0.001065 | Perplexity: 1540.479822
2025-09-27 12:52:31,036 Stage: Train 0.5 | Epoch: 243 | Iter: 369600 | Total Loss: 0.003637 | Recon Loss: 0.003104 | Commit Loss: 0.001065 | Perplexity: 1539.775292
2025-09-27 12:53:17,010 Stage: Train 0.5 | Epoch: 243 | Iter: 369800 | Total Loss: 0.003449 | Recon Loss: 0.002914 | Commit Loss: 0.001071 | Perplexity: 1542.872157
2025-09-27 12:54:03,290 Stage: Train 0.5 | Epoch: 243 | Iter: 370000 | Total Loss: 0.003577 | Recon Loss: 0.003041 | Commit Loss: 0.001073 | Perplexity: 1541.332153
2025-09-27 12:54:49,401 Stage: Train 0.5 | Epoch: 243 | Iter: 370200 | Total Loss: 0.003527 | Recon Loss: 0.002991 | Commit Loss: 0.001072 | Perplexity: 1541.887717
2025-09-27 12:55:35,859 Stage: Train 0.5 | Epoch: 243 | Iter: 370400 | Total Loss: 0.003565 | Recon Loss: 0.003031 | Commit Loss: 0.001068 | Perplexity: 1541.754474
2025-09-27 12:56:21,959 Stage: Train 0.5 | Epoch: 243 | Iter: 370600 | Total Loss: 0.003532 | Recon Loss: 0.002998 | Commit Loss: 0.001068 | Perplexity: 1540.695811
Trainning Epoch:  74%|███████▍  | 244/330 [22:43:56<8:22:52, 350.85s/it]2025-09-27 12:57:08,466 Stage: Train 0.5 | Epoch: 244 | Iter: 370800 | Total Loss: 0.003522 | Recon Loss: 0.002989 | Commit Loss: 0.001066 | Perplexity: 1537.986733
2025-09-27 12:57:54,657 Stage: Train 0.5 | Epoch: 244 | Iter: 371000 | Total Loss: 0.003623 | Recon Loss: 0.003089 | Commit Loss: 0.001069 | Perplexity: 1538.705204
2025-09-27 12:58:40,818 Stage: Train 0.5 | Epoch: 244 | Iter: 371200 | Total Loss: 0.003473 | Recon Loss: 0.002943 | Commit Loss: 0.001060 | Perplexity: 1539.135552
2025-09-27 12:59:27,067 Stage: Train 0.5 | Epoch: 244 | Iter: 371400 | Total Loss: 0.003595 | Recon Loss: 0.003060 | Commit Loss: 0.001070 | Perplexity: 1542.130381
2025-09-27 13:00:12,938 Stage: Train 0.5 | Epoch: 244 | Iter: 371600 | Total Loss: 0.003522 | Recon Loss: 0.002990 | Commit Loss: 0.001064 | Perplexity: 1542.305696
2025-09-27 13:00:59,177 Stage: Train 0.5 | Epoch: 244 | Iter: 371800 | Total Loss: 0.003549 | Recon Loss: 0.003012 | Commit Loss: 0.001074 | Perplexity: 1543.283375
2025-09-27 13:01:45,293 Stage: Train 0.5 | Epoch: 244 | Iter: 372000 | Total Loss: 0.003590 | Recon Loss: 0.003052 | Commit Loss: 0.001077 | Perplexity: 1542.598076
Trainning Epoch:  74%|███████▍  | 245/330 [22:49:47<8:17:03, 350.86s/it]2025-09-27 13:02:31,769 Stage: Train 0.5 | Epoch: 245 | Iter: 372200 | Total Loss: 0.003645 | Recon Loss: 0.003110 | Commit Loss: 0.001071 | Perplexity: 1540.250391
2025-09-27 13:03:17,930 Stage: Train 0.5 | Epoch: 245 | Iter: 372400 | Total Loss: 0.003503 | Recon Loss: 0.002969 | Commit Loss: 0.001069 | Perplexity: 1544.320271
2025-09-27 13:04:04,232 Stage: Train 0.5 | Epoch: 245 | Iter: 372600 | Total Loss: 0.003489 | Recon Loss: 0.002953 | Commit Loss: 0.001072 | Perplexity: 1543.554741
2025-09-27 13:04:48,556 Stage: Train 0.5 | Epoch: 245 | Iter: 372800 | Total Loss: 0.003552 | Recon Loss: 0.003019 | Commit Loss: 0.001066 | Perplexity: 1539.596942
2025-09-27 13:05:34,601 Stage: Train 0.5 | Epoch: 245 | Iter: 373000 | Total Loss: 0.003513 | Recon Loss: 0.002979 | Commit Loss: 0.001068 | Perplexity: 1541.615693
2025-09-27 13:06:20,805 Stage: Train 0.5 | Epoch: 245 | Iter: 373200 | Total Loss: 0.003590 | Recon Loss: 0.003057 | Commit Loss: 0.001066 | Perplexity: 1541.043880
2025-09-27 13:07:06,924 Stage: Train 0.5 | Epoch: 245 | Iter: 373400 | Total Loss: 0.003542 | Recon Loss: 0.003006 | Commit Loss: 0.001071 | Perplexity: 1541.176144
2025-09-27 13:07:52,656 Stage: Train 0.5 | Epoch: 245 | Iter: 373600 | Total Loss: 0.003540 | Recon Loss: 0.003007 | Commit Loss: 0.001066 | Perplexity: 1541.944656
Trainning Epoch:  75%|███████▍  | 246/330 [22:55:35<8:10:13, 350.16s/it]2025-09-27 13:08:39,060 Stage: Train 0.5 | Epoch: 246 | Iter: 373800 | Total Loss: 0.003460 | Recon Loss: 0.002928 | Commit Loss: 0.001065 | Perplexity: 1541.150381
2025-09-27 13:09:25,195 Stage: Train 0.5 | Epoch: 246 | Iter: 374000 | Total Loss: 0.003641 | Recon Loss: 0.003107 | Commit Loss: 0.001067 | Perplexity: 1541.981788
2025-09-27 13:10:11,339 Stage: Train 0.5 | Epoch: 246 | Iter: 374200 | Total Loss: 0.003591 | Recon Loss: 0.003059 | Commit Loss: 0.001064 | Perplexity: 1547.695529
2025-09-27 13:10:57,641 Stage: Train 0.5 | Epoch: 246 | Iter: 374400 | Total Loss: 0.003543 | Recon Loss: 0.003009 | Commit Loss: 0.001067 | Perplexity: 1542.038489
2025-09-27 13:11:43,849 Stage: Train 0.5 | Epoch: 246 | Iter: 374600 | Total Loss: 0.003486 | Recon Loss: 0.002953 | Commit Loss: 0.001065 | Perplexity: 1539.367116
2025-09-27 13:12:29,911 Stage: Train 0.5 | Epoch: 246 | Iter: 374800 | Total Loss: 0.003626 | Recon Loss: 0.003090 | Commit Loss: 0.001070 | Perplexity: 1543.015768
2025-09-27 13:13:16,128 Stage: Train 0.5 | Epoch: 246 | Iter: 375000 | Total Loss: 0.003613 | Recon Loss: 0.003076 | Commit Loss: 0.001074 | Perplexity: 1544.180685
Trainning Epoch:  75%|███████▍  | 247/330 [23:01:26<8:04:46, 350.44s/it]2025-09-27 13:14:02,688 Stage: Train 0.5 | Epoch: 247 | Iter: 375200 | Total Loss: 0.003553 | Recon Loss: 0.003022 | Commit Loss: 0.001061 | Perplexity: 1536.447612
2025-09-27 13:14:48,673 Stage: Train 0.5 | Epoch: 247 | Iter: 375400 | Total Loss: 0.003559 | Recon Loss: 0.003026 | Commit Loss: 0.001065 | Perplexity: 1543.293127
2025-09-27 13:15:34,684 Stage: Train 0.5 | Epoch: 247 | Iter: 375600 | Total Loss: 0.003608 | Recon Loss: 0.003076 | Commit Loss: 0.001064 | Perplexity: 1542.914329
2025-09-27 13:16:20,894 Stage: Train 0.5 | Epoch: 247 | Iter: 375800 | Total Loss: 0.003500 | Recon Loss: 0.002968 | Commit Loss: 0.001064 | Perplexity: 1541.643188
2025-09-27 13:17:07,050 Stage: Train 0.5 | Epoch: 247 | Iter: 376000 | Total Loss: 0.003504 | Recon Loss: 0.002975 | Commit Loss: 0.001059 | Perplexity: 1537.284684
2025-09-27 13:17:53,241 Stage: Train 0.5 | Epoch: 247 | Iter: 376200 | Total Loss: 0.003571 | Recon Loss: 0.003035 | Commit Loss: 0.001072 | Perplexity: 1546.324971
2025-09-27 13:18:39,438 Stage: Train 0.5 | Epoch: 247 | Iter: 376400 | Total Loss: 0.003548 | Recon Loss: 0.003015 | Commit Loss: 0.001065 | Perplexity: 1537.114119
2025-09-27 13:19:25,694 Stage: Train 0.5 | Epoch: 247 | Iter: 376600 | Total Loss: 0.003596 | Recon Loss: 0.003061 | Commit Loss: 0.001069 | Perplexity: 1543.099745
Trainning Epoch:  75%|███████▌  | 248/330 [23:07:17<7:59:04, 350.54s/it]2025-09-27 13:20:12,266 Stage: Train 0.5 | Epoch: 248 | Iter: 376800 | Total Loss: 0.003572 | Recon Loss: 0.003038 | Commit Loss: 0.001069 | Perplexity: 1545.018685
2025-09-27 13:20:58,348 Stage: Train 0.5 | Epoch: 248 | Iter: 377000 | Total Loss: 0.003483 | Recon Loss: 0.002949 | Commit Loss: 0.001067 | Perplexity: 1541.373818
2025-09-27 13:21:44,257 Stage: Train 0.5 | Epoch: 248 | Iter: 377200 | Total Loss: 0.003478 | Recon Loss: 0.002945 | Commit Loss: 0.001067 | Perplexity: 1544.982961
2025-09-27 13:22:30,355 Stage: Train 0.5 | Epoch: 248 | Iter: 377400 | Total Loss: 0.003539 | Recon Loss: 0.003011 | Commit Loss: 0.001056 | Perplexity: 1533.185227
2025-09-27 13:23:16,493 Stage: Train 0.5 | Epoch: 248 | Iter: 377600 | Total Loss: 0.003624 | Recon Loss: 0.003090 | Commit Loss: 0.001068 | Perplexity: 1540.800879
2025-09-27 13:24:02,572 Stage: Train 0.5 | Epoch: 248 | Iter: 377800 | Total Loss: 0.003473 | Recon Loss: 0.002944 | Commit Loss: 0.001059 | Perplexity: 1539.363051
2025-09-27 13:24:48,802 Stage: Train 0.5 | Epoch: 248 | Iter: 378000 | Total Loss: 0.003589 | Recon Loss: 0.003052 | Commit Loss: 0.001073 | Perplexity: 1545.499164
2025-09-27 13:25:35,016 Stage: Train 0.5 | Epoch: 248 | Iter: 378200 | Total Loss: 0.003504 | Recon Loss: 0.002970 | Commit Loss: 0.001069 | Perplexity: 1544.892100
Trainning Epoch:  75%|███████▌  | 249/330 [23:13:08<7:53:13, 350.54s/it]2025-09-27 13:26:21,352 Stage: Train 0.5 | Epoch: 249 | Iter: 378400 | Total Loss: 0.003496 | Recon Loss: 0.002966 | Commit Loss: 0.001059 | Perplexity: 1539.503893
2025-09-27 13:27:07,629 Stage: Train 0.5 | Epoch: 249 | Iter: 378600 | Total Loss: 0.003521 | Recon Loss: 0.002992 | Commit Loss: 0.001059 | Perplexity: 1540.667864
2025-09-27 13:27:53,773 Stage: Train 0.5 | Epoch: 249 | Iter: 378800 | Total Loss: 0.003611 | Recon Loss: 0.003081 | Commit Loss: 0.001061 | Perplexity: 1540.285852
2025-09-27 13:28:39,721 Stage: Train 0.5 | Epoch: 249 | Iter: 379000 | Total Loss: 0.003546 | Recon Loss: 0.003011 | Commit Loss: 0.001070 | Perplexity: 1544.906785
2025-09-27 13:29:25,926 Stage: Train 0.5 | Epoch: 249 | Iter: 379200 | Total Loss: 0.003539 | Recon Loss: 0.003003 | Commit Loss: 0.001071 | Perplexity: 1546.457984
2025-09-27 13:30:12,251 Stage: Train 0.5 | Epoch: 249 | Iter: 379400 | Total Loss: 0.003534 | Recon Loss: 0.002999 | Commit Loss: 0.001070 | Perplexity: 1544.636604
2025-09-27 13:30:58,298 Stage: Train 0.5 | Epoch: 249 | Iter: 379600 | Total Loss: 0.003473 | Recon Loss: 0.002941 | Commit Loss: 0.001063 | Perplexity: 1540.356517
Trainning Epoch:  76%|███████▌  | 250/330 [23:18:59<7:47:29, 350.61s/it]2025-09-27 13:31:44,729 Stage: Train 0.5 | Epoch: 250 | Iter: 379800 | Total Loss: 0.003542 | Recon Loss: 0.003011 | Commit Loss: 0.001062 | Perplexity: 1541.082590
2025-09-27 13:32:30,898 Stage: Train 0.5 | Epoch: 250 | Iter: 380000 | Total Loss: 0.003559 | Recon Loss: 0.003029 | Commit Loss: 0.001060 | Perplexity: 1543.585616
2025-09-27 13:32:30,899 Saving model at iteration 380000
2025-09-27 13:32:31,129 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000
2025-09-27 13:32:31,444 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/model.safetensors
2025-09-27 13:32:31,917 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/optimizer.bin
2025-09-27 13:32:31,918 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/scheduler.bin
2025-09-27 13:32:31,918 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/sampler.bin
2025-09-27 13:32:31,920 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/random_states_0.pkl
2025-09-27 13:33:18,209 Stage: Train 0.5 | Epoch: 250 | Iter: 380200 | Total Loss: 0.003462 | Recon Loss: 0.002930 | Commit Loss: 0.001064 | Perplexity: 1542.052216
2025-09-27 13:34:04,494 Stage: Train 0.5 | Epoch: 250 | Iter: 380400 | Total Loss: 0.003541 | Recon Loss: 0.003004 | Commit Loss: 0.001073 | Perplexity: 1544.352963
2025-09-27 13:34:50,634 Stage: Train 0.5 | Epoch: 250 | Iter: 380600 | Total Loss: 0.003600 | Recon Loss: 0.003066 | Commit Loss: 0.001068 | Perplexity: 1544.238019
2025-09-27 13:35:36,840 Stage: Train 0.5 | Epoch: 250 | Iter: 380800 | Total Loss: 0.003562 | Recon Loss: 0.003028 | Commit Loss: 0.001068 | Perplexity: 1545.026320
2025-09-27 13:36:22,840 Stage: Train 0.5 | Epoch: 250 | Iter: 381000 | Total Loss: 0.003544 | Recon Loss: 0.003014 | Commit Loss: 0.001060 | Perplexity: 1546.152382
2025-09-27 13:37:09,068 Stage: Train 0.5 | Epoch: 250 | Iter: 381200 | Total Loss: 0.003553 | Recon Loss: 0.003024 | Commit Loss: 0.001058 | Perplexity: 1538.972809
Trainning Epoch:  76%|███████▌  | 251/330 [23:24:51<7:42:12, 351.04s/it]2025-09-27 13:37:55,399 Stage: Train 0.5 | Epoch: 251 | Iter: 381400 | Total Loss: 0.003449 | Recon Loss: 0.002916 | Commit Loss: 0.001065 | Perplexity: 1542.951821
2025-09-27 13:38:41,655 Stage: Train 0.5 | Epoch: 251 | Iter: 381600 | Total Loss: 0.003556 | Recon Loss: 0.003030 | Commit Loss: 0.001051 | Perplexity: 1540.465559
2025-09-27 13:39:27,827 Stage: Train 0.5 | Epoch: 251 | Iter: 381800 | Total Loss: 0.003531 | Recon Loss: 0.003002 | Commit Loss: 0.001057 | Perplexity: 1540.009578
2025-09-27 13:40:14,139 Stage: Train 0.5 | Epoch: 251 | Iter: 382000 | Total Loss: 0.003511 | Recon Loss: 0.002977 | Commit Loss: 0.001068 | Perplexity: 1543.452970
2025-09-27 13:41:00,263 Stage: Train 0.5 | Epoch: 251 | Iter: 382200 | Total Loss: 0.003469 | Recon Loss: 0.002940 | Commit Loss: 0.001059 | Perplexity: 1542.462855
2025-09-27 13:41:46,492 Stage: Train 0.5 | Epoch: 251 | Iter: 382400 | Total Loss: 0.003562 | Recon Loss: 0.003029 | Commit Loss: 0.001066 | Perplexity: 1546.873942
2025-09-27 13:42:32,726 Stage: Train 0.5 | Epoch: 251 | Iter: 382600 | Total Loss: 0.003581 | Recon Loss: 0.003052 | Commit Loss: 0.001059 | Perplexity: 1537.375164
Trainning Epoch:  76%|███████▋  | 252/330 [23:30:42<7:36:17, 351.00s/it]2025-09-27 13:43:18,880 Stage: Train 0.5 | Epoch: 252 | Iter: 382800 | Total Loss: 0.003502 | Recon Loss: 0.002966 | Commit Loss: 0.001072 | Perplexity: 1545.330407
2025-09-27 13:44:04,949 Stage: Train 0.5 | Epoch: 252 | Iter: 383000 | Total Loss: 0.003555 | Recon Loss: 0.003022 | Commit Loss: 0.001067 | Perplexity: 1543.214899
2025-09-27 13:44:51,098 Stage: Train 0.5 | Epoch: 252 | Iter: 383200 | Total Loss: 0.003475 | Recon Loss: 0.002946 | Commit Loss: 0.001059 | Perplexity: 1541.317756
2025-09-27 13:45:37,254 Stage: Train 0.5 | Epoch: 252 | Iter: 383400 | Total Loss: 0.003539 | Recon Loss: 0.003009 | Commit Loss: 0.001061 | Perplexity: 1542.940310
2025-09-27 13:46:23,511 Stage: Train 0.5 | Epoch: 252 | Iter: 383600 | Total Loss: 0.003606 | Recon Loss: 0.003073 | Commit Loss: 0.001067 | Perplexity: 1541.434915
2025-09-27 13:47:09,735 Stage: Train 0.5 | Epoch: 252 | Iter: 383800 | Total Loss: 0.003555 | Recon Loss: 0.003027 | Commit Loss: 0.001056 | Perplexity: 1540.317356
2025-09-27 13:47:55,887 Stage: Train 0.5 | Epoch: 252 | Iter: 384000 | Total Loss: 0.003651 | Recon Loss: 0.003122 | Commit Loss: 0.001058 | Perplexity: 1540.706700
2025-09-27 13:48:42,079 Stage: Train 0.5 | Epoch: 252 | Iter: 384200 | Total Loss: 0.003446 | Recon Loss: 0.002913 | Commit Loss: 0.001066 | Perplexity: 1547.655126
Trainning Epoch:  77%|███████▋  | 253/330 [23:36:32<7:30:26, 350.99s/it]2025-09-27 13:49:28,451 Stage: Train 0.5 | Epoch: 253 | Iter: 384400 | Total Loss: 0.003480 | Recon Loss: 0.002951 | Commit Loss: 0.001059 | Perplexity: 1544.193248
2025-09-27 13:50:14,411 Stage: Train 0.5 | Epoch: 253 | Iter: 384600 | Total Loss: 0.003563 | Recon Loss: 0.003031 | Commit Loss: 0.001063 | Perplexity: 1546.485631
2025-09-27 13:51:00,652 Stage: Train 0.5 | Epoch: 253 | Iter: 384800 | Total Loss: 0.003507 | Recon Loss: 0.002981 | Commit Loss: 0.001053 | Perplexity: 1539.076494
2025-09-27 13:51:46,755 Stage: Train 0.5 | Epoch: 253 | Iter: 385000 | Total Loss: 0.003553 | Recon Loss: 0.003022 | Commit Loss: 0.001063 | Perplexity: 1543.090573
2025-09-27 13:52:32,965 Stage: Train 0.5 | Epoch: 253 | Iter: 385200 | Total Loss: 0.003483 | Recon Loss: 0.002954 | Commit Loss: 0.001058 | Perplexity: 1544.051744
2025-09-27 13:53:19,173 Stage: Train 0.5 | Epoch: 253 | Iter: 385400 | Total Loss: 0.003536 | Recon Loss: 0.003001 | Commit Loss: 0.001071 | Perplexity: 1547.416744
2025-09-27 13:54:05,332 Stage: Train 0.5 | Epoch: 253 | Iter: 385600 | Total Loss: 0.003517 | Recon Loss: 0.002991 | Commit Loss: 0.001053 | Perplexity: 1540.434272
2025-09-27 13:54:51,605 Stage: Train 0.5 | Epoch: 253 | Iter: 385800 | Total Loss: 0.003589 | Recon Loss: 0.003056 | Commit Loss: 0.001067 | Perplexity: 1543.742900
Trainning Epoch:  77%|███████▋  | 254/330 [23:42:23<7:24:31, 350.94s/it]2025-09-27 13:55:38,115 Stage: Train 0.5 | Epoch: 254 | Iter: 386000 | Total Loss: 0.003519 | Recon Loss: 0.002990 | Commit Loss: 0.001058 | Perplexity: 1541.495214
2025-09-27 13:56:24,263 Stage: Train 0.5 | Epoch: 254 | Iter: 386200 | Total Loss: 0.003524 | Recon Loss: 0.002993 | Commit Loss: 0.001062 | Perplexity: 1544.042782
2025-09-27 13:57:09,973 Stage: Train 0.5 | Epoch: 254 | Iter: 386400 | Total Loss: 0.003534 | Recon Loss: 0.003006 | Commit Loss: 0.001056 | Perplexity: 1540.254757
2025-09-27 13:57:56,261 Stage: Train 0.5 | Epoch: 254 | Iter: 386600 | Total Loss: 0.003453 | Recon Loss: 0.002926 | Commit Loss: 0.001054 | Perplexity: 1542.710279
2025-09-27 13:58:42,403 Stage: Train 0.5 | Epoch: 254 | Iter: 386800 | Total Loss: 0.003477 | Recon Loss: 0.002947 | Commit Loss: 0.001060 | Perplexity: 1544.193276
2025-09-27 13:59:28,565 Stage: Train 0.5 | Epoch: 254 | Iter: 387000 | Total Loss: 0.003564 | Recon Loss: 0.003033 | Commit Loss: 0.001062 | Perplexity: 1541.906871
2025-09-27 14:00:14,819 Stage: Train 0.5 | Epoch: 254 | Iter: 387200 | Total Loss: 0.003466 | Recon Loss: 0.002933 | Commit Loss: 0.001066 | Perplexity: 1548.983281
Trainning Epoch:  77%|███████▋  | 255/330 [23:48:14<7:18:34, 350.86s/it]2025-09-27 14:01:01,276 Stage: Train 0.5 | Epoch: 255 | Iter: 387400 | Total Loss: 0.003450 | Recon Loss: 0.002919 | Commit Loss: 0.001062 | Perplexity: 1542.951461
2025-09-27 14:01:47,395 Stage: Train 0.5 | Epoch: 255 | Iter: 387600 | Total Loss: 0.003554 | Recon Loss: 0.003022 | Commit Loss: 0.001063 | Perplexity: 1545.840552
2025-09-27 14:02:33,683 Stage: Train 0.5 | Epoch: 255 | Iter: 387800 | Total Loss: 0.003472 | Recon Loss: 0.002943 | Commit Loss: 0.001058 | Perplexity: 1545.789295
2025-09-27 14:03:19,866 Stage: Train 0.5 | Epoch: 255 | Iter: 388000 | Total Loss: 0.003446 | Recon Loss: 0.002916 | Commit Loss: 0.001061 | Perplexity: 1545.899380
2025-09-27 14:04:05,926 Stage: Train 0.5 | Epoch: 255 | Iter: 388200 | Total Loss: 0.003545 | Recon Loss: 0.003013 | Commit Loss: 0.001065 | Perplexity: 1544.626331
2025-09-27 14:04:51,857 Stage: Train 0.5 | Epoch: 255 | Iter: 388400 | Total Loss: 0.003523 | Recon Loss: 0.002994 | Commit Loss: 0.001058 | Perplexity: 1541.520563
2025-09-27 14:05:38,107 Stage: Train 0.5 | Epoch: 255 | Iter: 388600 | Total Loss: 0.003518 | Recon Loss: 0.002985 | Commit Loss: 0.001065 | Perplexity: 1546.760515
2025-09-27 14:06:24,301 Stage: Train 0.5 | Epoch: 255 | Iter: 388800 | Total Loss: 0.003572 | Recon Loss: 0.003047 | Commit Loss: 0.001051 | Perplexity: 1543.739822
Trainning Epoch:  78%|███████▊  | 256/330 [23:54:05<7:12:42, 350.85s/it]2025-09-27 14:07:10,783 Stage: Train 0.5 | Epoch: 256 | Iter: 389000 | Total Loss: 0.003524 | Recon Loss: 0.002996 | Commit Loss: 0.001056 | Perplexity: 1547.885659
2025-09-27 14:07:56,951 Stage: Train 0.5 | Epoch: 256 | Iter: 389200 | Total Loss: 0.003541 | Recon Loss: 0.003015 | Commit Loss: 0.001053 | Perplexity: 1538.031038
2025-09-27 14:08:43,102 Stage: Train 0.5 | Epoch: 256 | Iter: 389400 | Total Loss: 0.003448 | Recon Loss: 0.002920 | Commit Loss: 0.001056 | Perplexity: 1544.500393
2025-09-27 14:09:29,337 Stage: Train 0.5 | Epoch: 256 | Iter: 389600 | Total Loss: 0.003510 | Recon Loss: 0.002978 | Commit Loss: 0.001064 | Perplexity: 1545.558695
2025-09-27 14:10:15,519 Stage: Train 0.5 | Epoch: 256 | Iter: 389800 | Total Loss: 0.003514 | Recon Loss: 0.002985 | Commit Loss: 0.001059 | Perplexity: 1541.487578
2025-09-27 14:11:01,754 Stage: Train 0.5 | Epoch: 256 | Iter: 390000 | Total Loss: 0.003533 | Recon Loss: 0.003003 | Commit Loss: 0.001059 | Perplexity: 1541.814564
2025-09-27 14:11:47,694 Stage: Train 0.5 | Epoch: 256 | Iter: 390200 | Total Loss: 0.003507 | Recon Loss: 0.002978 | Commit Loss: 0.001059 | Perplexity: 1546.372147
Trainning Epoch:  78%|███████▊  | 257/330 [23:59:56<7:06:51, 350.85s/it]2025-09-27 14:12:34,166 Stage: Train 0.5 | Epoch: 257 | Iter: 390400 | Total Loss: 0.003521 | Recon Loss: 0.002991 | Commit Loss: 0.001061 | Perplexity: 1548.203759
2025-09-27 14:13:20,411 Stage: Train 0.5 | Epoch: 257 | Iter: 390600 | Total Loss: 0.003473 | Recon Loss: 0.002945 | Commit Loss: 0.001056 | Perplexity: 1545.615677
2025-09-27 14:14:06,708 Stage: Train 0.5 | Epoch: 257 | Iter: 390800 | Total Loss: 0.003482 | Recon Loss: 0.002958 | Commit Loss: 0.001048 | Perplexity: 1540.444834
2025-09-27 14:14:52,772 Stage: Train 0.5 | Epoch: 257 | Iter: 391000 | Total Loss: 0.003643 | Recon Loss: 0.003114 | Commit Loss: 0.001057 | Perplexity: 1543.915296
2025-09-27 14:15:38,973 Stage: Train 0.5 | Epoch: 257 | Iter: 391200 | Total Loss: 0.003470 | Recon Loss: 0.002941 | Commit Loss: 0.001060 | Perplexity: 1546.922626
2025-09-27 14:16:25,263 Stage: Train 0.5 | Epoch: 257 | Iter: 391400 | Total Loss: 0.003482 | Recon Loss: 0.002953 | Commit Loss: 0.001058 | Perplexity: 1541.449789
2025-09-27 14:17:11,405 Stage: Train 0.5 | Epoch: 257 | Iter: 391600 | Total Loss: 0.003590 | Recon Loss: 0.003062 | Commit Loss: 0.001056 | Perplexity: 1542.973453
2025-09-27 14:17:57,501 Stage: Train 0.5 | Epoch: 257 | Iter: 391800 | Total Loss: 0.003453 | Recon Loss: 0.002923 | Commit Loss: 0.001060 | Perplexity: 1547.845612
Trainning Epoch:  78%|███████▊  | 258/330 [24:05:47<7:01:07, 350.93s/it]2025-09-27 14:18:43,591 Stage: Train 0.5 | Epoch: 258 | Iter: 392000 | Total Loss: 0.003503 | Recon Loss: 0.002974 | Commit Loss: 0.001058 | Perplexity: 1538.233937
2025-09-27 14:19:29,845 Stage: Train 0.5 | Epoch: 258 | Iter: 392200 | Total Loss: 0.003505 | Recon Loss: 0.002976 | Commit Loss: 0.001059 | Perplexity: 1546.493892
2025-09-27 14:20:16,104 Stage: Train 0.5 | Epoch: 258 | Iter: 392400 | Total Loss: 0.003510 | Recon Loss: 0.002980 | Commit Loss: 0.001060 | Perplexity: 1546.713934
2025-09-27 14:21:02,273 Stage: Train 0.5 | Epoch: 258 | Iter: 392600 | Total Loss: 0.003482 | Recon Loss: 0.002955 | Commit Loss: 0.001055 | Perplexity: 1540.882463
2025-09-27 14:21:48,466 Stage: Train 0.5 | Epoch: 258 | Iter: 392800 | Total Loss: 0.003491 | Recon Loss: 0.002963 | Commit Loss: 0.001055 | Perplexity: 1543.053165
2025-09-27 14:22:34,642 Stage: Train 0.5 | Epoch: 258 | Iter: 393000 | Total Loss: 0.003431 | Recon Loss: 0.002904 | Commit Loss: 0.001054 | Perplexity: 1543.497458
2025-09-27 14:23:20,882 Stage: Train 0.5 | Epoch: 258 | Iter: 393200 | Total Loss: 0.003561 | Recon Loss: 0.003032 | Commit Loss: 0.001058 | Perplexity: 1544.474645
2025-09-27 14:24:07,159 Stage: Train 0.5 | Epoch: 258 | Iter: 393400 | Total Loss: 0.003473 | Recon Loss: 0.002942 | Commit Loss: 0.001063 | Perplexity: 1546.223694
Trainning Epoch:  78%|███████▊  | 259/330 [24:11:38<6:55:16, 350.94s/it]2025-09-27 14:24:53,614 Stage: Train 0.5 | Epoch: 259 | Iter: 393600 | Total Loss: 0.003473 | Recon Loss: 0.002949 | Commit Loss: 0.001049 | Perplexity: 1542.858135
2025-09-27 14:25:39,493 Stage: Train 0.5 | Epoch: 259 | Iter: 393800 | Total Loss: 0.003506 | Recon Loss: 0.002975 | Commit Loss: 0.001063 | Perplexity: 1549.469692
2025-09-27 14:26:25,706 Stage: Train 0.5 | Epoch: 259 | Iter: 394000 | Total Loss: 0.003511 | Recon Loss: 0.002982 | Commit Loss: 0.001058 | Perplexity: 1544.297463
2025-09-27 14:27:11,885 Stage: Train 0.5 | Epoch: 259 | Iter: 394200 | Total Loss: 0.003485 | Recon Loss: 0.002959 | Commit Loss: 0.001051 | Perplexity: 1539.284622
2025-09-27 14:27:57,982 Stage: Train 0.5 | Epoch: 259 | Iter: 394400 | Total Loss: 0.003564 | Recon Loss: 0.003040 | Commit Loss: 0.001049 | Perplexity: 1543.812123
2025-09-27 14:28:44,161 Stage: Train 0.5 | Epoch: 259 | Iter: 394600 | Total Loss: 0.003476 | Recon Loss: 0.002946 | Commit Loss: 0.001059 | Perplexity: 1546.312678
2025-09-27 14:29:30,402 Stage: Train 0.5 | Epoch: 259 | Iter: 394800 | Total Loss: 0.003520 | Recon Loss: 0.002990 | Commit Loss: 0.001060 | Perplexity: 1542.869786
Trainning Epoch:  79%|███████▉  | 260/330 [24:17:28<6:49:20, 350.86s/it]2025-09-27 14:30:16,865 Stage: Train 0.5 | Epoch: 260 | Iter: 395000 | Total Loss: 0.003610 | Recon Loss: 0.003081 | Commit Loss: 0.001058 | Perplexity: 1543.020890
2025-09-27 14:31:03,082 Stage: Train 0.5 | Epoch: 260 | Iter: 395200 | Total Loss: 0.003475 | Recon Loss: 0.002946 | Commit Loss: 0.001058 | Perplexity: 1541.043389
2025-09-27 14:31:49,150 Stage: Train 0.5 | Epoch: 260 | Iter: 395400 | Total Loss: 0.003487 | Recon Loss: 0.002961 | Commit Loss: 0.001052 | Perplexity: 1543.367335
2025-09-27 14:32:35,221 Stage: Train 0.5 | Epoch: 260 | Iter: 395600 | Total Loss: 0.003541 | Recon Loss: 0.003011 | Commit Loss: 0.001060 | Perplexity: 1542.630339
2025-09-27 14:33:20,924 Stage: Train 0.5 | Epoch: 260 | Iter: 395800 | Total Loss: 0.003444 | Recon Loss: 0.002914 | Commit Loss: 0.001060 | Perplexity: 1544.393739
2025-09-27 14:34:06,984 Stage: Train 0.5 | Epoch: 260 | Iter: 396000 | Total Loss: 0.003553 | Recon Loss: 0.003025 | Commit Loss: 0.001056 | Perplexity: 1543.731931
2025-09-27 14:34:53,215 Stage: Train 0.5 | Epoch: 260 | Iter: 396200 | Total Loss: 0.003453 | Recon Loss: 0.002926 | Commit Loss: 0.001055 | Perplexity: 1544.764166
2025-09-27 14:35:39,339 Stage: Train 0.5 | Epoch: 260 | Iter: 396400 | Total Loss: 0.003507 | Recon Loss: 0.002977 | Commit Loss: 0.001060 | Perplexity: 1550.658600
Trainning Epoch:  79%|███████▉  | 261/330 [24:23:19<6:43:16, 350.67s/it]2025-09-27 14:36:25,823 Stage: Train 0.5 | Epoch: 261 | Iter: 396600 | Total Loss: 0.003500 | Recon Loss: 0.002975 | Commit Loss: 0.001052 | Perplexity: 1542.134923
2025-09-27 14:37:12,162 Stage: Train 0.5 | Epoch: 261 | Iter: 396800 | Total Loss: 0.003558 | Recon Loss: 0.003031 | Commit Loss: 0.001053 | Perplexity: 1546.204815
2025-09-27 14:37:58,371 Stage: Train 0.5 | Epoch: 261 | Iter: 397000 | Total Loss: 0.003402 | Recon Loss: 0.002872 | Commit Loss: 0.001060 | Perplexity: 1545.472424
2025-09-27 14:38:43,082 Stage: Train 0.5 | Epoch: 261 | Iter: 397200 | Total Loss: 0.003501 | Recon Loss: 0.002978 | Commit Loss: 0.001047 | Perplexity: 1541.043675
2025-09-27 14:39:29,105 Stage: Train 0.5 | Epoch: 261 | Iter: 397400 | Total Loss: 0.003483 | Recon Loss: 0.002955 | Commit Loss: 0.001056 | Perplexity: 1546.063376
2025-09-27 14:40:15,009 Stage: Train 0.5 | Epoch: 261 | Iter: 397600 | Total Loss: 0.003482 | Recon Loss: 0.002953 | Commit Loss: 0.001058 | Perplexity: 1544.053767
2025-09-27 14:41:01,261 Stage: Train 0.5 | Epoch: 261 | Iter: 397800 | Total Loss: 0.003521 | Recon Loss: 0.002991 | Commit Loss: 0.001059 | Perplexity: 1544.946769
Trainning Epoch:  79%|███████▉  | 262/330 [24:29:08<6:36:58, 350.27s/it]2025-09-27 14:41:47,572 Stage: Train 0.5 | Epoch: 262 | Iter: 398000 | Total Loss: 0.003459 | Recon Loss: 0.002933 | Commit Loss: 0.001052 | Perplexity: 1543.645104
2025-09-27 14:42:33,745 Stage: Train 0.5 | Epoch: 262 | Iter: 398200 | Total Loss: 0.003519 | Recon Loss: 0.002996 | Commit Loss: 0.001047 | Perplexity: 1542.267540
2025-09-27 14:43:19,995 Stage: Train 0.5 | Epoch: 262 | Iter: 398400 | Total Loss: 0.003471 | Recon Loss: 0.002947 | Commit Loss: 0.001048 | Perplexity: 1541.492938
2025-09-27 14:44:06,052 Stage: Train 0.5 | Epoch: 262 | Iter: 398600 | Total Loss: 0.003442 | Recon Loss: 0.002913 | Commit Loss: 0.001057 | Perplexity: 1547.041248
2025-09-27 14:44:52,187 Stage: Train 0.5 | Epoch: 262 | Iter: 398800 | Total Loss: 0.003521 | Recon Loss: 0.002992 | Commit Loss: 0.001059 | Perplexity: 1547.418969
2025-09-27 14:45:38,327 Stage: Train 0.5 | Epoch: 262 | Iter: 399000 | Total Loss: 0.003543 | Recon Loss: 0.003014 | Commit Loss: 0.001058 | Perplexity: 1542.541416
2025-09-27 14:46:24,534 Stage: Train 0.5 | Epoch: 262 | Iter: 399200 | Total Loss: 0.003448 | Recon Loss: 0.002919 | Commit Loss: 0.001056 | Perplexity: 1545.563504
2025-09-27 14:47:10,577 Stage: Train 0.5 | Epoch: 262 | Iter: 399400 | Total Loss: 0.003480 | Recon Loss: 0.002953 | Commit Loss: 0.001054 | Perplexity: 1540.917211
Trainning Epoch:  80%|███████▉  | 263/330 [24:34:59<6:31:15, 350.38s/it]2025-09-27 14:47:56,987 Stage: Train 0.5 | Epoch: 263 | Iter: 399600 | Total Loss: 0.003484 | Recon Loss: 0.002954 | Commit Loss: 0.001060 | Perplexity: 1545.432744
2025-09-27 14:48:43,116 Stage: Train 0.5 | Epoch: 263 | Iter: 399800 | Total Loss: 0.003447 | Recon Loss: 0.002920 | Commit Loss: 0.001053 | Perplexity: 1546.684650
2025-09-27 14:49:29,322 Stage: Train 0.5 | Epoch: 263 | Iter: 400000 | Total Loss: 0.003398 | Recon Loss: 0.002873 | Commit Loss: 0.001049 | Perplexity: 1541.486744
2025-09-27 14:49:29,322 Saving model at iteration 400000
2025-09-27 14:49:29,801 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000
2025-09-27 14:49:30,124 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/model.safetensors
2025-09-27 14:49:30,541 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/optimizer.bin
2025-09-27 14:49:30,542 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/scheduler.bin
2025-09-27 14:49:30,542 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/sampler.bin
2025-09-27 14:49:30,543 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/random_states_0.pkl
2025-09-27 14:50:16,995 Stage: Train 0.5 | Epoch: 263 | Iter: 400200 | Total Loss: 0.003491 | Recon Loss: 0.002966 | Commit Loss: 0.001048 | Perplexity: 1543.159341
2025-09-27 14:51:03,204 Stage: Train 0.5 | Epoch: 263 | Iter: 400400 | Total Loss: 0.003462 | Recon Loss: 0.002937 | Commit Loss: 0.001051 | Perplexity: 1542.403702
2025-09-27 14:51:49,638 Stage: Train 0.5 | Epoch: 263 | Iter: 400600 | Total Loss: 0.003517 | Recon Loss: 0.002987 | Commit Loss: 0.001060 | Perplexity: 1546.288924
2025-09-27 14:52:35,841 Stage: Train 0.5 | Epoch: 263 | Iter: 400800 | Total Loss: 0.003461 | Recon Loss: 0.002932 | Commit Loss: 0.001060 | Perplexity: 1544.117197
2025-09-27 14:53:21,935 Stage: Train 0.5 | Epoch: 263 | Iter: 401000 | Total Loss: 0.003504 | Recon Loss: 0.002971 | Commit Loss: 0.001066 | Perplexity: 1548.761420
Trainning Epoch:  80%|████████  | 264/330 [24:40:51<6:26:11, 351.09s/it]2025-09-27 14:54:08,172 Stage: Train 0.5 | Epoch: 264 | Iter: 401200 | Total Loss: 0.003497 | Recon Loss: 0.002966 | Commit Loss: 0.001062 | Perplexity: 1547.153162
2025-09-27 14:54:54,408 Stage: Train 0.5 | Epoch: 264 | Iter: 401400 | Total Loss: 0.003482 | Recon Loss: 0.002956 | Commit Loss: 0.001051 | Perplexity: 1539.477327
2025-09-27 14:55:40,560 Stage: Train 0.5 | Epoch: 264 | Iter: 401600 | Total Loss: 0.003522 | Recon Loss: 0.002999 | Commit Loss: 0.001046 | Perplexity: 1543.815792
2025-09-27 14:56:26,708 Stage: Train 0.5 | Epoch: 264 | Iter: 401800 | Total Loss: 0.003468 | Recon Loss: 0.002940 | Commit Loss: 0.001055 | Perplexity: 1545.184744
2025-09-27 14:57:12,890 Stage: Train 0.5 | Epoch: 264 | Iter: 402000 | Total Loss: 0.003496 | Recon Loss: 0.002968 | Commit Loss: 0.001057 | Perplexity: 1548.397045
2025-09-27 14:57:59,028 Stage: Train 0.5 | Epoch: 264 | Iter: 402200 | Total Loss: 0.003462 | Recon Loss: 0.002934 | Commit Loss: 0.001055 | Perplexity: 1549.099049
2025-09-27 14:58:45,142 Stage: Train 0.5 | Epoch: 264 | Iter: 402400 | Total Loss: 0.003501 | Recon Loss: 0.002974 | Commit Loss: 0.001052 | Perplexity: 1541.920762
Trainning Epoch:  80%|████████  | 265/330 [24:46:42<6:20:13, 350.98s/it]2025-09-27 14:59:31,643 Stage: Train 0.5 | Epoch: 265 | Iter: 402600 | Total Loss: 0.003476 | Recon Loss: 0.002955 | Commit Loss: 0.001041 | Perplexity: 1536.975004
2025-09-27 15:00:17,865 Stage: Train 0.5 | Epoch: 265 | Iter: 402800 | Total Loss: 0.003454 | Recon Loss: 0.002926 | Commit Loss: 0.001057 | Perplexity: 1545.451931
2025-09-27 15:01:04,059 Stage: Train 0.5 | Epoch: 265 | Iter: 403000 | Total Loss: 0.003504 | Recon Loss: 0.002981 | Commit Loss: 0.001046 | Perplexity: 1542.642892
2025-09-27 15:01:49,998 Stage: Train 0.5 | Epoch: 265 | Iter: 403200 | Total Loss: 0.003453 | Recon Loss: 0.002928 | Commit Loss: 0.001052 | Perplexity: 1539.513267
2025-09-27 15:02:36,220 Stage: Train 0.5 | Epoch: 265 | Iter: 403400 | Total Loss: 0.003609 | Recon Loss: 0.003080 | Commit Loss: 0.001058 | Perplexity: 1547.068715
2025-09-27 15:03:22,451 Stage: Train 0.5 | Epoch: 265 | Iter: 403600 | Total Loss: 0.003453 | Recon Loss: 0.002929 | Commit Loss: 0.001048 | Perplexity: 1544.771151
2025-09-27 15:04:08,646 Stage: Train 0.5 | Epoch: 265 | Iter: 403800 | Total Loss: 0.003561 | Recon Loss: 0.003038 | Commit Loss: 0.001046 | Perplexity: 1539.812555
2025-09-27 15:04:55,036 Stage: Train 0.5 | Epoch: 265 | Iter: 404000 | Total Loss: 0.003356 | Recon Loss: 0.002826 | Commit Loss: 0.001062 | Perplexity: 1549.644875
Trainning Epoch:  81%|████████  | 266/330 [24:52:33<6:14:24, 351.01s/it]2025-09-27 15:05:41,390 Stage: Train 0.5 | Epoch: 266 | Iter: 404200 | Total Loss: 0.003503 | Recon Loss: 0.002975 | Commit Loss: 0.001057 | Perplexity: 1547.688010
2025-09-27 15:06:27,636 Stage: Train 0.5 | Epoch: 266 | Iter: 404400 | Total Loss: 0.003407 | Recon Loss: 0.002884 | Commit Loss: 0.001045 | Perplexity: 1541.059407
2025-09-27 15:07:13,833 Stage: Train 0.5 | Epoch: 266 | Iter: 404600 | Total Loss: 0.003410 | Recon Loss: 0.002886 | Commit Loss: 0.001048 | Perplexity: 1547.595800
2025-09-27 15:08:00,071 Stage: Train 0.5 | Epoch: 266 | Iter: 404800 | Total Loss: 0.003479 | Recon Loss: 0.002952 | Commit Loss: 0.001055 | Perplexity: 1544.400752
2025-09-27 15:08:46,054 Stage: Train 0.5 | Epoch: 266 | Iter: 405000 | Total Loss: 0.003447 | Recon Loss: 0.002921 | Commit Loss: 0.001051 | Perplexity: 1540.787318
2025-09-27 15:09:32,205 Stage: Train 0.5 | Epoch: 266 | Iter: 405200 | Total Loss: 0.003485 | Recon Loss: 0.002957 | Commit Loss: 0.001057 | Perplexity: 1548.888130
2025-09-27 15:10:18,299 Stage: Train 0.5 | Epoch: 266 | Iter: 405400 | Total Loss: 0.003486 | Recon Loss: 0.002957 | Commit Loss: 0.001058 | Perplexity: 1543.463905
Trainning Epoch:  81%|████████  | 267/330 [24:58:24<6:08:30, 350.95s/it]2025-09-27 15:11:04,813 Stage: Train 0.5 | Epoch: 267 | Iter: 405600 | Total Loss: 0.003491 | Recon Loss: 0.002964 | Commit Loss: 0.001055 | Perplexity: 1546.847366
2025-09-27 15:11:51,022 Stage: Train 0.5 | Epoch: 267 | Iter: 405800 | Total Loss: 0.003405 | Recon Loss: 0.002884 | Commit Loss: 0.001042 | Perplexity: 1538.161182
2025-09-27 15:12:37,160 Stage: Train 0.5 | Epoch: 267 | Iter: 406000 | Total Loss: 0.003575 | Recon Loss: 0.003047 | Commit Loss: 0.001055 | Perplexity: 1542.241735
2025-09-27 15:13:23,383 Stage: Train 0.5 | Epoch: 267 | Iter: 406200 | Total Loss: 0.003439 | Recon Loss: 0.002914 | Commit Loss: 0.001050 | Perplexity: 1548.207783
2025-09-27 15:14:09,398 Stage: Train 0.5 | Epoch: 267 | Iter: 406400 | Total Loss: 0.003417 | Recon Loss: 0.002887 | Commit Loss: 0.001058 | Perplexity: 1547.873491
2025-09-27 15:14:55,578 Stage: Train 0.5 | Epoch: 267 | Iter: 406600 | Total Loss: 0.003441 | Recon Loss: 0.002913 | Commit Loss: 0.001056 | Perplexity: 1546.620004
2025-09-27 15:15:41,424 Stage: Train 0.5 | Epoch: 267 | Iter: 406800 | Total Loss: 0.003470 | Recon Loss: 0.002946 | Commit Loss: 0.001049 | Perplexity: 1542.588835
2025-09-27 15:16:27,646 Stage: Train 0.5 | Epoch: 267 | Iter: 407000 | Total Loss: 0.003379 | Recon Loss: 0.002854 | Commit Loss: 0.001049 | Perplexity: 1544.937999
Trainning Epoch:  81%|████████  | 268/330 [25:04:15<6:02:33, 350.87s/it]2025-09-27 15:17:14,244 Stage: Train 0.5 | Epoch: 268 | Iter: 407200 | Total Loss: 0.003652 | Recon Loss: 0.003128 | Commit Loss: 0.001049 | Perplexity: 1543.152770
2025-09-27 15:18:00,340 Stage: Train 0.5 | Epoch: 268 | Iter: 407400 | Total Loss: 0.003407 | Recon Loss: 0.002882 | Commit Loss: 0.001049 | Perplexity: 1546.025976
2025-09-27 15:18:46,440 Stage: Train 0.5 | Epoch: 268 | Iter: 407600 | Total Loss: 0.003433 | Recon Loss: 0.002907 | Commit Loss: 0.001052 | Perplexity: 1543.966223
2025-09-27 15:19:32,535 Stage: Train 0.5 | Epoch: 268 | Iter: 407800 | Total Loss: 0.003489 | Recon Loss: 0.002963 | Commit Loss: 0.001051 | Perplexity: 1548.301505
2025-09-27 15:20:18,789 Stage: Train 0.5 | Epoch: 268 | Iter: 408000 | Total Loss: 0.003396 | Recon Loss: 0.002868 | Commit Loss: 0.001057 | Perplexity: 1548.072344
2025-09-27 15:21:04,991 Stage: Train 0.5 | Epoch: 268 | Iter: 408200 | Total Loss: 0.003447 | Recon Loss: 0.002921 | Commit Loss: 0.001052 | Perplexity: 1543.245240
2025-09-27 15:21:51,140 Stage: Train 0.5 | Epoch: 268 | Iter: 408400 | Total Loss: 0.003525 | Recon Loss: 0.003000 | Commit Loss: 0.001052 | Perplexity: 1545.939047
2025-09-27 15:22:37,042 Stage: Train 0.5 | Epoch: 268 | Iter: 408600 | Total Loss: 0.003401 | Recon Loss: 0.002876 | Commit Loss: 0.001051 | Perplexity: 1545.488450
Trainning Epoch:  82%|████████▏ | 269/330 [25:10:05<5:56:39, 350.80s/it]2025-09-27 15:23:23,559 Stage: Train 0.5 | Epoch: 269 | Iter: 408800 | Total Loss: 0.003524 | Recon Loss: 0.002999 | Commit Loss: 0.001050 | Perplexity: 1543.176330
2025-09-27 15:24:09,652 Stage: Train 0.5 | Epoch: 269 | Iter: 409000 | Total Loss: 0.003405 | Recon Loss: 0.002881 | Commit Loss: 0.001048 | Perplexity: 1546.195297
2025-09-27 15:24:55,768 Stage: Train 0.5 | Epoch: 269 | Iter: 409200 | Total Loss: 0.003414 | Recon Loss: 0.002891 | Commit Loss: 0.001047 | Perplexity: 1545.379772
2025-09-27 15:25:42,025 Stage: Train 0.5 | Epoch: 269 | Iter: 409400 | Total Loss: 0.003501 | Recon Loss: 0.002975 | Commit Loss: 0.001052 | Perplexity: 1545.454138
2025-09-27 15:26:28,248 Stage: Train 0.5 | Epoch: 269 | Iter: 409600 | Total Loss: 0.003429 | Recon Loss: 0.002903 | Commit Loss: 0.001052 | Perplexity: 1542.919485
2025-09-27 15:27:14,406 Stage: Train 0.5 | Epoch: 269 | Iter: 409800 | Total Loss: 0.003481 | Recon Loss: 0.002956 | Commit Loss: 0.001051 | Perplexity: 1546.747559
2025-09-27 15:28:00,532 Stage: Train 0.5 | Epoch: 269 | Iter: 410000 | Total Loss: 0.003385 | Recon Loss: 0.002859 | Commit Loss: 0.001053 | Perplexity: 1546.942633
Trainning Epoch:  82%|████████▏ | 270/330 [25:15:56<5:50:49, 350.83s/it]2025-09-27 15:28:47,015 Stage: Train 0.5 | Epoch: 270 | Iter: 410200 | Total Loss: 0.003526 | Recon Loss: 0.002999 | Commit Loss: 0.001054 | Perplexity: 1548.477705
2025-09-27 15:29:33,115 Stage: Train 0.5 | Epoch: 270 | Iter: 410400 | Total Loss: 0.003463 | Recon Loss: 0.002939 | Commit Loss: 0.001046 | Perplexity: 1543.816774
2025-09-27 15:30:18,891 Stage: Train 0.5 | Epoch: 270 | Iter: 410600 | Total Loss: 0.003453 | Recon Loss: 0.002929 | Commit Loss: 0.001049 | Perplexity: 1543.907827
2025-09-27 15:31:05,142 Stage: Train 0.5 | Epoch: 270 | Iter: 410800 | Total Loss: 0.003491 | Recon Loss: 0.002968 | Commit Loss: 0.001047 | Perplexity: 1543.989271
2025-09-27 15:31:51,343 Stage: Train 0.5 | Epoch: 270 | Iter: 411000 | Total Loss: 0.003403 | Recon Loss: 0.002878 | Commit Loss: 0.001049 | Perplexity: 1545.480346
2025-09-27 15:32:37,472 Stage: Train 0.5 | Epoch: 270 | Iter: 411200 | Total Loss: 0.003459 | Recon Loss: 0.002932 | Commit Loss: 0.001053 | Perplexity: 1542.700689
2025-09-27 15:33:23,686 Stage: Train 0.5 | Epoch: 270 | Iter: 411400 | Total Loss: 0.003513 | Recon Loss: 0.002986 | Commit Loss: 0.001053 | Perplexity: 1550.324445
2025-09-27 15:34:10,024 Stage: Train 0.5 | Epoch: 270 | Iter: 411600 | Total Loss: 0.003486 | Recon Loss: 0.002958 | Commit Loss: 0.001056 | Perplexity: 1547.046142
Trainning Epoch:  82%|████████▏ | 271/330 [25:21:47<5:44:58, 350.83s/it]2025-09-27 15:34:56,467 Stage: Train 0.5 | Epoch: 271 | Iter: 411800 | Total Loss: 0.003453 | Recon Loss: 0.002930 | Commit Loss: 0.001046 | Perplexity: 1544.108344
2025-09-27 15:35:42,554 Stage: Train 0.5 | Epoch: 271 | Iter: 412000 | Total Loss: 0.003402 | Recon Loss: 0.002879 | Commit Loss: 0.001046 | Perplexity: 1544.453591
2025-09-27 15:36:28,718 Stage: Train 0.5 | Epoch: 271 | Iter: 412200 | Total Loss: 0.003489 | Recon Loss: 0.002965 | Commit Loss: 0.001048 | Perplexity: 1545.091259
2025-09-27 15:37:14,489 Stage: Train 0.5 | Epoch: 271 | Iter: 412400 | Total Loss: 0.003466 | Recon Loss: 0.002941 | Commit Loss: 0.001051 | Perplexity: 1547.032804
2025-09-27 15:38:00,579 Stage: Train 0.5 | Epoch: 271 | Iter: 412600 | Total Loss: 0.003442 | Recon Loss: 0.002917 | Commit Loss: 0.001049 | Perplexity: 1548.313135
2025-09-27 15:38:46,863 Stage: Train 0.5 | Epoch: 271 | Iter: 412800 | Total Loss: 0.003555 | Recon Loss: 0.003028 | Commit Loss: 0.001054 | Perplexity: 1544.986724
2025-09-27 15:39:32,921 Stage: Train 0.5 | Epoch: 271 | Iter: 413000 | Total Loss: 0.003435 | Recon Loss: 0.002910 | Commit Loss: 0.001050 | Perplexity: 1543.429362
Trainning Epoch:  82%|████████▏ | 272/330 [25:27:37<5:39:00, 350.71s/it]2025-09-27 15:40:19,361 Stage: Train 0.5 | Epoch: 272 | Iter: 413200 | Total Loss: 0.003464 | Recon Loss: 0.002940 | Commit Loss: 0.001047 | Perplexity: 1540.038291
2025-09-27 15:41:05,512 Stage: Train 0.5 | Epoch: 272 | Iter: 413400 | Total Loss: 0.003437 | Recon Loss: 0.002915 | Commit Loss: 0.001043 | Perplexity: 1542.495810
2025-09-27 15:41:51,742 Stage: Train 0.5 | Epoch: 272 | Iter: 413600 | Total Loss: 0.003420 | Recon Loss: 0.002896 | Commit Loss: 0.001049 | Perplexity: 1545.552619
2025-09-27 15:42:37,855 Stage: Train 0.5 | Epoch: 272 | Iter: 413800 | Total Loss: 0.003432 | Recon Loss: 0.002911 | Commit Loss: 0.001042 | Perplexity: 1540.101254
2025-09-27 15:43:24,129 Stage: Train 0.5 | Epoch: 272 | Iter: 414000 | Total Loss: 0.003401 | Recon Loss: 0.002876 | Commit Loss: 0.001051 | Perplexity: 1548.251997
2025-09-27 15:44:10,088 Stage: Train 0.5 | Epoch: 272 | Iter: 414200 | Total Loss: 0.003483 | Recon Loss: 0.002954 | Commit Loss: 0.001057 | Perplexity: 1547.226138
2025-09-27 15:44:56,277 Stage: Train 0.5 | Epoch: 272 | Iter: 414400 | Total Loss: 0.003404 | Recon Loss: 0.002879 | Commit Loss: 0.001049 | Perplexity: 1547.115073
2025-09-27 15:45:42,536 Stage: Train 0.5 | Epoch: 272 | Iter: 414600 | Total Loss: 0.003453 | Recon Loss: 0.002929 | Commit Loss: 0.001047 | Perplexity: 1547.441976
Trainning Epoch:  83%|████████▎ | 273/330 [25:33:28<5:33:15, 350.81s/it]2025-09-27 15:46:29,045 Stage: Train 0.5 | Epoch: 273 | Iter: 414800 | Total Loss: 0.003482 | Recon Loss: 0.002960 | Commit Loss: 0.001044 | Perplexity: 1542.117894
2025-09-27 15:47:15,294 Stage: Train 0.5 | Epoch: 273 | Iter: 415000 | Total Loss: 0.003422 | Recon Loss: 0.002899 | Commit Loss: 0.001046 | Perplexity: 1544.547528
2025-09-27 15:48:01,526 Stage: Train 0.5 | Epoch: 273 | Iter: 415200 | Total Loss: 0.003444 | Recon Loss: 0.002920 | Commit Loss: 0.001048 | Perplexity: 1545.255227
2025-09-27 15:48:47,702 Stage: Train 0.5 | Epoch: 273 | Iter: 415400 | Total Loss: 0.003457 | Recon Loss: 0.002934 | Commit Loss: 0.001046 | Perplexity: 1551.000599
2025-09-27 15:49:33,785 Stage: Train 0.5 | Epoch: 273 | Iter: 415600 | Total Loss: 0.003529 | Recon Loss: 0.003004 | Commit Loss: 0.001049 | Perplexity: 1547.012988
2025-09-27 15:50:19,896 Stage: Train 0.5 | Epoch: 273 | Iter: 415800 | Total Loss: 0.003410 | Recon Loss: 0.002882 | Commit Loss: 0.001055 | Perplexity: 1550.428513
2025-09-27 15:51:06,111 Stage: Train 0.5 | Epoch: 273 | Iter: 416000 | Total Loss: 0.003498 | Recon Loss: 0.002972 | Commit Loss: 0.001051 | Perplexity: 1545.522084
2025-09-27 15:51:51,954 Stage: Train 0.5 | Epoch: 273 | Iter: 416200 | Total Loss: 0.003421 | Recon Loss: 0.002899 | Commit Loss: 0.001043 | Perplexity: 1539.959748
Trainning Epoch:  83%|████████▎ | 274/330 [25:39:19<5:27:20, 350.73s/it]2025-09-27 15:52:38,325 Stage: Train 0.5 | Epoch: 274 | Iter: 416400 | Total Loss: 0.003392 | Recon Loss: 0.002870 | Commit Loss: 0.001043 | Perplexity: 1545.868727
2025-09-27 15:53:24,536 Stage: Train 0.5 | Epoch: 274 | Iter: 416600 | Total Loss: 0.003423 | Recon Loss: 0.002901 | Commit Loss: 0.001044 | Perplexity: 1545.070202
2025-09-27 15:54:10,592 Stage: Train 0.5 | Epoch: 274 | Iter: 416800 | Total Loss: 0.003512 | Recon Loss: 0.002987 | Commit Loss: 0.001049 | Perplexity: 1548.577411
2025-09-27 15:54:56,833 Stage: Train 0.5 | Epoch: 274 | Iter: 417000 | Total Loss: 0.003485 | Recon Loss: 0.002964 | Commit Loss: 0.001044 | Perplexity: 1543.066607
2025-09-27 15:55:43,093 Stage: Train 0.5 | Epoch: 274 | Iter: 417200 | Total Loss: 0.003451 | Recon Loss: 0.002926 | Commit Loss: 0.001050 | Perplexity: 1547.399987
2025-09-27 15:56:29,153 Stage: Train 0.5 | Epoch: 274 | Iter: 417400 | Total Loss: 0.003390 | Recon Loss: 0.002863 | Commit Loss: 0.001053 | Perplexity: 1548.928105
2025-09-27 15:57:15,404 Stage: Train 0.5 | Epoch: 274 | Iter: 417600 | Total Loss: 0.003410 | Recon Loss: 0.002886 | Commit Loss: 0.001048 | Perplexity: 1547.469346
Trainning Epoch:  83%|████████▎ | 275/330 [25:45:10<5:21:34, 350.82s/it]2025-09-27 15:58:01,887 Stage: Train 0.5 | Epoch: 275 | Iter: 417800 | Total Loss: 0.003439 | Recon Loss: 0.002911 | Commit Loss: 0.001056 | Perplexity: 1545.396127
2025-09-27 15:58:47,791 Stage: Train 0.5 | Epoch: 275 | Iter: 418000 | Total Loss: 0.003397 | Recon Loss: 0.002875 | Commit Loss: 0.001044 | Perplexity: 1545.859920
2025-09-27 15:59:33,979 Stage: Train 0.5 | Epoch: 275 | Iter: 418200 | Total Loss: 0.003484 | Recon Loss: 0.002965 | Commit Loss: 0.001038 | Perplexity: 1543.642640
2025-09-27 16:00:20,166 Stage: Train 0.5 | Epoch: 275 | Iter: 418400 | Total Loss: 0.003477 | Recon Loss: 0.002955 | Commit Loss: 0.001046 | Perplexity: 1546.663027
2025-09-27 16:01:06,213 Stage: Train 0.5 | Epoch: 275 | Iter: 418600 | Total Loss: 0.003423 | Recon Loss: 0.002899 | Commit Loss: 0.001049 | Perplexity: 1548.078307
2025-09-27 16:01:52,476 Stage: Train 0.5 | Epoch: 275 | Iter: 418800 | Total Loss: 0.003398 | Recon Loss: 0.002874 | Commit Loss: 0.001049 | Perplexity: 1548.984962
2025-09-27 16:02:38,655 Stage: Train 0.5 | Epoch: 275 | Iter: 419000 | Total Loss: 0.003445 | Recon Loss: 0.002925 | Commit Loss: 0.001040 | Perplexity: 1540.686014
2025-09-27 16:03:24,741 Stage: Train 0.5 | Epoch: 275 | Iter: 419200 | Total Loss: 0.003660 | Recon Loss: 0.003134 | Commit Loss: 0.001052 | Perplexity: 1544.537599
Trainning Epoch:  84%|████████▎ | 276/330 [25:51:01<5:15:39, 350.74s/it]2025-09-27 16:04:11,114 Stage: Train 0.5 | Epoch: 276 | Iter: 419400 | Total Loss: 0.003370 | Recon Loss: 0.002845 | Commit Loss: 0.001050 | Perplexity: 1549.220161
2025-09-27 16:04:57,360 Stage: Train 0.5 | Epoch: 276 | Iter: 419600 | Total Loss: 0.003422 | Recon Loss: 0.002899 | Commit Loss: 0.001046 | Perplexity: 1548.799516
2025-09-27 16:05:43,400 Stage: Train 0.5 | Epoch: 276 | Iter: 419800 | Total Loss: 0.003567 | Recon Loss: 0.003044 | Commit Loss: 0.001046 | Perplexity: 1546.731791
2025-09-27 16:06:29,547 Stage: Train 0.5 | Epoch: 276 | Iter: 420000 | Total Loss: 0.003322 | Recon Loss: 0.002799 | Commit Loss: 0.001046 | Perplexity: 1546.032138
2025-09-27 16:06:29,547 Saving model at iteration 420000
2025-09-27 16:06:29,775 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000
2025-09-27 16:06:30,087 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/model.safetensors
2025-09-27 16:06:30,485 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/optimizer.bin
2025-09-27 16:06:30,486 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/scheduler.bin
2025-09-27 16:06:30,486 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/sampler.bin
2025-09-27 16:06:30,487 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/random_states_0.pkl
2025-09-27 16:07:16,716 Stage: Train 0.5 | Epoch: 276 | Iter: 420200 | Total Loss: 0.003450 | Recon Loss: 0.002928 | Commit Loss: 0.001045 | Perplexity: 1543.106748
2025-09-27 16:08:03,042 Stage: Train 0.5 | Epoch: 276 | Iter: 420400 | Total Loss: 0.003445 | Recon Loss: 0.002923 | Commit Loss: 0.001044 | Perplexity: 1542.561522
2025-09-27 16:08:49,249 Stage: Train 0.5 | Epoch: 276 | Iter: 420600 | Total Loss: 0.003395 | Recon Loss: 0.002871 | Commit Loss: 0.001048 | Perplexity: 1548.971863
Trainning Epoch:  84%|████████▍ | 277/330 [25:56:53<5:10:09, 351.13s/it]2025-09-27 16:09:35,727 Stage: Train 0.5 | Epoch: 277 | Iter: 420800 | Total Loss: 0.003486 | Recon Loss: 0.002962 | Commit Loss: 0.001047 | Perplexity: 1546.864637
2025-09-27 16:10:21,819 Stage: Train 0.5 | Epoch: 277 | Iter: 421000 | Total Loss: 0.003413 | Recon Loss: 0.002893 | Commit Loss: 0.001040 | Perplexity: 1543.258746
2025-09-27 16:11:07,957 Stage: Train 0.5 | Epoch: 277 | Iter: 421200 | Total Loss: 0.003407 | Recon Loss: 0.002888 | Commit Loss: 0.001038 | Perplexity: 1542.244102
2025-09-27 16:11:54,254 Stage: Train 0.5 | Epoch: 277 | Iter: 421400 | Total Loss: 0.003392 | Recon Loss: 0.002869 | Commit Loss: 0.001046 | Perplexity: 1552.672723
2025-09-27 16:12:38,747 Stage: Train 0.5 | Epoch: 277 | Iter: 421600 | Total Loss: 0.003414 | Recon Loss: 0.002893 | Commit Loss: 0.001043 | Perplexity: 1547.211299
2025-09-27 16:13:24,857 Stage: Train 0.5 | Epoch: 277 | Iter: 421800 | Total Loss: 0.003469 | Recon Loss: 0.002945 | Commit Loss: 0.001048 | Perplexity: 1551.930936
2025-09-27 16:14:10,958 Stage: Train 0.5 | Epoch: 277 | Iter: 422000 | Total Loss: 0.003431 | Recon Loss: 0.002908 | Commit Loss: 0.001046 | Perplexity: 1548.625901
2025-09-27 16:14:56,973 Stage: Train 0.5 | Epoch: 277 | Iter: 422200 | Total Loss: 0.003448 | Recon Loss: 0.002922 | Commit Loss: 0.001053 | Perplexity: 1547.683045
Trainning Epoch:  84%|████████▍ | 278/330 [26:02:42<5:03:44, 350.46s/it]2025-09-27 16:15:43,359 Stage: Train 0.5 | Epoch: 278 | Iter: 422400 | Total Loss: 0.003384 | Recon Loss: 0.002864 | Commit Loss: 0.001040 | Perplexity: 1541.383804
2025-09-27 16:16:29,602 Stage: Train 0.5 | Epoch: 278 | Iter: 422600 | Total Loss: 0.003441 | Recon Loss: 0.002919 | Commit Loss: 0.001045 | Perplexity: 1544.723193
2025-09-27 16:17:15,878 Stage: Train 0.5 | Epoch: 278 | Iter: 422800 | Total Loss: 0.003467 | Recon Loss: 0.002945 | Commit Loss: 0.001044 | Perplexity: 1547.813033
2025-09-27 16:18:02,184 Stage: Train 0.5 | Epoch: 278 | Iter: 423000 | Total Loss: 0.003375 | Recon Loss: 0.002852 | Commit Loss: 0.001046 | Perplexity: 1552.271286
2025-09-27 16:18:48,478 Stage: Train 0.5 | Epoch: 278 | Iter: 423200 | Total Loss: 0.003463 | Recon Loss: 0.002940 | Commit Loss: 0.001046 | Perplexity: 1549.525002
2025-09-27 16:19:34,738 Stage: Train 0.5 | Epoch: 278 | Iter: 423400 | Total Loss: 0.003461 | Recon Loss: 0.002939 | Commit Loss: 0.001043 | Perplexity: 1543.875538
2025-09-27 16:20:20,614 Stage: Train 0.5 | Epoch: 278 | Iter: 423600 | Total Loss: 0.003450 | Recon Loss: 0.002922 | Commit Loss: 0.001055 | Perplexity: 1553.732557
2025-09-27 16:21:06,867 Stage: Train 0.5 | Epoch: 278 | Iter: 423800 | Total Loss: 0.003405 | Recon Loss: 0.002884 | Commit Loss: 0.001042 | Perplexity: 1544.151295
Trainning Epoch:  85%|████████▍ | 279/330 [26:08:33<4:58:05, 350.69s/it]2025-09-27 16:21:53,300 Stage: Train 0.5 | Epoch: 279 | Iter: 424000 | Total Loss: 0.003459 | Recon Loss: 0.002937 | Commit Loss: 0.001045 | Perplexity: 1547.446941
2025-09-27 16:22:39,416 Stage: Train 0.5 | Epoch: 279 | Iter: 424200 | Total Loss: 0.003410 | Recon Loss: 0.002889 | Commit Loss: 0.001041 | Perplexity: 1548.505781
2025-09-27 16:23:25,589 Stage: Train 0.5 | Epoch: 279 | Iter: 424400 | Total Loss: 0.003344 | Recon Loss: 0.002824 | Commit Loss: 0.001040 | Perplexity: 1546.029728
2025-09-27 16:24:11,849 Stage: Train 0.5 | Epoch: 279 | Iter: 424600 | Total Loss: 0.003475 | Recon Loss: 0.002956 | Commit Loss: 0.001038 | Perplexity: 1548.728941
2025-09-27 16:24:58,001 Stage: Train 0.5 | Epoch: 279 | Iter: 424800 | Total Loss: 0.003370 | Recon Loss: 0.002849 | Commit Loss: 0.001043 | Perplexity: 1544.948591
2025-09-27 16:25:44,084 Stage: Train 0.5 | Epoch: 279 | Iter: 425000 | Total Loss: 0.003411 | Recon Loss: 0.002887 | Commit Loss: 0.001048 | Perplexity: 1548.742411
2025-09-27 16:26:30,261 Stage: Train 0.5 | Epoch: 279 | Iter: 425200 | Total Loss: 0.003420 | Recon Loss: 0.002897 | Commit Loss: 0.001046 | Perplexity: 1545.936851
Trainning Epoch:  85%|████████▍ | 280/330 [26:14:23<4:52:12, 350.66s/it]2025-09-27 16:27:16,422 Stage: Train 0.5 | Epoch: 280 | Iter: 425400 | Total Loss: 0.003414 | Recon Loss: 0.002890 | Commit Loss: 0.001048 | Perplexity: 1549.172492
2025-09-27 16:28:02,618 Stage: Train 0.5 | Epoch: 280 | Iter: 425600 | Total Loss: 0.003368 | Recon Loss: 0.002848 | Commit Loss: 0.001040 | Perplexity: 1547.615070
2025-09-27 16:28:48,812 Stage: Train 0.5 | Epoch: 280 | Iter: 425800 | Total Loss: 0.003421 | Recon Loss: 0.002900 | Commit Loss: 0.001042 | Perplexity: 1546.488297
2025-09-27 16:29:34,919 Stage: Train 0.5 | Epoch: 280 | Iter: 426000 | Total Loss: 0.003470 | Recon Loss: 0.002949 | Commit Loss: 0.001040 | Perplexity: 1548.107804
2025-09-27 16:30:21,116 Stage: Train 0.5 | Epoch: 280 | Iter: 426200 | Total Loss: 0.003326 | Recon Loss: 0.002805 | Commit Loss: 0.001042 | Perplexity: 1550.212490
2025-09-27 16:31:07,238 Stage: Train 0.5 | Epoch: 280 | Iter: 426400 | Total Loss: 0.003424 | Recon Loss: 0.002901 | Commit Loss: 0.001046 | Perplexity: 1547.176003
2025-09-27 16:31:53,540 Stage: Train 0.5 | Epoch: 280 | Iter: 426600 | Total Loss: 0.003457 | Recon Loss: 0.002935 | Commit Loss: 0.001045 | Perplexity: 1546.343093
2025-09-27 16:32:39,739 Stage: Train 0.5 | Epoch: 280 | Iter: 426800 | Total Loss: 0.003403 | Recon Loss: 0.002880 | Commit Loss: 0.001045 | Perplexity: 1545.734965
Trainning Epoch:  85%|████████▌ | 281/330 [26:20:14<4:46:28, 350.79s/it]2025-09-27 16:33:26,350 Stage: Train 0.5 | Epoch: 281 | Iter: 427000 | Total Loss: 0.003447 | Recon Loss: 0.002927 | Commit Loss: 0.001041 | Perplexity: 1549.254503
2025-09-27 16:34:12,230 Stage: Train 0.5 | Epoch: 281 | Iter: 427200 | Total Loss: 0.003416 | Recon Loss: 0.002897 | Commit Loss: 0.001039 | Perplexity: 1544.521635
2025-09-27 16:34:58,464 Stage: Train 0.5 | Epoch: 281 | Iter: 427400 | Total Loss: 0.003399 | Recon Loss: 0.002879 | Commit Loss: 0.001040 | Perplexity: 1548.439752
2025-09-27 16:35:44,556 Stage: Train 0.5 | Epoch: 281 | Iter: 427600 | Total Loss: 0.003378 | Recon Loss: 0.002858 | Commit Loss: 0.001040 | Perplexity: 1544.650410
2025-09-27 16:36:30,705 Stage: Train 0.5 | Epoch: 281 | Iter: 427800 | Total Loss: 0.003480 | Recon Loss: 0.002956 | Commit Loss: 0.001048 | Perplexity: 1551.070055
2025-09-27 16:37:16,848 Stage: Train 0.5 | Epoch: 281 | Iter: 428000 | Total Loss: 0.003518 | Recon Loss: 0.002998 | Commit Loss: 0.001041 | Perplexity: 1545.852475
2025-09-27 16:38:03,029 Stage: Train 0.5 | Epoch: 281 | Iter: 428200 | Total Loss: 0.003438 | Recon Loss: 0.002916 | Commit Loss: 0.001046 | Perplexity: 1547.310340
Trainning Epoch:  85%|████████▌ | 282/330 [26:26:05<4:40:37, 350.79s/it]2025-09-27 16:38:49,512 Stage: Train 0.5 | Epoch: 282 | Iter: 428400 | Total Loss: 0.003408 | Recon Loss: 0.002889 | Commit Loss: 0.001037 | Perplexity: 1544.070481
2025-09-27 16:39:35,571 Stage: Train 0.5 | Epoch: 282 | Iter: 428600 | Total Loss: 0.003439 | Recon Loss: 0.002916 | Commit Loss: 0.001045 | Perplexity: 1546.201705
2025-09-27 16:40:21,784 Stage: Train 0.5 | Epoch: 282 | Iter: 428800 | Total Loss: 0.003441 | Recon Loss: 0.002921 | Commit Loss: 0.001040 | Perplexity: 1550.050978
2025-09-27 16:41:07,784 Stage: Train 0.5 | Epoch: 282 | Iter: 429000 | Total Loss: 0.003412 | Recon Loss: 0.002891 | Commit Loss: 0.001041 | Perplexity: 1547.294554
2025-09-27 16:41:54,026 Stage: Train 0.5 | Epoch: 282 | Iter: 429200 | Total Loss: 0.003440 | Recon Loss: 0.002924 | Commit Loss: 0.001032 | Perplexity: 1541.258270
2025-09-27 16:42:40,141 Stage: Train 0.5 | Epoch: 282 | Iter: 429400 | Total Loss: 0.003363 | Recon Loss: 0.002845 | Commit Loss: 0.001037 | Perplexity: 1544.381711
2025-09-27 16:43:26,522 Stage: Train 0.5 | Epoch: 282 | Iter: 429600 | Total Loss: 0.003441 | Recon Loss: 0.002917 | Commit Loss: 0.001048 | Perplexity: 1551.367722
2025-09-27 16:44:12,774 Stage: Train 0.5 | Epoch: 282 | Iter: 429800 | Total Loss: 0.003393 | Recon Loss: 0.002870 | Commit Loss: 0.001045 | Perplexity: 1556.484377
Trainning Epoch:  86%|████████▌ | 283/330 [26:31:56<4:34:51, 350.89s/it]2025-09-27 16:44:59,352 Stage: Train 0.5 | Epoch: 283 | Iter: 430000 | Total Loss: 0.003397 | Recon Loss: 0.002875 | Commit Loss: 0.001043 | Perplexity: 1543.776774
2025-09-27 16:45:45,588 Stage: Train 0.5 | Epoch: 283 | Iter: 430200 | Total Loss: 0.003364 | Recon Loss: 0.002846 | Commit Loss: 0.001036 | Perplexity: 1547.965613
2025-09-27 16:46:31,864 Stage: Train 0.5 | Epoch: 283 | Iter: 430400 | Total Loss: 0.003381 | Recon Loss: 0.002861 | Commit Loss: 0.001040 | Perplexity: 1544.658353
2025-09-27 16:47:18,106 Stage: Train 0.5 | Epoch: 283 | Iter: 430600 | Total Loss: 0.003403 | Recon Loss: 0.002882 | Commit Loss: 0.001040 | Perplexity: 1549.136923
2025-09-27 16:48:04,387 Stage: Train 0.5 | Epoch: 283 | Iter: 430800 | Total Loss: 0.003422 | Recon Loss: 0.002899 | Commit Loss: 0.001046 | Perplexity: 1549.178983
2025-09-27 16:48:50,490 Stage: Train 0.5 | Epoch: 283 | Iter: 431000 | Total Loss: 0.003384 | Recon Loss: 0.002860 | Commit Loss: 0.001047 | Perplexity: 1548.552514
2025-09-27 16:49:36,727 Stage: Train 0.5 | Epoch: 283 | Iter: 431200 | Total Loss: 0.003391 | Recon Loss: 0.002870 | Commit Loss: 0.001042 | Perplexity: 1546.977311
Trainning Epoch:  86%|████████▌ | 284/330 [26:37:48<4:29:07, 351.02s/it]2025-09-27 16:50:23,209 Stage: Train 0.5 | Epoch: 284 | Iter: 431400 | Total Loss: 0.003454 | Recon Loss: 0.002931 | Commit Loss: 0.001045 | Perplexity: 1546.999543
2025-09-27 16:51:09,471 Stage: Train 0.5 | Epoch: 284 | Iter: 431600 | Total Loss: 0.003469 | Recon Loss: 0.002952 | Commit Loss: 0.001034 | Perplexity: 1546.690189
2025-09-27 16:51:55,881 Stage: Train 0.5 | Epoch: 284 | Iter: 431800 | Total Loss: 0.003417 | Recon Loss: 0.002897 | Commit Loss: 0.001040 | Perplexity: 1546.454067
2025-09-27 16:52:42,224 Stage: Train 0.5 | Epoch: 284 | Iter: 432000 | Total Loss: 0.003356 | Recon Loss: 0.002837 | Commit Loss: 0.001039 | Perplexity: 1548.608754
2025-09-27 16:53:28,603 Stage: Train 0.5 | Epoch: 284 | Iter: 432200 | Total Loss: 0.003467 | Recon Loss: 0.002946 | Commit Loss: 0.001043 | Perplexity: 1548.953036
2025-09-27 16:54:14,827 Stage: Train 0.5 | Epoch: 284 | Iter: 432400 | Total Loss: 0.003411 | Recon Loss: 0.002891 | Commit Loss: 0.001042 | Perplexity: 1550.966956
2025-09-27 16:55:01,063 Stage: Train 0.5 | Epoch: 284 | Iter: 432600 | Total Loss: 0.003522 | Recon Loss: 0.003004 | Commit Loss: 0.001035 | Perplexity: 1543.299820
2025-09-27 16:55:46,998 Stage: Train 0.5 | Epoch: 284 | Iter: 432800 | Total Loss: 0.003392 | Recon Loss: 0.002870 | Commit Loss: 0.001045 | Perplexity: 1550.649829
Trainning Epoch:  86%|████████▋ | 285/330 [26:43:39<4:23:24, 351.22s/it]2025-09-27 16:56:33,530 Stage: Train 0.5 | Epoch: 285 | Iter: 433000 | Total Loss: 0.003410 | Recon Loss: 0.002893 | Commit Loss: 0.001035 | Perplexity: 1547.067089
2025-09-27 16:57:19,835 Stage: Train 0.5 | Epoch: 285 | Iter: 433200 | Total Loss: 0.003454 | Recon Loss: 0.002935 | Commit Loss: 0.001037 | Perplexity: 1545.416039
2025-09-27 16:58:06,090 Stage: Train 0.5 | Epoch: 285 | Iter: 433400 | Total Loss: 0.003374 | Recon Loss: 0.002856 | Commit Loss: 0.001037 | Perplexity: 1547.644560
2025-09-27 16:58:52,357 Stage: Train 0.5 | Epoch: 285 | Iter: 433600 | Total Loss: 0.003491 | Recon Loss: 0.002970 | Commit Loss: 0.001041 | Perplexity: 1550.912769
2025-09-27 16:59:38,666 Stage: Train 0.5 | Epoch: 285 | Iter: 433800 | Total Loss: 0.003432 | Recon Loss: 0.002913 | Commit Loss: 0.001037 | Perplexity: 1547.923191
2025-09-27 17:00:25,000 Stage: Train 0.5 | Epoch: 285 | Iter: 434000 | Total Loss: 0.003411 | Recon Loss: 0.002892 | Commit Loss: 0.001037 | Perplexity: 1545.934076
2025-09-27 17:01:11,212 Stage: Train 0.5 | Epoch: 285 | Iter: 434200 | Total Loss: 0.003417 | Recon Loss: 0.002899 | Commit Loss: 0.001035 | Perplexity: 1544.682937
2025-09-27 17:01:57,332 Stage: Train 0.5 | Epoch: 285 | Iter: 434400 | Total Loss: 0.003385 | Recon Loss: 0.002865 | Commit Loss: 0.001042 | Perplexity: 1548.466702
Trainning Epoch:  87%|████████▋ | 286/330 [26:49:31<4:17:38, 351.32s/it]2025-09-27 17:02:43,450 Stage: Train 0.5 | Epoch: 286 | Iter: 434600 | Total Loss: 0.003476 | Recon Loss: 0.002954 | Commit Loss: 0.001043 | Perplexity: 1553.114589
2025-09-27 17:03:29,671 Stage: Train 0.5 | Epoch: 286 | Iter: 434800 | Total Loss: 0.003385 | Recon Loss: 0.002866 | Commit Loss: 0.001038 | Perplexity: 1549.901590
2025-09-27 17:04:15,764 Stage: Train 0.5 | Epoch: 286 | Iter: 435000 | Total Loss: 0.003386 | Recon Loss: 0.002867 | Commit Loss: 0.001038 | Perplexity: 1546.300688
2025-09-27 17:05:01,908 Stage: Train 0.5 | Epoch: 286 | Iter: 435200 | Total Loss: 0.003414 | Recon Loss: 0.002896 | Commit Loss: 0.001036 | Perplexity: 1548.453854
2025-09-27 17:05:48,209 Stage: Train 0.5 | Epoch: 286 | Iter: 435400 | Total Loss: 0.003408 | Recon Loss: 0.002891 | Commit Loss: 0.001034 | Perplexity: 1548.843757
2025-09-27 17:06:34,313 Stage: Train 0.5 | Epoch: 286 | Iter: 435600 | Total Loss: 0.003398 | Recon Loss: 0.002882 | Commit Loss: 0.001033 | Perplexity: 1544.056664
2025-09-27 17:07:20,538 Stage: Train 0.5 | Epoch: 286 | Iter: 435800 | Total Loss: 0.003390 | Recon Loss: 0.002868 | Commit Loss: 0.001044 | Perplexity: 1550.316235
Trainning Epoch:  87%|████████▋ | 287/330 [26:55:22<4:11:37, 351.11s/it]2025-09-27 17:08:07,001 Stage: Train 0.5 | Epoch: 287 | Iter: 436000 | Total Loss: 0.003431 | Recon Loss: 0.002913 | Commit Loss: 0.001036 | Perplexity: 1546.130829
2025-09-27 17:08:53,120 Stage: Train 0.5 | Epoch: 287 | Iter: 436200 | Total Loss: 0.003440 | Recon Loss: 0.002919 | Commit Loss: 0.001043 | Perplexity: 1551.396453
2025-09-27 17:09:39,001 Stage: Train 0.5 | Epoch: 287 | Iter: 436400 | Total Loss: 0.003432 | Recon Loss: 0.002912 | Commit Loss: 0.001040 | Perplexity: 1550.279395
2025-09-27 17:10:25,424 Stage: Train 0.5 | Epoch: 287 | Iter: 436600 | Total Loss: 0.003409 | Recon Loss: 0.002891 | Commit Loss: 0.001035 | Perplexity: 1546.208619
2025-09-27 17:11:11,656 Stage: Train 0.5 | Epoch: 287 | Iter: 436800 | Total Loss: 0.003372 | Recon Loss: 0.002856 | Commit Loss: 0.001031 | Perplexity: 1547.350043
2025-09-27 17:11:57,841 Stage: Train 0.5 | Epoch: 287 | Iter: 437000 | Total Loss: 0.003408 | Recon Loss: 0.002892 | Commit Loss: 0.001034 | Perplexity: 1549.644675
2025-09-27 17:12:44,042 Stage: Train 0.5 | Epoch: 287 | Iter: 437200 | Total Loss: 0.003411 | Recon Loss: 0.002890 | Commit Loss: 0.001043 | Perplexity: 1553.348823
2025-09-27 17:13:30,252 Stage: Train 0.5 | Epoch: 287 | Iter: 437400 | Total Loss: 0.003420 | Recon Loss: 0.002904 | Commit Loss: 0.001034 | Perplexity: 1547.248931
Trainning Epoch:  87%|████████▋ | 288/330 [27:01:13<4:05:45, 351.09s/it]2025-09-27 17:14:16,828 Stage: Train 0.5 | Epoch: 288 | Iter: 437600 | Total Loss: 0.003350 | Recon Loss: 0.002835 | Commit Loss: 0.001031 | Perplexity: 1544.404761
2025-09-27 17:15:03,142 Stage: Train 0.5 | Epoch: 288 | Iter: 437800 | Total Loss: 0.003416 | Recon Loss: 0.002898 | Commit Loss: 0.001035 | Perplexity: 1547.939731
2025-09-27 17:15:49,305 Stage: Train 0.5 | Epoch: 288 | Iter: 438000 | Total Loss: 0.003462 | Recon Loss: 0.002941 | Commit Loss: 0.001042 | Perplexity: 1550.612740
2025-09-27 17:16:35,478 Stage: Train 0.5 | Epoch: 288 | Iter: 438200 | Total Loss: 0.003414 | Recon Loss: 0.002896 | Commit Loss: 0.001035 | Perplexity: 1548.843188
2025-09-27 17:17:21,440 Stage: Train 0.5 | Epoch: 288 | Iter: 438400 | Total Loss: 0.003379 | Recon Loss: 0.002862 | Commit Loss: 0.001034 | Perplexity: 1547.044673
2025-09-27 17:18:07,691 Stage: Train 0.5 | Epoch: 288 | Iter: 438600 | Total Loss: 0.003400 | Recon Loss: 0.002882 | Commit Loss: 0.001037 | Perplexity: 1549.899851
2025-09-27 17:18:53,951 Stage: Train 0.5 | Epoch: 288 | Iter: 438800 | Total Loss: 0.003406 | Recon Loss: 0.002887 | Commit Loss: 0.001038 | Perplexity: 1546.772662
Trainning Epoch:  88%|████████▊ | 289/330 [27:07:04<3:59:55, 351.12s/it]2025-09-27 17:19:40,456 Stage: Train 0.5 | Epoch: 289 | Iter: 439000 | Total Loss: 0.003380 | Recon Loss: 0.002861 | Commit Loss: 0.001037 | Perplexity: 1547.841379
2025-09-27 17:20:26,715 Stage: Train 0.5 | Epoch: 289 | Iter: 439200 | Total Loss: 0.003320 | Recon Loss: 0.002800 | Commit Loss: 0.001039 | Perplexity: 1549.806371
2025-09-27 17:21:12,869 Stage: Train 0.5 | Epoch: 289 | Iter: 439400 | Total Loss: 0.003415 | Recon Loss: 0.002898 | Commit Loss: 0.001034 | Perplexity: 1548.385827
2025-09-27 17:21:59,140 Stage: Train 0.5 | Epoch: 289 | Iter: 439600 | Total Loss: 0.003349 | Recon Loss: 0.002834 | Commit Loss: 0.001032 | Perplexity: 1547.732722
2025-09-27 17:22:45,348 Stage: Train 0.5 | Epoch: 289 | Iter: 439800 | Total Loss: 0.003491 | Recon Loss: 0.002971 | Commit Loss: 0.001039 | Perplexity: 1549.039528
2025-09-27 17:23:31,490 Stage: Train 0.5 | Epoch: 289 | Iter: 440000 | Total Loss: 0.003375 | Recon Loss: 0.002858 | Commit Loss: 0.001034 | Perplexity: 1547.983026
2025-09-27 17:23:31,490 Saving model at iteration 440000
2025-09-27 17:23:31,694 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000
2025-09-27 17:23:31,993 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/model.safetensors
2025-09-27 17:23:32,403 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/optimizer.bin
2025-09-27 17:23:32,403 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/scheduler.bin
2025-09-27 17:23:32,404 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/sampler.bin
2025-09-27 17:23:32,405 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/random_states_0.pkl
2025-09-27 17:24:18,489 Stage: Train 0.5 | Epoch: 289 | Iter: 440200 | Total Loss: 0.003421 | Recon Loss: 0.002904 | Commit Loss: 0.001033 | Perplexity: 1547.041282
2025-09-27 17:25:04,695 Stage: Train 0.5 | Epoch: 289 | Iter: 440400 | Total Loss: 0.003427 | Recon Loss: 0.002907 | Commit Loss: 0.001040 | Perplexity: 1548.813038
Trainning Epoch:  88%|████████▊ | 290/330 [27:12:56<3:54:15, 351.40s/it]2025-09-27 17:25:51,106 Stage: Train 0.5 | Epoch: 290 | Iter: 440600 | Total Loss: 0.003339 | Recon Loss: 0.002824 | Commit Loss: 0.001031 | Perplexity: 1544.044016
2025-09-27 17:26:37,313 Stage: Train 0.5 | Epoch: 290 | Iter: 440800 | Total Loss: 0.003418 | Recon Loss: 0.002898 | Commit Loss: 0.001038 | Perplexity: 1550.963308
2025-09-27 17:27:23,604 Stage: Train 0.5 | Epoch: 290 | Iter: 441000 | Total Loss: 0.003360 | Recon Loss: 0.002839 | Commit Loss: 0.001043 | Perplexity: 1553.388095
2025-09-27 17:28:09,578 Stage: Train 0.5 | Epoch: 290 | Iter: 441200 | Total Loss: 0.003441 | Recon Loss: 0.002924 | Commit Loss: 0.001033 | Perplexity: 1545.492032
2025-09-27 17:28:55,894 Stage: Train 0.5 | Epoch: 290 | Iter: 441400 | Total Loss: 0.003321 | Recon Loss: 0.002804 | Commit Loss: 0.001034 | Perplexity: 1546.168784
2025-09-27 17:29:42,201 Stage: Train 0.5 | Epoch: 290 | Iter: 441600 | Total Loss: 0.003444 | Recon Loss: 0.002927 | Commit Loss: 0.001034 | Perplexity: 1550.269712
2025-09-27 17:30:28,294 Stage: Train 0.5 | Epoch: 290 | Iter: 441800 | Total Loss: 0.003429 | Recon Loss: 0.002914 | Commit Loss: 0.001031 | Perplexity: 1543.573600
2025-09-27 17:31:14,400 Stage: Train 0.5 | Epoch: 290 | Iter: 442000 | Total Loss: 0.003375 | Recon Loss: 0.002856 | Commit Loss: 0.001037 | Perplexity: 1550.812127
Trainning Epoch:  88%|████████▊ | 291/330 [27:18:47<3:48:19, 351.26s/it]2025-09-27 17:32:00,902 Stage: Train 0.5 | Epoch: 291 | Iter: 442200 | Total Loss: 0.003389 | Recon Loss: 0.002874 | Commit Loss: 0.001030 | Perplexity: 1542.693345
2025-09-27 17:32:47,146 Stage: Train 0.5 | Epoch: 291 | Iter: 442400 | Total Loss: 0.003447 | Recon Loss: 0.002930 | Commit Loss: 0.001034 | Perplexity: 1550.003112
2025-09-27 17:33:33,408 Stage: Train 0.5 | Epoch: 291 | Iter: 442600 | Total Loss: 0.003333 | Recon Loss: 0.002818 | Commit Loss: 0.001028 | Perplexity: 1547.288394
2025-09-27 17:34:19,866 Stage: Train 0.5 | Epoch: 291 | Iter: 442800 | Total Loss: 0.003439 | Recon Loss: 0.002923 | Commit Loss: 0.001033 | Perplexity: 1546.385690
2025-09-27 17:35:06,179 Stage: Train 0.5 | Epoch: 291 | Iter: 443000 | Total Loss: 0.003395 | Recon Loss: 0.002880 | Commit Loss: 0.001030 | Perplexity: 1545.796306
2025-09-27 17:35:52,307 Stage: Train 0.5 | Epoch: 291 | Iter: 443200 | Total Loss: 0.003409 | Recon Loss: 0.002889 | Commit Loss: 0.001041 | Perplexity: 1549.760247
2025-09-27 17:36:38,661 Stage: Train 0.5 | Epoch: 291 | Iter: 443400 | Total Loss: 0.003384 | Recon Loss: 0.002864 | Commit Loss: 0.001039 | Perplexity: 1550.364376
Trainning Epoch:  88%|████████▊ | 292/330 [27:24:39<3:42:33, 351.42s/it]2025-09-27 17:37:25,182 Stage: Train 0.5 | Epoch: 292 | Iter: 443600 | Total Loss: 0.003473 | Recon Loss: 0.002954 | Commit Loss: 0.001037 | Perplexity: 1545.257830
2025-09-27 17:38:11,042 Stage: Train 0.5 | Epoch: 292 | Iter: 443800 | Total Loss: 0.003381 | Recon Loss: 0.002864 | Commit Loss: 0.001033 | Perplexity: 1552.694694
2025-09-27 17:38:57,380 Stage: Train 0.5 | Epoch: 292 | Iter: 444000 | Total Loss: 0.003360 | Recon Loss: 0.002844 | Commit Loss: 0.001032 | Perplexity: 1548.607004
2025-09-27 17:39:43,632 Stage: Train 0.5 | Epoch: 292 | Iter: 444200 | Total Loss: 0.003370 | Recon Loss: 0.002853 | Commit Loss: 0.001034 | Perplexity: 1544.657115
2025-09-27 17:40:30,097 Stage: Train 0.5 | Epoch: 292 | Iter: 444400 | Total Loss: 0.003361 | Recon Loss: 0.002844 | Commit Loss: 0.001035 | Perplexity: 1549.223733
2025-09-27 17:41:16,479 Stage: Train 0.5 | Epoch: 292 | Iter: 444600 | Total Loss: 0.003433 | Recon Loss: 0.002917 | Commit Loss: 0.001033 | Perplexity: 1548.031937
2025-09-27 17:42:02,678 Stage: Train 0.5 | Epoch: 292 | Iter: 444800 | Total Loss: 0.003438 | Recon Loss: 0.002922 | Commit Loss: 0.001033 | Perplexity: 1547.060396
2025-09-27 17:42:49,009 Stage: Train 0.5 | Epoch: 292 | Iter: 445000 | Total Loss: 0.003378 | Recon Loss: 0.002862 | Commit Loss: 0.001032 | Perplexity: 1547.884063
Trainning Epoch:  89%|████████▉ | 293/330 [27:30:30<3:36:44, 351.48s/it]2025-09-27 17:43:35,583 Stage: Train 0.5 | Epoch: 293 | Iter: 445200 | Total Loss: 0.003494 | Recon Loss: 0.002975 | Commit Loss: 0.001036 | Perplexity: 1549.300014
2025-09-27 17:44:22,032 Stage: Train 0.5 | Epoch: 293 | Iter: 445400 | Total Loss: 0.003427 | Recon Loss: 0.002911 | Commit Loss: 0.001033 | Perplexity: 1547.930361
2025-09-27 17:45:08,158 Stage: Train 0.5 | Epoch: 293 | Iter: 445600 | Total Loss: 0.003342 | Recon Loss: 0.002825 | Commit Loss: 0.001034 | Perplexity: 1549.916366
2025-09-27 17:45:54,168 Stage: Train 0.5 | Epoch: 293 | Iter: 445800 | Total Loss: 0.003373 | Recon Loss: 0.002858 | Commit Loss: 0.001030 | Perplexity: 1548.410367
2025-09-27 17:46:38,684 Stage: Train 0.5 | Epoch: 293 | Iter: 446000 | Total Loss: 0.003396 | Recon Loss: 0.002878 | Commit Loss: 0.001036 | Perplexity: 1544.196913
2025-09-27 17:47:24,966 Stage: Train 0.5 | Epoch: 293 | Iter: 446200 | Total Loss: 0.003598 | Recon Loss: 0.003080 | Commit Loss: 0.001036 | Perplexity: 1548.036040
2025-09-27 17:48:11,239 Stage: Train 0.5 | Epoch: 293 | Iter: 446400 | Total Loss: 0.003294 | Recon Loss: 0.002779 | Commit Loss: 0.001029 | Perplexity: 1548.731331
Trainning Epoch:  89%|████████▉ | 294/330 [27:36:20<3:30:34, 350.95s/it]2025-09-27 17:48:57,718 Stage: Train 0.5 | Epoch: 294 | Iter: 446600 | Total Loss: 0.003309 | Recon Loss: 0.002794 | Commit Loss: 0.001030 | Perplexity: 1547.689791
2025-09-27 17:49:43,911 Stage: Train 0.5 | Epoch: 294 | Iter: 446800 | Total Loss: 0.003368 | Recon Loss: 0.002852 | Commit Loss: 0.001031 | Perplexity: 1549.537107
2025-09-27 17:50:30,052 Stage: Train 0.5 | Epoch: 294 | Iter: 447000 | Total Loss: 0.003433 | Recon Loss: 0.002921 | Commit Loss: 0.001024 | Perplexity: 1547.542220
2025-09-27 17:51:16,250 Stage: Train 0.5 | Epoch: 294 | Iter: 447200 | Total Loss: 0.003392 | Recon Loss: 0.002873 | Commit Loss: 0.001039 | Perplexity: 1551.421827
2025-09-27 17:52:02,470 Stage: Train 0.5 | Epoch: 294 | Iter: 447400 | Total Loss: 0.003321 | Recon Loss: 0.002807 | Commit Loss: 0.001027 | Perplexity: 1543.934325
2025-09-27 17:52:48,258 Stage: Train 0.5 | Epoch: 294 | Iter: 447600 | Total Loss: 0.003345 | Recon Loss: 0.002829 | Commit Loss: 0.001033 | Perplexity: 1548.507092
2025-09-27 17:53:34,363 Stage: Train 0.5 | Epoch: 294 | Iter: 447800 | Total Loss: 0.003498 | Recon Loss: 0.002981 | Commit Loss: 0.001035 | Perplexity: 1546.425221
2025-09-27 17:54:20,581 Stage: Train 0.5 | Epoch: 294 | Iter: 448000 | Total Loss: 0.003312 | Recon Loss: 0.002793 | Commit Loss: 0.001037 | Perplexity: 1548.424359
Trainning Epoch:  89%|████████▉ | 295/330 [27:42:11<3:24:39, 350.84s/it]2025-09-27 17:55:07,074 Stage: Train 0.5 | Epoch: 295 | Iter: 448200 | Total Loss: 0.003349 | Recon Loss: 0.002833 | Commit Loss: 0.001033 | Perplexity: 1549.708107
2025-09-27 17:55:53,307 Stage: Train 0.5 | Epoch: 295 | Iter: 448400 | Total Loss: 0.003366 | Recon Loss: 0.002851 | Commit Loss: 0.001029 | Perplexity: 1548.693887
2025-09-27 17:56:39,551 Stage: Train 0.5 | Epoch: 295 | Iter: 448600 | Total Loss: 0.003390 | Recon Loss: 0.002875 | Commit Loss: 0.001031 | Perplexity: 1542.258599
2025-09-27 17:57:25,800 Stage: Train 0.5 | Epoch: 295 | Iter: 448800 | Total Loss: 0.003435 | Recon Loss: 0.002919 | Commit Loss: 0.001030 | Perplexity: 1545.878960
2025-09-27 17:58:12,044 Stage: Train 0.5 | Epoch: 295 | Iter: 449000 | Total Loss: 0.003332 | Recon Loss: 0.002815 | Commit Loss: 0.001034 | Perplexity: 1552.887604
2025-09-27 17:58:58,132 Stage: Train 0.5 | Epoch: 295 | Iter: 449200 | Total Loss: 0.003389 | Recon Loss: 0.002872 | Commit Loss: 0.001034 | Perplexity: 1548.779489
2025-09-27 17:59:44,083 Stage: Train 0.5 | Epoch: 295 | Iter: 449400 | Total Loss: 0.003361 | Recon Loss: 0.002843 | Commit Loss: 0.001036 | Perplexity: 1551.410921
2025-09-27 18:00:30,382 Stage: Train 0.5 | Epoch: 295 | Iter: 449600 | Total Loss: 0.003378 | Recon Loss: 0.002858 | Commit Loss: 0.001040 | Perplexity: 1550.548439
Trainning Epoch:  90%|████████▉ | 296/330 [27:48:02<3:18:51, 350.92s/it]2025-09-27 18:01:16,882 Stage: Train 0.5 | Epoch: 296 | Iter: 449800 | Total Loss: 0.003432 | Recon Loss: 0.002918 | Commit Loss: 0.001029 | Perplexity: 1549.548073
2025-09-27 18:02:02,968 Stage: Train 0.5 | Epoch: 296 | Iter: 450000 | Total Loss: 0.003376 | Recon Loss: 0.002859 | Commit Loss: 0.001034 | Perplexity: 1551.960823
2025-09-27 18:02:49,139 Stage: Train 0.5 | Epoch: 296 | Iter: 450200 | Total Loss: 0.003386 | Recon Loss: 0.002870 | Commit Loss: 0.001032 | Perplexity: 1549.715974
2025-09-27 18:03:35,374 Stage: Train 0.5 | Epoch: 296 | Iter: 450400 | Total Loss: 0.003404 | Recon Loss: 0.002889 | Commit Loss: 0.001030 | Perplexity: 1548.122125
2025-09-27 18:04:21,569 Stage: Train 0.5 | Epoch: 296 | Iter: 450600 | Total Loss: 0.003307 | Recon Loss: 0.002790 | Commit Loss: 0.001033 | Perplexity: 1548.521194
2025-09-27 18:05:07,730 Stage: Train 0.5 | Epoch: 296 | Iter: 450800 | Total Loss: 0.003375 | Recon Loss: 0.002857 | Commit Loss: 0.001037 | Perplexity: 1549.814553
2025-09-27 18:05:53,891 Stage: Train 0.5 | Epoch: 296 | Iter: 451000 | Total Loss: 0.003344 | Recon Loss: 0.002828 | Commit Loss: 0.001031 | Perplexity: 1546.622103
Trainning Epoch:  90%|█████████ | 297/330 [27:53:53<3:13:01, 350.97s/it]2025-09-27 18:06:40,145 Stage: Train 0.5 | Epoch: 297 | Iter: 451200 | Total Loss: 0.003408 | Recon Loss: 0.002894 | Commit Loss: 0.001030 | Perplexity: 1547.446972
2025-09-27 18:07:26,269 Stage: Train 0.5 | Epoch: 297 | Iter: 451400 | Total Loss: 0.003413 | Recon Loss: 0.002900 | Commit Loss: 0.001026 | Perplexity: 1547.166525
2025-09-27 18:08:12,539 Stage: Train 0.5 | Epoch: 297 | Iter: 451600 | Total Loss: 0.003398 | Recon Loss: 0.002884 | Commit Loss: 0.001029 | Perplexity: 1549.207721
2025-09-27 18:08:58,800 Stage: Train 0.5 | Epoch: 297 | Iter: 451800 | Total Loss: 0.003399 | Recon Loss: 0.002883 | Commit Loss: 0.001031 | Perplexity: 1545.976846
2025-09-27 18:09:45,068 Stage: Train 0.5 | Epoch: 297 | Iter: 452000 | Total Loss: 0.003418 | Recon Loss: 0.002904 | Commit Loss: 0.001027 | Perplexity: 1546.463248
2025-09-27 18:10:31,307 Stage: Train 0.5 | Epoch: 297 | Iter: 452200 | Total Loss: 0.003377 | Recon Loss: 0.002862 | Commit Loss: 0.001030 | Perplexity: 1550.648842
2025-09-27 18:11:17,496 Stage: Train 0.5 | Epoch: 297 | Iter: 452400 | Total Loss: 0.003350 | Recon Loss: 0.002836 | Commit Loss: 0.001028 | Perplexity: 1546.018689
2025-09-27 18:12:03,585 Stage: Train 0.5 | Epoch: 297 | Iter: 452600 | Total Loss: 0.003355 | Recon Loss: 0.002839 | Commit Loss: 0.001033 | Perplexity: 1549.409417
Trainning Epoch:  90%|█████████ | 298/330 [27:59:44<3:07:10, 350.95s/it]2025-09-27 18:12:50,084 Stage: Train 0.5 | Epoch: 298 | Iter: 452800 | Total Loss: 0.003426 | Recon Loss: 0.002914 | Commit Loss: 0.001024 | Perplexity: 1543.549420
2025-09-27 18:13:36,414 Stage: Train 0.5 | Epoch: 298 | Iter: 453000 | Total Loss: 0.003335 | Recon Loss: 0.002819 | Commit Loss: 0.001031 | Perplexity: 1552.047384
2025-09-27 18:14:22,288 Stage: Train 0.5 | Epoch: 298 | Iter: 453200 | Total Loss: 0.003360 | Recon Loss: 0.002846 | Commit Loss: 0.001028 | Perplexity: 1549.435408
2025-09-27 18:15:08,668 Stage: Train 0.5 | Epoch: 298 | Iter: 453400 | Total Loss: 0.003388 | Recon Loss: 0.002870 | Commit Loss: 0.001036 | Perplexity: 1553.657202
2025-09-27 18:15:55,015 Stage: Train 0.5 | Epoch: 298 | Iter: 453600 | Total Loss: 0.003332 | Recon Loss: 0.002815 | Commit Loss: 0.001033 | Perplexity: 1553.154443
2025-09-27 18:16:41,391 Stage: Train 0.5 | Epoch: 298 | Iter: 453800 | Total Loss: 0.003352 | Recon Loss: 0.002838 | Commit Loss: 0.001029 | Perplexity: 1548.399056
2025-09-27 18:17:27,680 Stage: Train 0.5 | Epoch: 298 | Iter: 454000 | Total Loss: 0.003354 | Recon Loss: 0.002840 | Commit Loss: 0.001029 | Perplexity: 1547.626279
Trainning Epoch:  91%|█████████ | 299/330 [28:05:35<3:01:25, 351.16s/it]2025-09-27 18:18:14,197 Stage: Train 0.5 | Epoch: 299 | Iter: 454200 | Total Loss: 0.003355 | Recon Loss: 0.002837 | Commit Loss: 0.001037 | Perplexity: 1555.790471
2025-09-27 18:19:00,346 Stage: Train 0.5 | Epoch: 299 | Iter: 454400 | Total Loss: 0.003390 | Recon Loss: 0.002876 | Commit Loss: 0.001027 | Perplexity: 1548.943617
2025-09-27 18:19:46,554 Stage: Train 0.5 | Epoch: 299 | Iter: 454600 | Total Loss: 0.003350 | Recon Loss: 0.002837 | Commit Loss: 0.001026 | Perplexity: 1549.835695
2025-09-27 18:20:32,726 Stage: Train 0.5 | Epoch: 299 | Iter: 454800 | Total Loss: 0.003388 | Recon Loss: 0.002871 | Commit Loss: 0.001033 | Perplexity: 1554.491884
2025-09-27 18:21:18,736 Stage: Train 0.5 | Epoch: 299 | Iter: 455000 | Total Loss: 0.003333 | Recon Loss: 0.002817 | Commit Loss: 0.001030 | Perplexity: 1549.371669
2025-09-27 18:22:04,913 Stage: Train 0.5 | Epoch: 299 | Iter: 455200 | Total Loss: 0.003384 | Recon Loss: 0.002868 | Commit Loss: 0.001032 | Perplexity: 1547.740613
2025-09-27 18:22:51,115 Stage: Train 0.5 | Epoch: 299 | Iter: 455400 | Total Loss: 0.003371 | Recon Loss: 0.002859 | Commit Loss: 0.001025 | Perplexity: 1547.111193
2025-09-27 18:23:37,368 Stage: Train 0.5 | Epoch: 299 | Iter: 455600 | Total Loss: 0.003340 | Recon Loss: 0.002829 | Commit Loss: 0.001022 | Perplexity: 1544.920287
Trainning Epoch:  91%|█████████ | 300/330 [28:11:26<2:55:32, 351.10s/it]2025-09-27 18:24:23,817 Stage: Train 0.5 | Epoch: 300 | Iter: 455800 | Total Loss: 0.003388 | Recon Loss: 0.002872 | Commit Loss: 0.001033 | Perplexity: 1551.303875
2025-09-27 18:25:10,011 Stage: Train 0.5 | Epoch: 300 | Iter: 456000 | Total Loss: 0.003363 | Recon Loss: 0.002849 | Commit Loss: 0.001027 | Perplexity: 1550.782159
2025-09-27 18:25:56,148 Stage: Train 0.5 | Epoch: 300 | Iter: 456200 | Total Loss: 0.003300 | Recon Loss: 0.002786 | Commit Loss: 0.001027 | Perplexity: 1550.545019
2025-09-27 18:26:42,518 Stage: Train 0.5 | Epoch: 300 | Iter: 456400 | Total Loss: 0.003393 | Recon Loss: 0.002879 | Commit Loss: 0.001027 | Perplexity: 1552.257384
2025-09-27 18:27:28,725 Stage: Train 0.5 | Epoch: 300 | Iter: 456600 | Total Loss: 0.003413 | Recon Loss: 0.002899 | Commit Loss: 0.001030 | Perplexity: 1547.876617
2025-09-27 18:28:14,595 Stage: Train 0.5 | Epoch: 300 | Iter: 456800 | Total Loss: 0.003360 | Recon Loss: 0.002844 | Commit Loss: 0.001032 | Perplexity: 1551.134588
2025-09-27 18:29:00,854 Stage: Train 0.5 | Epoch: 300 | Iter: 457000 | Total Loss: 0.003357 | Recon Loss: 0.002842 | Commit Loss: 0.001029 | Perplexity: 1552.678011
2025-09-27 18:29:47,217 Stage: Train 0.5 | Epoch: 300 | Iter: 457200 | Total Loss: 0.003327 | Recon Loss: 0.002810 | Commit Loss: 0.001034 | Perplexity: 1551.470962
Trainning Epoch:  91%|█████████ | 301/330 [28:17:17<2:49:42, 351.12s/it]2025-09-27 18:30:33,943 Stage: Train 0.5 | Epoch: 301 | Iter: 457400 | Total Loss: 0.003388 | Recon Loss: 0.002873 | Commit Loss: 0.001029 | Perplexity: 1552.745924
2025-09-27 18:31:20,108 Stage: Train 0.5 | Epoch: 301 | Iter: 457600 | Total Loss: 0.003347 | Recon Loss: 0.002831 | Commit Loss: 0.001033 | Perplexity: 1547.865223
2025-09-27 18:32:06,213 Stage: Train 0.5 | Epoch: 301 | Iter: 457800 | Total Loss: 0.003348 | Recon Loss: 0.002834 | Commit Loss: 0.001028 | Perplexity: 1552.815963
2025-09-27 18:32:52,557 Stage: Train 0.5 | Epoch: 301 | Iter: 458000 | Total Loss: 0.003442 | Recon Loss: 0.002929 | Commit Loss: 0.001027 | Perplexity: 1549.157039
2025-09-27 18:33:38,902 Stage: Train 0.5 | Epoch: 301 | Iter: 458200 | Total Loss: 0.003284 | Recon Loss: 0.002770 | Commit Loss: 0.001028 | Perplexity: 1551.049319
2025-09-27 18:34:25,112 Stage: Train 0.5 | Epoch: 301 | Iter: 458400 | Total Loss: 0.003341 | Recon Loss: 0.002829 | Commit Loss: 0.001024 | Perplexity: 1550.563744
2025-09-27 18:35:10,924 Stage: Train 0.5 | Epoch: 301 | Iter: 458600 | Total Loss: 0.003425 | Recon Loss: 0.002911 | Commit Loss: 0.001029 | Perplexity: 1549.271643
Trainning Epoch:  92%|█████████▏| 302/330 [28:23:08<2:43:50, 351.11s/it]2025-09-27 18:35:57,300 Stage: Train 0.5 | Epoch: 302 | Iter: 458800 | Total Loss: 0.003294 | Recon Loss: 0.002779 | Commit Loss: 0.001029 | Perplexity: 1548.630743
2025-09-27 18:36:43,513 Stage: Train 0.5 | Epoch: 302 | Iter: 459000 | Total Loss: 0.003373 | Recon Loss: 0.002862 | Commit Loss: 0.001022 | Perplexity: 1548.213204
2025-09-27 18:37:29,589 Stage: Train 0.5 | Epoch: 302 | Iter: 459200 | Total Loss: 0.003554 | Recon Loss: 0.003042 | Commit Loss: 0.001024 | Perplexity: 1551.418300
2025-09-27 18:38:15,840 Stage: Train 0.5 | Epoch: 302 | Iter: 459400 | Total Loss: 0.003303 | Recon Loss: 0.002789 | Commit Loss: 0.001029 | Perplexity: 1550.965283
2025-09-27 18:39:01,909 Stage: Train 0.5 | Epoch: 302 | Iter: 459600 | Total Loss: 0.003371 | Recon Loss: 0.002854 | Commit Loss: 0.001034 | Perplexity: 1552.716353
2025-09-27 18:39:47,935 Stage: Train 0.5 | Epoch: 302 | Iter: 459800 | Total Loss: 0.003415 | Recon Loss: 0.002902 | Commit Loss: 0.001025 | Perplexity: 1549.104365
2025-09-27 18:40:34,220 Stage: Train 0.5 | Epoch: 302 | Iter: 460000 | Total Loss: 0.003369 | Recon Loss: 0.002856 | Commit Loss: 0.001025 | Perplexity: 1552.010670
2025-09-27 18:40:34,220 Saving model at iteration 460000
2025-09-27 18:40:34,437 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000
2025-09-27 18:40:34,773 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/model.safetensors
2025-09-27 18:40:35,199 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/optimizer.bin
2025-09-27 18:40:35,199 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/scheduler.bin
2025-09-27 18:40:35,199 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/sampler.bin
2025-09-27 18:40:35,200 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/random_states_0.pkl
2025-09-27 18:41:21,479 Stage: Train 0.5 | Epoch: 302 | Iter: 460200 | Total Loss: 0.003343 | Recon Loss: 0.002828 | Commit Loss: 0.001031 | Perplexity: 1553.077109
Trainning Epoch:  92%|█████████▏| 303/330 [28:29:00<2:38:06, 351.35s/it]2025-09-27 18:42:07,803 Stage: Train 0.5 | Epoch: 303 | Iter: 460400 | Total Loss: 0.003328 | Recon Loss: 0.002813 | Commit Loss: 0.001031 | Perplexity: 1549.919684
2025-09-27 18:42:53,610 Stage: Train 0.5 | Epoch: 303 | Iter: 460600 | Total Loss: 0.003313 | Recon Loss: 0.002800 | Commit Loss: 0.001026 | Perplexity: 1554.092738
2025-09-27 18:43:39,764 Stage: Train 0.5 | Epoch: 303 | Iter: 460800 | Total Loss: 0.003403 | Recon Loss: 0.002890 | Commit Loss: 0.001025 | Perplexity: 1550.440889
2025-09-27 18:44:25,866 Stage: Train 0.5 | Epoch: 303 | Iter: 461000 | Total Loss: 0.003323 | Recon Loss: 0.002809 | Commit Loss: 0.001028 | Perplexity: 1553.452379
2025-09-27 18:45:12,069 Stage: Train 0.5 | Epoch: 303 | Iter: 461200 | Total Loss: 0.003392 | Recon Loss: 0.002876 | Commit Loss: 0.001034 | Perplexity: 1553.836486
2025-09-27 18:45:58,450 Stage: Train 0.5 | Epoch: 303 | Iter: 461400 | Total Loss: 0.003370 | Recon Loss: 0.002856 | Commit Loss: 0.001028 | Perplexity: 1545.641376
2025-09-27 18:46:44,735 Stage: Train 0.5 | Epoch: 303 | Iter: 461600 | Total Loss: 0.003324 | Recon Loss: 0.002811 | Commit Loss: 0.001025 | Perplexity: 1553.194909
Trainning Epoch:  92%|█████████▏| 304/330 [28:34:51<2:32:10, 351.18s/it]2025-09-27 18:47:31,256 Stage: Train 0.5 | Epoch: 304 | Iter: 461800 | Total Loss: 0.003348 | Recon Loss: 0.002833 | Commit Loss: 0.001031 | Perplexity: 1546.573536
2025-09-27 18:48:17,476 Stage: Train 0.5 | Epoch: 304 | Iter: 462000 | Total Loss: 0.003360 | Recon Loss: 0.002847 | Commit Loss: 0.001025 | Perplexity: 1548.263500
2025-09-27 18:49:03,775 Stage: Train 0.5 | Epoch: 304 | Iter: 462200 | Total Loss: 0.003405 | Recon Loss: 0.002894 | Commit Loss: 0.001022 | Perplexity: 1549.789803
2025-09-27 18:49:49,747 Stage: Train 0.5 | Epoch: 304 | Iter: 462400 | Total Loss: 0.003262 | Recon Loss: 0.002751 | Commit Loss: 0.001023 | Perplexity: 1550.231522
2025-09-27 18:50:35,956 Stage: Train 0.5 | Epoch: 304 | Iter: 462600 | Total Loss: 0.003362 | Recon Loss: 0.002847 | Commit Loss: 0.001030 | Perplexity: 1550.971990
2025-09-27 18:51:22,137 Stage: Train 0.5 | Epoch: 304 | Iter: 462800 | Total Loss: 0.003345 | Recon Loss: 0.002830 | Commit Loss: 0.001029 | Perplexity: 1554.827477
2025-09-27 18:52:08,289 Stage: Train 0.5 | Epoch: 304 | Iter: 463000 | Total Loss: 0.003353 | Recon Loss: 0.002836 | Commit Loss: 0.001035 | Perplexity: 1550.335425
2025-09-27 18:52:54,530 Stage: Train 0.5 | Epoch: 304 | Iter: 463200 | Total Loss: 0.003360 | Recon Loss: 0.002846 | Commit Loss: 0.001028 | Perplexity: 1547.966922
Trainning Epoch:  92%|█████████▏| 305/330 [28:40:42<2:26:18, 351.12s/it]2025-09-27 18:53:40,983 Stage: Train 0.5 | Epoch: 305 | Iter: 463400 | Total Loss: 0.003351 | Recon Loss: 0.002841 | Commit Loss: 0.001019 | Perplexity: 1547.314026
2025-09-27 18:54:27,204 Stage: Train 0.5 | Epoch: 305 | Iter: 463600 | Total Loss: 0.003285 | Recon Loss: 0.002773 | Commit Loss: 0.001022 | Perplexity: 1548.864861
2025-09-27 18:55:13,491 Stage: Train 0.5 | Epoch: 305 | Iter: 463800 | Total Loss: 0.003320 | Recon Loss: 0.002813 | Commit Loss: 0.001015 | Perplexity: 1547.390862
2025-09-27 18:55:59,627 Stage: Train 0.5 | Epoch: 305 | Iter: 464000 | Total Loss: 0.003473 | Recon Loss: 0.002958 | Commit Loss: 0.001029 | Perplexity: 1549.787002
2025-09-27 18:56:45,428 Stage: Train 0.5 | Epoch: 305 | Iter: 464200 | Total Loss: 0.003343 | Recon Loss: 0.002827 | Commit Loss: 0.001032 | Perplexity: 1556.126548
2025-09-27 18:57:31,616 Stage: Train 0.5 | Epoch: 305 | Iter: 464400 | Total Loss: 0.003385 | Recon Loss: 0.002872 | Commit Loss: 0.001024 | Perplexity: 1549.178215
2025-09-27 18:58:17,856 Stage: Train 0.5 | Epoch: 305 | Iter: 464600 | Total Loss: 0.003330 | Recon Loss: 0.002817 | Commit Loss: 0.001025 | Perplexity: 1549.679819
2025-09-27 18:59:04,021 Stage: Train 0.5 | Epoch: 305 | Iter: 464800 | Total Loss: 0.003341 | Recon Loss: 0.002826 | Commit Loss: 0.001031 | Perplexity: 1554.685723
Trainning Epoch:  93%|█████████▎| 306/330 [28:46:33<2:20:24, 351.03s/it]2025-09-27 18:59:50,480 Stage: Train 0.5 | Epoch: 306 | Iter: 465000 | Total Loss: 0.003428 | Recon Loss: 0.002916 | Commit Loss: 0.001025 | Perplexity: 1548.569966
2025-09-27 19:00:36,671 Stage: Train 0.5 | Epoch: 306 | Iter: 465200 | Total Loss: 0.003339 | Recon Loss: 0.002826 | Commit Loss: 0.001026 | Perplexity: 1554.913286
2025-09-27 19:01:22,825 Stage: Train 0.5 | Epoch: 306 | Iter: 465400 | Total Loss: 0.003451 | Recon Loss: 0.002940 | Commit Loss: 0.001022 | Perplexity: 1545.295031
2025-09-27 19:02:08,946 Stage: Train 0.5 | Epoch: 306 | Iter: 465600 | Total Loss: 0.003280 | Recon Loss: 0.002768 | Commit Loss: 0.001025 | Perplexity: 1547.793215
2025-09-27 19:02:55,063 Stage: Train 0.5 | Epoch: 306 | Iter: 465800 | Total Loss: 0.003338 | Recon Loss: 0.002824 | Commit Loss: 0.001029 | Perplexity: 1554.847013
2025-09-27 19:03:41,006 Stage: Train 0.5 | Epoch: 306 | Iter: 466000 | Total Loss: 0.003297 | Recon Loss: 0.002785 | Commit Loss: 0.001025 | Perplexity: 1552.292845
2025-09-27 19:04:27,407 Stage: Train 0.5 | Epoch: 306 | Iter: 466200 | Total Loss: 0.003470 | Recon Loss: 0.002955 | Commit Loss: 0.001030 | Perplexity: 1551.255327
Trainning Epoch:  93%|█████████▎| 307/330 [28:52:24<2:14:33, 351.01s/it]2025-09-27 19:05:14,069 Stage: Train 0.5 | Epoch: 307 | Iter: 466400 | Total Loss: 0.003330 | Recon Loss: 0.002822 | Commit Loss: 0.001015 | Perplexity: 1543.989852
2025-09-27 19:06:00,265 Stage: Train 0.5 | Epoch: 307 | Iter: 466600 | Total Loss: 0.003299 | Recon Loss: 0.002787 | Commit Loss: 0.001024 | Perplexity: 1552.650248
2025-09-27 19:06:46,428 Stage: Train 0.5 | Epoch: 307 | Iter: 466800 | Total Loss: 0.003340 | Recon Loss: 0.002827 | Commit Loss: 0.001026 | Perplexity: 1555.048329
2025-09-27 19:07:32,647 Stage: Train 0.5 | Epoch: 307 | Iter: 467000 | Total Loss: 0.003319 | Recon Loss: 0.002803 | Commit Loss: 0.001032 | Perplexity: 1555.522391
2025-09-27 19:08:18,804 Stage: Train 0.5 | Epoch: 307 | Iter: 467200 | Total Loss: 0.003355 | Recon Loss: 0.002842 | Commit Loss: 0.001026 | Perplexity: 1550.732967
2025-09-27 19:09:05,029 Stage: Train 0.5 | Epoch: 307 | Iter: 467400 | Total Loss: 0.003375 | Recon Loss: 0.002861 | Commit Loss: 0.001027 | Perplexity: 1553.411540
2025-09-27 19:09:51,231 Stage: Train 0.5 | Epoch: 307 | Iter: 467600 | Total Loss: 0.003319 | Recon Loss: 0.002807 | Commit Loss: 0.001025 | Perplexity: 1547.370342
2025-09-27 19:10:37,421 Stage: Train 0.5 | Epoch: 307 | Iter: 467800 | Total Loss: 0.003341 | Recon Loss: 0.002828 | Commit Loss: 0.001026 | Perplexity: 1549.091398
Trainning Epoch:  93%|█████████▎| 308/330 [28:58:15<2:08:41, 350.96s/it]2025-09-27 19:11:23,577 Stage: Train 0.5 | Epoch: 308 | Iter: 468000 | Total Loss: 0.003321 | Recon Loss: 0.002811 | Commit Loss: 0.001020 | Perplexity: 1549.376488
2025-09-27 19:12:09,799 Stage: Train 0.5 | Epoch: 308 | Iter: 468200 | Total Loss: 0.003495 | Recon Loss: 0.002982 | Commit Loss: 0.001026 | Perplexity: 1550.849948
2025-09-27 19:12:55,923 Stage: Train 0.5 | Epoch: 308 | Iter: 468400 | Total Loss: 0.003278 | Recon Loss: 0.002766 | Commit Loss: 0.001024 | Perplexity: 1551.534733
2025-09-27 19:13:41,932 Stage: Train 0.5 | Epoch: 308 | Iter: 468600 | Total Loss: 0.003339 | Recon Loss: 0.002827 | Commit Loss: 0.001024 | Perplexity: 1551.358774
2025-09-27 19:14:28,173 Stage: Train 0.5 | Epoch: 308 | Iter: 468800 | Total Loss: 0.003343 | Recon Loss: 0.002830 | Commit Loss: 0.001025 | Perplexity: 1551.610995
2025-09-27 19:15:14,531 Stage: Train 0.5 | Epoch: 308 | Iter: 469000 | Total Loss: 0.003382 | Recon Loss: 0.002871 | Commit Loss: 0.001022 | Perplexity: 1554.468571
2025-09-27 19:16:00,571 Stage: Train 0.5 | Epoch: 308 | Iter: 469200 | Total Loss: 0.003326 | Recon Loss: 0.002815 | Commit Loss: 0.001022 | Perplexity: 1550.398362
Trainning Epoch:  94%|█████████▎| 309/330 [29:04:06<2:02:50, 350.98s/it]2025-09-27 19:16:47,074 Stage: Train 0.5 | Epoch: 309 | Iter: 469400 | Total Loss: 0.003392 | Recon Loss: 0.002883 | Commit Loss: 0.001019 | Perplexity: 1551.927883
2025-09-27 19:17:33,431 Stage: Train 0.5 | Epoch: 309 | Iter: 469600 | Total Loss: 0.003394 | Recon Loss: 0.002884 | Commit Loss: 0.001020 | Perplexity: 1548.656373
2025-09-27 19:18:19,243 Stage: Train 0.5 | Epoch: 309 | Iter: 469800 | Total Loss: 0.003281 | Recon Loss: 0.002770 | Commit Loss: 0.001022 | Perplexity: 1551.750001
2025-09-27 19:19:05,443 Stage: Train 0.5 | Epoch: 309 | Iter: 470000 | Total Loss: 0.003325 | Recon Loss: 0.002814 | Commit Loss: 0.001022 | Perplexity: 1551.380439
2025-09-27 19:19:51,769 Stage: Train 0.5 | Epoch: 309 | Iter: 470200 | Total Loss: 0.003341 | Recon Loss: 0.002830 | Commit Loss: 0.001023 | Perplexity: 1552.004741
2025-09-27 19:20:36,086 Stage: Train 0.5 | Epoch: 309 | Iter: 470400 | Total Loss: 0.003418 | Recon Loss: 0.002903 | Commit Loss: 0.001030 | Perplexity: 1552.678681
2025-09-27 19:21:22,297 Stage: Train 0.5 | Epoch: 309 | Iter: 470600 | Total Loss: 0.003290 | Recon Loss: 0.002774 | Commit Loss: 0.001032 | Perplexity: 1556.277830
2025-09-27 19:22:08,466 Stage: Train 0.5 | Epoch: 309 | Iter: 470800 | Total Loss: 0.003429 | Recon Loss: 0.002920 | Commit Loss: 0.001017 | Perplexity: 1547.648065
Trainning Epoch:  94%|█████████▍| 310/330 [29:09:55<1:56:48, 350.44s/it]2025-09-27 19:22:54,921 Stage: Train 0.5 | Epoch: 310 | Iter: 471000 | Total Loss: 0.003312 | Recon Loss: 0.002802 | Commit Loss: 0.001021 | Perplexity: 1552.380962
2025-09-27 19:23:40,981 Stage: Train 0.5 | Epoch: 310 | Iter: 471200 | Total Loss: 0.003405 | Recon Loss: 0.002894 | Commit Loss: 0.001023 | Perplexity: 1550.720796
2025-09-27 19:24:27,142 Stage: Train 0.5 | Epoch: 310 | Iter: 471400 | Total Loss: 0.003393 | Recon Loss: 0.002882 | Commit Loss: 0.001021 | Perplexity: 1551.735605
2025-09-27 19:25:13,036 Stage: Train 0.5 | Epoch: 310 | Iter: 471600 | Total Loss: 0.003321 | Recon Loss: 0.002811 | Commit Loss: 0.001019 | Perplexity: 1549.667803
2025-09-27 19:25:59,311 Stage: Train 0.5 | Epoch: 310 | Iter: 471800 | Total Loss: 0.003365 | Recon Loss: 0.002850 | Commit Loss: 0.001029 | Perplexity: 1557.518133
2025-09-27 19:26:45,436 Stage: Train 0.5 | Epoch: 310 | Iter: 472000 | Total Loss: 0.003285 | Recon Loss: 0.002777 | Commit Loss: 0.001017 | Perplexity: 1547.333760
2025-09-27 19:27:31,541 Stage: Train 0.5 | Epoch: 310 | Iter: 472200 | Total Loss: 0.003347 | Recon Loss: 0.002835 | Commit Loss: 0.001023 | Perplexity: 1551.966729
2025-09-27 19:28:17,901 Stage: Train 0.5 | Epoch: 310 | Iter: 472400 | Total Loss: 0.003351 | Recon Loss: 0.002839 | Commit Loss: 0.001023 | Perplexity: 1552.686403
Trainning Epoch:  94%|█████████▍| 311/330 [29:15:46<1:50:59, 350.52s/it]2025-09-27 19:29:04,432 Stage: Train 0.5 | Epoch: 311 | Iter: 472600 | Total Loss: 0.003378 | Recon Loss: 0.002864 | Commit Loss: 0.001028 | Perplexity: 1555.818262
2025-09-27 19:29:50,623 Stage: Train 0.5 | Epoch: 311 | Iter: 472800 | Total Loss: 0.003345 | Recon Loss: 0.002837 | Commit Loss: 0.001018 | Perplexity: 1552.311146
2025-09-27 19:30:36,882 Stage: Train 0.5 | Epoch: 311 | Iter: 473000 | Total Loss: 0.003282 | Recon Loss: 0.002768 | Commit Loss: 0.001028 | Perplexity: 1556.194355
2025-09-27 19:31:22,947 Stage: Train 0.5 | Epoch: 311 | Iter: 473200 | Total Loss: 0.003385 | Recon Loss: 0.002874 | Commit Loss: 0.001023 | Perplexity: 1550.080415
2025-09-27 19:32:08,882 Stage: Train 0.5 | Epoch: 311 | Iter: 473400 | Total Loss: 0.003381 | Recon Loss: 0.002871 | Commit Loss: 0.001021 | Perplexity: 1552.055424
2025-09-27 19:32:55,146 Stage: Train 0.5 | Epoch: 311 | Iter: 473600 | Total Loss: 0.003318 | Recon Loss: 0.002807 | Commit Loss: 0.001022 | Perplexity: 1548.290797
2025-09-27 19:33:41,325 Stage: Train 0.5 | Epoch: 311 | Iter: 473800 | Total Loss: 0.003351 | Recon Loss: 0.002844 | Commit Loss: 0.001014 | Perplexity: 1549.544409
Trainning Epoch:  95%|█████████▍| 312/330 [29:21:36<1:45:11, 350.61s/it]2025-09-27 19:34:27,693 Stage: Train 0.5 | Epoch: 312 | Iter: 474000 | Total Loss: 0.003285 | Recon Loss: 0.002774 | Commit Loss: 0.001022 | Perplexity: 1555.032042
2025-09-27 19:35:13,938 Stage: Train 0.5 | Epoch: 312 | Iter: 474200 | Total Loss: 0.003295 | Recon Loss: 0.002788 | Commit Loss: 0.001014 | Perplexity: 1551.054662
2025-09-27 19:36:00,091 Stage: Train 0.5 | Epoch: 312 | Iter: 474400 | Total Loss: 0.003341 | Recon Loss: 0.002829 | Commit Loss: 0.001024 | Perplexity: 1560.266781
2025-09-27 19:36:46,308 Stage: Train 0.5 | Epoch: 312 | Iter: 474600 | Total Loss: 0.003346 | Recon Loss: 0.002839 | Commit Loss: 0.001014 | Perplexity: 1546.893765
2025-09-27 19:37:32,433 Stage: Train 0.5 | Epoch: 312 | Iter: 474800 | Total Loss: 0.003364 | Recon Loss: 0.002853 | Commit Loss: 0.001023 | Perplexity: 1553.498434
2025-09-27 19:38:18,670 Stage: Train 0.5 | Epoch: 312 | Iter: 475000 | Total Loss: 0.003295 | Recon Loss: 0.002785 | Commit Loss: 0.001021 | Perplexity: 1551.644326
2025-09-27 19:39:04,930 Stage: Train 0.5 | Epoch: 312 | Iter: 475200 | Total Loss: 0.003361 | Recon Loss: 0.002849 | Commit Loss: 0.001024 | Perplexity: 1554.421337
2025-09-27 19:39:50,831 Stage: Train 0.5 | Epoch: 312 | Iter: 475400 | Total Loss: 0.003324 | Recon Loss: 0.002811 | Commit Loss: 0.001025 | Perplexity: 1554.918406
Trainning Epoch:  95%|█████████▍| 313/330 [29:27:27<1:39:21, 350.68s/it]2025-09-27 19:40:37,262 Stage: Train 0.5 | Epoch: 313 | Iter: 475600 | Total Loss: 0.003273 | Recon Loss: 0.002765 | Commit Loss: 0.001017 | Perplexity: 1550.685546
2025-09-27 19:41:23,515 Stage: Train 0.5 | Epoch: 313 | Iter: 475800 | Total Loss: 0.003315 | Recon Loss: 0.002805 | Commit Loss: 0.001019 | Perplexity: 1556.094233
2025-09-27 19:42:09,917 Stage: Train 0.5 | Epoch: 313 | Iter: 476000 | Total Loss: 0.003356 | Recon Loss: 0.002843 | Commit Loss: 0.001025 | Perplexity: 1554.184161
2025-09-27 19:42:56,258 Stage: Train 0.5 | Epoch: 313 | Iter: 476200 | Total Loss: 0.003313 | Recon Loss: 0.002802 | Commit Loss: 0.001022 | Perplexity: 1553.827798
2025-09-27 19:43:42,388 Stage: Train 0.5 | Epoch: 313 | Iter: 476400 | Total Loss: 0.003291 | Recon Loss: 0.002780 | Commit Loss: 0.001022 | Perplexity: 1554.037921
2025-09-27 19:44:28,690 Stage: Train 0.5 | Epoch: 313 | Iter: 476600 | Total Loss: 0.003355 | Recon Loss: 0.002844 | Commit Loss: 0.001023 | Perplexity: 1551.588494
2025-09-27 19:45:15,058 Stage: Train 0.5 | Epoch: 313 | Iter: 476800 | Total Loss: 0.003328 | Recon Loss: 0.002819 | Commit Loss: 0.001017 | Perplexity: 1551.883506
Trainning Epoch:  95%|█████████▌| 314/330 [29:33:19<1:33:36, 351.06s/it]2025-09-27 19:46:01,775 Stage: Train 0.5 | Epoch: 314 | Iter: 477000 | Total Loss: 0.003273 | Recon Loss: 0.002759 | Commit Loss: 0.001027 | Perplexity: 1558.104302
2025-09-27 19:46:47,790 Stage: Train 0.5 | Epoch: 314 | Iter: 477200 | Total Loss: 0.003306 | Recon Loss: 0.002801 | Commit Loss: 0.001008 | Perplexity: 1553.018917
2025-09-27 19:47:34,060 Stage: Train 0.5 | Epoch: 314 | Iter: 477400 | Total Loss: 0.003339 | Recon Loss: 0.002831 | Commit Loss: 0.001016 | Perplexity: 1554.870167
2025-09-27 19:48:20,400 Stage: Train 0.5 | Epoch: 314 | Iter: 477600 | Total Loss: 0.003293 | Recon Loss: 0.002777 | Commit Loss: 0.001032 | Perplexity: 1554.117202
2025-09-27 19:49:06,714 Stage: Train 0.5 | Epoch: 314 | Iter: 477800 | Total Loss: 0.003292 | Recon Loss: 0.002783 | Commit Loss: 0.001019 | Perplexity: 1552.755728
2025-09-27 19:49:52,955 Stage: Train 0.5 | Epoch: 314 | Iter: 478000 | Total Loss: 0.003400 | Recon Loss: 0.002889 | Commit Loss: 0.001021 | Perplexity: 1550.774349
2025-09-27 19:50:39,448 Stage: Train 0.5 | Epoch: 314 | Iter: 478200 | Total Loss: 0.003288 | Recon Loss: 0.002776 | Commit Loss: 0.001024 | Perplexity: 1556.777449
2025-09-27 19:51:25,823 Stage: Train 0.5 | Epoch: 314 | Iter: 478400 | Total Loss: 0.003367 | Recon Loss: 0.002858 | Commit Loss: 0.001018 | Perplexity: 1553.365764
Trainning Epoch:  95%|█████████▌| 315/330 [29:39:11<1:27:50, 351.33s/it]2025-09-27 19:52:12,565 Stage: Train 0.5 | Epoch: 315 | Iter: 478600 | Total Loss: 0.003311 | Recon Loss: 0.002803 | Commit Loss: 0.001017 | Perplexity: 1552.777592
2025-09-27 19:52:58,882 Stage: Train 0.5 | Epoch: 315 | Iter: 478800 | Total Loss: 0.003346 | Recon Loss: 0.002838 | Commit Loss: 0.001018 | Perplexity: 1553.369471
2025-09-27 19:53:45,114 Stage: Train 0.5 | Epoch: 315 | Iter: 479000 | Total Loss: 0.003291 | Recon Loss: 0.002781 | Commit Loss: 0.001019 | Perplexity: 1556.347869
2025-09-27 19:54:31,564 Stage: Train 0.5 | Epoch: 315 | Iter: 479200 | Total Loss: 0.003327 | Recon Loss: 0.002818 | Commit Loss: 0.001019 | Perplexity: 1552.733892
2025-09-27 19:55:17,875 Stage: Train 0.5 | Epoch: 315 | Iter: 479400 | Total Loss: 0.003296 | Recon Loss: 0.002788 | Commit Loss: 0.001017 | Perplexity: 1550.810982
2025-09-27 19:56:04,352 Stage: Train 0.5 | Epoch: 315 | Iter: 479600 | Total Loss: 0.003245 | Recon Loss: 0.002737 | Commit Loss: 0.001016 | Perplexity: 1555.277452
2025-09-27 19:56:50,715 Stage: Train 0.5 | Epoch: 315 | Iter: 479800 | Total Loss: 0.003363 | Recon Loss: 0.002854 | Commit Loss: 0.001018 | Perplexity: 1555.551210
2025-09-27 19:57:37,125 Stage: Train 0.5 | Epoch: 315 | Iter: 480000 | Total Loss: 0.003308 | Recon Loss: 0.002795 | Commit Loss: 0.001026 | Perplexity: 1555.076779
2025-09-27 19:57:37,126 Saving model at iteration 480000
2025-09-27 19:57:37,331 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000
2025-09-27 19:57:37,612 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/model.safetensors
2025-09-27 19:57:38,007 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/optimizer.bin
2025-09-27 19:57:38,007 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/scheduler.bin
2025-09-27 19:57:38,007 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/sampler.bin
2025-09-27 19:57:38,008 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 316/330 [29:45:05<1:22:07, 351.95s/it]2025-09-27 19:58:24,486 Stage: Train 0.5 | Epoch: 316 | Iter: 480200 | Total Loss: 0.003265 | Recon Loss: 0.002759 | Commit Loss: 0.001011 | Perplexity: 1554.231784
2025-09-27 19:59:10,731 Stage: Train 0.5 | Epoch: 316 | Iter: 480400 | Total Loss: 0.003338 | Recon Loss: 0.002831 | Commit Loss: 0.001012 | Perplexity: 1550.887173
2025-09-27 19:59:56,866 Stage: Train 0.5 | Epoch: 316 | Iter: 480600 | Total Loss: 0.003312 | Recon Loss: 0.002803 | Commit Loss: 0.001017 | Perplexity: 1553.984980
2025-09-27 20:00:42,618 Stage: Train 0.5 | Epoch: 316 | Iter: 480800 | Total Loss: 0.003359 | Recon Loss: 0.002850 | Commit Loss: 0.001018 | Perplexity: 1555.294914
2025-09-27 20:01:28,820 Stage: Train 0.5 | Epoch: 316 | Iter: 481000 | Total Loss: 0.003344 | Recon Loss: 0.002832 | Commit Loss: 0.001024 | Perplexity: 1556.281790
2025-09-27 20:02:15,301 Stage: Train 0.5 | Epoch: 316 | Iter: 481200 | Total Loss: 0.003265 | Recon Loss: 0.002759 | Commit Loss: 0.001011 | Perplexity: 1550.646337
2025-09-27 20:03:01,444 Stage: Train 0.5 | Epoch: 316 | Iter: 481400 | Total Loss: 0.003311 | Recon Loss: 0.002797 | Commit Loss: 0.001026 | Perplexity: 1560.254505
Trainning Epoch:  96%|█████████▌| 317/330 [29:50:56<1:16:11, 351.67s/it]2025-09-27 20:03:48,026 Stage: Train 0.5 | Epoch: 317 | Iter: 481600 | Total Loss: 0.003342 | Recon Loss: 0.002831 | Commit Loss: 0.001022 | Perplexity: 1553.572012
2025-09-27 20:04:34,171 Stage: Train 0.5 | Epoch: 317 | Iter: 481800 | Total Loss: 0.003360 | Recon Loss: 0.002852 | Commit Loss: 0.001015 | Perplexity: 1555.770330
2025-09-27 20:05:20,250 Stage: Train 0.5 | Epoch: 317 | Iter: 482000 | Total Loss: 0.003312 | Recon Loss: 0.002804 | Commit Loss: 0.001015 | Perplexity: 1554.513978
2025-09-27 20:06:06,394 Stage: Train 0.5 | Epoch: 317 | Iter: 482200 | Total Loss: 0.003292 | Recon Loss: 0.002783 | Commit Loss: 0.001017 | Perplexity: 1554.941908
2025-09-27 20:06:52,557 Stage: Train 0.5 | Epoch: 317 | Iter: 482400 | Total Loss: 0.003327 | Recon Loss: 0.002817 | Commit Loss: 0.001020 | Perplexity: 1552.071615
2025-09-27 20:07:38,876 Stage: Train 0.5 | Epoch: 317 | Iter: 482600 | Total Loss: 0.003262 | Recon Loss: 0.002754 | Commit Loss: 0.001017 | Perplexity: 1555.254343
2025-09-27 20:08:24,912 Stage: Train 0.5 | Epoch: 317 | Iter: 482800 | Total Loss: 0.003303 | Recon Loss: 0.002797 | Commit Loss: 0.001012 | Perplexity: 1555.064974
2025-09-27 20:09:11,158 Stage: Train 0.5 | Epoch: 317 | Iter: 483000 | Total Loss: 0.003327 | Recon Loss: 0.002817 | Commit Loss: 0.001021 | Perplexity: 1554.325245
Trainning Epoch:  96%|█████████▋| 318/330 [29:56:47<1:10:17, 351.43s/it]2025-09-27 20:09:57,575 Stage: Train 0.5 | Epoch: 318 | Iter: 483200 | Total Loss: 0.003235 | Recon Loss: 0.002728 | Commit Loss: 0.001014 | Perplexity: 1556.933303
2025-09-27 20:10:43,683 Stage: Train 0.5 | Epoch: 318 | Iter: 483400 | Total Loss: 0.003328 | Recon Loss: 0.002819 | Commit Loss: 0.001018 | Perplexity: 1557.412517
2025-09-27 20:11:29,870 Stage: Train 0.5 | Epoch: 318 | Iter: 483600 | Total Loss: 0.003252 | Recon Loss: 0.002746 | Commit Loss: 0.001012 | Perplexity: 1551.072757
2025-09-27 20:12:15,956 Stage: Train 0.5 | Epoch: 318 | Iter: 483800 | Total Loss: 0.003304 | Recon Loss: 0.002796 | Commit Loss: 0.001016 | Perplexity: 1551.343893
2025-09-27 20:13:02,273 Stage: Train 0.5 | Epoch: 318 | Iter: 484000 | Total Loss: 0.003263 | Recon Loss: 0.002754 | Commit Loss: 0.001019 | Perplexity: 1559.452620
2025-09-27 20:13:48,472 Stage: Train 0.5 | Epoch: 318 | Iter: 484200 | Total Loss: 0.003332 | Recon Loss: 0.002819 | Commit Loss: 0.001025 | Perplexity: 1560.040452
2025-09-27 20:14:34,705 Stage: Train 0.5 | Epoch: 318 | Iter: 484400 | Total Loss: 0.003295 | Recon Loss: 0.002788 | Commit Loss: 0.001014 | Perplexity: 1548.704813
Trainning Epoch:  97%|█████████▋| 319/330 [30:02:37<1:04:23, 351.25s/it]2025-09-27 20:15:20,905 Stage: Train 0.5 | Epoch: 319 | Iter: 484600 | Total Loss: 0.003332 | Recon Loss: 0.002822 | Commit Loss: 0.001019 | Perplexity: 1553.307146
2025-09-27 20:16:07,148 Stage: Train 0.5 | Epoch: 319 | Iter: 484800 | Total Loss: 0.003336 | Recon Loss: 0.002830 | Commit Loss: 0.001012 | Perplexity: 1550.655937
2025-09-27 20:16:53,486 Stage: Train 0.5 | Epoch: 319 | Iter: 485000 | Total Loss: 0.003361 | Recon Loss: 0.002852 | Commit Loss: 0.001018 | Perplexity: 1554.682963
2025-09-27 20:17:39,609 Stage: Train 0.5 | Epoch: 319 | Iter: 485200 | Total Loss: 0.003266 | Recon Loss: 0.002759 | Commit Loss: 0.001013 | Perplexity: 1549.126029
2025-09-27 20:18:25,824 Stage: Train 0.5 | Epoch: 319 | Iter: 485400 | Total Loss: 0.003303 | Recon Loss: 0.002795 | Commit Loss: 0.001015 | Perplexity: 1559.966810
2025-09-27 20:19:11,987 Stage: Train 0.5 | Epoch: 319 | Iter: 485600 | Total Loss: 0.003326 | Recon Loss: 0.002815 | Commit Loss: 0.001023 | Perplexity: 1562.762504
2025-09-27 20:19:58,517 Stage: Train 0.5 | Epoch: 319 | Iter: 485800 | Total Loss: 0.003323 | Recon Loss: 0.002818 | Commit Loss: 0.001011 | Perplexity: 1550.504710
2025-09-27 20:20:44,803 Stage: Train 0.5 | Epoch: 319 | Iter: 486000 | Total Loss: 0.003295 | Recon Loss: 0.002788 | Commit Loss: 0.001013 | Perplexity: 1553.393124
Trainning Epoch:  97%|█████████▋| 320/330 [30:08:29<58:33, 351.39s/it]  2025-09-27 20:21:31,384 Stage: Train 0.5 | Epoch: 320 | Iter: 486200 | Total Loss: 0.003261 | Recon Loss: 0.002755 | Commit Loss: 0.001012 | Perplexity: 1547.687471
2025-09-27 20:22:17,311 Stage: Train 0.5 | Epoch: 320 | Iter: 486400 | Total Loss: 0.003262 | Recon Loss: 0.002756 | Commit Loss: 0.001013 | Perplexity: 1553.501808
2025-09-27 20:23:03,516 Stage: Train 0.5 | Epoch: 320 | Iter: 486600 | Total Loss: 0.003267 | Recon Loss: 0.002758 | Commit Loss: 0.001019 | Perplexity: 1558.655986
2025-09-27 20:23:49,659 Stage: Train 0.5 | Epoch: 320 | Iter: 486800 | Total Loss: 0.003371 | Recon Loss: 0.002865 | Commit Loss: 0.001013 | Perplexity: 1554.246372
2025-09-27 20:24:35,776 Stage: Train 0.5 | Epoch: 320 | Iter: 487000 | Total Loss: 0.003289 | Recon Loss: 0.002779 | Commit Loss: 0.001019 | Perplexity: 1557.820193
2025-09-27 20:25:22,026 Stage: Train 0.5 | Epoch: 320 | Iter: 487200 | Total Loss: 0.003312 | Recon Loss: 0.002805 | Commit Loss: 0.001014 | Perplexity: 1556.809013
2025-09-27 20:26:08,144 Stage: Train 0.5 | Epoch: 320 | Iter: 487400 | Total Loss: 0.003348 | Recon Loss: 0.002837 | Commit Loss: 0.001021 | Perplexity: 1556.607075
Trainning Epoch:  97%|█████████▋| 321/330 [30:14:20<52:40, 351.16s/it]2025-09-27 20:26:54,473 Stage: Train 0.5 | Epoch: 321 | Iter: 487600 | Total Loss: 0.003232 | Recon Loss: 0.002725 | Commit Loss: 0.001014 | Perplexity: 1552.975583
2025-09-27 20:27:40,653 Stage: Train 0.5 | Epoch: 321 | Iter: 487800 | Total Loss: 0.003322 | Recon Loss: 0.002814 | Commit Loss: 0.001016 | Perplexity: 1557.859780
2025-09-27 20:28:26,630 Stage: Train 0.5 | Epoch: 321 | Iter: 488000 | Total Loss: 0.003228 | Recon Loss: 0.002724 | Commit Loss: 0.001008 | Perplexity: 1554.688361
2025-09-27 20:29:12,430 Stage: Train 0.5 | Epoch: 321 | Iter: 488200 | Total Loss: 0.003370 | Recon Loss: 0.002864 | Commit Loss: 0.001011 | Perplexity: 1553.188969
2025-09-27 20:29:58,713 Stage: Train 0.5 | Epoch: 321 | Iter: 488400 | Total Loss: 0.003309 | Recon Loss: 0.002801 | Commit Loss: 0.001017 | Perplexity: 1554.557534
2025-09-27 20:30:44,928 Stage: Train 0.5 | Epoch: 321 | Iter: 488600 | Total Loss: 0.003224 | Recon Loss: 0.002716 | Commit Loss: 0.001016 | Perplexity: 1553.999388
2025-09-27 20:31:31,159 Stage: Train 0.5 | Epoch: 321 | Iter: 488800 | Total Loss: 0.003323 | Recon Loss: 0.002813 | Commit Loss: 0.001020 | Perplexity: 1558.284091
2025-09-27 20:32:17,303 Stage: Train 0.5 | Epoch: 321 | Iter: 489000 | Total Loss: 0.003267 | Recon Loss: 0.002760 | Commit Loss: 0.001013 | Perplexity: 1554.338145
Trainning Epoch:  98%|█████████▊| 322/330 [30:20:10<46:47, 350.95s/it]2025-09-27 20:33:03,641 Stage: Train 0.5 | Epoch: 322 | Iter: 489200 | Total Loss: 0.003320 | Recon Loss: 0.002814 | Commit Loss: 0.001012 | Perplexity: 1555.386167
2025-09-27 20:33:49,827 Stage: Train 0.5 | Epoch: 322 | Iter: 489400 | Total Loss: 0.003296 | Recon Loss: 0.002792 | Commit Loss: 0.001007 | Perplexity: 1550.638888
2025-09-27 20:34:35,828 Stage: Train 0.5 | Epoch: 322 | Iter: 489600 | Total Loss: 0.003303 | Recon Loss: 0.002797 | Commit Loss: 0.001012 | Perplexity: 1556.408008
2025-09-27 20:35:21,950 Stage: Train 0.5 | Epoch: 322 | Iter: 489800 | Total Loss: 0.003261 | Recon Loss: 0.002753 | Commit Loss: 0.001016 | Perplexity: 1554.934583
2025-09-27 20:36:08,120 Stage: Train 0.5 | Epoch: 322 | Iter: 490000 | Total Loss: 0.003308 | Recon Loss: 0.002802 | Commit Loss: 0.001013 | Perplexity: 1555.835626
2025-09-27 20:36:53,880 Stage: Train 0.5 | Epoch: 322 | Iter: 490200 | Total Loss: 0.003303 | Recon Loss: 0.002797 | Commit Loss: 0.001013 | Perplexity: 1550.608295
2025-09-27 20:37:40,130 Stage: Train 0.5 | Epoch: 322 | Iter: 490400 | Total Loss: 0.003266 | Recon Loss: 0.002757 | Commit Loss: 0.001017 | Perplexity: 1555.261918
2025-09-27 20:38:26,517 Stage: Train 0.5 | Epoch: 322 | Iter: 490600 | Total Loss: 0.003317 | Recon Loss: 0.002807 | Commit Loss: 0.001019 | Perplexity: 1556.853699
Trainning Epoch:  98%|█████████▊| 323/330 [30:26:01<40:56, 350.87s/it]2025-09-27 20:39:12,969 Stage: Train 0.5 | Epoch: 323 | Iter: 490800 | Total Loss: 0.003276 | Recon Loss: 0.002768 | Commit Loss: 0.001014 | Perplexity: 1550.080361
2025-09-27 20:39:59,111 Stage: Train 0.5 | Epoch: 323 | Iter: 491000 | Total Loss: 0.003233 | Recon Loss: 0.002726 | Commit Loss: 0.001013 | Perplexity: 1555.063253
2025-09-27 20:40:45,293 Stage: Train 0.5 | Epoch: 323 | Iter: 491200 | Total Loss: 0.003319 | Recon Loss: 0.002813 | Commit Loss: 0.001013 | Perplexity: 1551.621506
2025-09-27 20:41:31,310 Stage: Train 0.5 | Epoch: 323 | Iter: 491400 | Total Loss: 0.003215 | Recon Loss: 0.002712 | Commit Loss: 0.001007 | Perplexity: 1552.625782
2025-09-27 20:42:17,506 Stage: Train 0.5 | Epoch: 323 | Iter: 491600 | Total Loss: 0.003326 | Recon Loss: 0.002817 | Commit Loss: 0.001019 | Perplexity: 1556.571946
2025-09-27 20:43:03,709 Stage: Train 0.5 | Epoch: 323 | Iter: 491800 | Total Loss: 0.003316 | Recon Loss: 0.002807 | Commit Loss: 0.001017 | Perplexity: 1560.823267
2025-09-27 20:43:49,532 Stage: Train 0.5 | Epoch: 323 | Iter: 492000 | Total Loss: 0.003282 | Recon Loss: 0.002775 | Commit Loss: 0.001014 | Perplexity: 1559.450197
Trainning Epoch:  98%|█████████▊| 324/330 [30:31:51<35:04, 350.73s/it]2025-09-27 20:44:35,971 Stage: Train 0.5 | Epoch: 324 | Iter: 492200 | Total Loss: 0.003259 | Recon Loss: 0.002750 | Commit Loss: 0.001017 | Perplexity: 1555.339478
2025-09-27 20:45:22,140 Stage: Train 0.5 | Epoch: 324 | Iter: 492400 | Total Loss: 0.003301 | Recon Loss: 0.002795 | Commit Loss: 0.001013 | Perplexity: 1553.958674
2025-09-27 20:46:08,311 Stage: Train 0.5 | Epoch: 324 | Iter: 492600 | Total Loss: 0.003297 | Recon Loss: 0.002792 | Commit Loss: 0.001010 | Perplexity: 1553.451465
2025-09-27 20:46:54,490 Stage: Train 0.5 | Epoch: 324 | Iter: 492800 | Total Loss: 0.003229 | Recon Loss: 0.002724 | Commit Loss: 0.001010 | Perplexity: 1556.807523
2025-09-27 20:47:40,740 Stage: Train 0.5 | Epoch: 324 | Iter: 493000 | Total Loss: 0.003256 | Recon Loss: 0.002749 | Commit Loss: 0.001013 | Perplexity: 1553.417043
2025-09-27 20:48:26,870 Stage: Train 0.5 | Epoch: 324 | Iter: 493200 | Total Loss: 0.003296 | Recon Loss: 0.002787 | Commit Loss: 0.001017 | Perplexity: 1556.724908
2025-09-27 20:49:13,140 Stage: Train 0.5 | Epoch: 324 | Iter: 493400 | Total Loss: 0.003275 | Recon Loss: 0.002770 | Commit Loss: 0.001011 | Perplexity: 1552.825707
2025-09-27 20:49:59,471 Stage: Train 0.5 | Epoch: 324 | Iter: 493600 | Total Loss: 0.003295 | Recon Loss: 0.002788 | Commit Loss: 0.001014 | Perplexity: 1554.068676
Trainning Epoch:  98%|█████████▊| 325/330 [30:37:42<29:14, 350.86s/it]2025-09-27 20:50:45,563 Stage: Train 0.5 | Epoch: 325 | Iter: 493800 | Total Loss: 0.003284 | Recon Loss: 0.002777 | Commit Loss: 0.001014 | Perplexity: 1557.347895
2025-09-27 20:51:31,743 Stage: Train 0.5 | Epoch: 325 | Iter: 494000 | Total Loss: 0.003351 | Recon Loss: 0.002845 | Commit Loss: 0.001013 | Perplexity: 1557.282542
2025-09-27 20:52:17,956 Stage: Train 0.5 | Epoch: 325 | Iter: 494200 | Total Loss: 0.003251 | Recon Loss: 0.002748 | Commit Loss: 0.001006 | Perplexity: 1551.537042
2025-09-27 20:53:04,185 Stage: Train 0.5 | Epoch: 325 | Iter: 494400 | Total Loss: 0.003260 | Recon Loss: 0.002750 | Commit Loss: 0.001019 | Perplexity: 1557.903595
2025-09-27 20:53:50,264 Stage: Train 0.5 | Epoch: 325 | Iter: 494600 | Total Loss: 0.003301 | Recon Loss: 0.002793 | Commit Loss: 0.001017 | Perplexity: 1559.376213
2025-09-27 20:54:34,558 Stage: Train 0.5 | Epoch: 325 | Iter: 494800 | Total Loss: 0.003308 | Recon Loss: 0.002799 | Commit Loss: 0.001017 | Perplexity: 1552.974261
2025-09-27 20:55:20,785 Stage: Train 0.5 | Epoch: 325 | Iter: 495000 | Total Loss: 0.003294 | Recon Loss: 0.002789 | Commit Loss: 0.001011 | Perplexity: 1549.737702
Trainning Epoch:  99%|█████████▉| 326/330 [30:43:31<23:21, 350.28s/it]2025-09-27 20:56:07,293 Stage: Train 0.5 | Epoch: 326 | Iter: 495200 | Total Loss: 0.003254 | Recon Loss: 0.002749 | Commit Loss: 0.001009 | Perplexity: 1552.427277
2025-09-27 20:56:53,457 Stage: Train 0.5 | Epoch: 326 | Iter: 495400 | Total Loss: 0.003246 | Recon Loss: 0.002743 | Commit Loss: 0.001007 | Perplexity: 1551.317805
2025-09-27 20:57:39,746 Stage: Train 0.5 | Epoch: 326 | Iter: 495600 | Total Loss: 0.003328 | Recon Loss: 0.002824 | Commit Loss: 0.001009 | Perplexity: 1552.469979
2025-09-27 20:58:25,719 Stage: Train 0.5 | Epoch: 326 | Iter: 495800 | Total Loss: 0.003210 | Recon Loss: 0.002702 | Commit Loss: 0.001015 | Perplexity: 1558.257249
2025-09-27 20:59:12,001 Stage: Train 0.5 | Epoch: 326 | Iter: 496000 | Total Loss: 0.003274 | Recon Loss: 0.002766 | Commit Loss: 0.001016 | Perplexity: 1557.724753
2025-09-27 20:59:58,163 Stage: Train 0.5 | Epoch: 326 | Iter: 496200 | Total Loss: 0.003327 | Recon Loss: 0.002820 | Commit Loss: 0.001016 | Perplexity: 1558.136175
2025-09-27 21:00:44,252 Stage: Train 0.5 | Epoch: 326 | Iter: 496400 | Total Loss: 0.003312 | Recon Loss: 0.002803 | Commit Loss: 0.001018 | Perplexity: 1559.359414
2025-09-27 21:01:30,316 Stage: Train 0.5 | Epoch: 326 | Iter: 496600 | Total Loss: 0.003298 | Recon Loss: 0.002792 | Commit Loss: 0.001011 | Perplexity: 1556.387879
Trainning Epoch:  99%|█████████▉| 327/330 [30:49:22<17:31, 350.44s/it]2025-09-27 21:02:16,740 Stage: Train 0.5 | Epoch: 327 | Iter: 496800 | Total Loss: 0.003229 | Recon Loss: 0.002728 | Commit Loss: 0.001002 | Perplexity: 1549.053518
2025-09-27 21:03:02,936 Stage: Train 0.5 | Epoch: 327 | Iter: 497000 | Total Loss: 0.003320 | Recon Loss: 0.002813 | Commit Loss: 0.001013 | Perplexity: 1555.659004
2025-09-27 21:03:49,144 Stage: Train 0.5 | Epoch: 327 | Iter: 497200 | Total Loss: 0.003254 | Recon Loss: 0.002747 | Commit Loss: 0.001013 | Perplexity: 1561.325305
2025-09-27 21:04:35,283 Stage: Train 0.5 | Epoch: 327 | Iter: 497400 | Total Loss: 0.003337 | Recon Loss: 0.002828 | Commit Loss: 0.001019 | Perplexity: 1560.019186
2025-09-27 21:05:21,307 Stage: Train 0.5 | Epoch: 327 | Iter: 497600 | Total Loss: 0.003306 | Recon Loss: 0.002801 | Commit Loss: 0.001010 | Perplexity: 1552.113918
2025-09-27 21:06:07,355 Stage: Train 0.5 | Epoch: 327 | Iter: 497800 | Total Loss: 0.003251 | Recon Loss: 0.002749 | Commit Loss: 0.001005 | Perplexity: 1556.283959
2025-09-27 21:06:53,490 Stage: Train 0.5 | Epoch: 327 | Iter: 498000 | Total Loss: 0.003257 | Recon Loss: 0.002752 | Commit Loss: 0.001009 | Perplexity: 1556.954717
2025-09-27 21:07:39,600 Stage: Train 0.5 | Epoch: 327 | Iter: 498200 | Total Loss: 0.003277 | Recon Loss: 0.002769 | Commit Loss: 0.001017 | Perplexity: 1553.930817
Trainning Epoch:  99%|█████████▉| 328/330 [30:55:13<11:40, 350.45s/it]2025-09-27 21:08:26,082 Stage: Train 0.5 | Epoch: 328 | Iter: 498400 | Total Loss: 0.003245 | Recon Loss: 0.002740 | Commit Loss: 0.001010 | Perplexity: 1553.153131
2025-09-27 21:09:12,123 Stage: Train 0.5 | Epoch: 328 | Iter: 498600 | Total Loss: 0.003336 | Recon Loss: 0.002830 | Commit Loss: 0.001011 | Perplexity: 1553.751897
2025-09-27 21:09:58,198 Stage: Train 0.5 | Epoch: 328 | Iter: 498800 | Total Loss: 0.003204 | Recon Loss: 0.002702 | Commit Loss: 0.001003 | Perplexity: 1548.235072
2025-09-27 21:10:44,267 Stage: Train 0.5 | Epoch: 328 | Iter: 499000 | Total Loss: 0.003307 | Recon Loss: 0.002801 | Commit Loss: 0.001013 | Perplexity: 1555.196989
2025-09-27 21:11:30,396 Stage: Train 0.5 | Epoch: 328 | Iter: 499200 | Total Loss: 0.003271 | Recon Loss: 0.002764 | Commit Loss: 0.001013 | Perplexity: 1556.429125
2025-09-27 21:12:16,113 Stage: Train 0.5 | Epoch: 328 | Iter: 499400 | Total Loss: 0.003331 | Recon Loss: 0.002825 | Commit Loss: 0.001012 | Perplexity: 1558.993245
2025-09-27 21:13:02,288 Stage: Train 0.5 | Epoch: 328 | Iter: 499600 | Total Loss: 0.003344 | Recon Loss: 0.002838 | Commit Loss: 0.001011 | Perplexity: 1557.641908
Trainning Epoch: 100%|█████████▉| 329/330 [31:01:03<05:50, 350.37s/it]2025-09-27 21:13:48,600 Stage: Train 0.5 | Epoch: 329 | Iter: 499800 | Total Loss: 0.003282 | Recon Loss: 0.002775 | Commit Loss: 0.001013 | Perplexity: 1553.857721
2025-09-27 21:14:34,848 Stage: Train 0.5 | Epoch: 329 | Iter: 500000 | Total Loss: 0.003299 | Recon Loss: 0.002796 | Commit Loss: 0.001005 | Perplexity: 1553.332223
2025-09-27 21:14:34,848 Saving model at iteration 500000
2025-09-27 21:14:35,335 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000
2025-09-27 21:14:35,664 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/model.safetensors
2025-09-27 21:14:36,074 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/optimizer.bin
2025-09-27 21:14:36,074 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/scheduler.bin
2025-09-27 21:14:36,074 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/sampler.bin
2025-09-27 21:14:36,075 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/random_states_0.pkl
Trainning Epoch: 100%|█████████▉| 329/330 [31:02:02<05:39, 339.58s/it]
2025-09-27 21:14:36,133 Training finished
