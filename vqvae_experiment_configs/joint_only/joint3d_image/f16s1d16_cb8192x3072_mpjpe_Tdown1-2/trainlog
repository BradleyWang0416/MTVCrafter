The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-27 06:48:43,036 
python train_vqvae_new.py --batch_size 64 --config vqvae_experiment_configs/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/config.yaml --data_mode null --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2 --not_find_unused_parameters --nb_code 8192 --codebook_dim 3072 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level 3 --vision_guidance_ratio 0 --fix_weights --resume_pth  --joint_data_type joint3d_image_normed --downsample_time [1,2] --frame_upsample_rate [2.0,1.0]
--- Logging error ---
Traceback (most recent call last):
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 1100, in emit
    msg = self.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 943, in format
    return fmt.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 678, in format
    record.message = record.getMessage()
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 368, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/wxs/MTVCrafter/train_vqvae_new.py", line 318, in <module>
    logger.info('\nPID ', os.getpid())
Message: '\nPID '
Arguments: (3106441,)
--- Logging error ---
Traceback (most recent call last):
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 1100, in emit
    msg = self.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 943, in format
    return fmt.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 678, in format
    record.message = record.getMessage()
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 368, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/wxs/MTVCrafter/train_vqvae_new.py", line 318, in <module>
    logger.info('\nPID ', os.getpid())
Message: '\nPID '
Arguments: (3106441,)
2025-09-27 06:48:53,143 Data loaded with 97196 samples
2025-09-27 06:48:53,821 Trainable parameters: 48,323,075
2025-09-27 06:48:53,821 Non-trainable parameters: 0
2025-09-27 06:48:54,966 Number of trainable parameters: 48.323075 M
2025-09-27 06:48:54,967 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'null', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [[]], 'normalize': 'isotropic', 'filter_invalid_images': False, 'processed_image_shape': None, 'backbone': 'hrnet_32', 'get_item_list': ['factor_2_5d', 'joint3d_image_normed', 'joint3d_image_scale', 'joint3d_image_transl', 'joint_2_5d_image'], 'config': 'vqvae_experiment_configs/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/config.yaml', 'resume_pth': '', 'batch_size': 64, 'commit_ratio': 0.5, 'nb_code': 8192, 'codebook_dim': 3072, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_image_normed', 'hrnet_output_level': 3, 'fix_weights': True, 'vision_guidance_ratio': 0.0, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0]}
Trainning Epoch:   0%|          | 0/330 [00:00<?, ?it/s]2025-09-27 06:49:52,715 current_lr 0.000008 at iteration 200
2025-09-27 06:49:52,988 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.169111 | Recon Loss: 0.165860 | Commit Loss: 0.006503 | Perplexity: 2194.675686
2025-09-27 06:50:49,058 current_lr 0.000016 at iteration 400
2025-09-27 06:50:49,340 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.094099 | Recon Loss: 0.080391 | Commit Loss: 0.027416 | Perplexity: 1730.174344
2025-09-27 06:51:45,582 current_lr 0.000024 at iteration 600
2025-09-27 06:51:45,844 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.099262 | Recon Loss: 0.067750 | Commit Loss: 0.063025 | Perplexity: 1783.037201
2025-09-27 06:52:41,971 current_lr 0.000032 at iteration 800
2025-09-27 06:52:42,234 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.099270 | Recon Loss: 0.060819 | Commit Loss: 0.076901 | Perplexity: 1864.032225
2025-09-27 06:53:38,447 current_lr 0.000040 at iteration 1000
2025-09-27 06:53:38,718 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.097658 | Recon Loss: 0.056590 | Commit Loss: 0.082135 | Perplexity: 1891.080516
2025-09-27 06:54:34,967 current_lr 0.000048 at iteration 1200
2025-09-27 06:54:35,240 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.090898 | Recon Loss: 0.052322 | Commit Loss: 0.077152 | Perplexity: 1877.538636
2025-09-27 06:55:31,119 current_lr 0.000056 at iteration 1400
2025-09-27 06:55:31,403 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.082933 | Recon Loss: 0.048931 | Commit Loss: 0.068004 | Perplexity: 1888.654366
Trainning Epoch:   0%|          | 1/330 [07:10<39:18:13, 430.07s/it]2025-09-27 06:56:27,876 current_lr 0.000064 at iteration 1600
2025-09-27 06:56:28,147 Stage: Warm Up | Epoch: 1 | Iter: 1600 | Total Loss: 0.073922 | Recon Loss: 0.044627 | Commit Loss: 0.058590 | Perplexity: 1913.887598
2025-09-27 06:57:24,132 current_lr 0.000072 at iteration 1800
2025-09-27 06:57:24,396 Stage: Warm Up | Epoch: 1 | Iter: 1800 | Total Loss: 0.065173 | Recon Loss: 0.040505 | Commit Loss: 0.049336 | Perplexity: 1922.635992
2025-09-27 06:58:20,437 current_lr 0.000080 at iteration 2000
2025-09-27 06:58:20,703 Stage: Warm Up | Epoch: 1 | Iter: 2000 | Total Loss: 0.058628 | Recon Loss: 0.038227 | Commit Loss: 0.040801 | Perplexity: 1931.882370
2025-09-27 06:59:16,718 current_lr 0.000088 at iteration 2200
2025-09-27 06:59:16,988 Stage: Warm Up | Epoch: 1 | Iter: 2200 | Total Loss: 0.053375 | Recon Loss: 0.036340 | Commit Loss: 0.034069 | Perplexity: 1942.354725
2025-09-27 07:00:13,158 current_lr 0.000096 at iteration 2400
2025-09-27 07:00:13,434 Stage: Warm Up | Epoch: 1 | Iter: 2400 | Total Loss: 0.049309 | Recon Loss: 0.034779 | Commit Loss: 0.029059 | Perplexity: 1951.673876
2025-09-27 07:01:09,405 current_lr 0.000104 at iteration 2600
2025-09-27 07:01:09,685 Stage: Warm Up | Epoch: 1 | Iter: 2600 | Total Loss: 0.044905 | Recon Loss: 0.032316 | Commit Loss: 0.025178 | Perplexity: 1952.967001
2025-09-27 07:02:05,820 current_lr 0.000112 at iteration 2800
2025-09-27 07:02:06,092 Stage: Warm Up | Epoch: 1 | Iter: 2800 | Total Loss: 0.041988 | Recon Loss: 0.031446 | Commit Loss: 0.021084 | Perplexity: 1946.826463
2025-09-27 07:03:02,359 current_lr 0.000120 at iteration 3000
2025-09-27 07:03:02,632 Stage: Warm Up | Epoch: 1 | Iter: 3000 | Total Loss: 0.038580 | Recon Loss: 0.029626 | Commit Loss: 0.017909 | Perplexity: 1946.422773
Trainning Epoch:   1%|          | 2/330 [14:18<39:05:17, 429.02s/it]2025-09-27 07:03:58,859 current_lr 0.000128 at iteration 3200
2025-09-27 07:03:59,127 Stage: Warm Up | Epoch: 2 | Iter: 3200 | Total Loss: 0.035970 | Recon Loss: 0.028275 | Commit Loss: 0.015391 | Perplexity: 1944.677636
2025-09-27 07:04:55,378 current_lr 0.000136 at iteration 3400
2025-09-27 07:04:55,655 Stage: Warm Up | Epoch: 2 | Iter: 3400 | Total Loss: 0.035071 | Recon Loss: 0.028774 | Commit Loss: 0.012594 | Perplexity: 1924.900387
2025-09-27 07:05:51,918 current_lr 0.000144 at iteration 3600
2025-09-27 07:05:52,181 Stage: Warm Up | Epoch: 2 | Iter: 3600 | Total Loss: 0.031098 | Recon Loss: 0.025751 | Commit Loss: 0.010695 | Perplexity: 1925.757396
2025-09-27 07:06:47,818 current_lr 0.000152 at iteration 3800
2025-09-27 07:06:48,092 Stage: Warm Up | Epoch: 2 | Iter: 3800 | Total Loss: 0.029640 | Recon Loss: 0.025263 | Commit Loss: 0.008752 | Perplexity: 1912.121562
2025-09-27 07:07:44,355 current_lr 0.000160 at iteration 4000
2025-09-27 07:07:44,629 Stage: Warm Up | Epoch: 2 | Iter: 4000 | Total Loss: 0.028979 | Recon Loss: 0.025279 | Commit Loss: 0.007400 | Perplexity: 1907.539988
2025-09-27 07:08:39,380 current_lr 0.000168 at iteration 4200
2025-09-27 07:08:39,651 Stage: Warm Up | Epoch: 2 | Iter: 4200 | Total Loss: 0.027835 | Recon Loss: 0.024686 | Commit Loss: 0.006296 | Perplexity: 1900.174565
2025-09-27 07:09:35,808 current_lr 0.000176 at iteration 4400
2025-09-27 07:09:36,090 Stage: Warm Up | Epoch: 2 | Iter: 4400 | Total Loss: 0.026189 | Recon Loss: 0.023398 | Commit Loss: 0.005582 | Perplexity: 1908.055527
Trainning Epoch:   1%|          | 3/330 [21:25<38:53:50, 428.23s/it]2025-09-27 07:10:32,716 current_lr 0.000184 at iteration 4600
2025-09-27 07:10:32,993 Stage: Warm Up | Epoch: 3 | Iter: 4600 | Total Loss: 0.024191 | Recon Loss: 0.021726 | Commit Loss: 0.004930 | Perplexity: 1912.152012
2025-09-27 07:11:29,242 current_lr 0.000192 at iteration 4800
2025-09-27 07:11:29,519 Stage: Warm Up | Epoch: 3 | Iter: 4800 | Total Loss: 0.023336 | Recon Loss: 0.021146 | Commit Loss: 0.004380 | Perplexity: 1923.494940
2025-09-27 07:12:25,242 current_lr 0.000200 at iteration 5000
2025-09-27 07:12:25,524 Stage: Warm Up | Epoch: 3 | Iter: 5000 | Total Loss: 0.022326 | Recon Loss: 0.020365 | Commit Loss: 0.003923 | Perplexity: 1924.998661
2025-09-27 07:13:21,915 Stage: Train 0.5 | Epoch: 3 | Iter: 5200 | Total Loss: 0.021390 | Recon Loss: 0.019649 | Commit Loss: 0.003483 | Perplexity: 1933.581760
2025-09-27 07:14:18,331 Stage: Train 0.5 | Epoch: 3 | Iter: 5400 | Total Loss: 0.020340 | Recon Loss: 0.018762 | Commit Loss: 0.003155 | Perplexity: 1950.247587
2025-09-27 07:15:14,828 Stage: Train 0.5 | Epoch: 3 | Iter: 5600 | Total Loss: 0.020295 | Recon Loss: 0.018862 | Commit Loss: 0.002867 | Perplexity: 1941.157742
2025-09-27 07:16:11,372 Stage: Train 0.5 | Epoch: 3 | Iter: 5800 | Total Loss: 0.019399 | Recon Loss: 0.018071 | Commit Loss: 0.002656 | Perplexity: 1945.220110
2025-09-27 07:17:07,893 Stage: Train 0.5 | Epoch: 3 | Iter: 6000 | Total Loss: 0.018063 | Recon Loss: 0.016839 | Commit Loss: 0.002449 | Perplexity: 1959.683802
Trainning Epoch:   1%|          | 4/330 [28:34<38:47:16, 428.33s/it]2025-09-27 07:18:03,841 Stage: Train 0.5 | Epoch: 4 | Iter: 6200 | Total Loss: 0.018576 | Recon Loss: 0.017477 | Commit Loss: 0.002199 | Perplexity: 1947.015801
2025-09-27 07:18:59,894 Stage: Train 0.5 | Epoch: 4 | Iter: 6400 | Total Loss: 0.016864 | Recon Loss: 0.015862 | Commit Loss: 0.002006 | Perplexity: 1935.993295
2025-09-27 07:19:56,353 Stage: Train 0.5 | Epoch: 4 | Iter: 6600 | Total Loss: 0.016787 | Recon Loss: 0.015887 | Commit Loss: 0.001800 | Perplexity: 1913.373204
2025-09-27 07:20:53,109 Stage: Train 0.5 | Epoch: 4 | Iter: 6800 | Total Loss: 0.015763 | Recon Loss: 0.014906 | Commit Loss: 0.001713 | Perplexity: 1916.092509
2025-09-27 07:21:49,671 Stage: Train 0.5 | Epoch: 4 | Iter: 7000 | Total Loss: 0.015183 | Recon Loss: 0.014351 | Commit Loss: 0.001664 | Perplexity: 1927.986811
2025-09-27 07:22:46,111 Stage: Train 0.5 | Epoch: 4 | Iter: 7200 | Total Loss: 0.014781 | Recon Loss: 0.013972 | Commit Loss: 0.001619 | Perplexity: 1928.315648
2025-09-27 07:23:42,610 Stage: Train 0.5 | Epoch: 4 | Iter: 7400 | Total Loss: 0.014826 | Recon Loss: 0.014060 | Commit Loss: 0.001531 | Perplexity: 1903.764199
Trainning Epoch:   2%|▏         | 5/330 [35:42<38:39:52, 428.29s/it]2025-09-27 07:24:38,893 Stage: Train 0.5 | Epoch: 5 | Iter: 7600 | Total Loss: 0.014438 | Recon Loss: 0.013690 | Commit Loss: 0.001496 | Perplexity: 1888.439736
2025-09-27 07:25:35,392 Stage: Train 0.5 | Epoch: 5 | Iter: 7800 | Total Loss: 0.014031 | Recon Loss: 0.013303 | Commit Loss: 0.001455 | Perplexity: 1885.090206
2025-09-27 07:26:31,791 Stage: Train 0.5 | Epoch: 5 | Iter: 8000 | Total Loss: 0.013824 | Recon Loss: 0.013098 | Commit Loss: 0.001453 | Perplexity: 1892.458287
2025-09-27 07:27:28,169 Stage: Train 0.5 | Epoch: 5 | Iter: 8200 | Total Loss: 0.013631 | Recon Loss: 0.012922 | Commit Loss: 0.001418 | Perplexity: 1907.314954
2025-09-27 07:28:24,642 Stage: Train 0.5 | Epoch: 5 | Iter: 8400 | Total Loss: 0.013922 | Recon Loss: 0.013223 | Commit Loss: 0.001398 | Perplexity: 1911.513450
2025-09-27 07:29:21,318 Stage: Train 0.5 | Epoch: 5 | Iter: 8600 | Total Loss: 0.012895 | Recon Loss: 0.012202 | Commit Loss: 0.001385 | Perplexity: 1921.244748
2025-09-27 07:30:17,359 Stage: Train 0.5 | Epoch: 5 | Iter: 8800 | Total Loss: 0.012973 | Recon Loss: 0.012280 | Commit Loss: 0.001385 | Perplexity: 1923.235329
2025-09-27 07:31:13,612 Stage: Train 0.5 | Epoch: 5 | Iter: 9000 | Total Loss: 0.013230 | Recon Loss: 0.012558 | Commit Loss: 0.001344 | Perplexity: 1941.273288
Trainning Epoch:   2%|▏         | 6/330 [42:50<38:33:03, 428.34s/it]2025-09-27 07:32:10,248 Stage: Train 0.5 | Epoch: 6 | Iter: 9200 | Total Loss: 0.012736 | Recon Loss: 0.012085 | Commit Loss: 0.001302 | Perplexity: 1935.051034
2025-09-27 07:33:06,940 Stage: Train 0.5 | Epoch: 6 | Iter: 9400 | Total Loss: 0.012126 | Recon Loss: 0.011449 | Commit Loss: 0.001354 | Perplexity: 1937.940056
2025-09-27 07:34:03,456 Stage: Train 0.5 | Epoch: 6 | Iter: 9600 | Total Loss: 0.012233 | Recon Loss: 0.011559 | Commit Loss: 0.001347 | Perplexity: 1933.656603
2025-09-27 07:34:59,856 Stage: Train 0.5 | Epoch: 6 | Iter: 9800 | Total Loss: 0.012651 | Recon Loss: 0.011989 | Commit Loss: 0.001325 | Perplexity: 1931.870447
2025-09-27 07:35:56,017 Stage: Train 0.5 | Epoch: 6 | Iter: 10000 | Total Loss: 0.012228 | Recon Loss: 0.011562 | Commit Loss: 0.001333 | Perplexity: 1929.650135
2025-09-27 07:36:52,420 Stage: Train 0.5 | Epoch: 6 | Iter: 10200 | Total Loss: 0.011830 | Recon Loss: 0.011169 | Commit Loss: 0.001321 | Perplexity: 1923.589627
2025-09-27 07:37:48,912 Stage: Train 0.5 | Epoch: 6 | Iter: 10400 | Total Loss: 0.011658 | Recon Loss: 0.010994 | Commit Loss: 0.001328 | Perplexity: 1936.924800
2025-09-27 07:38:45,452 Stage: Train 0.5 | Epoch: 6 | Iter: 10600 | Total Loss: 0.011364 | Recon Loss: 0.010681 | Commit Loss: 0.001365 | Perplexity: 1949.652410
Trainning Epoch:   2%|▏         | 7/330 [49:59<38:27:07, 428.57s/it]2025-09-27 07:39:42,038 Stage: Train 0.5 | Epoch: 7 | Iter: 10800 | Total Loss: 0.011561 | Recon Loss: 0.010897 | Commit Loss: 0.001330 | Perplexity: 1946.188759
2025-09-27 07:40:38,618 Stage: Train 0.5 | Epoch: 7 | Iter: 11000 | Total Loss: 0.011875 | Recon Loss: 0.011211 | Commit Loss: 0.001329 | Perplexity: 1941.816656
2025-09-27 07:41:34,668 Stage: Train 0.5 | Epoch: 7 | Iter: 11200 | Total Loss: 0.011410 | Recon Loss: 0.010733 | Commit Loss: 0.001353 | Perplexity: 1952.269461
2025-09-27 07:42:30,968 Stage: Train 0.5 | Epoch: 7 | Iter: 11400 | Total Loss: 0.012204 | Recon Loss: 0.011542 | Commit Loss: 0.001324 | Perplexity: 1940.669323
2025-09-27 07:43:27,182 Stage: Train 0.5 | Epoch: 7 | Iter: 11600 | Total Loss: 0.011492 | Recon Loss: 0.010853 | Commit Loss: 0.001278 | Perplexity: 1940.301491
2025-09-27 07:44:23,629 Stage: Train 0.5 | Epoch: 7 | Iter: 11800 | Total Loss: 0.011348 | Recon Loss: 0.010695 | Commit Loss: 0.001307 | Perplexity: 1944.756638
2025-09-27 07:45:19,937 Stage: Train 0.5 | Epoch: 7 | Iter: 12000 | Total Loss: 0.011123 | Recon Loss: 0.010435 | Commit Loss: 0.001374 | Perplexity: 1939.793226
Trainning Epoch:   2%|▏         | 8/330 [57:07<38:19:01, 428.39s/it]2025-09-27 07:46:16,423 Stage: Train 0.5 | Epoch: 8 | Iter: 12200 | Total Loss: 0.010840 | Recon Loss: 0.010173 | Commit Loss: 0.001333 | Perplexity: 1934.610215
2025-09-27 07:47:12,814 Stage: Train 0.5 | Epoch: 8 | Iter: 12400 | Total Loss: 0.010958 | Recon Loss: 0.010291 | Commit Loss: 0.001335 | Perplexity: 1934.046984
2025-09-27 07:48:09,047 Stage: Train 0.5 | Epoch: 8 | Iter: 12600 | Total Loss: 0.011261 | Recon Loss: 0.010577 | Commit Loss: 0.001369 | Perplexity: 1938.670322
2025-09-27 07:49:05,677 Stage: Train 0.5 | Epoch: 8 | Iter: 12800 | Total Loss: 0.010525 | Recon Loss: 0.009863 | Commit Loss: 0.001326 | Perplexity: 1952.840366
2025-09-27 07:50:02,063 Stage: Train 0.5 | Epoch: 8 | Iter: 13000 | Total Loss: 0.011051 | Recon Loss: 0.010388 | Commit Loss: 0.001325 | Perplexity: 1940.976263
2025-09-27 07:50:58,428 Stage: Train 0.5 | Epoch: 8 | Iter: 13200 | Total Loss: 0.010596 | Recon Loss: 0.009909 | Commit Loss: 0.001374 | Perplexity: 1950.371506
2025-09-27 07:51:54,822 Stage: Train 0.5 | Epoch: 8 | Iter: 13400 | Total Loss: 0.010915 | Recon Loss: 0.010285 | Commit Loss: 0.001260 | Perplexity: 1947.597187
2025-09-27 07:52:51,398 Stage: Train 0.5 | Epoch: 8 | Iter: 13600 | Total Loss: 0.010099 | Recon Loss: 0.009408 | Commit Loss: 0.001382 | Perplexity: 1950.495181
Trainning Epoch:   3%|▎         | 9/330 [1:04:16<38:11:37, 428.34s/it]2025-09-27 07:53:47,695 Stage: Train 0.5 | Epoch: 9 | Iter: 13800 | Total Loss: 0.010746 | Recon Loss: 0.010079 | Commit Loss: 0.001335 | Perplexity: 1934.233760
2025-09-27 07:54:44,172 Stage: Train 0.5 | Epoch: 9 | Iter: 14000 | Total Loss: 0.010777 | Recon Loss: 0.010127 | Commit Loss: 0.001300 | Perplexity: 1940.407699
2025-09-27 07:55:40,513 Stage: Train 0.5 | Epoch: 9 | Iter: 14200 | Total Loss: 0.010481 | Recon Loss: 0.009804 | Commit Loss: 0.001352 | Perplexity: 1938.378248
2025-09-27 07:56:36,703 Stage: Train 0.5 | Epoch: 9 | Iter: 14400 | Total Loss: 0.010286 | Recon Loss: 0.009610 | Commit Loss: 0.001351 | Perplexity: 1932.263572
2025-09-27 07:57:33,090 Stage: Train 0.5 | Epoch: 9 | Iter: 14600 | Total Loss: 0.010005 | Recon Loss: 0.009329 | Commit Loss: 0.001352 | Perplexity: 1936.365989
2025-09-27 07:58:29,319 Stage: Train 0.5 | Epoch: 9 | Iter: 14800 | Total Loss: 0.010297 | Recon Loss: 0.009614 | Commit Loss: 0.001366 | Perplexity: 1932.362335
2025-09-27 07:59:25,500 Stage: Train 0.5 | Epoch: 9 | Iter: 15000 | Total Loss: 0.009958 | Recon Loss: 0.009266 | Commit Loss: 0.001383 | Perplexity: 1931.583431
Trainning Epoch:   3%|▎         | 10/330 [1:11:23<38:03:11, 428.10s/it]2025-09-27 08:00:21,640 Stage: Train 0.5 | Epoch: 10 | Iter: 15200 | Total Loss: 0.010493 | Recon Loss: 0.009794 | Commit Loss: 0.001398 | Perplexity: 1924.356041
2025-09-27 08:01:18,363 Stage: Train 0.5 | Epoch: 10 | Iter: 15400 | Total Loss: 0.009525 | Recon Loss: 0.008847 | Commit Loss: 0.001357 | Perplexity: 1926.180367
2025-09-27 08:02:15,171 Stage: Train 0.5 | Epoch: 10 | Iter: 15600 | Total Loss: 0.009857 | Recon Loss: 0.009147 | Commit Loss: 0.001420 | Perplexity: 1937.169716
2025-09-27 08:03:11,576 Stage: Train 0.5 | Epoch: 10 | Iter: 15800 | Total Loss: 0.009737 | Recon Loss: 0.009013 | Commit Loss: 0.001447 | Perplexity: 1930.623792
2025-09-27 08:04:08,152 Stage: Train 0.5 | Epoch: 10 | Iter: 16000 | Total Loss: 0.010014 | Recon Loss: 0.009333 | Commit Loss: 0.001363 | Perplexity: 1926.243914
2025-09-27 08:05:04,318 Stage: Train 0.5 | Epoch: 10 | Iter: 16200 | Total Loss: 0.009726 | Recon Loss: 0.009013 | Commit Loss: 0.001426 | Perplexity: 1934.059428
2025-09-27 08:06:00,621 Stage: Train 0.5 | Epoch: 10 | Iter: 16400 | Total Loss: 0.010073 | Recon Loss: 0.009358 | Commit Loss: 0.001431 | Perplexity: 1938.138680
2025-09-27 08:06:56,998 Stage: Train 0.5 | Epoch: 10 | Iter: 16600 | Total Loss: 0.009411 | Recon Loss: 0.008698 | Commit Loss: 0.001425 | Perplexity: 1939.402490
Trainning Epoch:   3%|▎         | 11/330 [1:18:32<37:57:39, 428.40s/it]2025-09-27 08:07:53,505 Stage: Train 0.5 | Epoch: 11 | Iter: 16800 | Total Loss: 0.009726 | Recon Loss: 0.008994 | Commit Loss: 0.001465 | Perplexity: 1933.444273
2025-09-27 08:08:49,741 Stage: Train 0.5 | Epoch: 11 | Iter: 17000 | Total Loss: 0.010052 | Recon Loss: 0.009332 | Commit Loss: 0.001439 | Perplexity: 1927.030837
2025-09-27 08:09:46,449 Stage: Train 0.5 | Epoch: 11 | Iter: 17200 | Total Loss: 0.009920 | Recon Loss: 0.009220 | Commit Loss: 0.001400 | Perplexity: 1934.470531
2025-09-27 08:10:42,578 Stage: Train 0.5 | Epoch: 11 | Iter: 17400 | Total Loss: 0.009376 | Recon Loss: 0.008648 | Commit Loss: 0.001457 | Perplexity: 1933.462760
2025-09-27 08:11:39,126 Stage: Train 0.5 | Epoch: 11 | Iter: 17600 | Total Loss: 0.009753 | Recon Loss: 0.009028 | Commit Loss: 0.001451 | Perplexity: 1933.681074
2025-09-27 08:12:35,830 Stage: Train 0.5 | Epoch: 11 | Iter: 17800 | Total Loss: 0.009617 | Recon Loss: 0.008892 | Commit Loss: 0.001449 | Perplexity: 1937.067231
2025-09-27 08:13:32,334 Stage: Train 0.5 | Epoch: 11 | Iter: 18000 | Total Loss: 0.009277 | Recon Loss: 0.008551 | Commit Loss: 0.001451 | Perplexity: 1943.478386
2025-09-27 08:14:28,894 Stage: Train 0.5 | Epoch: 11 | Iter: 18200 | Total Loss: 0.009549 | Recon Loss: 0.008823 | Commit Loss: 0.001452 | Perplexity: 1933.612639
Trainning Epoch:   4%|▎         | 12/330 [1:25:41<37:51:37, 428.61s/it]2025-09-27 08:15:25,534 Stage: Train 0.5 | Epoch: 12 | Iter: 18400 | Total Loss: 0.009383 | Recon Loss: 0.008653 | Commit Loss: 0.001460 | Perplexity: 1932.877782
2025-09-27 08:16:21,383 Stage: Train 0.5 | Epoch: 12 | Iter: 18600 | Total Loss: 0.009702 | Recon Loss: 0.008955 | Commit Loss: 0.001495 | Perplexity: 1929.699654
2025-09-27 08:17:18,005 Stage: Train 0.5 | Epoch: 12 | Iter: 18800 | Total Loss: 0.009008 | Recon Loss: 0.008280 | Commit Loss: 0.001456 | Perplexity: 1938.042499
2025-09-27 08:18:14,525 Stage: Train 0.5 | Epoch: 12 | Iter: 19000 | Total Loss: 0.009197 | Recon Loss: 0.008440 | Commit Loss: 0.001513 | Perplexity: 1932.309089
2025-09-27 08:19:11,309 Stage: Train 0.5 | Epoch: 12 | Iter: 19200 | Total Loss: 0.009409 | Recon Loss: 0.008692 | Commit Loss: 0.001434 | Perplexity: 1926.663209
2025-09-27 08:20:07,739 Stage: Train 0.5 | Epoch: 12 | Iter: 19400 | Total Loss: 0.009237 | Recon Loss: 0.008505 | Commit Loss: 0.001465 | Perplexity: 1936.807538
2025-09-27 08:21:04,214 Stage: Train 0.5 | Epoch: 12 | Iter: 19600 | Total Loss: 0.009290 | Recon Loss: 0.008540 | Commit Loss: 0.001501 | Perplexity: 1934.063397
Trainning Epoch:   4%|▍         | 13/330 [1:32:50<37:44:55, 428.69s/it]2025-09-27 08:22:00,762 Stage: Train 0.5 | Epoch: 13 | Iter: 19800 | Total Loss: 0.008884 | Recon Loss: 0.008125 | Commit Loss: 0.001518 | Perplexity: 1933.885101
2025-09-27 08:22:56,939 Stage: Train 0.5 | Epoch: 13 | Iter: 20000 | Total Loss: 0.009084 | Recon Loss: 0.008360 | Commit Loss: 0.001449 | Perplexity: 1929.628903
2025-09-27 08:22:56,939 Saving model at iteration 20000
2025-09-27 08:22:57,125 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000
2025-09-27 08:22:57,582 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/model.safetensors
2025-09-27 08:22:58,086 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/optimizer.bin
2025-09-27 08:22:58,086 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/scheduler.bin
2025-09-27 08:22:58,087 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/sampler.bin
2025-09-27 08:22:58,088 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/random_states_0.pkl
2025-09-27 08:23:54,782 Stage: Train 0.5 | Epoch: 13 | Iter: 20200 | Total Loss: 0.009105 | Recon Loss: 0.008359 | Commit Loss: 0.001492 | Perplexity: 1926.933846
2025-09-27 08:24:51,649 Stage: Train 0.5 | Epoch: 13 | Iter: 20400 | Total Loss: 0.009004 | Recon Loss: 0.008277 | Commit Loss: 0.001454 | Perplexity: 1925.417809
2025-09-27 08:25:46,971 Stage: Train 0.5 | Epoch: 13 | Iter: 20600 | Total Loss: 0.009008 | Recon Loss: 0.008266 | Commit Loss: 0.001484 | Perplexity: 1930.916214
2025-09-27 08:26:43,631 Stage: Train 0.5 | Epoch: 13 | Iter: 20800 | Total Loss: 0.008997 | Recon Loss: 0.008262 | Commit Loss: 0.001471 | Perplexity: 1930.401389
2025-09-27 08:27:40,231 Stage: Train 0.5 | Epoch: 13 | Iter: 21000 | Total Loss: 0.008811 | Recon Loss: 0.008078 | Commit Loss: 0.001465 | Perplexity: 1933.105829
2025-09-27 08:28:36,294 Stage: Train 0.5 | Epoch: 13 | Iter: 21200 | Total Loss: 0.009005 | Recon Loss: 0.008273 | Commit Loss: 0.001464 | Perplexity: 1930.035946
Trainning Epoch:   4%|▍         | 14/330 [1:40:00<37:38:49, 428.89s/it]2025-09-27 08:29:32,927 Stage: Train 0.5 | Epoch: 14 | Iter: 21400 | Total Loss: 0.008715 | Recon Loss: 0.007985 | Commit Loss: 0.001459 | Perplexity: 1924.182931
2025-09-27 08:30:29,078 Stage: Train 0.5 | Epoch: 14 | Iter: 21600 | Total Loss: 0.008755 | Recon Loss: 0.008012 | Commit Loss: 0.001486 | Perplexity: 1935.500016
2025-09-27 08:31:25,356 Stage: Train 0.5 | Epoch: 14 | Iter: 21800 | Total Loss: 0.008646 | Recon Loss: 0.007912 | Commit Loss: 0.001467 | Perplexity: 1931.329841
2025-09-27 08:32:21,504 Stage: Train 0.5 | Epoch: 14 | Iter: 22000 | Total Loss: 0.009091 | Recon Loss: 0.008370 | Commit Loss: 0.001442 | Perplexity: 1932.118372
2025-09-27 08:33:17,783 Stage: Train 0.5 | Epoch: 14 | Iter: 22200 | Total Loss: 0.008248 | Recon Loss: 0.007506 | Commit Loss: 0.001485 | Perplexity: 1928.745508
2025-09-27 08:34:13,574 Stage: Train 0.5 | Epoch: 14 | Iter: 22400 | Total Loss: 0.008888 | Recon Loss: 0.008169 | Commit Loss: 0.001439 | Perplexity: 1925.441721
2025-09-27 08:35:10,142 Stage: Train 0.5 | Epoch: 14 | Iter: 22600 | Total Loss: 0.008603 | Recon Loss: 0.007875 | Commit Loss: 0.001456 | Perplexity: 1927.673900
Trainning Epoch:   5%|▍         | 15/330 [1:47:07<37:28:57, 428.37s/it]2025-09-27 08:36:06,520 Stage: Train 0.5 | Epoch: 15 | Iter: 22800 | Total Loss: 0.008600 | Recon Loss: 0.007862 | Commit Loss: 0.001477 | Perplexity: 1929.011273
2025-09-27 08:37:02,731 Stage: Train 0.5 | Epoch: 15 | Iter: 23000 | Total Loss: 0.008431 | Recon Loss: 0.007700 | Commit Loss: 0.001463 | Perplexity: 1935.010675
2025-09-27 08:37:58,875 Stage: Train 0.5 | Epoch: 15 | Iter: 23200 | Total Loss: 0.008462 | Recon Loss: 0.007721 | Commit Loss: 0.001482 | Perplexity: 1926.377051
2025-09-27 08:38:55,092 Stage: Train 0.5 | Epoch: 15 | Iter: 23400 | Total Loss: 0.008475 | Recon Loss: 0.007737 | Commit Loss: 0.001476 | Perplexity: 1930.645464
2025-09-27 08:39:51,051 Stage: Train 0.5 | Epoch: 15 | Iter: 23600 | Total Loss: 0.008408 | Recon Loss: 0.007659 | Commit Loss: 0.001498 | Perplexity: 1928.649133
2025-09-27 08:40:47,273 Stage: Train 0.5 | Epoch: 15 | Iter: 23800 | Total Loss: 0.008578 | Recon Loss: 0.007841 | Commit Loss: 0.001475 | Perplexity: 1926.692396
2025-09-27 08:41:43,529 Stage: Train 0.5 | Epoch: 15 | Iter: 24000 | Total Loss: 0.008720 | Recon Loss: 0.007982 | Commit Loss: 0.001475 | Perplexity: 1925.563743
2025-09-27 08:42:40,024 Stage: Train 0.5 | Epoch: 15 | Iter: 24200 | Total Loss: 0.008254 | Recon Loss: 0.007521 | Commit Loss: 0.001466 | Perplexity: 1930.046241
Trainning Epoch:   5%|▍         | 16/330 [1:54:14<37:19:47, 427.99s/it]2025-09-27 08:43:36,593 Stage: Train 0.5 | Epoch: 16 | Iter: 24400 | Total Loss: 0.008605 | Recon Loss: 0.007884 | Commit Loss: 0.001444 | Perplexity: 1922.700009
2025-09-27 08:44:32,895 Stage: Train 0.5 | Epoch: 16 | Iter: 24600 | Total Loss: 0.008182 | Recon Loss: 0.007442 | Commit Loss: 0.001480 | Perplexity: 1924.515539
2025-09-27 08:45:29,033 Stage: Train 0.5 | Epoch: 16 | Iter: 24800 | Total Loss: 0.008321 | Recon Loss: 0.007602 | Commit Loss: 0.001438 | Perplexity: 1930.138866
2025-09-27 08:46:25,139 Stage: Train 0.5 | Epoch: 16 | Iter: 25000 | Total Loss: 0.008264 | Recon Loss: 0.007510 | Commit Loss: 0.001506 | Perplexity: 1926.854858
2025-09-27 08:47:21,473 Stage: Train 0.5 | Epoch: 16 | Iter: 25200 | Total Loss: 0.008350 | Recon Loss: 0.007618 | Commit Loss: 0.001464 | Perplexity: 1930.018666
2025-09-27 08:48:17,879 Stage: Train 0.5 | Epoch: 16 | Iter: 25400 | Total Loss: 0.008146 | Recon Loss: 0.007411 | Commit Loss: 0.001471 | Perplexity: 1922.513578
2025-09-27 08:49:14,329 Stage: Train 0.5 | Epoch: 16 | Iter: 25600 | Total Loss: 0.008353 | Recon Loss: 0.007631 | Commit Loss: 0.001444 | Perplexity: 1933.803661
2025-09-27 08:50:10,955 Stage: Train 0.5 | Epoch: 16 | Iter: 25800 | Total Loss: 0.008193 | Recon Loss: 0.007445 | Commit Loss: 0.001497 | Perplexity: 1925.180323
Trainning Epoch:   5%|▌         | 17/330 [2:01:22<37:13:01, 428.06s/it]2025-09-27 08:51:07,772 Stage: Train 0.5 | Epoch: 17 | Iter: 26000 | Total Loss: 0.007879 | Recon Loss: 0.007129 | Commit Loss: 0.001500 | Perplexity: 1928.856462
2025-09-27 08:52:03,880 Stage: Train 0.5 | Epoch: 17 | Iter: 26200 | Total Loss: 0.008230 | Recon Loss: 0.007494 | Commit Loss: 0.001473 | Perplexity: 1926.661085
2025-09-27 08:53:00,194 Stage: Train 0.5 | Epoch: 17 | Iter: 26400 | Total Loss: 0.008384 | Recon Loss: 0.007655 | Commit Loss: 0.001457 | Perplexity: 1928.644393
2025-09-27 08:53:56,618 Stage: Train 0.5 | Epoch: 17 | Iter: 26600 | Total Loss: 0.008373 | Recon Loss: 0.007644 | Commit Loss: 0.001459 | Perplexity: 1926.722722
2025-09-27 08:54:52,814 Stage: Train 0.5 | Epoch: 17 | Iter: 26800 | Total Loss: 0.007644 | Recon Loss: 0.006912 | Commit Loss: 0.001465 | Perplexity: 1929.968307
2025-09-27 08:55:48,927 Stage: Train 0.5 | Epoch: 17 | Iter: 27000 | Total Loss: 0.007871 | Recon Loss: 0.007116 | Commit Loss: 0.001511 | Perplexity: 1930.484967
2025-09-27 08:56:45,299 Stage: Train 0.5 | Epoch: 17 | Iter: 27200 | Total Loss: 0.008258 | Recon Loss: 0.007519 | Commit Loss: 0.001478 | Perplexity: 1929.242600
Trainning Epoch:   5%|▌         | 18/330 [2:08:30<37:05:13, 427.93s/it]2025-09-27 08:57:41,602 Stage: Train 0.5 | Epoch: 18 | Iter: 27400 | Total Loss: 0.008381 | Recon Loss: 0.007655 | Commit Loss: 0.001451 | Perplexity: 1927.193395
2025-09-27 08:58:38,122 Stage: Train 0.5 | Epoch: 18 | Iter: 27600 | Total Loss: 0.007734 | Recon Loss: 0.006995 | Commit Loss: 0.001479 | Perplexity: 1930.804227
2025-09-27 08:59:34,509 Stage: Train 0.5 | Epoch: 18 | Iter: 27800 | Total Loss: 0.008096 | Recon Loss: 0.007371 | Commit Loss: 0.001450 | Perplexity: 1927.728331
2025-09-27 09:00:30,703 Stage: Train 0.5 | Epoch: 18 | Iter: 28000 | Total Loss: 0.008060 | Recon Loss: 0.007336 | Commit Loss: 0.001448 | Perplexity: 1927.353209
2025-09-27 09:01:27,164 Stage: Train 0.5 | Epoch: 18 | Iter: 28200 | Total Loss: 0.007789 | Recon Loss: 0.007052 | Commit Loss: 0.001475 | Perplexity: 1932.637803
2025-09-27 09:02:23,886 Stage: Train 0.5 | Epoch: 18 | Iter: 28400 | Total Loss: 0.008131 | Recon Loss: 0.007405 | Commit Loss: 0.001452 | Perplexity: 1930.093354
2025-09-27 09:03:19,875 Stage: Train 0.5 | Epoch: 18 | Iter: 28600 | Total Loss: 0.007819 | Recon Loss: 0.007080 | Commit Loss: 0.001478 | Perplexity: 1932.764028
2025-09-27 09:04:16,181 Stage: Train 0.5 | Epoch: 18 | Iter: 28800 | Total Loss: 0.007826 | Recon Loss: 0.007089 | Commit Loss: 0.001475 | Perplexity: 1924.712776
Trainning Epoch:   6%|▌         | 19/330 [2:15:38<36:58:33, 428.02s/it]2025-09-27 09:05:12,625 Stage: Train 0.5 | Epoch: 19 | Iter: 29000 | Total Loss: 0.007988 | Recon Loss: 0.007260 | Commit Loss: 0.001457 | Perplexity: 1933.930415
2025-09-27 09:06:09,215 Stage: Train 0.5 | Epoch: 19 | Iter: 29200 | Total Loss: 0.008005 | Recon Loss: 0.007266 | Commit Loss: 0.001479 | Perplexity: 1934.450220
2025-09-27 09:07:05,624 Stage: Train 0.5 | Epoch: 19 | Iter: 29400 | Total Loss: 0.007746 | Recon Loss: 0.007015 | Commit Loss: 0.001463 | Perplexity: 1925.464711
2025-09-27 09:08:02,105 Stage: Train 0.5 | Epoch: 19 | Iter: 29600 | Total Loss: 0.007865 | Recon Loss: 0.007138 | Commit Loss: 0.001455 | Perplexity: 1925.103263
2025-09-27 09:08:58,128 Stage: Train 0.5 | Epoch: 19 | Iter: 29800 | Total Loss: 0.007929 | Recon Loss: 0.007214 | Commit Loss: 0.001430 | Perplexity: 1926.144614
2025-09-27 09:09:54,326 Stage: Train 0.5 | Epoch: 19 | Iter: 30000 | Total Loss: 0.007658 | Recon Loss: 0.006917 | Commit Loss: 0.001482 | Perplexity: 1931.135144
2025-09-27 09:10:50,383 Stage: Train 0.5 | Epoch: 19 | Iter: 30200 | Total Loss: 0.007960 | Recon Loss: 0.007233 | Commit Loss: 0.001453 | Perplexity: 1923.484328
Trainning Epoch:   6%|▌         | 20/330 [2:22:46<36:51:04, 427.95s/it]2025-09-27 09:11:46,904 Stage: Train 0.5 | Epoch: 20 | Iter: 30400 | Total Loss: 0.007753 | Recon Loss: 0.007017 | Commit Loss: 0.001472 | Perplexity: 1928.485741
2025-09-27 09:12:43,155 Stage: Train 0.5 | Epoch: 20 | Iter: 30600 | Total Loss: 0.007872 | Recon Loss: 0.007144 | Commit Loss: 0.001456 | Perplexity: 1930.342948
2025-09-27 09:13:39,686 Stage: Train 0.5 | Epoch: 20 | Iter: 30800 | Total Loss: 0.007685 | Recon Loss: 0.006960 | Commit Loss: 0.001450 | Perplexity: 1927.094526
2025-09-27 09:14:36,109 Stage: Train 0.5 | Epoch: 20 | Iter: 31000 | Total Loss: 0.007547 | Recon Loss: 0.006803 | Commit Loss: 0.001489 | Perplexity: 1933.818685
2025-09-27 09:15:32,302 Stage: Train 0.5 | Epoch: 20 | Iter: 31200 | Total Loss: 0.007719 | Recon Loss: 0.006990 | Commit Loss: 0.001459 | Perplexity: 1929.619503
2025-09-27 09:16:28,650 Stage: Train 0.5 | Epoch: 20 | Iter: 31400 | Total Loss: 0.007588 | Recon Loss: 0.006861 | Commit Loss: 0.001454 | Perplexity: 1933.241633
2025-09-27 09:17:24,782 Stage: Train 0.5 | Epoch: 20 | Iter: 31600 | Total Loss: 0.008013 | Recon Loss: 0.007280 | Commit Loss: 0.001467 | Perplexity: 1923.386246
2025-09-27 09:18:21,416 Stage: Train 0.5 | Epoch: 20 | Iter: 31800 | Total Loss: 0.007907 | Recon Loss: 0.007180 | Commit Loss: 0.001456 | Perplexity: 1931.330492
Trainning Epoch:   6%|▋         | 21/330 [2:29:54<36:44:16, 428.01s/it]2025-09-27 09:19:17,920 Stage: Train 0.5 | Epoch: 21 | Iter: 32000 | Total Loss: 0.007488 | Recon Loss: 0.006769 | Commit Loss: 0.001436 | Perplexity: 1925.193683
2025-09-27 09:20:14,212 Stage: Train 0.5 | Epoch: 21 | Iter: 32200 | Total Loss: 0.007659 | Recon Loss: 0.006930 | Commit Loss: 0.001458 | Perplexity: 1925.920281
2025-09-27 09:21:10,170 Stage: Train 0.5 | Epoch: 21 | Iter: 32400 | Total Loss: 0.007594 | Recon Loss: 0.006880 | Commit Loss: 0.001429 | Perplexity: 1926.491931
2025-09-27 09:22:06,446 Stage: Train 0.5 | Epoch: 21 | Iter: 32600 | Total Loss: 0.007686 | Recon Loss: 0.006968 | Commit Loss: 0.001437 | Perplexity: 1936.799364
2025-09-27 09:23:02,965 Stage: Train 0.5 | Epoch: 21 | Iter: 32800 | Total Loss: 0.007880 | Recon Loss: 0.007166 | Commit Loss: 0.001429 | Perplexity: 1935.633431
2025-09-27 09:23:59,631 Stage: Train 0.5 | Epoch: 21 | Iter: 33000 | Total Loss: 0.007292 | Recon Loss: 0.006568 | Commit Loss: 0.001448 | Perplexity: 1933.763325
2025-09-27 09:24:55,911 Stage: Train 0.5 | Epoch: 21 | Iter: 33200 | Total Loss: 0.007650 | Recon Loss: 0.006933 | Commit Loss: 0.001434 | Perplexity: 1926.034568
2025-09-27 09:25:52,431 Stage: Train 0.5 | Epoch: 21 | Iter: 33400 | Total Loss: 0.007506 | Recon Loss: 0.006771 | Commit Loss: 0.001470 | Perplexity: 1933.910049
Trainning Epoch:   7%|▋         | 22/330 [2:37:02<36:37:28, 428.08s/it]2025-09-27 09:26:48,571 Stage: Train 0.5 | Epoch: 22 | Iter: 33600 | Total Loss: 0.007407 | Recon Loss: 0.006690 | Commit Loss: 0.001435 | Perplexity: 1927.446772
2025-09-27 09:27:45,092 Stage: Train 0.5 | Epoch: 22 | Iter: 33800 | Total Loss: 0.007310 | Recon Loss: 0.006588 | Commit Loss: 0.001445 | Perplexity: 1935.954066
2025-09-27 09:28:41,441 Stage: Train 0.5 | Epoch: 22 | Iter: 34000 | Total Loss: 0.007642 | Recon Loss: 0.006902 | Commit Loss: 0.001480 | Perplexity: 1927.412848
2025-09-27 09:29:37,923 Stage: Train 0.5 | Epoch: 22 | Iter: 34200 | Total Loss: 0.007203 | Recon Loss: 0.006477 | Commit Loss: 0.001452 | Perplexity: 1933.198148
2025-09-27 09:30:34,201 Stage: Train 0.5 | Epoch: 22 | Iter: 34400 | Total Loss: 0.007440 | Recon Loss: 0.006712 | Commit Loss: 0.001457 | Perplexity: 1936.523870
2025-09-27 09:31:30,297 Stage: Train 0.5 | Epoch: 22 | Iter: 34600 | Total Loss: 0.007508 | Recon Loss: 0.006782 | Commit Loss: 0.001452 | Perplexity: 1931.912874
2025-09-27 09:32:26,337 Stage: Train 0.5 | Epoch: 22 | Iter: 34800 | Total Loss: 0.007738 | Recon Loss: 0.007023 | Commit Loss: 0.001431 | Perplexity: 1930.121755
Trainning Epoch:   7%|▋         | 23/330 [2:44:10<36:29:43, 427.96s/it]2025-09-27 09:33:23,156 Stage: Train 0.5 | Epoch: 23 | Iter: 35000 | Total Loss: 0.007066 | Recon Loss: 0.006338 | Commit Loss: 0.001456 | Perplexity: 1932.496513
2025-09-27 09:34:19,639 Stage: Train 0.5 | Epoch: 23 | Iter: 35200 | Total Loss: 0.007485 | Recon Loss: 0.006763 | Commit Loss: 0.001445 | Perplexity: 1934.112100
2025-09-27 09:35:16,100 Stage: Train 0.5 | Epoch: 23 | Iter: 35400 | Total Loss: 0.007293 | Recon Loss: 0.006558 | Commit Loss: 0.001469 | Perplexity: 1936.847523
2025-09-27 09:36:12,475 Stage: Train 0.5 | Epoch: 23 | Iter: 35600 | Total Loss: 0.007388 | Recon Loss: 0.006670 | Commit Loss: 0.001435 | Perplexity: 1936.703774
2025-09-27 09:37:09,155 Stage: Train 0.5 | Epoch: 23 | Iter: 35800 | Total Loss: 0.007237 | Recon Loss: 0.006514 | Commit Loss: 0.001447 | Perplexity: 1930.895062
2025-09-27 09:38:05,292 Stage: Train 0.5 | Epoch: 23 | Iter: 36000 | Total Loss: 0.007421 | Recon Loss: 0.006708 | Commit Loss: 0.001427 | Perplexity: 1936.551187
2025-09-27 09:39:01,512 Stage: Train 0.5 | Epoch: 23 | Iter: 36200 | Total Loss: 0.007216 | Recon Loss: 0.006498 | Commit Loss: 0.001435 | Perplexity: 1937.389801
2025-09-27 09:39:57,896 Stage: Train 0.5 | Epoch: 23 | Iter: 36400 | Total Loss: 0.007065 | Recon Loss: 0.006345 | Commit Loss: 0.001440 | Perplexity: 1937.063170
Trainning Epoch:   7%|▋         | 24/330 [2:51:18<36:23:23, 428.12s/it]2025-09-27 09:40:54,453 Stage: Train 0.5 | Epoch: 24 | Iter: 36600 | Total Loss: 0.007439 | Recon Loss: 0.006730 | Commit Loss: 0.001419 | Perplexity: 1935.242058
2025-09-27 09:41:50,091 Stage: Train 0.5 | Epoch: 24 | Iter: 36800 | Total Loss: 0.007476 | Recon Loss: 0.006755 | Commit Loss: 0.001442 | Perplexity: 1934.049506
2025-09-27 09:42:45,457 Stage: Train 0.5 | Epoch: 24 | Iter: 37000 | Total Loss: 0.007517 | Recon Loss: 0.006815 | Commit Loss: 0.001403 | Perplexity: 1932.657238
2025-09-27 09:43:41,717 Stage: Train 0.5 | Epoch: 24 | Iter: 37200 | Total Loss: 0.007111 | Recon Loss: 0.006393 | Commit Loss: 0.001436 | Perplexity: 1939.421002
2025-09-27 09:44:37,855 Stage: Train 0.5 | Epoch: 24 | Iter: 37400 | Total Loss: 0.007028 | Recon Loss: 0.006311 | Commit Loss: 0.001434 | Perplexity: 1940.374421
2025-09-27 09:45:34,142 Stage: Train 0.5 | Epoch: 24 | Iter: 37600 | Total Loss: 0.007172 | Recon Loss: 0.006455 | Commit Loss: 0.001434 | Perplexity: 1940.143214
2025-09-27 09:46:30,546 Stage: Train 0.5 | Epoch: 24 | Iter: 37800 | Total Loss: 0.007126 | Recon Loss: 0.006399 | Commit Loss: 0.001454 | Perplexity: 1938.420911
Trainning Epoch:   8%|▊         | 25/330 [2:58:25<36:13:48, 427.63s/it]2025-09-27 09:47:27,376 Stage: Train 0.5 | Epoch: 25 | Iter: 38000 | Total Loss: 0.007311 | Recon Loss: 0.006597 | Commit Loss: 0.001428 | Perplexity: 1932.788254
2025-09-27 09:48:23,901 Stage: Train 0.5 | Epoch: 25 | Iter: 38200 | Total Loss: 0.007071 | Recon Loss: 0.006352 | Commit Loss: 0.001438 | Perplexity: 1940.594044
2025-09-27 09:49:20,493 Stage: Train 0.5 | Epoch: 25 | Iter: 38400 | Total Loss: 0.007130 | Recon Loss: 0.006396 | Commit Loss: 0.001466 | Perplexity: 1936.516060
2025-09-27 09:50:16,632 Stage: Train 0.5 | Epoch: 25 | Iter: 38600 | Total Loss: 0.007748 | Recon Loss: 0.007044 | Commit Loss: 0.001408 | Perplexity: 1935.428727
2025-09-27 09:51:12,912 Stage: Train 0.5 | Epoch: 25 | Iter: 38800 | Total Loss: 0.006919 | Recon Loss: 0.006220 | Commit Loss: 0.001398 | Perplexity: 1934.653940
2025-09-27 09:52:09,355 Stage: Train 0.5 | Epoch: 25 | Iter: 39000 | Total Loss: 0.007097 | Recon Loss: 0.006386 | Commit Loss: 0.001423 | Perplexity: 1936.750876
2025-09-27 09:53:05,831 Stage: Train 0.5 | Epoch: 25 | Iter: 39200 | Total Loss: 0.007109 | Recon Loss: 0.006391 | Commit Loss: 0.001437 | Perplexity: 1945.207256
2025-09-27 09:54:02,210 Stage: Train 0.5 | Epoch: 25 | Iter: 39400 | Total Loss: 0.007090 | Recon Loss: 0.006371 | Commit Loss: 0.001438 | Perplexity: 1937.500059
Trainning Epoch:   8%|▊         | 26/330 [3:05:33<36:08:04, 427.91s/it]2025-09-27 09:54:58,948 Stage: Train 0.5 | Epoch: 26 | Iter: 39600 | Total Loss: 0.007237 | Recon Loss: 0.006517 | Commit Loss: 0.001439 | Perplexity: 1931.761687
2025-09-27 09:55:55,070 Stage: Train 0.5 | Epoch: 26 | Iter: 39800 | Total Loss: 0.007095 | Recon Loss: 0.006399 | Commit Loss: 0.001392 | Perplexity: 1939.632491
2025-09-27 09:56:51,631 Stage: Train 0.5 | Epoch: 26 | Iter: 40000 | Total Loss: 0.007097 | Recon Loss: 0.006378 | Commit Loss: 0.001438 | Perplexity: 1935.578861
2025-09-27 09:56:51,631 Saving model at iteration 40000
2025-09-27 09:56:51,816 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000
2025-09-27 09:56:52,283 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/model.safetensors
2025-09-27 09:56:52,761 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/optimizer.bin
2025-09-27 09:56:52,761 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/scheduler.bin
2025-09-27 09:56:52,761 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/sampler.bin
2025-09-27 09:56:52,762 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/random_states_0.pkl
2025-09-27 09:57:49,813 Stage: Train 0.5 | Epoch: 26 | Iter: 40200 | Total Loss: 0.007190 | Recon Loss: 0.006481 | Commit Loss: 0.001418 | Perplexity: 1937.137737
2025-09-27 09:58:46,210 Stage: Train 0.5 | Epoch: 26 | Iter: 40400 | Total Loss: 0.006915 | Recon Loss: 0.006198 | Commit Loss: 0.001434 | Perplexity: 1946.302661
2025-09-27 09:59:42,555 Stage: Train 0.5 | Epoch: 26 | Iter: 40600 | Total Loss: 0.007142 | Recon Loss: 0.006428 | Commit Loss: 0.001428 | Perplexity: 1940.055040
2025-09-27 10:00:39,072 Stage: Train 0.5 | Epoch: 26 | Iter: 40800 | Total Loss: 0.007015 | Recon Loss: 0.006300 | Commit Loss: 0.001431 | Perplexity: 1937.986033
2025-09-27 10:01:35,333 Stage: Train 0.5 | Epoch: 26 | Iter: 41000 | Total Loss: 0.006956 | Recon Loss: 0.006244 | Commit Loss: 0.001424 | Perplexity: 1943.643499
Trainning Epoch:   8%|▊         | 27/330 [3:12:43<36:04:27, 428.60s/it]2025-09-27 10:02:31,910 Stage: Train 0.5 | Epoch: 27 | Iter: 41200 | Total Loss: 0.006909 | Recon Loss: 0.006200 | Commit Loss: 0.001419 | Perplexity: 1939.044338
2025-09-27 10:03:28,187 Stage: Train 0.5 | Epoch: 27 | Iter: 41400 | Total Loss: 0.007274 | Recon Loss: 0.006573 | Commit Loss: 0.001402 | Perplexity: 1938.392571
2025-09-27 10:04:24,732 Stage: Train 0.5 | Epoch: 27 | Iter: 41600 | Total Loss: 0.006822 | Recon Loss: 0.006097 | Commit Loss: 0.001450 | Perplexity: 1945.624149
2025-09-27 10:05:21,174 Stage: Train 0.5 | Epoch: 27 | Iter: 41800 | Total Loss: 0.006966 | Recon Loss: 0.006266 | Commit Loss: 0.001400 | Perplexity: 1934.059672
2025-09-27 10:06:17,616 Stage: Train 0.5 | Epoch: 27 | Iter: 42000 | Total Loss: 0.007077 | Recon Loss: 0.006364 | Commit Loss: 0.001425 | Perplexity: 1944.501506
2025-09-27 10:07:13,818 Stage: Train 0.5 | Epoch: 27 | Iter: 42200 | Total Loss: 0.006945 | Recon Loss: 0.006238 | Commit Loss: 0.001413 | Perplexity: 1936.746984
2025-09-27 10:08:10,269 Stage: Train 0.5 | Epoch: 27 | Iter: 42400 | Total Loss: 0.006780 | Recon Loss: 0.006066 | Commit Loss: 0.001427 | Perplexity: 1942.726245
Trainning Epoch:   8%|▊         | 28/330 [3:19:52<35:57:06, 428.57s/it]2025-09-27 10:09:06,827 Stage: Train 0.5 | Epoch: 28 | Iter: 42600 | Total Loss: 0.007096 | Recon Loss: 0.006391 | Commit Loss: 0.001410 | Perplexity: 1942.414856
2025-09-27 10:10:03,484 Stage: Train 0.5 | Epoch: 28 | Iter: 42800 | Total Loss: 0.006714 | Recon Loss: 0.006003 | Commit Loss: 0.001421 | Perplexity: 1943.834881
2025-09-27 10:10:59,787 Stage: Train 0.5 | Epoch: 28 | Iter: 43000 | Total Loss: 0.006742 | Recon Loss: 0.006033 | Commit Loss: 0.001419 | Perplexity: 1942.013087
2025-09-27 10:11:56,397 Stage: Train 0.5 | Epoch: 28 | Iter: 43200 | Total Loss: 0.006935 | Recon Loss: 0.006223 | Commit Loss: 0.001422 | Perplexity: 1942.263081
2025-09-27 10:12:52,653 Stage: Train 0.5 | Epoch: 28 | Iter: 43400 | Total Loss: 0.006750 | Recon Loss: 0.006024 | Commit Loss: 0.001452 | Perplexity: 1943.028450
2025-09-27 10:13:48,811 Stage: Train 0.5 | Epoch: 28 | Iter: 43600 | Total Loss: 0.006720 | Recon Loss: 0.006003 | Commit Loss: 0.001434 | Perplexity: 1943.294020
2025-09-27 10:14:45,301 Stage: Train 0.5 | Epoch: 28 | Iter: 43800 | Total Loss: 0.006818 | Recon Loss: 0.006106 | Commit Loss: 0.001423 | Perplexity: 1940.396785
2025-09-27 10:15:41,693 Stage: Train 0.5 | Epoch: 28 | Iter: 44000 | Total Loss: 0.006780 | Recon Loss: 0.006075 | Commit Loss: 0.001409 | Perplexity: 1939.034368
Trainning Epoch:   9%|▉         | 29/330 [3:27:01<35:49:58, 428.57s/it]2025-09-27 10:16:38,278 Stage: Train 0.5 | Epoch: 29 | Iter: 44200 | Total Loss: 0.006608 | Recon Loss: 0.005892 | Commit Loss: 0.001431 | Perplexity: 1943.630266
2025-09-27 10:17:34,657 Stage: Train 0.5 | Epoch: 29 | Iter: 44400 | Total Loss: 0.006890 | Recon Loss: 0.006176 | Commit Loss: 0.001428 | Perplexity: 1943.719325
2025-09-27 10:18:31,086 Stage: Train 0.5 | Epoch: 29 | Iter: 44600 | Total Loss: 0.006736 | Recon Loss: 0.006031 | Commit Loss: 0.001411 | Perplexity: 1942.847153
2025-09-27 10:19:27,327 Stage: Train 0.5 | Epoch: 29 | Iter: 44800 | Total Loss: 0.006839 | Recon Loss: 0.006132 | Commit Loss: 0.001415 | Perplexity: 1940.169562
2025-09-27 10:20:23,744 Stage: Train 0.5 | Epoch: 29 | Iter: 45000 | Total Loss: 0.006653 | Recon Loss: 0.005931 | Commit Loss: 0.001444 | Perplexity: 1945.772828
2025-09-27 10:21:20,244 Stage: Train 0.5 | Epoch: 29 | Iter: 45200 | Total Loss: 0.006646 | Recon Loss: 0.005927 | Commit Loss: 0.001438 | Perplexity: 1944.997617
2025-09-27 10:22:16,767 Stage: Train 0.5 | Epoch: 29 | Iter: 45400 | Total Loss: 0.007223 | Recon Loss: 0.006513 | Commit Loss: 0.001421 | Perplexity: 1938.174780
Trainning Epoch:   9%|▉         | 30/330 [3:34:09<35:43:12, 428.64s/it]2025-09-27 10:23:13,507 Stage: Train 0.5 | Epoch: 30 | Iter: 45600 | Total Loss: 0.006510 | Recon Loss: 0.005806 | Commit Loss: 0.001409 | Perplexity: 1952.041304
2025-09-27 10:24:10,069 Stage: Train 0.5 | Epoch: 30 | Iter: 45800 | Total Loss: 0.006712 | Recon Loss: 0.006010 | Commit Loss: 0.001404 | Perplexity: 1947.042441
2025-09-27 10:25:05,743 Stage: Train 0.5 | Epoch: 30 | Iter: 46000 | Total Loss: 0.006898 | Recon Loss: 0.006189 | Commit Loss: 0.001419 | Perplexity: 1944.067350
2025-09-27 10:26:02,329 Stage: Train 0.5 | Epoch: 30 | Iter: 46200 | Total Loss: 0.006542 | Recon Loss: 0.005828 | Commit Loss: 0.001427 | Perplexity: 1950.030703
2025-09-27 10:26:58,826 Stage: Train 0.5 | Epoch: 30 | Iter: 46400 | Total Loss: 0.006639 | Recon Loss: 0.005922 | Commit Loss: 0.001433 | Perplexity: 1947.411108
2025-09-27 10:27:55,207 Stage: Train 0.5 | Epoch: 30 | Iter: 46600 | Total Loss: 0.006503 | Recon Loss: 0.005787 | Commit Loss: 0.001433 | Perplexity: 1944.293262
2025-09-27 10:28:51,438 Stage: Train 0.5 | Epoch: 30 | Iter: 46800 | Total Loss: 0.006702 | Recon Loss: 0.005993 | Commit Loss: 0.001418 | Perplexity: 1948.008064
2025-09-27 10:29:47,613 Stage: Train 0.5 | Epoch: 30 | Iter: 47000 | Total Loss: 0.006646 | Recon Loss: 0.005934 | Commit Loss: 0.001425 | Perplexity: 1948.543666
Trainning Epoch:   9%|▉         | 31/330 [3:41:17<35:34:55, 428.41s/it]2025-09-27 10:30:43,838 Stage: Train 0.5 | Epoch: 31 | Iter: 47200 | Total Loss: 0.006504 | Recon Loss: 0.005785 | Commit Loss: 0.001438 | Perplexity: 1950.909774
2025-09-27 10:31:40,174 Stage: Train 0.5 | Epoch: 31 | Iter: 47400 | Total Loss: 0.006832 | Recon Loss: 0.006125 | Commit Loss: 0.001414 | Perplexity: 1941.706448
2025-09-27 10:32:36,581 Stage: Train 0.5 | Epoch: 31 | Iter: 47600 | Total Loss: 0.006760 | Recon Loss: 0.006049 | Commit Loss: 0.001421 | Perplexity: 1947.034703
2025-09-27 10:33:32,706 Stage: Train 0.5 | Epoch: 31 | Iter: 47800 | Total Loss: 0.006377 | Recon Loss: 0.005670 | Commit Loss: 0.001414 | Perplexity: 1948.149136
2025-09-27 10:34:29,199 Stage: Train 0.5 | Epoch: 31 | Iter: 48000 | Total Loss: 0.006612 | Recon Loss: 0.005897 | Commit Loss: 0.001429 | Perplexity: 1945.187208
2025-09-27 10:35:25,670 Stage: Train 0.5 | Epoch: 31 | Iter: 48200 | Total Loss: 0.006794 | Recon Loss: 0.006078 | Commit Loss: 0.001434 | Perplexity: 1944.656501
2025-09-27 10:36:21,915 Stage: Train 0.5 | Epoch: 31 | Iter: 48400 | Total Loss: 0.006467 | Recon Loss: 0.005766 | Commit Loss: 0.001403 | Perplexity: 1947.728253
2025-09-27 10:37:18,516 Stage: Train 0.5 | Epoch: 31 | Iter: 48600 | Total Loss: 0.006812 | Recon Loss: 0.006105 | Commit Loss: 0.001415 | Perplexity: 1949.968176
Trainning Epoch:  10%|▉         | 32/330 [3:48:25<35:27:16, 428.31s/it]2025-09-27 10:38:15,229 Stage: Train 0.5 | Epoch: 32 | Iter: 48800 | Total Loss: 0.006647 | Recon Loss: 0.005932 | Commit Loss: 0.001430 | Perplexity: 1951.912197
2025-09-27 10:39:11,789 Stage: Train 0.5 | Epoch: 32 | Iter: 49000 | Total Loss: 0.006427 | Recon Loss: 0.005726 | Commit Loss: 0.001403 | Perplexity: 1949.523949
2025-09-27 10:40:08,357 Stage: Train 0.5 | Epoch: 32 | Iter: 49200 | Total Loss: 0.006363 | Recon Loss: 0.005652 | Commit Loss: 0.001423 | Perplexity: 1950.467346
2025-09-27 10:41:04,779 Stage: Train 0.5 | Epoch: 32 | Iter: 49400 | Total Loss: 0.006579 | Recon Loss: 0.005867 | Commit Loss: 0.001425 | Perplexity: 1952.936221
2025-09-27 10:42:01,195 Stage: Train 0.5 | Epoch: 32 | Iter: 49600 | Total Loss: 0.006601 | Recon Loss: 0.005896 | Commit Loss: 0.001410 | Perplexity: 1950.101055
2025-09-27 10:42:57,360 Stage: Train 0.5 | Epoch: 32 | Iter: 49800 | Total Loss: 0.006514 | Recon Loss: 0.005799 | Commit Loss: 0.001430 | Perplexity: 1950.192189
2025-09-27 10:43:53,734 Stage: Train 0.5 | Epoch: 32 | Iter: 50000 | Total Loss: 0.006431 | Recon Loss: 0.005717 | Commit Loss: 0.001428 | Perplexity: 1946.371946
Trainning Epoch:  10%|█         | 33/330 [3:55:34<35:20:53, 428.46s/it]2025-09-27 10:44:50,236 Stage: Train 0.5 | Epoch: 33 | Iter: 50200 | Total Loss: 0.006430 | Recon Loss: 0.005718 | Commit Loss: 0.001424 | Perplexity: 1951.092073
2025-09-27 10:45:46,449 Stage: Train 0.5 | Epoch: 33 | Iter: 50400 | Total Loss: 0.006445 | Recon Loss: 0.005731 | Commit Loss: 0.001429 | Perplexity: 1953.631918
2025-09-27 10:46:42,950 Stage: Train 0.5 | Epoch: 33 | Iter: 50600 | Total Loss: 0.006675 | Recon Loss: 0.005969 | Commit Loss: 0.001412 | Perplexity: 1946.988312
2025-09-27 10:47:39,345 Stage: Train 0.5 | Epoch: 33 | Iter: 50800 | Total Loss: 0.006566 | Recon Loss: 0.005860 | Commit Loss: 0.001412 | Perplexity: 1948.203628
2025-09-27 10:48:35,401 Stage: Train 0.5 | Epoch: 33 | Iter: 51000 | Total Loss: 0.006517 | Recon Loss: 0.005797 | Commit Loss: 0.001440 | Perplexity: 1954.724487
2025-09-27 10:49:31,902 Stage: Train 0.5 | Epoch: 33 | Iter: 51200 | Total Loss: 0.006428 | Recon Loss: 0.005724 | Commit Loss: 0.001408 | Perplexity: 1949.339976
2025-09-27 10:50:28,569 Stage: Train 0.5 | Epoch: 33 | Iter: 51400 | Total Loss: 0.006331 | Recon Loss: 0.005620 | Commit Loss: 0.001423 | Perplexity: 1955.820546
2025-09-27 10:51:24,984 Stage: Train 0.5 | Epoch: 33 | Iter: 51600 | Total Loss: 0.006410 | Recon Loss: 0.005700 | Commit Loss: 0.001419 | Perplexity: 1952.272234
Trainning Epoch:  10%|█         | 34/330 [4:02:42<35:13:30, 428.41s/it]2025-09-27 10:52:21,383 Stage: Train 0.5 | Epoch: 34 | Iter: 51800 | Total Loss: 0.006704 | Recon Loss: 0.006004 | Commit Loss: 0.001400 | Perplexity: 1944.942093
2025-09-27 10:53:17,942 Stage: Train 0.5 | Epoch: 34 | Iter: 52000 | Total Loss: 0.006321 | Recon Loss: 0.005609 | Commit Loss: 0.001424 | Perplexity: 1954.929022
2025-09-27 10:54:13,971 Stage: Train 0.5 | Epoch: 34 | Iter: 52200 | Total Loss: 0.006415 | Recon Loss: 0.005706 | Commit Loss: 0.001417 | Perplexity: 1947.456662
2025-09-27 10:55:10,405 Stage: Train 0.5 | Epoch: 34 | Iter: 52400 | Total Loss: 0.006330 | Recon Loss: 0.005615 | Commit Loss: 0.001431 | Perplexity: 1961.407868
2025-09-27 10:56:06,805 Stage: Train 0.5 | Epoch: 34 | Iter: 52600 | Total Loss: 0.006329 | Recon Loss: 0.005626 | Commit Loss: 0.001406 | Perplexity: 1954.713615
2025-09-27 10:57:03,260 Stage: Train 0.5 | Epoch: 34 | Iter: 52800 | Total Loss: 0.006435 | Recon Loss: 0.005725 | Commit Loss: 0.001419 | Perplexity: 1948.478436
2025-09-27 10:57:59,546 Stage: Train 0.5 | Epoch: 34 | Iter: 53000 | Total Loss: 0.006424 | Recon Loss: 0.005714 | Commit Loss: 0.001419 | Perplexity: 1948.109869
Trainning Epoch:  11%|█         | 35/330 [4:09:49<35:04:15, 427.99s/it]2025-09-27 10:58:54,979 Stage: Train 0.5 | Epoch: 35 | Iter: 53200 | Total Loss: 0.006347 | Recon Loss: 0.005639 | Commit Loss: 0.001416 | Perplexity: 1948.604615
2025-09-27 10:59:50,885 Stage: Train 0.5 | Epoch: 35 | Iter: 53400 | Total Loss: 0.006446 | Recon Loss: 0.005744 | Commit Loss: 0.001405 | Perplexity: 1947.322064
2025-09-27 11:00:47,443 Stage: Train 0.5 | Epoch: 35 | Iter: 53600 | Total Loss: 0.006417 | Recon Loss: 0.005713 | Commit Loss: 0.001407 | Perplexity: 1952.073536
2025-09-27 11:01:43,890 Stage: Train 0.5 | Epoch: 35 | Iter: 53800 | Total Loss: 0.006308 | Recon Loss: 0.005595 | Commit Loss: 0.001425 | Perplexity: 1958.753073
2025-09-27 11:02:40,277 Stage: Train 0.5 | Epoch: 35 | Iter: 54000 | Total Loss: 0.006643 | Recon Loss: 0.005940 | Commit Loss: 0.001406 | Perplexity: 1949.631846
2025-09-27 11:03:36,745 Stage: Train 0.5 | Epoch: 35 | Iter: 54200 | Total Loss: 0.006213 | Recon Loss: 0.005507 | Commit Loss: 0.001412 | Perplexity: 1956.585873
2025-09-27 11:04:33,139 Stage: Train 0.5 | Epoch: 35 | Iter: 54400 | Total Loss: 0.006322 | Recon Loss: 0.005611 | Commit Loss: 0.001421 | Perplexity: 1958.369051
2025-09-27 11:05:29,323 Stage: Train 0.5 | Epoch: 35 | Iter: 54600 | Total Loss: 0.006392 | Recon Loss: 0.005686 | Commit Loss: 0.001413 | Perplexity: 1957.114589
Trainning Epoch:  11%|█         | 36/330 [4:16:58<34:57:18, 428.02s/it]2025-09-27 11:06:25,766 Stage: Train 0.5 | Epoch: 36 | Iter: 54800 | Total Loss: 0.006334 | Recon Loss: 0.005632 | Commit Loss: 0.001402 | Perplexity: 1947.508907
2025-09-27 11:07:22,065 Stage: Train 0.5 | Epoch: 36 | Iter: 55000 | Total Loss: 0.006202 | Recon Loss: 0.005495 | Commit Loss: 0.001415 | Perplexity: 1951.162412
2025-09-27 11:08:18,584 Stage: Train 0.5 | Epoch: 36 | Iter: 55200 | Total Loss: 0.006218 | Recon Loss: 0.005505 | Commit Loss: 0.001426 | Perplexity: 1960.184638
2025-09-27 11:09:14,812 Stage: Train 0.5 | Epoch: 36 | Iter: 55400 | Total Loss: 0.006430 | Recon Loss: 0.005731 | Commit Loss: 0.001398 | Perplexity: 1952.095693
2025-09-27 11:10:11,280 Stage: Train 0.5 | Epoch: 36 | Iter: 55600 | Total Loss: 0.006248 | Recon Loss: 0.005542 | Commit Loss: 0.001411 | Perplexity: 1957.229551
2025-09-27 11:11:07,818 Stage: Train 0.5 | Epoch: 36 | Iter: 55800 | Total Loss: 0.006287 | Recon Loss: 0.005568 | Commit Loss: 0.001438 | Perplexity: 1960.172377
2025-09-27 11:12:03,946 Stage: Train 0.5 | Epoch: 36 | Iter: 56000 | Total Loss: 0.006438 | Recon Loss: 0.005730 | Commit Loss: 0.001415 | Perplexity: 1955.021252
2025-09-27 11:13:00,466 Stage: Train 0.5 | Epoch: 36 | Iter: 56200 | Total Loss: 0.006110 | Recon Loss: 0.005395 | Commit Loss: 0.001431 | Perplexity: 1962.776075
Trainning Epoch:  11%|█         | 37/330 [4:24:06<34:50:33, 428.10s/it]2025-09-27 11:13:57,085 Stage: Train 0.5 | Epoch: 37 | Iter: 56400 | Total Loss: 0.006354 | Recon Loss: 0.005646 | Commit Loss: 0.001415 | Perplexity: 1957.291036
2025-09-27 11:14:53,201 Stage: Train 0.5 | Epoch: 37 | Iter: 56600 | Total Loss: 0.006444 | Recon Loss: 0.005744 | Commit Loss: 0.001400 | Perplexity: 1954.554787
2025-09-27 11:15:49,563 Stage: Train 0.5 | Epoch: 37 | Iter: 56800 | Total Loss: 0.006177 | Recon Loss: 0.005469 | Commit Loss: 0.001415 | Perplexity: 1955.949320
2025-09-27 11:16:46,029 Stage: Train 0.5 | Epoch: 37 | Iter: 57000 | Total Loss: 0.006298 | Recon Loss: 0.005585 | Commit Loss: 0.001425 | Perplexity: 1955.473159
2025-09-27 11:17:42,121 Stage: Train 0.5 | Epoch: 37 | Iter: 57200 | Total Loss: 0.006096 | Recon Loss: 0.005396 | Commit Loss: 0.001402 | Perplexity: 1954.504294
2025-09-27 11:18:38,490 Stage: Train 0.5 | Epoch: 37 | Iter: 57400 | Total Loss: 0.006279 | Recon Loss: 0.005564 | Commit Loss: 0.001431 | Perplexity: 1959.791172
2025-09-27 11:19:34,758 Stage: Train 0.5 | Epoch: 37 | Iter: 57600 | Total Loss: 0.006294 | Recon Loss: 0.005591 | Commit Loss: 0.001405 | Perplexity: 1952.928993
Trainning Epoch:  12%|█▏        | 38/330 [4:31:14<34:42:55, 428.00s/it]2025-09-27 11:20:31,212 Stage: Train 0.5 | Epoch: 38 | Iter: 57800 | Total Loss: 0.006322 | Recon Loss: 0.005612 | Commit Loss: 0.001419 | Perplexity: 1951.863453
2025-09-27 11:21:27,762 Stage: Train 0.5 | Epoch: 38 | Iter: 58000 | Total Loss: 0.006104 | Recon Loss: 0.005397 | Commit Loss: 0.001414 | Perplexity: 1957.285099
2025-09-27 11:22:24,155 Stage: Train 0.5 | Epoch: 38 | Iter: 58200 | Total Loss: 0.006485 | Recon Loss: 0.005789 | Commit Loss: 0.001392 | Perplexity: 1954.707628
2025-09-27 11:23:20,184 Stage: Train 0.5 | Epoch: 38 | Iter: 58400 | Total Loss: 0.006128 | Recon Loss: 0.005423 | Commit Loss: 0.001410 | Perplexity: 1955.335674
2025-09-27 11:24:16,665 Stage: Train 0.5 | Epoch: 38 | Iter: 58600 | Total Loss: 0.006254 | Recon Loss: 0.005555 | Commit Loss: 0.001397 | Perplexity: 1956.768197
2025-09-27 11:25:13,123 Stage: Train 0.5 | Epoch: 38 | Iter: 58800 | Total Loss: 0.006081 | Recon Loss: 0.005376 | Commit Loss: 0.001410 | Perplexity: 1957.262200
2025-09-27 11:26:09,337 Stage: Train 0.5 | Epoch: 38 | Iter: 59000 | Total Loss: 0.006294 | Recon Loss: 0.005604 | Commit Loss: 0.001382 | Perplexity: 1953.345050
2025-09-27 11:27:05,688 Stage: Train 0.5 | Epoch: 38 | Iter: 59200 | Total Loss: 0.006156 | Recon Loss: 0.005444 | Commit Loss: 0.001423 | Perplexity: 1962.355265
Trainning Epoch:  12%|█▏        | 39/330 [4:38:22<34:36:08, 428.07s/it]2025-09-27 11:28:02,130 Stage: Train 0.5 | Epoch: 39 | Iter: 59400 | Total Loss: 0.006046 | Recon Loss: 0.005333 | Commit Loss: 0.001425 | Perplexity: 1959.126368
2025-09-27 11:28:58,124 Stage: Train 0.5 | Epoch: 39 | Iter: 59600 | Total Loss: 0.006112 | Recon Loss: 0.005400 | Commit Loss: 0.001425 | Perplexity: 1961.944598
2025-09-27 11:29:55,166 Stage: Train 0.5 | Epoch: 39 | Iter: 59800 | Total Loss: 0.006272 | Recon Loss: 0.005575 | Commit Loss: 0.001393 | Perplexity: 1958.268036
2025-09-27 11:30:51,543 Stage: Train 0.5 | Epoch: 39 | Iter: 60000 | Total Loss: 0.006278 | Recon Loss: 0.005581 | Commit Loss: 0.001392 | Perplexity: 1954.913475
2025-09-27 11:30:51,544 Saving model at iteration 60000
2025-09-27 11:30:51,758 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000
2025-09-27 11:30:52,267 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/model.safetensors
2025-09-27 11:30:52,792 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/optimizer.bin
2025-09-27 11:30:52,792 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/scheduler.bin
2025-09-27 11:30:52,792 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/sampler.bin
2025-09-27 11:30:52,793 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/random_states_0.pkl
2025-09-27 11:31:49,102 Stage: Train 0.5 | Epoch: 39 | Iter: 60200 | Total Loss: 0.005957 | Recon Loss: 0.005250 | Commit Loss: 0.001414 | Perplexity: 1962.825586
2025-09-27 11:32:45,495 Stage: Train 0.5 | Epoch: 39 | Iter: 60400 | Total Loss: 0.006095 | Recon Loss: 0.005390 | Commit Loss: 0.001410 | Perplexity: 1958.864270
2025-09-27 11:33:41,836 Stage: Train 0.5 | Epoch: 39 | Iter: 60600 | Total Loss: 0.006046 | Recon Loss: 0.005342 | Commit Loss: 0.001408 | Perplexity: 1958.525359
Trainning Epoch:  12%|█▏        | 40/330 [4:45:31<34:31:19, 428.55s/it]2025-09-27 11:34:38,434 Stage: Train 0.5 | Epoch: 40 | Iter: 60800 | Total Loss: 0.006067 | Recon Loss: 0.005364 | Commit Loss: 0.001405 | Perplexity: 1956.791917
2025-09-27 11:35:34,354 Stage: Train 0.5 | Epoch: 40 | Iter: 61000 | Total Loss: 0.006152 | Recon Loss: 0.005451 | Commit Loss: 0.001402 | Perplexity: 1956.509500
2025-09-27 11:36:30,432 Stage: Train 0.5 | Epoch: 40 | Iter: 61200 | Total Loss: 0.006118 | Recon Loss: 0.005412 | Commit Loss: 0.001413 | Perplexity: 1959.505840
2025-09-27 11:37:26,806 Stage: Train 0.5 | Epoch: 40 | Iter: 61400 | Total Loss: 0.006067 | Recon Loss: 0.005365 | Commit Loss: 0.001405 | Perplexity: 1959.882557
2025-09-27 11:38:23,190 Stage: Train 0.5 | Epoch: 40 | Iter: 61600 | Total Loss: 0.006063 | Recon Loss: 0.005364 | Commit Loss: 0.001397 | Perplexity: 1962.113029
2025-09-27 11:39:19,426 Stage: Train 0.5 | Epoch: 40 | Iter: 61800 | Total Loss: 0.006150 | Recon Loss: 0.005441 | Commit Loss: 0.001418 | Perplexity: 1969.202030
2025-09-27 11:40:15,667 Stage: Train 0.5 | Epoch: 40 | Iter: 62000 | Total Loss: 0.005889 | Recon Loss: 0.005182 | Commit Loss: 0.001414 | Perplexity: 1961.685020
2025-09-27 11:41:11,687 Stage: Train 0.5 | Epoch: 40 | Iter: 62200 | Total Loss: 0.005939 | Recon Loss: 0.005234 | Commit Loss: 0.001409 | Perplexity: 1961.596270
Trainning Epoch:  12%|█▏        | 41/330 [4:52:38<34:21:52, 428.07s/it]2025-09-27 11:42:08,195 Stage: Train 0.5 | Epoch: 41 | Iter: 62400 | Total Loss: 0.006120 | Recon Loss: 0.005424 | Commit Loss: 0.001394 | Perplexity: 1959.493922
2025-09-27 11:43:04,486 Stage: Train 0.5 | Epoch: 41 | Iter: 62600 | Total Loss: 0.006208 | Recon Loss: 0.005502 | Commit Loss: 0.001412 | Perplexity: 1966.477535
2025-09-27 11:44:00,674 Stage: Train 0.5 | Epoch: 41 | Iter: 62800 | Total Loss: 0.006033 | Recon Loss: 0.005318 | Commit Loss: 0.001431 | Perplexity: 1964.007972
2025-09-27 11:44:56,956 Stage: Train 0.5 | Epoch: 41 | Iter: 63000 | Total Loss: 0.005936 | Recon Loss: 0.005233 | Commit Loss: 0.001406 | Perplexity: 1961.741437
2025-09-27 11:45:53,404 Stage: Train 0.5 | Epoch: 41 | Iter: 63200 | Total Loss: 0.005949 | Recon Loss: 0.005240 | Commit Loss: 0.001418 | Perplexity: 1965.421093
2025-09-27 11:46:49,317 Stage: Train 0.5 | Epoch: 41 | Iter: 63400 | Total Loss: 0.006018 | Recon Loss: 0.005307 | Commit Loss: 0.001422 | Perplexity: 1966.400325
2025-09-27 11:47:45,763 Stage: Train 0.5 | Epoch: 41 | Iter: 63600 | Total Loss: 0.006131 | Recon Loss: 0.005435 | Commit Loss: 0.001392 | Perplexity: 1958.233830
Trainning Epoch:  13%|█▎        | 42/330 [4:59:46<34:13:56, 427.91s/it]2025-09-27 11:48:42,171 Stage: Train 0.5 | Epoch: 42 | Iter: 63800 | Total Loss: 0.005951 | Recon Loss: 0.005248 | Commit Loss: 0.001406 | Perplexity: 1959.077150
2025-09-27 11:49:38,565 Stage: Train 0.5 | Epoch: 42 | Iter: 64000 | Total Loss: 0.005972 | Recon Loss: 0.005270 | Commit Loss: 0.001403 | Perplexity: 1960.692783
2025-09-27 11:50:34,753 Stage: Train 0.5 | Epoch: 42 | Iter: 64200 | Total Loss: 0.005874 | Recon Loss: 0.005166 | Commit Loss: 0.001415 | Perplexity: 1968.438226
2025-09-27 11:51:31,274 Stage: Train 0.5 | Epoch: 42 | Iter: 64400 | Total Loss: 0.006034 | Recon Loss: 0.005328 | Commit Loss: 0.001412 | Perplexity: 1961.507337
2025-09-27 11:52:27,362 Stage: Train 0.5 | Epoch: 42 | Iter: 64600 | Total Loss: 0.006065 | Recon Loss: 0.005358 | Commit Loss: 0.001414 | Perplexity: 1967.041980
2025-09-27 11:53:23,797 Stage: Train 0.5 | Epoch: 42 | Iter: 64800 | Total Loss: 0.005998 | Recon Loss: 0.005288 | Commit Loss: 0.001419 | Perplexity: 1964.544310
2025-09-27 11:54:20,196 Stage: Train 0.5 | Epoch: 42 | Iter: 65000 | Total Loss: 0.006035 | Recon Loss: 0.005334 | Commit Loss: 0.001401 | Perplexity: 1965.241656
2025-09-27 11:55:16,716 Stage: Train 0.5 | Epoch: 42 | Iter: 65200 | Total Loss: 0.006001 | Recon Loss: 0.005310 | Commit Loss: 0.001382 | Perplexity: 1967.671754
Trainning Epoch:  13%|█▎        | 43/330 [5:06:54<34:07:31, 428.05s/it]2025-09-27 11:56:13,417 Stage: Train 0.5 | Epoch: 43 | Iter: 65400 | Total Loss: 0.005825 | Recon Loss: 0.005120 | Commit Loss: 0.001409 | Perplexity: 1959.501048
2025-09-27 11:57:09,750 Stage: Train 0.5 | Epoch: 43 | Iter: 65600 | Total Loss: 0.006097 | Recon Loss: 0.005401 | Commit Loss: 0.001392 | Perplexity: 1962.300659
2025-09-27 11:58:05,602 Stage: Train 0.5 | Epoch: 43 | Iter: 65800 | Total Loss: 0.005914 | Recon Loss: 0.005215 | Commit Loss: 0.001397 | Perplexity: 1962.783975
2025-09-27 11:59:01,797 Stage: Train 0.5 | Epoch: 43 | Iter: 66000 | Total Loss: 0.006008 | Recon Loss: 0.005302 | Commit Loss: 0.001412 | Perplexity: 1963.425591
2025-09-27 11:59:58,175 Stage: Train 0.5 | Epoch: 43 | Iter: 66200 | Total Loss: 0.005888 | Recon Loss: 0.005195 | Commit Loss: 0.001386 | Perplexity: 1964.385679
2025-09-27 12:00:54,265 Stage: Train 0.5 | Epoch: 43 | Iter: 66400 | Total Loss: 0.006009 | Recon Loss: 0.005310 | Commit Loss: 0.001398 | Perplexity: 1964.802241
2025-09-27 12:01:50,602 Stage: Train 0.5 | Epoch: 43 | Iter: 66600 | Total Loss: 0.005933 | Recon Loss: 0.005231 | Commit Loss: 0.001403 | Perplexity: 1966.164285
2025-09-27 12:02:47,127 Stage: Train 0.5 | Epoch: 43 | Iter: 66800 | Total Loss: 0.005889 | Recon Loss: 0.005180 | Commit Loss: 0.001418 | Perplexity: 1972.205142
Trainning Epoch:  13%|█▎        | 44/330 [5:14:02<33:59:27, 427.86s/it]2025-09-27 12:03:43,666 Stage: Train 0.5 | Epoch: 44 | Iter: 67000 | Total Loss: 0.005954 | Recon Loss: 0.005252 | Commit Loss: 0.001404 | Perplexity: 1960.032028
2025-09-27 12:04:39,613 Stage: Train 0.5 | Epoch: 44 | Iter: 67200 | Total Loss: 0.005900 | Recon Loss: 0.005203 | Commit Loss: 0.001394 | Perplexity: 1958.725545
2025-09-27 12:05:35,780 Stage: Train 0.5 | Epoch: 44 | Iter: 67400 | Total Loss: 0.005940 | Recon Loss: 0.005236 | Commit Loss: 0.001409 | Perplexity: 1968.571727
2025-09-27 12:06:32,044 Stage: Train 0.5 | Epoch: 44 | Iter: 67600 | Total Loss: 0.005859 | Recon Loss: 0.005155 | Commit Loss: 0.001409 | Perplexity: 1970.762488
2025-09-27 12:07:28,508 Stage: Train 0.5 | Epoch: 44 | Iter: 67800 | Total Loss: 0.005931 | Recon Loss: 0.005234 | Commit Loss: 0.001392 | Perplexity: 1966.203397
2025-09-27 12:08:24,819 Stage: Train 0.5 | Epoch: 44 | Iter: 68000 | Total Loss: 0.005843 | Recon Loss: 0.005132 | Commit Loss: 0.001422 | Perplexity: 1966.913804
2025-09-27 12:09:20,994 Stage: Train 0.5 | Epoch: 44 | Iter: 68200 | Total Loss: 0.005786 | Recon Loss: 0.005091 | Commit Loss: 0.001389 | Perplexity: 1966.851851
Trainning Epoch:  14%|█▎        | 45/330 [5:21:09<33:51:19, 427.65s/it]2025-09-27 12:10:17,200 Stage: Train 0.5 | Epoch: 45 | Iter: 68400 | Total Loss: 0.006147 | Recon Loss: 0.005444 | Commit Loss: 0.001406 | Perplexity: 1963.059262
2025-09-27 12:11:13,452 Stage: Train 0.5 | Epoch: 45 | Iter: 68600 | Total Loss: 0.005700 | Recon Loss: 0.004998 | Commit Loss: 0.001404 | Perplexity: 1965.150491
2025-09-27 12:12:09,807 Stage: Train 0.5 | Epoch: 45 | Iter: 68800 | Total Loss: 0.005978 | Recon Loss: 0.005286 | Commit Loss: 0.001382 | Perplexity: 1968.005855
2025-09-27 12:13:06,162 Stage: Train 0.5 | Epoch: 45 | Iter: 69000 | Total Loss: 0.005901 | Recon Loss: 0.005210 | Commit Loss: 0.001383 | Perplexity: 1961.920057
2025-09-27 12:14:02,584 Stage: Train 0.5 | Epoch: 45 | Iter: 69200 | Total Loss: 0.005597 | Recon Loss: 0.004896 | Commit Loss: 0.001402 | Perplexity: 1971.131635
2025-09-27 12:14:58,912 Stage: Train 0.5 | Epoch: 45 | Iter: 69400 | Total Loss: 0.006097 | Recon Loss: 0.005413 | Commit Loss: 0.001368 | Perplexity: 1964.193098
2025-09-27 12:15:54,038 Stage: Train 0.5 | Epoch: 45 | Iter: 69600 | Total Loss: 0.005956 | Recon Loss: 0.005259 | Commit Loss: 0.001395 | Perplexity: 1963.689095
2025-09-27 12:16:50,209 Stage: Train 0.5 | Epoch: 45 | Iter: 69800 | Total Loss: 0.005666 | Recon Loss: 0.004959 | Commit Loss: 0.001414 | Perplexity: 1973.756623
Trainning Epoch:  14%|█▍        | 46/330 [5:28:16<33:42:46, 427.35s/it]2025-09-27 12:17:46,744 Stage: Train 0.5 | Epoch: 46 | Iter: 70000 | Total Loss: 0.005947 | Recon Loss: 0.005247 | Commit Loss: 0.001400 | Perplexity: 1966.313997
2025-09-27 12:18:43,095 Stage: Train 0.5 | Epoch: 46 | Iter: 70200 | Total Loss: 0.005711 | Recon Loss: 0.005018 | Commit Loss: 0.001386 | Perplexity: 1969.603296
2025-09-27 12:19:39,410 Stage: Train 0.5 | Epoch: 46 | Iter: 70400 | Total Loss: 0.005788 | Recon Loss: 0.005086 | Commit Loss: 0.001404 | Perplexity: 1971.091588
2025-09-27 12:20:35,762 Stage: Train 0.5 | Epoch: 46 | Iter: 70600 | Total Loss: 0.005689 | Recon Loss: 0.004981 | Commit Loss: 0.001416 | Perplexity: 1966.031036
2025-09-27 12:21:31,534 Stage: Train 0.5 | Epoch: 46 | Iter: 70800 | Total Loss: 0.005937 | Recon Loss: 0.005241 | Commit Loss: 0.001391 | Perplexity: 1962.148802
2025-09-27 12:22:28,058 Stage: Train 0.5 | Epoch: 46 | Iter: 71000 | Total Loss: 0.005793 | Recon Loss: 0.005088 | Commit Loss: 0.001410 | Perplexity: 1973.114564
2025-09-27 12:23:24,365 Stage: Train 0.5 | Epoch: 46 | Iter: 71200 | Total Loss: 0.005847 | Recon Loss: 0.005147 | Commit Loss: 0.001401 | Perplexity: 1964.003389
Trainning Epoch:  14%|█▍        | 47/330 [5:35:23<33:36:13, 427.47s/it]2025-09-27 12:24:20,958 Stage: Train 0.5 | Epoch: 47 | Iter: 71400 | Total Loss: 0.005595 | Recon Loss: 0.004902 | Commit Loss: 0.001386 | Perplexity: 1968.976334
2025-09-27 12:25:17,330 Stage: Train 0.5 | Epoch: 47 | Iter: 71600 | Total Loss: 0.006036 | Recon Loss: 0.005335 | Commit Loss: 0.001403 | Perplexity: 1965.309183
2025-09-27 12:26:13,710 Stage: Train 0.5 | Epoch: 47 | Iter: 71800 | Total Loss: 0.005731 | Recon Loss: 0.005029 | Commit Loss: 0.001404 | Perplexity: 1964.734108
2025-09-27 12:27:10,136 Stage: Train 0.5 | Epoch: 47 | Iter: 72000 | Total Loss: 0.005705 | Recon Loss: 0.005017 | Commit Loss: 0.001376 | Perplexity: 1963.641640
2025-09-27 12:28:06,212 Stage: Train 0.5 | Epoch: 47 | Iter: 72200 | Total Loss: 0.006198 | Recon Loss: 0.005505 | Commit Loss: 0.001385 | Perplexity: 1966.024164
2025-09-27 12:29:02,738 Stage: Train 0.5 | Epoch: 47 | Iter: 72400 | Total Loss: 0.005956 | Recon Loss: 0.005269 | Commit Loss: 0.001373 | Perplexity: 1966.643159
2025-09-27 12:29:59,154 Stage: Train 0.5 | Epoch: 47 | Iter: 72600 | Total Loss: 0.005540 | Recon Loss: 0.004841 | Commit Loss: 0.001397 | Perplexity: 1964.988707
2025-09-27 12:30:55,762 Stage: Train 0.5 | Epoch: 47 | Iter: 72800 | Total Loss: 0.005927 | Recon Loss: 0.005226 | Commit Loss: 0.001401 | Perplexity: 1974.525119
Trainning Epoch:  15%|█▍        | 48/330 [5:42:32<33:30:34, 427.78s/it]2025-09-27 12:31:52,232 Stage: Train 0.5 | Epoch: 48 | Iter: 73000 | Total Loss: 0.005703 | Recon Loss: 0.005013 | Commit Loss: 0.001381 | Perplexity: 1965.539957
2025-09-27 12:32:48,836 Stage: Train 0.5 | Epoch: 48 | Iter: 73200 | Total Loss: 0.005937 | Recon Loss: 0.005249 | Commit Loss: 0.001376 | Perplexity: 1966.861368
2025-09-27 12:33:44,820 Stage: Train 0.5 | Epoch: 48 | Iter: 73400 | Total Loss: 0.005763 | Recon Loss: 0.005064 | Commit Loss: 0.001398 | Perplexity: 1970.182983
2025-09-27 12:34:41,318 Stage: Train 0.5 | Epoch: 48 | Iter: 73600 | Total Loss: 0.005848 | Recon Loss: 0.005153 | Commit Loss: 0.001390 | Perplexity: 1965.481328
2025-09-27 12:35:37,994 Stage: Train 0.5 | Epoch: 48 | Iter: 73800 | Total Loss: 0.005618 | Recon Loss: 0.004924 | Commit Loss: 0.001388 | Perplexity: 1971.020405
2025-09-27 12:36:34,333 Stage: Train 0.5 | Epoch: 48 | Iter: 74000 | Total Loss: 0.005857 | Recon Loss: 0.005167 | Commit Loss: 0.001381 | Perplexity: 1962.225587
2025-09-27 12:37:30,674 Stage: Train 0.5 | Epoch: 48 | Iter: 74200 | Total Loss: 0.005742 | Recon Loss: 0.005048 | Commit Loss: 0.001388 | Perplexity: 1969.370208
2025-09-27 12:38:27,217 Stage: Train 0.5 | Epoch: 48 | Iter: 74400 | Total Loss: 0.005780 | Recon Loss: 0.005092 | Commit Loss: 0.001376 | Perplexity: 1967.798648
Trainning Epoch:  15%|█▍        | 49/330 [5:49:40<33:24:41, 428.05s/it]2025-09-27 12:39:23,617 Stage: Train 0.5 | Epoch: 49 | Iter: 74600 | Total Loss: 0.005715 | Recon Loss: 0.005018 | Commit Loss: 0.001394 | Perplexity: 1975.160538
2025-09-27 12:40:20,058 Stage: Train 0.5 | Epoch: 49 | Iter: 74800 | Total Loss: 0.005594 | Recon Loss: 0.004906 | Commit Loss: 0.001377 | Perplexity: 1967.502213
2025-09-27 12:41:16,381 Stage: Train 0.5 | Epoch: 49 | Iter: 75000 | Total Loss: 0.005684 | Recon Loss: 0.004987 | Commit Loss: 0.001394 | Perplexity: 1970.921125
2025-09-27 12:42:12,925 Stage: Train 0.5 | Epoch: 49 | Iter: 75200 | Total Loss: 0.005658 | Recon Loss: 0.004967 | Commit Loss: 0.001382 | Perplexity: 1969.394974
2025-09-27 12:43:09,373 Stage: Train 0.5 | Epoch: 49 | Iter: 75400 | Total Loss: 0.005733 | Recon Loss: 0.005044 | Commit Loss: 0.001377 | Perplexity: 1967.900115
2025-09-27 12:44:05,859 Stage: Train 0.5 | Epoch: 49 | Iter: 75600 | Total Loss: 0.005651 | Recon Loss: 0.004958 | Commit Loss: 0.001386 | Perplexity: 1965.889787
2025-09-27 12:45:01,791 Stage: Train 0.5 | Epoch: 49 | Iter: 75800 | Total Loss: 0.005946 | Recon Loss: 0.005243 | Commit Loss: 0.001406 | Perplexity: 1965.920927
Trainning Epoch:  15%|█▌        | 50/330 [5:56:49<33:17:32, 428.05s/it]2025-09-27 12:45:58,277 Stage: Train 0.5 | Epoch: 50 | Iter: 76000 | Total Loss: 0.005626 | Recon Loss: 0.004926 | Commit Loss: 0.001399 | Perplexity: 1966.564246
2025-09-27 12:46:54,593 Stage: Train 0.5 | Epoch: 50 | Iter: 76200 | Total Loss: 0.005709 | Recon Loss: 0.005018 | Commit Loss: 0.001382 | Perplexity: 1968.118483
2025-09-27 12:47:50,890 Stage: Train 0.5 | Epoch: 50 | Iter: 76400 | Total Loss: 0.005874 | Recon Loss: 0.005190 | Commit Loss: 0.001367 | Perplexity: 1961.775928
2025-09-27 12:48:47,146 Stage: Train 0.5 | Epoch: 50 | Iter: 76600 | Total Loss: 0.005578 | Recon Loss: 0.004885 | Commit Loss: 0.001386 | Perplexity: 1964.462255
2025-09-27 12:49:43,788 Stage: Train 0.5 | Epoch: 50 | Iter: 76800 | Total Loss: 0.005618 | Recon Loss: 0.004924 | Commit Loss: 0.001388 | Perplexity: 1978.254583
2025-09-27 12:50:39,702 Stage: Train 0.5 | Epoch: 50 | Iter: 77000 | Total Loss: 0.005651 | Recon Loss: 0.004960 | Commit Loss: 0.001381 | Perplexity: 1965.277421
2025-09-27 12:51:36,161 Stage: Train 0.5 | Epoch: 50 | Iter: 77200 | Total Loss: 0.005546 | Recon Loss: 0.004852 | Commit Loss: 0.001388 | Perplexity: 1972.754860
2025-09-27 12:52:32,690 Stage: Train 0.5 | Epoch: 50 | Iter: 77400 | Total Loss: 0.005729 | Recon Loss: 0.005037 | Commit Loss: 0.001384 | Perplexity: 1967.387068
Trainning Epoch:  15%|█▌        | 51/330 [6:03:57<33:10:27, 428.05s/it]2025-09-27 12:53:29,218 Stage: Train 0.5 | Epoch: 51 | Iter: 77600 | Total Loss: 0.005708 | Recon Loss: 0.005017 | Commit Loss: 0.001383 | Perplexity: 1965.839196
2025-09-27 12:54:25,546 Stage: Train 0.5 | Epoch: 51 | Iter: 77800 | Total Loss: 0.005673 | Recon Loss: 0.004983 | Commit Loss: 0.001378 | Perplexity: 1967.993983
2025-09-27 12:55:22,060 Stage: Train 0.5 | Epoch: 51 | Iter: 78000 | Total Loss: 0.005541 | Recon Loss: 0.004853 | Commit Loss: 0.001375 | Perplexity: 1963.336762
2025-09-27 12:56:18,398 Stage: Train 0.5 | Epoch: 51 | Iter: 78200 | Total Loss: 0.005623 | Recon Loss: 0.004932 | Commit Loss: 0.001382 | Perplexity: 1973.089179
2025-09-27 12:57:14,483 Stage: Train 0.5 | Epoch: 51 | Iter: 78400 | Total Loss: 0.005569 | Recon Loss: 0.004873 | Commit Loss: 0.001392 | Perplexity: 1974.351205
2025-09-27 12:58:10,715 Stage: Train 0.5 | Epoch: 51 | Iter: 78600 | Total Loss: 0.005551 | Recon Loss: 0.004858 | Commit Loss: 0.001385 | Perplexity: 1971.458824
2025-09-27 12:59:06,937 Stage: Train 0.5 | Epoch: 51 | Iter: 78800 | Total Loss: 0.005591 | Recon Loss: 0.004897 | Commit Loss: 0.001388 | Perplexity: 1970.079603
Trainning Epoch:  16%|█▌        | 52/330 [6:11:04<33:03:04, 428.00s/it]2025-09-27 13:00:03,571 Stage: Train 0.5 | Epoch: 52 | Iter: 79000 | Total Loss: 0.005518 | Recon Loss: 0.004827 | Commit Loss: 0.001381 | Perplexity: 1966.000494
2025-09-27 13:00:59,743 Stage: Train 0.5 | Epoch: 52 | Iter: 79200 | Total Loss: 0.005784 | Recon Loss: 0.005101 | Commit Loss: 0.001366 | Perplexity: 1967.558502
2025-09-27 13:01:56,304 Stage: Train 0.5 | Epoch: 52 | Iter: 79400 | Total Loss: 0.005601 | Recon Loss: 0.004905 | Commit Loss: 0.001392 | Perplexity: 1969.700082
2025-09-27 13:02:52,295 Stage: Train 0.5 | Epoch: 52 | Iter: 79600 | Total Loss: 0.005488 | Recon Loss: 0.004797 | Commit Loss: 0.001382 | Perplexity: 1970.151168
2025-09-27 13:03:48,578 Stage: Train 0.5 | Epoch: 52 | Iter: 79800 | Total Loss: 0.005591 | Recon Loss: 0.004898 | Commit Loss: 0.001387 | Perplexity: 1971.305744
2025-09-27 13:04:44,839 Stage: Train 0.5 | Epoch: 52 | Iter: 80000 | Total Loss: 0.005478 | Recon Loss: 0.004781 | Commit Loss: 0.001394 | Perplexity: 1975.562512
2025-09-27 13:04:44,839 Saving model at iteration 80000
2025-09-27 13:04:45,042 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000
2025-09-27 13:04:45,512 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/model.safetensors
2025-09-27 13:04:46,038 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/optimizer.bin
2025-09-27 13:04:46,038 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/scheduler.bin
2025-09-27 13:04:46,038 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/sampler.bin
2025-09-27 13:04:46,039 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/random_states_0.pkl
2025-09-27 13:05:42,942 Stage: Train 0.5 | Epoch: 52 | Iter: 80200 | Total Loss: 0.005568 | Recon Loss: 0.004875 | Commit Loss: 0.001386 | Perplexity: 1968.882471
2025-09-27 13:06:39,196 Stage: Train 0.5 | Epoch: 52 | Iter: 80400 | Total Loss: 0.005608 | Recon Loss: 0.004916 | Commit Loss: 0.001384 | Perplexity: 1973.703922
Trainning Epoch:  16%|█▌        | 53/330 [6:18:14<32:57:49, 428.41s/it]2025-09-27 13:07:35,714 Stage: Train 0.5 | Epoch: 53 | Iter: 80600 | Total Loss: 0.005513 | Recon Loss: 0.004818 | Commit Loss: 0.001390 | Perplexity: 1970.561785
2025-09-27 13:08:31,704 Stage: Train 0.5 | Epoch: 53 | Iter: 80800 | Total Loss: 0.005745 | Recon Loss: 0.005060 | Commit Loss: 0.001370 | Perplexity: 1970.455952
2025-09-27 13:09:28,009 Stage: Train 0.5 | Epoch: 53 | Iter: 81000 | Total Loss: 0.005500 | Recon Loss: 0.004804 | Commit Loss: 0.001393 | Perplexity: 1974.602940
2025-09-27 13:10:24,442 Stage: Train 0.5 | Epoch: 53 | Iter: 81200 | Total Loss: 0.005644 | Recon Loss: 0.004951 | Commit Loss: 0.001386 | Perplexity: 1972.241138
2025-09-27 13:11:20,756 Stage: Train 0.5 | Epoch: 53 | Iter: 81400 | Total Loss: 0.005586 | Recon Loss: 0.004900 | Commit Loss: 0.001374 | Perplexity: 1970.837318
2025-09-27 13:12:17,326 Stage: Train 0.5 | Epoch: 53 | Iter: 81600 | Total Loss: 0.005672 | Recon Loss: 0.004977 | Commit Loss: 0.001390 | Perplexity: 1973.738470
2025-09-27 13:13:13,773 Stage: Train 0.5 | Epoch: 53 | Iter: 81800 | Total Loss: 0.005367 | Recon Loss: 0.004678 | Commit Loss: 0.001378 | Perplexity: 1981.676918
2025-09-27 13:14:09,599 Stage: Train 0.5 | Epoch: 53 | Iter: 82000 | Total Loss: 0.005571 | Recon Loss: 0.004881 | Commit Loss: 0.001380 | Perplexity: 1975.464009
Trainning Epoch:  16%|█▋        | 54/330 [6:25:21<32:49:35, 428.17s/it]2025-09-27 13:15:06,099 Stage: Train 0.5 | Epoch: 54 | Iter: 82200 | Total Loss: 0.005545 | Recon Loss: 0.004856 | Commit Loss: 0.001378 | Perplexity: 1981.645248
2025-09-27 13:16:02,468 Stage: Train 0.5 | Epoch: 54 | Iter: 82400 | Total Loss: 0.005559 | Recon Loss: 0.004867 | Commit Loss: 0.001384 | Perplexity: 1981.151417
2025-09-27 13:16:59,002 Stage: Train 0.5 | Epoch: 54 | Iter: 82600 | Total Loss: 0.005607 | Recon Loss: 0.004920 | Commit Loss: 0.001373 | Perplexity: 1977.807406
2025-09-27 13:17:55,254 Stage: Train 0.5 | Epoch: 54 | Iter: 82800 | Total Loss: 0.005364 | Recon Loss: 0.004680 | Commit Loss: 0.001367 | Perplexity: 1978.747621
2025-09-27 13:18:51,555 Stage: Train 0.5 | Epoch: 54 | Iter: 83000 | Total Loss: 0.005590 | Recon Loss: 0.004891 | Commit Loss: 0.001397 | Perplexity: 1977.622414
2025-09-27 13:19:47,828 Stage: Train 0.5 | Epoch: 54 | Iter: 83200 | Total Loss: 0.005431 | Recon Loss: 0.004741 | Commit Loss: 0.001380 | Perplexity: 1978.093855
2025-09-27 13:20:43,696 Stage: Train 0.5 | Epoch: 54 | Iter: 83400 | Total Loss: 0.005480 | Recon Loss: 0.004800 | Commit Loss: 0.001360 | Perplexity: 1979.329487
Trainning Epoch:  17%|█▋        | 55/330 [6:32:29<32:41:45, 428.02s/it]2025-09-27 13:21:40,288 Stage: Train 0.5 | Epoch: 55 | Iter: 83600 | Total Loss: 0.005540 | Recon Loss: 0.004840 | Commit Loss: 0.001399 | Perplexity: 1982.052501
2025-09-27 13:22:36,734 Stage: Train 0.5 | Epoch: 55 | Iter: 83800 | Total Loss: 0.005412 | Recon Loss: 0.004731 | Commit Loss: 0.001363 | Perplexity: 1981.710872
2025-09-27 13:23:33,153 Stage: Train 0.5 | Epoch: 55 | Iter: 84000 | Total Loss: 0.005393 | Recon Loss: 0.004698 | Commit Loss: 0.001391 | Perplexity: 1986.172300
2025-09-27 13:24:29,422 Stage: Train 0.5 | Epoch: 55 | Iter: 84200 | Total Loss: 0.005528 | Recon Loss: 0.004833 | Commit Loss: 0.001389 | Perplexity: 1978.462368
2025-09-27 13:25:25,613 Stage: Train 0.5 | Epoch: 55 | Iter: 84400 | Total Loss: 0.005548 | Recon Loss: 0.004862 | Commit Loss: 0.001372 | Perplexity: 1975.722191
2025-09-27 13:26:21,659 Stage: Train 0.5 | Epoch: 55 | Iter: 84600 | Total Loss: 0.005470 | Recon Loss: 0.004781 | Commit Loss: 0.001379 | Perplexity: 1984.439023
2025-09-27 13:27:17,774 Stage: Train 0.5 | Epoch: 55 | Iter: 84800 | Total Loss: 0.005570 | Recon Loss: 0.004880 | Commit Loss: 0.001380 | Perplexity: 1979.415078
2025-09-27 13:28:14,076 Stage: Train 0.5 | Epoch: 55 | Iter: 85000 | Total Loss: 0.005371 | Recon Loss: 0.004682 | Commit Loss: 0.001378 | Perplexity: 1979.601365
Trainning Epoch:  17%|█▋        | 56/330 [6:39:37<32:33:55, 427.87s/it]2025-09-27 13:29:10,561 Stage: Train 0.5 | Epoch: 56 | Iter: 85200 | Total Loss: 0.005571 | Recon Loss: 0.004886 | Commit Loss: 0.001371 | Perplexity: 1980.111307
2025-09-27 13:30:06,683 Stage: Train 0.5 | Epoch: 56 | Iter: 85400 | Total Loss: 0.005388 | Recon Loss: 0.004704 | Commit Loss: 0.001367 | Perplexity: 1983.720980
2025-09-27 13:31:03,029 Stage: Train 0.5 | Epoch: 56 | Iter: 85600 | Total Loss: 0.005460 | Recon Loss: 0.004771 | Commit Loss: 0.001379 | Perplexity: 1983.481949
2025-09-27 13:31:59,120 Stage: Train 0.5 | Epoch: 56 | Iter: 85800 | Total Loss: 0.005402 | Recon Loss: 0.004713 | Commit Loss: 0.001379 | Perplexity: 1984.995806
2025-09-27 13:32:54,379 Stage: Train 0.5 | Epoch: 56 | Iter: 86000 | Total Loss: 0.005418 | Recon Loss: 0.004738 | Commit Loss: 0.001360 | Perplexity: 1982.741665
2025-09-27 13:33:50,811 Stage: Train 0.5 | Epoch: 56 | Iter: 86200 | Total Loss: 0.005583 | Recon Loss: 0.004891 | Commit Loss: 0.001385 | Perplexity: 1987.867048
2025-09-27 13:34:47,117 Stage: Train 0.5 | Epoch: 56 | Iter: 86400 | Total Loss: 0.005398 | Recon Loss: 0.004716 | Commit Loss: 0.001363 | Perplexity: 1984.216187
Trainning Epoch:  17%|█▋        | 57/330 [6:46:43<32:24:59, 427.47s/it]2025-09-27 13:35:43,603 Stage: Train 0.5 | Epoch: 57 | Iter: 86600 | Total Loss: 0.005422 | Recon Loss: 0.004725 | Commit Loss: 0.001393 | Perplexity: 1982.196862
2025-09-27 13:36:40,002 Stage: Train 0.5 | Epoch: 57 | Iter: 86800 | Total Loss: 0.005373 | Recon Loss: 0.004685 | Commit Loss: 0.001375 | Perplexity: 1987.290776
2025-09-27 13:37:35,913 Stage: Train 0.5 | Epoch: 57 | Iter: 87000 | Total Loss: 0.005409 | Recon Loss: 0.004726 | Commit Loss: 0.001367 | Perplexity: 1980.043145
2025-09-27 13:38:32,241 Stage: Train 0.5 | Epoch: 57 | Iter: 87200 | Total Loss: 0.005346 | Recon Loss: 0.004658 | Commit Loss: 0.001375 | Perplexity: 1988.188198
2025-09-27 13:39:28,781 Stage: Train 0.5 | Epoch: 57 | Iter: 87400 | Total Loss: 0.005388 | Recon Loss: 0.004707 | Commit Loss: 0.001361 | Perplexity: 1981.281766
2025-09-27 13:40:25,155 Stage: Train 0.5 | Epoch: 57 | Iter: 87600 | Total Loss: 0.005531 | Recon Loss: 0.004837 | Commit Loss: 0.001388 | Perplexity: 1986.698566
2025-09-27 13:41:21,575 Stage: Train 0.5 | Epoch: 57 | Iter: 87800 | Total Loss: 0.005404 | Recon Loss: 0.004711 | Commit Loss: 0.001384 | Perplexity: 1980.234697
2025-09-27 13:42:17,830 Stage: Train 0.5 | Epoch: 57 | Iter: 88000 | Total Loss: 0.005381 | Recon Loss: 0.004697 | Commit Loss: 0.001368 | Perplexity: 1984.129760
Trainning Epoch:  18%|█▊        | 58/330 [6:53:51<32:18:26, 427.60s/it]2025-09-27 13:43:14,287 Stage: Train 0.5 | Epoch: 58 | Iter: 88200 | Total Loss: 0.005354 | Recon Loss: 0.004679 | Commit Loss: 0.001350 | Perplexity: 1982.035277
2025-09-27 13:44:10,247 Stage: Train 0.5 | Epoch: 58 | Iter: 88400 | Total Loss: 0.005358 | Recon Loss: 0.004671 | Commit Loss: 0.001375 | Perplexity: 1988.840044
2025-09-27 13:45:06,423 Stage: Train 0.5 | Epoch: 58 | Iter: 88600 | Total Loss: 0.005721 | Recon Loss: 0.005045 | Commit Loss: 0.001354 | Perplexity: 1976.433704
2025-09-27 13:46:02,769 Stage: Train 0.5 | Epoch: 58 | Iter: 88800 | Total Loss: 0.005151 | Recon Loss: 0.004458 | Commit Loss: 0.001387 | Perplexity: 1992.698620
2025-09-27 13:46:58,854 Stage: Train 0.5 | Epoch: 58 | Iter: 89000 | Total Loss: 0.005524 | Recon Loss: 0.004845 | Commit Loss: 0.001357 | Perplexity: 1980.918687
2025-09-27 13:47:55,221 Stage: Train 0.5 | Epoch: 58 | Iter: 89200 | Total Loss: 0.005330 | Recon Loss: 0.004640 | Commit Loss: 0.001380 | Perplexity: 1985.968298
2025-09-27 13:48:51,570 Stage: Train 0.5 | Epoch: 58 | Iter: 89400 | Total Loss: 0.005393 | Recon Loss: 0.004709 | Commit Loss: 0.001367 | Perplexity: 1987.925970
2025-09-27 13:49:47,534 Stage: Train 0.5 | Epoch: 58 | Iter: 89600 | Total Loss: 0.005312 | Recon Loss: 0.004622 | Commit Loss: 0.001379 | Perplexity: 1990.607305
Trainning Epoch:  18%|█▊        | 59/330 [7:00:58<32:10:19, 427.38s/it]2025-09-27 13:50:43,857 Stage: Train 0.5 | Epoch: 59 | Iter: 89800 | Total Loss: 0.005315 | Recon Loss: 0.004632 | Commit Loss: 0.001366 | Perplexity: 1990.212979
2025-09-27 13:51:40,255 Stage: Train 0.5 | Epoch: 59 | Iter: 90000 | Total Loss: 0.005538 | Recon Loss: 0.004853 | Commit Loss: 0.001370 | Perplexity: 1990.091516
2025-09-27 13:52:36,625 Stage: Train 0.5 | Epoch: 59 | Iter: 90200 | Total Loss: 0.005267 | Recon Loss: 0.004581 | Commit Loss: 0.001371 | Perplexity: 1989.111041
2025-09-27 13:53:32,946 Stage: Train 0.5 | Epoch: 59 | Iter: 90400 | Total Loss: 0.005355 | Recon Loss: 0.004676 | Commit Loss: 0.001357 | Perplexity: 1982.766792
2025-09-27 13:54:29,270 Stage: Train 0.5 | Epoch: 59 | Iter: 90600 | Total Loss: 0.005515 | Recon Loss: 0.004831 | Commit Loss: 0.001367 | Perplexity: 1988.171712
2025-09-27 13:55:25,116 Stage: Train 0.5 | Epoch: 59 | Iter: 90800 | Total Loss: 0.005449 | Recon Loss: 0.004771 | Commit Loss: 0.001355 | Perplexity: 1979.776806
2025-09-27 13:56:21,589 Stage: Train 0.5 | Epoch: 59 | Iter: 91000 | Total Loss: 0.005379 | Recon Loss: 0.004696 | Commit Loss: 0.001366 | Perplexity: 1985.390925
Trainning Epoch:  18%|█▊        | 60/330 [7:08:06<32:03:50, 427.52s/it]2025-09-27 13:57:18,294 Stage: Train 0.5 | Epoch: 60 | Iter: 91200 | Total Loss: 0.005343 | Recon Loss: 0.004666 | Commit Loss: 0.001356 | Perplexity: 1981.688835
2025-09-27 13:58:14,629 Stage: Train 0.5 | Epoch: 60 | Iter: 91400 | Total Loss: 0.005475 | Recon Loss: 0.004800 | Commit Loss: 0.001350 | Perplexity: 1980.908113
2025-09-27 13:59:10,838 Stage: Train 0.5 | Epoch: 60 | Iter: 91600 | Total Loss: 0.005271 | Recon Loss: 0.004590 | Commit Loss: 0.001362 | Perplexity: 1980.865945
2025-09-27 14:00:07,063 Stage: Train 0.5 | Epoch: 60 | Iter: 91800 | Total Loss: 0.005385 | Recon Loss: 0.004708 | Commit Loss: 0.001355 | Perplexity: 1984.464880
2025-09-27 14:01:02,980 Stage: Train 0.5 | Epoch: 60 | Iter: 92000 | Total Loss: 0.005287 | Recon Loss: 0.004608 | Commit Loss: 0.001359 | Perplexity: 1984.906790
2025-09-27 14:01:59,332 Stage: Train 0.5 | Epoch: 60 | Iter: 92200 | Total Loss: 0.005343 | Recon Loss: 0.004662 | Commit Loss: 0.001363 | Perplexity: 1984.027845
2025-09-27 14:02:55,635 Stage: Train 0.5 | Epoch: 60 | Iter: 92400 | Total Loss: 0.005271 | Recon Loss: 0.004587 | Commit Loss: 0.001370 | Perplexity: 1988.966614
2025-09-27 14:03:52,063 Stage: Train 0.5 | Epoch: 60 | Iter: 92600 | Total Loss: 0.005424 | Recon Loss: 0.004738 | Commit Loss: 0.001371 | Perplexity: 1986.677433
Trainning Epoch:  18%|█▊        | 61/330 [7:15:13<31:56:40, 427.51s/it]2025-09-27 14:04:48,770 Stage: Train 0.5 | Epoch: 61 | Iter: 92800 | Total Loss: 0.005272 | Recon Loss: 0.004585 | Commit Loss: 0.001373 | Perplexity: 1985.401382
2025-09-27 14:05:44,982 Stage: Train 0.5 | Epoch: 61 | Iter: 93000 | Total Loss: 0.005383 | Recon Loss: 0.004706 | Commit Loss: 0.001354 | Perplexity: 1983.632833
2025-09-27 14:06:40,932 Stage: Train 0.5 | Epoch: 61 | Iter: 93200 | Total Loss: 0.005309 | Recon Loss: 0.004625 | Commit Loss: 0.001369 | Perplexity: 1991.214114
2025-09-27 14:07:37,421 Stage: Train 0.5 | Epoch: 61 | Iter: 93400 | Total Loss: 0.005202 | Recon Loss: 0.004526 | Commit Loss: 0.001352 | Perplexity: 1982.392578
2025-09-27 14:08:34,038 Stage: Train 0.5 | Epoch: 61 | Iter: 93600 | Total Loss: 0.005342 | Recon Loss: 0.004662 | Commit Loss: 0.001358 | Perplexity: 1985.041572
2025-09-27 14:09:30,474 Stage: Train 0.5 | Epoch: 61 | Iter: 93800 | Total Loss: 0.005337 | Recon Loss: 0.004654 | Commit Loss: 0.001366 | Perplexity: 1992.285587
2025-09-27 14:10:26,819 Stage: Train 0.5 | Epoch: 61 | Iter: 94000 | Total Loss: 0.005253 | Recon Loss: 0.004577 | Commit Loss: 0.001353 | Perplexity: 1984.263990
Trainning Epoch:  19%|█▉        | 62/330 [7:22:21<31:50:17, 427.68s/it]2025-09-27 14:11:23,224 Stage: Train 0.5 | Epoch: 62 | Iter: 94200 | Total Loss: 0.005196 | Recon Loss: 0.004519 | Commit Loss: 0.001353 | Perplexity: 1982.979625
2025-09-27 14:12:19,603 Stage: Train 0.5 | Epoch: 62 | Iter: 94400 | Total Loss: 0.005303 | Recon Loss: 0.004624 | Commit Loss: 0.001358 | Perplexity: 1985.171661
2025-09-27 14:13:15,490 Stage: Train 0.5 | Epoch: 62 | Iter: 94600 | Total Loss: 0.005298 | Recon Loss: 0.004621 | Commit Loss: 0.001353 | Perplexity: 1986.128168
2025-09-27 14:14:11,722 Stage: Train 0.5 | Epoch: 62 | Iter: 94800 | Total Loss: 0.005270 | Recon Loss: 0.004592 | Commit Loss: 0.001357 | Perplexity: 1984.521047
2025-09-27 14:15:08,165 Stage: Train 0.5 | Epoch: 62 | Iter: 95000 | Total Loss: 0.005204 | Recon Loss: 0.004521 | Commit Loss: 0.001366 | Perplexity: 1985.037482
2025-09-27 14:16:04,465 Stage: Train 0.5 | Epoch: 62 | Iter: 95200 | Total Loss: 0.005255 | Recon Loss: 0.004576 | Commit Loss: 0.001359 | Perplexity: 1994.586130
2025-09-27 14:17:00,871 Stage: Train 0.5 | Epoch: 62 | Iter: 95400 | Total Loss: 0.005343 | Recon Loss: 0.004669 | Commit Loss: 0.001348 | Perplexity: 1980.562913
2025-09-27 14:17:57,409 Stage: Train 0.5 | Epoch: 62 | Iter: 95600 | Total Loss: 0.005294 | Recon Loss: 0.004613 | Commit Loss: 0.001361 | Perplexity: 1991.363340
Trainning Epoch:  19%|█▉        | 63/330 [7:29:29<31:42:56, 427.63s/it]2025-09-27 14:18:53,703 Stage: Train 0.5 | Epoch: 63 | Iter: 95800 | Total Loss: 0.005257 | Recon Loss: 0.004579 | Commit Loss: 0.001355 | Perplexity: 1980.044169
2025-09-27 14:19:50,038 Stage: Train 0.5 | Epoch: 63 | Iter: 96000 | Total Loss: 0.005183 | Recon Loss: 0.004507 | Commit Loss: 0.001353 | Perplexity: 1989.879100
2025-09-27 14:20:46,368 Stage: Train 0.5 | Epoch: 63 | Iter: 96200 | Total Loss: 0.005214 | Recon Loss: 0.004535 | Commit Loss: 0.001358 | Perplexity: 1982.016088
2025-09-27 14:21:42,715 Stage: Train 0.5 | Epoch: 63 | Iter: 96400 | Total Loss: 0.005323 | Recon Loss: 0.004645 | Commit Loss: 0.001356 | Perplexity: 1984.695303
2025-09-27 14:22:39,029 Stage: Train 0.5 | Epoch: 63 | Iter: 96600 | Total Loss: 0.005426 | Recon Loss: 0.004756 | Commit Loss: 0.001340 | Perplexity: 1981.098185
2025-09-27 14:23:35,429 Stage: Train 0.5 | Epoch: 63 | Iter: 96800 | Total Loss: 0.005155 | Recon Loss: 0.004475 | Commit Loss: 0.001359 | Perplexity: 1986.025430
2025-09-27 14:24:31,329 Stage: Train 0.5 | Epoch: 63 | Iter: 97000 | Total Loss: 0.005296 | Recon Loss: 0.004620 | Commit Loss: 0.001353 | Perplexity: 1980.609552
2025-09-27 14:25:27,674 Stage: Train 0.5 | Epoch: 63 | Iter: 97200 | Total Loss: 0.005138 | Recon Loss: 0.004467 | Commit Loss: 0.001341 | Perplexity: 1981.175821
Trainning Epoch:  19%|█▉        | 64/330 [7:36:37<31:36:05, 427.69s/it]2025-09-27 14:26:24,127 Stage: Train 0.5 | Epoch: 64 | Iter: 97400 | Total Loss: 0.005301 | Recon Loss: 0.004626 | Commit Loss: 0.001349 | Perplexity: 1978.346248
2025-09-27 14:27:20,448 Stage: Train 0.5 | Epoch: 64 | Iter: 97600 | Total Loss: 0.005202 | Recon Loss: 0.004521 | Commit Loss: 0.001362 | Perplexity: 1991.848748
2025-09-27 14:28:16,829 Stage: Train 0.5 | Epoch: 64 | Iter: 97800 | Total Loss: 0.005361 | Recon Loss: 0.004688 | Commit Loss: 0.001346 | Perplexity: 1987.189602
2025-09-27 14:29:13,175 Stage: Train 0.5 | Epoch: 64 | Iter: 98000 | Total Loss: 0.005047 | Recon Loss: 0.004369 | Commit Loss: 0.001356 | Perplexity: 1992.330888
2025-09-27 14:30:09,260 Stage: Train 0.5 | Epoch: 64 | Iter: 98200 | Total Loss: 0.005222 | Recon Loss: 0.004543 | Commit Loss: 0.001358 | Perplexity: 1984.805871
2025-09-27 14:31:05,485 Stage: Train 0.5 | Epoch: 64 | Iter: 98400 | Total Loss: 0.005034 | Recon Loss: 0.004359 | Commit Loss: 0.001350 | Perplexity: 1987.159626
2025-09-27 14:32:01,934 Stage: Train 0.5 | Epoch: 64 | Iter: 98600 | Total Loss: 0.005249 | Recon Loss: 0.004578 | Commit Loss: 0.001342 | Perplexity: 1984.663186
Trainning Epoch:  20%|█▉        | 65/330 [7:43:45<31:29:15, 427.76s/it]2025-09-27 14:32:58,701 Stage: Train 0.5 | Epoch: 65 | Iter: 98800 | Total Loss: 0.005289 | Recon Loss: 0.004616 | Commit Loss: 0.001347 | Perplexity: 1989.293725
2025-09-27 14:33:55,234 Stage: Train 0.5 | Epoch: 65 | Iter: 99000 | Total Loss: 0.005199 | Recon Loss: 0.004532 | Commit Loss: 0.001334 | Perplexity: 1984.353086
2025-09-27 14:34:51,467 Stage: Train 0.5 | Epoch: 65 | Iter: 99200 | Total Loss: 0.005174 | Recon Loss: 0.004497 | Commit Loss: 0.001353 | Perplexity: 1987.979925
2025-09-27 14:35:47,665 Stage: Train 0.5 | Epoch: 65 | Iter: 99400 | Total Loss: 0.005299 | Recon Loss: 0.004631 | Commit Loss: 0.001337 | Perplexity: 1983.009891
2025-09-27 14:36:43,583 Stage: Train 0.5 | Epoch: 65 | Iter: 99600 | Total Loss: 0.005141 | Recon Loss: 0.004467 | Commit Loss: 0.001347 | Perplexity: 1988.022988
2025-09-27 14:37:39,717 Stage: Train 0.5 | Epoch: 65 | Iter: 99800 | Total Loss: 0.005174 | Recon Loss: 0.004501 | Commit Loss: 0.001346 | Perplexity: 1994.199760
2025-09-27 14:38:36,046 Stage: Train 0.5 | Epoch: 65 | Iter: 100000 | Total Loss: 0.005183 | Recon Loss: 0.004499 | Commit Loss: 0.001367 | Perplexity: 1991.540918
2025-09-27 14:38:36,046 Saving model at iteration 100000
2025-09-27 14:38:36,252 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000
2025-09-27 14:38:36,744 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/model.safetensors
2025-09-27 14:38:37,307 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/optimizer.bin
2025-09-27 14:38:37,308 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/scheduler.bin
2025-09-27 14:38:37,308 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/sampler.bin
2025-09-27 14:38:37,309 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/random_states_0.pkl
2025-09-27 14:39:33,847 Stage: Train 0.5 | Epoch: 65 | Iter: 100200 | Total Loss: 0.005247 | Recon Loss: 0.004571 | Commit Loss: 0.001353 | Perplexity: 1990.615527
Trainning Epoch:  20%|██        | 66/330 [7:50:54<31:23:39, 428.10s/it]2025-09-27 14:40:30,213 Stage: Train 0.5 | Epoch: 66 | Iter: 100400 | Total Loss: 0.005159 | Recon Loss: 0.004492 | Commit Loss: 0.001335 | Perplexity: 1984.247615
2025-09-27 14:41:26,518 Stage: Train 0.5 | Epoch: 66 | Iter: 100600 | Total Loss: 0.005321 | Recon Loss: 0.004647 | Commit Loss: 0.001349 | Perplexity: 1989.378688
2025-09-27 14:42:22,483 Stage: Train 0.5 | Epoch: 66 | Iter: 100800 | Total Loss: 0.005069 | Recon Loss: 0.004389 | Commit Loss: 0.001360 | Perplexity: 1992.833901
2025-09-27 14:43:18,769 Stage: Train 0.5 | Epoch: 66 | Iter: 101000 | Total Loss: 0.005232 | Recon Loss: 0.004553 | Commit Loss: 0.001358 | Perplexity: 1987.989851
2025-09-27 14:44:15,267 Stage: Train 0.5 | Epoch: 66 | Iter: 101200 | Total Loss: 0.005042 | Recon Loss: 0.004368 | Commit Loss: 0.001347 | Perplexity: 1993.799285
2025-09-27 14:45:11,700 Stage: Train 0.5 | Epoch: 66 | Iter: 101400 | Total Loss: 0.005196 | Recon Loss: 0.004524 | Commit Loss: 0.001345 | Perplexity: 1994.337450
2025-09-27 14:46:08,283 Stage: Train 0.5 | Epoch: 66 | Iter: 101600 | Total Loss: 0.005135 | Recon Loss: 0.004456 | Commit Loss: 0.001358 | Perplexity: 1995.018766
Trainning Epoch:  20%|██        | 67/330 [7:58:01<31:16:20, 428.06s/it]2025-09-27 14:47:04,714 Stage: Train 0.5 | Epoch: 67 | Iter: 101800 | Total Loss: 0.005171 | Recon Loss: 0.004494 | Commit Loss: 0.001353 | Perplexity: 1992.292111
2025-09-27 14:48:00,699 Stage: Train 0.5 | Epoch: 67 | Iter: 102000 | Total Loss: 0.005265 | Recon Loss: 0.004591 | Commit Loss: 0.001348 | Perplexity: 1990.586489
2025-09-27 14:48:57,011 Stage: Train 0.5 | Epoch: 67 | Iter: 102200 | Total Loss: 0.005117 | Recon Loss: 0.004448 | Commit Loss: 0.001338 | Perplexity: 1991.227777
2025-09-27 14:49:51,495 Stage: Train 0.5 | Epoch: 67 | Iter: 102400 | Total Loss: 0.005085 | Recon Loss: 0.004416 | Commit Loss: 0.001338 | Perplexity: 1995.589202
2025-09-27 14:50:47,880 Stage: Train 0.5 | Epoch: 67 | Iter: 102600 | Total Loss: 0.005286 | Recon Loss: 0.004615 | Commit Loss: 0.001343 | Perplexity: 1989.707411
2025-09-27 14:51:43,963 Stage: Train 0.5 | Epoch: 67 | Iter: 102800 | Total Loss: 0.005068 | Recon Loss: 0.004392 | Commit Loss: 0.001351 | Perplexity: 1988.100262
2025-09-27 14:52:40,346 Stage: Train 0.5 | Epoch: 67 | Iter: 103000 | Total Loss: 0.005237 | Recon Loss: 0.004562 | Commit Loss: 0.001350 | Perplexity: 1990.047280
2025-09-27 14:53:36,293 Stage: Train 0.5 | Epoch: 67 | Iter: 103200 | Total Loss: 0.005321 | Recon Loss: 0.004648 | Commit Loss: 0.001347 | Perplexity: 1993.948447
Trainning Epoch:  21%|██        | 68/330 [8:05:07<31:05:31, 427.22s/it]2025-09-27 14:54:32,913 Stage: Train 0.5 | Epoch: 68 | Iter: 103400 | Total Loss: 0.005133 | Recon Loss: 0.004465 | Commit Loss: 0.001337 | Perplexity: 1989.012529
2025-09-27 14:55:29,420 Stage: Train 0.5 | Epoch: 68 | Iter: 103600 | Total Loss: 0.005063 | Recon Loss: 0.004395 | Commit Loss: 0.001336 | Perplexity: 1992.835040
2025-09-27 14:56:25,692 Stage: Train 0.5 | Epoch: 68 | Iter: 103800 | Total Loss: 0.005171 | Recon Loss: 0.004499 | Commit Loss: 0.001345 | Perplexity: 1992.877729
2025-09-27 14:57:21,973 Stage: Train 0.5 | Epoch: 68 | Iter: 104000 | Total Loss: 0.005212 | Recon Loss: 0.004543 | Commit Loss: 0.001338 | Perplexity: 1990.491220
2025-09-27 14:58:18,526 Stage: Train 0.5 | Epoch: 68 | Iter: 104200 | Total Loss: 0.005113 | Recon Loss: 0.004445 | Commit Loss: 0.001336 | Perplexity: 1991.677389
2025-09-27 14:59:14,970 Stage: Train 0.5 | Epoch: 68 | Iter: 104400 | Total Loss: 0.005092 | Recon Loss: 0.004424 | Commit Loss: 0.001336 | Perplexity: 1990.723040
2025-09-27 15:00:11,018 Stage: Train 0.5 | Epoch: 68 | Iter: 104600 | Total Loss: 0.004979 | Recon Loss: 0.004306 | Commit Loss: 0.001345 | Perplexity: 1995.664224
2025-09-27 15:01:07,350 Stage: Train 0.5 | Epoch: 68 | Iter: 104800 | Total Loss: 0.005084 | Recon Loss: 0.004406 | Commit Loss: 0.001357 | Perplexity: 1998.146443
Trainning Epoch:  21%|██        | 69/330 [8:12:15<30:59:41, 427.52s/it]2025-09-27 15:02:03,900 Stage: Train 0.5 | Epoch: 69 | Iter: 105000 | Total Loss: 0.005037 | Recon Loss: 0.004367 | Commit Loss: 0.001340 | Perplexity: 1993.971950
2025-09-27 15:03:00,317 Stage: Train 0.5 | Epoch: 69 | Iter: 105200 | Total Loss: 0.005200 | Recon Loss: 0.004536 | Commit Loss: 0.001328 | Perplexity: 1992.621586
2025-09-27 15:03:56,744 Stage: Train 0.5 | Epoch: 69 | Iter: 105400 | Total Loss: 0.005002 | Recon Loss: 0.004327 | Commit Loss: 0.001350 | Perplexity: 1996.005362
2025-09-27 15:04:53,041 Stage: Train 0.5 | Epoch: 69 | Iter: 105600 | Total Loss: 0.005178 | Recon Loss: 0.004507 | Commit Loss: 0.001340 | Perplexity: 1993.715845
2025-09-27 15:05:49,198 Stage: Train 0.5 | Epoch: 69 | Iter: 105800 | Total Loss: 0.005063 | Recon Loss: 0.004394 | Commit Loss: 0.001338 | Perplexity: 1995.199600
2025-09-27 15:06:45,582 Stage: Train 0.5 | Epoch: 69 | Iter: 106000 | Total Loss: 0.005120 | Recon Loss: 0.004445 | Commit Loss: 0.001349 | Perplexity: 1998.316644
2025-09-27 15:07:41,959 Stage: Train 0.5 | Epoch: 69 | Iter: 106200 | Total Loss: 0.004975 | Recon Loss: 0.004304 | Commit Loss: 0.001341 | Perplexity: 1993.352371
Trainning Epoch:  21%|██        | 70/330 [8:19:23<30:53:20, 427.69s/it]2025-09-27 15:08:38,467 Stage: Train 0.5 | Epoch: 70 | Iter: 106400 | Total Loss: 0.005025 | Recon Loss: 0.004352 | Commit Loss: 0.001345 | Perplexity: 1995.174343
2025-09-27 15:09:34,780 Stage: Train 0.5 | Epoch: 70 | Iter: 106600 | Total Loss: 0.005120 | Recon Loss: 0.004449 | Commit Loss: 0.001341 | Perplexity: 1995.275883
2025-09-27 15:10:31,177 Stage: Train 0.5 | Epoch: 70 | Iter: 106800 | Total Loss: 0.005070 | Recon Loss: 0.004401 | Commit Loss: 0.001337 | Perplexity: 1997.925898
2025-09-27 15:11:27,280 Stage: Train 0.5 | Epoch: 70 | Iter: 107000 | Total Loss: 0.005121 | Recon Loss: 0.004450 | Commit Loss: 0.001343 | Perplexity: 1999.658336
2025-09-27 15:12:23,570 Stage: Train 0.5 | Epoch: 70 | Iter: 107200 | Total Loss: 0.005043 | Recon Loss: 0.004378 | Commit Loss: 0.001330 | Perplexity: 1994.432803
2025-09-27 15:13:19,814 Stage: Train 0.5 | Epoch: 70 | Iter: 107400 | Total Loss: 0.005243 | Recon Loss: 0.004584 | Commit Loss: 0.001318 | Perplexity: 1989.869681
2025-09-27 15:14:16,341 Stage: Train 0.5 | Epoch: 70 | Iter: 107600 | Total Loss: 0.004991 | Recon Loss: 0.004329 | Commit Loss: 0.001324 | Perplexity: 1988.104368
2025-09-27 15:15:12,822 Stage: Train 0.5 | Epoch: 70 | Iter: 107800 | Total Loss: 0.005138 | Recon Loss: 0.004459 | Commit Loss: 0.001358 | Perplexity: 2000.823159
Trainning Epoch:  22%|██▏       | 71/330 [8:26:31<30:46:45, 427.82s/it]2025-09-27 15:16:09,515 Stage: Train 0.5 | Epoch: 71 | Iter: 108000 | Total Loss: 0.005112 | Recon Loss: 0.004452 | Commit Loss: 0.001321 | Perplexity: 1992.470400
2025-09-27 15:17:05,508 Stage: Train 0.5 | Epoch: 71 | Iter: 108200 | Total Loss: 0.005069 | Recon Loss: 0.004406 | Commit Loss: 0.001325 | Perplexity: 1998.640912
2025-09-27 15:18:02,098 Stage: Train 0.5 | Epoch: 71 | Iter: 108400 | Total Loss: 0.005019 | Recon Loss: 0.004348 | Commit Loss: 0.001341 | Perplexity: 1996.604535
2025-09-27 15:18:58,443 Stage: Train 0.5 | Epoch: 71 | Iter: 108600 | Total Loss: 0.005162 | Recon Loss: 0.004490 | Commit Loss: 0.001343 | Perplexity: 2000.737930
2025-09-27 15:19:54,959 Stage: Train 0.5 | Epoch: 71 | Iter: 108800 | Total Loss: 0.004946 | Recon Loss: 0.004282 | Commit Loss: 0.001327 | Perplexity: 1994.458917
2025-09-27 15:20:51,294 Stage: Train 0.5 | Epoch: 71 | Iter: 109000 | Total Loss: 0.005143 | Recon Loss: 0.004476 | Commit Loss: 0.001333 | Perplexity: 1993.698941
2025-09-27 15:21:47,621 Stage: Train 0.5 | Epoch: 71 | Iter: 109200 | Total Loss: 0.004991 | Recon Loss: 0.004328 | Commit Loss: 0.001326 | Perplexity: 1996.007245
Trainning Epoch:  22%|██▏       | 72/330 [8:33:39<30:40:07, 427.93s/it]2025-09-27 15:22:43,687 Stage: Train 0.5 | Epoch: 72 | Iter: 109400 | Total Loss: 0.005070 | Recon Loss: 0.004406 | Commit Loss: 0.001327 | Perplexity: 1997.253392
2025-09-27 15:23:40,039 Stage: Train 0.5 | Epoch: 72 | Iter: 109600 | Total Loss: 0.005029 | Recon Loss: 0.004364 | Commit Loss: 0.001329 | Perplexity: 1999.803153
2025-09-27 15:24:36,518 Stage: Train 0.5 | Epoch: 72 | Iter: 109800 | Total Loss: 0.005074 | Recon Loss: 0.004412 | Commit Loss: 0.001324 | Perplexity: 1995.262510
2025-09-27 15:25:32,760 Stage: Train 0.5 | Epoch: 72 | Iter: 110000 | Total Loss: 0.004970 | Recon Loss: 0.004299 | Commit Loss: 0.001342 | Perplexity: 1996.501086
2025-09-27 15:26:28,906 Stage: Train 0.5 | Epoch: 72 | Iter: 110200 | Total Loss: 0.005019 | Recon Loss: 0.004361 | Commit Loss: 0.001316 | Perplexity: 1992.497971
2025-09-27 15:27:25,301 Stage: Train 0.5 | Epoch: 72 | Iter: 110400 | Total Loss: 0.005142 | Recon Loss: 0.004481 | Commit Loss: 0.001323 | Perplexity: 1997.829099
2025-09-27 15:28:21,850 Stage: Train 0.5 | Epoch: 72 | Iter: 110600 | Total Loss: 0.004986 | Recon Loss: 0.004319 | Commit Loss: 0.001334 | Perplexity: 1997.044705
2025-09-27 15:29:17,669 Stage: Train 0.5 | Epoch: 72 | Iter: 110800 | Total Loss: 0.004973 | Recon Loss: 0.004297 | Commit Loss: 0.001353 | Perplexity: 2002.265607
Trainning Epoch:  22%|██▏       | 73/330 [8:40:47<30:32:18, 427.78s/it]2025-09-27 15:30:14,386 Stage: Train 0.5 | Epoch: 73 | Iter: 111000 | Total Loss: 0.004988 | Recon Loss: 0.004329 | Commit Loss: 0.001317 | Perplexity: 1998.704841
2025-09-27 15:31:10,544 Stage: Train 0.5 | Epoch: 73 | Iter: 111200 | Total Loss: 0.005044 | Recon Loss: 0.004385 | Commit Loss: 0.001318 | Perplexity: 1997.221323
2025-09-27 15:32:07,225 Stage: Train 0.5 | Epoch: 73 | Iter: 111400 | Total Loss: 0.004900 | Recon Loss: 0.004233 | Commit Loss: 0.001332 | Perplexity: 2001.015871
2025-09-27 15:33:03,574 Stage: Train 0.5 | Epoch: 73 | Iter: 111600 | Total Loss: 0.005085 | Recon Loss: 0.004422 | Commit Loss: 0.001326 | Perplexity: 1997.076765
2025-09-27 15:33:59,926 Stage: Train 0.5 | Epoch: 73 | Iter: 111800 | Total Loss: 0.004948 | Recon Loss: 0.004280 | Commit Loss: 0.001335 | Perplexity: 1998.622277
2025-09-27 15:34:55,997 Stage: Train 0.5 | Epoch: 73 | Iter: 112000 | Total Loss: 0.005055 | Recon Loss: 0.004392 | Commit Loss: 0.001328 | Perplexity: 1993.672884
2025-09-27 15:35:52,293 Stage: Train 0.5 | Epoch: 73 | Iter: 112200 | Total Loss: 0.004908 | Recon Loss: 0.004242 | Commit Loss: 0.001332 | Perplexity: 1999.823561
2025-09-27 15:36:48,667 Stage: Train 0.5 | Epoch: 73 | Iter: 112400 | Total Loss: 0.005011 | Recon Loss: 0.004345 | Commit Loss: 0.001333 | Perplexity: 1998.548059
Trainning Epoch:  22%|██▏       | 74/330 [8:47:55<30:25:34, 427.87s/it]2025-09-27 15:37:45,285 Stage: Train 0.5 | Epoch: 74 | Iter: 112600 | Total Loss: 0.004924 | Recon Loss: 0.004256 | Commit Loss: 0.001337 | Perplexity: 1997.957800
2025-09-27 15:38:41,584 Stage: Train 0.5 | Epoch: 74 | Iter: 112800 | Total Loss: 0.005050 | Recon Loss: 0.004389 | Commit Loss: 0.001321 | Perplexity: 1999.604844
2025-09-27 15:39:38,122 Stage: Train 0.5 | Epoch: 74 | Iter: 113000 | Total Loss: 0.004918 | Recon Loss: 0.004250 | Commit Loss: 0.001336 | Perplexity: 2004.260695
2025-09-27 15:40:34,171 Stage: Train 0.5 | Epoch: 74 | Iter: 113200 | Total Loss: 0.005002 | Recon Loss: 0.004345 | Commit Loss: 0.001313 | Perplexity: 1993.918214
2025-09-27 15:41:30,501 Stage: Train 0.5 | Epoch: 74 | Iter: 113400 | Total Loss: 0.004992 | Recon Loss: 0.004317 | Commit Loss: 0.001348 | Perplexity: 2002.614943
2025-09-27 15:42:27,046 Stage: Train 0.5 | Epoch: 74 | Iter: 113600 | Total Loss: 0.004890 | Recon Loss: 0.004234 | Commit Loss: 0.001312 | Perplexity: 1991.883546
2025-09-27 15:43:23,522 Stage: Train 0.5 | Epoch: 74 | Iter: 113800 | Total Loss: 0.004959 | Recon Loss: 0.004297 | Commit Loss: 0.001323 | Perplexity: 1996.825323
Trainning Epoch:  23%|██▎       | 75/330 [8:55:03<30:19:12, 428.05s/it]2025-09-27 15:44:20,120 Stage: Train 0.5 | Epoch: 75 | Iter: 114000 | Total Loss: 0.005029 | Recon Loss: 0.004365 | Commit Loss: 0.001327 | Perplexity: 1996.253922
2025-09-27 15:45:16,408 Stage: Train 0.5 | Epoch: 75 | Iter: 114200 | Total Loss: 0.004976 | Recon Loss: 0.004320 | Commit Loss: 0.001312 | Perplexity: 1996.552984
2025-09-27 15:46:12,329 Stage: Train 0.5 | Epoch: 75 | Iter: 114400 | Total Loss: 0.004871 | Recon Loss: 0.004215 | Commit Loss: 0.001313 | Perplexity: 1998.828443
2025-09-27 15:47:08,564 Stage: Train 0.5 | Epoch: 75 | Iter: 114600 | Total Loss: 0.005025 | Recon Loss: 0.004358 | Commit Loss: 0.001334 | Perplexity: 1999.231251
2025-09-27 15:48:04,850 Stage: Train 0.5 | Epoch: 75 | Iter: 114800 | Total Loss: 0.004905 | Recon Loss: 0.004242 | Commit Loss: 0.001326 | Perplexity: 2001.844229
2025-09-27 15:49:01,292 Stage: Train 0.5 | Epoch: 75 | Iter: 115000 | Total Loss: 0.004929 | Recon Loss: 0.004265 | Commit Loss: 0.001328 | Perplexity: 2000.619906
2025-09-27 15:49:57,807 Stage: Train 0.5 | Epoch: 75 | Iter: 115200 | Total Loss: 0.004995 | Recon Loss: 0.004329 | Commit Loss: 0.001333 | Perplexity: 2000.839785
2025-09-27 15:50:54,144 Stage: Train 0.5 | Epoch: 75 | Iter: 115400 | Total Loss: 0.004948 | Recon Loss: 0.004282 | Commit Loss: 0.001332 | Perplexity: 1996.200045
Trainning Epoch:  23%|██▎       | 76/330 [9:02:11<30:11:40, 427.95s/it]2025-09-27 15:51:50,772 Stage: Train 0.5 | Epoch: 76 | Iter: 115600 | Total Loss: 0.004923 | Recon Loss: 0.004266 | Commit Loss: 0.001314 | Perplexity: 2000.563041
2025-09-27 15:52:46,898 Stage: Train 0.5 | Epoch: 76 | Iter: 115800 | Total Loss: 0.004882 | Recon Loss: 0.004222 | Commit Loss: 0.001321 | Perplexity: 1996.550812
2025-09-27 15:53:43,285 Stage: Train 0.5 | Epoch: 76 | Iter: 116000 | Total Loss: 0.004913 | Recon Loss: 0.004252 | Commit Loss: 0.001321 | Perplexity: 2000.231112
2025-09-27 15:54:39,867 Stage: Train 0.5 | Epoch: 76 | Iter: 116200 | Total Loss: 0.004946 | Recon Loss: 0.004284 | Commit Loss: 0.001324 | Perplexity: 2000.190203
2025-09-27 15:55:36,118 Stage: Train 0.5 | Epoch: 76 | Iter: 116400 | Total Loss: 0.004956 | Recon Loss: 0.004292 | Commit Loss: 0.001329 | Perplexity: 1999.632920
2025-09-27 15:56:32,622 Stage: Train 0.5 | Epoch: 76 | Iter: 116600 | Total Loss: 0.004988 | Recon Loss: 0.004320 | Commit Loss: 0.001335 | Perplexity: 2002.138580
2025-09-27 15:57:28,851 Stage: Train 0.5 | Epoch: 76 | Iter: 116800 | Total Loss: 0.004888 | Recon Loss: 0.004228 | Commit Loss: 0.001320 | Perplexity: 1997.862885
Trainning Epoch:  23%|██▎       | 77/330 [9:09:19<30:04:17, 427.89s/it]2025-09-27 15:58:24,910 Stage: Train 0.5 | Epoch: 77 | Iter: 117000 | Total Loss: 0.005238 | Recon Loss: 0.004579 | Commit Loss: 0.001318 | Perplexity: 1995.351330
2025-09-27 15:59:21,394 Stage: Train 0.5 | Epoch: 77 | Iter: 117200 | Total Loss: 0.005086 | Recon Loss: 0.004433 | Commit Loss: 0.001306 | Perplexity: 1996.399036
2025-09-27 16:00:17,793 Stage: Train 0.5 | Epoch: 77 | Iter: 117400 | Total Loss: 0.004894 | Recon Loss: 0.004236 | Commit Loss: 0.001317 | Perplexity: 1996.543097
2025-09-27 16:01:14,194 Stage: Train 0.5 | Epoch: 77 | Iter: 117600 | Total Loss: 0.004815 | Recon Loss: 0.004157 | Commit Loss: 0.001316 | Perplexity: 2001.102874
2025-09-27 16:02:10,355 Stage: Train 0.5 | Epoch: 77 | Iter: 117800 | Total Loss: 0.004994 | Recon Loss: 0.004331 | Commit Loss: 0.001327 | Perplexity: 1999.501731
2025-09-27 16:03:06,745 Stage: Train 0.5 | Epoch: 77 | Iter: 118000 | Total Loss: 0.004871 | Recon Loss: 0.004216 | Commit Loss: 0.001310 | Perplexity: 2000.192022
2025-09-27 16:04:02,791 Stage: Train 0.5 | Epoch: 77 | Iter: 118200 | Total Loss: 0.004934 | Recon Loss: 0.004270 | Commit Loss: 0.001329 | Perplexity: 1997.032454
2025-09-27 16:04:59,131 Stage: Train 0.5 | Epoch: 77 | Iter: 118400 | Total Loss: 0.005004 | Recon Loss: 0.004348 | Commit Loss: 0.001311 | Perplexity: 1995.169122
Trainning Epoch:  24%|██▎       | 78/330 [9:16:27<29:57:04, 427.87s/it]2025-09-27 16:05:55,416 Stage: Train 0.5 | Epoch: 78 | Iter: 118600 | Total Loss: 0.004887 | Recon Loss: 0.004233 | Commit Loss: 0.001309 | Perplexity: 1999.627302
2025-09-27 16:06:50,760 Stage: Train 0.5 | Epoch: 78 | Iter: 118800 | Total Loss: 0.004777 | Recon Loss: 0.004124 | Commit Loss: 0.001305 | Perplexity: 1998.084345
2025-09-27 16:07:47,100 Stage: Train 0.5 | Epoch: 78 | Iter: 119000 | Total Loss: 0.005029 | Recon Loss: 0.004369 | Commit Loss: 0.001320 | Perplexity: 2002.602385
2025-09-27 16:08:43,290 Stage: Train 0.5 | Epoch: 78 | Iter: 119200 | Total Loss: 0.004789 | Recon Loss: 0.004125 | Commit Loss: 0.001327 | Perplexity: 2000.889609
2025-09-27 16:09:39,331 Stage: Train 0.5 | Epoch: 78 | Iter: 119400 | Total Loss: 0.005116 | Recon Loss: 0.004460 | Commit Loss: 0.001312 | Perplexity: 2001.282910
2025-09-27 16:10:35,813 Stage: Train 0.5 | Epoch: 78 | Iter: 119600 | Total Loss: 0.004861 | Recon Loss: 0.004204 | Commit Loss: 0.001314 | Perplexity: 2002.299293
2025-09-27 16:11:32,269 Stage: Train 0.5 | Epoch: 78 | Iter: 119800 | Total Loss: 0.004888 | Recon Loss: 0.004233 | Commit Loss: 0.001311 | Perplexity: 2001.092581
2025-09-27 16:12:28,647 Stage: Train 0.5 | Epoch: 78 | Iter: 120000 | Total Loss: 0.004950 | Recon Loss: 0.004298 | Commit Loss: 0.001305 | Perplexity: 1993.120682
2025-09-27 16:12:28,648 Saving model at iteration 120000
2025-09-27 16:12:28,847 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000
2025-09-27 16:12:29,288 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/model.safetensors
2025-09-27 16:12:29,760 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/optimizer.bin
2025-09-27 16:12:29,761 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/scheduler.bin
2025-09-27 16:12:29,761 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/sampler.bin
2025-09-27 16:12:29,762 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/random_states_0.pkl
Trainning Epoch:  24%|██▍       | 79/330 [9:23:35<29:49:59, 427.88s/it]2025-09-27 16:13:26,398 Stage: Train 0.5 | Epoch: 79 | Iter: 120200 | Total Loss: 0.004816 | Recon Loss: 0.004160 | Commit Loss: 0.001313 | Perplexity: 1999.175096
2025-09-27 16:14:22,817 Stage: Train 0.5 | Epoch: 79 | Iter: 120400 | Total Loss: 0.005012 | Recon Loss: 0.004362 | Commit Loss: 0.001300 | Perplexity: 1999.212870
2025-09-27 16:15:18,855 Stage: Train 0.5 | Epoch: 79 | Iter: 120600 | Total Loss: 0.004909 | Recon Loss: 0.004254 | Commit Loss: 0.001310 | Perplexity: 1997.984547
2025-09-27 16:16:15,077 Stage: Train 0.5 | Epoch: 79 | Iter: 120800 | Total Loss: 0.005004 | Recon Loss: 0.004346 | Commit Loss: 0.001314 | Perplexity: 1995.578459
2025-09-27 16:17:11,513 Stage: Train 0.5 | Epoch: 79 | Iter: 121000 | Total Loss: 0.004861 | Recon Loss: 0.004207 | Commit Loss: 0.001308 | Perplexity: 1996.407006
2025-09-27 16:18:07,672 Stage: Train 0.5 | Epoch: 79 | Iter: 121200 | Total Loss: 0.004837 | Recon Loss: 0.004176 | Commit Loss: 0.001322 | Perplexity: 2000.190431
2025-09-27 16:19:03,998 Stage: Train 0.5 | Epoch: 79 | Iter: 121400 | Total Loss: 0.004861 | Recon Loss: 0.004206 | Commit Loss: 0.001309 | Perplexity: 1997.362370
Trainning Epoch:  24%|██▍       | 80/330 [9:30:42<29:42:40, 427.84s/it]2025-09-27 16:20:00,507 Stage: Train 0.5 | Epoch: 80 | Iter: 121600 | Total Loss: 0.004971 | Recon Loss: 0.004311 | Commit Loss: 0.001319 | Perplexity: 1992.249504
2025-09-27 16:20:56,912 Stage: Train 0.5 | Epoch: 80 | Iter: 121800 | Total Loss: 0.004869 | Recon Loss: 0.004219 | Commit Loss: 0.001300 | Perplexity: 1996.103879
2025-09-27 16:21:53,005 Stage: Train 0.5 | Epoch: 80 | Iter: 122000 | Total Loss: 0.004890 | Recon Loss: 0.004235 | Commit Loss: 0.001309 | Perplexity: 1999.686337
2025-09-27 16:22:49,406 Stage: Train 0.5 | Epoch: 80 | Iter: 122200 | Total Loss: 0.004705 | Recon Loss: 0.004052 | Commit Loss: 0.001306 | Perplexity: 1996.911622
2025-09-27 16:23:45,805 Stage: Train 0.5 | Epoch: 80 | Iter: 122400 | Total Loss: 0.004878 | Recon Loss: 0.004223 | Commit Loss: 0.001311 | Perplexity: 1998.169676
2025-09-27 16:24:42,263 Stage: Train 0.5 | Epoch: 80 | Iter: 122600 | Total Loss: 0.004837 | Recon Loss: 0.004181 | Commit Loss: 0.001312 | Perplexity: 2000.623377
2025-09-27 16:25:38,740 Stage: Train 0.5 | Epoch: 80 | Iter: 122800 | Total Loss: 0.004946 | Recon Loss: 0.004291 | Commit Loss: 0.001310 | Perplexity: 1999.033948
2025-09-27 16:26:35,081 Stage: Train 0.5 | Epoch: 80 | Iter: 123000 | Total Loss: 0.004766 | Recon Loss: 0.004109 | Commit Loss: 0.001315 | Perplexity: 1999.501534
Trainning Epoch:  25%|██▍       | 81/330 [9:37:51<29:36:04, 427.97s/it]2025-09-27 16:27:31,398 Stage: Train 0.5 | Epoch: 81 | Iter: 123200 | Total Loss: 0.004916 | Recon Loss: 0.004261 | Commit Loss: 0.001311 | Perplexity: 1995.886494
2025-09-27 16:28:27,760 Stage: Train 0.5 | Epoch: 81 | Iter: 123400 | Total Loss: 0.004846 | Recon Loss: 0.004196 | Commit Loss: 0.001300 | Perplexity: 1997.034938
2025-09-27 16:29:24,265 Stage: Train 0.5 | Epoch: 81 | Iter: 123600 | Total Loss: 0.004775 | Recon Loss: 0.004119 | Commit Loss: 0.001313 | Perplexity: 2000.178079
2025-09-27 16:30:20,734 Stage: Train 0.5 | Epoch: 81 | Iter: 123800 | Total Loss: 0.004829 | Recon Loss: 0.004171 | Commit Loss: 0.001315 | Perplexity: 2003.217300
2025-09-27 16:31:17,111 Stage: Train 0.5 | Epoch: 81 | Iter: 124000 | Total Loss: 0.004808 | Recon Loss: 0.004157 | Commit Loss: 0.001301 | Perplexity: 1998.219418
2025-09-27 16:32:13,425 Stage: Train 0.5 | Epoch: 81 | Iter: 124200 | Total Loss: 0.004801 | Recon Loss: 0.004143 | Commit Loss: 0.001315 | Perplexity: 2004.970751
2025-09-27 16:33:09,456 Stage: Train 0.5 | Epoch: 81 | Iter: 124400 | Total Loss: 0.004825 | Recon Loss: 0.004167 | Commit Loss: 0.001316 | Perplexity: 1995.379585
Trainning Epoch:  25%|██▍       | 82/330 [9:44:59<29:28:55, 427.97s/it]2025-09-27 16:34:06,039 Stage: Train 0.5 | Epoch: 82 | Iter: 124600 | Total Loss: 0.004821 | Recon Loss: 0.004170 | Commit Loss: 0.001301 | Perplexity: 1993.558316
2025-09-27 16:35:02,490 Stage: Train 0.5 | Epoch: 82 | Iter: 124800 | Total Loss: 0.004814 | Recon Loss: 0.004165 | Commit Loss: 0.001298 | Perplexity: 2001.743710
2025-09-27 16:35:58,807 Stage: Train 0.5 | Epoch: 82 | Iter: 125000 | Total Loss: 0.004718 | Recon Loss: 0.004060 | Commit Loss: 0.001317 | Perplexity: 2000.433797
2025-09-27 16:36:55,341 Stage: Train 0.5 | Epoch: 82 | Iter: 125200 | Total Loss: 0.004764 | Recon Loss: 0.004103 | Commit Loss: 0.001322 | Perplexity: 2000.012291
2025-09-27 16:37:51,669 Stage: Train 0.5 | Epoch: 82 | Iter: 125400 | Total Loss: 0.004818 | Recon Loss: 0.004159 | Commit Loss: 0.001317 | Perplexity: 1998.521257
2025-09-27 16:38:47,686 Stage: Train 0.5 | Epoch: 82 | Iter: 125600 | Total Loss: 0.004942 | Recon Loss: 0.004294 | Commit Loss: 0.001297 | Perplexity: 1992.118907
2025-09-27 16:39:44,166 Stage: Train 0.5 | Epoch: 82 | Iter: 125800 | Total Loss: 0.004908 | Recon Loss: 0.004251 | Commit Loss: 0.001315 | Perplexity: 1999.864879
2025-09-27 16:40:40,613 Stage: Train 0.5 | Epoch: 82 | Iter: 126000 | Total Loss: 0.004786 | Recon Loss: 0.004131 | Commit Loss: 0.001309 | Perplexity: 2000.682524
Trainning Epoch:  25%|██▌       | 83/330 [9:52:07<29:22:09, 428.05s/it]2025-09-27 16:41:37,071 Stage: Train 0.5 | Epoch: 83 | Iter: 126200 | Total Loss: 0.004798 | Recon Loss: 0.004145 | Commit Loss: 0.001306 | Perplexity: 1997.996133
2025-09-27 16:42:33,390 Stage: Train 0.5 | Epoch: 83 | Iter: 126400 | Total Loss: 0.004821 | Recon Loss: 0.004164 | Commit Loss: 0.001314 | Perplexity: 2001.060439
2025-09-27 16:43:29,520 Stage: Train 0.5 | Epoch: 83 | Iter: 126600 | Total Loss: 0.004769 | Recon Loss: 0.004120 | Commit Loss: 0.001298 | Perplexity: 2001.919007
2025-09-27 16:44:25,802 Stage: Train 0.5 | Epoch: 83 | Iter: 126800 | Total Loss: 0.004806 | Recon Loss: 0.004155 | Commit Loss: 0.001301 | Perplexity: 1997.689783
2025-09-27 16:45:21,766 Stage: Train 0.5 | Epoch: 83 | Iter: 127000 | Total Loss: 0.004826 | Recon Loss: 0.004170 | Commit Loss: 0.001314 | Perplexity: 1998.220573
2025-09-27 16:46:18,024 Stage: Train 0.5 | Epoch: 83 | Iter: 127200 | Total Loss: 0.004737 | Recon Loss: 0.004078 | Commit Loss: 0.001317 | Perplexity: 1997.460919
2025-09-27 16:47:14,380 Stage: Train 0.5 | Epoch: 83 | Iter: 127400 | Total Loss: 0.004806 | Recon Loss: 0.004146 | Commit Loss: 0.001320 | Perplexity: 1999.731227
Trainning Epoch:  25%|██▌       | 84/330 [9:59:14<29:14:01, 427.81s/it]2025-09-27 16:48:10,812 Stage: Train 0.5 | Epoch: 84 | Iter: 127600 | Total Loss: 0.004812 | Recon Loss: 0.004163 | Commit Loss: 0.001297 | Perplexity: 1992.075532
2025-09-27 16:49:07,159 Stage: Train 0.5 | Epoch: 84 | Iter: 127800 | Total Loss: 0.004725 | Recon Loss: 0.004072 | Commit Loss: 0.001305 | Perplexity: 1993.376497
2025-09-27 16:50:03,469 Stage: Train 0.5 | Epoch: 84 | Iter: 128000 | Total Loss: 0.004754 | Recon Loss: 0.004102 | Commit Loss: 0.001303 | Perplexity: 1999.902686
2025-09-27 16:50:59,560 Stage: Train 0.5 | Epoch: 84 | Iter: 128200 | Total Loss: 0.004988 | Recon Loss: 0.004335 | Commit Loss: 0.001306 | Perplexity: 2000.204791
2025-09-27 16:51:55,797 Stage: Train 0.5 | Epoch: 84 | Iter: 128400 | Total Loss: 0.004709 | Recon Loss: 0.004058 | Commit Loss: 0.001302 | Perplexity: 2000.141498
2025-09-27 16:52:52,119 Stage: Train 0.5 | Epoch: 84 | Iter: 128600 | Total Loss: 0.004728 | Recon Loss: 0.004071 | Commit Loss: 0.001314 | Perplexity: 2000.187518
2025-09-27 16:53:48,319 Stage: Train 0.5 | Epoch: 84 | Iter: 128800 | Total Loss: 0.004814 | Recon Loss: 0.004160 | Commit Loss: 0.001308 | Perplexity: 1999.500731
2025-09-27 16:54:44,810 Stage: Train 0.5 | Epoch: 84 | Iter: 129000 | Total Loss: 0.004765 | Recon Loss: 0.004108 | Commit Loss: 0.001314 | Perplexity: 2000.749172
Trainning Epoch:  26%|██▌       | 85/330 [10:06:22<29:06:42, 427.77s/it]2025-09-27 16:55:41,273 Stage: Train 0.5 | Epoch: 85 | Iter: 129200 | Total Loss: 0.004892 | Recon Loss: 0.004240 | Commit Loss: 0.001304 | Perplexity: 1994.907498
2025-09-27 16:56:37,220 Stage: Train 0.5 | Epoch: 85 | Iter: 129400 | Total Loss: 0.004746 | Recon Loss: 0.004090 | Commit Loss: 0.001312 | Perplexity: 1999.426171
2025-09-27 16:57:33,614 Stage: Train 0.5 | Epoch: 85 | Iter: 129600 | Total Loss: 0.004751 | Recon Loss: 0.004098 | Commit Loss: 0.001306 | Perplexity: 1997.900367
2025-09-27 16:58:29,909 Stage: Train 0.5 | Epoch: 85 | Iter: 129800 | Total Loss: 0.004719 | Recon Loss: 0.004063 | Commit Loss: 0.001312 | Perplexity: 2004.017224
2025-09-27 16:59:26,338 Stage: Train 0.5 | Epoch: 85 | Iter: 130000 | Total Loss: 0.004808 | Recon Loss: 0.004159 | Commit Loss: 0.001299 | Perplexity: 1991.393788
2025-09-27 17:00:22,384 Stage: Train 0.5 | Epoch: 85 | Iter: 130200 | Total Loss: 0.004778 | Recon Loss: 0.004127 | Commit Loss: 0.001302 | Perplexity: 1997.492275
2025-09-27 17:01:18,711 Stage: Train 0.5 | Epoch: 85 | Iter: 130400 | Total Loss: 0.004797 | Recon Loss: 0.004143 | Commit Loss: 0.001306 | Perplexity: 1998.860432
2025-09-27 17:02:14,828 Stage: Train 0.5 | Epoch: 85 | Iter: 130600 | Total Loss: 0.004747 | Recon Loss: 0.004095 | Commit Loss: 0.001303 | Perplexity: 1999.034749
Trainning Epoch:  26%|██▌       | 86/330 [10:13:29<28:59:02, 427.63s/it]2025-09-27 17:03:11,447 Stage: Train 0.5 | Epoch: 86 | Iter: 130800 | Total Loss: 0.004756 | Recon Loss: 0.004110 | Commit Loss: 0.001291 | Perplexity: 1991.817914
2025-09-27 17:04:07,894 Stage: Train 0.5 | Epoch: 86 | Iter: 131000 | Total Loss: 0.004737 | Recon Loss: 0.004079 | Commit Loss: 0.001315 | Perplexity: 2003.315086
2025-09-27 17:05:04,291 Stage: Train 0.5 | Epoch: 86 | Iter: 131200 | Total Loss: 0.004701 | Recon Loss: 0.004056 | Commit Loss: 0.001290 | Perplexity: 1992.174092
2025-09-27 17:06:00,624 Stage: Train 0.5 | Epoch: 86 | Iter: 131400 | Total Loss: 0.004681 | Recon Loss: 0.004026 | Commit Loss: 0.001310 | Perplexity: 1999.452388
2025-09-27 17:06:57,025 Stage: Train 0.5 | Epoch: 86 | Iter: 131600 | Total Loss: 0.004744 | Recon Loss: 0.004086 | Commit Loss: 0.001314 | Perplexity: 1998.329099
2025-09-27 17:07:53,223 Stage: Train 0.5 | Epoch: 86 | Iter: 131800 | Total Loss: 0.004769 | Recon Loss: 0.004117 | Commit Loss: 0.001303 | Perplexity: 1996.181870
2025-09-27 17:08:49,285 Stage: Train 0.5 | Epoch: 86 | Iter: 132000 | Total Loss: 0.004770 | Recon Loss: 0.004119 | Commit Loss: 0.001303 | Perplexity: 2000.532101
Trainning Epoch:  26%|██▋       | 87/330 [10:20:37<28:52:20, 427.74s/it]2025-09-27 17:09:45,868 Stage: Train 0.5 | Epoch: 87 | Iter: 132200 | Total Loss: 0.004788 | Recon Loss: 0.004133 | Commit Loss: 0.001311 | Perplexity: 2002.955573
2025-09-27 17:10:42,133 Stage: Train 0.5 | Epoch: 87 | Iter: 132400 | Total Loss: 0.004654 | Recon Loss: 0.004000 | Commit Loss: 0.001308 | Perplexity: 2002.038808
2025-09-27 17:11:38,645 Stage: Train 0.5 | Epoch: 87 | Iter: 132600 | Total Loss: 0.004793 | Recon Loss: 0.004141 | Commit Loss: 0.001303 | Perplexity: 1998.674969
2025-09-27 17:12:35,088 Stage: Train 0.5 | Epoch: 87 | Iter: 132800 | Total Loss: 0.004708 | Recon Loss: 0.004055 | Commit Loss: 0.001305 | Perplexity: 2000.640043
2025-09-27 17:13:31,577 Stage: Train 0.5 | Epoch: 87 | Iter: 133000 | Total Loss: 0.004884 | Recon Loss: 0.004237 | Commit Loss: 0.001295 | Perplexity: 1996.757064
2025-09-27 17:14:27,482 Stage: Train 0.5 | Epoch: 87 | Iter: 133200 | Total Loss: 0.004759 | Recon Loss: 0.004113 | Commit Loss: 0.001293 | Perplexity: 2000.036348
2025-09-27 17:15:23,951 Stage: Train 0.5 | Epoch: 87 | Iter: 133400 | Total Loss: 0.004808 | Recon Loss: 0.004156 | Commit Loss: 0.001304 | Perplexity: 2004.016431
2025-09-27 17:16:20,351 Stage: Train 0.5 | Epoch: 87 | Iter: 133600 | Total Loss: 0.004647 | Recon Loss: 0.003995 | Commit Loss: 0.001304 | Perplexity: 1996.128501
Trainning Epoch:  27%|██▋       | 88/330 [10:27:45<28:45:49, 427.89s/it]2025-09-27 17:17:16,892 Stage: Train 0.5 | Epoch: 88 | Iter: 133800 | Total Loss: 0.004848 | Recon Loss: 0.004201 | Commit Loss: 0.001293 | Perplexity: 1995.757264
2025-09-27 17:18:13,211 Stage: Train 0.5 | Epoch: 88 | Iter: 134000 | Total Loss: 0.004653 | Recon Loss: 0.003997 | Commit Loss: 0.001312 | Perplexity: 2009.772396
2025-09-27 17:19:09,506 Stage: Train 0.5 | Epoch: 88 | Iter: 134200 | Total Loss: 0.004637 | Recon Loss: 0.003984 | Commit Loss: 0.001305 | Perplexity: 2001.585149
2025-09-27 17:20:05,234 Stage: Train 0.5 | Epoch: 88 | Iter: 134400 | Total Loss: 0.004779 | Recon Loss: 0.004133 | Commit Loss: 0.001293 | Perplexity: 2005.713498
2025-09-27 17:21:01,639 Stage: Train 0.5 | Epoch: 88 | Iter: 134600 | Total Loss: 0.004677 | Recon Loss: 0.004029 | Commit Loss: 0.001296 | Perplexity: 1995.214089
2025-09-27 17:21:57,968 Stage: Train 0.5 | Epoch: 88 | Iter: 134800 | Total Loss: 0.004761 | Recon Loss: 0.004112 | Commit Loss: 0.001298 | Perplexity: 2003.121463
2025-09-27 17:22:54,303 Stage: Train 0.5 | Epoch: 88 | Iter: 135000 | Total Loss: 0.004664 | Recon Loss: 0.004011 | Commit Loss: 0.001306 | Perplexity: 2001.657540
Trainning Epoch:  27%|██▋       | 89/330 [10:34:51<28:36:27, 427.33s/it]2025-09-27 17:23:49,608 Stage: Train 0.5 | Epoch: 89 | Iter: 135200 | Total Loss: 0.004708 | Recon Loss: 0.004058 | Commit Loss: 0.001299 | Perplexity: 1999.002954
2025-09-27 17:24:45,950 Stage: Train 0.5 | Epoch: 89 | Iter: 135400 | Total Loss: 0.005065 | Recon Loss: 0.004419 | Commit Loss: 0.001291 | Perplexity: 1995.851541
2025-09-27 17:25:41,876 Stage: Train 0.5 | Epoch: 89 | Iter: 135600 | Total Loss: 0.004682 | Recon Loss: 0.004036 | Commit Loss: 0.001292 | Perplexity: 2000.474432
2025-09-27 17:26:38,197 Stage: Train 0.5 | Epoch: 89 | Iter: 135800 | Total Loss: 0.004740 | Recon Loss: 0.004096 | Commit Loss: 0.001288 | Perplexity: 2002.297570
2025-09-27 17:27:34,404 Stage: Train 0.5 | Epoch: 89 | Iter: 136000 | Total Loss: 0.004628 | Recon Loss: 0.003984 | Commit Loss: 0.001288 | Perplexity: 2002.445416
2025-09-27 17:28:30,967 Stage: Train 0.5 | Epoch: 89 | Iter: 136200 | Total Loss: 0.004651 | Recon Loss: 0.004001 | Commit Loss: 0.001301 | Perplexity: 2004.107729
2025-09-27 17:29:27,231 Stage: Train 0.5 | Epoch: 89 | Iter: 136400 | Total Loss: 0.004638 | Recon Loss: 0.003988 | Commit Loss: 0.001299 | Perplexity: 2000.204562
2025-09-27 17:30:23,686 Stage: Train 0.5 | Epoch: 89 | Iter: 136600 | Total Loss: 0.004757 | Recon Loss: 0.004107 | Commit Loss: 0.001301 | Perplexity: 2003.912727
Trainning Epoch:  27%|██▋       | 90/330 [10:41:59<28:29:53, 427.47s/it]2025-09-27 17:31:20,093 Stage: Train 0.5 | Epoch: 90 | Iter: 136800 | Total Loss: 0.004712 | Recon Loss: 0.004063 | Commit Loss: 0.001297 | Perplexity: 2003.087712
2025-09-27 17:32:16,154 Stage: Train 0.5 | Epoch: 90 | Iter: 137000 | Total Loss: 0.004623 | Recon Loss: 0.003976 | Commit Loss: 0.001294 | Perplexity: 2001.950614
2025-09-27 17:33:12,617 Stage: Train 0.5 | Epoch: 90 | Iter: 137200 | Total Loss: 0.004836 | Recon Loss: 0.004187 | Commit Loss: 0.001297 | Perplexity: 2004.823397
2025-09-27 17:34:09,005 Stage: Train 0.5 | Epoch: 90 | Iter: 137400 | Total Loss: 0.004665 | Recon Loss: 0.004017 | Commit Loss: 0.001295 | Perplexity: 2006.409460
2025-09-27 17:35:05,492 Stage: Train 0.5 | Epoch: 90 | Iter: 137600 | Total Loss: 0.004666 | Recon Loss: 0.004022 | Commit Loss: 0.001288 | Perplexity: 2005.113041
2025-09-27 17:36:02,037 Stage: Train 0.5 | Epoch: 90 | Iter: 137800 | Total Loss: 0.004892 | Recon Loss: 0.004247 | Commit Loss: 0.001292 | Perplexity: 2000.987429
2025-09-27 17:36:58,459 Stage: Train 0.5 | Epoch: 90 | Iter: 138000 | Total Loss: 0.004591 | Recon Loss: 0.003942 | Commit Loss: 0.001297 | Perplexity: 2000.117599
2025-09-27 17:37:54,427 Stage: Train 0.5 | Epoch: 90 | Iter: 138200 | Total Loss: 0.004747 | Recon Loss: 0.004103 | Commit Loss: 0.001288 | Perplexity: 2002.848323
Trainning Epoch:  28%|██▊       | 91/330 [10:49:07<28:23:31, 427.66s/it]2025-09-27 17:38:51,042 Stage: Train 0.5 | Epoch: 91 | Iter: 138400 | Total Loss: 0.004662 | Recon Loss: 0.004013 | Commit Loss: 0.001299 | Perplexity: 2003.989714
2025-09-27 17:39:47,536 Stage: Train 0.5 | Epoch: 91 | Iter: 138600 | Total Loss: 0.004698 | Recon Loss: 0.004056 | Commit Loss: 0.001284 | Perplexity: 1999.890284
2025-09-27 17:40:43,725 Stage: Train 0.5 | Epoch: 91 | Iter: 138800 | Total Loss: 0.004620 | Recon Loss: 0.003973 | Commit Loss: 0.001293 | Perplexity: 1998.622935
2025-09-27 17:41:40,154 Stage: Train 0.5 | Epoch: 91 | Iter: 139000 | Total Loss: 0.004705 | Recon Loss: 0.004059 | Commit Loss: 0.001293 | Perplexity: 2002.655306
2025-09-27 17:42:36,657 Stage: Train 0.5 | Epoch: 91 | Iter: 139200 | Total Loss: 0.004631 | Recon Loss: 0.003982 | Commit Loss: 0.001298 | Perplexity: 2008.513702
2025-09-27 17:43:32,633 Stage: Train 0.5 | Epoch: 91 | Iter: 139400 | Total Loss: 0.004751 | Recon Loss: 0.004106 | Commit Loss: 0.001291 | Perplexity: 1999.515209
2025-09-27 17:44:28,957 Stage: Train 0.5 | Epoch: 91 | Iter: 139600 | Total Loss: 0.004625 | Recon Loss: 0.003976 | Commit Loss: 0.001298 | Perplexity: 2002.329940
Trainning Epoch:  28%|██▊       | 92/330 [10:56:15<28:16:50, 427.78s/it]2025-09-27 17:45:25,484 Stage: Train 0.5 | Epoch: 92 | Iter: 139800 | Total Loss: 0.004792 | Recon Loss: 0.004151 | Commit Loss: 0.001281 | Perplexity: 2001.450748
2025-09-27 17:46:21,780 Stage: Train 0.5 | Epoch: 92 | Iter: 140000 | Total Loss: 0.004715 | Recon Loss: 0.004069 | Commit Loss: 0.001292 | Perplexity: 2006.316221
2025-09-27 17:46:21,780 Saving model at iteration 140000
2025-09-27 17:46:21,958 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000
2025-09-27 17:46:22,426 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/model.safetensors
2025-09-27 17:46:22,941 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/optimizer.bin
2025-09-27 17:46:22,941 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/scheduler.bin
2025-09-27 17:46:22,941 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/sampler.bin
2025-09-27 17:46:22,942 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/random_states_0.pkl
2025-09-27 17:47:19,659 Stage: Train 0.5 | Epoch: 92 | Iter: 140200 | Total Loss: 0.004575 | Recon Loss: 0.003928 | Commit Loss: 0.001293 | Perplexity: 2006.002692
2025-09-27 17:48:15,980 Stage: Train 0.5 | Epoch: 92 | Iter: 140400 | Total Loss: 0.004670 | Recon Loss: 0.004023 | Commit Loss: 0.001294 | Perplexity: 2004.629221
2025-09-27 17:49:11,955 Stage: Train 0.5 | Epoch: 92 | Iter: 140600 | Total Loss: 0.004702 | Recon Loss: 0.004060 | Commit Loss: 0.001284 | Perplexity: 1996.005519
2025-09-27 17:50:08,425 Stage: Train 0.5 | Epoch: 92 | Iter: 140800 | Total Loss: 0.004739 | Recon Loss: 0.004094 | Commit Loss: 0.001291 | Perplexity: 2003.878397
2025-09-27 17:51:04,902 Stage: Train 0.5 | Epoch: 92 | Iter: 141000 | Total Loss: 0.004545 | Recon Loss: 0.003900 | Commit Loss: 0.001290 | Perplexity: 2003.091107
2025-09-27 17:52:01,242 Stage: Train 0.5 | Epoch: 92 | Iter: 141200 | Total Loss: 0.004639 | Recon Loss: 0.003999 | Commit Loss: 0.001281 | Perplexity: 2000.182209
Trainning Epoch:  28%|██▊       | 93/330 [11:03:25<28:11:44, 428.29s/it]2025-09-27 17:52:57,993 Stage: Train 0.5 | Epoch: 93 | Iter: 141400 | Total Loss: 0.004611 | Recon Loss: 0.003964 | Commit Loss: 0.001294 | Perplexity: 1998.590636
2025-09-27 17:53:54,372 Stage: Train 0.5 | Epoch: 93 | Iter: 141600 | Total Loss: 0.004633 | Recon Loss: 0.003991 | Commit Loss: 0.001284 | Perplexity: 2001.270782
2025-09-27 17:54:50,206 Stage: Train 0.5 | Epoch: 93 | Iter: 141800 | Total Loss: 0.004693 | Recon Loss: 0.004050 | Commit Loss: 0.001287 | Perplexity: 1999.187020
2025-09-27 17:55:46,510 Stage: Train 0.5 | Epoch: 93 | Iter: 142000 | Total Loss: 0.004640 | Recon Loss: 0.003999 | Commit Loss: 0.001282 | Perplexity: 1996.256238
2025-09-27 17:56:42,851 Stage: Train 0.5 | Epoch: 93 | Iter: 142200 | Total Loss: 0.004616 | Recon Loss: 0.003971 | Commit Loss: 0.001290 | Perplexity: 2005.162751
2025-09-27 17:57:39,134 Stage: Train 0.5 | Epoch: 93 | Iter: 142400 | Total Loss: 0.004611 | Recon Loss: 0.003968 | Commit Loss: 0.001286 | Perplexity: 2004.688775
2025-09-27 17:58:35,532 Stage: Train 0.5 | Epoch: 93 | Iter: 142600 | Total Loss: 0.004608 | Recon Loss: 0.003957 | Commit Loss: 0.001302 | Perplexity: 2006.502747
Trainning Epoch:  28%|██▊       | 94/330 [11:10:33<28:04:03, 428.15s/it]2025-09-27 17:59:32,134 Stage: Train 0.5 | Epoch: 94 | Iter: 142800 | Total Loss: 0.004624 | Recon Loss: 0.003977 | Commit Loss: 0.001293 | Perplexity: 2006.108251
2025-09-27 18:00:28,386 Stage: Train 0.5 | Epoch: 94 | Iter: 143000 | Total Loss: 0.004918 | Recon Loss: 0.004274 | Commit Loss: 0.001288 | Perplexity: 2007.071503
2025-09-27 18:01:24,383 Stage: Train 0.5 | Epoch: 94 | Iter: 143200 | Total Loss: 0.004441 | Recon Loss: 0.003801 | Commit Loss: 0.001280 | Perplexity: 2006.206462
2025-09-27 18:02:20,967 Stage: Train 0.5 | Epoch: 94 | Iter: 143400 | Total Loss: 0.004585 | Recon Loss: 0.003943 | Commit Loss: 0.001285 | Perplexity: 2004.389828
2025-09-27 18:03:17,386 Stage: Train 0.5 | Epoch: 94 | Iter: 143600 | Total Loss: 0.004597 | Recon Loss: 0.003957 | Commit Loss: 0.001281 | Perplexity: 2001.219229
2025-09-27 18:04:13,714 Stage: Train 0.5 | Epoch: 94 | Iter: 143800 | Total Loss: 0.004618 | Recon Loss: 0.003975 | Commit Loss: 0.001287 | Perplexity: 2000.664401
2025-09-27 18:05:10,147 Stage: Train 0.5 | Epoch: 94 | Iter: 144000 | Total Loss: 0.004642 | Recon Loss: 0.003995 | Commit Loss: 0.001294 | Perplexity: 2009.592406
2025-09-27 18:06:06,656 Stage: Train 0.5 | Epoch: 94 | Iter: 144200 | Total Loss: 0.004719 | Recon Loss: 0.004075 | Commit Loss: 0.001287 | Perplexity: 2004.469613
Trainning Epoch:  29%|██▉       | 95/330 [11:17:40<27:56:33, 428.06s/it]2025-09-27 18:07:02,834 Stage: Train 0.5 | Epoch: 95 | Iter: 144400 | Total Loss: 0.004604 | Recon Loss: 0.003962 | Commit Loss: 0.001285 | Perplexity: 2003.081442
2025-09-27 18:07:59,444 Stage: Train 0.5 | Epoch: 95 | Iter: 144600 | Total Loss: 0.004671 | Recon Loss: 0.004030 | Commit Loss: 0.001282 | Perplexity: 2004.976008
2025-09-27 18:08:55,874 Stage: Train 0.5 | Epoch: 95 | Iter: 144800 | Total Loss: 0.004537 | Recon Loss: 0.003895 | Commit Loss: 0.001284 | Perplexity: 2008.704817
2025-09-27 18:09:52,213 Stage: Train 0.5 | Epoch: 95 | Iter: 145000 | Total Loss: 0.004612 | Recon Loss: 0.003971 | Commit Loss: 0.001283 | Perplexity: 2001.158789
2025-09-27 18:10:48,702 Stage: Train 0.5 | Epoch: 95 | Iter: 145200 | Total Loss: 0.004606 | Recon Loss: 0.003964 | Commit Loss: 0.001284 | Perplexity: 2004.641879
2025-09-27 18:11:45,120 Stage: Train 0.5 | Epoch: 95 | Iter: 145400 | Total Loss: 0.004573 | Recon Loss: 0.003929 | Commit Loss: 0.001288 | Perplexity: 2003.537041
2025-09-27 18:12:41,336 Stage: Train 0.5 | Epoch: 95 | Iter: 145600 | Total Loss: 0.004653 | Recon Loss: 0.004012 | Commit Loss: 0.001281 | Perplexity: 2001.642968
2025-09-27 18:13:37,666 Stage: Train 0.5 | Epoch: 95 | Iter: 145800 | Total Loss: 0.004645 | Recon Loss: 0.004007 | Commit Loss: 0.001276 | Perplexity: 1995.609500
Trainning Epoch:  29%|██▉       | 96/330 [11:24:49<27:50:06, 428.23s/it]2025-09-27 18:14:34,161 Stage: Train 0.5 | Epoch: 96 | Iter: 146000 | Total Loss: 0.004646 | Recon Loss: 0.004008 | Commit Loss: 0.001277 | Perplexity: 2000.320555
2025-09-27 18:15:30,451 Stage: Train 0.5 | Epoch: 96 | Iter: 146200 | Total Loss: 0.004620 | Recon Loss: 0.003980 | Commit Loss: 0.001278 | Perplexity: 2003.169371
2025-09-27 18:16:26,702 Stage: Train 0.5 | Epoch: 96 | Iter: 146400 | Total Loss: 0.004516 | Recon Loss: 0.003878 | Commit Loss: 0.001275 | Perplexity: 2002.349325
2025-09-27 18:17:22,983 Stage: Train 0.5 | Epoch: 96 | Iter: 146600 | Total Loss: 0.004611 | Recon Loss: 0.003971 | Commit Loss: 0.001280 | Perplexity: 2011.251226
2025-09-27 18:18:18,988 Stage: Train 0.5 | Epoch: 96 | Iter: 146800 | Total Loss: 0.004882 | Recon Loss: 0.004245 | Commit Loss: 0.001274 | Perplexity: 2000.949399
2025-09-27 18:19:15,419 Stage: Train 0.5 | Epoch: 96 | Iter: 147000 | Total Loss: 0.004531 | Recon Loss: 0.003891 | Commit Loss: 0.001280 | Perplexity: 2004.907073
2025-09-27 18:20:11,888 Stage: Train 0.5 | Epoch: 96 | Iter: 147200 | Total Loss: 0.004604 | Recon Loss: 0.003959 | Commit Loss: 0.001289 | Perplexity: 2006.671794
Trainning Epoch:  29%|██▉       | 97/330 [11:31:57<27:42:15, 428.05s/it]2025-09-27 18:21:08,323 Stage: Train 0.5 | Epoch: 97 | Iter: 147400 | Total Loss: 0.004599 | Recon Loss: 0.003954 | Commit Loss: 0.001289 | Perplexity: 2004.749518
2025-09-27 18:22:04,731 Stage: Train 0.5 | Epoch: 97 | Iter: 147600 | Total Loss: 0.004521 | Recon Loss: 0.003876 | Commit Loss: 0.001291 | Perplexity: 2009.676336
2025-09-27 18:23:01,049 Stage: Train 0.5 | Epoch: 97 | Iter: 147800 | Total Loss: 0.004582 | Recon Loss: 0.003942 | Commit Loss: 0.001279 | Perplexity: 2003.491487
2025-09-27 18:23:57,395 Stage: Train 0.5 | Epoch: 97 | Iter: 148000 | Total Loss: 0.004625 | Recon Loss: 0.003993 | Commit Loss: 0.001264 | Perplexity: 1999.743485
2025-09-27 18:24:53,423 Stage: Train 0.5 | Epoch: 97 | Iter: 148200 | Total Loss: 0.004654 | Recon Loss: 0.004020 | Commit Loss: 0.001268 | Perplexity: 2000.394927
2025-09-27 18:25:49,617 Stage: Train 0.5 | Epoch: 97 | Iter: 148400 | Total Loss: 0.004616 | Recon Loss: 0.003981 | Commit Loss: 0.001271 | Perplexity: 2002.013033
2025-09-27 18:26:45,755 Stage: Train 0.5 | Epoch: 97 | Iter: 148600 | Total Loss: 0.004630 | Recon Loss: 0.003986 | Commit Loss: 0.001289 | Perplexity: 2005.931307
2025-09-27 18:27:42,100 Stage: Train 0.5 | Epoch: 97 | Iter: 148800 | Total Loss: 0.004562 | Recon Loss: 0.003922 | Commit Loss: 0.001280 | Perplexity: 2008.551807
Trainning Epoch:  30%|██▉       | 98/330 [11:39:04<27:34:19, 427.84s/it]2025-09-27 18:28:38,503 Stage: Train 0.5 | Epoch: 98 | Iter: 149000 | Total Loss: 0.004675 | Recon Loss: 0.004038 | Commit Loss: 0.001276 | Perplexity: 2002.379266
2025-09-27 18:29:34,741 Stage: Train 0.5 | Epoch: 98 | Iter: 149200 | Total Loss: 0.004530 | Recon Loss: 0.003899 | Commit Loss: 0.001262 | Perplexity: 2001.225059
2025-09-27 18:30:30,404 Stage: Train 0.5 | Epoch: 98 | Iter: 149400 | Total Loss: 0.004479 | Recon Loss: 0.003836 | Commit Loss: 0.001287 | Perplexity: 2005.294340
2025-09-27 18:31:26,757 Stage: Train 0.5 | Epoch: 98 | Iter: 149600 | Total Loss: 0.004702 | Recon Loss: 0.004054 | Commit Loss: 0.001296 | Perplexity: 2006.294321
2025-09-27 18:32:23,208 Stage: Train 0.5 | Epoch: 98 | Iter: 149800 | Total Loss: 0.004729 | Recon Loss: 0.004095 | Commit Loss: 0.001269 | Perplexity: 1997.991013
2025-09-27 18:33:19,424 Stage: Train 0.5 | Epoch: 98 | Iter: 150000 | Total Loss: 0.004527 | Recon Loss: 0.003890 | Commit Loss: 0.001274 | Perplexity: 2000.116809
2025-09-27 18:34:15,788 Stage: Train 0.5 | Epoch: 98 | Iter: 150200 | Total Loss: 0.004597 | Recon Loss: 0.003965 | Commit Loss: 0.001264 | Perplexity: 2001.543502
Trainning Epoch:  30%|███       | 99/330 [11:46:11<27:26:36, 427.69s/it]2025-09-27 18:35:12,371 Stage: Train 0.5 | Epoch: 99 | Iter: 150400 | Total Loss: 0.004619 | Recon Loss: 0.003988 | Commit Loss: 0.001262 | Perplexity: 1996.212523
2025-09-27 18:36:08,546 Stage: Train 0.5 | Epoch: 99 | Iter: 150600 | Total Loss: 0.004563 | Recon Loss: 0.003931 | Commit Loss: 0.001265 | Perplexity: 2001.754321
2025-09-27 18:37:05,025 Stage: Train 0.5 | Epoch: 99 | Iter: 150800 | Total Loss: 0.004450 | Recon Loss: 0.003811 | Commit Loss: 0.001277 | Perplexity: 2004.111426
2025-09-27 18:38:01,549 Stage: Train 0.5 | Epoch: 99 | Iter: 151000 | Total Loss: 0.004569 | Recon Loss: 0.003932 | Commit Loss: 0.001274 | Perplexity: 2005.754426
2025-09-27 18:38:57,854 Stage: Train 0.5 | Epoch: 99 | Iter: 151200 | Total Loss: 0.004574 | Recon Loss: 0.003943 | Commit Loss: 0.001262 | Perplexity: 1997.025117
2025-09-27 18:39:54,509 Stage: Train 0.5 | Epoch: 99 | Iter: 151400 | Total Loss: 0.004539 | Recon Loss: 0.003902 | Commit Loss: 0.001274 | Perplexity: 1997.394200
2025-09-27 18:40:49,566 Stage: Train 0.5 | Epoch: 99 | Iter: 151600 | Total Loss: 0.004629 | Recon Loss: 0.003991 | Commit Loss: 0.001275 | Perplexity: 2005.380521
2025-09-27 18:41:45,593 Stage: Train 0.5 | Epoch: 99 | Iter: 151800 | Total Loss: 0.004487 | Recon Loss: 0.003848 | Commit Loss: 0.001279 | Perplexity: 2010.937706
Trainning Epoch:  30%|███       | 100/330 [11:53:18<27:18:52, 427.53s/it]2025-09-27 18:42:42,352 Stage: Train 0.5 | Epoch: 100 | Iter: 152000 | Total Loss: 0.004517 | Recon Loss: 0.003885 | Commit Loss: 0.001265 | Perplexity: 2002.694837
2025-09-27 18:43:38,751 Stage: Train 0.5 | Epoch: 100 | Iter: 152200 | Total Loss: 0.004485 | Recon Loss: 0.003849 | Commit Loss: 0.001274 | Perplexity: 2006.861630
2025-09-27 18:44:35,102 Stage: Train 0.5 | Epoch: 100 | Iter: 152400 | Total Loss: 0.004515 | Recon Loss: 0.003877 | Commit Loss: 0.001275 | Perplexity: 2001.772947
2025-09-27 18:45:31,334 Stage: Train 0.5 | Epoch: 100 | Iter: 152600 | Total Loss: 0.004609 | Recon Loss: 0.003976 | Commit Loss: 0.001268 | Perplexity: 2006.581955
2025-09-27 18:46:27,745 Stage: Train 0.5 | Epoch: 100 | Iter: 152800 | Total Loss: 0.004491 | Recon Loss: 0.003855 | Commit Loss: 0.001272 | Perplexity: 2000.069515
2025-09-27 18:47:24,038 Stage: Train 0.5 | Epoch: 100 | Iter: 153000 | Total Loss: 0.004539 | Recon Loss: 0.003899 | Commit Loss: 0.001281 | Perplexity: 2008.385336
2025-09-27 18:48:19,949 Stage: Train 0.5 | Epoch: 100 | Iter: 153200 | Total Loss: 0.004585 | Recon Loss: 0.003951 | Commit Loss: 0.001268 | Perplexity: 2001.331104
2025-09-27 18:49:16,110 Stage: Train 0.5 | Epoch: 100 | Iter: 153400 | Total Loss: 0.004583 | Recon Loss: 0.003940 | Commit Loss: 0.001287 | Perplexity: 2012.881278
Trainning Epoch:  31%|███       | 101/330 [12:00:26<27:11:39, 427.51s/it]2025-09-27 18:50:12,580 Stage: Train 0.5 | Epoch: 101 | Iter: 153600 | Total Loss: 0.004436 | Recon Loss: 0.003799 | Commit Loss: 0.001273 | Perplexity: 2001.325499
2025-09-27 18:51:09,013 Stage: Train 0.5 | Epoch: 101 | Iter: 153800 | Total Loss: 0.004685 | Recon Loss: 0.004052 | Commit Loss: 0.001266 | Perplexity: 2004.754041
2025-09-27 18:52:05,461 Stage: Train 0.5 | Epoch: 101 | Iter: 154000 | Total Loss: 0.004406 | Recon Loss: 0.003770 | Commit Loss: 0.001272 | Perplexity: 2006.880497
2025-09-27 18:53:01,746 Stage: Train 0.5 | Epoch: 101 | Iter: 154200 | Total Loss: 0.004548 | Recon Loss: 0.003907 | Commit Loss: 0.001282 | Perplexity: 2008.248954
2025-09-27 18:53:57,767 Stage: Train 0.5 | Epoch: 101 | Iter: 154400 | Total Loss: 0.004591 | Recon Loss: 0.003960 | Commit Loss: 0.001261 | Perplexity: 2007.332009
2025-09-27 18:54:54,013 Stage: Train 0.5 | Epoch: 101 | Iter: 154600 | Total Loss: 0.004470 | Recon Loss: 0.003840 | Commit Loss: 0.001260 | Perplexity: 2000.224730
2025-09-27 18:55:50,389 Stage: Train 0.5 | Epoch: 101 | Iter: 154800 | Total Loss: 0.004403 | Recon Loss: 0.003768 | Commit Loss: 0.001270 | Perplexity: 2002.897360
Trainning Epoch:  31%|███       | 102/330 [12:07:34<27:04:57, 427.62s/it]2025-09-27 18:56:46,919 Stage: Train 0.5 | Epoch: 102 | Iter: 155000 | Total Loss: 0.004504 | Recon Loss: 0.003869 | Commit Loss: 0.001270 | Perplexity: 2001.469461
2025-09-27 18:57:43,057 Stage: Train 0.5 | Epoch: 102 | Iter: 155200 | Total Loss: 0.004551 | Recon Loss: 0.003921 | Commit Loss: 0.001260 | Perplexity: 2003.066683
2025-09-27 18:58:39,531 Stage: Train 0.5 | Epoch: 102 | Iter: 155400 | Total Loss: 0.004381 | Recon Loss: 0.003742 | Commit Loss: 0.001277 | Perplexity: 2004.452469
2025-09-27 18:59:35,451 Stage: Train 0.5 | Epoch: 102 | Iter: 155600 | Total Loss: 0.004585 | Recon Loss: 0.003951 | Commit Loss: 0.001269 | Perplexity: 2002.933450
2025-09-27 19:00:31,730 Stage: Train 0.5 | Epoch: 102 | Iter: 155800 | Total Loss: 0.004548 | Recon Loss: 0.003913 | Commit Loss: 0.001271 | Perplexity: 2003.672233
2025-09-27 19:01:28,166 Stage: Train 0.5 | Epoch: 102 | Iter: 156000 | Total Loss: 0.004430 | Recon Loss: 0.003794 | Commit Loss: 0.001272 | Perplexity: 2005.347477
2025-09-27 19:02:24,589 Stage: Train 0.5 | Epoch: 102 | Iter: 156200 | Total Loss: 0.004574 | Recon Loss: 0.003945 | Commit Loss: 0.001260 | Perplexity: 2002.003589
2025-09-27 19:03:20,930 Stage: Train 0.5 | Epoch: 102 | Iter: 156400 | Total Loss: 0.004474 | Recon Loss: 0.003843 | Commit Loss: 0.001262 | Perplexity: 2007.083693
Trainning Epoch:  31%|███       | 103/330 [12:14:41<26:57:50, 427.62s/it]2025-09-27 19:04:17,147 Stage: Train 0.5 | Epoch: 103 | Iter: 156600 | Total Loss: 0.004572 | Recon Loss: 0.003940 | Commit Loss: 0.001263 | Perplexity: 2002.927765
2025-09-27 19:05:13,004 Stage: Train 0.5 | Epoch: 103 | Iter: 156800 | Total Loss: 0.004493 | Recon Loss: 0.003863 | Commit Loss: 0.001261 | Perplexity: 2003.286630
2025-09-27 19:06:09,269 Stage: Train 0.5 | Epoch: 103 | Iter: 157000 | Total Loss: 0.004477 | Recon Loss: 0.003844 | Commit Loss: 0.001266 | Perplexity: 2007.161408
2025-09-27 19:07:05,624 Stage: Train 0.5 | Epoch: 103 | Iter: 157200 | Total Loss: 0.004482 | Recon Loss: 0.003850 | Commit Loss: 0.001264 | Perplexity: 2006.787987
2025-09-27 19:08:02,104 Stage: Train 0.5 | Epoch: 103 | Iter: 157400 | Total Loss: 0.004513 | Recon Loss: 0.003878 | Commit Loss: 0.001270 | Perplexity: 2006.793863
2025-09-27 19:08:58,360 Stage: Train 0.5 | Epoch: 103 | Iter: 157600 | Total Loss: 0.004462 | Recon Loss: 0.003833 | Commit Loss: 0.001259 | Perplexity: 2004.388032
2025-09-27 19:09:54,591 Stage: Train 0.5 | Epoch: 103 | Iter: 157800 | Total Loss: 0.004512 | Recon Loss: 0.003884 | Commit Loss: 0.001255 | Perplexity: 2000.132260
Trainning Epoch:  32%|███▏      | 104/330 [12:21:49<26:50:19, 427.52s/it]2025-09-27 19:10:50,802 Stage: Train 0.5 | Epoch: 104 | Iter: 158000 | Total Loss: 0.004510 | Recon Loss: 0.003876 | Commit Loss: 0.001269 | Perplexity: 2000.857971
2025-09-27 19:11:47,069 Stage: Train 0.5 | Epoch: 104 | Iter: 158200 | Total Loss: 0.004491 | Recon Loss: 0.003859 | Commit Loss: 0.001263 | Perplexity: 2005.149834
2025-09-27 19:12:43,611 Stage: Train 0.5 | Epoch: 104 | Iter: 158400 | Total Loss: 0.004550 | Recon Loss: 0.003920 | Commit Loss: 0.001260 | Perplexity: 2000.458721
2025-09-27 19:13:40,073 Stage: Train 0.5 | Epoch: 104 | Iter: 158600 | Total Loss: 0.004399 | Recon Loss: 0.003768 | Commit Loss: 0.001262 | Perplexity: 2010.048076
2025-09-27 19:14:36,396 Stage: Train 0.5 | Epoch: 104 | Iter: 158800 | Total Loss: 0.004539 | Recon Loss: 0.003906 | Commit Loss: 0.001266 | Perplexity: 2007.141148
2025-09-27 19:15:32,686 Stage: Train 0.5 | Epoch: 104 | Iter: 159000 | Total Loss: 0.004422 | Recon Loss: 0.003790 | Commit Loss: 0.001264 | Perplexity: 2008.427692
2025-09-27 19:16:29,091 Stage: Train 0.5 | Epoch: 104 | Iter: 159200 | Total Loss: 0.004484 | Recon Loss: 0.003850 | Commit Loss: 0.001267 | Perplexity: 2009.343496
2025-09-27 19:17:25,006 Stage: Train 0.5 | Epoch: 104 | Iter: 159400 | Total Loss: 0.004513 | Recon Loss: 0.003884 | Commit Loss: 0.001258 | Perplexity: 2001.315576
Trainning Epoch:  32%|███▏      | 105/330 [12:28:56<26:43:10, 427.51s/it]2025-09-27 19:18:21,555 Stage: Train 0.5 | Epoch: 105 | Iter: 159600 | Total Loss: 0.004409 | Recon Loss: 0.003776 | Commit Loss: 0.001265 | Perplexity: 2003.405479
2025-09-27 19:19:17,863 Stage: Train 0.5 | Epoch: 105 | Iter: 159800 | Total Loss: 0.004501 | Recon Loss: 0.003873 | Commit Loss: 0.001254 | Perplexity: 2004.545184
2025-09-27 19:20:13,941 Stage: Train 0.5 | Epoch: 105 | Iter: 160000 | Total Loss: 0.004467 | Recon Loss: 0.003833 | Commit Loss: 0.001267 | Perplexity: 2005.113337
2025-09-27 19:20:13,941 Saving model at iteration 160000
2025-09-27 19:20:14,503 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000
2025-09-27 19:20:14,990 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/model.safetensors
2025-09-27 19:20:15,511 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/optimizer.bin
2025-09-27 19:20:15,512 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/scheduler.bin
2025-09-27 19:20:15,512 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/sampler.bin
2025-09-27 19:20:15,513 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/random_states_0.pkl
2025-09-27 19:21:12,002 Stage: Train 0.5 | Epoch: 105 | Iter: 160200 | Total Loss: 0.004435 | Recon Loss: 0.003803 | Commit Loss: 0.001264 | Perplexity: 2007.564020
2025-09-27 19:22:08,366 Stage: Train 0.5 | Epoch: 105 | Iter: 160400 | Total Loss: 0.004482 | Recon Loss: 0.003850 | Commit Loss: 0.001264 | Perplexity: 2006.895004
2025-09-27 19:23:04,482 Stage: Train 0.5 | Epoch: 105 | Iter: 160600 | Total Loss: 0.004523 | Recon Loss: 0.003891 | Commit Loss: 0.001265 | Perplexity: 2010.104669
2025-09-27 19:24:00,830 Stage: Train 0.5 | Epoch: 105 | Iter: 160800 | Total Loss: 0.004510 | Recon Loss: 0.003885 | Commit Loss: 0.001249 | Perplexity: 2006.703093
2025-09-27 19:24:57,255 Stage: Train 0.5 | Epoch: 105 | Iter: 161000 | Total Loss: 0.004392 | Recon Loss: 0.003759 | Commit Loss: 0.001267 | Perplexity: 2005.143666
Trainning Epoch:  32%|███▏      | 106/330 [12:36:06<26:38:14, 428.10s/it]2025-09-27 19:25:53,660 Stage: Train 0.5 | Epoch: 106 | Iter: 161200 | Total Loss: 0.004361 | Recon Loss: 0.003733 | Commit Loss: 0.001254 | Perplexity: 2003.594396
2025-09-27 19:26:49,884 Stage: Train 0.5 | Epoch: 106 | Iter: 161400 | Total Loss: 0.004618 | Recon Loss: 0.003992 | Commit Loss: 0.001254 | Perplexity: 2002.854099
2025-09-27 19:27:46,283 Stage: Train 0.5 | Epoch: 106 | Iter: 161600 | Total Loss: 0.004456 | Recon Loss: 0.003827 | Commit Loss: 0.001258 | Perplexity: 2001.463698
2025-09-27 19:28:42,018 Stage: Train 0.5 | Epoch: 106 | Iter: 161800 | Total Loss: 0.004439 | Recon Loss: 0.003807 | Commit Loss: 0.001264 | Perplexity: 2008.551892
2025-09-27 19:29:38,213 Stage: Train 0.5 | Epoch: 106 | Iter: 162000 | Total Loss: 0.004465 | Recon Loss: 0.003835 | Commit Loss: 0.001261 | Perplexity: 2005.683739
2025-09-27 19:30:34,497 Stage: Train 0.5 | Epoch: 106 | Iter: 162200 | Total Loss: 0.004363 | Recon Loss: 0.003732 | Commit Loss: 0.001261 | Perplexity: 2013.240573
2025-09-27 19:31:30,811 Stage: Train 0.5 | Epoch: 106 | Iter: 162400 | Total Loss: 0.004607 | Recon Loss: 0.003977 | Commit Loss: 0.001261 | Perplexity: 2004.500809
Trainning Epoch:  32%|███▏      | 107/330 [12:43:13<26:29:55, 427.78s/it]2025-09-27 19:32:27,144 Stage: Train 0.5 | Epoch: 107 | Iter: 162600 | Total Loss: 0.004406 | Recon Loss: 0.003784 | Commit Loss: 0.001245 | Perplexity: 1997.318755
2025-09-27 19:33:23,458 Stage: Train 0.5 | Epoch: 107 | Iter: 162800 | Total Loss: 0.004351 | Recon Loss: 0.003725 | Commit Loss: 0.001253 | Perplexity: 2009.566050
2025-09-27 19:34:19,385 Stage: Train 0.5 | Epoch: 107 | Iter: 163000 | Total Loss: 0.004425 | Recon Loss: 0.003794 | Commit Loss: 0.001262 | Perplexity: 2010.034418
2025-09-27 19:35:15,580 Stage: Train 0.5 | Epoch: 107 | Iter: 163200 | Total Loss: 0.004487 | Recon Loss: 0.003857 | Commit Loss: 0.001261 | Perplexity: 2009.056273
2025-09-27 19:36:11,909 Stage: Train 0.5 | Epoch: 107 | Iter: 163400 | Total Loss: 0.004395 | Recon Loss: 0.003771 | Commit Loss: 0.001248 | Perplexity: 2005.662570
2025-09-27 19:37:08,144 Stage: Train 0.5 | Epoch: 107 | Iter: 163600 | Total Loss: 0.004398 | Recon Loss: 0.003767 | Commit Loss: 0.001263 | Perplexity: 2010.372176
2025-09-27 19:38:04,461 Stage: Train 0.5 | Epoch: 107 | Iter: 163800 | Total Loss: 0.004510 | Recon Loss: 0.003878 | Commit Loss: 0.001265 | Perplexity: 2007.168917
2025-09-27 19:39:00,589 Stage: Train 0.5 | Epoch: 107 | Iter: 164000 | Total Loss: 0.004447 | Recon Loss: 0.003817 | Commit Loss: 0.001259 | Perplexity: 2005.064976
Trainning Epoch:  33%|███▎      | 108/330 [12:50:20<26:21:51, 427.53s/it]2025-09-27 19:39:57,109 Stage: Train 0.5 | Epoch: 108 | Iter: 164200 | Total Loss: 0.004436 | Recon Loss: 0.003809 | Commit Loss: 0.001253 | Perplexity: 2009.136732
2025-09-27 19:40:53,012 Stage: Train 0.5 | Epoch: 108 | Iter: 164400 | Total Loss: 0.004401 | Recon Loss: 0.003774 | Commit Loss: 0.001255 | Perplexity: 2006.148380
2025-09-27 19:41:49,130 Stage: Train 0.5 | Epoch: 108 | Iter: 164600 | Total Loss: 0.004481 | Recon Loss: 0.003853 | Commit Loss: 0.001255 | Perplexity: 2004.673563
2025-09-27 19:42:45,336 Stage: Train 0.5 | Epoch: 108 | Iter: 164800 | Total Loss: 0.004341 | Recon Loss: 0.003708 | Commit Loss: 0.001265 | Perplexity: 2009.514125
2025-09-27 19:43:41,692 Stage: Train 0.5 | Epoch: 108 | Iter: 165000 | Total Loss: 0.004493 | Recon Loss: 0.003869 | Commit Loss: 0.001248 | Perplexity: 2003.832457
2025-09-27 19:44:38,241 Stage: Train 0.5 | Epoch: 108 | Iter: 165200 | Total Loss: 0.004406 | Recon Loss: 0.003779 | Commit Loss: 0.001253 | Perplexity: 2009.196618
2025-09-27 19:45:34,617 Stage: Train 0.5 | Epoch: 108 | Iter: 165400 | Total Loss: 0.004461 | Recon Loss: 0.003832 | Commit Loss: 0.001258 | Perplexity: 2003.029916
Trainning Epoch:  33%|███▎      | 109/330 [12:57:27<26:14:24, 427.44s/it]2025-09-27 19:46:30,796 Stage: Train 0.5 | Epoch: 109 | Iter: 165600 | Total Loss: 0.004379 | Recon Loss: 0.003753 | Commit Loss: 0.001252 | Perplexity: 2001.641889
2025-09-27 19:47:27,374 Stage: Train 0.5 | Epoch: 109 | Iter: 165800 | Total Loss: 0.004454 | Recon Loss: 0.003833 | Commit Loss: 0.001243 | Perplexity: 2004.532623
2025-09-27 19:48:23,858 Stage: Train 0.5 | Epoch: 109 | Iter: 166000 | Total Loss: 0.004493 | Recon Loss: 0.003871 | Commit Loss: 0.001244 | Perplexity: 2006.108259
2025-09-27 19:49:20,410 Stage: Train 0.5 | Epoch: 109 | Iter: 166200 | Total Loss: 0.004409 | Recon Loss: 0.003782 | Commit Loss: 0.001254 | Perplexity: 2008.779124
2025-09-27 19:50:16,938 Stage: Train 0.5 | Epoch: 109 | Iter: 166400 | Total Loss: 0.004430 | Recon Loss: 0.003802 | Commit Loss: 0.001256 | Perplexity: 2003.769340
2025-09-27 19:51:13,469 Stage: Train 0.5 | Epoch: 109 | Iter: 166600 | Total Loss: 0.004477 | Recon Loss: 0.003851 | Commit Loss: 0.001252 | Perplexity: 2010.897549
2025-09-27 19:52:09,562 Stage: Train 0.5 | Epoch: 109 | Iter: 166800 | Total Loss: 0.004331 | Recon Loss: 0.003707 | Commit Loss: 0.001248 | Perplexity: 2009.544949
2025-09-27 19:53:06,061 Stage: Train 0.5 | Epoch: 109 | Iter: 167000 | Total Loss: 0.004419 | Recon Loss: 0.003793 | Commit Loss: 0.001253 | Perplexity: 2002.417914
Trainning Epoch:  33%|███▎      | 110/330 [13:04:36<26:09:05, 427.93s/it]2025-09-27 19:54:02,540 Stage: Train 0.5 | Epoch: 110 | Iter: 167200 | Total Loss: 0.004376 | Recon Loss: 0.003747 | Commit Loss: 0.001258 | Perplexity: 2007.958231
2025-09-27 19:54:59,137 Stage: Train 0.5 | Epoch: 110 | Iter: 167400 | Total Loss: 0.004435 | Recon Loss: 0.003814 | Commit Loss: 0.001241 | Perplexity: 2002.449429
2025-09-27 19:55:55,707 Stage: Train 0.5 | Epoch: 110 | Iter: 167600 | Total Loss: 0.004343 | Recon Loss: 0.003722 | Commit Loss: 0.001243 | Perplexity: 2009.442302
2025-09-27 19:56:52,167 Stage: Train 0.5 | Epoch: 110 | Iter: 167800 | Total Loss: 0.004406 | Recon Loss: 0.003777 | Commit Loss: 0.001258 | Perplexity: 2014.817463
2025-09-27 19:57:47,163 Stage: Train 0.5 | Epoch: 110 | Iter: 168000 | Total Loss: 0.004410 | Recon Loss: 0.003784 | Commit Loss: 0.001252 | Perplexity: 2005.235777
2025-09-27 19:58:43,445 Stage: Train 0.5 | Epoch: 110 | Iter: 168200 | Total Loss: 0.004388 | Recon Loss: 0.003763 | Commit Loss: 0.001251 | Perplexity: 2008.619218
2025-09-27 19:59:39,742 Stage: Train 0.5 | Epoch: 110 | Iter: 168400 | Total Loss: 0.004454 | Recon Loss: 0.003824 | Commit Loss: 0.001259 | Perplexity: 2011.270374
2025-09-27 20:00:36,105 Stage: Train 0.5 | Epoch: 110 | Iter: 168600 | Total Loss: 0.004513 | Recon Loss: 0.003876 | Commit Loss: 0.001275 | Perplexity: 2014.630338
Trainning Epoch:  34%|███▎      | 111/330 [13:11:43<26:01:04, 427.69s/it]2025-09-27 20:01:32,718 Stage: Train 0.5 | Epoch: 111 | Iter: 168800 | Total Loss: 0.004329 | Recon Loss: 0.003705 | Commit Loss: 0.001248 | Perplexity: 2011.353952
2025-09-27 20:02:28,975 Stage: Train 0.5 | Epoch: 111 | Iter: 169000 | Total Loss: 0.004415 | Recon Loss: 0.003792 | Commit Loss: 0.001247 | Perplexity: 2007.426284
2025-09-27 20:03:25,120 Stage: Train 0.5 | Epoch: 111 | Iter: 169200 | Total Loss: 0.004370 | Recon Loss: 0.003741 | Commit Loss: 0.001258 | Perplexity: 2010.873405
2025-09-27 20:04:21,038 Stage: Train 0.5 | Epoch: 111 | Iter: 169400 | Total Loss: 0.004404 | Recon Loss: 0.003777 | Commit Loss: 0.001255 | Perplexity: 2009.598162
2025-09-27 20:05:17,470 Stage: Train 0.5 | Epoch: 111 | Iter: 169600 | Total Loss: 0.004472 | Recon Loss: 0.003852 | Commit Loss: 0.001240 | Perplexity: 2007.613932
2025-09-27 20:06:13,579 Stage: Train 0.5 | Epoch: 111 | Iter: 169800 | Total Loss: 0.004332 | Recon Loss: 0.003709 | Commit Loss: 0.001246 | Perplexity: 2005.949972
2025-09-27 20:07:09,771 Stage: Train 0.5 | Epoch: 111 | Iter: 170000 | Total Loss: 0.004345 | Recon Loss: 0.003719 | Commit Loss: 0.001252 | Perplexity: 2006.396639
Trainning Epoch:  34%|███▍      | 112/330 [13:18:50<25:53:22, 427.53s/it]2025-09-27 20:08:06,177 Stage: Train 0.5 | Epoch: 112 | Iter: 170200 | Total Loss: 0.004413 | Recon Loss: 0.003795 | Commit Loss: 0.001236 | Perplexity: 2010.361575
2025-09-27 20:09:02,645 Stage: Train 0.5 | Epoch: 112 | Iter: 170400 | Total Loss: 0.004331 | Recon Loss: 0.003707 | Commit Loss: 0.001248 | Perplexity: 2006.537404
2025-09-27 20:09:58,616 Stage: Train 0.5 | Epoch: 112 | Iter: 170600 | Total Loss: 0.004416 | Recon Loss: 0.003791 | Commit Loss: 0.001250 | Perplexity: 2006.871691
2025-09-27 20:10:55,043 Stage: Train 0.5 | Epoch: 112 | Iter: 170800 | Total Loss: 0.004455 | Recon Loss: 0.003831 | Commit Loss: 0.001248 | Perplexity: 2007.670190
2025-09-27 20:11:51,493 Stage: Train 0.5 | Epoch: 112 | Iter: 171000 | Total Loss: 0.004398 | Recon Loss: 0.003775 | Commit Loss: 0.001247 | Perplexity: 2009.500325
2025-09-27 20:12:47,894 Stage: Train 0.5 | Epoch: 112 | Iter: 171200 | Total Loss: 0.004404 | Recon Loss: 0.003783 | Commit Loss: 0.001243 | Perplexity: 2003.561008
2025-09-27 20:13:44,402 Stage: Train 0.5 | Epoch: 112 | Iter: 171400 | Total Loss: 0.004320 | Recon Loss: 0.003702 | Commit Loss: 0.001236 | Perplexity: 2005.834532
2025-09-27 20:14:40,715 Stage: Train 0.5 | Epoch: 112 | Iter: 171600 | Total Loss: 0.004361 | Recon Loss: 0.003732 | Commit Loss: 0.001258 | Perplexity: 2013.827216
Trainning Epoch:  34%|███▍      | 113/330 [13:25:58<25:46:55, 427.72s/it]2025-09-27 20:15:36,642 Stage: Train 0.5 | Epoch: 113 | Iter: 171800 | Total Loss: 0.004358 | Recon Loss: 0.003737 | Commit Loss: 0.001243 | Perplexity: 2004.887488
2025-09-27 20:16:33,130 Stage: Train 0.5 | Epoch: 113 | Iter: 172000 | Total Loss: 0.004432 | Recon Loss: 0.003816 | Commit Loss: 0.001233 | Perplexity: 2009.369833
2025-09-27 20:17:29,467 Stage: Train 0.5 | Epoch: 113 | Iter: 172200 | Total Loss: 0.004362 | Recon Loss: 0.003739 | Commit Loss: 0.001247 | Perplexity: 2015.137368
2025-09-27 20:18:26,101 Stage: Train 0.5 | Epoch: 113 | Iter: 172400 | Total Loss: 0.004244 | Recon Loss: 0.003620 | Commit Loss: 0.001248 | Perplexity: 2008.814988
2025-09-27 20:19:22,665 Stage: Train 0.5 | Epoch: 113 | Iter: 172600 | Total Loss: 0.004409 | Recon Loss: 0.003788 | Commit Loss: 0.001241 | Perplexity: 2006.857086
2025-09-27 20:20:18,917 Stage: Train 0.5 | Epoch: 113 | Iter: 172800 | Total Loss: 0.004333 | Recon Loss: 0.003706 | Commit Loss: 0.001252 | Perplexity: 2010.862703
2025-09-27 20:21:14,775 Stage: Train 0.5 | Epoch: 113 | Iter: 173000 | Total Loss: 0.004319 | Recon Loss: 0.003688 | Commit Loss: 0.001262 | Perplexity: 2017.663712
Trainning Epoch:  35%|███▍      | 114/330 [13:33:06<25:39:47, 427.72s/it]2025-09-27 20:22:11,407 Stage: Train 0.5 | Epoch: 114 | Iter: 173200 | Total Loss: 0.004773 | Recon Loss: 0.004152 | Commit Loss: 0.001242 | Perplexity: 2003.161198
2025-09-27 20:23:07,781 Stage: Train 0.5 | Epoch: 114 | Iter: 173400 | Total Loss: 0.004169 | Recon Loss: 0.003548 | Commit Loss: 0.001242 | Perplexity: 2013.849669
2025-09-27 20:24:04,219 Stage: Train 0.5 | Epoch: 114 | Iter: 173600 | Total Loss: 0.004298 | Recon Loss: 0.003678 | Commit Loss: 0.001240 | Perplexity: 2008.332664
2025-09-27 20:25:00,547 Stage: Train 0.5 | Epoch: 114 | Iter: 173800 | Total Loss: 0.004352 | Recon Loss: 0.003728 | Commit Loss: 0.001249 | Perplexity: 2012.412584
2025-09-27 20:25:56,994 Stage: Train 0.5 | Epoch: 114 | Iter: 174000 | Total Loss: 0.004409 | Recon Loss: 0.003790 | Commit Loss: 0.001237 | Perplexity: 2009.941498
2025-09-27 20:26:53,542 Stage: Train 0.5 | Epoch: 114 | Iter: 174200 | Total Loss: 0.004300 | Recon Loss: 0.003675 | Commit Loss: 0.001249 | Perplexity: 2011.205414
2025-09-27 20:27:49,674 Stage: Train 0.5 | Epoch: 114 | Iter: 174400 | Total Loss: 0.004347 | Recon Loss: 0.003722 | Commit Loss: 0.001251 | Perplexity: 2006.490747
2025-09-27 20:28:46,122 Stage: Train 0.5 | Epoch: 114 | Iter: 174600 | Total Loss: 0.004328 | Recon Loss: 0.003703 | Commit Loss: 0.001250 | Perplexity: 2011.279621
Trainning Epoch:  35%|███▍      | 115/330 [13:40:15<25:33:23, 427.92s/it]2025-09-27 20:29:42,583 Stage: Train 0.5 | Epoch: 115 | Iter: 174800 | Total Loss: 0.004411 | Recon Loss: 0.003788 | Commit Loss: 0.001246 | Perplexity: 2007.325651
2025-09-27 20:30:39,015 Stage: Train 0.5 | Epoch: 115 | Iter: 175000 | Total Loss: 0.004452 | Recon Loss: 0.003831 | Commit Loss: 0.001242 | Perplexity: 2006.497140
2025-09-27 20:31:35,305 Stage: Train 0.5 | Epoch: 115 | Iter: 175200 | Total Loss: 0.004197 | Recon Loss: 0.003570 | Commit Loss: 0.001254 | Perplexity: 2012.874853
2025-09-27 20:32:31,849 Stage: Train 0.5 | Epoch: 115 | Iter: 175400 | Total Loss: 0.004418 | Recon Loss: 0.003797 | Commit Loss: 0.001242 | Perplexity: 2010.567033
2025-09-27 20:33:27,855 Stage: Train 0.5 | Epoch: 115 | Iter: 175600 | Total Loss: 0.004322 | Recon Loss: 0.003705 | Commit Loss: 0.001235 | Perplexity: 2012.446001
2025-09-27 20:34:24,175 Stage: Train 0.5 | Epoch: 115 | Iter: 175800 | Total Loss: 0.004343 | Recon Loss: 0.003721 | Commit Loss: 0.001243 | Perplexity: 2010.406008
2025-09-27 20:35:20,632 Stage: Train 0.5 | Epoch: 115 | Iter: 176000 | Total Loss: 0.004439 | Recon Loss: 0.003818 | Commit Loss: 0.001243 | Perplexity: 2009.316272
2025-09-27 20:36:17,022 Stage: Train 0.5 | Epoch: 115 | Iter: 176200 | Total Loss: 0.004226 | Recon Loss: 0.003604 | Commit Loss: 0.001244 | Perplexity: 2009.268496
Trainning Epoch:  35%|███▌      | 116/330 [13:47:23<25:26:25, 427.97s/it]2025-09-27 20:37:13,582 Stage: Train 0.5 | Epoch: 116 | Iter: 176400 | Total Loss: 0.004309 | Recon Loss: 0.003688 | Commit Loss: 0.001243 | Perplexity: 2011.029426
2025-09-27 20:38:09,833 Stage: Train 0.5 | Epoch: 116 | Iter: 176600 | Total Loss: 0.004336 | Recon Loss: 0.003718 | Commit Loss: 0.001238 | Perplexity: 2010.579535
2025-09-27 20:39:05,814 Stage: Train 0.5 | Epoch: 116 | Iter: 176800 | Total Loss: 0.004309 | Recon Loss: 0.003687 | Commit Loss: 0.001243 | Perplexity: 2009.373022
2025-09-27 20:40:02,040 Stage: Train 0.5 | Epoch: 116 | Iter: 177000 | Total Loss: 0.004348 | Recon Loss: 0.003732 | Commit Loss: 0.001232 | Perplexity: 2010.028878
2025-09-27 20:40:58,626 Stage: Train 0.5 | Epoch: 116 | Iter: 177200 | Total Loss: 0.004284 | Recon Loss: 0.003666 | Commit Loss: 0.001235 | Perplexity: 2006.644990
2025-09-27 20:41:55,041 Stage: Train 0.5 | Epoch: 116 | Iter: 177400 | Total Loss: 0.004439 | Recon Loss: 0.003820 | Commit Loss: 0.001238 | Perplexity: 2007.745903
2025-09-27 20:42:51,327 Stage: Train 0.5 | Epoch: 116 | Iter: 177600 | Total Loss: 0.004258 | Recon Loss: 0.003633 | Commit Loss: 0.001249 | Perplexity: 2010.842126
Trainning Epoch:  35%|███▌      | 117/330 [13:54:30<25:19:10, 427.94s/it]2025-09-27 20:43:47,845 Stage: Train 0.5 | Epoch: 117 | Iter: 177800 | Total Loss: 0.004271 | Recon Loss: 0.003656 | Commit Loss: 0.001231 | Perplexity: 2004.803068
2025-09-27 20:44:43,829 Stage: Train 0.5 | Epoch: 117 | Iter: 178000 | Total Loss: 0.004263 | Recon Loss: 0.003641 | Commit Loss: 0.001244 | Perplexity: 2010.020444
2025-09-27 20:45:40,182 Stage: Train 0.5 | Epoch: 117 | Iter: 178200 | Total Loss: 0.004298 | Recon Loss: 0.003682 | Commit Loss: 0.001233 | Perplexity: 2007.323564
2025-09-27 20:46:36,496 Stage: Train 0.5 | Epoch: 117 | Iter: 178400 | Total Loss: 0.004228 | Recon Loss: 0.003607 | Commit Loss: 0.001243 | Perplexity: 2013.605727
2025-09-27 20:47:32,747 Stage: Train 0.5 | Epoch: 117 | Iter: 178600 | Total Loss: 0.004323 | Recon Loss: 0.003700 | Commit Loss: 0.001246 | Perplexity: 2010.837073
2025-09-27 20:48:29,095 Stage: Train 0.5 | Epoch: 117 | Iter: 178800 | Total Loss: 0.004337 | Recon Loss: 0.003718 | Commit Loss: 0.001238 | Perplexity: 2008.042297
2025-09-27 20:49:25,546 Stage: Train 0.5 | Epoch: 117 | Iter: 179000 | Total Loss: 0.004308 | Recon Loss: 0.003688 | Commit Loss: 0.001241 | Perplexity: 2009.133138
2025-09-27 20:50:21,711 Stage: Train 0.5 | Epoch: 117 | Iter: 179200 | Total Loss: 0.004360 | Recon Loss: 0.003743 | Commit Loss: 0.001233 | Perplexity: 2003.751798
Trainning Epoch:  36%|███▌      | 118/330 [14:01:38<25:11:36, 427.81s/it]2025-09-27 20:51:18,103 Stage: Train 0.5 | Epoch: 118 | Iter: 179400 | Total Loss: 0.004331 | Recon Loss: 0.003717 | Commit Loss: 0.001228 | Perplexity: 2007.414929
2025-09-27 20:52:14,480 Stage: Train 0.5 | Epoch: 118 | Iter: 179600 | Total Loss: 0.004273 | Recon Loss: 0.003654 | Commit Loss: 0.001236 | Perplexity: 2012.118923
2025-09-27 20:53:10,804 Stage: Train 0.5 | Epoch: 118 | Iter: 179800 | Total Loss: 0.004307 | Recon Loss: 0.003683 | Commit Loss: 0.001248 | Perplexity: 2010.295626
2025-09-27 20:54:07,189 Stage: Train 0.5 | Epoch: 118 | Iter: 180000 | Total Loss: 0.004339 | Recon Loss: 0.003721 | Commit Loss: 0.001237 | Perplexity: 2013.789927
2025-09-27 20:54:07,189 Saving model at iteration 180000
2025-09-27 20:54:07,686 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000
2025-09-27 20:54:08,218 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/model.safetensors
2025-09-27 20:54:08,759 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/optimizer.bin
2025-09-27 20:54:08,759 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/scheduler.bin
2025-09-27 20:54:08,759 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/sampler.bin
2025-09-27 20:54:08,760 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/random_states_0.pkl
2025-09-27 20:55:05,086 Stage: Train 0.5 | Epoch: 118 | Iter: 180200 | Total Loss: 0.004360 | Recon Loss: 0.003742 | Commit Loss: 0.001237 | Perplexity: 2012.884555
2025-09-27 20:56:01,388 Stage: Train 0.5 | Epoch: 118 | Iter: 180400 | Total Loss: 0.004317 | Recon Loss: 0.003696 | Commit Loss: 0.001241 | Perplexity: 2008.676462
2025-09-27 20:56:57,227 Stage: Train 0.5 | Epoch: 118 | Iter: 180600 | Total Loss: 0.004359 | Recon Loss: 0.003745 | Commit Loss: 0.001228 | Perplexity: 2004.448937
Trainning Epoch:  36%|███▌      | 119/330 [14:08:47<25:05:45, 428.18s/it]2025-09-27 20:57:53,653 Stage: Train 0.5 | Epoch: 119 | Iter: 180800 | Total Loss: 0.004313 | Recon Loss: 0.003697 | Commit Loss: 0.001232 | Perplexity: 2008.409778
2025-09-27 20:58:49,774 Stage: Train 0.5 | Epoch: 119 | Iter: 181000 | Total Loss: 0.004351 | Recon Loss: 0.003739 | Commit Loss: 0.001225 | Perplexity: 2010.298108
2025-09-27 20:59:46,171 Stage: Train 0.5 | Epoch: 119 | Iter: 181200 | Total Loss: 0.004264 | Recon Loss: 0.003646 | Commit Loss: 0.001236 | Perplexity: 2009.666345
2025-09-27 21:00:42,555 Stage: Train 0.5 | Epoch: 119 | Iter: 181400 | Total Loss: 0.004368 | Recon Loss: 0.003751 | Commit Loss: 0.001235 | Perplexity: 2006.715667
2025-09-27 21:01:38,999 Stage: Train 0.5 | Epoch: 119 | Iter: 181600 | Total Loss: 0.004241 | Recon Loss: 0.003624 | Commit Loss: 0.001234 | Perplexity: 2008.942919
2025-09-27 21:02:35,002 Stage: Train 0.5 | Epoch: 119 | Iter: 181800 | Total Loss: 0.004384 | Recon Loss: 0.003767 | Commit Loss: 0.001234 | Perplexity: 2004.157701
2025-09-27 21:03:31,378 Stage: Train 0.5 | Epoch: 119 | Iter: 182000 | Total Loss: 0.004188 | Recon Loss: 0.003572 | Commit Loss: 0.001233 | Perplexity: 2009.788271
2025-09-27 21:04:27,716 Stage: Train 0.5 | Epoch: 119 | Iter: 182200 | Total Loss: 0.004295 | Recon Loss: 0.003683 | Commit Loss: 0.001223 | Perplexity: 2004.264827
Trainning Epoch:  36%|███▋      | 120/330 [14:15:55<24:58:03, 428.02s/it]2025-09-27 21:05:24,081 Stage: Train 0.5 | Epoch: 120 | Iter: 182400 | Total Loss: 0.004291 | Recon Loss: 0.003673 | Commit Loss: 0.001236 | Perplexity: 2011.758962
2025-09-27 21:06:20,570 Stage: Train 0.5 | Epoch: 120 | Iter: 182600 | Total Loss: 0.004309 | Recon Loss: 0.003696 | Commit Loss: 0.001227 | Perplexity: 2011.808148
2025-09-27 21:07:16,990 Stage: Train 0.5 | Epoch: 120 | Iter: 182800 | Total Loss: 0.004369 | Recon Loss: 0.003755 | Commit Loss: 0.001227 | Perplexity: 2003.524417
2025-09-27 21:08:13,054 Stage: Train 0.5 | Epoch: 120 | Iter: 183000 | Total Loss: 0.004423 | Recon Loss: 0.003809 | Commit Loss: 0.001229 | Perplexity: 2010.044853
2025-09-27 21:09:09,365 Stage: Train 0.5 | Epoch: 120 | Iter: 183200 | Total Loss: 0.004240 | Recon Loss: 0.003624 | Commit Loss: 0.001231 | Perplexity: 2008.998115
2025-09-27 21:10:05,596 Stage: Train 0.5 | Epoch: 120 | Iter: 183400 | Total Loss: 0.004329 | Recon Loss: 0.003717 | Commit Loss: 0.001223 | Perplexity: 2009.214051
2025-09-27 21:11:01,989 Stage: Train 0.5 | Epoch: 120 | Iter: 183600 | Total Loss: 0.004160 | Recon Loss: 0.003544 | Commit Loss: 0.001231 | Perplexity: 2013.498770
Trainning Epoch:  37%|███▋      | 121/330 [14:23:03<24:50:52, 428.00s/it]2025-09-27 21:11:58,615 Stage: Train 0.5 | Epoch: 121 | Iter: 183800 | Total Loss: 0.004299 | Recon Loss: 0.003679 | Commit Loss: 0.001240 | Perplexity: 2009.398323
2025-09-27 21:12:54,946 Stage: Train 0.5 | Epoch: 121 | Iter: 184000 | Total Loss: 0.004364 | Recon Loss: 0.003753 | Commit Loss: 0.001222 | Perplexity: 2007.443053
2025-09-27 21:13:50,905 Stage: Train 0.5 | Epoch: 121 | Iter: 184200 | Total Loss: 0.004194 | Recon Loss: 0.003582 | Commit Loss: 0.001225 | Perplexity: 2022.395720
2025-09-27 21:14:40,688 Stage: Train 0.5 | Epoch: 121 | Iter: 184400 | Total Loss: 0.004242 | Recon Loss: 0.003627 | Commit Loss: 0.001229 | Perplexity: 2017.125291
2025-09-27 21:15:07,198 Stage: Train 0.5 | Epoch: 121 | Iter: 184600 | Total Loss: 0.004292 | Recon Loss: 0.003674 | Commit Loss: 0.001236 | Perplexity: 2012.591890
2025-09-27 21:15:33,795 Stage: Train 0.5 | Epoch: 121 | Iter: 184800 | Total Loss: 0.004292 | Recon Loss: 0.003673 | Commit Loss: 0.001238 | Perplexity: 2011.585641
2025-09-27 21:16:00,315 Stage: Train 0.5 | Epoch: 121 | Iter: 185000 | Total Loss: 0.004333 | Recon Loss: 0.003720 | Commit Loss: 0.001225 | Perplexity: 2010.524174
2025-09-27 21:16:26,734 Stage: Train 0.5 | Epoch: 121 | Iter: 185200 | Total Loss: 0.004335 | Recon Loss: 0.003726 | Commit Loss: 0.001218 | Perplexity: 2006.251168
Trainning Epoch:  37%|███▋      | 122/330 [14:27:47<22:14:19, 384.90s/it]2025-09-27 21:16:53,563 Stage: Train 0.5 | Epoch: 122 | Iter: 185400 | Total Loss: 0.004276 | Recon Loss: 0.003667 | Commit Loss: 0.001218 | Perplexity: 2002.125460
2025-09-27 21:17:20,127 Stage: Train 0.5 | Epoch: 122 | Iter: 185600 | Total Loss: 0.004251 | Recon Loss: 0.003636 | Commit Loss: 0.001229 | Perplexity: 2015.005298
2025-09-27 21:17:46,592 Stage: Train 0.5 | Epoch: 122 | Iter: 185800 | Total Loss: 0.004302 | Recon Loss: 0.003690 | Commit Loss: 0.001225 | Perplexity: 2009.515125
2025-09-27 21:18:13,096 Stage: Train 0.5 | Epoch: 122 | Iter: 186000 | Total Loss: 0.004235 | Recon Loss: 0.003622 | Commit Loss: 0.001227 | Perplexity: 2013.888672
2025-09-27 21:18:39,703 Stage: Train 0.5 | Epoch: 122 | Iter: 186200 | Total Loss: 0.004338 | Recon Loss: 0.003720 | Commit Loss: 0.001235 | Perplexity: 2015.302941
2025-09-27 21:19:06,231 Stage: Train 0.5 | Epoch: 122 | Iter: 186400 | Total Loss: 0.004284 | Recon Loss: 0.003673 | Commit Loss: 0.001223 | Perplexity: 2016.562059
2025-09-27 21:19:32,741 Stage: Train 0.5 | Epoch: 122 | Iter: 186600 | Total Loss: 0.004180 | Recon Loss: 0.003568 | Commit Loss: 0.001223 | Perplexity: 2011.181257
2025-09-27 21:19:59,234 Stage: Train 0.5 | Epoch: 122 | Iter: 186800 | Total Loss: 0.004325 | Recon Loss: 0.003709 | Commit Loss: 0.001233 | Perplexity: 2011.080910
Trainning Epoch:  37%|███▋      | 123/330 [14:31:09<18:58:15, 329.93s/it]2025-09-27 21:20:25,926 Stage: Train 0.5 | Epoch: 123 | Iter: 187000 | Total Loss: 0.004217 | Recon Loss: 0.003603 | Commit Loss: 0.001228 | Perplexity: 2008.421068
2025-09-27 21:20:52,490 Stage: Train 0.5 | Epoch: 123 | Iter: 187200 | Total Loss: 0.004269 | Recon Loss: 0.003655 | Commit Loss: 0.001230 | Perplexity: 2012.643427
2025-09-27 21:21:18,977 Stage: Train 0.5 | Epoch: 123 | Iter: 187400 | Total Loss: 0.004239 | Recon Loss: 0.003629 | Commit Loss: 0.001220 | Perplexity: 2009.884356
2025-09-27 21:21:45,454 Stage: Train 0.5 | Epoch: 123 | Iter: 187600 | Total Loss: 0.004408 | Recon Loss: 0.003799 | Commit Loss: 0.001218 | Perplexity: 2006.644559
2025-09-27 21:22:12,114 Stage: Train 0.5 | Epoch: 123 | Iter: 187800 | Total Loss: 0.004150 | Recon Loss: 0.003535 | Commit Loss: 0.001229 | Perplexity: 2015.095745
2025-09-27 21:22:38,715 Stage: Train 0.5 | Epoch: 123 | Iter: 188000 | Total Loss: 0.004237 | Recon Loss: 0.003624 | Commit Loss: 0.001225 | Perplexity: 2016.344247
2025-09-27 21:23:05,271 Stage: Train 0.5 | Epoch: 123 | Iter: 188200 | Total Loss: 0.004194 | Recon Loss: 0.003581 | Commit Loss: 0.001225 | Perplexity: 2016.423516
Trainning Epoch:  38%|███▊      | 124/330 [14:34:31<16:40:55, 291.53s/it]2025-09-27 21:23:32,128 Stage: Train 0.5 | Epoch: 124 | Iter: 188400 | Total Loss: 0.004229 | Recon Loss: 0.003617 | Commit Loss: 0.001224 | Perplexity: 2012.496734
2025-09-27 21:23:58,586 Stage: Train 0.5 | Epoch: 124 | Iter: 188600 | Total Loss: 0.004186 | Recon Loss: 0.003572 | Commit Loss: 0.001228 | Perplexity: 2018.507153
2025-09-27 21:24:25,144 Stage: Train 0.5 | Epoch: 124 | Iter: 188800 | Total Loss: 0.004315 | Recon Loss: 0.003705 | Commit Loss: 0.001219 | Perplexity: 2013.879662
2025-09-27 21:24:51,776 Stage: Train 0.5 | Epoch: 124 | Iter: 189000 | Total Loss: 0.004242 | Recon Loss: 0.003632 | Commit Loss: 0.001219 | Perplexity: 2006.947400
2025-09-27 21:25:18,419 Stage: Train 0.5 | Epoch: 124 | Iter: 189200 | Total Loss: 0.004179 | Recon Loss: 0.003567 | Commit Loss: 0.001224 | Perplexity: 2013.040153
2025-09-27 21:25:44,995 Stage: Train 0.5 | Epoch: 124 | Iter: 189400 | Total Loss: 0.004278 | Recon Loss: 0.003660 | Commit Loss: 0.001237 | Perplexity: 2015.009202
2025-09-27 21:26:11,533 Stage: Train 0.5 | Epoch: 124 | Iter: 189600 | Total Loss: 0.004263 | Recon Loss: 0.003656 | Commit Loss: 0.001214 | Perplexity: 2006.152900
2025-09-27 21:26:38,092 Stage: Train 0.5 | Epoch: 124 | Iter: 189800 | Total Loss: 0.004196 | Recon Loss: 0.003579 | Commit Loss: 0.001233 | Perplexity: 2009.905992
Trainning Epoch:  38%|███▊      | 125/330 [14:37:53<15:04:19, 264.68s/it]2025-09-27 21:27:04,894 Stage: Train 0.5 | Epoch: 125 | Iter: 190000 | Total Loss: 0.004232 | Recon Loss: 0.003616 | Commit Loss: 0.001232 | Perplexity: 2011.130832
2025-09-27 21:27:31,451 Stage: Train 0.5 | Epoch: 125 | Iter: 190200 | Total Loss: 0.004287 | Recon Loss: 0.003677 | Commit Loss: 0.001219 | Perplexity: 2012.430771
2025-09-27 21:27:57,963 Stage: Train 0.5 | Epoch: 125 | Iter: 190400 | Total Loss: 0.004226 | Recon Loss: 0.003620 | Commit Loss: 0.001212 | Perplexity: 2016.896116
2025-09-27 21:28:24,571 Stage: Train 0.5 | Epoch: 125 | Iter: 190600 | Total Loss: 0.004274 | Recon Loss: 0.003661 | Commit Loss: 0.001225 | Perplexity: 2010.107240
2025-09-27 21:28:51,162 Stage: Train 0.5 | Epoch: 125 | Iter: 190800 | Total Loss: 0.004188 | Recon Loss: 0.003574 | Commit Loss: 0.001227 | Perplexity: 2014.471702
2025-09-27 21:29:17,747 Stage: Train 0.5 | Epoch: 125 | Iter: 191000 | Total Loss: 0.004260 | Recon Loss: 0.003646 | Commit Loss: 0.001229 | Perplexity: 2013.411426
2025-09-27 21:29:44,272 Stage: Train 0.5 | Epoch: 125 | Iter: 191200 | Total Loss: 0.004268 | Recon Loss: 0.003658 | Commit Loss: 0.001221 | Perplexity: 2014.659512
Trainning Epoch:  38%|███▊      | 126/330 [14:41:15<13:55:56, 245.86s/it]2025-09-27 21:30:11,076 Stage: Train 0.5 | Epoch: 126 | Iter: 191400 | Total Loss: 0.004285 | Recon Loss: 0.003676 | Commit Loss: 0.001218 | Perplexity: 2007.537618
2025-09-27 21:30:37,562 Stage: Train 0.5 | Epoch: 126 | Iter: 191600 | Total Loss: 0.004190 | Recon Loss: 0.003576 | Commit Loss: 0.001227 | Perplexity: 2017.929518
2025-09-27 21:31:04,187 Stage: Train 0.5 | Epoch: 126 | Iter: 191800 | Total Loss: 0.004270 | Recon Loss: 0.003665 | Commit Loss: 0.001210 | Perplexity: 2012.374111
2025-09-27 21:31:30,730 Stage: Train 0.5 | Epoch: 126 | Iter: 192000 | Total Loss: 0.004121 | Recon Loss: 0.003507 | Commit Loss: 0.001229 | Perplexity: 2018.010063
2025-09-27 21:31:57,364 Stage: Train 0.5 | Epoch: 126 | Iter: 192200 | Total Loss: 0.004245 | Recon Loss: 0.003635 | Commit Loss: 0.001220 | Perplexity: 2014.607509
2025-09-27 21:32:23,936 Stage: Train 0.5 | Epoch: 126 | Iter: 192400 | Total Loss: 0.004293 | Recon Loss: 0.003688 | Commit Loss: 0.001210 | Perplexity: 2012.355537
2025-09-27 21:32:50,520 Stage: Train 0.5 | Epoch: 126 | Iter: 192600 | Total Loss: 0.004156 | Recon Loss: 0.003544 | Commit Loss: 0.001224 | Perplexity: 2015.084705
2025-09-27 21:33:17,128 Stage: Train 0.5 | Epoch: 126 | Iter: 192800 | Total Loss: 0.004240 | Recon Loss: 0.003623 | Commit Loss: 0.001233 | Perplexity: 2013.666506
Trainning Epoch:  38%|███▊      | 127/330 [14:44:37<13:07:24, 232.73s/it]2025-09-27 21:33:43,927 Stage: Train 0.5 | Epoch: 127 | Iter: 193000 | Total Loss: 0.004163 | Recon Loss: 0.003548 | Commit Loss: 0.001229 | Perplexity: 2020.779791
2025-09-27 21:34:10,559 Stage: Train 0.5 | Epoch: 127 | Iter: 193200 | Total Loss: 0.004229 | Recon Loss: 0.003619 | Commit Loss: 0.001221 | Perplexity: 2017.602061
2025-09-27 21:34:37,186 Stage: Train 0.5 | Epoch: 127 | Iter: 193400 | Total Loss: 0.004271 | Recon Loss: 0.003658 | Commit Loss: 0.001226 | Perplexity: 2015.204460
2025-09-27 21:35:03,785 Stage: Train 0.5 | Epoch: 127 | Iter: 193600 | Total Loss: 0.004255 | Recon Loss: 0.003649 | Commit Loss: 0.001211 | Perplexity: 2012.911953
2025-09-27 21:35:30,330 Stage: Train 0.5 | Epoch: 127 | Iter: 193800 | Total Loss: 0.004191 | Recon Loss: 0.003585 | Commit Loss: 0.001212 | Perplexity: 2015.620599
2025-09-27 21:35:56,888 Stage: Train 0.5 | Epoch: 127 | Iter: 194000 | Total Loss: 0.004154 | Recon Loss: 0.003547 | Commit Loss: 0.001216 | Perplexity: 2013.139951
2025-09-27 21:36:23,589 Stage: Train 0.5 | Epoch: 127 | Iter: 194200 | Total Loss: 0.004324 | Recon Loss: 0.003715 | Commit Loss: 0.001218 | Perplexity: 2012.923322
2025-09-27 21:36:50,059 Stage: Train 0.5 | Epoch: 127 | Iter: 194400 | Total Loss: 0.004260 | Recon Loss: 0.003655 | Commit Loss: 0.001210 | Perplexity: 2009.208805
Trainning Epoch:  39%|███▉      | 128/330 [14:47:59<12:32:40, 223.57s/it]2025-09-27 21:37:16,816 Stage: Train 0.5 | Epoch: 128 | Iter: 194600 | Total Loss: 0.004142 | Recon Loss: 0.003532 | Commit Loss: 0.001219 | Perplexity: 2020.116310
2025-09-27 21:37:43,399 Stage: Train 0.5 | Epoch: 128 | Iter: 194800 | Total Loss: 0.004264 | Recon Loss: 0.003659 | Commit Loss: 0.001210 | Perplexity: 2011.302754
2025-09-27 21:38:09,885 Stage: Train 0.5 | Epoch: 128 | Iter: 195000 | Total Loss: 0.004174 | Recon Loss: 0.003564 | Commit Loss: 0.001221 | Perplexity: 2013.177479
2025-09-27 21:38:36,438 Stage: Train 0.5 | Epoch: 128 | Iter: 195200 | Total Loss: 0.004207 | Recon Loss: 0.003598 | Commit Loss: 0.001217 | Perplexity: 2013.008318
2025-09-27 21:39:02,916 Stage: Train 0.5 | Epoch: 128 | Iter: 195400 | Total Loss: 0.004118 | Recon Loss: 0.003514 | Commit Loss: 0.001208 | Perplexity: 2009.456120
2025-09-27 21:39:29,294 Stage: Train 0.5 | Epoch: 128 | Iter: 195600 | Total Loss: 0.004239 | Recon Loss: 0.003633 | Commit Loss: 0.001211 | Perplexity: 2008.594005
2025-09-27 21:39:55,850 Stage: Train 0.5 | Epoch: 128 | Iter: 195800 | Total Loss: 0.004215 | Recon Loss: 0.003605 | Commit Loss: 0.001220 | Perplexity: 2021.507181
Trainning Epoch:  39%|███▉      | 129/330 [14:51:20<12:06:51, 216.97s/it]2025-09-27 21:40:22,634 Stage: Train 0.5 | Epoch: 129 | Iter: 196000 | Total Loss: 0.004171 | Recon Loss: 0.003565 | Commit Loss: 0.001211 | Perplexity: 2011.572902
2025-09-27 21:40:49,121 Stage: Train 0.5 | Epoch: 129 | Iter: 196200 | Total Loss: 0.004251 | Recon Loss: 0.003646 | Commit Loss: 0.001210 | Perplexity: 2012.605458
2025-09-27 21:41:15,764 Stage: Train 0.5 | Epoch: 129 | Iter: 196400 | Total Loss: 0.004265 | Recon Loss: 0.003656 | Commit Loss: 0.001218 | Perplexity: 2016.568212
2025-09-27 21:41:42,335 Stage: Train 0.5 | Epoch: 129 | Iter: 196600 | Total Loss: 0.004102 | Recon Loss: 0.003497 | Commit Loss: 0.001210 | Perplexity: 2016.125959
2025-09-27 21:42:08,881 Stage: Train 0.5 | Epoch: 129 | Iter: 196800 | Total Loss: 0.004276 | Recon Loss: 0.003667 | Commit Loss: 0.001217 | Perplexity: 2013.759829
2025-09-27 21:42:35,409 Stage: Train 0.5 | Epoch: 129 | Iter: 197000 | Total Loss: 0.004130 | Recon Loss: 0.003525 | Commit Loss: 0.001210 | Perplexity: 2010.959888
2025-09-27 21:43:02,008 Stage: Train 0.5 | Epoch: 129 | Iter: 197200 | Total Loss: 0.004203 | Recon Loss: 0.003596 | Commit Loss: 0.001215 | Perplexity: 2013.578019
2025-09-27 21:43:28,471 Stage: Train 0.5 | Epoch: 129 | Iter: 197400 | Total Loss: 0.004213 | Recon Loss: 0.003603 | Commit Loss: 0.001219 | Perplexity: 2011.622969
Trainning Epoch:  39%|███▉      | 130/330 [14:54:42<11:48:09, 212.45s/it]2025-09-27 21:43:55,215 Stage: Train 0.5 | Epoch: 130 | Iter: 197600 | Total Loss: 0.004155 | Recon Loss: 0.003547 | Commit Loss: 0.001216 | Perplexity: 2013.235129
2025-09-27 21:44:21,682 Stage: Train 0.5 | Epoch: 130 | Iter: 197800 | Total Loss: 0.004305 | Recon Loss: 0.003700 | Commit Loss: 0.001210 | Perplexity: 2011.133353
2025-09-27 21:44:48,267 Stage: Train 0.5 | Epoch: 130 | Iter: 198000 | Total Loss: 0.004108 | Recon Loss: 0.003508 | Commit Loss: 0.001200 | Perplexity: 2011.330941
2025-09-27 21:45:14,785 Stage: Train 0.5 | Epoch: 130 | Iter: 198200 | Total Loss: 0.004129 | Recon Loss: 0.003520 | Commit Loss: 0.001218 | Perplexity: 2021.724602
2025-09-27 21:45:41,321 Stage: Train 0.5 | Epoch: 130 | Iter: 198400 | Total Loss: 0.004162 | Recon Loss: 0.003560 | Commit Loss: 0.001204 | Perplexity: 2011.564327
2025-09-27 21:46:07,865 Stage: Train 0.5 | Epoch: 130 | Iter: 198600 | Total Loss: 0.004173 | Recon Loss: 0.003566 | Commit Loss: 0.001214 | Perplexity: 2017.107008
2025-09-27 21:46:34,395 Stage: Train 0.5 | Epoch: 130 | Iter: 198800 | Total Loss: 0.004111 | Recon Loss: 0.003500 | Commit Loss: 0.001222 | Perplexity: 2016.825278
Trainning Epoch:  40%|███▉      | 131/330 [14:58:04<11:34:03, 209.26s/it]2025-09-27 21:47:01,279 Stage: Train 0.5 | Epoch: 131 | Iter: 199000 | Total Loss: 0.004186 | Recon Loss: 0.003578 | Commit Loss: 0.001215 | Perplexity: 2005.832689
2025-09-27 21:47:27,915 Stage: Train 0.5 | Epoch: 131 | Iter: 199200 | Total Loss: 0.004180 | Recon Loss: 0.003576 | Commit Loss: 0.001207 | Perplexity: 2013.433765
2025-09-27 21:47:54,577 Stage: Train 0.5 | Epoch: 131 | Iter: 199400 | Total Loss: 0.004180 | Recon Loss: 0.003579 | Commit Loss: 0.001203 | Perplexity: 2016.136534
2025-09-27 21:48:21,187 Stage: Train 0.5 | Epoch: 131 | Iter: 199600 | Total Loss: 0.004208 | Recon Loss: 0.003603 | Commit Loss: 0.001212 | Perplexity: 2017.377078
2025-09-27 21:48:47,796 Stage: Train 0.5 | Epoch: 131 | Iter: 199800 | Total Loss: 0.004091 | Recon Loss: 0.003482 | Commit Loss: 0.001217 | Perplexity: 2018.972938
2025-09-27 21:49:14,358 Stage: Train 0.5 | Epoch: 131 | Iter: 200000 | Total Loss: 0.004376 | Recon Loss: 0.003770 | Commit Loss: 0.001212 | Perplexity: 2010.696516
2025-09-27 21:49:14,358 Saving model at iteration 200000
2025-09-27 21:49:14,556 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000
2025-09-27 21:49:15,058 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/model.safetensors
2025-09-27 21:49:15,553 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/optimizer.bin
2025-09-27 21:49:15,553 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/scheduler.bin
2025-09-27 21:49:15,553 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/sampler.bin
2025-09-27 21:49:15,554 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/random_states_0.pkl
2025-09-27 21:49:42,067 Stage: Train 0.5 | Epoch: 131 | Iter: 200200 | Total Loss: 0.004129 | Recon Loss: 0.003523 | Commit Loss: 0.001210 | Perplexity: 2016.140652
2025-09-27 21:50:08,567 Stage: Train 0.5 | Epoch: 131 | Iter: 200400 | Total Loss: 0.004155 | Recon Loss: 0.003549 | Commit Loss: 0.001212 | Perplexity: 2012.293958
Trainning Epoch:  40%|████      | 132/330 [15:01:27<11:24:40, 207.48s/it]2025-09-27 21:50:35,448 Stage: Train 0.5 | Epoch: 132 | Iter: 200600 | Total Loss: 0.004188 | Recon Loss: 0.003589 | Commit Loss: 0.001199 | Perplexity: 2012.096295
2025-09-27 21:51:02,040 Stage: Train 0.5 | Epoch: 132 | Iter: 200800 | Total Loss: 0.004177 | Recon Loss: 0.003581 | Commit Loss: 0.001193 | Perplexity: 2011.133786
2025-09-27 21:51:28,704 Stage: Train 0.5 | Epoch: 132 | Iter: 201000 | Total Loss: 0.004190 | Recon Loss: 0.003586 | Commit Loss: 0.001208 | Perplexity: 2010.737159
2025-09-27 21:51:55,351 Stage: Train 0.5 | Epoch: 132 | Iter: 201200 | Total Loss: 0.004203 | Recon Loss: 0.003599 | Commit Loss: 0.001206 | Perplexity: 2016.045933
2025-09-27 21:52:21,875 Stage: Train 0.5 | Epoch: 132 | Iter: 201400 | Total Loss: 0.004148 | Recon Loss: 0.003547 | Commit Loss: 0.001202 | Perplexity: 2012.974078
2025-09-27 21:52:48,550 Stage: Train 0.5 | Epoch: 132 | Iter: 201600 | Total Loss: 0.004336 | Recon Loss: 0.003730 | Commit Loss: 0.001212 | Perplexity: 2015.829534
2025-09-27 21:53:15,148 Stage: Train 0.5 | Epoch: 132 | Iter: 201800 | Total Loss: 0.004063 | Recon Loss: 0.003458 | Commit Loss: 0.001212 | Perplexity: 2019.609635
2025-09-27 21:53:41,888 Stage: Train 0.5 | Epoch: 132 | Iter: 202000 | Total Loss: 0.004280 | Recon Loss: 0.003672 | Commit Loss: 0.001216 | Perplexity: 2015.780137
Trainning Epoch:  40%|████      | 133/330 [15:04:50<11:16:22, 206.00s/it]2025-09-27 21:54:08,679 Stage: Train 0.5 | Epoch: 133 | Iter: 202200 | Total Loss: 0.004141 | Recon Loss: 0.003544 | Commit Loss: 0.001194 | Perplexity: 2013.766041
2025-09-27 21:54:35,286 Stage: Train 0.5 | Epoch: 133 | Iter: 202400 | Total Loss: 0.004120 | Recon Loss: 0.003519 | Commit Loss: 0.001203 | Perplexity: 2017.170139
2025-09-27 21:55:01,972 Stage: Train 0.5 | Epoch: 133 | Iter: 202600 | Total Loss: 0.004127 | Recon Loss: 0.003529 | Commit Loss: 0.001198 | Perplexity: 2013.224824
2025-09-27 21:55:28,579 Stage: Train 0.5 | Epoch: 133 | Iter: 202800 | Total Loss: 0.004055 | Recon Loss: 0.003446 | Commit Loss: 0.001218 | Perplexity: 2022.363611
2025-09-27 21:55:55,200 Stage: Train 0.5 | Epoch: 133 | Iter: 203000 | Total Loss: 0.004167 | Recon Loss: 0.003561 | Commit Loss: 0.001210 | Perplexity: 2013.865225
2025-09-27 21:56:21,786 Stage: Train 0.5 | Epoch: 133 | Iter: 203200 | Total Loss: 0.004261 | Recon Loss: 0.003658 | Commit Loss: 0.001205 | Perplexity: 2014.842733
2025-09-27 21:56:48,495 Stage: Train 0.5 | Epoch: 133 | Iter: 203400 | Total Loss: 0.004110 | Recon Loss: 0.003504 | Commit Loss: 0.001211 | Perplexity: 2019.189971
Trainning Epoch:  41%|████      | 134/330 [15:08:12<11:09:26, 204.93s/it]2025-09-27 21:57:15,342 Stage: Train 0.5 | Epoch: 134 | Iter: 203600 | Total Loss: 0.004139 | Recon Loss: 0.003535 | Commit Loss: 0.001208 | Perplexity: 2011.804951
2025-09-27 21:57:42,010 Stage: Train 0.5 | Epoch: 134 | Iter: 203800 | Total Loss: 0.004157 | Recon Loss: 0.003562 | Commit Loss: 0.001189 | Perplexity: 2010.220679
2025-09-27 21:58:08,692 Stage: Train 0.5 | Epoch: 134 | Iter: 204000 | Total Loss: 0.004098 | Recon Loss: 0.003494 | Commit Loss: 0.001207 | Perplexity: 2016.266508
2025-09-27 21:58:35,382 Stage: Train 0.5 | Epoch: 134 | Iter: 204200 | Total Loss: 0.004241 | Recon Loss: 0.003642 | Commit Loss: 0.001198 | Perplexity: 2008.589319
2025-09-27 21:59:02,068 Stage: Train 0.5 | Epoch: 134 | Iter: 204400 | Total Loss: 0.004090 | Recon Loss: 0.003486 | Commit Loss: 0.001208 | Perplexity: 2018.392607
2025-09-27 21:59:28,743 Stage: Train 0.5 | Epoch: 134 | Iter: 204600 | Total Loss: 0.004124 | Recon Loss: 0.003525 | Commit Loss: 0.001198 | Perplexity: 2012.801307
2025-09-27 21:59:55,147 Stage: Train 0.5 | Epoch: 134 | Iter: 204800 | Total Loss: 0.004133 | Recon Loss: 0.003529 | Commit Loss: 0.001208 | Perplexity: 2018.669257
2025-09-27 22:00:21,689 Stage: Train 0.5 | Epoch: 134 | Iter: 205000 | Total Loss: 0.004147 | Recon Loss: 0.003545 | Commit Loss: 0.001205 | Perplexity: 2010.583288
Trainning Epoch:  41%|████      | 135/330 [15:11:35<11:03:35, 204.18s/it]2025-09-27 22:00:48,632 Stage: Train 0.5 | Epoch: 135 | Iter: 205200 | Total Loss: 0.004069 | Recon Loss: 0.003469 | Commit Loss: 0.001201 | Perplexity: 2012.622628
2025-09-27 22:01:15,214 Stage: Train 0.5 | Epoch: 135 | Iter: 205400 | Total Loss: 0.004139 | Recon Loss: 0.003542 | Commit Loss: 0.001193 | Perplexity: 2013.845522
2025-09-27 22:01:41,821 Stage: Train 0.5 | Epoch: 135 | Iter: 205600 | Total Loss: 0.004106 | Recon Loss: 0.003506 | Commit Loss: 0.001199 | Perplexity: 2014.098480
2025-09-27 22:02:08,333 Stage: Train 0.5 | Epoch: 135 | Iter: 205800 | Total Loss: 0.004197 | Recon Loss: 0.003595 | Commit Loss: 0.001203 | Perplexity: 2012.742881
2025-09-27 22:02:34,793 Stage: Train 0.5 | Epoch: 135 | Iter: 206000 | Total Loss: 0.004171 | Recon Loss: 0.003571 | Commit Loss: 0.001200 | Perplexity: 2012.296636
2025-09-27 22:03:01,484 Stage: Train 0.5 | Epoch: 135 | Iter: 206200 | Total Loss: 0.004192 | Recon Loss: 0.003590 | Commit Loss: 0.001203 | Perplexity: 2015.013302
2025-09-27 22:03:27,979 Stage: Train 0.5 | Epoch: 135 | Iter: 206400 | Total Loss: 0.004182 | Recon Loss: 0.003585 | Commit Loss: 0.001194 | Perplexity: 2012.564637
Trainning Epoch:  41%|████      | 136/330 [15:14:57<10:58:11, 203.56s/it]2025-09-27 22:03:54,822 Stage: Train 0.5 | Epoch: 136 | Iter: 206600 | Total Loss: 0.004125 | Recon Loss: 0.003527 | Commit Loss: 0.001195 | Perplexity: 2014.024577
2025-09-27 22:04:21,463 Stage: Train 0.5 | Epoch: 136 | Iter: 206800 | Total Loss: 0.004066 | Recon Loss: 0.003472 | Commit Loss: 0.001189 | Perplexity: 2013.077786
2025-09-27 22:04:48,250 Stage: Train 0.5 | Epoch: 136 | Iter: 207000 | Total Loss: 0.004085 | Recon Loss: 0.003488 | Commit Loss: 0.001194 | Perplexity: 2015.580763
2025-09-27 22:05:14,896 Stage: Train 0.5 | Epoch: 136 | Iter: 207200 | Total Loss: 0.004186 | Recon Loss: 0.003585 | Commit Loss: 0.001203 | Perplexity: 2011.509262
2025-09-27 22:05:41,575 Stage: Train 0.5 | Epoch: 136 | Iter: 207400 | Total Loss: 0.004144 | Recon Loss: 0.003550 | Commit Loss: 0.001189 | Perplexity: 2008.699331
2025-09-27 22:06:08,192 Stage: Train 0.5 | Epoch: 136 | Iter: 207600 | Total Loss: 0.004175 | Recon Loss: 0.003576 | Commit Loss: 0.001199 | Perplexity: 2019.908781
2025-09-27 22:06:34,746 Stage: Train 0.5 | Epoch: 136 | Iter: 207800 | Total Loss: 0.004097 | Recon Loss: 0.003502 | Commit Loss: 0.001191 | Perplexity: 2012.603306
2025-09-27 22:07:01,340 Stage: Train 0.5 | Epoch: 136 | Iter: 208000 | Total Loss: 0.004261 | Recon Loss: 0.003662 | Commit Loss: 0.001197 | Perplexity: 2014.606753
Trainning Epoch:  42%|████▏     | 137/330 [15:18:20<10:53:51, 203.27s/it]2025-09-27 22:07:28,093 Stage: Train 0.5 | Epoch: 137 | Iter: 208200 | Total Loss: 0.004100 | Recon Loss: 0.003500 | Commit Loss: 0.001200 | Perplexity: 2016.877969
2025-09-27 22:07:54,703 Stage: Train 0.5 | Epoch: 137 | Iter: 208400 | Total Loss: 0.004168 | Recon Loss: 0.003572 | Commit Loss: 0.001192 | Perplexity: 2015.408189
2025-09-27 22:08:21,373 Stage: Train 0.5 | Epoch: 137 | Iter: 208600 | Total Loss: 0.004128 | Recon Loss: 0.003528 | Commit Loss: 0.001199 | Perplexity: 2018.552344
2025-09-27 22:08:47,959 Stage: Train 0.5 | Epoch: 137 | Iter: 208800 | Total Loss: 0.004069 | Recon Loss: 0.003473 | Commit Loss: 0.001192 | Perplexity: 2013.414837
2025-09-27 22:09:14,532 Stage: Train 0.5 | Epoch: 137 | Iter: 209000 | Total Loss: 0.004082 | Recon Loss: 0.003485 | Commit Loss: 0.001195 | Perplexity: 2011.706977
2025-09-27 22:09:41,121 Stage: Train 0.5 | Epoch: 137 | Iter: 209200 | Total Loss: 0.004077 | Recon Loss: 0.003482 | Commit Loss: 0.001191 | Perplexity: 2010.227263
2025-09-27 22:10:07,726 Stage: Train 0.5 | Epoch: 137 | Iter: 209400 | Total Loss: 0.004164 | Recon Loss: 0.003567 | Commit Loss: 0.001195 | Perplexity: 2013.846924
2025-09-27 22:10:34,317 Stage: Train 0.5 | Epoch: 137 | Iter: 209600 | Total Loss: 0.004186 | Recon Loss: 0.003586 | Commit Loss: 0.001200 | Perplexity: 2012.121943
Trainning Epoch:  42%|████▏     | 138/330 [15:21:42<10:49:25, 202.94s/it]2025-09-27 22:11:01,140 Stage: Train 0.5 | Epoch: 138 | Iter: 209800 | Total Loss: 0.004146 | Recon Loss: 0.003557 | Commit Loss: 0.001180 | Perplexity: 2009.976991
2025-09-27 22:11:27,843 Stage: Train 0.5 | Epoch: 138 | Iter: 210000 | Total Loss: 0.004084 | Recon Loss: 0.003490 | Commit Loss: 0.001190 | Perplexity: 2016.910144
2025-09-27 22:11:54,471 Stage: Train 0.5 | Epoch: 138 | Iter: 210200 | Total Loss: 0.004079 | Recon Loss: 0.003479 | Commit Loss: 0.001200 | Perplexity: 2018.081346
2025-09-27 22:12:20,954 Stage: Train 0.5 | Epoch: 138 | Iter: 210400 | Total Loss: 0.004112 | Recon Loss: 0.003512 | Commit Loss: 0.001199 | Perplexity: 2015.735885
2025-09-27 22:12:47,457 Stage: Train 0.5 | Epoch: 138 | Iter: 210600 | Total Loss: 0.004236 | Recon Loss: 0.003641 | Commit Loss: 0.001191 | Perplexity: 2010.927354
2025-09-27 22:13:13,955 Stage: Train 0.5 | Epoch: 138 | Iter: 210800 | Total Loss: 0.003980 | Recon Loss: 0.003388 | Commit Loss: 0.001185 | Perplexity: 2011.845962
2025-09-27 22:13:40,460 Stage: Train 0.5 | Epoch: 138 | Iter: 211000 | Total Loss: 0.004151 | Recon Loss: 0.003552 | Commit Loss: 0.001198 | Perplexity: 2011.381854
Trainning Epoch:  42%|████▏     | 139/330 [15:25:04<10:45:04, 202.64s/it]2025-09-27 22:14:07,259 Stage: Train 0.5 | Epoch: 139 | Iter: 211200 | Total Loss: 0.004061 | Recon Loss: 0.003464 | Commit Loss: 0.001195 | Perplexity: 2011.608238
2025-09-27 22:14:33,915 Stage: Train 0.5 | Epoch: 139 | Iter: 211400 | Total Loss: 0.004138 | Recon Loss: 0.003544 | Commit Loss: 0.001188 | Perplexity: 2018.697425
2025-09-27 22:15:00,451 Stage: Train 0.5 | Epoch: 139 | Iter: 211600 | Total Loss: 0.004142 | Recon Loss: 0.003547 | Commit Loss: 0.001189 | Perplexity: 2008.591702
2025-09-27 22:15:27,104 Stage: Train 0.5 | Epoch: 139 | Iter: 211800 | Total Loss: 0.004094 | Recon Loss: 0.003500 | Commit Loss: 0.001189 | Perplexity: 2015.130920
2025-09-27 22:15:53,619 Stage: Train 0.5 | Epoch: 139 | Iter: 212000 | Total Loss: 0.004051 | Recon Loss: 0.003459 | Commit Loss: 0.001185 | Perplexity: 2017.238807
2025-09-27 22:16:20,033 Stage: Train 0.5 | Epoch: 139 | Iter: 212200 | Total Loss: 0.004200 | Recon Loss: 0.003599 | Commit Loss: 0.001202 | Perplexity: 2013.612164
2025-09-27 22:16:46,597 Stage: Train 0.5 | Epoch: 139 | Iter: 212400 | Total Loss: 0.004044 | Recon Loss: 0.003454 | Commit Loss: 0.001180 | Perplexity: 2012.772010
2025-09-27 22:17:13,050 Stage: Train 0.5 | Epoch: 139 | Iter: 212600 | Total Loss: 0.004206 | Recon Loss: 0.003613 | Commit Loss: 0.001186 | Perplexity: 2010.656474
Trainning Epoch:  42%|████▏     | 140/330 [15:28:26<10:40:56, 202.40s/it]2025-09-27 22:17:39,811 Stage: Train 0.5 | Epoch: 140 | Iter: 212800 | Total Loss: 0.004104 | Recon Loss: 0.003513 | Commit Loss: 0.001181 | Perplexity: 2008.253943
2025-09-27 22:18:06,392 Stage: Train 0.5 | Epoch: 140 | Iter: 213000 | Total Loss: 0.004039 | Recon Loss: 0.003450 | Commit Loss: 0.001180 | Perplexity: 2013.200231
2025-09-27 22:18:32,867 Stage: Train 0.5 | Epoch: 140 | Iter: 213200 | Total Loss: 0.004120 | Recon Loss: 0.003522 | Commit Loss: 0.001196 | Perplexity: 2015.786412
2025-09-27 22:18:59,456 Stage: Train 0.5 | Epoch: 140 | Iter: 213400 | Total Loss: 0.004061 | Recon Loss: 0.003466 | Commit Loss: 0.001190 | Perplexity: 2016.949662
2025-09-27 22:19:26,011 Stage: Train 0.5 | Epoch: 140 | Iter: 213600 | Total Loss: 0.004134 | Recon Loss: 0.003542 | Commit Loss: 0.001184 | Perplexity: 2009.480683
2025-09-27 22:19:52,508 Stage: Train 0.5 | Epoch: 140 | Iter: 213800 | Total Loss: 0.004026 | Recon Loss: 0.003435 | Commit Loss: 0.001182 | Perplexity: 2008.584976
2025-09-27 22:20:19,153 Stage: Train 0.5 | Epoch: 140 | Iter: 214000 | Total Loss: 0.004090 | Recon Loss: 0.003495 | Commit Loss: 0.001190 | Perplexity: 2011.963962
Trainning Epoch:  43%|████▎     | 141/330 [15:31:48<10:37:13, 202.29s/it]2025-09-27 22:20:46,095 Stage: Train 0.5 | Epoch: 141 | Iter: 214200 | Total Loss: 0.004068 | Recon Loss: 0.003473 | Commit Loss: 0.001191 | Perplexity: 2018.578794
2025-09-27 22:21:12,745 Stage: Train 0.5 | Epoch: 141 | Iter: 214400 | Total Loss: 0.004052 | Recon Loss: 0.003461 | Commit Loss: 0.001183 | Perplexity: 2011.048804
2025-09-27 22:21:39,288 Stage: Train 0.5 | Epoch: 141 | Iter: 214600 | Total Loss: 0.004086 | Recon Loss: 0.003494 | Commit Loss: 0.001183 | Perplexity: 2015.531520
2025-09-27 22:22:05,811 Stage: Train 0.5 | Epoch: 141 | Iter: 214800 | Total Loss: 0.004072 | Recon Loss: 0.003476 | Commit Loss: 0.001192 | Perplexity: 2017.587820
2025-09-27 22:22:32,315 Stage: Train 0.5 | Epoch: 141 | Iter: 215000 | Total Loss: 0.004288 | Recon Loss: 0.003692 | Commit Loss: 0.001193 | Perplexity: 2018.783873
2025-09-27 22:22:58,900 Stage: Train 0.5 | Epoch: 141 | Iter: 215200 | Total Loss: 0.004008 | Recon Loss: 0.003414 | Commit Loss: 0.001188 | Perplexity: 2011.194061
2025-09-27 22:23:25,412 Stage: Train 0.5 | Epoch: 141 | Iter: 215400 | Total Loss: 0.004093 | Recon Loss: 0.003498 | Commit Loss: 0.001190 | Perplexity: 2016.452916
2025-09-27 22:23:52,045 Stage: Train 0.5 | Epoch: 141 | Iter: 215600 | Total Loss: 0.004115 | Recon Loss: 0.003523 | Commit Loss: 0.001185 | Perplexity: 2015.542780
Trainning Epoch:  43%|████▎     | 142/330 [15:35:10<10:33:36, 202.21s/it]2025-09-27 22:24:18,902 Stage: Train 0.5 | Epoch: 142 | Iter: 215800 | Total Loss: 0.004113 | Recon Loss: 0.003524 | Commit Loss: 0.001178 | Perplexity: 2009.047723
2025-09-27 22:24:45,498 Stage: Train 0.5 | Epoch: 142 | Iter: 216000 | Total Loss: 0.004085 | Recon Loss: 0.003495 | Commit Loss: 0.001180 | Perplexity: 2015.322355
2025-09-27 22:25:12,032 Stage: Train 0.5 | Epoch: 142 | Iter: 216200 | Total Loss: 0.004034 | Recon Loss: 0.003449 | Commit Loss: 0.001170 | Perplexity: 2015.338401
2025-09-27 22:25:38,631 Stage: Train 0.5 | Epoch: 142 | Iter: 216400 | Total Loss: 0.004063 | Recon Loss: 0.003470 | Commit Loss: 0.001186 | Perplexity: 2011.748461
2025-09-27 22:26:05,112 Stage: Train 0.5 | Epoch: 142 | Iter: 216600 | Total Loss: 0.004124 | Recon Loss: 0.003529 | Commit Loss: 0.001189 | Perplexity: 2016.010681
2025-09-27 22:26:31,812 Stage: Train 0.5 | Epoch: 142 | Iter: 216800 | Total Loss: 0.004068 | Recon Loss: 0.003470 | Commit Loss: 0.001195 | Perplexity: 2020.380562
2025-09-27 22:26:58,370 Stage: Train 0.5 | Epoch: 142 | Iter: 217000 | Total Loss: 0.004011 | Recon Loss: 0.003416 | Commit Loss: 0.001189 | Perplexity: 2017.300710
2025-09-27 22:27:24,904 Stage: Train 0.5 | Epoch: 142 | Iter: 217200 | Total Loss: 0.004098 | Recon Loss: 0.003502 | Commit Loss: 0.001192 | Perplexity: 2013.590069
Trainning Epoch:  43%|████▎     | 143/330 [15:38:32<10:30:05, 202.17s/it]2025-09-27 22:27:51,736 Stage: Train 0.5 | Epoch: 143 | Iter: 217400 | Total Loss: 0.004159 | Recon Loss: 0.003568 | Commit Loss: 0.001181 | Perplexity: 2016.215861
2025-09-27 22:28:18,422 Stage: Train 0.5 | Epoch: 143 | Iter: 217600 | Total Loss: 0.004006 | Recon Loss: 0.003414 | Commit Loss: 0.001185 | Perplexity: 2015.169849
2025-09-27 22:28:45,082 Stage: Train 0.5 | Epoch: 143 | Iter: 217800 | Total Loss: 0.004019 | Recon Loss: 0.003430 | Commit Loss: 0.001179 | Perplexity: 2013.359662
2025-09-27 22:29:11,699 Stage: Train 0.5 | Epoch: 143 | Iter: 218000 | Total Loss: 0.004013 | Recon Loss: 0.003421 | Commit Loss: 0.001183 | Perplexity: 2016.183399
2025-09-27 22:29:38,420 Stage: Train 0.5 | Epoch: 143 | Iter: 218200 | Total Loss: 0.004075 | Recon Loss: 0.003479 | Commit Loss: 0.001192 | Perplexity: 2013.462463
2025-09-27 22:30:04,984 Stage: Train 0.5 | Epoch: 143 | Iter: 218400 | Total Loss: 0.004089 | Recon Loss: 0.003493 | Commit Loss: 0.001192 | Perplexity: 2023.118782
2025-09-27 22:30:31,552 Stage: Train 0.5 | Epoch: 143 | Iter: 218600 | Total Loss: 0.004111 | Recon Loss: 0.003518 | Commit Loss: 0.001186 | Perplexity: 2018.128039
Trainning Epoch:  44%|████▎     | 144/330 [15:41:54<10:27:04, 202.28s/it]2025-09-27 22:30:58,396 Stage: Train 0.5 | Epoch: 144 | Iter: 218800 | Total Loss: 0.003978 | Recon Loss: 0.003386 | Commit Loss: 0.001183 | Perplexity: 2016.609657
2025-09-27 22:31:25,063 Stage: Train 0.5 | Epoch: 144 | Iter: 219000 | Total Loss: 0.004061 | Recon Loss: 0.003470 | Commit Loss: 0.001182 | Perplexity: 2015.689106
2025-09-27 22:31:51,650 Stage: Train 0.5 | Epoch: 144 | Iter: 219200 | Total Loss: 0.004054 | Recon Loss: 0.003463 | Commit Loss: 0.001182 | Perplexity: 2014.196321
2025-09-27 22:32:18,278 Stage: Train 0.5 | Epoch: 144 | Iter: 219400 | Total Loss: 0.004201 | Recon Loss: 0.003608 | Commit Loss: 0.001184 | Perplexity: 2018.552528
2025-09-27 22:32:44,912 Stage: Train 0.5 | Epoch: 144 | Iter: 219600 | Total Loss: 0.004010 | Recon Loss: 0.003423 | Commit Loss: 0.001175 | Perplexity: 2010.817179
2025-09-27 22:33:11,472 Stage: Train 0.5 | Epoch: 144 | Iter: 219800 | Total Loss: 0.004123 | Recon Loss: 0.003527 | Commit Loss: 0.001193 | Perplexity: 2015.514304
2025-09-27 22:33:38,070 Stage: Train 0.5 | Epoch: 144 | Iter: 220000 | Total Loss: 0.004014 | Recon Loss: 0.003422 | Commit Loss: 0.001184 | Perplexity: 2019.014822
2025-09-27 22:33:38,070 Saving model at iteration 220000
2025-09-27 22:33:38,254 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000
2025-09-27 22:33:38,732 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/model.safetensors
2025-09-27 22:33:39,244 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/optimizer.bin
2025-09-27 22:33:39,244 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/scheduler.bin
2025-09-27 22:33:39,244 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/sampler.bin
2025-09-27 22:33:39,245 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/random_states_0.pkl
2025-09-27 22:34:05,896 Stage: Train 0.5 | Epoch: 144 | Iter: 220200 | Total Loss: 0.004029 | Recon Loss: 0.003440 | Commit Loss: 0.001178 | Perplexity: 2014.009840
Trainning Epoch:  44%|████▍     | 145/330 [15:45:18<10:24:57, 202.69s/it]2025-09-27 22:34:32,710 Stage: Train 0.5 | Epoch: 145 | Iter: 220400 | Total Loss: 0.003997 | Recon Loss: 0.003403 | Commit Loss: 0.001187 | Perplexity: 2016.733851
2025-09-27 22:34:59,213 Stage: Train 0.5 | Epoch: 145 | Iter: 220600 | Total Loss: 0.004061 | Recon Loss: 0.003469 | Commit Loss: 0.001186 | Perplexity: 2018.679288
2025-09-27 22:35:25,749 Stage: Train 0.5 | Epoch: 145 | Iter: 220800 | Total Loss: 0.004054 | Recon Loss: 0.003459 | Commit Loss: 0.001189 | Perplexity: 2017.496734
2025-09-27 22:35:52,313 Stage: Train 0.5 | Epoch: 145 | Iter: 221000 | Total Loss: 0.004091 | Recon Loss: 0.003499 | Commit Loss: 0.001183 | Perplexity: 2013.929708
2025-09-27 22:36:18,929 Stage: Train 0.5 | Epoch: 145 | Iter: 221200 | Total Loss: 0.004055 | Recon Loss: 0.003465 | Commit Loss: 0.001181 | Perplexity: 2017.695715
2025-09-27 22:36:45,416 Stage: Train 0.5 | Epoch: 145 | Iter: 221400 | Total Loss: 0.003965 | Recon Loss: 0.003378 | Commit Loss: 0.001174 | Perplexity: 2013.168522
2025-09-27 22:37:12,035 Stage: Train 0.5 | Epoch: 145 | Iter: 221600 | Total Loss: 0.003979 | Recon Loss: 0.003391 | Commit Loss: 0.001177 | Perplexity: 2010.162715
Trainning Epoch:  44%|████▍     | 146/330 [15:48:40<10:20:45, 202.42s/it]2025-09-27 22:37:38,795 Stage: Train 0.5 | Epoch: 146 | Iter: 221800 | Total Loss: 0.004091 | Recon Loss: 0.003505 | Commit Loss: 0.001174 | Perplexity: 2007.488842
2025-09-27 22:38:05,302 Stage: Train 0.5 | Epoch: 146 | Iter: 222000 | Total Loss: 0.003935 | Recon Loss: 0.003346 | Commit Loss: 0.001177 | Perplexity: 2016.456655
2025-09-27 22:38:31,895 Stage: Train 0.5 | Epoch: 146 | Iter: 222200 | Total Loss: 0.003975 | Recon Loss: 0.003388 | Commit Loss: 0.001173 | Perplexity: 2015.627081
2025-09-27 22:38:58,390 Stage: Train 0.5 | Epoch: 146 | Iter: 222400 | Total Loss: 0.004096 | Recon Loss: 0.003503 | Commit Loss: 0.001186 | Perplexity: 2016.152067
2025-09-27 22:39:24,810 Stage: Train 0.5 | Epoch: 146 | Iter: 222600 | Total Loss: 0.004021 | Recon Loss: 0.003429 | Commit Loss: 0.001184 | Perplexity: 2019.472003
2025-09-27 22:39:51,413 Stage: Train 0.5 | Epoch: 146 | Iter: 222800 | Total Loss: 0.004141 | Recon Loss: 0.003547 | Commit Loss: 0.001187 | Perplexity: 2015.444792
2025-09-27 22:40:17,928 Stage: Train 0.5 | Epoch: 146 | Iter: 223000 | Total Loss: 0.003952 | Recon Loss: 0.003364 | Commit Loss: 0.001177 | Perplexity: 2012.908840
2025-09-27 22:40:44,534 Stage: Train 0.5 | Epoch: 146 | Iter: 223200 | Total Loss: 0.004063 | Recon Loss: 0.003477 | Commit Loss: 0.001172 | Perplexity: 2012.823319
Trainning Epoch:  45%|████▍     | 147/330 [15:52:01<10:16:46, 202.22s/it]2025-09-27 22:41:11,348 Stage: Train 0.5 | Epoch: 147 | Iter: 223400 | Total Loss: 0.004087 | Recon Loss: 0.003499 | Commit Loss: 0.001176 | Perplexity: 2012.762241
2025-09-27 22:41:37,843 Stage: Train 0.5 | Epoch: 147 | Iter: 223600 | Total Loss: 0.004021 | Recon Loss: 0.003431 | Commit Loss: 0.001182 | Perplexity: 2016.195544
2025-09-27 22:42:04,288 Stage: Train 0.5 | Epoch: 147 | Iter: 223800 | Total Loss: 0.004032 | Recon Loss: 0.003444 | Commit Loss: 0.001176 | Perplexity: 2019.560795
2025-09-27 22:42:30,846 Stage: Train 0.5 | Epoch: 147 | Iter: 224000 | Total Loss: 0.004048 | Recon Loss: 0.003460 | Commit Loss: 0.001176 | Perplexity: 2015.637185
2025-09-27 22:42:57,362 Stage: Train 0.5 | Epoch: 147 | Iter: 224200 | Total Loss: 0.004004 | Recon Loss: 0.003417 | Commit Loss: 0.001174 | Perplexity: 2015.508729
2025-09-27 22:43:23,892 Stage: Train 0.5 | Epoch: 147 | Iter: 224400 | Total Loss: 0.004010 | Recon Loss: 0.003419 | Commit Loss: 0.001184 | Perplexity: 2017.914852
2025-09-27 22:43:50,501 Stage: Train 0.5 | Epoch: 147 | Iter: 224600 | Total Loss: 0.004025 | Recon Loss: 0.003439 | Commit Loss: 0.001172 | Perplexity: 2012.202397
2025-09-27 22:44:17,058 Stage: Train 0.5 | Epoch: 147 | Iter: 224800 | Total Loss: 0.003975 | Recon Loss: 0.003387 | Commit Loss: 0.001176 | Perplexity: 2012.309402
Trainning Epoch:  45%|████▍     | 148/330 [15:55:23<10:12:59, 202.08s/it]2025-09-27 22:44:43,817 Stage: Train 0.5 | Epoch: 148 | Iter: 225000 | Total Loss: 0.004060 | Recon Loss: 0.003473 | Commit Loss: 0.001174 | Perplexity: 2011.565023
2025-09-27 22:45:10,335 Stage: Train 0.5 | Epoch: 148 | Iter: 225200 | Total Loss: 0.003987 | Recon Loss: 0.003402 | Commit Loss: 0.001171 | Perplexity: 2018.355545
2025-09-27 22:45:36,967 Stage: Train 0.5 | Epoch: 148 | Iter: 225400 | Total Loss: 0.004027 | Recon Loss: 0.003432 | Commit Loss: 0.001191 | Perplexity: 2022.028461
2025-09-27 22:46:03,532 Stage: Train 0.5 | Epoch: 148 | Iter: 225600 | Total Loss: 0.004018 | Recon Loss: 0.003427 | Commit Loss: 0.001181 | Perplexity: 2019.722126
2025-09-27 22:46:30,140 Stage: Train 0.5 | Epoch: 148 | Iter: 225800 | Total Loss: 0.004059 | Recon Loss: 0.003473 | Commit Loss: 0.001173 | Perplexity: 2014.856923
2025-09-27 22:46:56,693 Stage: Train 0.5 | Epoch: 148 | Iter: 226000 | Total Loss: 0.004042 | Recon Loss: 0.003452 | Commit Loss: 0.001180 | Perplexity: 2017.162493
2025-09-27 22:47:23,379 Stage: Train 0.5 | Epoch: 148 | Iter: 226200 | Total Loss: 0.004003 | Recon Loss: 0.003416 | Commit Loss: 0.001175 | Perplexity: 2012.804020
Trainning Epoch:  45%|████▌     | 149/330 [15:58:45<10:09:37, 202.09s/it]2025-09-27 22:47:50,090 Stage: Train 0.5 | Epoch: 149 | Iter: 226400 | Total Loss: 0.004030 | Recon Loss: 0.003447 | Commit Loss: 0.001166 | Perplexity: 2011.998080
2025-09-27 22:48:16,482 Stage: Train 0.5 | Epoch: 149 | Iter: 226600 | Total Loss: 0.004000 | Recon Loss: 0.003410 | Commit Loss: 0.001178 | Perplexity: 2018.985217
2025-09-27 22:48:42,967 Stage: Train 0.5 | Epoch: 149 | Iter: 226800 | Total Loss: 0.004081 | Recon Loss: 0.003495 | Commit Loss: 0.001172 | Perplexity: 2012.506180
2025-09-27 22:49:09,546 Stage: Train 0.5 | Epoch: 149 | Iter: 227000 | Total Loss: 0.003979 | Recon Loss: 0.003391 | Commit Loss: 0.001175 | Perplexity: 2012.741498
2025-09-27 22:49:36,057 Stage: Train 0.5 | Epoch: 149 | Iter: 227200 | Total Loss: 0.003950 | Recon Loss: 0.003367 | Commit Loss: 0.001166 | Perplexity: 2015.070948
2025-09-27 22:50:02,610 Stage: Train 0.5 | Epoch: 149 | Iter: 227400 | Total Loss: 0.004084 | Recon Loss: 0.003496 | Commit Loss: 0.001175 | Perplexity: 2018.579394
2025-09-27 22:50:29,178 Stage: Train 0.5 | Epoch: 149 | Iter: 227600 | Total Loss: 0.003899 | Recon Loss: 0.003316 | Commit Loss: 0.001168 | Perplexity: 2014.434325
2025-09-27 22:50:55,695 Stage: Train 0.5 | Epoch: 149 | Iter: 227800 | Total Loss: 0.004049 | Recon Loss: 0.003463 | Commit Loss: 0.001172 | Perplexity: 2013.643354
Trainning Epoch:  45%|████▌     | 150/330 [16:02:07<10:05:49, 201.94s/it]2025-09-27 22:51:22,522 Stage: Train 0.5 | Epoch: 150 | Iter: 228000 | Total Loss: 0.003965 | Recon Loss: 0.003377 | Commit Loss: 0.001175 | Perplexity: 2014.761934
2025-09-27 22:51:49,176 Stage: Train 0.5 | Epoch: 150 | Iter: 228200 | Total Loss: 0.003945 | Recon Loss: 0.003359 | Commit Loss: 0.001171 | Perplexity: 2014.049490
2025-09-27 22:52:15,759 Stage: Train 0.5 | Epoch: 150 | Iter: 228400 | Total Loss: 0.004024 | Recon Loss: 0.003435 | Commit Loss: 0.001178 | Perplexity: 2021.863016
2025-09-27 22:52:42,382 Stage: Train 0.5 | Epoch: 150 | Iter: 228600 | Total Loss: 0.003968 | Recon Loss: 0.003380 | Commit Loss: 0.001177 | Perplexity: 2018.594116
2025-09-27 22:53:09,004 Stage: Train 0.5 | Epoch: 150 | Iter: 228800 | Total Loss: 0.003990 | Recon Loss: 0.003404 | Commit Loss: 0.001171 | Perplexity: 2014.338546
2025-09-27 22:53:35,547 Stage: Train 0.5 | Epoch: 150 | Iter: 229000 | Total Loss: 0.003974 | Recon Loss: 0.003390 | Commit Loss: 0.001169 | Perplexity: 2017.925217
2025-09-27 22:54:02,127 Stage: Train 0.5 | Epoch: 150 | Iter: 229200 | Total Loss: 0.004095 | Recon Loss: 0.003509 | Commit Loss: 0.001171 | Perplexity: 2015.147892
Trainning Epoch:  46%|████▌     | 151/330 [16:05:29<10:02:37, 202.00s/it]2025-09-27 22:54:28,810 Stage: Train 0.5 | Epoch: 151 | Iter: 229400 | Total Loss: 0.004033 | Recon Loss: 0.003443 | Commit Loss: 0.001180 | Perplexity: 2018.210511
2025-09-27 22:54:55,448 Stage: Train 0.5 | Epoch: 151 | Iter: 229600 | Total Loss: 0.003956 | Recon Loss: 0.003373 | Commit Loss: 0.001166 | Perplexity: 2020.048600
2025-09-27 22:55:21,968 Stage: Train 0.5 | Epoch: 151 | Iter: 229800 | Total Loss: 0.004036 | Recon Loss: 0.003452 | Commit Loss: 0.001169 | Perplexity: 2014.685033
2025-09-27 22:55:48,511 Stage: Train 0.5 | Epoch: 151 | Iter: 230000 | Total Loss: 0.003978 | Recon Loss: 0.003395 | Commit Loss: 0.001167 | Perplexity: 2014.260224
2025-09-27 22:56:15,059 Stage: Train 0.5 | Epoch: 151 | Iter: 230200 | Total Loss: 0.003963 | Recon Loss: 0.003377 | Commit Loss: 0.001173 | Perplexity: 2018.845505
2025-09-27 22:56:41,659 Stage: Train 0.5 | Epoch: 151 | Iter: 230400 | Total Loss: 0.003967 | Recon Loss: 0.003380 | Commit Loss: 0.001174 | Perplexity: 2018.878948
2025-09-27 22:57:08,246 Stage: Train 0.5 | Epoch: 151 | Iter: 230600 | Total Loss: 0.003965 | Recon Loss: 0.003378 | Commit Loss: 0.001174 | Perplexity: 2018.125061
2025-09-27 22:57:34,874 Stage: Train 0.5 | Epoch: 151 | Iter: 230800 | Total Loss: 0.004091 | Recon Loss: 0.003508 | Commit Loss: 0.001167 | Perplexity: 2011.735790
Trainning Epoch:  46%|████▌     | 152/330 [16:08:51<9:59:22, 202.04s/it] 2025-09-27 22:58:01,661 Stage: Train 0.5 | Epoch: 152 | Iter: 231000 | Total Loss: 0.003942 | Recon Loss: 0.003359 | Commit Loss: 0.001166 | Perplexity: 2015.259146
2025-09-27 22:58:28,203 Stage: Train 0.5 | Epoch: 152 | Iter: 231200 | Total Loss: 0.003901 | Recon Loss: 0.003319 | Commit Loss: 0.001164 | Perplexity: 2015.705590
2025-09-27 22:58:54,950 Stage: Train 0.5 | Epoch: 152 | Iter: 231400 | Total Loss: 0.003973 | Recon Loss: 0.003391 | Commit Loss: 0.001164 | Perplexity: 2016.867787
2025-09-27 22:59:21,445 Stage: Train 0.5 | Epoch: 152 | Iter: 231600 | Total Loss: 0.003962 | Recon Loss: 0.003377 | Commit Loss: 0.001170 | Perplexity: 2018.143491
2025-09-27 22:59:47,887 Stage: Train 0.5 | Epoch: 152 | Iter: 231800 | Total Loss: 0.003972 | Recon Loss: 0.003385 | Commit Loss: 0.001174 | Perplexity: 2017.986317
2025-09-27 23:00:14,534 Stage: Train 0.5 | Epoch: 152 | Iter: 232000 | Total Loss: 0.003928 | Recon Loss: 0.003344 | Commit Loss: 0.001169 | Perplexity: 2017.895178
2025-09-27 23:00:41,074 Stage: Train 0.5 | Epoch: 152 | Iter: 232200 | Total Loss: 0.004036 | Recon Loss: 0.003450 | Commit Loss: 0.001172 | Perplexity: 2015.991440
2025-09-27 23:01:07,605 Stage: Train 0.5 | Epoch: 152 | Iter: 232400 | Total Loss: 0.003964 | Recon Loss: 0.003377 | Commit Loss: 0.001174 | Perplexity: 2018.251903
Trainning Epoch:  46%|████▋     | 153/330 [16:12:13<9:55:55, 202.01s/it]2025-09-27 23:01:34,327 Stage: Train 0.5 | Epoch: 153 | Iter: 232600 | Total Loss: 0.003950 | Recon Loss: 0.003363 | Commit Loss: 0.001173 | Perplexity: 2014.068519
2025-09-27 23:02:00,960 Stage: Train 0.5 | Epoch: 153 | Iter: 232800 | Total Loss: 0.004018 | Recon Loss: 0.003432 | Commit Loss: 0.001170 | Perplexity: 2021.504931
2025-09-27 23:02:27,494 Stage: Train 0.5 | Epoch: 153 | Iter: 233000 | Total Loss: 0.003963 | Recon Loss: 0.003380 | Commit Loss: 0.001166 | Perplexity: 2016.946971
2025-09-27 23:02:54,190 Stage: Train 0.5 | Epoch: 153 | Iter: 233200 | Total Loss: 0.003949 | Recon Loss: 0.003368 | Commit Loss: 0.001161 | Perplexity: 2014.104090
2025-09-27 23:03:20,836 Stage: Train 0.5 | Epoch: 153 | Iter: 233400 | Total Loss: 0.003961 | Recon Loss: 0.003380 | Commit Loss: 0.001162 | Perplexity: 2017.271602
2025-09-27 23:03:47,375 Stage: Train 0.5 | Epoch: 153 | Iter: 233600 | Total Loss: 0.003980 | Recon Loss: 0.003397 | Commit Loss: 0.001166 | Perplexity: 2019.395588
2025-09-27 23:04:14,111 Stage: Train 0.5 | Epoch: 153 | Iter: 233800 | Total Loss: 0.003869 | Recon Loss: 0.003281 | Commit Loss: 0.001175 | Perplexity: 2022.154880
Trainning Epoch:  47%|████▋     | 154/330 [16:15:35<9:52:48, 202.09s/it]2025-09-27 23:04:41,041 Stage: Train 0.5 | Epoch: 154 | Iter: 234000 | Total Loss: 0.004115 | Recon Loss: 0.003530 | Commit Loss: 0.001169 | Perplexity: 2017.300018
2025-09-27 23:05:07,615 Stage: Train 0.5 | Epoch: 154 | Iter: 234200 | Total Loss: 0.003984 | Recon Loss: 0.003403 | Commit Loss: 0.001162 | Perplexity: 2012.464126
2025-09-27 23:05:34,187 Stage: Train 0.5 | Epoch: 154 | Iter: 234400 | Total Loss: 0.003982 | Recon Loss: 0.003401 | Commit Loss: 0.001161 | Perplexity: 2020.364941
2025-09-27 23:06:00,864 Stage: Train 0.5 | Epoch: 154 | Iter: 234600 | Total Loss: 0.004030 | Recon Loss: 0.003444 | Commit Loss: 0.001171 | Perplexity: 2018.840532
2025-09-27 23:06:27,465 Stage: Train 0.5 | Epoch: 154 | Iter: 234800 | Total Loss: 0.004299 | Recon Loss: 0.003714 | Commit Loss: 0.001170 | Perplexity: 2011.408228
2025-09-27 23:06:53,840 Stage: Train 0.5 | Epoch: 154 | Iter: 235000 | Total Loss: 0.003866 | Recon Loss: 0.003287 | Commit Loss: 0.001158 | Perplexity: 2012.589781
2025-09-27 23:07:20,494 Stage: Train 0.5 | Epoch: 154 | Iter: 235200 | Total Loss: 0.003930 | Recon Loss: 0.003353 | Commit Loss: 0.001154 | Perplexity: 2010.579467
2025-09-27 23:07:47,118 Stage: Train 0.5 | Epoch: 154 | Iter: 235400 | Total Loss: 0.004039 | Recon Loss: 0.003457 | Commit Loss: 0.001164 | Perplexity: 2017.872483
Trainning Epoch:  47%|████▋     | 155/330 [16:18:58<9:49:36, 202.15s/it]2025-09-27 23:08:13,907 Stage: Train 0.5 | Epoch: 155 | Iter: 235600 | Total Loss: 0.003979 | Recon Loss: 0.003394 | Commit Loss: 0.001171 | Perplexity: 2017.959848
2025-09-27 23:08:40,428 Stage: Train 0.5 | Epoch: 155 | Iter: 235800 | Total Loss: 0.003958 | Recon Loss: 0.003381 | Commit Loss: 0.001154 | Perplexity: 2015.942638
2025-09-27 23:09:06,882 Stage: Train 0.5 | Epoch: 155 | Iter: 236000 | Total Loss: 0.004050 | Recon Loss: 0.003471 | Commit Loss: 0.001157 | Perplexity: 2013.213757
2025-09-27 23:09:33,483 Stage: Train 0.5 | Epoch: 155 | Iter: 236200 | Total Loss: 0.003907 | Recon Loss: 0.003324 | Commit Loss: 0.001166 | Perplexity: 2021.071043
2025-09-27 23:10:00,054 Stage: Train 0.5 | Epoch: 155 | Iter: 236400 | Total Loss: 0.003971 | Recon Loss: 0.003391 | Commit Loss: 0.001160 | Perplexity: 2012.138505
2025-09-27 23:10:26,610 Stage: Train 0.5 | Epoch: 155 | Iter: 236600 | Total Loss: 0.003981 | Recon Loss: 0.003400 | Commit Loss: 0.001161 | Perplexity: 2018.757446
2025-09-27 23:10:53,077 Stage: Train 0.5 | Epoch: 155 | Iter: 236800 | Total Loss: 0.003934 | Recon Loss: 0.003350 | Commit Loss: 0.001167 | Perplexity: 2020.923064
Trainning Epoch:  47%|████▋     | 156/330 [16:22:19<9:45:52, 202.03s/it]2025-09-27 23:11:19,863 Stage: Train 0.5 | Epoch: 156 | Iter: 237000 | Total Loss: 0.003981 | Recon Loss: 0.003401 | Commit Loss: 0.001160 | Perplexity: 2019.998376
2025-09-27 23:11:46,430 Stage: Train 0.5 | Epoch: 156 | Iter: 237200 | Total Loss: 0.003946 | Recon Loss: 0.003364 | Commit Loss: 0.001164 | Perplexity: 2018.745236
2025-09-27 23:12:12,836 Stage: Train 0.5 | Epoch: 156 | Iter: 237400 | Total Loss: 0.003935 | Recon Loss: 0.003355 | Commit Loss: 0.001161 | Perplexity: 2018.966249
2025-09-27 23:12:39,399 Stage: Train 0.5 | Epoch: 156 | Iter: 237600 | Total Loss: 0.003969 | Recon Loss: 0.003388 | Commit Loss: 0.001161 | Perplexity: 2016.617117
2025-09-27 23:13:05,857 Stage: Train 0.5 | Epoch: 156 | Iter: 237800 | Total Loss: 0.003926 | Recon Loss: 0.003348 | Commit Loss: 0.001155 | Perplexity: 2016.654461
2025-09-27 23:13:32,320 Stage: Train 0.5 | Epoch: 156 | Iter: 238000 | Total Loss: 0.003900 | Recon Loss: 0.003316 | Commit Loss: 0.001168 | Perplexity: 2020.811353
2025-09-27 23:13:58,871 Stage: Train 0.5 | Epoch: 156 | Iter: 238200 | Total Loss: 0.004001 | Recon Loss: 0.003420 | Commit Loss: 0.001163 | Perplexity: 2019.301895
2025-09-27 23:14:25,351 Stage: Train 0.5 | Epoch: 156 | Iter: 238400 | Total Loss: 0.003884 | Recon Loss: 0.003302 | Commit Loss: 0.001163 | Perplexity: 2016.410963
Trainning Epoch:  48%|████▊     | 157/330 [16:25:41<9:42:01, 201.86s/it]2025-09-27 23:14:52,087 Stage: Train 0.5 | Epoch: 157 | Iter: 238600 | Total Loss: 0.004156 | Recon Loss: 0.003575 | Commit Loss: 0.001162 | Perplexity: 2014.966943
2025-09-27 23:15:18,664 Stage: Train 0.5 | Epoch: 157 | Iter: 238800 | Total Loss: 0.003873 | Recon Loss: 0.003291 | Commit Loss: 0.001165 | Perplexity: 2024.130154
2025-09-27 23:15:45,275 Stage: Train 0.5 | Epoch: 157 | Iter: 239000 | Total Loss: 0.003933 | Recon Loss: 0.003349 | Commit Loss: 0.001168 | Perplexity: 2018.159188
2025-09-27 23:16:11,882 Stage: Train 0.5 | Epoch: 157 | Iter: 239200 | Total Loss: 0.003860 | Recon Loss: 0.003276 | Commit Loss: 0.001168 | Perplexity: 2020.850629
2025-09-27 23:16:38,439 Stage: Train 0.5 | Epoch: 157 | Iter: 239400 | Total Loss: 0.004044 | Recon Loss: 0.003465 | Commit Loss: 0.001157 | Perplexity: 2017.218349
2025-09-27 23:17:04,985 Stage: Train 0.5 | Epoch: 157 | Iter: 239600 | Total Loss: 0.003952 | Recon Loss: 0.003373 | Commit Loss: 0.001158 | Perplexity: 2014.277076
2025-09-27 23:17:31,613 Stage: Train 0.5 | Epoch: 157 | Iter: 239800 | Total Loss: 0.003901 | Recon Loss: 0.003318 | Commit Loss: 0.001167 | Perplexity: 2024.524351
2025-09-27 23:17:58,190 Stage: Train 0.5 | Epoch: 157 | Iter: 240000 | Total Loss: 0.003913 | Recon Loss: 0.003332 | Commit Loss: 0.001162 | Perplexity: 2016.606776
2025-09-27 23:17:58,191 Saving model at iteration 240000
2025-09-27 23:17:58,710 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000
2025-09-27 23:17:59,161 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/model.safetensors
2025-09-27 23:17:59,628 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/optimizer.bin
2025-09-27 23:17:59,628 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/scheduler.bin
2025-09-27 23:17:59,628 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/sampler.bin
2025-09-27 23:17:59,629 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 158/330 [16:29:05<9:40:12, 202.40s/it]2025-09-27 23:18:26,595 Stage: Train 0.5 | Epoch: 158 | Iter: 240200 | Total Loss: 0.003921 | Recon Loss: 0.003343 | Commit Loss: 0.001157 | Perplexity: 2015.066345
2025-09-27 23:18:53,276 Stage: Train 0.5 | Epoch: 158 | Iter: 240400 | Total Loss: 0.003895 | Recon Loss: 0.003316 | Commit Loss: 0.001159 | Perplexity: 2017.558468
2025-09-27 23:19:19,896 Stage: Train 0.5 | Epoch: 158 | Iter: 240600 | Total Loss: 0.003963 | Recon Loss: 0.003378 | Commit Loss: 0.001169 | Perplexity: 2024.205873
2025-09-27 23:19:46,464 Stage: Train 0.5 | Epoch: 158 | Iter: 240800 | Total Loss: 0.003991 | Recon Loss: 0.003413 | Commit Loss: 0.001155 | Perplexity: 2013.250454
2025-09-27 23:20:13,071 Stage: Train 0.5 | Epoch: 158 | Iter: 241000 | Total Loss: 0.003872 | Recon Loss: 0.003291 | Commit Loss: 0.001163 | Perplexity: 2021.420079
2025-09-27 23:20:39,653 Stage: Train 0.5 | Epoch: 158 | Iter: 241200 | Total Loss: 0.003958 | Recon Loss: 0.003378 | Commit Loss: 0.001160 | Perplexity: 2016.580559
2025-09-27 23:21:06,343 Stage: Train 0.5 | Epoch: 158 | Iter: 241400 | Total Loss: 0.003858 | Recon Loss: 0.003279 | Commit Loss: 0.001159 | Perplexity: 2018.366162
Trainning Epoch:  48%|████▊     | 159/330 [16:32:27<9:36:46, 202.38s/it]2025-09-27 23:21:32,998 Stage: Train 0.5 | Epoch: 159 | Iter: 241600 | Total Loss: 0.003921 | Recon Loss: 0.003339 | Commit Loss: 0.001163 | Perplexity: 2020.817264
2025-09-27 23:21:59,472 Stage: Train 0.5 | Epoch: 159 | Iter: 241800 | Total Loss: 0.004072 | Recon Loss: 0.003493 | Commit Loss: 0.001158 | Perplexity: 2017.731581
2025-09-27 23:22:25,968 Stage: Train 0.5 | Epoch: 159 | Iter: 242000 | Total Loss: 0.003848 | Recon Loss: 0.003268 | Commit Loss: 0.001159 | Perplexity: 2019.039418
2025-09-27 23:22:52,535 Stage: Train 0.5 | Epoch: 159 | Iter: 242200 | Total Loss: 0.003957 | Recon Loss: 0.003382 | Commit Loss: 0.001150 | Perplexity: 2019.234207
2025-09-27 23:23:18,990 Stage: Train 0.5 | Epoch: 159 | Iter: 242400 | Total Loss: 0.003893 | Recon Loss: 0.003315 | Commit Loss: 0.001157 | Perplexity: 2019.874631
2025-09-27 23:23:45,484 Stage: Train 0.5 | Epoch: 159 | Iter: 242600 | Total Loss: 0.003911 | Recon Loss: 0.003331 | Commit Loss: 0.001159 | Perplexity: 2017.455667
2025-09-27 23:24:12,103 Stage: Train 0.5 | Epoch: 159 | Iter: 242800 | Total Loss: 0.004008 | Recon Loss: 0.003432 | Commit Loss: 0.001151 | Perplexity: 2018.244923
2025-09-27 23:24:38,595 Stage: Train 0.5 | Epoch: 159 | Iter: 243000 | Total Loss: 0.003906 | Recon Loss: 0.003324 | Commit Loss: 0.001166 | Perplexity: 2026.643342
Trainning Epoch:  48%|████▊     | 160/330 [16:35:48<9:32:45, 202.15s/it]2025-09-27 23:25:05,337 Stage: Train 0.5 | Epoch: 160 | Iter: 243200 | Total Loss: 0.004001 | Recon Loss: 0.003424 | Commit Loss: 0.001154 | Perplexity: 2019.156404
2025-09-27 23:25:31,984 Stage: Train 0.5 | Epoch: 160 | Iter: 243400 | Total Loss: 0.003835 | Recon Loss: 0.003257 | Commit Loss: 0.001156 | Perplexity: 2023.330408
2025-09-27 23:25:58,610 Stage: Train 0.5 | Epoch: 160 | Iter: 243600 | Total Loss: 0.003931 | Recon Loss: 0.003355 | Commit Loss: 0.001154 | Perplexity: 2017.589689
2025-09-27 23:26:25,247 Stage: Train 0.5 | Epoch: 160 | Iter: 243800 | Total Loss: 0.003999 | Recon Loss: 0.003417 | Commit Loss: 0.001164 | Perplexity: 2022.121198
2025-09-27 23:26:51,817 Stage: Train 0.5 | Epoch: 160 | Iter: 244000 | Total Loss: 0.004008 | Recon Loss: 0.003433 | Commit Loss: 0.001150 | Perplexity: 2013.586907
2025-09-27 23:27:18,305 Stage: Train 0.5 | Epoch: 160 | Iter: 244200 | Total Loss: 0.003858 | Recon Loss: 0.003277 | Commit Loss: 0.001161 | Perplexity: 2021.209061
2025-09-27 23:27:44,733 Stage: Train 0.5 | Epoch: 160 | Iter: 244400 | Total Loss: 0.003932 | Recon Loss: 0.003350 | Commit Loss: 0.001164 | Perplexity: 2020.702021
Trainning Epoch:  49%|████▉     | 161/330 [16:39:10<9:29:11, 202.08s/it]2025-09-27 23:28:11,466 Stage: Train 0.5 | Epoch: 161 | Iter: 244600 | Total Loss: 0.003884 | Recon Loss: 0.003305 | Commit Loss: 0.001158 | Perplexity: 2012.894675
2025-09-27 23:28:38,060 Stage: Train 0.5 | Epoch: 161 | Iter: 244800 | Total Loss: 0.003908 | Recon Loss: 0.003332 | Commit Loss: 0.001151 | Perplexity: 2018.560932
2025-09-27 23:29:04,716 Stage: Train 0.5 | Epoch: 161 | Iter: 245000 | Total Loss: 0.003888 | Recon Loss: 0.003308 | Commit Loss: 0.001159 | Perplexity: 2021.589470
2025-09-27 23:29:31,275 Stage: Train 0.5 | Epoch: 161 | Iter: 245200 | Total Loss: 0.003982 | Recon Loss: 0.003407 | Commit Loss: 0.001149 | Perplexity: 2020.152951
2025-09-27 23:29:57,982 Stage: Train 0.5 | Epoch: 161 | Iter: 245400 | Total Loss: 0.003889 | Recon Loss: 0.003315 | Commit Loss: 0.001148 | Perplexity: 2015.523357
2025-09-27 23:30:24,625 Stage: Train 0.5 | Epoch: 161 | Iter: 245600 | Total Loss: 0.003960 | Recon Loss: 0.003386 | Commit Loss: 0.001148 | Perplexity: 2016.480826
2025-09-27 23:30:51,311 Stage: Train 0.5 | Epoch: 161 | Iter: 245800 | Total Loss: 0.004254 | Recon Loss: 0.003674 | Commit Loss: 0.001159 | Perplexity: 2018.511488
2025-09-27 23:31:17,956 Stage: Train 0.5 | Epoch: 161 | Iter: 246000 | Total Loss: 0.003846 | Recon Loss: 0.003267 | Commit Loss: 0.001157 | Perplexity: 2021.455460
Trainning Epoch:  49%|████▉     | 162/330 [16:42:33<9:26:08, 202.19s/it]2025-09-27 23:31:44,667 Stage: Train 0.5 | Epoch: 162 | Iter: 246200 | Total Loss: 0.003969 | Recon Loss: 0.003390 | Commit Loss: 0.001159 | Perplexity: 2021.244587
2025-09-27 23:32:11,196 Stage: Train 0.5 | Epoch: 162 | Iter: 246400 | Total Loss: 0.003866 | Recon Loss: 0.003288 | Commit Loss: 0.001157 | Perplexity: 2022.064776
2025-09-27 23:32:37,688 Stage: Train 0.5 | Epoch: 162 | Iter: 246600 | Total Loss: 0.003917 | Recon Loss: 0.003345 | Commit Loss: 0.001143 | Perplexity: 2017.541745
2025-09-27 23:33:04,172 Stage: Train 0.5 | Epoch: 162 | Iter: 246800 | Total Loss: 0.003900 | Recon Loss: 0.003323 | Commit Loss: 0.001153 | Perplexity: 2018.156801
2025-09-27 23:33:30,838 Stage: Train 0.5 | Epoch: 162 | Iter: 247000 | Total Loss: 0.003893 | Recon Loss: 0.003318 | Commit Loss: 0.001150 | Perplexity: 2015.426315
2025-09-27 23:33:57,534 Stage: Train 0.5 | Epoch: 162 | Iter: 247200 | Total Loss: 0.003861 | Recon Loss: 0.003283 | Commit Loss: 0.001156 | Perplexity: 2022.928624
2025-09-27 23:34:24,209 Stage: Train 0.5 | Epoch: 162 | Iter: 247400 | Total Loss: 0.004016 | Recon Loss: 0.003437 | Commit Loss: 0.001157 | Perplexity: 2018.508204
Trainning Epoch:  49%|████▉     | 163/330 [16:45:55<9:22:41, 202.16s/it]2025-09-27 23:34:51,006 Stage: Train 0.5 | Epoch: 163 | Iter: 247600 | Total Loss: 0.003844 | Recon Loss: 0.003265 | Commit Loss: 0.001158 | Perplexity: 2021.516579
2025-09-27 23:35:17,629 Stage: Train 0.5 | Epoch: 163 | Iter: 247800 | Total Loss: 0.003943 | Recon Loss: 0.003367 | Commit Loss: 0.001152 | Perplexity: 2029.118935
2025-09-27 23:35:44,286 Stage: Train 0.5 | Epoch: 163 | Iter: 248000 | Total Loss: 0.003866 | Recon Loss: 0.003291 | Commit Loss: 0.001149 | Perplexity: 2016.571735
2025-09-27 23:36:10,795 Stage: Train 0.5 | Epoch: 163 | Iter: 248200 | Total Loss: 0.003839 | Recon Loss: 0.003267 | Commit Loss: 0.001143 | Perplexity: 2011.942611
2025-09-27 23:36:37,354 Stage: Train 0.5 | Epoch: 163 | Iter: 248400 | Total Loss: 0.003965 | Recon Loss: 0.003390 | Commit Loss: 0.001150 | Perplexity: 2021.965343
2025-09-27 23:37:03,976 Stage: Train 0.5 | Epoch: 163 | Iter: 248600 | Total Loss: 0.003910 | Recon Loss: 0.003332 | Commit Loss: 0.001157 | Perplexity: 2022.154250
2025-09-27 23:37:30,506 Stage: Train 0.5 | Epoch: 163 | Iter: 248800 | Total Loss: 0.003933 | Recon Loss: 0.003360 | Commit Loss: 0.001145 | Perplexity: 2019.196910
2025-09-27 23:37:56,943 Stage: Train 0.5 | Epoch: 163 | Iter: 249000 | Total Loss: 0.003928 | Recon Loss: 0.003353 | Commit Loss: 0.001149 | Perplexity: 2019.541067
Trainning Epoch:  50%|████▉     | 164/330 [16:49:17<9:19:07, 202.09s/it]2025-09-27 23:38:23,685 Stage: Train 0.5 | Epoch: 164 | Iter: 249200 | Total Loss: 0.003910 | Recon Loss: 0.003332 | Commit Loss: 0.001156 | Perplexity: 2018.530747
2025-09-27 23:38:50,209 Stage: Train 0.5 | Epoch: 164 | Iter: 249400 | Total Loss: 0.003908 | Recon Loss: 0.003339 | Commit Loss: 0.001138 | Perplexity: 2014.523475
2025-09-27 23:39:16,827 Stage: Train 0.5 | Epoch: 164 | Iter: 249600 | Total Loss: 0.003949 | Recon Loss: 0.003374 | Commit Loss: 0.001150 | Perplexity: 2015.724869
2025-09-27 23:39:43,299 Stage: Train 0.5 | Epoch: 164 | Iter: 249800 | Total Loss: 0.003949 | Recon Loss: 0.003370 | Commit Loss: 0.001158 | Perplexity: 2023.861877
2025-09-27 23:40:09,883 Stage: Train 0.5 | Epoch: 164 | Iter: 250000 | Total Loss: 0.003852 | Recon Loss: 0.003278 | Commit Loss: 0.001147 | Perplexity: 2018.914045
2025-09-27 23:40:36,347 Stage: Train 0.5 | Epoch: 164 | Iter: 250200 | Total Loss: 0.003928 | Recon Loss: 0.003353 | Commit Loss: 0.001150 | Perplexity: 2021.694446
2025-09-27 23:41:02,964 Stage: Train 0.5 | Epoch: 164 | Iter: 250400 | Total Loss: 0.003902 | Recon Loss: 0.003330 | Commit Loss: 0.001144 | Perplexity: 2018.228155
2025-09-27 23:41:29,469 Stage: Train 0.5 | Epoch: 164 | Iter: 250600 | Total Loss: 0.003915 | Recon Loss: 0.003343 | Commit Loss: 0.001145 | Perplexity: 2018.412863
Trainning Epoch:  50%|█████     | 165/330 [16:52:39<9:15:28, 201.99s/it]2025-09-27 23:41:56,168 Stage: Train 0.5 | Epoch: 165 | Iter: 250800 | Total Loss: 0.003862 | Recon Loss: 0.003289 | Commit Loss: 0.001147 | Perplexity: 2014.875744
2025-09-27 23:42:22,781 Stage: Train 0.5 | Epoch: 165 | Iter: 251000 | Total Loss: 0.003871 | Recon Loss: 0.003298 | Commit Loss: 0.001145 | Perplexity: 2018.828252
2025-09-27 23:42:49,405 Stage: Train 0.5 | Epoch: 165 | Iter: 251200 | Total Loss: 0.003850 | Recon Loss: 0.003277 | Commit Loss: 0.001146 | Perplexity: 2019.360438
2025-09-27 23:43:15,928 Stage: Train 0.5 | Epoch: 165 | Iter: 251400 | Total Loss: 0.003928 | Recon Loss: 0.003354 | Commit Loss: 0.001148 | Perplexity: 2016.482123
2025-09-27 23:43:42,600 Stage: Train 0.5 | Epoch: 165 | Iter: 251600 | Total Loss: 0.003898 | Recon Loss: 0.003326 | Commit Loss: 0.001143 | Perplexity: 2024.309154
2025-09-27 23:44:09,281 Stage: Train 0.5 | Epoch: 165 | Iter: 251800 | Total Loss: 0.003847 | Recon Loss: 0.003273 | Commit Loss: 0.001147 | Perplexity: 2020.945676
2025-09-27 23:44:35,795 Stage: Train 0.5 | Epoch: 165 | Iter: 252000 | Total Loss: 0.003905 | Recon Loss: 0.003329 | Commit Loss: 0.001153 | Perplexity: 2018.476386
Trainning Epoch:  50%|█████     | 166/330 [16:56:01<9:12:16, 202.05s/it]2025-09-27 23:45:02,542 Stage: Train 0.5 | Epoch: 166 | Iter: 252200 | Total Loss: 0.003885 | Recon Loss: 0.003313 | Commit Loss: 0.001144 | Perplexity: 2013.212853
2025-09-27 23:45:29,029 Stage: Train 0.5 | Epoch: 166 | Iter: 252400 | Total Loss: 0.003911 | Recon Loss: 0.003336 | Commit Loss: 0.001150 | Perplexity: 2024.401434
2025-09-27 23:45:55,825 Stage: Train 0.5 | Epoch: 166 | Iter: 252600 | Total Loss: 0.003809 | Recon Loss: 0.003238 | Commit Loss: 0.001142 | Perplexity: 2017.664378
2025-09-27 23:46:22,389 Stage: Train 0.5 | Epoch: 166 | Iter: 252800 | Total Loss: 0.003887 | Recon Loss: 0.003314 | Commit Loss: 0.001145 | Perplexity: 2014.915153
2025-09-27 23:46:49,004 Stage: Train 0.5 | Epoch: 166 | Iter: 253000 | Total Loss: 0.003889 | Recon Loss: 0.003313 | Commit Loss: 0.001152 | Perplexity: 2022.390201
2025-09-27 23:47:15,465 Stage: Train 0.5 | Epoch: 166 | Iter: 253200 | Total Loss: 0.003865 | Recon Loss: 0.003292 | Commit Loss: 0.001145 | Perplexity: 2016.506475
2025-09-27 23:47:42,035 Stage: Train 0.5 | Epoch: 166 | Iter: 253400 | Total Loss: 0.003859 | Recon Loss: 0.003287 | Commit Loss: 0.001144 | Perplexity: 2020.549928
2025-09-27 23:48:08,789 Stage: Train 0.5 | Epoch: 166 | Iter: 253600 | Total Loss: 0.004003 | Recon Loss: 0.003429 | Commit Loss: 0.001147 | Perplexity: 2021.912381
Trainning Epoch:  51%|█████     | 167/330 [16:59:23<9:09:01, 202.10s/it]2025-09-27 23:48:35,610 Stage: Train 0.5 | Epoch: 167 | Iter: 253800 | Total Loss: 0.003823 | Recon Loss: 0.003254 | Commit Loss: 0.001138 | Perplexity: 2013.866506
2025-09-27 23:49:02,281 Stage: Train 0.5 | Epoch: 167 | Iter: 254000 | Total Loss: 0.004104 | Recon Loss: 0.003536 | Commit Loss: 0.001136 | Perplexity: 2012.889982
2025-09-27 23:49:28,846 Stage: Train 0.5 | Epoch: 167 | Iter: 254200 | Total Loss: 0.003778 | Recon Loss: 0.003205 | Commit Loss: 0.001147 | Perplexity: 2022.898434
2025-09-27 23:49:55,363 Stage: Train 0.5 | Epoch: 167 | Iter: 254400 | Total Loss: 0.003880 | Recon Loss: 0.003311 | Commit Loss: 0.001138 | Perplexity: 2015.329653
2025-09-27 23:50:21,839 Stage: Train 0.5 | Epoch: 167 | Iter: 254600 | Total Loss: 0.003845 | Recon Loss: 0.003266 | Commit Loss: 0.001156 | Perplexity: 2018.019518
2025-09-27 23:50:48,462 Stage: Train 0.5 | Epoch: 167 | Iter: 254800 | Total Loss: 0.003933 | Recon Loss: 0.003358 | Commit Loss: 0.001150 | Perplexity: 2022.705790
2025-09-27 23:51:15,114 Stage: Train 0.5 | Epoch: 167 | Iter: 255000 | Total Loss: 0.003797 | Recon Loss: 0.003224 | Commit Loss: 0.001146 | Perplexity: 2018.786354
Trainning Epoch:  51%|█████     | 168/330 [17:02:45<9:05:36, 202.08s/it]2025-09-27 23:51:41,778 Stage: Train 0.5 | Epoch: 168 | Iter: 255200 | Total Loss: 0.003873 | Recon Loss: 0.003303 | Commit Loss: 0.001140 | Perplexity: 2017.573646
2025-09-27 23:52:08,280 Stage: Train 0.5 | Epoch: 168 | Iter: 255400 | Total Loss: 0.003863 | Recon Loss: 0.003292 | Commit Loss: 0.001142 | Perplexity: 2023.027911
2025-09-27 23:52:34,877 Stage: Train 0.5 | Epoch: 168 | Iter: 255600 | Total Loss: 0.003948 | Recon Loss: 0.003371 | Commit Loss: 0.001155 | Perplexity: 2022.703500
2025-09-27 23:53:01,564 Stage: Train 0.5 | Epoch: 168 | Iter: 255800 | Total Loss: 0.003895 | Recon Loss: 0.003325 | Commit Loss: 0.001140 | Perplexity: 2017.682039
2025-09-27 23:53:28,139 Stage: Train 0.5 | Epoch: 168 | Iter: 256000 | Total Loss: 0.003937 | Recon Loss: 0.003371 | Commit Loss: 0.001133 | Perplexity: 2016.994481
2025-09-27 23:53:54,500 Stage: Train 0.5 | Epoch: 168 | Iter: 256200 | Total Loss: 0.003817 | Recon Loss: 0.003249 | Commit Loss: 0.001136 | Perplexity: 2019.030113
2025-09-27 23:54:20,967 Stage: Train 0.5 | Epoch: 168 | Iter: 256400 | Total Loss: 0.003818 | Recon Loss: 0.003241 | Commit Loss: 0.001154 | Perplexity: 2019.669341
2025-09-27 23:54:47,658 Stage: Train 0.5 | Epoch: 168 | Iter: 256600 | Total Loss: 0.003998 | Recon Loss: 0.003428 | Commit Loss: 0.001140 | Perplexity: 2017.875822
Trainning Epoch:  51%|█████     | 169/330 [17:06:07<9:02:08, 202.04s/it]2025-09-27 23:55:14,433 Stage: Train 0.5 | Epoch: 169 | Iter: 256800 | Total Loss: 0.003802 | Recon Loss: 0.003231 | Commit Loss: 0.001143 | Perplexity: 2020.098466
2025-09-27 23:55:41,089 Stage: Train 0.5 | Epoch: 169 | Iter: 257000 | Total Loss: 0.003912 | Recon Loss: 0.003341 | Commit Loss: 0.001141 | Perplexity: 2018.027125
2025-09-27 23:56:07,689 Stage: Train 0.5 | Epoch: 169 | Iter: 257200 | Total Loss: 0.003917 | Recon Loss: 0.003346 | Commit Loss: 0.001141 | Perplexity: 2021.127216
2025-09-27 23:56:34,366 Stage: Train 0.5 | Epoch: 169 | Iter: 257400 | Total Loss: 0.003796 | Recon Loss: 0.003228 | Commit Loss: 0.001137 | Perplexity: 2019.943635
2025-09-27 23:57:00,967 Stage: Train 0.5 | Epoch: 169 | Iter: 257600 | Total Loss: 0.003832 | Recon Loss: 0.003260 | Commit Loss: 0.001143 | Perplexity: 2020.467784
2025-09-27 23:57:27,554 Stage: Train 0.5 | Epoch: 169 | Iter: 257800 | Total Loss: 0.003848 | Recon Loss: 0.003276 | Commit Loss: 0.001143 | Perplexity: 2022.053990
2025-09-27 23:57:54,123 Stage: Train 0.5 | Epoch: 169 | Iter: 258000 | Total Loss: 0.003877 | Recon Loss: 0.003308 | Commit Loss: 0.001137 | Perplexity: 2022.789357
2025-09-27 23:58:20,569 Stage: Train 0.5 | Epoch: 169 | Iter: 258200 | Total Loss: 0.003829 | Recon Loss: 0.003254 | Commit Loss: 0.001150 | Perplexity: 2021.793909
Trainning Epoch:  52%|█████▏    | 170/330 [17:09:29<8:58:50, 202.06s/it]2025-09-27 23:58:47,439 Stage: Train 0.5 | Epoch: 170 | Iter: 258400 | Total Loss: 0.003798 | Recon Loss: 0.003227 | Commit Loss: 0.001142 | Perplexity: 2017.517584
2025-09-27 23:59:13,937 Stage: Train 0.5 | Epoch: 170 | Iter: 258600 | Total Loss: 0.003814 | Recon Loss: 0.003242 | Commit Loss: 0.001144 | Perplexity: 2023.585981
2025-09-27 23:59:40,544 Stage: Train 0.5 | Epoch: 170 | Iter: 258800 | Total Loss: 0.003829 | Recon Loss: 0.003260 | Commit Loss: 0.001138 | Perplexity: 2021.072169
2025-09-28 00:00:07,150 Stage: Train 0.5 | Epoch: 170 | Iter: 259000 | Total Loss: 0.003820 | Recon Loss: 0.003250 | Commit Loss: 0.001141 | Perplexity: 2022.622852
2025-09-28 00:00:33,711 Stage: Train 0.5 | Epoch: 170 | Iter: 259200 | Total Loss: 0.003845 | Recon Loss: 0.003277 | Commit Loss: 0.001135 | Perplexity: 2019.226641
2025-09-28 00:01:00,194 Stage: Train 0.5 | Epoch: 170 | Iter: 259400 | Total Loss: 0.003857 | Recon Loss: 0.003287 | Commit Loss: 0.001140 | Perplexity: 2021.207293
2025-09-28 00:01:26,695 Stage: Train 0.5 | Epoch: 170 | Iter: 259600 | Total Loss: 0.003797 | Recon Loss: 0.003225 | Commit Loss: 0.001145 | Perplexity: 2024.522714
Trainning Epoch:  52%|█████▏    | 171/330 [17:12:51<8:55:12, 201.96s/it]2025-09-28 00:01:53,314 Stage: Train 0.5 | Epoch: 171 | Iter: 259800 | Total Loss: 0.003892 | Recon Loss: 0.003324 | Commit Loss: 0.001136 | Perplexity: 2017.218704
2025-09-28 00:02:19,868 Stage: Train 0.5 | Epoch: 171 | Iter: 260000 | Total Loss: 0.003851 | Recon Loss: 0.003282 | Commit Loss: 0.001137 | Perplexity: 2023.480989
2025-09-28 00:02:19,868 Saving model at iteration 260000
2025-09-28 00:02:20,518 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000
2025-09-28 00:02:21,014 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/model.safetensors
2025-09-28 00:02:21,585 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/optimizer.bin
2025-09-28 00:02:21,586 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/scheduler.bin
2025-09-28 00:02:21,586 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/sampler.bin
2025-09-28 00:02:21,587 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/random_states_0.pkl
2025-09-28 00:02:48,108 Stage: Train 0.5 | Epoch: 171 | Iter: 260200 | Total Loss: 0.003807 | Recon Loss: 0.003238 | Commit Loss: 0.001139 | Perplexity: 2024.815873
2025-09-28 00:03:14,740 Stage: Train 0.5 | Epoch: 171 | Iter: 260400 | Total Loss: 0.003846 | Recon Loss: 0.003275 | Commit Loss: 0.001142 | Perplexity: 2020.384697
2025-09-28 00:03:41,273 Stage: Train 0.5 | Epoch: 171 | Iter: 260600 | Total Loss: 0.003860 | Recon Loss: 0.003295 | Commit Loss: 0.001130 | Perplexity: 2016.914250
2025-09-28 00:04:07,951 Stage: Train 0.5 | Epoch: 171 | Iter: 260800 | Total Loss: 0.003853 | Recon Loss: 0.003283 | Commit Loss: 0.001141 | Perplexity: 2022.705378
2025-09-28 00:04:34,562 Stage: Train 0.5 | Epoch: 171 | Iter: 261000 | Total Loss: 0.003829 | Recon Loss: 0.003262 | Commit Loss: 0.001133 | Perplexity: 2018.111521
2025-09-28 00:05:01,273 Stage: Train 0.5 | Epoch: 171 | Iter: 261200 | Total Loss: 0.003841 | Recon Loss: 0.003267 | Commit Loss: 0.001149 | Perplexity: 2023.457465
Trainning Epoch:  52%|█████▏    | 172/330 [17:16:15<8:53:31, 202.61s/it]2025-09-28 00:05:28,199 Stage: Train 0.5 | Epoch: 172 | Iter: 261400 | Total Loss: 0.004021 | Recon Loss: 0.003447 | Commit Loss: 0.001148 | Perplexity: 2020.272977
2025-09-28 00:05:54,906 Stage: Train 0.5 | Epoch: 172 | Iter: 261600 | Total Loss: 0.003803 | Recon Loss: 0.003235 | Commit Loss: 0.001137 | Perplexity: 2019.576975
2025-09-28 00:06:21,516 Stage: Train 0.5 | Epoch: 172 | Iter: 261800 | Total Loss: 0.003811 | Recon Loss: 0.003246 | Commit Loss: 0.001131 | Perplexity: 2026.794315
2025-09-28 00:06:48,169 Stage: Train 0.5 | Epoch: 172 | Iter: 262000 | Total Loss: 0.003808 | Recon Loss: 0.003239 | Commit Loss: 0.001138 | Perplexity: 2028.335626
2025-09-28 00:07:14,894 Stage: Train 0.5 | Epoch: 172 | Iter: 262200 | Total Loss: 0.004000 | Recon Loss: 0.003433 | Commit Loss: 0.001134 | Perplexity: 2019.840984
2025-09-28 00:07:41,607 Stage: Train 0.5 | Epoch: 172 | Iter: 262400 | Total Loss: 0.003821 | Recon Loss: 0.003255 | Commit Loss: 0.001131 | Perplexity: 2020.691853
2025-09-28 00:08:08,163 Stage: Train 0.5 | Epoch: 172 | Iter: 262600 | Total Loss: 0.004085 | Recon Loss: 0.003513 | Commit Loss: 0.001144 | Perplexity: 2028.305347
Trainning Epoch:  52%|█████▏    | 173/330 [17:19:38<8:50:11, 202.62s/it]2025-09-28 00:08:35,006 Stage: Train 0.5 | Epoch: 173 | Iter: 262800 | Total Loss: 0.003775 | Recon Loss: 0.003210 | Commit Loss: 0.001130 | Perplexity: 2020.021964
2025-09-28 00:09:01,491 Stage: Train 0.5 | Epoch: 173 | Iter: 263000 | Total Loss: 0.003799 | Recon Loss: 0.003234 | Commit Loss: 0.001130 | Perplexity: 2022.311557
2025-09-28 00:09:28,106 Stage: Train 0.5 | Epoch: 173 | Iter: 263200 | Total Loss: 0.003828 | Recon Loss: 0.003260 | Commit Loss: 0.001136 | Perplexity: 2020.960980
2025-09-28 00:09:54,753 Stage: Train 0.5 | Epoch: 173 | Iter: 263400 | Total Loss: 0.004194 | Recon Loss: 0.003629 | Commit Loss: 0.001129 | Perplexity: 2019.840989
2025-09-28 00:10:21,323 Stage: Train 0.5 | Epoch: 173 | Iter: 263600 | Total Loss: 0.003689 | Recon Loss: 0.003125 | Commit Loss: 0.001128 | Perplexity: 2024.502485
2025-09-28 00:10:47,935 Stage: Train 0.5 | Epoch: 173 | Iter: 263800 | Total Loss: 0.003778 | Recon Loss: 0.003212 | Commit Loss: 0.001133 | Perplexity: 2024.167992
2025-09-28 00:11:14,473 Stage: Train 0.5 | Epoch: 173 | Iter: 264000 | Total Loss: 0.003829 | Recon Loss: 0.003260 | Commit Loss: 0.001138 | Perplexity: 2025.634575
2025-09-28 00:11:41,115 Stage: Train 0.5 | Epoch: 173 | Iter: 264200 | Total Loss: 0.003833 | Recon Loss: 0.003268 | Commit Loss: 0.001130 | Perplexity: 2021.492984
Trainning Epoch:  53%|█████▎    | 174/330 [17:23:00<8:46:30, 202.50s/it]2025-09-28 00:12:08,075 Stage: Train 0.5 | Epoch: 174 | Iter: 264400 | Total Loss: 0.003902 | Recon Loss: 0.003335 | Commit Loss: 0.001134 | Perplexity: 2023.657404
2025-09-28 00:12:34,593 Stage: Train 0.5 | Epoch: 174 | Iter: 264600 | Total Loss: 0.003827 | Recon Loss: 0.003264 | Commit Loss: 0.001126 | Perplexity: 2020.148123
2025-09-28 00:13:01,257 Stage: Train 0.5 | Epoch: 174 | Iter: 264800 | Total Loss: 0.003856 | Recon Loss: 0.003289 | Commit Loss: 0.001134 | Perplexity: 2021.168911
2025-09-28 00:13:27,897 Stage: Train 0.5 | Epoch: 174 | Iter: 265000 | Total Loss: 0.003790 | Recon Loss: 0.003225 | Commit Loss: 0.001129 | Perplexity: 2026.618125
2025-09-28 00:13:54,430 Stage: Train 0.5 | Epoch: 174 | Iter: 265200 | Total Loss: 0.003840 | Recon Loss: 0.003275 | Commit Loss: 0.001129 | Perplexity: 2025.679426
2025-09-28 00:14:21,053 Stage: Train 0.5 | Epoch: 174 | Iter: 265400 | Total Loss: 0.003850 | Recon Loss: 0.003284 | Commit Loss: 0.001132 | Perplexity: 2025.054620
2025-09-28 00:14:47,800 Stage: Train 0.5 | Epoch: 174 | Iter: 265600 | Total Loss: 0.003839 | Recon Loss: 0.003278 | Commit Loss: 0.001122 | Perplexity: 2019.583555
2025-09-28 00:15:14,399 Stage: Train 0.5 | Epoch: 174 | Iter: 265800 | Total Loss: 0.003845 | Recon Loss: 0.003279 | Commit Loss: 0.001132 | Perplexity: 2022.640555
Trainning Epoch:  53%|█████▎    | 175/330 [17:26:22<8:43:05, 202.49s/it]2025-09-28 00:15:41,291 Stage: Train 0.5 | Epoch: 175 | Iter: 266000 | Total Loss: 0.003777 | Recon Loss: 0.003211 | Commit Loss: 0.001132 | Perplexity: 2022.800775
2025-09-28 00:16:07,915 Stage: Train 0.5 | Epoch: 175 | Iter: 266200 | Total Loss: 0.003841 | Recon Loss: 0.003277 | Commit Loss: 0.001129 | Perplexity: 2029.438386
2025-09-28 00:16:34,586 Stage: Train 0.5 | Epoch: 175 | Iter: 266400 | Total Loss: 0.003754 | Recon Loss: 0.003193 | Commit Loss: 0.001123 | Perplexity: 2020.707151
2025-09-28 00:17:01,101 Stage: Train 0.5 | Epoch: 175 | Iter: 266600 | Total Loss: 0.003804 | Recon Loss: 0.003241 | Commit Loss: 0.001126 | Perplexity: 2021.567391
2025-09-28 00:17:27,771 Stage: Train 0.5 | Epoch: 175 | Iter: 266800 | Total Loss: 0.003811 | Recon Loss: 0.003250 | Commit Loss: 0.001121 | Perplexity: 2020.387792
2025-09-28 00:17:54,319 Stage: Train 0.5 | Epoch: 175 | Iter: 267000 | Total Loss: 0.003846 | Recon Loss: 0.003279 | Commit Loss: 0.001133 | Perplexity: 2025.547966
2025-09-28 00:18:20,959 Stage: Train 0.5 | Epoch: 175 | Iter: 267200 | Total Loss: 0.003817 | Recon Loss: 0.003247 | Commit Loss: 0.001139 | Perplexity: 2029.031500
Trainning Epoch:  53%|█████▎    | 176/330 [17:29:45<8:39:38, 202.46s/it]2025-09-28 00:18:47,803 Stage: Train 0.5 | Epoch: 176 | Iter: 267400 | Total Loss: 0.003899 | Recon Loss: 0.003336 | Commit Loss: 0.001126 | Perplexity: 2019.384423
2025-09-28 00:19:14,415 Stage: Train 0.5 | Epoch: 176 | Iter: 267600 | Total Loss: 0.003898 | Recon Loss: 0.003338 | Commit Loss: 0.001119 | Perplexity: 2015.128666
2025-09-28 00:19:40,973 Stage: Train 0.5 | Epoch: 176 | Iter: 267800 | Total Loss: 0.003714 | Recon Loss: 0.003152 | Commit Loss: 0.001124 | Perplexity: 2024.181443
2025-09-28 00:20:07,519 Stage: Train 0.5 | Epoch: 176 | Iter: 268000 | Total Loss: 0.003791 | Recon Loss: 0.003226 | Commit Loss: 0.001131 | Perplexity: 2023.052432
2025-09-28 00:20:34,134 Stage: Train 0.5 | Epoch: 176 | Iter: 268200 | Total Loss: 0.003784 | Recon Loss: 0.003216 | Commit Loss: 0.001135 | Perplexity: 2026.380906
2025-09-28 00:21:00,676 Stage: Train 0.5 | Epoch: 176 | Iter: 268400 | Total Loss: 0.003898 | Recon Loss: 0.003333 | Commit Loss: 0.001130 | Perplexity: 2021.876249
2025-09-28 00:21:27,256 Stage: Train 0.5 | Epoch: 176 | Iter: 268600 | Total Loss: 0.003834 | Recon Loss: 0.003268 | Commit Loss: 0.001131 | Perplexity: 2025.435914
2025-09-28 00:21:53,919 Stage: Train 0.5 | Epoch: 176 | Iter: 268800 | Total Loss: 0.003730 | Recon Loss: 0.003167 | Commit Loss: 0.001125 | Perplexity: 2022.943798
Trainning Epoch:  54%|█████▎    | 177/330 [17:33:07<8:36:00, 202.36s/it]2025-09-28 00:22:20,600 Stage: Train 0.5 | Epoch: 177 | Iter: 269000 | Total Loss: 0.003900 | Recon Loss: 0.003344 | Commit Loss: 0.001111 | Perplexity: 2017.261914
2025-09-28 00:22:47,137 Stage: Train 0.5 | Epoch: 177 | Iter: 269200 | Total Loss: 0.003774 | Recon Loss: 0.003212 | Commit Loss: 0.001125 | Perplexity: 2026.892108
2025-09-28 00:23:13,711 Stage: Train 0.5 | Epoch: 177 | Iter: 269400 | Total Loss: 0.003748 | Recon Loss: 0.003187 | Commit Loss: 0.001120 | Perplexity: 2027.317910
2025-09-28 00:23:40,392 Stage: Train 0.5 | Epoch: 177 | Iter: 269600 | Total Loss: 0.003857 | Recon Loss: 0.003291 | Commit Loss: 0.001133 | Perplexity: 2024.521427
2025-09-28 00:24:06,882 Stage: Train 0.5 | Epoch: 177 | Iter: 269800 | Total Loss: 0.003830 | Recon Loss: 0.003265 | Commit Loss: 0.001130 | Perplexity: 2028.269479
2025-09-28 00:24:33,489 Stage: Train 0.5 | Epoch: 177 | Iter: 270000 | Total Loss: 0.003818 | Recon Loss: 0.003250 | Commit Loss: 0.001137 | Perplexity: 2022.607237
2025-09-28 00:24:59,964 Stage: Train 0.5 | Epoch: 177 | Iter: 270200 | Total Loss: 0.003787 | Recon Loss: 0.003225 | Commit Loss: 0.001124 | Perplexity: 2020.644323
Trainning Epoch:  54%|█████▍    | 178/330 [17:36:29<8:32:12, 202.19s/it]2025-09-28 00:25:26,667 Stage: Train 0.5 | Epoch: 178 | Iter: 270400 | Total Loss: 0.003849 | Recon Loss: 0.003284 | Commit Loss: 0.001131 | Perplexity: 2017.150012
2025-09-28 00:25:53,210 Stage: Train 0.5 | Epoch: 178 | Iter: 270600 | Total Loss: 0.003761 | Recon Loss: 0.003199 | Commit Loss: 0.001124 | Perplexity: 2025.910241
2025-09-28 00:26:19,818 Stage: Train 0.5 | Epoch: 178 | Iter: 270800 | Total Loss: 0.003859 | Recon Loss: 0.003296 | Commit Loss: 0.001126 | Perplexity: 2023.239691
2025-09-28 00:26:46,262 Stage: Train 0.5 | Epoch: 178 | Iter: 271000 | Total Loss: 0.003743 | Recon Loss: 0.003182 | Commit Loss: 0.001123 | Perplexity: 2021.619808
2025-09-28 00:27:12,752 Stage: Train 0.5 | Epoch: 178 | Iter: 271200 | Total Loss: 0.003920 | Recon Loss: 0.003355 | Commit Loss: 0.001130 | Perplexity: 2021.561905
2025-09-28 00:27:39,296 Stage: Train 0.5 | Epoch: 178 | Iter: 271400 | Total Loss: 0.003719 | Recon Loss: 0.003162 | Commit Loss: 0.001114 | Perplexity: 2025.360308
2025-09-28 00:28:05,898 Stage: Train 0.5 | Epoch: 178 | Iter: 271600 | Total Loss: 0.003833 | Recon Loss: 0.003269 | Commit Loss: 0.001128 | Perplexity: 2025.850866
2025-09-28 00:28:32,603 Stage: Train 0.5 | Epoch: 178 | Iter: 271800 | Total Loss: 0.003737 | Recon Loss: 0.003172 | Commit Loss: 0.001129 | Perplexity: 2025.620081
Trainning Epoch:  54%|█████▍    | 179/330 [17:39:51<8:28:42, 202.14s/it]2025-09-28 00:28:59,419 Stage: Train 0.5 | Epoch: 179 | Iter: 272000 | Total Loss: 0.003820 | Recon Loss: 0.003261 | Commit Loss: 0.001118 | Perplexity: 2020.672863
2025-09-28 00:29:26,015 Stage: Train 0.5 | Epoch: 179 | Iter: 272200 | Total Loss: 0.003740 | Recon Loss: 0.003180 | Commit Loss: 0.001120 | Perplexity: 2019.899006
2025-09-28 00:29:52,537 Stage: Train 0.5 | Epoch: 179 | Iter: 272400 | Total Loss: 0.003710 | Recon Loss: 0.003152 | Commit Loss: 0.001118 | Perplexity: 2019.611969
2025-09-28 00:30:19,098 Stage: Train 0.5 | Epoch: 179 | Iter: 272600 | Total Loss: 0.003788 | Recon Loss: 0.003223 | Commit Loss: 0.001129 | Perplexity: 2032.866140
2025-09-28 00:30:45,854 Stage: Train 0.5 | Epoch: 179 | Iter: 272800 | Total Loss: 0.003827 | Recon Loss: 0.003261 | Commit Loss: 0.001131 | Perplexity: 2030.419722
2025-09-28 00:31:12,440 Stage: Train 0.5 | Epoch: 179 | Iter: 273000 | Total Loss: 0.003795 | Recon Loss: 0.003233 | Commit Loss: 0.001125 | Perplexity: 2030.551243
2025-09-28 00:31:39,000 Stage: Train 0.5 | Epoch: 179 | Iter: 273200 | Total Loss: 0.003882 | Recon Loss: 0.003319 | Commit Loss: 0.001126 | Perplexity: 2023.149060
2025-09-28 00:32:05,650 Stage: Train 0.5 | Epoch: 179 | Iter: 273400 | Total Loss: 0.003823 | Recon Loss: 0.003261 | Commit Loss: 0.001123 | Perplexity: 2023.341168
Trainning Epoch:  55%|█████▍    | 180/330 [17:43:13<8:25:24, 202.16s/it]2025-09-28 00:32:32,355 Stage: Train 0.5 | Epoch: 180 | Iter: 273600 | Total Loss: 0.003704 | Recon Loss: 0.003143 | Commit Loss: 0.001123 | Perplexity: 2028.522234
2025-09-28 00:32:58,910 Stage: Train 0.5 | Epoch: 180 | Iter: 273800 | Total Loss: 0.003797 | Recon Loss: 0.003238 | Commit Loss: 0.001117 | Perplexity: 2024.786334
2025-09-28 00:33:25,533 Stage: Train 0.5 | Epoch: 180 | Iter: 274000 | Total Loss: 0.003835 | Recon Loss: 0.003276 | Commit Loss: 0.001119 | Perplexity: 2019.291275
2025-09-28 00:33:52,181 Stage: Train 0.5 | Epoch: 180 | Iter: 274200 | Total Loss: 0.003709 | Recon Loss: 0.003150 | Commit Loss: 0.001119 | Perplexity: 2026.710856
2025-09-28 00:34:18,736 Stage: Train 0.5 | Epoch: 180 | Iter: 274400 | Total Loss: 0.003814 | Recon Loss: 0.003254 | Commit Loss: 0.001121 | Perplexity: 2020.449424
2025-09-28 00:34:45,329 Stage: Train 0.5 | Epoch: 180 | Iter: 274600 | Total Loss: 0.003726 | Recon Loss: 0.003163 | Commit Loss: 0.001125 | Perplexity: 2026.091591
2025-09-28 00:35:12,001 Stage: Train 0.5 | Epoch: 180 | Iter: 274800 | Total Loss: 0.003819 | Recon Loss: 0.003255 | Commit Loss: 0.001127 | Perplexity: 2025.350696
Trainning Epoch:  55%|█████▍    | 181/330 [17:46:35<8:21:59, 202.15s/it]2025-09-28 00:35:38,684 Stage: Train 0.5 | Epoch: 181 | Iter: 275000 | Total Loss: 0.003795 | Recon Loss: 0.003239 | Commit Loss: 0.001112 | Perplexity: 2014.139523
2025-09-28 00:36:05,330 Stage: Train 0.5 | Epoch: 181 | Iter: 275200 | Total Loss: 0.003765 | Recon Loss: 0.003205 | Commit Loss: 0.001120 | Perplexity: 2021.714402
2025-09-28 00:36:31,796 Stage: Train 0.5 | Epoch: 181 | Iter: 275400 | Total Loss: 0.003739 | Recon Loss: 0.003181 | Commit Loss: 0.001115 | Perplexity: 2027.294114
2025-09-28 00:36:58,395 Stage: Train 0.5 | Epoch: 181 | Iter: 275600 | Total Loss: 0.003784 | Recon Loss: 0.003226 | Commit Loss: 0.001116 | Perplexity: 2019.977131
2025-09-28 00:37:24,911 Stage: Train 0.5 | Epoch: 181 | Iter: 275800 | Total Loss: 0.003774 | Recon Loss: 0.003212 | Commit Loss: 0.001123 | Perplexity: 2030.450056
2025-09-28 00:37:51,408 Stage: Train 0.5 | Epoch: 181 | Iter: 276000 | Total Loss: 0.003800 | Recon Loss: 0.003243 | Commit Loss: 0.001112 | Perplexity: 2020.686404
2025-09-28 00:38:18,070 Stage: Train 0.5 | Epoch: 181 | Iter: 276200 | Total Loss: 0.003886 | Recon Loss: 0.003326 | Commit Loss: 0.001121 | Perplexity: 2017.959182
2025-09-28 00:38:44,574 Stage: Train 0.5 | Epoch: 181 | Iter: 276400 | Total Loss: 0.003776 | Recon Loss: 0.003218 | Commit Loss: 0.001115 | Perplexity: 2025.332570
Trainning Epoch:  55%|█████▌    | 182/330 [17:49:57<8:18:25, 202.06s/it]2025-09-28 00:39:11,415 Stage: Train 0.5 | Epoch: 182 | Iter: 276600 | Total Loss: 0.003829 | Recon Loss: 0.003272 | Commit Loss: 0.001113 | Perplexity: 2024.983108
2025-09-28 00:39:38,040 Stage: Train 0.5 | Epoch: 182 | Iter: 276800 | Total Loss: 0.003727 | Recon Loss: 0.003171 | Commit Loss: 0.001112 | Perplexity: 2023.097812
2025-09-28 00:40:04,829 Stage: Train 0.5 | Epoch: 182 | Iter: 277000 | Total Loss: 0.003720 | Recon Loss: 0.003162 | Commit Loss: 0.001117 | Perplexity: 2025.437651
2025-09-28 00:40:31,433 Stage: Train 0.5 | Epoch: 182 | Iter: 277200 | Total Loss: 0.003827 | Recon Loss: 0.003269 | Commit Loss: 0.001117 | Perplexity: 2028.309161
2025-09-28 00:40:58,132 Stage: Train 0.5 | Epoch: 182 | Iter: 277400 | Total Loss: 0.003793 | Recon Loss: 0.003237 | Commit Loss: 0.001111 | Perplexity: 2021.853163
2025-09-28 00:41:24,891 Stage: Train 0.5 | Epoch: 182 | Iter: 277600 | Total Loss: 0.003758 | Recon Loss: 0.003197 | Commit Loss: 0.001121 | Perplexity: 2026.042997
2025-09-28 00:41:51,582 Stage: Train 0.5 | Epoch: 182 | Iter: 277800 | Total Loss: 0.003751 | Recon Loss: 0.003189 | Commit Loss: 0.001124 | Perplexity: 2027.007582
Trainning Epoch:  55%|█████▌    | 183/330 [17:53:20<8:15:43, 202.33s/it]2025-09-28 00:42:18,539 Stage: Train 0.5 | Epoch: 183 | Iter: 278000 | Total Loss: 0.003905 | Recon Loss: 0.003345 | Commit Loss: 0.001120 | Perplexity: 2024.050724
2025-09-28 00:42:45,078 Stage: Train 0.5 | Epoch: 183 | Iter: 278200 | Total Loss: 0.003628 | Recon Loss: 0.003066 | Commit Loss: 0.001123 | Perplexity: 2026.892917
2025-09-28 00:43:11,785 Stage: Train 0.5 | Epoch: 183 | Iter: 278400 | Total Loss: 0.003735 | Recon Loss: 0.003179 | Commit Loss: 0.001112 | Perplexity: 2024.206993
2025-09-28 00:43:38,425 Stage: Train 0.5 | Epoch: 183 | Iter: 278600 | Total Loss: 0.003750 | Recon Loss: 0.003192 | Commit Loss: 0.001115 | Perplexity: 2025.503546
2025-09-28 00:44:05,186 Stage: Train 0.5 | Epoch: 183 | Iter: 278800 | Total Loss: 0.003798 | Recon Loss: 0.003240 | Commit Loss: 0.001115 | Perplexity: 2026.902278
2025-09-28 00:44:31,766 Stage: Train 0.5 | Epoch: 183 | Iter: 279000 | Total Loss: 0.003720 | Recon Loss: 0.003165 | Commit Loss: 0.001109 | Perplexity: 2022.957990
2025-09-28 00:44:58,322 Stage: Train 0.5 | Epoch: 183 | Iter: 279200 | Total Loss: 0.003817 | Recon Loss: 0.003263 | Commit Loss: 0.001109 | Perplexity: 2024.395602
2025-09-28 00:45:24,994 Stage: Train 0.5 | Epoch: 183 | Iter: 279400 | Total Loss: 0.003749 | Recon Loss: 0.003191 | Commit Loss: 0.001116 | Perplexity: 2029.079703
Trainning Epoch:  56%|█████▌    | 184/330 [17:56:42<8:12:28, 202.39s/it]2025-09-28 00:45:51,847 Stage: Train 0.5 | Epoch: 184 | Iter: 279600 | Total Loss: 0.003768 | Recon Loss: 0.003211 | Commit Loss: 0.001114 | Perplexity: 2027.252648
2025-09-28 00:46:18,531 Stage: Train 0.5 | Epoch: 184 | Iter: 279800 | Total Loss: 0.003796 | Recon Loss: 0.003240 | Commit Loss: 0.001112 | Perplexity: 2026.392408
2025-09-28 00:46:45,172 Stage: Train 0.5 | Epoch: 184 | Iter: 280000 | Total Loss: 0.003677 | Recon Loss: 0.003121 | Commit Loss: 0.001111 | Perplexity: 2023.602522
2025-09-28 00:46:45,172 Saving model at iteration 280000
2025-09-28 00:46:45,362 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000
2025-09-28 00:46:45,843 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/model.safetensors
2025-09-28 00:46:46,409 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/optimizer.bin
2025-09-28 00:46:46,409 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/scheduler.bin
2025-09-28 00:46:46,409 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/sampler.bin
2025-09-28 00:46:46,410 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/random_states_0.pkl
2025-09-28 00:47:13,139 Stage: Train 0.5 | Epoch: 184 | Iter: 280200 | Total Loss: 0.003831 | Recon Loss: 0.003276 | Commit Loss: 0.001110 | Perplexity: 2024.833488
2025-09-28 00:47:39,752 Stage: Train 0.5 | Epoch: 184 | Iter: 280400 | Total Loss: 0.003683 | Recon Loss: 0.003130 | Commit Loss: 0.001106 | Perplexity: 2020.679907
2025-09-28 00:48:06,502 Stage: Train 0.5 | Epoch: 184 | Iter: 280600 | Total Loss: 0.003818 | Recon Loss: 0.003265 | Commit Loss: 0.001106 | Perplexity: 2026.373647
2025-09-28 00:48:33,286 Stage: Train 0.5 | Epoch: 184 | Iter: 280800 | Total Loss: 0.003744 | Recon Loss: 0.003190 | Commit Loss: 0.001109 | Perplexity: 2023.428450
2025-09-28 00:49:00,064 Stage: Train 0.5 | Epoch: 184 | Iter: 281000 | Total Loss: 0.003766 | Recon Loss: 0.003209 | Commit Loss: 0.001113 | Perplexity: 2027.100560
Trainning Epoch:  56%|█████▌    | 185/330 [18:00:07<8:10:29, 202.96s/it]2025-09-28 00:49:26,962 Stage: Train 0.5 | Epoch: 185 | Iter: 281200 | Total Loss: 0.003653 | Recon Loss: 0.003097 | Commit Loss: 0.001112 | Perplexity: 2024.802833
2025-09-28 00:49:53,627 Stage: Train 0.5 | Epoch: 185 | Iter: 281400 | Total Loss: 0.003779 | Recon Loss: 0.003229 | Commit Loss: 0.001101 | Perplexity: 2019.957706
2025-09-28 00:50:20,290 Stage: Train 0.5 | Epoch: 185 | Iter: 281600 | Total Loss: 0.003669 | Recon Loss: 0.003111 | Commit Loss: 0.001115 | Perplexity: 2026.946430
2025-09-28 00:50:46,975 Stage: Train 0.5 | Epoch: 185 | Iter: 281800 | Total Loss: 0.003829 | Recon Loss: 0.003271 | Commit Loss: 0.001116 | Perplexity: 2026.053715
2025-09-28 00:51:13,703 Stage: Train 0.5 | Epoch: 185 | Iter: 282000 | Total Loss: 0.003758 | Recon Loss: 0.003209 | Commit Loss: 0.001099 | Perplexity: 2019.610972
2025-09-28 00:51:40,436 Stage: Train 0.5 | Epoch: 185 | Iter: 282200 | Total Loss: 0.003815 | Recon Loss: 0.003258 | Commit Loss: 0.001114 | Perplexity: 2028.799659
2025-09-28 00:52:07,046 Stage: Train 0.5 | Epoch: 185 | Iter: 282400 | Total Loss: 0.003745 | Recon Loss: 0.003185 | Commit Loss: 0.001121 | Perplexity: 2026.980547
Trainning Epoch:  56%|█████▋    | 186/330 [18:03:29<8:07:02, 202.94s/it]2025-09-28 00:52:33,883 Stage: Train 0.5 | Epoch: 186 | Iter: 282600 | Total Loss: 0.003715 | Recon Loss: 0.003161 | Commit Loss: 0.001109 | Perplexity: 2024.779272
2025-09-28 00:53:00,609 Stage: Train 0.5 | Epoch: 186 | Iter: 282800 | Total Loss: 0.003769 | Recon Loss: 0.003217 | Commit Loss: 0.001104 | Perplexity: 2024.684011
2025-09-28 00:53:27,274 Stage: Train 0.5 | Epoch: 186 | Iter: 283000 | Total Loss: 0.003714 | Recon Loss: 0.003159 | Commit Loss: 0.001110 | Perplexity: 2029.690659
2025-09-28 00:53:53,958 Stage: Train 0.5 | Epoch: 186 | Iter: 283200 | Total Loss: 0.003718 | Recon Loss: 0.003162 | Commit Loss: 0.001111 | Perplexity: 2026.740478
2025-09-28 00:54:20,654 Stage: Train 0.5 | Epoch: 186 | Iter: 283400 | Total Loss: 0.003729 | Recon Loss: 0.003175 | Commit Loss: 0.001108 | Perplexity: 2033.865258
2025-09-28 00:54:47,273 Stage: Train 0.5 | Epoch: 186 | Iter: 283600 | Total Loss: 0.003701 | Recon Loss: 0.003143 | Commit Loss: 0.001116 | Perplexity: 2028.000280
2025-09-28 00:55:13,845 Stage: Train 0.5 | Epoch: 186 | Iter: 283800 | Total Loss: 0.003803 | Recon Loss: 0.003249 | Commit Loss: 0.001108 | Perplexity: 2024.790643
2025-09-28 00:55:40,489 Stage: Train 0.5 | Epoch: 186 | Iter: 284000 | Total Loss: 0.003759 | Recon Loss: 0.003203 | Commit Loss: 0.001112 | Perplexity: 2028.261227
Trainning Epoch:  57%|█████▋    | 187/330 [18:06:52<8:03:26, 202.84s/it]2025-09-28 00:56:07,392 Stage: Train 0.5 | Epoch: 187 | Iter: 284200 | Total Loss: 0.003935 | Recon Loss: 0.003383 | Commit Loss: 0.001104 | Perplexity: 2020.327838
2025-09-28 00:56:34,102 Stage: Train 0.5 | Epoch: 187 | Iter: 284400 | Total Loss: 0.003613 | Recon Loss: 0.003057 | Commit Loss: 0.001113 | Perplexity: 2027.716121
2025-09-28 00:57:00,855 Stage: Train 0.5 | Epoch: 187 | Iter: 284600 | Total Loss: 0.003760 | Recon Loss: 0.003206 | Commit Loss: 0.001109 | Perplexity: 2024.841412
2025-09-28 00:57:27,307 Stage: Train 0.5 | Epoch: 187 | Iter: 284800 | Total Loss: 0.003785 | Recon Loss: 0.003230 | Commit Loss: 0.001108 | Perplexity: 2026.488320
2025-09-28 00:57:53,857 Stage: Train 0.5 | Epoch: 187 | Iter: 285000 | Total Loss: 0.003701 | Recon Loss: 0.003148 | Commit Loss: 0.001106 | Perplexity: 2030.068361
2025-09-28 00:58:20,500 Stage: Train 0.5 | Epoch: 187 | Iter: 285200 | Total Loss: 0.003880 | Recon Loss: 0.003325 | Commit Loss: 0.001111 | Perplexity: 2026.777620
2025-09-28 00:58:47,040 Stage: Train 0.5 | Epoch: 187 | Iter: 285400 | Total Loss: 0.003647 | Recon Loss: 0.003096 | Commit Loss: 0.001100 | Perplexity: 2022.590938
Trainning Epoch:  57%|█████▋    | 188/330 [18:10:14<7:59:37, 202.66s/it]2025-09-28 00:59:13,784 Stage: Train 0.5 | Epoch: 188 | Iter: 285600 | Total Loss: 0.003676 | Recon Loss: 0.003120 | Commit Loss: 0.001112 | Perplexity: 2031.077021
2025-09-28 00:59:40,279 Stage: Train 0.5 | Epoch: 188 | Iter: 285800 | Total Loss: 0.004047 | Recon Loss: 0.003498 | Commit Loss: 0.001100 | Perplexity: 2022.168083
2025-09-28 01:00:06,968 Stage: Train 0.5 | Epoch: 188 | Iter: 286000 | Total Loss: 0.003586 | Recon Loss: 0.003034 | Commit Loss: 0.001103 | Perplexity: 2026.334767
2025-09-28 01:00:33,527 Stage: Train 0.5 | Epoch: 188 | Iter: 286200 | Total Loss: 0.003932 | Recon Loss: 0.003377 | Commit Loss: 0.001109 | Perplexity: 2022.940187
2025-09-28 01:01:00,188 Stage: Train 0.5 | Epoch: 188 | Iter: 286400 | Total Loss: 0.003647 | Recon Loss: 0.003095 | Commit Loss: 0.001104 | Perplexity: 2026.520593
2025-09-28 01:01:26,844 Stage: Train 0.5 | Epoch: 188 | Iter: 286600 | Total Loss: 0.003804 | Recon Loss: 0.003251 | Commit Loss: 0.001106 | Perplexity: 2031.170897
2025-09-28 01:01:53,391 Stage: Train 0.5 | Epoch: 188 | Iter: 286800 | Total Loss: 0.003827 | Recon Loss: 0.003273 | Commit Loss: 0.001108 | Perplexity: 2026.273008
2025-09-28 01:02:20,041 Stage: Train 0.5 | Epoch: 188 | Iter: 287000 | Total Loss: 0.003644 | Recon Loss: 0.003093 | Commit Loss: 0.001102 | Perplexity: 2029.727614
Trainning Epoch:  57%|█████▋    | 189/330 [18:13:37<7:56:01, 202.56s/it]2025-09-28 01:02:46,871 Stage: Train 0.5 | Epoch: 189 | Iter: 287200 | Total Loss: 0.003770 | Recon Loss: 0.003220 | Commit Loss: 0.001100 | Perplexity: 2022.052311
2025-09-28 01:03:13,501 Stage: Train 0.5 | Epoch: 189 | Iter: 287400 | Total Loss: 0.003767 | Recon Loss: 0.003217 | Commit Loss: 0.001100 | Perplexity: 2020.824528
2025-09-28 01:03:40,195 Stage: Train 0.5 | Epoch: 189 | Iter: 287600 | Total Loss: 0.003747 | Recon Loss: 0.003198 | Commit Loss: 0.001099 | Perplexity: 2028.070937
2025-09-28 01:04:06,897 Stage: Train 0.5 | Epoch: 189 | Iter: 287800 | Total Loss: 0.003698 | Recon Loss: 0.003143 | Commit Loss: 0.001109 | Perplexity: 2030.207545
2025-09-28 01:04:33,575 Stage: Train 0.5 | Epoch: 189 | Iter: 288000 | Total Loss: 0.003745 | Recon Loss: 0.003196 | Commit Loss: 0.001098 | Perplexity: 2026.403480
2025-09-28 01:05:00,178 Stage: Train 0.5 | Epoch: 189 | Iter: 288200 | Total Loss: 0.003697 | Recon Loss: 0.003144 | Commit Loss: 0.001107 | Perplexity: 2023.682768
2025-09-28 01:05:26,817 Stage: Train 0.5 | Epoch: 189 | Iter: 288400 | Total Loss: 0.003771 | Recon Loss: 0.003219 | Commit Loss: 0.001104 | Perplexity: 2025.025756
2025-09-28 01:05:53,471 Stage: Train 0.5 | Epoch: 189 | Iter: 288600 | Total Loss: 0.003718 | Recon Loss: 0.003168 | Commit Loss: 0.001100 | Perplexity: 2027.199197
Trainning Epoch:  58%|█████▊    | 190/330 [18:16:59<7:52:42, 202.59s/it]2025-09-28 01:06:20,188 Stage: Train 0.5 | Epoch: 190 | Iter: 288800 | Total Loss: 0.003699 | Recon Loss: 0.003147 | Commit Loss: 0.001103 | Perplexity: 2027.474937
2025-09-28 01:06:46,866 Stage: Train 0.5 | Epoch: 190 | Iter: 289000 | Total Loss: 0.003700 | Recon Loss: 0.003149 | Commit Loss: 0.001103 | Perplexity: 2031.313769
2025-09-28 01:07:13,382 Stage: Train 0.5 | Epoch: 190 | Iter: 289200 | Total Loss: 0.003686 | Recon Loss: 0.003136 | Commit Loss: 0.001100 | Perplexity: 2024.867027
2025-09-28 01:07:39,827 Stage: Train 0.5 | Epoch: 190 | Iter: 289400 | Total Loss: 0.003743 | Recon Loss: 0.003192 | Commit Loss: 0.001100 | Perplexity: 2025.864143
2025-09-28 01:08:06,357 Stage: Train 0.5 | Epoch: 190 | Iter: 289600 | Total Loss: 0.003650 | Recon Loss: 0.003093 | Commit Loss: 0.001113 | Perplexity: 2024.143159
2025-09-28 01:08:32,908 Stage: Train 0.5 | Epoch: 190 | Iter: 289800 | Total Loss: 0.003687 | Recon Loss: 0.003131 | Commit Loss: 0.001111 | Perplexity: 2029.343143
2025-09-28 01:08:59,474 Stage: Train 0.5 | Epoch: 190 | Iter: 290000 | Total Loss: 0.003778 | Recon Loss: 0.003228 | Commit Loss: 0.001101 | Perplexity: 2023.086439
Trainning Epoch:  58%|█████▊    | 191/330 [18:20:21<7:48:43, 202.33s/it]2025-09-28 01:09:26,172 Stage: Train 0.5 | Epoch: 191 | Iter: 290200 | Total Loss: 0.003744 | Recon Loss: 0.003194 | Commit Loss: 0.001101 | Perplexity: 2026.222330
2025-09-28 01:09:52,814 Stage: Train 0.5 | Epoch: 191 | Iter: 290400 | Total Loss: 0.003671 | Recon Loss: 0.003121 | Commit Loss: 0.001098 | Perplexity: 2026.951860
2025-09-28 01:10:19,463 Stage: Train 0.5 | Epoch: 191 | Iter: 290600 | Total Loss: 0.003703 | Recon Loss: 0.003150 | Commit Loss: 0.001106 | Perplexity: 2029.138799
2025-09-28 01:10:46,093 Stage: Train 0.5 | Epoch: 191 | Iter: 290800 | Total Loss: 0.003690 | Recon Loss: 0.003142 | Commit Loss: 0.001095 | Perplexity: 2027.856934
2025-09-28 01:11:12,698 Stage: Train 0.5 | Epoch: 191 | Iter: 291000 | Total Loss: 0.003715 | Recon Loss: 0.003164 | Commit Loss: 0.001101 | Perplexity: 2027.560259
2025-09-28 01:11:39,192 Stage: Train 0.5 | Epoch: 191 | Iter: 291200 | Total Loss: 0.003800 | Recon Loss: 0.003248 | Commit Loss: 0.001104 | Perplexity: 2027.793959
2025-09-28 01:12:05,795 Stage: Train 0.5 | Epoch: 191 | Iter: 291400 | Total Loss: 0.003652 | Recon Loss: 0.003102 | Commit Loss: 0.001101 | Perplexity: 2028.810651
2025-09-28 01:12:32,389 Stage: Train 0.5 | Epoch: 191 | Iter: 291600 | Total Loss: 0.003782 | Recon Loss: 0.003232 | Commit Loss: 0.001100 | Perplexity: 2023.331564
Trainning Epoch:  58%|█████▊    | 192/330 [18:23:43<7:45:19, 202.31s/it]2025-09-28 01:12:59,207 Stage: Train 0.5 | Epoch: 192 | Iter: 291800 | Total Loss: 0.003707 | Recon Loss: 0.003156 | Commit Loss: 0.001103 | Perplexity: 2029.610834
2025-09-28 01:13:25,809 Stage: Train 0.5 | Epoch: 192 | Iter: 292000 | Total Loss: 0.003715 | Recon Loss: 0.003166 | Commit Loss: 0.001097 | Perplexity: 2024.989929
2025-09-28 01:13:52,305 Stage: Train 0.5 | Epoch: 192 | Iter: 292200 | Total Loss: 0.003783 | Recon Loss: 0.003238 | Commit Loss: 0.001091 | Perplexity: 2016.441837
2025-09-28 01:14:18,925 Stage: Train 0.5 | Epoch: 192 | Iter: 292400 | Total Loss: 0.003649 | Recon Loss: 0.003098 | Commit Loss: 0.001103 | Perplexity: 2031.336879
2025-09-28 01:14:45,475 Stage: Train 0.5 | Epoch: 192 | Iter: 292600 | Total Loss: 0.003628 | Recon Loss: 0.003079 | Commit Loss: 0.001097 | Perplexity: 2022.345891
2025-09-28 01:15:11,945 Stage: Train 0.5 | Epoch: 192 | Iter: 292800 | Total Loss: 0.003763 | Recon Loss: 0.003214 | Commit Loss: 0.001097 | Perplexity: 2029.056483
2025-09-28 01:15:38,638 Stage: Train 0.5 | Epoch: 192 | Iter: 293000 | Total Loss: 0.003765 | Recon Loss: 0.003216 | Commit Loss: 0.001098 | Perplexity: 2027.852618
Trainning Epoch:  58%|█████▊    | 193/330 [18:27:05<7:41:43, 202.21s/it]2025-09-28 01:16:05,406 Stage: Train 0.5 | Epoch: 193 | Iter: 293200 | Total Loss: 0.003722 | Recon Loss: 0.003178 | Commit Loss: 0.001089 | Perplexity: 2023.905620
2025-09-28 01:16:31,971 Stage: Train 0.5 | Epoch: 193 | Iter: 293400 | Total Loss: 0.003697 | Recon Loss: 0.003151 | Commit Loss: 0.001093 | Perplexity: 2027.205060
2025-09-28 01:16:58,602 Stage: Train 0.5 | Epoch: 193 | Iter: 293600 | Total Loss: 0.003660 | Recon Loss: 0.003113 | Commit Loss: 0.001094 | Perplexity: 2026.904365
2025-09-28 01:17:25,356 Stage: Train 0.5 | Epoch: 193 | Iter: 293800 | Total Loss: 0.003734 | Recon Loss: 0.003184 | Commit Loss: 0.001101 | Perplexity: 2026.159487
2025-09-28 01:17:52,036 Stage: Train 0.5 | Epoch: 193 | Iter: 294000 | Total Loss: 0.003706 | Recon Loss: 0.003155 | Commit Loss: 0.001102 | Perplexity: 2027.504292
2025-09-28 01:18:18,691 Stage: Train 0.5 | Epoch: 193 | Iter: 294200 | Total Loss: 0.003647 | Recon Loss: 0.003098 | Commit Loss: 0.001099 | Perplexity: 2031.605302
2025-09-28 01:18:45,312 Stage: Train 0.5 | Epoch: 193 | Iter: 294400 | Total Loss: 0.003715 | Recon Loss: 0.003164 | Commit Loss: 0.001102 | Perplexity: 2030.610659
2025-09-28 01:19:11,948 Stage: Train 0.5 | Epoch: 193 | Iter: 294600 | Total Loss: 0.003719 | Recon Loss: 0.003173 | Commit Loss: 0.001093 | Perplexity: 2029.030842
Trainning Epoch:  59%|█████▉    | 194/330 [18:30:28<7:38:37, 202.33s/it]2025-09-28 01:19:38,709 Stage: Train 0.5 | Epoch: 194 | Iter: 294800 | Total Loss: 0.003765 | Recon Loss: 0.003225 | Commit Loss: 0.001079 | Perplexity: 2021.846030
2025-09-28 01:20:05,417 Stage: Train 0.5 | Epoch: 194 | Iter: 295000 | Total Loss: 0.003845 | Recon Loss: 0.003296 | Commit Loss: 0.001100 | Perplexity: 2028.587534
2025-09-28 01:20:32,058 Stage: Train 0.5 | Epoch: 194 | Iter: 295200 | Total Loss: 0.003630 | Recon Loss: 0.003084 | Commit Loss: 0.001091 | Perplexity: 2024.965960
2025-09-28 01:20:58,599 Stage: Train 0.5 | Epoch: 194 | Iter: 295400 | Total Loss: 0.003685 | Recon Loss: 0.003133 | Commit Loss: 0.001105 | Perplexity: 2027.538069
2025-09-28 01:21:25,230 Stage: Train 0.5 | Epoch: 194 | Iter: 295600 | Total Loss: 0.003841 | Recon Loss: 0.003292 | Commit Loss: 0.001098 | Perplexity: 2029.626418
2025-09-28 01:21:51,945 Stage: Train 0.5 | Epoch: 194 | Iter: 295800 | Total Loss: 0.003701 | Recon Loss: 0.003156 | Commit Loss: 0.001090 | Perplexity: 2027.744521
2025-09-28 01:22:18,666 Stage: Train 0.5 | Epoch: 194 | Iter: 296000 | Total Loss: 0.003591 | Recon Loss: 0.003045 | Commit Loss: 0.001092 | Perplexity: 2028.152014
2025-09-28 01:22:45,368 Stage: Train 0.5 | Epoch: 194 | Iter: 296200 | Total Loss: 0.003721 | Recon Loss: 0.003172 | Commit Loss: 0.001098 | Perplexity: 2030.799000
Trainning Epoch:  59%|█████▉    | 195/330 [18:33:51<7:35:27, 202.43s/it]2025-09-28 01:23:12,252 Stage: Train 0.5 | Epoch: 195 | Iter: 296400 | Total Loss: 0.003704 | Recon Loss: 0.003162 | Commit Loss: 0.001084 | Perplexity: 2026.528708
2025-09-28 01:23:38,939 Stage: Train 0.5 | Epoch: 195 | Iter: 296600 | Total Loss: 0.003706 | Recon Loss: 0.003162 | Commit Loss: 0.001087 | Perplexity: 2026.274120
2025-09-28 01:24:05,561 Stage: Train 0.5 | Epoch: 195 | Iter: 296800 | Total Loss: 0.003725 | Recon Loss: 0.003179 | Commit Loss: 0.001092 | Perplexity: 2024.407319
2025-09-28 01:24:32,237 Stage: Train 0.5 | Epoch: 195 | Iter: 297000 | Total Loss: 0.003687 | Recon Loss: 0.003139 | Commit Loss: 0.001096 | Perplexity: 2026.700374
2025-09-28 01:24:59,088 Stage: Train 0.5 | Epoch: 195 | Iter: 297200 | Total Loss: 0.003683 | Recon Loss: 0.003137 | Commit Loss: 0.001092 | Perplexity: 2027.751326
2025-09-28 01:25:25,747 Stage: Train 0.5 | Epoch: 195 | Iter: 297400 | Total Loss: 0.003631 | Recon Loss: 0.003084 | Commit Loss: 0.001093 | Perplexity: 2030.103810
2025-09-28 01:25:52,374 Stage: Train 0.5 | Epoch: 195 | Iter: 297600 | Total Loss: 0.003765 | Recon Loss: 0.003214 | Commit Loss: 0.001101 | Perplexity: 2033.091351
Trainning Epoch:  59%|█████▉    | 196/330 [18:37:13<7:32:21, 202.55s/it]2025-09-28 01:26:19,146 Stage: Train 0.5 | Epoch: 196 | Iter: 297800 | Total Loss: 0.003798 | Recon Loss: 0.003256 | Commit Loss: 0.001083 | Perplexity: 2024.673933
2025-09-28 01:26:45,762 Stage: Train 0.5 | Epoch: 196 | Iter: 298000 | Total Loss: 0.003592 | Recon Loss: 0.003046 | Commit Loss: 0.001092 | Perplexity: 2032.702090
2025-09-28 01:27:12,348 Stage: Train 0.5 | Epoch: 196 | Iter: 298200 | Total Loss: 0.003704 | Recon Loss: 0.003160 | Commit Loss: 0.001089 | Perplexity: 2025.847431
2025-09-28 01:27:38,954 Stage: Train 0.5 | Epoch: 196 | Iter: 298400 | Total Loss: 0.003643 | Recon Loss: 0.003096 | Commit Loss: 0.001093 | Perplexity: 2027.352081
2025-09-28 01:28:05,656 Stage: Train 0.5 | Epoch: 196 | Iter: 298600 | Total Loss: 0.003709 | Recon Loss: 0.003161 | Commit Loss: 0.001097 | Perplexity: 2026.821627
2025-09-28 01:28:32,299 Stage: Train 0.5 | Epoch: 196 | Iter: 298800 | Total Loss: 0.003732 | Recon Loss: 0.003189 | Commit Loss: 0.001086 | Perplexity: 2027.029096
2025-09-28 01:28:58,774 Stage: Train 0.5 | Epoch: 196 | Iter: 299000 | Total Loss: 0.003760 | Recon Loss: 0.003212 | Commit Loss: 0.001095 | Perplexity: 2030.879461
2025-09-28 01:29:25,338 Stage: Train 0.5 | Epoch: 196 | Iter: 299200 | Total Loss: 0.003690 | Recon Loss: 0.003146 | Commit Loss: 0.001089 | Perplexity: 2024.862857
Trainning Epoch:  60%|█████▉    | 197/330 [18:40:36<7:28:45, 202.44s/it]2025-09-28 01:29:52,171 Stage: Train 0.5 | Epoch: 197 | Iter: 299400 | Total Loss: 0.003673 | Recon Loss: 0.003130 | Commit Loss: 0.001087 | Perplexity: 2030.844098
2025-09-28 01:30:18,725 Stage: Train 0.5 | Epoch: 197 | Iter: 299600 | Total Loss: 0.003595 | Recon Loss: 0.003054 | Commit Loss: 0.001082 | Perplexity: 2027.483281
2025-09-28 01:30:45,447 Stage: Train 0.5 | Epoch: 197 | Iter: 299800 | Total Loss: 0.003725 | Recon Loss: 0.003177 | Commit Loss: 0.001095 | Perplexity: 2029.704509
2025-09-28 01:31:12,074 Stage: Train 0.5 | Epoch: 197 | Iter: 300000 | Total Loss: 0.003760 | Recon Loss: 0.003217 | Commit Loss: 0.001088 | Perplexity: 2025.790255
2025-09-28 01:31:12,074 Saving model at iteration 300000
2025-09-28 01:31:12,285 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000
2025-09-28 01:31:12,768 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/model.safetensors
2025-09-28 01:31:13,384 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/optimizer.bin
2025-09-28 01:31:13,390 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/scheduler.bin
2025-09-28 01:31:13,391 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/sampler.bin
2025-09-28 01:31:13,391 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/random_states_0.pkl
2025-09-28 01:31:40,312 Stage: Train 0.5 | Epoch: 197 | Iter: 300200 | Total Loss: 0.003675 | Recon Loss: 0.003129 | Commit Loss: 0.001091 | Perplexity: 2028.788577
2025-09-28 01:32:06,863 Stage: Train 0.5 | Epoch: 197 | Iter: 300400 | Total Loss: 0.003664 | Recon Loss: 0.003121 | Commit Loss: 0.001086 | Perplexity: 2029.106492
2025-09-28 01:32:33,537 Stage: Train 0.5 | Epoch: 197 | Iter: 300600 | Total Loss: 0.003712 | Recon Loss: 0.003165 | Commit Loss: 0.001093 | Perplexity: 2028.402745
Trainning Epoch:  60%|██████    | 198/330 [18:44:00<7:26:21, 202.89s/it]2025-09-28 01:33:00,279 Stage: Train 0.5 | Epoch: 198 | Iter: 300800 | Total Loss: 0.003806 | Recon Loss: 0.003260 | Commit Loss: 0.001093 | Perplexity: 2022.636346
2025-09-28 01:33:26,844 Stage: Train 0.5 | Epoch: 198 | Iter: 301000 | Total Loss: 0.003636 | Recon Loss: 0.003095 | Commit Loss: 0.001083 | Perplexity: 2028.199796
2025-09-28 01:33:53,415 Stage: Train 0.5 | Epoch: 198 | Iter: 301200 | Total Loss: 0.003615 | Recon Loss: 0.003071 | Commit Loss: 0.001087 | Perplexity: 2029.059901
2025-09-28 01:34:20,003 Stage: Train 0.5 | Epoch: 198 | Iter: 301400 | Total Loss: 0.003673 | Recon Loss: 0.003131 | Commit Loss: 0.001085 | Perplexity: 2025.836572
2025-09-28 01:34:46,611 Stage: Train 0.5 | Epoch: 198 | Iter: 301600 | Total Loss: 0.003643 | Recon Loss: 0.003102 | Commit Loss: 0.001082 | Perplexity: 2025.287081
2025-09-28 01:35:13,170 Stage: Train 0.5 | Epoch: 198 | Iter: 301800 | Total Loss: 0.003759 | Recon Loss: 0.003217 | Commit Loss: 0.001084 | Perplexity: 2023.694389
2025-09-28 01:35:39,793 Stage: Train 0.5 | Epoch: 198 | Iter: 302000 | Total Loss: 0.003653 | Recon Loss: 0.003108 | Commit Loss: 0.001091 | Perplexity: 2028.115295
2025-09-28 01:36:06,385 Stage: Train 0.5 | Epoch: 198 | Iter: 302200 | Total Loss: 0.003761 | Recon Loss: 0.003215 | Commit Loss: 0.001092 | Perplexity: 2030.835154
Trainning Epoch:  60%|██████    | 199/330 [18:47:22<7:22:32, 202.69s/it]2025-09-28 01:36:33,244 Stage: Train 0.5 | Epoch: 199 | Iter: 302400 | Total Loss: 0.003630 | Recon Loss: 0.003088 | Commit Loss: 0.001085 | Perplexity: 2027.927713
2025-09-28 01:36:59,903 Stage: Train 0.5 | Epoch: 199 | Iter: 302600 | Total Loss: 0.003659 | Recon Loss: 0.003116 | Commit Loss: 0.001086 | Perplexity: 2024.056046
2025-09-28 01:37:26,523 Stage: Train 0.5 | Epoch: 199 | Iter: 302800 | Total Loss: 0.003653 | Recon Loss: 0.003108 | Commit Loss: 0.001089 | Perplexity: 2029.110651
2025-09-28 01:37:53,179 Stage: Train 0.5 | Epoch: 199 | Iter: 303000 | Total Loss: 0.003678 | Recon Loss: 0.003138 | Commit Loss: 0.001080 | Perplexity: 2028.357471
2025-09-28 01:38:19,859 Stage: Train 0.5 | Epoch: 199 | Iter: 303200 | Total Loss: 0.003663 | Recon Loss: 0.003118 | Commit Loss: 0.001090 | Perplexity: 2032.902231
2025-09-28 01:38:46,483 Stage: Train 0.5 | Epoch: 199 | Iter: 303400 | Total Loss: 0.003671 | Recon Loss: 0.003130 | Commit Loss: 0.001082 | Perplexity: 2023.795328
2025-09-28 01:39:12,849 Stage: Train 0.5 | Epoch: 199 | Iter: 303600 | Total Loss: 0.003744 | Recon Loss: 0.003202 | Commit Loss: 0.001085 | Perplexity: 2027.435049
2025-09-28 01:39:39,368 Stage: Train 0.5 | Epoch: 199 | Iter: 303800 | Total Loss: 0.003616 | Recon Loss: 0.003071 | Commit Loss: 0.001090 | Perplexity: 2031.212134
Trainning Epoch:  61%|██████    | 200/330 [18:50:44<7:18:48, 202.52s/it]2025-09-28 01:40:06,205 Stage: Train 0.5 | Epoch: 200 | Iter: 304000 | Total Loss: 0.003747 | Recon Loss: 0.003206 | Commit Loss: 0.001081 | Perplexity: 2024.921618
2025-09-28 01:40:32,796 Stage: Train 0.5 | Epoch: 200 | Iter: 304200 | Total Loss: 0.003647 | Recon Loss: 0.003104 | Commit Loss: 0.001085 | Perplexity: 2027.872896
2025-09-28 01:40:59,463 Stage: Train 0.5 | Epoch: 200 | Iter: 304400 | Total Loss: 0.003701 | Recon Loss: 0.003165 | Commit Loss: 0.001073 | Perplexity: 2023.729934
2025-09-28 01:41:26,039 Stage: Train 0.5 | Epoch: 200 | Iter: 304600 | Total Loss: 0.003677 | Recon Loss: 0.003132 | Commit Loss: 0.001090 | Perplexity: 2024.849924
2025-09-28 01:41:52,622 Stage: Train 0.5 | Epoch: 200 | Iter: 304800 | Total Loss: 0.003651 | Recon Loss: 0.003109 | Commit Loss: 0.001084 | Perplexity: 2031.129464
2025-09-28 01:42:19,186 Stage: Train 0.5 | Epoch: 200 | Iter: 305000 | Total Loss: 0.003661 | Recon Loss: 0.003120 | Commit Loss: 0.001082 | Perplexity: 2031.314526
2025-09-28 01:42:45,778 Stage: Train 0.5 | Epoch: 200 | Iter: 305200 | Total Loss: 0.003729 | Recon Loss: 0.003188 | Commit Loss: 0.001084 | Perplexity: 2029.793681
Trainning Epoch:  61%|██████    | 201/330 [18:54:06<7:15:11, 202.42s/it]2025-09-28 01:43:12,508 Stage: Train 0.5 | Epoch: 201 | Iter: 305400 | Total Loss: 0.003641 | Recon Loss: 0.003100 | Commit Loss: 0.001081 | Perplexity: 2032.239460
2025-09-28 01:43:39,129 Stage: Train 0.5 | Epoch: 201 | Iter: 305600 | Total Loss: 0.003640 | Recon Loss: 0.003099 | Commit Loss: 0.001081 | Perplexity: 2030.368560
2025-09-28 01:44:05,831 Stage: Train 0.5 | Epoch: 201 | Iter: 305800 | Total Loss: 0.003667 | Recon Loss: 0.003125 | Commit Loss: 0.001083 | Perplexity: 2028.937864
2025-09-28 01:44:32,383 Stage: Train 0.5 | Epoch: 201 | Iter: 306000 | Total Loss: 0.003640 | Recon Loss: 0.003101 | Commit Loss: 0.001076 | Perplexity: 2026.454051
2025-09-28 01:44:59,024 Stage: Train 0.5 | Epoch: 201 | Iter: 306200 | Total Loss: 0.003677 | Recon Loss: 0.003133 | Commit Loss: 0.001088 | Perplexity: 2027.727156
2025-09-28 01:45:25,663 Stage: Train 0.5 | Epoch: 201 | Iter: 306400 | Total Loss: 0.003698 | Recon Loss: 0.003156 | Commit Loss: 0.001085 | Perplexity: 2024.380447
2025-09-28 01:45:52,256 Stage: Train 0.5 | Epoch: 201 | Iter: 306600 | Total Loss: 0.003913 | Recon Loss: 0.003372 | Commit Loss: 0.001082 | Perplexity: 2022.979432
2025-09-28 01:46:18,851 Stage: Train 0.5 | Epoch: 201 | Iter: 306800 | Total Loss: 0.003592 | Recon Loss: 0.003051 | Commit Loss: 0.001081 | Perplexity: 2027.027759
Trainning Epoch:  61%|██████    | 202/330 [18:57:28<7:11:47, 202.40s/it]2025-09-28 01:46:45,720 Stage: Train 0.5 | Epoch: 202 | Iter: 307000 | Total Loss: 0.003629 | Recon Loss: 0.003092 | Commit Loss: 0.001074 | Perplexity: 2024.353865
2025-09-28 01:47:12,253 Stage: Train 0.5 | Epoch: 202 | Iter: 307200 | Total Loss: 0.003646 | Recon Loss: 0.003102 | Commit Loss: 0.001088 | Perplexity: 2031.933375
2025-09-28 01:47:38,816 Stage: Train 0.5 | Epoch: 202 | Iter: 307400 | Total Loss: 0.003689 | Recon Loss: 0.003146 | Commit Loss: 0.001086 | Perplexity: 2031.351612
2025-09-28 01:48:05,431 Stage: Train 0.5 | Epoch: 202 | Iter: 307600 | Total Loss: 0.003589 | Recon Loss: 0.003050 | Commit Loss: 0.001079 | Perplexity: 2025.073354
2025-09-28 01:48:31,955 Stage: Train 0.5 | Epoch: 202 | Iter: 307800 | Total Loss: 0.003668 | Recon Loss: 0.003126 | Commit Loss: 0.001083 | Perplexity: 2030.300228
2025-09-28 01:48:58,526 Stage: Train 0.5 | Epoch: 202 | Iter: 308000 | Total Loss: 0.003686 | Recon Loss: 0.003146 | Commit Loss: 0.001081 | Perplexity: 2025.130042
2025-09-28 01:49:24,929 Stage: Train 0.5 | Epoch: 202 | Iter: 308200 | Total Loss: 0.003687 | Recon Loss: 0.003146 | Commit Loss: 0.001082 | Perplexity: 2028.125354
Trainning Epoch:  62%|██████▏   | 203/330 [19:00:50<7:08:10, 202.29s/it]2025-09-28 01:49:51,859 Stage: Train 0.5 | Epoch: 203 | Iter: 308400 | Total Loss: 0.003673 | Recon Loss: 0.003133 | Commit Loss: 0.001080 | Perplexity: 2030.269704
2025-09-28 01:50:18,504 Stage: Train 0.5 | Epoch: 203 | Iter: 308600 | Total Loss: 0.003609 | Recon Loss: 0.003068 | Commit Loss: 0.001083 | Perplexity: 2029.538810
2025-09-28 01:50:45,030 Stage: Train 0.5 | Epoch: 203 | Iter: 308800 | Total Loss: 0.003656 | Recon Loss: 0.003117 | Commit Loss: 0.001077 | Perplexity: 2029.030115
2025-09-28 01:51:11,592 Stage: Train 0.5 | Epoch: 203 | Iter: 309000 | Total Loss: 0.003666 | Recon Loss: 0.003128 | Commit Loss: 0.001075 | Perplexity: 2026.690020
2025-09-28 01:51:38,211 Stage: Train 0.5 | Epoch: 203 | Iter: 309200 | Total Loss: 0.003639 | Recon Loss: 0.003102 | Commit Loss: 0.001075 | Perplexity: 2028.961875
2025-09-28 01:52:04,791 Stage: Train 0.5 | Epoch: 203 | Iter: 309400 | Total Loss: 0.003633 | Recon Loss: 0.003093 | Commit Loss: 0.001078 | Perplexity: 2025.634893
2025-09-28 01:52:31,334 Stage: Train 0.5 | Epoch: 203 | Iter: 309600 | Total Loss: 0.003724 | Recon Loss: 0.003182 | Commit Loss: 0.001083 | Perplexity: 2029.398806
2025-09-28 01:52:57,961 Stage: Train 0.5 | Epoch: 203 | Iter: 309800 | Total Loss: 0.003633 | Recon Loss: 0.003096 | Commit Loss: 0.001075 | Perplexity: 2028.652581
Trainning Epoch:  62%|██████▏   | 204/330 [19:04:13<7:04:45, 202.27s/it]2025-09-28 01:53:24,867 Stage: Train 0.5 | Epoch: 204 | Iter: 310000 | Total Loss: 0.003698 | Recon Loss: 0.003158 | Commit Loss: 0.001080 | Perplexity: 2034.095829
2025-09-28 01:53:51,395 Stage: Train 0.5 | Epoch: 204 | Iter: 310200 | Total Loss: 0.003563 | Recon Loss: 0.003026 | Commit Loss: 0.001074 | Perplexity: 2026.800854
2025-09-28 01:54:17,925 Stage: Train 0.5 | Epoch: 204 | Iter: 310400 | Total Loss: 0.003729 | Recon Loss: 0.003189 | Commit Loss: 0.001081 | Perplexity: 2030.474003
2025-09-28 01:54:44,443 Stage: Train 0.5 | Epoch: 204 | Iter: 310600 | Total Loss: 0.003655 | Recon Loss: 0.003117 | Commit Loss: 0.001077 | Perplexity: 2027.222675
2025-09-28 01:55:11,043 Stage: Train 0.5 | Epoch: 204 | Iter: 310800 | Total Loss: 0.003586 | Recon Loss: 0.003044 | Commit Loss: 0.001082 | Perplexity: 2031.971487
2025-09-28 01:55:37,643 Stage: Train 0.5 | Epoch: 204 | Iter: 311000 | Total Loss: 0.003649 | Recon Loss: 0.003113 | Commit Loss: 0.001073 | Perplexity: 2028.386977
2025-09-28 01:56:04,113 Stage: Train 0.5 | Epoch: 204 | Iter: 311200 | Total Loss: 0.003700 | Recon Loss: 0.003166 | Commit Loss: 0.001068 | Perplexity: 2027.118965
Trainning Epoch:  62%|██████▏   | 205/330 [19:07:34<7:01:04, 202.11s/it]2025-09-28 01:56:30,757 Stage: Train 0.5 | Epoch: 205 | Iter: 311400 | Total Loss: 0.003609 | Recon Loss: 0.003069 | Commit Loss: 0.001081 | Perplexity: 2029.927155
2025-09-28 01:56:57,422 Stage: Train 0.5 | Epoch: 205 | Iter: 311600 | Total Loss: 0.003629 | Recon Loss: 0.003094 | Commit Loss: 0.001071 | Perplexity: 2030.817072
2025-09-28 01:57:24,051 Stage: Train 0.5 | Epoch: 205 | Iter: 311800 | Total Loss: 0.003639 | Recon Loss: 0.003100 | Commit Loss: 0.001077 | Perplexity: 2034.647440
2025-09-28 01:57:50,663 Stage: Train 0.5 | Epoch: 205 | Iter: 312000 | Total Loss: 0.003659 | Recon Loss: 0.003118 | Commit Loss: 0.001083 | Perplexity: 2029.975054
2025-09-28 01:58:17,239 Stage: Train 0.5 | Epoch: 205 | Iter: 312200 | Total Loss: 0.003632 | Recon Loss: 0.003093 | Commit Loss: 0.001078 | Perplexity: 2032.406202
2025-09-28 01:58:43,912 Stage: Train 0.5 | Epoch: 205 | Iter: 312400 | Total Loss: 0.003618 | Recon Loss: 0.003080 | Commit Loss: 0.001075 | Perplexity: 2028.406967
2025-09-28 01:59:10,495 Stage: Train 0.5 | Epoch: 205 | Iter: 312600 | Total Loss: 0.003555 | Recon Loss: 0.003017 | Commit Loss: 0.001077 | Perplexity: 2026.651343
2025-09-28 01:59:36,994 Stage: Train 0.5 | Epoch: 205 | Iter: 312800 | Total Loss: 0.003661 | Recon Loss: 0.003118 | Commit Loss: 0.001085 | Perplexity: 2029.424465
Trainning Epoch:  62%|██████▏   | 206/330 [19:10:57<6:57:49, 202.17s/it]2025-09-28 02:00:03,848 Stage: Train 0.5 | Epoch: 206 | Iter: 313000 | Total Loss: 0.003622 | Recon Loss: 0.003086 | Commit Loss: 0.001074 | Perplexity: 2030.803767
2025-09-28 02:00:30,526 Stage: Train 0.5 | Epoch: 206 | Iter: 313200 | Total Loss: 0.003592 | Recon Loss: 0.003053 | Commit Loss: 0.001078 | Perplexity: 2024.535826
2025-09-28 02:00:57,201 Stage: Train 0.5 | Epoch: 206 | Iter: 313400 | Total Loss: 0.003601 | Recon Loss: 0.003062 | Commit Loss: 0.001078 | Perplexity: 2030.322385
2025-09-28 02:01:23,859 Stage: Train 0.5 | Epoch: 206 | Iter: 313600 | Total Loss: 0.003635 | Recon Loss: 0.003097 | Commit Loss: 0.001076 | Perplexity: 2033.437534
2025-09-28 02:01:50,367 Stage: Train 0.5 | Epoch: 206 | Iter: 313800 | Total Loss: 0.003586 | Recon Loss: 0.003046 | Commit Loss: 0.001081 | Perplexity: 2034.129382
2025-09-28 02:02:17,013 Stage: Train 0.5 | Epoch: 206 | Iter: 314000 | Total Loss: 0.003659 | Recon Loss: 0.003123 | Commit Loss: 0.001072 | Perplexity: 2029.482112
2025-09-28 02:02:43,651 Stage: Train 0.5 | Epoch: 206 | Iter: 314200 | Total Loss: 0.003673 | Recon Loss: 0.003137 | Commit Loss: 0.001072 | Perplexity: 2028.618052
2025-09-28 02:03:10,270 Stage: Train 0.5 | Epoch: 206 | Iter: 314400 | Total Loss: 0.003629 | Recon Loss: 0.003096 | Commit Loss: 0.001066 | Perplexity: 2027.693948
Trainning Epoch:  63%|██████▎   | 207/330 [19:14:19<6:54:38, 202.26s/it]2025-09-28 02:03:37,085 Stage: Train 0.5 | Epoch: 207 | Iter: 314600 | Total Loss: 0.003582 | Recon Loss: 0.003045 | Commit Loss: 0.001072 | Perplexity: 2029.645793
2025-09-28 02:04:03,768 Stage: Train 0.5 | Epoch: 207 | Iter: 314800 | Total Loss: 0.003693 | Recon Loss: 0.003154 | Commit Loss: 0.001078 | Perplexity: 2033.760259
2025-09-28 02:04:30,379 Stage: Train 0.5 | Epoch: 207 | Iter: 315000 | Total Loss: 0.003601 | Recon Loss: 0.003064 | Commit Loss: 0.001074 | Perplexity: 2029.193107
2025-09-28 02:04:56,923 Stage: Train 0.5 | Epoch: 207 | Iter: 315200 | Total Loss: 0.003563 | Recon Loss: 0.003031 | Commit Loss: 0.001063 | Perplexity: 2026.643538
2025-09-28 02:05:23,520 Stage: Train 0.5 | Epoch: 207 | Iter: 315400 | Total Loss: 0.003676 | Recon Loss: 0.003137 | Commit Loss: 0.001079 | Perplexity: 2029.384970
2025-09-28 02:05:49,991 Stage: Train 0.5 | Epoch: 207 | Iter: 315600 | Total Loss: 0.003552 | Recon Loss: 0.003015 | Commit Loss: 0.001073 | Perplexity: 2033.127278
2025-09-28 02:06:16,634 Stage: Train 0.5 | Epoch: 207 | Iter: 315800 | Total Loss: 0.003693 | Recon Loss: 0.003159 | Commit Loss: 0.001068 | Perplexity: 2026.431342
Trainning Epoch:  63%|██████▎   | 208/330 [19:17:41<6:51:12, 202.23s/it]2025-09-28 02:06:43,364 Stage: Train 0.5 | Epoch: 208 | Iter: 316000 | Total Loss: 0.003652 | Recon Loss: 0.003113 | Commit Loss: 0.001077 | Perplexity: 2026.838775
2025-09-28 02:07:09,971 Stage: Train 0.5 | Epoch: 208 | Iter: 316200 | Total Loss: 0.003544 | Recon Loss: 0.003007 | Commit Loss: 0.001072 | Perplexity: 2026.567268
2025-09-28 02:07:36,591 Stage: Train 0.5 | Epoch: 208 | Iter: 316400 | Total Loss: 0.003654 | Recon Loss: 0.003119 | Commit Loss: 0.001070 | Perplexity: 2028.157065
2025-09-28 02:08:03,140 Stage: Train 0.5 | Epoch: 208 | Iter: 316600 | Total Loss: 0.003622 | Recon Loss: 0.003088 | Commit Loss: 0.001068 | Perplexity: 2027.052310
2025-09-28 02:08:29,661 Stage: Train 0.5 | Epoch: 208 | Iter: 316800 | Total Loss: 0.003843 | Recon Loss: 0.003311 | Commit Loss: 0.001064 | Perplexity: 2022.402835
2025-09-28 02:08:56,274 Stage: Train 0.5 | Epoch: 208 | Iter: 317000 | Total Loss: 0.003520 | Recon Loss: 0.002984 | Commit Loss: 0.001072 | Perplexity: 2030.504036
2025-09-28 02:09:22,825 Stage: Train 0.5 | Epoch: 208 | Iter: 317200 | Total Loss: 0.003631 | Recon Loss: 0.003093 | Commit Loss: 0.001075 | Perplexity: 2033.681617
2025-09-28 02:09:49,485 Stage: Train 0.5 | Epoch: 208 | Iter: 317400 | Total Loss: 0.003640 | Recon Loss: 0.003104 | Commit Loss: 0.001072 | Perplexity: 2035.350756
Trainning Epoch:  63%|██████▎   | 209/330 [19:21:03<6:47:46, 202.20s/it]2025-09-28 02:10:16,507 Stage: Train 0.5 | Epoch: 209 | Iter: 317600 | Total Loss: 0.003626 | Recon Loss: 0.003093 | Commit Loss: 0.001067 | Perplexity: 2023.881124
2025-09-28 02:10:43,064 Stage: Train 0.5 | Epoch: 209 | Iter: 317800 | Total Loss: 0.003709 | Recon Loss: 0.003179 | Commit Loss: 0.001059 | Perplexity: 2021.014099
2025-09-28 02:11:09,554 Stage: Train 0.5 | Epoch: 209 | Iter: 318000 | Total Loss: 0.003568 | Recon Loss: 0.003038 | Commit Loss: 0.001060 | Perplexity: 2026.825430
2025-09-28 02:11:36,103 Stage: Train 0.5 | Epoch: 209 | Iter: 318200 | Total Loss: 0.003600 | Recon Loss: 0.003061 | Commit Loss: 0.001079 | Perplexity: 2039.893586
2025-09-28 02:12:02,729 Stage: Train 0.5 | Epoch: 209 | Iter: 318400 | Total Loss: 0.003612 | Recon Loss: 0.003077 | Commit Loss: 0.001070 | Perplexity: 2029.238394
2025-09-28 02:12:29,412 Stage: Train 0.5 | Epoch: 209 | Iter: 318600 | Total Loss: 0.003565 | Recon Loss: 0.003025 | Commit Loss: 0.001081 | Perplexity: 2035.433069
2025-09-28 02:12:56,051 Stage: Train 0.5 | Epoch: 209 | Iter: 318800 | Total Loss: 0.003596 | Recon Loss: 0.003059 | Commit Loss: 0.001075 | Perplexity: 2034.038479
Trainning Epoch:  64%|██████▎   | 210/330 [19:24:26<6:44:32, 202.27s/it]2025-09-28 02:13:22,927 Stage: Train 0.5 | Epoch: 210 | Iter: 319000 | Total Loss: 0.003659 | Recon Loss: 0.003121 | Commit Loss: 0.001076 | Perplexity: 2031.460138
2025-09-28 02:13:49,519 Stage: Train 0.5 | Epoch: 210 | Iter: 319200 | Total Loss: 0.003569 | Recon Loss: 0.003034 | Commit Loss: 0.001070 | Perplexity: 2031.671712
2025-09-28 02:14:16,001 Stage: Train 0.5 | Epoch: 210 | Iter: 319400 | Total Loss: 0.003634 | Recon Loss: 0.003095 | Commit Loss: 0.001078 | Perplexity: 2025.649248
2025-09-28 02:14:42,571 Stage: Train 0.5 | Epoch: 210 | Iter: 319600 | Total Loss: 0.003561 | Recon Loss: 0.003028 | Commit Loss: 0.001067 | Perplexity: 2027.517141
2025-09-28 02:15:09,118 Stage: Train 0.5 | Epoch: 210 | Iter: 319800 | Total Loss: 0.003615 | Recon Loss: 0.003081 | Commit Loss: 0.001069 | Perplexity: 2031.809448
2025-09-28 02:15:35,723 Stage: Train 0.5 | Epoch: 210 | Iter: 320000 | Total Loss: 0.003613 | Recon Loss: 0.003078 | Commit Loss: 0.001069 | Perplexity: 2034.193206
2025-09-28 02:15:35,723 Saving model at iteration 320000
2025-09-28 02:15:35,984 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000
2025-09-28 02:15:36,472 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/model.safetensors
2025-09-28 02:15:36,997 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/optimizer.bin
2025-09-28 02:15:36,997 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/scheduler.bin
2025-09-28 02:15:36,997 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/sampler.bin
2025-09-28 02:15:36,998 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/random_states_0.pkl
2025-09-28 02:16:03,700 Stage: Train 0.5 | Epoch: 210 | Iter: 320200 | Total Loss: 0.003607 | Recon Loss: 0.003071 | Commit Loss: 0.001071 | Perplexity: 2034.622206
2025-09-28 02:16:30,170 Stage: Train 0.5 | Epoch: 210 | Iter: 320400 | Total Loss: 0.003617 | Recon Loss: 0.003082 | Commit Loss: 0.001069 | Perplexity: 2034.700588
Trainning Epoch:  64%|██████▍   | 211/330 [19:27:49<6:41:47, 202.58s/it]2025-09-28 02:16:56,976 Stage: Train 0.5 | Epoch: 211 | Iter: 320600 | Total Loss: 0.003680 | Recon Loss: 0.003151 | Commit Loss: 0.001058 | Perplexity: 2024.057201
2025-09-28 02:17:23,590 Stage: Train 0.5 | Epoch: 211 | Iter: 320800 | Total Loss: 0.003504 | Recon Loss: 0.002971 | Commit Loss: 0.001067 | Perplexity: 2034.804879
2025-09-28 02:17:50,234 Stage: Train 0.5 | Epoch: 211 | Iter: 321000 | Total Loss: 0.003611 | Recon Loss: 0.003078 | Commit Loss: 0.001065 | Perplexity: 2033.221611
2025-09-28 02:18:16,858 Stage: Train 0.5 | Epoch: 211 | Iter: 321200 | Total Loss: 0.003613 | Recon Loss: 0.003076 | Commit Loss: 0.001074 | Perplexity: 2031.338942
2025-09-28 02:18:43,435 Stage: Train 0.5 | Epoch: 211 | Iter: 321400 | Total Loss: 0.003581 | Recon Loss: 0.003050 | Commit Loss: 0.001062 | Perplexity: 2027.077272
2025-09-28 02:19:10,082 Stage: Train 0.5 | Epoch: 211 | Iter: 321600 | Total Loss: 0.003555 | Recon Loss: 0.003022 | Commit Loss: 0.001066 | Perplexity: 2027.248696
2025-09-28 02:19:36,635 Stage: Train 0.5 | Epoch: 211 | Iter: 321800 | Total Loss: 0.003792 | Recon Loss: 0.003259 | Commit Loss: 0.001065 | Perplexity: 2033.361886
2025-09-28 02:20:03,197 Stage: Train 0.5 | Epoch: 211 | Iter: 322000 | Total Loss: 0.003654 | Recon Loss: 0.003117 | Commit Loss: 0.001073 | Perplexity: 2028.354725
Trainning Epoch:  64%|██████▍   | 212/330 [19:31:12<6:38:16, 202.51s/it]2025-09-28 02:20:30,228 Stage: Train 0.5 | Epoch: 212 | Iter: 322200 | Total Loss: 0.003601 | Recon Loss: 0.003070 | Commit Loss: 0.001062 | Perplexity: 2030.722133
2025-09-28 02:20:56,885 Stage: Train 0.5 | Epoch: 212 | Iter: 322400 | Total Loss: 0.003576 | Recon Loss: 0.003045 | Commit Loss: 0.001063 | Perplexity: 2026.455190
2025-09-28 02:21:23,451 Stage: Train 0.5 | Epoch: 212 | Iter: 322600 | Total Loss: 0.003633 | Recon Loss: 0.003101 | Commit Loss: 0.001064 | Perplexity: 2031.733762
2025-09-28 02:21:50,017 Stage: Train 0.5 | Epoch: 212 | Iter: 322800 | Total Loss: 0.003594 | Recon Loss: 0.003058 | Commit Loss: 0.001073 | Perplexity: 2032.558996
2025-09-28 02:22:16,648 Stage: Train 0.5 | Epoch: 212 | Iter: 323000 | Total Loss: 0.003584 | Recon Loss: 0.003052 | Commit Loss: 0.001065 | Perplexity: 2031.031786
2025-09-28 02:22:43,215 Stage: Train 0.5 | Epoch: 212 | Iter: 323200 | Total Loss: 0.003607 | Recon Loss: 0.003075 | Commit Loss: 0.001064 | Perplexity: 2026.668020
2025-09-28 02:23:09,863 Stage: Train 0.5 | Epoch: 212 | Iter: 323400 | Total Loss: 0.003602 | Recon Loss: 0.003069 | Commit Loss: 0.001067 | Perplexity: 2034.272084
Trainning Epoch:  65%|██████▍   | 213/330 [19:34:34<6:34:49, 202.47s/it]2025-09-28 02:23:36,655 Stage: Train 0.5 | Epoch: 213 | Iter: 323600 | Total Loss: 0.003603 | Recon Loss: 0.003070 | Commit Loss: 0.001065 | Perplexity: 2032.340533
2025-09-28 02:24:03,289 Stage: Train 0.5 | Epoch: 213 | Iter: 323800 | Total Loss: 0.003605 | Recon Loss: 0.003078 | Commit Loss: 0.001056 | Perplexity: 2024.007448
2025-09-28 02:24:29,893 Stage: Train 0.5 | Epoch: 213 | Iter: 324000 | Total Loss: 0.003605 | Recon Loss: 0.003073 | Commit Loss: 0.001064 | Perplexity: 2029.009477
2025-09-28 02:24:56,410 Stage: Train 0.5 | Epoch: 213 | Iter: 324200 | Total Loss: 0.003809 | Recon Loss: 0.003279 | Commit Loss: 0.001059 | Perplexity: 2027.362355
2025-09-28 02:25:22,942 Stage: Train 0.5 | Epoch: 213 | Iter: 324400 | Total Loss: 0.003418 | Recon Loss: 0.002887 | Commit Loss: 0.001061 | Perplexity: 2029.950272
2025-09-28 02:25:49,484 Stage: Train 0.5 | Epoch: 213 | Iter: 324600 | Total Loss: 0.003665 | Recon Loss: 0.003131 | Commit Loss: 0.001069 | Perplexity: 2033.011286
2025-09-28 02:26:16,094 Stage: Train 0.5 | Epoch: 213 | Iter: 324800 | Total Loss: 0.003619 | Recon Loss: 0.003084 | Commit Loss: 0.001069 | Perplexity: 2032.209467
2025-09-28 02:26:42,636 Stage: Train 0.5 | Epoch: 213 | Iter: 325000 | Total Loss: 0.003647 | Recon Loss: 0.003112 | Commit Loss: 0.001069 | Perplexity: 2029.005182
Trainning Epoch:  65%|██████▍   | 214/330 [19:37:56<6:31:08, 202.31s/it]2025-09-28 02:27:09,342 Stage: Train 0.5 | Epoch: 214 | Iter: 325200 | Total Loss: 0.003582 | Recon Loss: 0.003053 | Commit Loss: 0.001058 | Perplexity: 2029.470730
2025-09-28 02:27:35,944 Stage: Train 0.5 | Epoch: 214 | Iter: 325400 | Total Loss: 0.003612 | Recon Loss: 0.003081 | Commit Loss: 0.001060 | Perplexity: 2028.531813
2025-09-28 02:28:02,570 Stage: Train 0.5 | Epoch: 214 | Iter: 325600 | Total Loss: 0.003570 | Recon Loss: 0.003036 | Commit Loss: 0.001068 | Perplexity: 2029.698506
2025-09-28 02:28:29,162 Stage: Train 0.5 | Epoch: 214 | Iter: 325800 | Total Loss: 0.003572 | Recon Loss: 0.003037 | Commit Loss: 0.001069 | Perplexity: 2033.776704
2025-09-28 02:28:55,870 Stage: Train 0.5 | Epoch: 214 | Iter: 326000 | Total Loss: 0.003640 | Recon Loss: 0.003108 | Commit Loss: 0.001064 | Perplexity: 2029.599272
2025-09-28 02:29:22,552 Stage: Train 0.5 | Epoch: 214 | Iter: 326200 | Total Loss: 0.003479 | Recon Loss: 0.002950 | Commit Loss: 0.001059 | Perplexity: 2024.603642
2025-09-28 02:29:49,127 Stage: Train 0.5 | Epoch: 214 | Iter: 326400 | Total Loss: 0.003719 | Recon Loss: 0.003185 | Commit Loss: 0.001067 | Perplexity: 2030.482884
Trainning Epoch:  65%|██████▌   | 215/330 [19:41:18<6:27:47, 202.33s/it]2025-09-28 02:30:15,902 Stage: Train 0.5 | Epoch: 215 | Iter: 326600 | Total Loss: 0.003522 | Recon Loss: 0.002992 | Commit Loss: 0.001059 | Perplexity: 2031.281805
2025-09-28 02:30:42,374 Stage: Train 0.5 | Epoch: 215 | Iter: 326800 | Total Loss: 0.003521 | Recon Loss: 0.002991 | Commit Loss: 0.001060 | Perplexity: 2033.678675
2025-09-28 02:31:08,930 Stage: Train 0.5 | Epoch: 215 | Iter: 327000 | Total Loss: 0.003624 | Recon Loss: 0.003093 | Commit Loss: 0.001063 | Perplexity: 2032.271616
2025-09-28 02:31:35,485 Stage: Train 0.5 | Epoch: 215 | Iter: 327200 | Total Loss: 0.003592 | Recon Loss: 0.003060 | Commit Loss: 0.001063 | Perplexity: 2028.189671
2025-09-28 02:32:02,137 Stage: Train 0.5 | Epoch: 215 | Iter: 327400 | Total Loss: 0.003542 | Recon Loss: 0.003012 | Commit Loss: 0.001060 | Perplexity: 2032.117150
2025-09-28 02:32:28,754 Stage: Train 0.5 | Epoch: 215 | Iter: 327600 | Total Loss: 0.003590 | Recon Loss: 0.003060 | Commit Loss: 0.001061 | Perplexity: 2026.423219
2025-09-28 02:32:55,245 Stage: Train 0.5 | Epoch: 215 | Iter: 327800 | Total Loss: 0.003624 | Recon Loss: 0.003085 | Commit Loss: 0.001077 | Perplexity: 2033.188424
2025-09-28 02:33:21,832 Stage: Train 0.5 | Epoch: 215 | Iter: 328000 | Total Loss: 0.003668 | Recon Loss: 0.003138 | Commit Loss: 0.001060 | Perplexity: 2028.701321
Trainning Epoch:  65%|██████▌   | 216/330 [19:44:40<6:24:15, 202.24s/it]2025-09-28 02:33:48,799 Stage: Train 0.5 | Epoch: 216 | Iter: 328200 | Total Loss: 0.003581 | Recon Loss: 0.003052 | Commit Loss: 0.001058 | Perplexity: 2026.099517
2025-09-28 02:34:15,463 Stage: Train 0.5 | Epoch: 216 | Iter: 328400 | Total Loss: 0.003622 | Recon Loss: 0.003090 | Commit Loss: 0.001065 | Perplexity: 2035.794360
2025-09-28 02:34:42,058 Stage: Train 0.5 | Epoch: 216 | Iter: 328600 | Total Loss: 0.003529 | Recon Loss: 0.002997 | Commit Loss: 0.001064 | Perplexity: 2031.885654
2025-09-28 02:35:08,683 Stage: Train 0.5 | Epoch: 216 | Iter: 328800 | Total Loss: 0.003552 | Recon Loss: 0.003024 | Commit Loss: 0.001056 | Perplexity: 2028.211800
2025-09-28 02:35:35,228 Stage: Train 0.5 | Epoch: 216 | Iter: 329000 | Total Loss: 0.003683 | Recon Loss: 0.003151 | Commit Loss: 0.001065 | Perplexity: 2034.963809
2025-09-28 02:36:01,823 Stage: Train 0.5 | Epoch: 216 | Iter: 329200 | Total Loss: 0.003568 | Recon Loss: 0.003037 | Commit Loss: 0.001061 | Perplexity: 2028.553972
2025-09-28 02:36:28,412 Stage: Train 0.5 | Epoch: 216 | Iter: 329400 | Total Loss: 0.003551 | Recon Loss: 0.003022 | Commit Loss: 0.001056 | Perplexity: 2029.620604
2025-09-28 02:36:54,948 Stage: Train 0.5 | Epoch: 216 | Iter: 329600 | Total Loss: 0.003618 | Recon Loss: 0.003091 | Commit Loss: 0.001053 | Perplexity: 2027.457599
Trainning Epoch:  66%|██████▌   | 217/330 [19:48:03<6:20:57, 202.28s/it]2025-09-28 02:37:21,896 Stage: Train 0.5 | Epoch: 217 | Iter: 329800 | Total Loss: 0.003484 | Recon Loss: 0.002954 | Commit Loss: 0.001060 | Perplexity: 2028.866716
2025-09-28 02:37:48,423 Stage: Train 0.5 | Epoch: 217 | Iter: 330000 | Total Loss: 0.003591 | Recon Loss: 0.003054 | Commit Loss: 0.001074 | Perplexity: 2032.020474
2025-09-28 02:38:14,943 Stage: Train 0.5 | Epoch: 217 | Iter: 330200 | Total Loss: 0.003586 | Recon Loss: 0.003055 | Commit Loss: 0.001062 | Perplexity: 2031.473143
2025-09-28 02:38:41,460 Stage: Train 0.5 | Epoch: 217 | Iter: 330400 | Total Loss: 0.003529 | Recon Loss: 0.003002 | Commit Loss: 0.001054 | Perplexity: 2037.412663
2025-09-28 02:39:08,072 Stage: Train 0.5 | Epoch: 217 | Iter: 330600 | Total Loss: 0.003580 | Recon Loss: 0.003052 | Commit Loss: 0.001057 | Perplexity: 2025.113128
2025-09-28 02:39:34,593 Stage: Train 0.5 | Epoch: 217 | Iter: 330800 | Total Loss: 0.003545 | Recon Loss: 0.003009 | Commit Loss: 0.001072 | Perplexity: 2036.726722
2025-09-28 02:40:01,168 Stage: Train 0.5 | Epoch: 217 | Iter: 331000 | Total Loss: 0.003587 | Recon Loss: 0.003059 | Commit Loss: 0.001056 | Perplexity: 2025.466997
Trainning Epoch:  66%|██████▌   | 218/330 [19:51:25<6:17:23, 202.17s/it]2025-09-28 02:40:27,980 Stage: Train 0.5 | Epoch: 218 | Iter: 331200 | Total Loss: 0.003551 | Recon Loss: 0.003024 | Commit Loss: 0.001054 | Perplexity: 2029.836282
2025-09-28 02:40:54,514 Stage: Train 0.5 | Epoch: 218 | Iter: 331400 | Total Loss: 0.003579 | Recon Loss: 0.003051 | Commit Loss: 0.001055 | Perplexity: 2029.705749
2025-09-28 02:41:21,142 Stage: Train 0.5 | Epoch: 218 | Iter: 331600 | Total Loss: 0.003565 | Recon Loss: 0.003033 | Commit Loss: 0.001064 | Perplexity: 2032.680237
2025-09-28 02:41:47,848 Stage: Train 0.5 | Epoch: 218 | Iter: 331800 | Total Loss: 0.003529 | Recon Loss: 0.003002 | Commit Loss: 0.001054 | Perplexity: 2022.993266
2025-09-28 02:42:14,291 Stage: Train 0.5 | Epoch: 218 | Iter: 332000 | Total Loss: 0.003585 | Recon Loss: 0.003054 | Commit Loss: 0.001062 | Perplexity: 2034.079121
2025-09-28 02:42:40,867 Stage: Train 0.5 | Epoch: 218 | Iter: 332200 | Total Loss: 0.003569 | Recon Loss: 0.003042 | Commit Loss: 0.001054 | Perplexity: 2030.465352
2025-09-28 02:43:07,531 Stage: Train 0.5 | Epoch: 218 | Iter: 332400 | Total Loss: 0.003592 | Recon Loss: 0.003061 | Commit Loss: 0.001063 | Perplexity: 2034.123163
2025-09-28 02:43:34,184 Stage: Train 0.5 | Epoch: 218 | Iter: 332600 | Total Loss: 0.003574 | Recon Loss: 0.003045 | Commit Loss: 0.001059 | Perplexity: 2031.693305
Trainning Epoch:  66%|██████▋   | 219/330 [19:54:47<6:14:02, 202.18s/it]2025-09-28 02:44:00,951 Stage: Train 0.5 | Epoch: 219 | Iter: 332800 | Total Loss: 0.003554 | Recon Loss: 0.003028 | Commit Loss: 0.001053 | Perplexity: 2031.542519
2025-09-28 02:44:27,431 Stage: Train 0.5 | Epoch: 219 | Iter: 333000 | Total Loss: 0.003553 | Recon Loss: 0.003023 | Commit Loss: 0.001059 | Perplexity: 2031.224475
2025-09-28 02:44:53,908 Stage: Train 0.5 | Epoch: 219 | Iter: 333200 | Total Loss: 0.003508 | Recon Loss: 0.002981 | Commit Loss: 0.001054 | Perplexity: 2027.939835
2025-09-28 02:45:20,381 Stage: Train 0.5 | Epoch: 219 | Iter: 333400 | Total Loss: 0.003591 | Recon Loss: 0.003066 | Commit Loss: 0.001051 | Perplexity: 2026.203540
2025-09-28 02:45:46,867 Stage: Train 0.5 | Epoch: 219 | Iter: 333600 | Total Loss: 0.003586 | Recon Loss: 0.003060 | Commit Loss: 0.001052 | Perplexity: 2026.417750
2025-09-28 02:46:13,636 Stage: Train 0.5 | Epoch: 219 | Iter: 333800 | Total Loss: 0.003517 | Recon Loss: 0.002985 | Commit Loss: 0.001063 | Perplexity: 2031.818806
2025-09-28 02:46:40,171 Stage: Train 0.5 | Epoch: 219 | Iter: 334000 | Total Loss: 0.003568 | Recon Loss: 0.003037 | Commit Loss: 0.001062 | Perplexity: 2032.792768
Trainning Epoch:  67%|██████▋   | 220/330 [19:58:09<6:10:27, 202.06s/it]2025-09-28 02:47:06,938 Stage: Train 0.5 | Epoch: 220 | Iter: 334200 | Total Loss: 0.003581 | Recon Loss: 0.003052 | Commit Loss: 0.001058 | Perplexity: 2030.980365
2025-09-28 02:47:33,535 Stage: Train 0.5 | Epoch: 220 | Iter: 334400 | Total Loss: 0.003601 | Recon Loss: 0.003071 | Commit Loss: 0.001061 | Perplexity: 2030.682469
2025-09-28 02:48:00,009 Stage: Train 0.5 | Epoch: 220 | Iter: 334600 | Total Loss: 0.003548 | Recon Loss: 0.003022 | Commit Loss: 0.001052 | Perplexity: 2030.575513
2025-09-28 02:48:26,606 Stage: Train 0.5 | Epoch: 220 | Iter: 334800 | Total Loss: 0.003585 | Recon Loss: 0.003059 | Commit Loss: 0.001053 | Perplexity: 2033.002546
2025-09-28 02:48:53,162 Stage: Train 0.5 | Epoch: 220 | Iter: 335000 | Total Loss: 0.003656 | Recon Loss: 0.003132 | Commit Loss: 0.001048 | Perplexity: 2032.781228
2025-09-28 02:49:19,735 Stage: Train 0.5 | Epoch: 220 | Iter: 335200 | Total Loss: 0.003489 | Recon Loss: 0.002964 | Commit Loss: 0.001050 | Perplexity: 2030.012490
2025-09-28 02:49:46,273 Stage: Train 0.5 | Epoch: 220 | Iter: 335400 | Total Loss: 0.003593 | Recon Loss: 0.003063 | Commit Loss: 0.001060 | Perplexity: 2029.784418
2025-09-28 02:50:12,820 Stage: Train 0.5 | Epoch: 220 | Iter: 335600 | Total Loss: 0.003567 | Recon Loss: 0.003038 | Commit Loss: 0.001057 | Perplexity: 2029.980217
Trainning Epoch:  67%|██████▋   | 221/330 [20:01:31<6:07:00, 202.03s/it]2025-09-28 02:50:39,598 Stage: Train 0.5 | Epoch: 221 | Iter: 335800 | Total Loss: 0.003565 | Recon Loss: 0.003039 | Commit Loss: 0.001051 | Perplexity: 2033.435836
2025-09-28 02:51:06,100 Stage: Train 0.5 | Epoch: 221 | Iter: 336000 | Total Loss: 0.003551 | Recon Loss: 0.003023 | Commit Loss: 0.001057 | Perplexity: 2032.709120
2025-09-28 02:51:32,804 Stage: Train 0.5 | Epoch: 221 | Iter: 336200 | Total Loss: 0.003585 | Recon Loss: 0.003055 | Commit Loss: 0.001060 | Perplexity: 2040.505989
2025-09-28 02:51:59,348 Stage: Train 0.5 | Epoch: 221 | Iter: 336400 | Total Loss: 0.003554 | Recon Loss: 0.003027 | Commit Loss: 0.001054 | Perplexity: 2025.463636
2025-09-28 02:52:25,953 Stage: Train 0.5 | Epoch: 221 | Iter: 336600 | Total Loss: 0.003613 | Recon Loss: 0.003088 | Commit Loss: 0.001048 | Perplexity: 2027.133179
2025-09-28 02:52:52,420 Stage: Train 0.5 | Epoch: 221 | Iter: 336800 | Total Loss: 0.003611 | Recon Loss: 0.003084 | Commit Loss: 0.001056 | Perplexity: 2033.059662
2025-09-28 02:53:19,062 Stage: Train 0.5 | Epoch: 221 | Iter: 337000 | Total Loss: 0.003481 | Recon Loss: 0.002954 | Commit Loss: 0.001053 | Perplexity: 2029.023311
2025-09-28 02:53:45,586 Stage: Train 0.5 | Epoch: 221 | Iter: 337200 | Total Loss: 0.003621 | Recon Loss: 0.003095 | Commit Loss: 0.001051 | Perplexity: 2029.352573
Trainning Epoch:  67%|██████▋   | 222/330 [20:04:53<6:03:37, 202.01s/it]2025-09-28 02:54:12,340 Stage: Train 0.5 | Epoch: 222 | Iter: 337400 | Total Loss: 0.003494 | Recon Loss: 0.002972 | Commit Loss: 0.001044 | Perplexity: 2030.764550
2025-09-28 02:54:38,982 Stage: Train 0.5 | Epoch: 222 | Iter: 337600 | Total Loss: 0.003596 | Recon Loss: 0.003070 | Commit Loss: 0.001054 | Perplexity: 2025.207214
2025-09-28 02:55:05,457 Stage: Train 0.5 | Epoch: 222 | Iter: 337800 | Total Loss: 0.003547 | Recon Loss: 0.003022 | Commit Loss: 0.001050 | Perplexity: 2030.437704
2025-09-28 02:55:32,068 Stage: Train 0.5 | Epoch: 222 | Iter: 338000 | Total Loss: 0.003546 | Recon Loss: 0.003017 | Commit Loss: 0.001058 | Perplexity: 2030.064965
2025-09-28 02:55:58,577 Stage: Train 0.5 | Epoch: 222 | Iter: 338200 | Total Loss: 0.003596 | Recon Loss: 0.003067 | Commit Loss: 0.001059 | Perplexity: 2033.483315
2025-09-28 02:56:25,146 Stage: Train 0.5 | Epoch: 222 | Iter: 338400 | Total Loss: 0.003530 | Recon Loss: 0.003005 | Commit Loss: 0.001049 | Perplexity: 2030.479631
2025-09-28 02:56:51,727 Stage: Train 0.5 | Epoch: 222 | Iter: 338600 | Total Loss: 0.003493 | Recon Loss: 0.002966 | Commit Loss: 0.001053 | Perplexity: 2035.138862
Trainning Epoch:  68%|██████▊   | 223/330 [20:08:14<6:00:11, 201.98s/it]2025-09-28 02:57:18,455 Stage: Train 0.5 | Epoch: 223 | Iter: 338800 | Total Loss: 0.003521 | Recon Loss: 0.002996 | Commit Loss: 0.001049 | Perplexity: 2028.678814
2025-09-28 02:57:44,907 Stage: Train 0.5 | Epoch: 223 | Iter: 339000 | Total Loss: 0.003518 | Recon Loss: 0.002994 | Commit Loss: 0.001048 | Perplexity: 2031.924543
2025-09-28 02:58:11,672 Stage: Train 0.5 | Epoch: 223 | Iter: 339200 | Total Loss: 0.003574 | Recon Loss: 0.003048 | Commit Loss: 0.001052 | Perplexity: 2033.390759
2025-09-28 02:58:38,259 Stage: Train 0.5 | Epoch: 223 | Iter: 339400 | Total Loss: 0.003528 | Recon Loss: 0.003002 | Commit Loss: 0.001052 | Perplexity: 2028.506586
2025-09-28 02:59:04,990 Stage: Train 0.5 | Epoch: 223 | Iter: 339600 | Total Loss: 0.003595 | Recon Loss: 0.003069 | Commit Loss: 0.001052 | Perplexity: 2033.424548
2025-09-28 02:59:31,636 Stage: Train 0.5 | Epoch: 223 | Iter: 339800 | Total Loss: 0.003521 | Recon Loss: 0.002995 | Commit Loss: 0.001052 | Perplexity: 2030.841937
2025-09-28 02:59:58,240 Stage: Train 0.5 | Epoch: 223 | Iter: 340000 | Total Loss: 0.003597 | Recon Loss: 0.003073 | Commit Loss: 0.001048 | Perplexity: 2029.038063
2025-09-28 02:59:58,240 Saving model at iteration 340000
2025-09-28 02:59:58,430 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000
2025-09-28 02:59:58,892 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/model.safetensors
2025-09-28 02:59:59,361 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/optimizer.bin
2025-09-28 02:59:59,362 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/scheduler.bin
2025-09-28 02:59:59,362 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/sampler.bin
2025-09-28 02:59:59,363 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/random_states_0.pkl
2025-09-28 03:00:25,983 Stage: Train 0.5 | Epoch: 223 | Iter: 340200 | Total Loss: 0.003528 | Recon Loss: 0.002998 | Commit Loss: 0.001059 | Perplexity: 2032.962259
Trainning Epoch:  68%|██████▊   | 224/330 [20:11:38<5:57:42, 202.47s/it]2025-09-28 03:00:52,869 Stage: Train 0.5 | Epoch: 224 | Iter: 340400 | Total Loss: 0.003569 | Recon Loss: 0.003043 | Commit Loss: 0.001051 | Perplexity: 2031.421784
2025-09-28 03:01:19,527 Stage: Train 0.5 | Epoch: 224 | Iter: 340600 | Total Loss: 0.003527 | Recon Loss: 0.003004 | Commit Loss: 0.001047 | Perplexity: 2031.288931
2025-09-28 03:01:46,140 Stage: Train 0.5 | Epoch: 224 | Iter: 340800 | Total Loss: 0.003508 | Recon Loss: 0.002983 | Commit Loss: 0.001049 | Perplexity: 2035.886812
2025-09-28 03:02:12,640 Stage: Train 0.5 | Epoch: 224 | Iter: 341000 | Total Loss: 0.003585 | Recon Loss: 0.003059 | Commit Loss: 0.001052 | Perplexity: 2031.549185
2025-09-28 03:02:39,153 Stage: Train 0.5 | Epoch: 224 | Iter: 341200 | Total Loss: 0.003595 | Recon Loss: 0.003072 | Commit Loss: 0.001047 | Perplexity: 2026.384279
2025-09-28 03:03:05,719 Stage: Train 0.5 | Epoch: 224 | Iter: 341400 | Total Loss: 0.003516 | Recon Loss: 0.002992 | Commit Loss: 0.001049 | Perplexity: 2028.631137
2025-09-28 03:03:32,256 Stage: Train 0.5 | Epoch: 224 | Iter: 341600 | Total Loss: 0.003592 | Recon Loss: 0.003069 | Commit Loss: 0.001047 | Perplexity: 2030.070814
Trainning Epoch:  68%|██████▊   | 225/330 [20:15:00<5:54:06, 202.35s/it]2025-09-28 03:03:59,035 Stage: Train 0.5 | Epoch: 225 | Iter: 341800 | Total Loss: 0.003509 | Recon Loss: 0.002983 | Commit Loss: 0.001051 | Perplexity: 2031.595104
2025-09-28 03:04:25,580 Stage: Train 0.5 | Epoch: 225 | Iter: 342000 | Total Loss: 0.003502 | Recon Loss: 0.002977 | Commit Loss: 0.001051 | Perplexity: 2034.776920
2025-09-28 03:04:52,051 Stage: Train 0.5 | Epoch: 225 | Iter: 342200 | Total Loss: 0.003512 | Recon Loss: 0.002987 | Commit Loss: 0.001051 | Perplexity: 2032.788315
2025-09-28 03:05:18,526 Stage: Train 0.5 | Epoch: 225 | Iter: 342400 | Total Loss: 0.003554 | Recon Loss: 0.003032 | Commit Loss: 0.001043 | Perplexity: 2032.126068
2025-09-28 03:05:45,044 Stage: Train 0.5 | Epoch: 225 | Iter: 342600 | Total Loss: 0.003506 | Recon Loss: 0.002982 | Commit Loss: 0.001048 | Perplexity: 2031.602995
2025-09-28 03:06:11,515 Stage: Train 0.5 | Epoch: 225 | Iter: 342800 | Total Loss: 0.003541 | Recon Loss: 0.003018 | Commit Loss: 0.001047 | Perplexity: 2029.242241
2025-09-28 03:06:38,071 Stage: Train 0.5 | Epoch: 225 | Iter: 343000 | Total Loss: 0.003480 | Recon Loss: 0.002953 | Commit Loss: 0.001053 | Perplexity: 2039.671885
2025-09-28 03:07:04,686 Stage: Train 0.5 | Epoch: 225 | Iter: 343200 | Total Loss: 0.003590 | Recon Loss: 0.003064 | Commit Loss: 0.001052 | Perplexity: 2030.793093
Trainning Epoch:  68%|██████▊   | 226/330 [20:18:22<5:50:21, 202.13s/it]2025-09-28 03:07:31,438 Stage: Train 0.5 | Epoch: 226 | Iter: 343400 | Total Loss: 0.003543 | Recon Loss: 0.003018 | Commit Loss: 0.001050 | Perplexity: 2027.401544
2025-09-28 03:07:58,091 Stage: Train 0.5 | Epoch: 226 | Iter: 343600 | Total Loss: 0.003492 | Recon Loss: 0.002972 | Commit Loss: 0.001040 | Perplexity: 2032.820523
2025-09-28 03:08:24,783 Stage: Train 0.5 | Epoch: 226 | Iter: 343800 | Total Loss: 0.003485 | Recon Loss: 0.002964 | Commit Loss: 0.001043 | Perplexity: 2028.769655
2025-09-28 03:08:51,195 Stage: Train 0.5 | Epoch: 226 | Iter: 344000 | Total Loss: 0.003553 | Recon Loss: 0.003029 | Commit Loss: 0.001050 | Perplexity: 2031.119523
2025-09-28 03:09:17,847 Stage: Train 0.5 | Epoch: 226 | Iter: 344200 | Total Loss: 0.003556 | Recon Loss: 0.003033 | Commit Loss: 0.001045 | Perplexity: 2036.678505
2025-09-28 03:09:44,450 Stage: Train 0.5 | Epoch: 226 | Iter: 344400 | Total Loss: 0.003508 | Recon Loss: 0.002985 | Commit Loss: 0.001044 | Perplexity: 2035.471497
2025-09-28 03:10:11,122 Stage: Train 0.5 | Epoch: 226 | Iter: 344600 | Total Loss: 0.003522 | Recon Loss: 0.002994 | Commit Loss: 0.001056 | Perplexity: 2035.520908
2025-09-28 03:10:37,697 Stage: Train 0.5 | Epoch: 226 | Iter: 344800 | Total Loss: 0.003543 | Recon Loss: 0.003019 | Commit Loss: 0.001048 | Perplexity: 2034.180062
Trainning Epoch:  69%|██████▉   | 227/330 [20:21:44<5:47:02, 202.16s/it]2025-09-28 03:11:04,444 Stage: Train 0.5 | Epoch: 227 | Iter: 345000 | Total Loss: 0.003513 | Recon Loss: 0.002990 | Commit Loss: 0.001047 | Perplexity: 2032.078694
2025-09-28 03:11:30,953 Stage: Train 0.5 | Epoch: 227 | Iter: 345200 | Total Loss: 0.003610 | Recon Loss: 0.003086 | Commit Loss: 0.001047 | Perplexity: 2029.582817
2025-09-28 03:11:57,553 Stage: Train 0.5 | Epoch: 227 | Iter: 345400 | Total Loss: 0.003488 | Recon Loss: 0.002970 | Commit Loss: 0.001037 | Perplexity: 2030.354309
2025-09-28 03:12:24,146 Stage: Train 0.5 | Epoch: 227 | Iter: 345600 | Total Loss: 0.003518 | Recon Loss: 0.002994 | Commit Loss: 0.001048 | Perplexity: 2034.528717
2025-09-28 03:12:50,742 Stage: Train 0.5 | Epoch: 227 | Iter: 345800 | Total Loss: 0.003516 | Recon Loss: 0.002994 | Commit Loss: 0.001044 | Perplexity: 2031.360572
2025-09-28 03:13:17,387 Stage: Train 0.5 | Epoch: 227 | Iter: 346000 | Total Loss: 0.003529 | Recon Loss: 0.003006 | Commit Loss: 0.001046 | Perplexity: 2035.524648
2025-09-28 03:13:43,976 Stage: Train 0.5 | Epoch: 227 | Iter: 346200 | Total Loss: 0.003623 | Recon Loss: 0.003100 | Commit Loss: 0.001047 | Perplexity: 2032.292850
Trainning Epoch:  69%|██████▉   | 228/330 [20:25:06<5:43:38, 202.15s/it]2025-09-28 03:14:10,789 Stage: Train 0.5 | Epoch: 228 | Iter: 346400 | Total Loss: 0.003466 | Recon Loss: 0.002940 | Commit Loss: 0.001051 | Perplexity: 2037.588530
2025-09-28 03:14:37,263 Stage: Train 0.5 | Epoch: 228 | Iter: 346600 | Total Loss: 0.003513 | Recon Loss: 0.002993 | Commit Loss: 0.001039 | Perplexity: 2033.864265
2025-09-28 03:15:03,644 Stage: Train 0.5 | Epoch: 228 | Iter: 346800 | Total Loss: 0.003573 | Recon Loss: 0.003047 | Commit Loss: 0.001053 | Perplexity: 2036.053022
2025-09-28 03:15:30,248 Stage: Train 0.5 | Epoch: 228 | Iter: 347000 | Total Loss: 0.003511 | Recon Loss: 0.002989 | Commit Loss: 0.001045 | Perplexity: 2031.951085
2025-09-28 03:15:56,905 Stage: Train 0.5 | Epoch: 228 | Iter: 347200 | Total Loss: 0.003494 | Recon Loss: 0.002972 | Commit Loss: 0.001043 | Perplexity: 2029.798259
2025-09-28 03:16:23,518 Stage: Train 0.5 | Epoch: 228 | Iter: 347400 | Total Loss: 0.003528 | Recon Loss: 0.003005 | Commit Loss: 0.001046 | Perplexity: 2032.392118
2025-09-28 03:16:50,088 Stage: Train 0.5 | Epoch: 228 | Iter: 347600 | Total Loss: 0.003529 | Recon Loss: 0.003003 | Commit Loss: 0.001052 | Perplexity: 2035.276423
2025-09-28 03:17:16,676 Stage: Train 0.5 | Epoch: 228 | Iter: 347800 | Total Loss: 0.003553 | Recon Loss: 0.003031 | Commit Loss: 0.001044 | Perplexity: 2024.438949
Trainning Epoch:  69%|██████▉   | 229/330 [20:28:28<5:40:08, 202.07s/it]2025-09-28 03:17:43,327 Stage: Train 0.5 | Epoch: 229 | Iter: 348000 | Total Loss: 0.003487 | Recon Loss: 0.002967 | Commit Loss: 0.001039 | Perplexity: 2034.059034
2025-09-28 03:18:09,840 Stage: Train 0.5 | Epoch: 229 | Iter: 348200 | Total Loss: 0.003521 | Recon Loss: 0.003003 | Commit Loss: 0.001037 | Perplexity: 2036.165290
2025-09-28 03:18:36,466 Stage: Train 0.5 | Epoch: 229 | Iter: 348400 | Total Loss: 0.003472 | Recon Loss: 0.002950 | Commit Loss: 0.001044 | Perplexity: 2027.734279
2025-09-28 03:19:02,975 Stage: Train 0.5 | Epoch: 229 | Iter: 348600 | Total Loss: 0.003635 | Recon Loss: 0.003114 | Commit Loss: 0.001042 | Perplexity: 2030.029158
2025-09-28 03:19:29,572 Stage: Train 0.5 | Epoch: 229 | Iter: 348800 | Total Loss: 0.003465 | Recon Loss: 0.002947 | Commit Loss: 0.001037 | Perplexity: 2027.894485
2025-09-28 03:19:56,172 Stage: Train 0.5 | Epoch: 229 | Iter: 349000 | Total Loss: 0.003659 | Recon Loss: 0.003136 | Commit Loss: 0.001046 | Perplexity: 2033.679484
2025-09-28 03:20:22,721 Stage: Train 0.5 | Epoch: 229 | Iter: 349200 | Total Loss: 0.003469 | Recon Loss: 0.002947 | Commit Loss: 0.001043 | Perplexity: 2031.945790
Trainning Epoch:  70%|██████▉   | 230/330 [20:31:50<5:36:39, 201.99s/it]2025-09-28 03:20:49,408 Stage: Train 0.5 | Epoch: 230 | Iter: 349400 | Total Loss: 0.003521 | Recon Loss: 0.002997 | Commit Loss: 0.001048 | Perplexity: 2033.441461
2025-09-28 03:21:16,030 Stage: Train 0.5 | Epoch: 230 | Iter: 349600 | Total Loss: 0.003515 | Recon Loss: 0.002995 | Commit Loss: 0.001038 | Perplexity: 2032.407924
2025-09-28 03:21:42,634 Stage: Train 0.5 | Epoch: 230 | Iter: 349800 | Total Loss: 0.003459 | Recon Loss: 0.002939 | Commit Loss: 0.001040 | Perplexity: 2032.416942
2025-09-28 03:22:09,083 Stage: Train 0.5 | Epoch: 230 | Iter: 350000 | Total Loss: 0.003574 | Recon Loss: 0.003055 | Commit Loss: 0.001038 | Perplexity: 2028.916202
2025-09-28 03:22:35,701 Stage: Train 0.5 | Epoch: 230 | Iter: 350200 | Total Loss: 0.003477 | Recon Loss: 0.002956 | Commit Loss: 0.001042 | Perplexity: 2035.302714
2025-09-28 03:23:02,080 Stage: Train 0.5 | Epoch: 230 | Iter: 350400 | Total Loss: 0.003609 | Recon Loss: 0.003086 | Commit Loss: 0.001047 | Perplexity: 2031.585042
2025-09-28 03:23:28,538 Stage: Train 0.5 | Epoch: 230 | Iter: 350600 | Total Loss: 0.003483 | Recon Loss: 0.002963 | Commit Loss: 0.001039 | Perplexity: 2035.657813
2025-09-28 03:23:55,035 Stage: Train 0.5 | Epoch: 230 | Iter: 350800 | Total Loss: 0.003511 | Recon Loss: 0.002989 | Commit Loss: 0.001044 | Perplexity: 2035.283642
Trainning Epoch:  70%|███████   | 231/330 [20:35:11<5:33:06, 201.88s/it]2025-09-28 03:24:21,845 Stage: Train 0.5 | Epoch: 231 | Iter: 351000 | Total Loss: 0.003514 | Recon Loss: 0.002991 | Commit Loss: 0.001046 | Perplexity: 2034.630625
2025-09-28 03:24:48,411 Stage: Train 0.5 | Epoch: 231 | Iter: 351200 | Total Loss: 0.003546 | Recon Loss: 0.003026 | Commit Loss: 0.001041 | Perplexity: 2034.352986
2025-09-28 03:25:15,025 Stage: Train 0.5 | Epoch: 231 | Iter: 351400 | Total Loss: 0.003568 | Recon Loss: 0.003048 | Commit Loss: 0.001040 | Perplexity: 2033.308693
2025-09-28 03:25:41,713 Stage: Train 0.5 | Epoch: 231 | Iter: 351600 | Total Loss: 0.003452 | Recon Loss: 0.002933 | Commit Loss: 0.001037 | Perplexity: 2035.498352
2025-09-28 03:26:08,388 Stage: Train 0.5 | Epoch: 231 | Iter: 351800 | Total Loss: 0.003481 | Recon Loss: 0.002961 | Commit Loss: 0.001040 | Perplexity: 2031.008416
2025-09-28 03:26:35,045 Stage: Train 0.5 | Epoch: 231 | Iter: 352000 | Total Loss: 0.003476 | Recon Loss: 0.002956 | Commit Loss: 0.001039 | Perplexity: 2034.170924
2025-09-28 03:27:01,680 Stage: Train 0.5 | Epoch: 231 | Iter: 352200 | Total Loss: 0.003590 | Recon Loss: 0.003071 | Commit Loss: 0.001039 | Perplexity: 2027.959777
2025-09-28 03:27:28,235 Stage: Train 0.5 | Epoch: 231 | Iter: 352400 | Total Loss: 0.003493 | Recon Loss: 0.002972 | Commit Loss: 0.001043 | Perplexity: 2034.058287
Trainning Epoch:  70%|███████   | 232/330 [20:38:34<5:30:00, 202.05s/it]2025-09-28 03:27:55,122 Stage: Train 0.5 | Epoch: 232 | Iter: 352600 | Total Loss: 0.003495 | Recon Loss: 0.002975 | Commit Loss: 0.001041 | Perplexity: 2031.354537
2025-09-28 03:28:21,711 Stage: Train 0.5 | Epoch: 232 | Iter: 352800 | Total Loss: 0.003755 | Recon Loss: 0.003238 | Commit Loss: 0.001035 | Perplexity: 2023.997941
2025-09-28 03:28:48,389 Stage: Train 0.5 | Epoch: 232 | Iter: 353000 | Total Loss: 0.003426 | Recon Loss: 0.002906 | Commit Loss: 0.001040 | Perplexity: 2038.616594
2025-09-28 03:29:15,003 Stage: Train 0.5 | Epoch: 232 | Iter: 353200 | Total Loss: 0.003521 | Recon Loss: 0.003005 | Commit Loss: 0.001033 | Perplexity: 2029.512729
2025-09-28 03:29:41,664 Stage: Train 0.5 | Epoch: 232 | Iter: 353400 | Total Loss: 0.003597 | Recon Loss: 0.003073 | Commit Loss: 0.001048 | Perplexity: 2031.403235
2025-09-28 03:30:08,219 Stage: Train 0.5 | Epoch: 232 | Iter: 353600 | Total Loss: 0.003487 | Recon Loss: 0.002971 | Commit Loss: 0.001033 | Perplexity: 2036.806342
2025-09-28 03:30:34,756 Stage: Train 0.5 | Epoch: 232 | Iter: 353800 | Total Loss: 0.003525 | Recon Loss: 0.003006 | Commit Loss: 0.001037 | Perplexity: 2035.343292
Trainning Epoch:  71%|███████   | 233/330 [20:41:56<5:26:47, 202.14s/it]2025-09-28 03:31:01,536 Stage: Train 0.5 | Epoch: 233 | Iter: 354000 | Total Loss: 0.003504 | Recon Loss: 0.002985 | Commit Loss: 0.001038 | Perplexity: 2030.624992
2025-09-28 03:31:28,096 Stage: Train 0.5 | Epoch: 233 | Iter: 354200 | Total Loss: 0.003610 | Recon Loss: 0.003095 | Commit Loss: 0.001030 | Perplexity: 2032.058652
2025-09-28 03:31:54,758 Stage: Train 0.5 | Epoch: 233 | Iter: 354400 | Total Loss: 0.003478 | Recon Loss: 0.002960 | Commit Loss: 0.001036 | Perplexity: 2033.977421
2025-09-28 03:32:21,379 Stage: Train 0.5 | Epoch: 233 | Iter: 354600 | Total Loss: 0.003446 | Recon Loss: 0.002929 | Commit Loss: 0.001034 | Perplexity: 2038.008776
2025-09-28 03:32:47,967 Stage: Train 0.5 | Epoch: 233 | Iter: 354800 | Total Loss: 0.003507 | Recon Loss: 0.002984 | Commit Loss: 0.001046 | Perplexity: 2034.265984
2025-09-28 03:33:14,609 Stage: Train 0.5 | Epoch: 233 | Iter: 355000 | Total Loss: 0.003473 | Recon Loss: 0.002951 | Commit Loss: 0.001044 | Perplexity: 2034.817318
2025-09-28 03:33:41,231 Stage: Train 0.5 | Epoch: 233 | Iter: 355200 | Total Loss: 0.003546 | Recon Loss: 0.003028 | Commit Loss: 0.001035 | Perplexity: 2034.151531
2025-09-28 03:34:07,620 Stage: Train 0.5 | Epoch: 233 | Iter: 355400 | Total Loss: 0.003421 | Recon Loss: 0.002900 | Commit Loss: 0.001041 | Perplexity: 2039.483142
Trainning Epoch:  71%|███████   | 234/330 [20:45:18<5:23:25, 202.14s/it]2025-09-28 03:34:34,392 Stage: Train 0.5 | Epoch: 234 | Iter: 355600 | Total Loss: 0.003479 | Recon Loss: 0.002964 | Commit Loss: 0.001032 | Perplexity: 2039.862136
2025-09-28 03:35:00,915 Stage: Train 0.5 | Epoch: 234 | Iter: 355800 | Total Loss: 0.003482 | Recon Loss: 0.002967 | Commit Loss: 0.001030 | Perplexity: 2036.054158
2025-09-28 03:35:27,464 Stage: Train 0.5 | Epoch: 234 | Iter: 356000 | Total Loss: 0.003528 | Recon Loss: 0.003011 | Commit Loss: 0.001033 | Perplexity: 2028.915436
2025-09-28 03:35:54,132 Stage: Train 0.5 | Epoch: 234 | Iter: 356200 | Total Loss: 0.003492 | Recon Loss: 0.002975 | Commit Loss: 0.001033 | Perplexity: 2032.340405
2025-09-28 03:36:20,581 Stage: Train 0.5 | Epoch: 234 | Iter: 356400 | Total Loss: 0.003681 | Recon Loss: 0.003164 | Commit Loss: 0.001033 | Perplexity: 2030.156662
2025-09-28 03:36:47,036 Stage: Train 0.5 | Epoch: 234 | Iter: 356600 | Total Loss: 0.003463 | Recon Loss: 0.002944 | Commit Loss: 0.001038 | Perplexity: 2034.485416
2025-09-28 03:37:13,733 Stage: Train 0.5 | Epoch: 234 | Iter: 356800 | Total Loss: 0.003426 | Recon Loss: 0.002906 | Commit Loss: 0.001040 | Perplexity: 2031.407512
Trainning Epoch:  71%|███████   | 235/330 [20:48:40<5:19:55, 202.06s/it]2025-09-28 03:37:40,514 Stage: Train 0.5 | Epoch: 235 | Iter: 357000 | Total Loss: 0.003527 | Recon Loss: 0.003007 | Commit Loss: 0.001040 | Perplexity: 2038.188229
2025-09-28 03:38:07,103 Stage: Train 0.5 | Epoch: 235 | Iter: 357200 | Total Loss: 0.003534 | Recon Loss: 0.003019 | Commit Loss: 0.001030 | Perplexity: 2034.361702
2025-09-28 03:38:33,738 Stage: Train 0.5 | Epoch: 235 | Iter: 357400 | Total Loss: 0.003499 | Recon Loss: 0.002979 | Commit Loss: 0.001039 | Perplexity: 2034.339753
2025-09-28 03:39:00,353 Stage: Train 0.5 | Epoch: 235 | Iter: 357600 | Total Loss: 0.003480 | Recon Loss: 0.002964 | Commit Loss: 0.001033 | Perplexity: 2034.863510
2025-09-28 03:39:26,987 Stage: Train 0.5 | Epoch: 235 | Iter: 357800 | Total Loss: 0.003570 | Recon Loss: 0.003052 | Commit Loss: 0.001035 | Perplexity: 2034.375217
2025-09-28 03:39:53,607 Stage: Train 0.5 | Epoch: 235 | Iter: 358000 | Total Loss: 0.003393 | Recon Loss: 0.002879 | Commit Loss: 0.001028 | Perplexity: 2028.978137
2025-09-28 03:40:20,197 Stage: Train 0.5 | Epoch: 235 | Iter: 358200 | Total Loss: 0.003528 | Recon Loss: 0.003011 | Commit Loss: 0.001035 | Perplexity: 2035.456923
2025-09-28 03:40:46,871 Stage: Train 0.5 | Epoch: 235 | Iter: 358400 | Total Loss: 0.003511 | Recon Loss: 0.002993 | Commit Loss: 0.001037 | Perplexity: 2034.316022
Trainning Epoch:  72%|███████▏  | 236/330 [20:52:03<5:16:41, 202.14s/it]2025-09-28 03:41:13,570 Stage: Train 0.5 | Epoch: 236 | Iter: 358600 | Total Loss: 0.003480 | Recon Loss: 0.002966 | Commit Loss: 0.001029 | Perplexity: 2031.456800
2025-09-28 03:41:40,244 Stage: Train 0.5 | Epoch: 236 | Iter: 358800 | Total Loss: 0.003513 | Recon Loss: 0.003003 | Commit Loss: 0.001020 | Perplexity: 2024.683849
2025-09-28 03:42:06,849 Stage: Train 0.5 | Epoch: 236 | Iter: 359000 | Total Loss: 0.003495 | Recon Loss: 0.002978 | Commit Loss: 0.001034 | Perplexity: 2033.019973
2025-09-28 03:42:33,419 Stage: Train 0.5 | Epoch: 236 | Iter: 359200 | Total Loss: 0.003549 | Recon Loss: 0.003035 | Commit Loss: 0.001028 | Perplexity: 2030.364428
2025-09-28 03:43:00,036 Stage: Train 0.5 | Epoch: 236 | Iter: 359400 | Total Loss: 0.003490 | Recon Loss: 0.002973 | Commit Loss: 0.001035 | Perplexity: 2031.511871
2025-09-28 03:43:26,648 Stage: Train 0.5 | Epoch: 236 | Iter: 359600 | Total Loss: 0.003573 | Recon Loss: 0.003052 | Commit Loss: 0.001043 | Perplexity: 2036.457115
2025-09-28 03:43:53,188 Stage: Train 0.5 | Epoch: 236 | Iter: 359800 | Total Loss: 0.003464 | Recon Loss: 0.002948 | Commit Loss: 0.001033 | Perplexity: 2036.035535
2025-09-28 03:44:19,641 Stage: Train 0.5 | Epoch: 236 | Iter: 360000 | Total Loss: 0.003553 | Recon Loss: 0.003036 | Commit Loss: 0.001035 | Perplexity: 2038.178021
2025-09-28 03:44:19,641 Saving model at iteration 360000
2025-09-28 03:44:20,023 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000
2025-09-28 03:44:20,525 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/model.safetensors
2025-09-28 03:44:21,102 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/optimizer.bin
2025-09-28 03:44:21,103 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/scheduler.bin
2025-09-28 03:44:21,103 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/sampler.bin
2025-09-28 03:44:21,104 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/random_states_0.pkl
Trainning Epoch:  72%|███████▏  | 237/330 [20:55:26<5:13:57, 202.56s/it]2025-09-28 03:44:48,132 Stage: Train 0.5 | Epoch: 237 | Iter: 360200 | Total Loss: 0.003489 | Recon Loss: 0.002974 | Commit Loss: 0.001029 | Perplexity: 2036.518852
2025-09-28 03:45:14,601 Stage: Train 0.5 | Epoch: 237 | Iter: 360400 | Total Loss: 0.003433 | Recon Loss: 0.002919 | Commit Loss: 0.001028 | Perplexity: 2034.896999
2025-09-28 03:45:41,208 Stage: Train 0.5 | Epoch: 237 | Iter: 360600 | Total Loss: 0.003476 | Recon Loss: 0.002955 | Commit Loss: 0.001042 | Perplexity: 2039.897863
2025-09-28 03:46:07,732 Stage: Train 0.5 | Epoch: 237 | Iter: 360800 | Total Loss: 0.003412 | Recon Loss: 0.002897 | Commit Loss: 0.001029 | Perplexity: 2032.486199
2025-09-28 03:46:34,399 Stage: Train 0.5 | Epoch: 237 | Iter: 361000 | Total Loss: 0.003470 | Recon Loss: 0.002955 | Commit Loss: 0.001030 | Perplexity: 2035.843893
2025-09-28 03:47:00,988 Stage: Train 0.5 | Epoch: 237 | Iter: 361200 | Total Loss: 0.003525 | Recon Loss: 0.003011 | Commit Loss: 0.001028 | Perplexity: 2030.747610
2025-09-28 03:47:27,724 Stage: Train 0.5 | Epoch: 237 | Iter: 361400 | Total Loss: 0.003456 | Recon Loss: 0.002941 | Commit Loss: 0.001031 | Perplexity: 2032.928401
Trainning Epoch:  72%|███████▏  | 238/330 [20:58:48<5:10:32, 202.52s/it]2025-09-28 03:47:54,612 Stage: Train 0.5 | Epoch: 238 | Iter: 361600 | Total Loss: 0.003450 | Recon Loss: 0.002934 | Commit Loss: 0.001031 | Perplexity: 2035.078824
2025-09-28 03:48:21,266 Stage: Train 0.5 | Epoch: 238 | Iter: 361800 | Total Loss: 0.003482 | Recon Loss: 0.002969 | Commit Loss: 0.001027 | Perplexity: 2032.513670
2025-09-28 03:48:47,918 Stage: Train 0.5 | Epoch: 238 | Iter: 362000 | Total Loss: 0.003483 | Recon Loss: 0.002968 | Commit Loss: 0.001029 | Perplexity: 2037.221467
2025-09-28 03:49:14,548 Stage: Train 0.5 | Epoch: 238 | Iter: 362200 | Total Loss: 0.003568 | Recon Loss: 0.003056 | Commit Loss: 0.001025 | Perplexity: 2031.282334
2025-09-28 03:49:41,169 Stage: Train 0.5 | Epoch: 238 | Iter: 362400 | Total Loss: 0.003402 | Recon Loss: 0.002886 | Commit Loss: 0.001032 | Perplexity: 2032.572195
2025-09-28 03:50:07,599 Stage: Train 0.5 | Epoch: 238 | Iter: 362600 | Total Loss: 0.003527 | Recon Loss: 0.003011 | Commit Loss: 0.001033 | Perplexity: 2037.647334
2025-09-28 03:50:34,071 Stage: Train 0.5 | Epoch: 238 | Iter: 362800 | Total Loss: 0.003451 | Recon Loss: 0.002937 | Commit Loss: 0.001027 | Perplexity: 2034.283350
2025-09-28 03:51:00,691 Stage: Train 0.5 | Epoch: 238 | Iter: 363000 | Total Loss: 0.003469 | Recon Loss: 0.002955 | Commit Loss: 0.001029 | Perplexity: 2029.306748
Trainning Epoch:  72%|███████▏  | 239/330 [21:02:11<5:07:00, 202.42s/it]2025-09-28 03:51:27,403 Stage: Train 0.5 | Epoch: 239 | Iter: 363200 | Total Loss: 0.003529 | Recon Loss: 0.003016 | Commit Loss: 0.001026 | Perplexity: 2029.802990
2025-09-28 03:51:54,073 Stage: Train 0.5 | Epoch: 239 | Iter: 363400 | Total Loss: 0.003407 | Recon Loss: 0.002894 | Commit Loss: 0.001027 | Perplexity: 2036.923218
2025-09-28 03:52:20,553 Stage: Train 0.5 | Epoch: 239 | Iter: 363600 | Total Loss: 0.003436 | Recon Loss: 0.002922 | Commit Loss: 0.001029 | Perplexity: 2031.293066
2025-09-28 03:52:47,209 Stage: Train 0.5 | Epoch: 239 | Iter: 363800 | Total Loss: 0.003592 | Recon Loss: 0.003079 | Commit Loss: 0.001028 | Perplexity: 2036.283004
2025-09-28 03:53:13,686 Stage: Train 0.5 | Epoch: 239 | Iter: 364000 | Total Loss: 0.003428 | Recon Loss: 0.002916 | Commit Loss: 0.001025 | Perplexity: 2032.267487
2025-09-28 03:53:40,228 Stage: Train 0.5 | Epoch: 239 | Iter: 364200 | Total Loss: 0.003423 | Recon Loss: 0.002910 | Commit Loss: 0.001026 | Perplexity: 2036.399447
2025-09-28 03:54:06,871 Stage: Train 0.5 | Epoch: 239 | Iter: 364400 | Total Loss: 0.003678 | Recon Loss: 0.003162 | Commit Loss: 0.001033 | Perplexity: 2035.086407
Trainning Epoch:  73%|███████▎  | 240/330 [21:05:33<5:03:27, 202.31s/it]2025-09-28 03:54:33,717 Stage: Train 0.5 | Epoch: 240 | Iter: 364600 | Total Loss: 0.003364 | Recon Loss: 0.002848 | Commit Loss: 0.001031 | Perplexity: 2035.356735
2025-09-28 03:55:00,287 Stage: Train 0.5 | Epoch: 240 | Iter: 364800 | Total Loss: 0.003481 | Recon Loss: 0.002967 | Commit Loss: 0.001029 | Perplexity: 2035.451022
2025-09-28 03:55:26,910 Stage: Train 0.5 | Epoch: 240 | Iter: 365000 | Total Loss: 0.003589 | Recon Loss: 0.003072 | Commit Loss: 0.001035 | Perplexity: 2039.736844
2025-09-28 03:55:53,483 Stage: Train 0.5 | Epoch: 240 | Iter: 365200 | Total Loss: 0.003378 | Recon Loss: 0.002863 | Commit Loss: 0.001029 | Perplexity: 2036.692695
2025-09-28 03:56:19,996 Stage: Train 0.5 | Epoch: 240 | Iter: 365400 | Total Loss: 0.003443 | Recon Loss: 0.002929 | Commit Loss: 0.001028 | Perplexity: 2037.865167
2025-09-28 03:56:46,553 Stage: Train 0.5 | Epoch: 240 | Iter: 365600 | Total Loss: 0.003487 | Recon Loss: 0.002973 | Commit Loss: 0.001027 | Perplexity: 2033.578132
2025-09-28 03:57:13,125 Stage: Train 0.5 | Epoch: 240 | Iter: 365800 | Total Loss: 0.003483 | Recon Loss: 0.002970 | Commit Loss: 0.001026 | Perplexity: 2030.841020
2025-09-28 03:57:39,733 Stage: Train 0.5 | Epoch: 240 | Iter: 366000 | Total Loss: 0.003487 | Recon Loss: 0.002976 | Commit Loss: 0.001022 | Perplexity: 2032.072741
Trainning Epoch:  73%|███████▎  | 241/330 [21:08:55<4:59:59, 202.24s/it]2025-09-28 03:58:06,596 Stage: Train 0.5 | Epoch: 241 | Iter: 366200 | Total Loss: 0.003436 | Recon Loss: 0.002927 | Commit Loss: 0.001017 | Perplexity: 2027.928955
2025-09-28 03:58:33,198 Stage: Train 0.5 | Epoch: 241 | Iter: 366400 | Total Loss: 0.003466 | Recon Loss: 0.002956 | Commit Loss: 0.001021 | Perplexity: 2034.944640
2025-09-28 03:58:59,789 Stage: Train 0.5 | Epoch: 241 | Iter: 366600 | Total Loss: 0.003442 | Recon Loss: 0.002930 | Commit Loss: 0.001025 | Perplexity: 2034.678704
2025-09-28 03:59:26,424 Stage: Train 0.5 | Epoch: 241 | Iter: 366800 | Total Loss: 0.003465 | Recon Loss: 0.002954 | Commit Loss: 0.001023 | Perplexity: 2036.430264
2025-09-28 03:59:53,043 Stage: Train 0.5 | Epoch: 241 | Iter: 367000 | Total Loss: 0.003577 | Recon Loss: 0.003060 | Commit Loss: 0.001033 | Perplexity: 2035.875902
2025-09-28 04:00:19,568 Stage: Train 0.5 | Epoch: 241 | Iter: 367200 | Total Loss: 0.003432 | Recon Loss: 0.002921 | Commit Loss: 0.001023 | Perplexity: 2040.518889
2025-09-28 04:00:46,205 Stage: Train 0.5 | Epoch: 241 | Iter: 367400 | Total Loss: 0.003520 | Recon Loss: 0.003007 | Commit Loss: 0.001027 | Perplexity: 2034.409700
Trainning Epoch:  73%|███████▎  | 242/330 [21:12:17<4:56:38, 202.26s/it]2025-09-28 04:01:13,055 Stage: Train 0.5 | Epoch: 242 | Iter: 367600 | Total Loss: 0.003435 | Recon Loss: 0.002921 | Commit Loss: 0.001027 | Perplexity: 2035.370663
2025-09-28 04:01:39,580 Stage: Train 0.5 | Epoch: 242 | Iter: 367800 | Total Loss: 0.003421 | Recon Loss: 0.002907 | Commit Loss: 0.001028 | Perplexity: 2040.591410
2025-09-28 04:02:06,216 Stage: Train 0.5 | Epoch: 242 | Iter: 368000 | Total Loss: 0.003422 | Recon Loss: 0.002909 | Commit Loss: 0.001026 | Perplexity: 2037.357327
2025-09-28 04:02:32,847 Stage: Train 0.5 | Epoch: 242 | Iter: 368200 | Total Loss: 0.003531 | Recon Loss: 0.003017 | Commit Loss: 0.001028 | Perplexity: 2034.567735
2025-09-28 04:02:59,418 Stage: Train 0.5 | Epoch: 242 | Iter: 368400 | Total Loss: 0.003424 | Recon Loss: 0.002913 | Commit Loss: 0.001021 | Perplexity: 2034.886158
2025-09-28 04:03:25,943 Stage: Train 0.5 | Epoch: 242 | Iter: 368600 | Total Loss: 0.003443 | Recon Loss: 0.002929 | Commit Loss: 0.001029 | Perplexity: 2034.287276
2025-09-28 04:03:52,659 Stage: Train 0.5 | Epoch: 242 | Iter: 368800 | Total Loss: 0.003439 | Recon Loss: 0.002927 | Commit Loss: 0.001023 | Perplexity: 2040.129062
2025-09-28 04:04:19,022 Stage: Train 0.5 | Epoch: 242 | Iter: 369000 | Total Loss: 0.003522 | Recon Loss: 0.003016 | Commit Loss: 0.001012 | Perplexity: 2028.929171
Trainning Epoch:  74%|███████▎  | 243/330 [21:15:39<4:53:10, 202.19s/it]2025-09-28 04:04:45,877 Stage: Train 0.5 | Epoch: 243 | Iter: 369200 | Total Loss: 0.003567 | Recon Loss: 0.003057 | Commit Loss: 0.001021 | Perplexity: 2032.662104
2025-09-28 04:05:12,452 Stage: Train 0.5 | Epoch: 243 | Iter: 369400 | Total Loss: 0.003401 | Recon Loss: 0.002893 | Commit Loss: 0.001017 | Perplexity: 2035.338030
2025-09-28 04:05:39,065 Stage: Train 0.5 | Epoch: 243 | Iter: 369600 | Total Loss: 0.003528 | Recon Loss: 0.003017 | Commit Loss: 0.001022 | Perplexity: 2029.919801
2025-09-28 04:06:05,666 Stage: Train 0.5 | Epoch: 243 | Iter: 369800 | Total Loss: 0.003404 | Recon Loss: 0.002893 | Commit Loss: 0.001021 | Perplexity: 2031.821174
2025-09-28 04:06:32,251 Stage: Train 0.5 | Epoch: 243 | Iter: 370000 | Total Loss: 0.003570 | Recon Loss: 0.003060 | Commit Loss: 0.001019 | Perplexity: 2030.427482
2025-09-28 04:06:58,780 Stage: Train 0.5 | Epoch: 243 | Iter: 370200 | Total Loss: 0.003495 | Recon Loss: 0.002985 | Commit Loss: 0.001020 | Perplexity: 2040.916744
2025-09-28 04:07:25,425 Stage: Train 0.5 | Epoch: 243 | Iter: 370400 | Total Loss: 0.003427 | Recon Loss: 0.002916 | Commit Loss: 0.001022 | Perplexity: 2036.595371
2025-09-28 04:07:52,005 Stage: Train 0.5 | Epoch: 243 | Iter: 370600 | Total Loss: 0.003457 | Recon Loss: 0.002945 | Commit Loss: 0.001025 | Perplexity: 2037.380538
Trainning Epoch:  74%|███████▍  | 244/330 [21:19:01<4:49:48, 202.20s/it]2025-09-28 04:08:18,737 Stage: Train 0.5 | Epoch: 244 | Iter: 370800 | Total Loss: 0.003475 | Recon Loss: 0.002965 | Commit Loss: 0.001019 | Perplexity: 2030.023773
2025-09-28 04:08:45,267 Stage: Train 0.5 | Epoch: 244 | Iter: 371000 | Total Loss: 0.003425 | Recon Loss: 0.002913 | Commit Loss: 0.001023 | Perplexity: 2038.524225
2025-09-28 04:09:11,788 Stage: Train 0.5 | Epoch: 244 | Iter: 371200 | Total Loss: 0.003428 | Recon Loss: 0.002920 | Commit Loss: 0.001017 | Perplexity: 2030.116378
2025-09-28 04:09:38,344 Stage: Train 0.5 | Epoch: 244 | Iter: 371400 | Total Loss: 0.003574 | Recon Loss: 0.003063 | Commit Loss: 0.001023 | Perplexity: 2031.630748
2025-09-28 04:10:04,921 Stage: Train 0.5 | Epoch: 244 | Iter: 371600 | Total Loss: 0.003334 | Recon Loss: 0.002825 | Commit Loss: 0.001018 | Perplexity: 2032.943561
2025-09-28 04:10:31,436 Stage: Train 0.5 | Epoch: 244 | Iter: 371800 | Total Loss: 0.003512 | Recon Loss: 0.002998 | Commit Loss: 0.001029 | Perplexity: 2042.758960
2025-09-28 04:10:57,915 Stage: Train 0.5 | Epoch: 244 | Iter: 372000 | Total Loss: 0.003523 | Recon Loss: 0.003010 | Commit Loss: 0.001026 | Perplexity: 2035.842479
Trainning Epoch:  74%|███████▍  | 245/330 [21:22:23<4:46:14, 202.05s/it]2025-09-28 04:11:24,680 Stage: Train 0.5 | Epoch: 245 | Iter: 372200 | Total Loss: 0.003424 | Recon Loss: 0.002916 | Commit Loss: 0.001017 | Perplexity: 2034.863911
2025-09-28 04:11:51,245 Stage: Train 0.5 | Epoch: 245 | Iter: 372400 | Total Loss: 0.003471 | Recon Loss: 0.002961 | Commit Loss: 0.001021 | Perplexity: 2036.169249
2025-09-28 04:12:17,727 Stage: Train 0.5 | Epoch: 245 | Iter: 372600 | Total Loss: 0.003473 | Recon Loss: 0.002966 | Commit Loss: 0.001014 | Perplexity: 2035.534282
2025-09-28 04:12:44,408 Stage: Train 0.5 | Epoch: 245 | Iter: 372800 | Total Loss: 0.003429 | Recon Loss: 0.002919 | Commit Loss: 0.001019 | Perplexity: 2038.671255
2025-09-28 04:13:10,913 Stage: Train 0.5 | Epoch: 245 | Iter: 373000 | Total Loss: 0.003493 | Recon Loss: 0.002981 | Commit Loss: 0.001024 | Perplexity: 2038.889478
2025-09-28 04:13:37,202 Stage: Train 0.5 | Epoch: 245 | Iter: 373200 | Total Loss: 0.003471 | Recon Loss: 0.002960 | Commit Loss: 0.001022 | Perplexity: 2034.486494
2025-09-28 04:14:03,635 Stage: Train 0.5 | Epoch: 245 | Iter: 373400 | Total Loss: 0.003399 | Recon Loss: 0.002891 | Commit Loss: 0.001017 | Perplexity: 2033.538871
2025-09-28 04:14:30,261 Stage: Train 0.5 | Epoch: 245 | Iter: 373600 | Total Loss: 0.003464 | Recon Loss: 0.002954 | Commit Loss: 0.001019 | Perplexity: 2033.769590
Trainning Epoch:  75%|███████▍  | 246/330 [21:25:45<4:42:39, 201.90s/it]2025-09-28 04:14:57,041 Stage: Train 0.5 | Epoch: 246 | Iter: 373800 | Total Loss: 0.003382 | Recon Loss: 0.002874 | Commit Loss: 0.001015 | Perplexity: 2034.071481
2025-09-28 04:15:23,653 Stage: Train 0.5 | Epoch: 246 | Iter: 374000 | Total Loss: 0.003421 | Recon Loss: 0.002913 | Commit Loss: 0.001016 | Perplexity: 2038.111668
2025-09-28 04:15:50,143 Stage: Train 0.5 | Epoch: 246 | Iter: 374200 | Total Loss: 0.003408 | Recon Loss: 0.002898 | Commit Loss: 0.001020 | Perplexity: 2040.052846
2025-09-28 04:16:16,711 Stage: Train 0.5 | Epoch: 246 | Iter: 374400 | Total Loss: 0.003389 | Recon Loss: 0.002874 | Commit Loss: 0.001031 | Perplexity: 2041.981401
2025-09-28 04:16:43,336 Stage: Train 0.5 | Epoch: 246 | Iter: 374600 | Total Loss: 0.003513 | Recon Loss: 0.003003 | Commit Loss: 0.001019 | Perplexity: 2031.814893
2025-09-28 04:17:09,769 Stage: Train 0.5 | Epoch: 246 | Iter: 374800 | Total Loss: 0.003418 | Recon Loss: 0.002910 | Commit Loss: 0.001016 | Perplexity: 2032.330279
2025-09-28 04:17:36,299 Stage: Train 0.5 | Epoch: 246 | Iter: 375000 | Total Loss: 0.003506 | Recon Loss: 0.002998 | Commit Loss: 0.001015 | Perplexity: 2035.936938
Trainning Epoch:  75%|███████▍  | 247/330 [21:29:06<4:39:15, 201.87s/it]2025-09-28 04:18:03,007 Stage: Train 0.5 | Epoch: 247 | Iter: 375200 | Total Loss: 0.003457 | Recon Loss: 0.002949 | Commit Loss: 0.001017 | Perplexity: 2034.251368
2025-09-28 04:18:29,491 Stage: Train 0.5 | Epoch: 247 | Iter: 375400 | Total Loss: 0.003536 | Recon Loss: 0.003024 | Commit Loss: 0.001024 | Perplexity: 2041.615719
2025-09-28 04:18:56,138 Stage: Train 0.5 | Epoch: 247 | Iter: 375600 | Total Loss: 0.003430 | Recon Loss: 0.002919 | Commit Loss: 0.001021 | Perplexity: 2043.262947
2025-09-28 04:19:22,740 Stage: Train 0.5 | Epoch: 247 | Iter: 375800 | Total Loss: 0.003391 | Recon Loss: 0.002886 | Commit Loss: 0.001011 | Perplexity: 2030.089626
2025-09-28 04:19:49,284 Stage: Train 0.5 | Epoch: 247 | Iter: 376000 | Total Loss: 0.003436 | Recon Loss: 0.002932 | Commit Loss: 0.001008 | Perplexity: 2028.652720
2025-09-28 04:20:15,860 Stage: Train 0.5 | Epoch: 247 | Iter: 376200 | Total Loss: 0.003445 | Recon Loss: 0.002933 | Commit Loss: 0.001024 | Perplexity: 2037.638738
2025-09-28 04:20:42,453 Stage: Train 0.5 | Epoch: 247 | Iter: 376400 | Total Loss: 0.003513 | Recon Loss: 0.003008 | Commit Loss: 0.001011 | Perplexity: 2032.688179
2025-09-28 04:21:09,088 Stage: Train 0.5 | Epoch: 247 | Iter: 376600 | Total Loss: 0.003423 | Recon Loss: 0.002916 | Commit Loss: 0.001014 | Perplexity: 2037.277825
Trainning Epoch:  75%|███████▌  | 248/330 [21:32:29<4:35:58, 201.94s/it]2025-09-28 04:21:35,850 Stage: Train 0.5 | Epoch: 248 | Iter: 376800 | Total Loss: 0.003434 | Recon Loss: 0.002923 | Commit Loss: 0.001022 | Perplexity: 2042.726177
2025-09-28 04:22:02,291 Stage: Train 0.5 | Epoch: 248 | Iter: 377000 | Total Loss: 0.003469 | Recon Loss: 0.002962 | Commit Loss: 0.001013 | Perplexity: 2038.566013
2025-09-28 04:22:28,961 Stage: Train 0.5 | Epoch: 248 | Iter: 377200 | Total Loss: 0.003364 | Recon Loss: 0.002856 | Commit Loss: 0.001018 | Perplexity: 2042.794052
2025-09-28 04:22:55,539 Stage: Train 0.5 | Epoch: 248 | Iter: 377400 | Total Loss: 0.003419 | Recon Loss: 0.002911 | Commit Loss: 0.001017 | Perplexity: 2034.127437
2025-09-28 04:23:22,172 Stage: Train 0.5 | Epoch: 248 | Iter: 377600 | Total Loss: 0.003427 | Recon Loss: 0.002919 | Commit Loss: 0.001015 | Perplexity: 2039.684526
2025-09-28 04:23:48,673 Stage: Train 0.5 | Epoch: 248 | Iter: 377800 | Total Loss: 0.003400 | Recon Loss: 0.002891 | Commit Loss: 0.001016 | Perplexity: 2035.797377
2025-09-28 04:24:15,243 Stage: Train 0.5 | Epoch: 248 | Iter: 378000 | Total Loss: 0.003498 | Recon Loss: 0.002990 | Commit Loss: 0.001015 | Perplexity: 2038.280983
2025-09-28 04:24:41,757 Stage: Train 0.5 | Epoch: 248 | Iter: 378200 | Total Loss: 0.003389 | Recon Loss: 0.002882 | Commit Loss: 0.001012 | Perplexity: 2038.300461
Trainning Epoch:  75%|███████▌  | 249/330 [21:35:50<4:32:36, 201.93s/it]2025-09-28 04:25:08,610 Stage: Train 0.5 | Epoch: 249 | Iter: 378400 | Total Loss: 0.003510 | Recon Loss: 0.003006 | Commit Loss: 0.001008 | Perplexity: 2035.247324
2025-09-28 04:25:35,262 Stage: Train 0.5 | Epoch: 249 | Iter: 378600 | Total Loss: 0.003367 | Recon Loss: 0.002860 | Commit Loss: 0.001014 | Perplexity: 2038.860729
2025-09-28 04:26:01,862 Stage: Train 0.5 | Epoch: 249 | Iter: 378800 | Total Loss: 0.003418 | Recon Loss: 0.002913 | Commit Loss: 0.001011 | Perplexity: 2038.579329
2025-09-28 04:26:28,454 Stage: Train 0.5 | Epoch: 249 | Iter: 379000 | Total Loss: 0.003396 | Recon Loss: 0.002888 | Commit Loss: 0.001016 | Perplexity: 2036.362281
2025-09-28 04:26:55,174 Stage: Train 0.5 | Epoch: 249 | Iter: 379200 | Total Loss: 0.003421 | Recon Loss: 0.002911 | Commit Loss: 0.001021 | Perplexity: 2040.267351
2025-09-28 04:27:21,804 Stage: Train 0.5 | Epoch: 249 | Iter: 379400 | Total Loss: 0.003521 | Recon Loss: 0.003018 | Commit Loss: 0.001007 | Perplexity: 2029.233729
2025-09-28 04:27:48,260 Stage: Train 0.5 | Epoch: 249 | Iter: 379600 | Total Loss: 0.003432 | Recon Loss: 0.002923 | Commit Loss: 0.001019 | Perplexity: 2041.469113
Trainning Epoch:  76%|███████▌  | 250/330 [21:39:13<4:29:24, 202.05s/it]2025-09-28 04:28:14,978 Stage: Train 0.5 | Epoch: 250 | Iter: 379800 | Total Loss: 0.003469 | Recon Loss: 0.002964 | Commit Loss: 0.001010 | Perplexity: 2036.805182
2025-09-28 04:28:41,530 Stage: Train 0.5 | Epoch: 250 | Iter: 380000 | Total Loss: 0.003448 | Recon Loss: 0.002946 | Commit Loss: 0.001004 | Perplexity: 2036.275054
2025-09-28 04:28:41,530 Saving model at iteration 380000
2025-09-28 04:28:41,714 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000
2025-09-28 04:28:42,165 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/model.safetensors
2025-09-28 04:28:42,682 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/optimizer.bin
2025-09-28 04:28:42,683 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/scheduler.bin
2025-09-28 04:28:42,683 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/sampler.bin
2025-09-28 04:28:42,684 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/random_states_0.pkl
2025-09-28 04:29:09,442 Stage: Train 0.5 | Epoch: 250 | Iter: 380200 | Total Loss: 0.003411 | Recon Loss: 0.002908 | Commit Loss: 0.001007 | Perplexity: 2036.806373
2025-09-28 04:29:35,978 Stage: Train 0.5 | Epoch: 250 | Iter: 380400 | Total Loss: 0.003395 | Recon Loss: 0.002889 | Commit Loss: 0.001011 | Perplexity: 2037.598430
2025-09-28 04:30:02,532 Stage: Train 0.5 | Epoch: 250 | Iter: 380600 | Total Loss: 0.003422 | Recon Loss: 0.002911 | Commit Loss: 0.001024 | Perplexity: 2046.269706
2025-09-28 04:30:29,070 Stage: Train 0.5 | Epoch: 250 | Iter: 380800 | Total Loss: 0.003640 | Recon Loss: 0.003130 | Commit Loss: 0.001020 | Perplexity: 2035.248884
2025-09-28 04:30:55,583 Stage: Train 0.5 | Epoch: 250 | Iter: 381000 | Total Loss: 0.003350 | Recon Loss: 0.002845 | Commit Loss: 0.001010 | Perplexity: 2035.714595
2025-09-28 04:31:22,175 Stage: Train 0.5 | Epoch: 250 | Iter: 381200 | Total Loss: 0.003349 | Recon Loss: 0.002847 | Commit Loss: 0.001005 | Perplexity: 2034.628304
Trainning Epoch:  76%|███████▌  | 251/330 [21:42:36<4:26:29, 202.40s/it]2025-09-28 04:31:49,136 Stage: Train 0.5 | Epoch: 251 | Iter: 381400 | Total Loss: 0.003446 | Recon Loss: 0.002944 | Commit Loss: 0.001003 | Perplexity: 2034.907833
2025-09-28 04:32:15,811 Stage: Train 0.5 | Epoch: 251 | Iter: 381600 | Total Loss: 0.003379 | Recon Loss: 0.002871 | Commit Loss: 0.001015 | Perplexity: 2046.708374
2025-09-28 04:32:42,344 Stage: Train 0.5 | Epoch: 251 | Iter: 381800 | Total Loss: 0.003411 | Recon Loss: 0.002905 | Commit Loss: 0.001012 | Perplexity: 2038.805009
2025-09-28 04:33:08,963 Stage: Train 0.5 | Epoch: 251 | Iter: 382000 | Total Loss: 0.003455 | Recon Loss: 0.002947 | Commit Loss: 0.001016 | Perplexity: 2041.561918
2025-09-28 04:33:35,527 Stage: Train 0.5 | Epoch: 251 | Iter: 382200 | Total Loss: 0.003386 | Recon Loss: 0.002883 | Commit Loss: 0.001006 | Perplexity: 2028.810059
2025-09-28 04:34:02,164 Stage: Train 0.5 | Epoch: 251 | Iter: 382400 | Total Loss: 0.003399 | Recon Loss: 0.002891 | Commit Loss: 0.001017 | Perplexity: 2040.895000
2025-09-28 04:34:28,857 Stage: Train 0.5 | Epoch: 251 | Iter: 382600 | Total Loss: 0.003408 | Recon Loss: 0.002904 | Commit Loss: 0.001009 | Perplexity: 2038.551802
Trainning Epoch:  76%|███████▋  | 252/330 [21:45:58<4:23:09, 202.42s/it]2025-09-28 04:34:55,686 Stage: Train 0.5 | Epoch: 252 | Iter: 382800 | Total Loss: 0.003428 | Recon Loss: 0.002918 | Commit Loss: 0.001020 | Perplexity: 2042.554737
2025-09-28 04:35:22,439 Stage: Train 0.5 | Epoch: 252 | Iter: 383000 | Total Loss: 0.003447 | Recon Loss: 0.002940 | Commit Loss: 0.001015 | Perplexity: 2042.522089
2025-09-28 04:35:49,140 Stage: Train 0.5 | Epoch: 252 | Iter: 383200 | Total Loss: 0.003413 | Recon Loss: 0.002910 | Commit Loss: 0.001006 | Perplexity: 2031.585556
2025-09-28 04:36:15,707 Stage: Train 0.5 | Epoch: 252 | Iter: 383400 | Total Loss: 0.003380 | Recon Loss: 0.002875 | Commit Loss: 0.001011 | Perplexity: 2042.401522
2025-09-28 04:36:42,413 Stage: Train 0.5 | Epoch: 252 | Iter: 383600 | Total Loss: 0.003513 | Recon Loss: 0.003014 | Commit Loss: 0.000997 | Perplexity: 2033.291177
2025-09-28 04:37:08,992 Stage: Train 0.5 | Epoch: 252 | Iter: 383800 | Total Loss: 0.003366 | Recon Loss: 0.002861 | Commit Loss: 0.001011 | Perplexity: 2040.396138
2025-09-28 04:37:35,586 Stage: Train 0.5 | Epoch: 252 | Iter: 384000 | Total Loss: 0.003418 | Recon Loss: 0.002912 | Commit Loss: 0.001013 | Perplexity: 2037.541163
2025-09-28 04:38:02,144 Stage: Train 0.5 | Epoch: 252 | Iter: 384200 | Total Loss: 0.003499 | Recon Loss: 0.002994 | Commit Loss: 0.001010 | Perplexity: 2039.192634
Trainning Epoch:  77%|███████▋  | 253/330 [21:49:21<4:19:48, 202.45s/it]2025-09-28 04:38:29,044 Stage: Train 0.5 | Epoch: 253 | Iter: 384400 | Total Loss: 0.003373 | Recon Loss: 0.002870 | Commit Loss: 0.001007 | Perplexity: 2036.475499
2025-09-28 04:38:55,671 Stage: Train 0.5 | Epoch: 253 | Iter: 384600 | Total Loss: 0.003384 | Recon Loss: 0.002882 | Commit Loss: 0.001004 | Perplexity: 2036.155781
2025-09-28 04:39:22,233 Stage: Train 0.5 | Epoch: 253 | Iter: 384800 | Total Loss: 0.003407 | Recon Loss: 0.002906 | Commit Loss: 0.001002 | Perplexity: 2037.582599
2025-09-28 04:39:48,800 Stage: Train 0.5 | Epoch: 253 | Iter: 385000 | Total Loss: 0.003428 | Recon Loss: 0.002927 | Commit Loss: 0.001003 | Perplexity: 2036.433715
2025-09-28 04:40:15,282 Stage: Train 0.5 | Epoch: 253 | Iter: 385200 | Total Loss: 0.003374 | Recon Loss: 0.002868 | Commit Loss: 0.001012 | Perplexity: 2039.737549
2025-09-28 04:40:41,850 Stage: Train 0.5 | Epoch: 253 | Iter: 385400 | Total Loss: 0.003869 | Recon Loss: 0.003360 | Commit Loss: 0.001018 | Perplexity: 2037.379248
2025-09-28 04:41:08,427 Stage: Train 0.5 | Epoch: 253 | Iter: 385600 | Total Loss: 0.003332 | Recon Loss: 0.002829 | Commit Loss: 0.001006 | Perplexity: 2037.391544
2025-09-28 04:41:35,063 Stage: Train 0.5 | Epoch: 253 | Iter: 385800 | Total Loss: 0.003321 | Recon Loss: 0.002821 | Commit Loss: 0.001000 | Perplexity: 2029.829143
Trainning Epoch:  77%|███████▋  | 254/330 [21:52:43<4:16:18, 202.35s/it]2025-09-28 04:42:01,971 Stage: Train 0.5 | Epoch: 254 | Iter: 386000 | Total Loss: 0.003400 | Recon Loss: 0.002898 | Commit Loss: 0.001004 | Perplexity: 2034.966317
2025-09-28 04:42:28,570 Stage: Train 0.5 | Epoch: 254 | Iter: 386200 | Total Loss: 0.003477 | Recon Loss: 0.002973 | Commit Loss: 0.001008 | Perplexity: 2036.818996
2025-09-28 04:42:55,149 Stage: Train 0.5 | Epoch: 254 | Iter: 386400 | Total Loss: 0.003504 | Recon Loss: 0.003006 | Commit Loss: 0.000995 | Perplexity: 2030.932709
2025-09-28 04:43:21,847 Stage: Train 0.5 | Epoch: 254 | Iter: 386600 | Total Loss: 0.003354 | Recon Loss: 0.002851 | Commit Loss: 0.001005 | Perplexity: 2037.729849
2025-09-28 04:43:48,479 Stage: Train 0.5 | Epoch: 254 | Iter: 386800 | Total Loss: 0.003453 | Recon Loss: 0.002948 | Commit Loss: 0.001009 | Perplexity: 2042.298720
2025-09-28 04:44:15,058 Stage: Train 0.5 | Epoch: 254 | Iter: 387000 | Total Loss: 0.003403 | Recon Loss: 0.002899 | Commit Loss: 0.001009 | Perplexity: 2040.705847
2025-09-28 04:44:41,601 Stage: Train 0.5 | Epoch: 254 | Iter: 387200 | Total Loss: 0.003376 | Recon Loss: 0.002870 | Commit Loss: 0.001013 | Perplexity: 2052.202436
Trainning Epoch:  77%|███████▋  | 255/330 [21:56:05<4:12:55, 202.34s/it]2025-09-28 04:45:08,418 Stage: Train 0.5 | Epoch: 255 | Iter: 387400 | Total Loss: 0.003455 | Recon Loss: 0.002956 | Commit Loss: 0.000998 | Perplexity: 2030.237818
2025-09-28 04:45:34,978 Stage: Train 0.5 | Epoch: 255 | Iter: 387600 | Total Loss: 0.003368 | Recon Loss: 0.002866 | Commit Loss: 0.001004 | Perplexity: 2039.973670
2025-09-28 04:46:01,487 Stage: Train 0.5 | Epoch: 255 | Iter: 387800 | Total Loss: 0.003412 | Recon Loss: 0.002912 | Commit Loss: 0.001002 | Perplexity: 2041.547986
2025-09-28 04:46:28,064 Stage: Train 0.5 | Epoch: 255 | Iter: 388000 | Total Loss: 0.003473 | Recon Loss: 0.002973 | Commit Loss: 0.000999 | Perplexity: 2032.934429
2025-09-28 04:46:54,598 Stage: Train 0.5 | Epoch: 255 | Iter: 388200 | Total Loss: 0.003342 | Recon Loss: 0.002840 | Commit Loss: 0.001004 | Perplexity: 2039.749412
2025-09-28 04:47:21,188 Stage: Train 0.5 | Epoch: 255 | Iter: 388400 | Total Loss: 0.003545 | Recon Loss: 0.003041 | Commit Loss: 0.001007 | Perplexity: 2036.316195
2025-09-28 04:47:47,699 Stage: Train 0.5 | Epoch: 255 | Iter: 388600 | Total Loss: 0.003356 | Recon Loss: 0.002852 | Commit Loss: 0.001009 | Perplexity: 2037.265665
2025-09-28 04:48:14,214 Stage: Train 0.5 | Epoch: 255 | Iter: 388800 | Total Loss: 0.003393 | Recon Loss: 0.002889 | Commit Loss: 0.001009 | Perplexity: 2038.274738
Trainning Epoch:  78%|███████▊  | 256/330 [21:59:27<4:09:22, 202.19s/it]2025-09-28 04:48:40,904 Stage: Train 0.5 | Epoch: 256 | Iter: 389000 | Total Loss: 0.003374 | Recon Loss: 0.002872 | Commit Loss: 0.001004 | Perplexity: 2036.131404
2025-09-28 04:49:07,522 Stage: Train 0.5 | Epoch: 256 | Iter: 389200 | Total Loss: 0.003385 | Recon Loss: 0.002882 | Commit Loss: 0.001005 | Perplexity: 2040.975705
2025-09-28 04:49:34,180 Stage: Train 0.5 | Epoch: 256 | Iter: 389400 | Total Loss: 0.003437 | Recon Loss: 0.002935 | Commit Loss: 0.001005 | Perplexity: 2039.663993
2025-09-28 04:50:00,763 Stage: Train 0.5 | Epoch: 256 | Iter: 389600 | Total Loss: 0.003411 | Recon Loss: 0.002909 | Commit Loss: 0.001003 | Perplexity: 2035.301036
2025-09-28 04:50:27,371 Stage: Train 0.5 | Epoch: 256 | Iter: 389800 | Total Loss: 0.003465 | Recon Loss: 0.002966 | Commit Loss: 0.000999 | Perplexity: 2039.854218
2025-09-28 04:50:53,924 Stage: Train 0.5 | Epoch: 256 | Iter: 390000 | Total Loss: 0.003352 | Recon Loss: 0.002852 | Commit Loss: 0.000999 | Perplexity: 2035.788251
2025-09-28 04:51:20,475 Stage: Train 0.5 | Epoch: 256 | Iter: 390200 | Total Loss: 0.003372 | Recon Loss: 0.002867 | Commit Loss: 0.001008 | Perplexity: 2043.093395
Trainning Epoch:  78%|███████▊  | 257/330 [22:02:49<4:05:58, 202.17s/it]2025-09-28 04:51:47,255 Stage: Train 0.5 | Epoch: 257 | Iter: 390400 | Total Loss: 0.003404 | Recon Loss: 0.002903 | Commit Loss: 0.001001 | Perplexity: 2038.451945
2025-09-28 04:52:13,783 Stage: Train 0.5 | Epoch: 257 | Iter: 390600 | Total Loss: 0.003406 | Recon Loss: 0.002906 | Commit Loss: 0.001000 | Perplexity: 2038.174673
2025-09-28 04:52:40,354 Stage: Train 0.5 | Epoch: 257 | Iter: 390800 | Total Loss: 0.003372 | Recon Loss: 0.002871 | Commit Loss: 0.001004 | Perplexity: 2043.935229
2025-09-28 04:53:06,880 Stage: Train 0.5 | Epoch: 257 | Iter: 391000 | Total Loss: 0.003407 | Recon Loss: 0.002903 | Commit Loss: 0.001007 | Perplexity: 2040.695031
2025-09-28 04:53:33,409 Stage: Train 0.5 | Epoch: 257 | Iter: 391200 | Total Loss: 0.003397 | Recon Loss: 0.002895 | Commit Loss: 0.001003 | Perplexity: 2043.735941
2025-09-28 04:54:00,010 Stage: Train 0.5 | Epoch: 257 | Iter: 391400 | Total Loss: 0.003442 | Recon Loss: 0.002943 | Commit Loss: 0.000999 | Perplexity: 2037.028582
2025-09-28 04:54:26,595 Stage: Train 0.5 | Epoch: 257 | Iter: 391600 | Total Loss: 0.003373 | Recon Loss: 0.002871 | Commit Loss: 0.001005 | Perplexity: 2040.795756
2025-09-28 04:54:53,020 Stage: Train 0.5 | Epoch: 257 | Iter: 391800 | Total Loss: 0.003367 | Recon Loss: 0.002867 | Commit Loss: 0.001000 | Perplexity: 2033.899032
Trainning Epoch:  78%|███████▊  | 258/330 [22:06:11<4:02:28, 202.06s/it]2025-09-28 04:55:19,779 Stage: Train 0.5 | Epoch: 258 | Iter: 392000 | Total Loss: 0.003404 | Recon Loss: 0.002909 | Commit Loss: 0.000990 | Perplexity: 2033.542203
2025-09-28 04:55:46,447 Stage: Train 0.5 | Epoch: 258 | Iter: 392200 | Total Loss: 0.003392 | Recon Loss: 0.002892 | Commit Loss: 0.000999 | Perplexity: 2037.248154
2025-09-28 04:56:12,958 Stage: Train 0.5 | Epoch: 258 | Iter: 392400 | Total Loss: 0.003389 | Recon Loss: 0.002888 | Commit Loss: 0.001003 | Perplexity: 2042.991573
2025-09-28 04:56:39,452 Stage: Train 0.5 | Epoch: 258 | Iter: 392600 | Total Loss: 0.003398 | Recon Loss: 0.002893 | Commit Loss: 0.001010 | Perplexity: 2043.053082
2025-09-28 04:57:06,118 Stage: Train 0.5 | Epoch: 258 | Iter: 392800 | Total Loss: 0.003394 | Recon Loss: 0.002894 | Commit Loss: 0.001000 | Perplexity: 2038.839540
2025-09-28 04:57:32,683 Stage: Train 0.5 | Epoch: 258 | Iter: 393000 | Total Loss: 0.003416 | Recon Loss: 0.002919 | Commit Loss: 0.000994 | Perplexity: 2038.215712
2025-09-28 04:57:59,296 Stage: Train 0.5 | Epoch: 258 | Iter: 393200 | Total Loss: 0.003405 | Recon Loss: 0.002900 | Commit Loss: 0.001010 | Perplexity: 2042.550162
2025-09-28 04:58:25,843 Stage: Train 0.5 | Epoch: 258 | Iter: 393400 | Total Loss: 0.003341 | Recon Loss: 0.002839 | Commit Loss: 0.001004 | Perplexity: 2043.737083
Trainning Epoch:  78%|███████▊  | 259/330 [22:09:33<3:59:04, 202.04s/it]2025-09-28 04:58:52,607 Stage: Train 0.5 | Epoch: 259 | Iter: 393600 | Total Loss: 0.003400 | Recon Loss: 0.002897 | Commit Loss: 0.001006 | Perplexity: 2036.892296
2025-09-28 04:59:19,156 Stage: Train 0.5 | Epoch: 259 | Iter: 393800 | Total Loss: 0.003397 | Recon Loss: 0.002896 | Commit Loss: 0.001002 | Perplexity: 2042.167940
2025-09-28 04:59:45,819 Stage: Train 0.5 | Epoch: 259 | Iter: 394000 | Total Loss: 0.003489 | Recon Loss: 0.002993 | Commit Loss: 0.000992 | Perplexity: 2036.737758
2025-09-28 05:00:12,300 Stage: Train 0.5 | Epoch: 259 | Iter: 394200 | Total Loss: 0.003356 | Recon Loss: 0.002855 | Commit Loss: 0.001003 | Perplexity: 2040.761246
2025-09-28 05:00:38,893 Stage: Train 0.5 | Epoch: 259 | Iter: 394400 | Total Loss: 0.003366 | Recon Loss: 0.002868 | Commit Loss: 0.000995 | Perplexity: 2037.308361
2025-09-28 05:01:05,553 Stage: Train 0.5 | Epoch: 259 | Iter: 394600 | Total Loss: 0.003423 | Recon Loss: 0.002925 | Commit Loss: 0.000996 | Perplexity: 2036.661483
2025-09-28 05:01:32,056 Stage: Train 0.5 | Epoch: 259 | Iter: 394800 | Total Loss: 0.003348 | Recon Loss: 0.002847 | Commit Loss: 0.001002 | Perplexity: 2039.646566
Trainning Epoch:  79%|███████▉  | 260/330 [22:12:55<3:55:42, 202.04s/it]2025-09-28 05:01:58,766 Stage: Train 0.5 | Epoch: 260 | Iter: 395000 | Total Loss: 0.003357 | Recon Loss: 0.002857 | Commit Loss: 0.001000 | Perplexity: 2041.005677
2025-09-28 05:02:25,335 Stage: Train 0.5 | Epoch: 260 | Iter: 395200 | Total Loss: 0.003456 | Recon Loss: 0.002956 | Commit Loss: 0.001001 | Perplexity: 2040.438892
2025-09-28 05:02:52,002 Stage: Train 0.5 | Epoch: 260 | Iter: 395400 | Total Loss: 0.003324 | Recon Loss: 0.002831 | Commit Loss: 0.000987 | Perplexity: 2033.508798
2025-09-28 05:03:18,628 Stage: Train 0.5 | Epoch: 260 | Iter: 395600 | Total Loss: 0.003378 | Recon Loss: 0.002880 | Commit Loss: 0.000996 | Perplexity: 2037.943906
2025-09-28 05:03:45,228 Stage: Train 0.5 | Epoch: 260 | Iter: 395800 | Total Loss: 0.003392 | Recon Loss: 0.002896 | Commit Loss: 0.000992 | Perplexity: 2038.475585
2025-09-28 05:04:11,903 Stage: Train 0.5 | Epoch: 260 | Iter: 396000 | Total Loss: 0.003431 | Recon Loss: 0.002930 | Commit Loss: 0.001001 | Perplexity: 2040.505626
2025-09-28 05:04:38,466 Stage: Train 0.5 | Epoch: 260 | Iter: 396200 | Total Loss: 0.003324 | Recon Loss: 0.002821 | Commit Loss: 0.001006 | Perplexity: 2042.255890
2025-09-28 05:05:04,824 Stage: Train 0.5 | Epoch: 260 | Iter: 396400 | Total Loss: 0.003416 | Recon Loss: 0.002914 | Commit Loss: 0.001005 | Perplexity: 2041.198518
Trainning Epoch:  79%|███████▉  | 261/330 [22:16:17<3:52:22, 202.06s/it]2025-09-28 05:05:31,889 Stage: Train 0.5 | Epoch: 261 | Iter: 396600 | Total Loss: 0.003414 | Recon Loss: 0.002918 | Commit Loss: 0.000993 | Perplexity: 2038.037204
2025-09-28 05:05:58,539 Stage: Train 0.5 | Epoch: 261 | Iter: 396800 | Total Loss: 0.003372 | Recon Loss: 0.002872 | Commit Loss: 0.001000 | Perplexity: 2040.977836
2025-09-28 05:06:24,998 Stage: Train 0.5 | Epoch: 261 | Iter: 397000 | Total Loss: 0.003340 | Recon Loss: 0.002839 | Commit Loss: 0.001003 | Perplexity: 2044.525255
2025-09-28 05:06:51,549 Stage: Train 0.5 | Epoch: 261 | Iter: 397200 | Total Loss: 0.003372 | Recon Loss: 0.002876 | Commit Loss: 0.000994 | Perplexity: 2039.517479
2025-09-28 05:07:18,112 Stage: Train 0.5 | Epoch: 261 | Iter: 397400 | Total Loss: 0.003336 | Recon Loss: 0.002838 | Commit Loss: 0.000997 | Perplexity: 2037.458948
2025-09-28 05:07:44,695 Stage: Train 0.5 | Epoch: 261 | Iter: 397600 | Total Loss: 0.003438 | Recon Loss: 0.002939 | Commit Loss: 0.000999 | Perplexity: 2044.045576
2025-09-28 05:08:11,324 Stage: Train 0.5 | Epoch: 261 | Iter: 397800 | Total Loss: 0.003409 | Recon Loss: 0.002908 | Commit Loss: 0.001001 | Perplexity: 2041.776841
Trainning Epoch:  79%|███████▉  | 262/330 [22:19:39<3:49:02, 202.09s/it]2025-09-28 05:08:38,037 Stage: Train 0.5 | Epoch: 262 | Iter: 398000 | Total Loss: 0.003309 | Recon Loss: 0.002813 | Commit Loss: 0.000992 | Perplexity: 2036.079318
2025-09-28 05:09:04,635 Stage: Train 0.5 | Epoch: 262 | Iter: 398200 | Total Loss: 0.003684 | Recon Loss: 0.003188 | Commit Loss: 0.000993 | Perplexity: 2040.147646
2025-09-28 05:09:31,097 Stage: Train 0.5 | Epoch: 262 | Iter: 398400 | Total Loss: 0.003219 | Recon Loss: 0.002722 | Commit Loss: 0.000995 | Perplexity: 2041.684362
2025-09-28 05:09:57,644 Stage: Train 0.5 | Epoch: 262 | Iter: 398600 | Total Loss: 0.003349 | Recon Loss: 0.002853 | Commit Loss: 0.000993 | Perplexity: 2041.327041
2025-09-28 05:10:24,225 Stage: Train 0.5 | Epoch: 262 | Iter: 398800 | Total Loss: 0.003357 | Recon Loss: 0.002857 | Commit Loss: 0.001000 | Perplexity: 2047.235977
2025-09-28 05:10:50,841 Stage: Train 0.5 | Epoch: 262 | Iter: 399000 | Total Loss: 0.003380 | Recon Loss: 0.002882 | Commit Loss: 0.000997 | Perplexity: 2040.969670
2025-09-28 05:11:17,404 Stage: Train 0.5 | Epoch: 262 | Iter: 399200 | Total Loss: 0.003394 | Recon Loss: 0.002896 | Commit Loss: 0.000997 | Perplexity: 2039.120031
2025-09-28 05:11:43,984 Stage: Train 0.5 | Epoch: 262 | Iter: 399400 | Total Loss: 0.003392 | Recon Loss: 0.002895 | Commit Loss: 0.000993 | Perplexity: 2035.192573
Trainning Epoch:  80%|███████▉  | 263/330 [22:23:01<3:45:38, 202.06s/it]2025-09-28 05:12:10,834 Stage: Train 0.5 | Epoch: 263 | Iter: 399600 | Total Loss: 0.003357 | Recon Loss: 0.002862 | Commit Loss: 0.000990 | Perplexity: 2037.090319
2025-09-28 05:12:37,367 Stage: Train 0.5 | Epoch: 263 | Iter: 399800 | Total Loss: 0.003496 | Recon Loss: 0.003002 | Commit Loss: 0.000987 | Perplexity: 2035.018832
2025-09-28 05:13:03,880 Stage: Train 0.5 | Epoch: 263 | Iter: 400000 | Total Loss: 0.003371 | Recon Loss: 0.002876 | Commit Loss: 0.000991 | Perplexity: 2040.015511
2025-09-28 05:13:03,880 Saving model at iteration 400000
2025-09-28 05:13:04,070 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000
2025-09-28 05:13:04,525 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/model.safetensors
2025-09-28 05:13:05,014 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/optimizer.bin
2025-09-28 05:13:05,014 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/scheduler.bin
2025-09-28 05:13:05,014 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/sampler.bin
2025-09-28 05:13:05,015 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/random_states_0.pkl
2025-09-28 05:13:31,655 Stage: Train 0.5 | Epoch: 263 | Iter: 400200 | Total Loss: 0.003364 | Recon Loss: 0.002868 | Commit Loss: 0.000993 | Perplexity: 2038.579238
2025-09-28 05:13:58,218 Stage: Train 0.5 | Epoch: 263 | Iter: 400400 | Total Loss: 0.003391 | Recon Loss: 0.002891 | Commit Loss: 0.001000 | Perplexity: 2045.750633
2025-09-28 05:14:24,772 Stage: Train 0.5 | Epoch: 263 | Iter: 400600 | Total Loss: 0.003516 | Recon Loss: 0.003016 | Commit Loss: 0.001001 | Perplexity: 2040.514514
2025-09-28 05:14:51,389 Stage: Train 0.5 | Epoch: 263 | Iter: 400800 | Total Loss: 0.003332 | Recon Loss: 0.002835 | Commit Loss: 0.000996 | Perplexity: 2039.811417
2025-09-28 05:15:18,108 Stage: Train 0.5 | Epoch: 263 | Iter: 401000 | Total Loss: 0.003406 | Recon Loss: 0.002910 | Commit Loss: 0.000993 | Perplexity: 2039.070061
Trainning Epoch:  80%|████████  | 264/330 [22:26:25<3:42:40, 202.43s/it]2025-09-28 05:15:44,856 Stage: Train 0.5 | Epoch: 264 | Iter: 401200 | Total Loss: 0.003384 | Recon Loss: 0.002888 | Commit Loss: 0.000992 | Perplexity: 2038.189439
2025-09-28 05:16:11,435 Stage: Train 0.5 | Epoch: 264 | Iter: 401400 | Total Loss: 0.003357 | Recon Loss: 0.002862 | Commit Loss: 0.000990 | Perplexity: 2037.489167
2025-09-28 05:16:38,007 Stage: Train 0.5 | Epoch: 264 | Iter: 401600 | Total Loss: 0.003349 | Recon Loss: 0.002852 | Commit Loss: 0.000994 | Perplexity: 2039.500681
2025-09-28 05:17:04,621 Stage: Train 0.5 | Epoch: 264 | Iter: 401800 | Total Loss: 0.003427 | Recon Loss: 0.002932 | Commit Loss: 0.000990 | Perplexity: 2036.246674
2025-09-28 05:17:31,199 Stage: Train 0.5 | Epoch: 264 | Iter: 402000 | Total Loss: 0.003340 | Recon Loss: 0.002844 | Commit Loss: 0.000993 | Perplexity: 2046.137267
2025-09-28 05:17:57,875 Stage: Train 0.5 | Epoch: 264 | Iter: 402200 | Total Loss: 0.003321 | Recon Loss: 0.002826 | Commit Loss: 0.000990 | Perplexity: 2042.713665
2025-09-28 05:18:24,393 Stage: Train 0.5 | Epoch: 264 | Iter: 402400 | Total Loss: 0.003423 | Recon Loss: 0.002924 | Commit Loss: 0.000999 | Perplexity: 2045.661645
Trainning Epoch:  80%|████████  | 265/330 [22:29:47<3:39:12, 202.35s/it]2025-09-28 05:18:51,232 Stage: Train 0.5 | Epoch: 265 | Iter: 402600 | Total Loss: 0.003312 | Recon Loss: 0.002813 | Commit Loss: 0.000998 | Perplexity: 2048.524745
2025-09-28 05:19:17,755 Stage: Train 0.5 | Epoch: 265 | Iter: 402800 | Total Loss: 0.003368 | Recon Loss: 0.002876 | Commit Loss: 0.000985 | Perplexity: 2039.360642
2025-09-28 05:19:44,379 Stage: Train 0.5 | Epoch: 265 | Iter: 403000 | Total Loss: 0.003330 | Recon Loss: 0.002833 | Commit Loss: 0.000994 | Perplexity: 2038.711359
2025-09-28 05:20:10,907 Stage: Train 0.5 | Epoch: 265 | Iter: 403200 | Total Loss: 0.003434 | Recon Loss: 0.002938 | Commit Loss: 0.000991 | Perplexity: 2046.800713
2025-09-28 05:20:37,506 Stage: Train 0.5 | Epoch: 265 | Iter: 403400 | Total Loss: 0.003374 | Recon Loss: 0.002876 | Commit Loss: 0.000997 | Perplexity: 2045.205970
2025-09-28 05:21:04,170 Stage: Train 0.5 | Epoch: 265 | Iter: 403600 | Total Loss: 0.003355 | Recon Loss: 0.002860 | Commit Loss: 0.000990 | Perplexity: 2042.541740
2025-09-28 05:21:30,788 Stage: Train 0.5 | Epoch: 265 | Iter: 403800 | Total Loss: 0.003374 | Recon Loss: 0.002880 | Commit Loss: 0.000989 | Perplexity: 2036.232964
2025-09-28 05:21:57,416 Stage: Train 0.5 | Epoch: 265 | Iter: 404000 | Total Loss: 0.003397 | Recon Loss: 0.002902 | Commit Loss: 0.000991 | Perplexity: 2036.476526
Trainning Epoch:  81%|████████  | 266/330 [22:33:09<3:35:48, 202.32s/it]2025-09-28 05:22:24,206 Stage: Train 0.5 | Epoch: 266 | Iter: 404200 | Total Loss: 0.003388 | Recon Loss: 0.002898 | Commit Loss: 0.000981 | Perplexity: 2035.677076
2025-09-28 05:22:50,934 Stage: Train 0.5 | Epoch: 266 | Iter: 404400 | Total Loss: 0.003299 | Recon Loss: 0.002806 | Commit Loss: 0.000986 | Perplexity: 2040.043661
2025-09-28 05:23:17,634 Stage: Train 0.5 | Epoch: 266 | Iter: 404600 | Total Loss: 0.003409 | Recon Loss: 0.002912 | Commit Loss: 0.000995 | Perplexity: 2040.057613
2025-09-28 05:23:44,187 Stage: Train 0.5 | Epoch: 266 | Iter: 404800 | Total Loss: 0.003357 | Recon Loss: 0.002863 | Commit Loss: 0.000988 | Perplexity: 2044.135962
2025-09-28 05:24:10,777 Stage: Train 0.5 | Epoch: 266 | Iter: 405000 | Total Loss: 0.003374 | Recon Loss: 0.002877 | Commit Loss: 0.000994 | Perplexity: 2043.879217
2025-09-28 05:24:37,371 Stage: Train 0.5 | Epoch: 266 | Iter: 405200 | Total Loss: 0.003342 | Recon Loss: 0.002848 | Commit Loss: 0.000989 | Perplexity: 2036.386211
2025-09-28 05:25:03,909 Stage: Train 0.5 | Epoch: 266 | Iter: 405400 | Total Loss: 0.003430 | Recon Loss: 0.002929 | Commit Loss: 0.001003 | Perplexity: 2043.992717
Trainning Epoch:  81%|████████  | 267/330 [22:36:31<3:32:24, 202.29s/it]2025-09-28 05:25:30,656 Stage: Train 0.5 | Epoch: 267 | Iter: 405600 | Total Loss: 0.003354 | Recon Loss: 0.002859 | Commit Loss: 0.000990 | Perplexity: 2043.494132
2025-09-28 05:25:57,281 Stage: Train 0.5 | Epoch: 267 | Iter: 405800 | Total Loss: 0.003294 | Recon Loss: 0.002802 | Commit Loss: 0.000984 | Perplexity: 2040.919045
2025-09-28 05:26:23,911 Stage: Train 0.5 | Epoch: 267 | Iter: 406000 | Total Loss: 0.003382 | Recon Loss: 0.002889 | Commit Loss: 0.000987 | Perplexity: 2042.879841
2025-09-28 05:26:50,522 Stage: Train 0.5 | Epoch: 267 | Iter: 406200 | Total Loss: 0.003389 | Recon Loss: 0.002893 | Commit Loss: 0.000991 | Perplexity: 2039.833914
2025-09-28 05:27:17,149 Stage: Train 0.5 | Epoch: 267 | Iter: 406400 | Total Loss: 0.003328 | Recon Loss: 0.002835 | Commit Loss: 0.000986 | Perplexity: 2039.652869
2025-09-28 05:27:43,821 Stage: Train 0.5 | Epoch: 267 | Iter: 406600 | Total Loss: 0.003388 | Recon Loss: 0.002894 | Commit Loss: 0.000988 | Perplexity: 2046.104458
2025-09-28 05:28:10,333 Stage: Train 0.5 | Epoch: 267 | Iter: 406800 | Total Loss: 0.003281 | Recon Loss: 0.002782 | Commit Loss: 0.000997 | Perplexity: 2049.614218
2025-09-28 05:28:36,794 Stage: Train 0.5 | Epoch: 267 | Iter: 407000 | Total Loss: 0.003440 | Recon Loss: 0.002946 | Commit Loss: 0.000987 | Perplexity: 2041.072394
Trainning Epoch:  81%|████████  | 268/330 [22:39:54<3:29:00, 202.27s/it]2025-09-28 05:29:03,641 Stage: Train 0.5 | Epoch: 268 | Iter: 407200 | Total Loss: 0.003341 | Recon Loss: 0.002848 | Commit Loss: 0.000986 | Perplexity: 2041.327160
2025-09-28 05:29:30,251 Stage: Train 0.5 | Epoch: 268 | Iter: 407400 | Total Loss: 0.003382 | Recon Loss: 0.002885 | Commit Loss: 0.000995 | Perplexity: 2049.967456
2025-09-28 05:29:56,897 Stage: Train 0.5 | Epoch: 268 | Iter: 407600 | Total Loss: 0.003368 | Recon Loss: 0.002872 | Commit Loss: 0.000993 | Perplexity: 2044.715993
2025-09-28 05:30:23,298 Stage: Train 0.5 | Epoch: 268 | Iter: 407800 | Total Loss: 0.003346 | Recon Loss: 0.002855 | Commit Loss: 0.000981 | Perplexity: 2035.388375
2025-09-28 05:30:49,980 Stage: Train 0.5 | Epoch: 268 | Iter: 408000 | Total Loss: 0.003351 | Recon Loss: 0.002855 | Commit Loss: 0.000993 | Perplexity: 2040.590420
2025-09-28 05:31:16,612 Stage: Train 0.5 | Epoch: 268 | Iter: 408200 | Total Loss: 0.003457 | Recon Loss: 0.002963 | Commit Loss: 0.000989 | Perplexity: 2044.152360
2025-09-28 05:31:43,308 Stage: Train 0.5 | Epoch: 268 | Iter: 408400 | Total Loss: 0.003335 | Recon Loss: 0.002839 | Commit Loss: 0.000993 | Perplexity: 2043.900195
2025-09-28 05:32:09,878 Stage: Train 0.5 | Epoch: 268 | Iter: 408600 | Total Loss: 0.003506 | Recon Loss: 0.003012 | Commit Loss: 0.000988 | Perplexity: 2038.787082
Trainning Epoch:  82%|████████▏ | 269/330 [22:43:16<3:25:38, 202.27s/it]2025-09-28 05:32:36,707 Stage: Train 0.5 | Epoch: 269 | Iter: 408800 | Total Loss: 0.003271 | Recon Loss: 0.002779 | Commit Loss: 0.000985 | Perplexity: 2041.807969
2025-09-28 05:33:03,253 Stage: Train 0.5 | Epoch: 269 | Iter: 409000 | Total Loss: 0.003302 | Recon Loss: 0.002810 | Commit Loss: 0.000983 | Perplexity: 2038.486722
2025-09-28 05:33:29,822 Stage: Train 0.5 | Epoch: 269 | Iter: 409200 | Total Loss: 0.003408 | Recon Loss: 0.002914 | Commit Loss: 0.000987 | Perplexity: 2046.584971
2025-09-28 05:33:56,425 Stage: Train 0.5 | Epoch: 269 | Iter: 409400 | Total Loss: 0.003335 | Recon Loss: 0.002842 | Commit Loss: 0.000986 | Perplexity: 2041.076055
2025-09-28 05:34:23,117 Stage: Train 0.5 | Epoch: 269 | Iter: 409600 | Total Loss: 0.003353 | Recon Loss: 0.002859 | Commit Loss: 0.000987 | Perplexity: 2041.144927
2025-09-28 05:34:49,703 Stage: Train 0.5 | Epoch: 269 | Iter: 409800 | Total Loss: 0.003312 | Recon Loss: 0.002816 | Commit Loss: 0.000992 | Perplexity: 2046.171241
2025-09-28 05:35:16,432 Stage: Train 0.5 | Epoch: 269 | Iter: 410000 | Total Loss: 0.003371 | Recon Loss: 0.002878 | Commit Loss: 0.000985 | Perplexity: 2043.946039
Trainning Epoch:  82%|████████▏ | 270/330 [22:46:38<3:22:19, 202.32s/it]2025-09-28 05:35:43,195 Stage: Train 0.5 | Epoch: 270 | Iter: 410200 | Total Loss: 0.003370 | Recon Loss: 0.002877 | Commit Loss: 0.000987 | Perplexity: 2037.289752
2025-09-28 05:36:09,723 Stage: Train 0.5 | Epoch: 270 | Iter: 410400 | Total Loss: 0.003353 | Recon Loss: 0.002863 | Commit Loss: 0.000980 | Perplexity: 2042.284560
2025-09-28 05:36:36,370 Stage: Train 0.5 | Epoch: 270 | Iter: 410600 | Total Loss: 0.003409 | Recon Loss: 0.002915 | Commit Loss: 0.000988 | Perplexity: 2044.510371
2025-09-28 05:37:03,014 Stage: Train 0.5 | Epoch: 270 | Iter: 410800 | Total Loss: 0.003290 | Recon Loss: 0.002802 | Commit Loss: 0.000976 | Perplexity: 2040.721860
2025-09-28 05:37:29,569 Stage: Train 0.5 | Epoch: 270 | Iter: 411000 | Total Loss: 0.003318 | Recon Loss: 0.002821 | Commit Loss: 0.000994 | Perplexity: 2044.931539
2025-09-28 05:37:56,148 Stage: Train 0.5 | Epoch: 270 | Iter: 411200 | Total Loss: 0.003357 | Recon Loss: 0.002864 | Commit Loss: 0.000987 | Perplexity: 2044.420209
2025-09-28 05:38:22,763 Stage: Train 0.5 | Epoch: 270 | Iter: 411400 | Total Loss: 0.003382 | Recon Loss: 0.002889 | Commit Loss: 0.000987 | Perplexity: 2043.660927
2025-09-28 05:38:49,405 Stage: Train 0.5 | Epoch: 270 | Iter: 411600 | Total Loss: 0.003331 | Recon Loss: 0.002837 | Commit Loss: 0.000987 | Perplexity: 2039.007428
Trainning Epoch:  82%|████████▏ | 271/330 [22:50:00<3:18:54, 202.28s/it]2025-09-28 05:39:16,291 Stage: Train 0.5 | Epoch: 271 | Iter: 411800 | Total Loss: 0.003271 | Recon Loss: 0.002776 | Commit Loss: 0.000989 | Perplexity: 2042.346654
2025-09-28 05:39:42,764 Stage: Train 0.5 | Epoch: 271 | Iter: 412000 | Total Loss: 0.003352 | Recon Loss: 0.002863 | Commit Loss: 0.000979 | Perplexity: 2039.678225
2025-09-28 05:40:09,416 Stage: Train 0.5 | Epoch: 271 | Iter: 412200 | Total Loss: 0.003313 | Recon Loss: 0.002824 | Commit Loss: 0.000977 | Perplexity: 2038.098024
2025-09-28 05:40:35,864 Stage: Train 0.5 | Epoch: 271 | Iter: 412400 | Total Loss: 0.003320 | Recon Loss: 0.002828 | Commit Loss: 0.000983 | Perplexity: 2038.858043
2025-09-28 05:41:02,502 Stage: Train 0.5 | Epoch: 271 | Iter: 412600 | Total Loss: 0.003357 | Recon Loss: 0.002864 | Commit Loss: 0.000987 | Perplexity: 2047.706738
2025-09-28 05:41:29,032 Stage: Train 0.5 | Epoch: 271 | Iter: 412800 | Total Loss: 0.003386 | Recon Loss: 0.002891 | Commit Loss: 0.000992 | Perplexity: 2049.520641
2025-09-28 05:41:55,626 Stage: Train 0.5 | Epoch: 271 | Iter: 413000 | Total Loss: 0.003344 | Recon Loss: 0.002848 | Commit Loss: 0.000991 | Perplexity: 2044.940670
Trainning Epoch:  82%|████████▏ | 272/330 [22:53:22<3:15:27, 202.19s/it]2025-09-28 05:42:22,407 Stage: Train 0.5 | Epoch: 272 | Iter: 413200 | Total Loss: 0.003359 | Recon Loss: 0.002867 | Commit Loss: 0.000984 | Perplexity: 2036.654327
2025-09-28 05:42:49,008 Stage: Train 0.5 | Epoch: 272 | Iter: 413400 | Total Loss: 0.003439 | Recon Loss: 0.002948 | Commit Loss: 0.000983 | Perplexity: 2043.713595
2025-09-28 05:43:15,610 Stage: Train 0.5 | Epoch: 272 | Iter: 413600 | Total Loss: 0.003313 | Recon Loss: 0.002823 | Commit Loss: 0.000981 | Perplexity: 2041.600146
2025-09-28 05:43:42,303 Stage: Train 0.5 | Epoch: 272 | Iter: 413800 | Total Loss: 0.003330 | Recon Loss: 0.002838 | Commit Loss: 0.000983 | Perplexity: 2040.678999
2025-09-28 05:44:08,871 Stage: Train 0.5 | Epoch: 272 | Iter: 414000 | Total Loss: 0.003354 | Recon Loss: 0.002864 | Commit Loss: 0.000980 | Perplexity: 2043.425981
2025-09-28 05:44:35,461 Stage: Train 0.5 | Epoch: 272 | Iter: 414200 | Total Loss: 0.003334 | Recon Loss: 0.002841 | Commit Loss: 0.000985 | Perplexity: 2045.420460
2025-09-28 05:45:01,914 Stage: Train 0.5 | Epoch: 272 | Iter: 414400 | Total Loss: 0.003341 | Recon Loss: 0.002846 | Commit Loss: 0.000989 | Perplexity: 2049.882257
2025-09-28 05:45:28,603 Stage: Train 0.5 | Epoch: 272 | Iter: 414600 | Total Loss: 0.003352 | Recon Loss: 0.002857 | Commit Loss: 0.000990 | Perplexity: 2043.642948
Trainning Epoch:  83%|████████▎ | 273/330 [22:56:45<3:12:05, 202.20s/it]2025-09-28 05:45:55,443 Stage: Train 0.5 | Epoch: 273 | Iter: 414800 | Total Loss: 0.003295 | Recon Loss: 0.002801 | Commit Loss: 0.000987 | Perplexity: 2044.331528
2025-09-28 05:46:21,980 Stage: Train 0.5 | Epoch: 273 | Iter: 415000 | Total Loss: 0.003416 | Recon Loss: 0.002929 | Commit Loss: 0.000974 | Perplexity: 2036.362475
2025-09-28 05:46:48,607 Stage: Train 0.5 | Epoch: 273 | Iter: 415200 | Total Loss: 0.003389 | Recon Loss: 0.002896 | Commit Loss: 0.000987 | Perplexity: 2043.844137
2025-09-28 05:47:15,170 Stage: Train 0.5 | Epoch: 273 | Iter: 415400 | Total Loss: 0.003384 | Recon Loss: 0.002895 | Commit Loss: 0.000979 | Perplexity: 2043.871830
2025-09-28 05:47:41,653 Stage: Train 0.5 | Epoch: 273 | Iter: 415600 | Total Loss: 0.003344 | Recon Loss: 0.002854 | Commit Loss: 0.000980 | Perplexity: 2039.203586
2025-09-28 05:48:08,236 Stage: Train 0.5 | Epoch: 273 | Iter: 415800 | Total Loss: 0.003337 | Recon Loss: 0.002845 | Commit Loss: 0.000984 | Perplexity: 2045.761245
2025-09-28 05:48:34,829 Stage: Train 0.5 | Epoch: 273 | Iter: 416000 | Total Loss: 0.003329 | Recon Loss: 0.002834 | Commit Loss: 0.000990 | Perplexity: 2043.115209
2025-09-28 05:49:01,337 Stage: Train 0.5 | Epoch: 273 | Iter: 416200 | Total Loss: 0.003338 | Recon Loss: 0.002849 | Commit Loss: 0.000978 | Perplexity: 2044.068395
Trainning Epoch:  83%|████████▎ | 274/330 [23:00:07<3:08:39, 202.13s/it]2025-09-28 05:49:28,126 Stage: Train 0.5 | Epoch: 274 | Iter: 416400 | Total Loss: 0.003378 | Recon Loss: 0.002887 | Commit Loss: 0.000982 | Perplexity: 2037.911501
2025-09-28 05:49:54,683 Stage: Train 0.5 | Epoch: 274 | Iter: 416600 | Total Loss: 0.003311 | Recon Loss: 0.002825 | Commit Loss: 0.000970 | Perplexity: 2040.893806
2025-09-28 05:50:21,310 Stage: Train 0.5 | Epoch: 274 | Iter: 416800 | Total Loss: 0.003348 | Recon Loss: 0.002856 | Commit Loss: 0.000985 | Perplexity: 2046.746064
2025-09-28 05:50:48,068 Stage: Train 0.5 | Epoch: 274 | Iter: 417000 | Total Loss: 0.003352 | Recon Loss: 0.002861 | Commit Loss: 0.000983 | Perplexity: 2040.562555
2025-09-28 05:51:14,711 Stage: Train 0.5 | Epoch: 274 | Iter: 417200 | Total Loss: 0.003260 | Recon Loss: 0.002768 | Commit Loss: 0.000983 | Perplexity: 2045.517172
2025-09-28 05:51:41,248 Stage: Train 0.5 | Epoch: 274 | Iter: 417400 | Total Loss: 0.003301 | Recon Loss: 0.002813 | Commit Loss: 0.000976 | Perplexity: 2039.008163
2025-09-28 05:52:07,675 Stage: Train 0.5 | Epoch: 274 | Iter: 417600 | Total Loss: 0.003352 | Recon Loss: 0.002860 | Commit Loss: 0.000984 | Perplexity: 2043.235128
Trainning Epoch:  83%|████████▎ | 275/330 [23:03:29<3:05:18, 202.16s/it]2025-09-28 05:52:34,583 Stage: Train 0.5 | Epoch: 275 | Iter: 417800 | Total Loss: 0.003367 | Recon Loss: 0.002875 | Commit Loss: 0.000984 | Perplexity: 2042.406378
2025-09-28 05:53:01,231 Stage: Train 0.5 | Epoch: 275 | Iter: 418000 | Total Loss: 0.003268 | Recon Loss: 0.002780 | Commit Loss: 0.000977 | Perplexity: 2045.018207
2025-09-28 05:53:27,877 Stage: Train 0.5 | Epoch: 275 | Iter: 418200 | Total Loss: 0.003388 | Recon Loss: 0.002897 | Commit Loss: 0.000982 | Perplexity: 2040.521792
2025-09-28 05:53:54,556 Stage: Train 0.5 | Epoch: 275 | Iter: 418400 | Total Loss: 0.003324 | Recon Loss: 0.002837 | Commit Loss: 0.000974 | Perplexity: 2036.963170
2025-09-28 05:54:21,119 Stage: Train 0.5 | Epoch: 275 | Iter: 418600 | Total Loss: 0.003296 | Recon Loss: 0.002805 | Commit Loss: 0.000981 | Perplexity: 2042.243404
2025-09-28 05:54:47,705 Stage: Train 0.5 | Epoch: 275 | Iter: 418800 | Total Loss: 0.003711 | Recon Loss: 0.003218 | Commit Loss: 0.000986 | Perplexity: 2044.341357
2025-09-28 05:55:14,315 Stage: Train 0.5 | Epoch: 275 | Iter: 419000 | Total Loss: 0.003263 | Recon Loss: 0.002775 | Commit Loss: 0.000976 | Perplexity: 2043.524553
2025-09-28 05:55:40,954 Stage: Train 0.5 | Epoch: 275 | Iter: 419200 | Total Loss: 0.003226 | Recon Loss: 0.002733 | Commit Loss: 0.000986 | Perplexity: 2046.168575
Trainning Epoch:  84%|████████▎ | 276/330 [23:06:51<3:02:00, 202.23s/it]2025-09-28 05:56:07,700 Stage: Train 0.5 | Epoch: 276 | Iter: 419400 | Total Loss: 0.003343 | Recon Loss: 0.002853 | Commit Loss: 0.000980 | Perplexity: 2037.728920
2025-09-28 05:56:34,290 Stage: Train 0.5 | Epoch: 276 | Iter: 419600 | Total Loss: 0.003425 | Recon Loss: 0.002935 | Commit Loss: 0.000981 | Perplexity: 2041.478506
2025-09-28 05:57:00,804 Stage: Train 0.5 | Epoch: 276 | Iter: 419800 | Total Loss: 0.003267 | Recon Loss: 0.002779 | Commit Loss: 0.000977 | Perplexity: 2041.838034
2025-09-28 05:57:27,364 Stage: Train 0.5 | Epoch: 276 | Iter: 420000 | Total Loss: 0.003365 | Recon Loss: 0.002878 | Commit Loss: 0.000974 | Perplexity: 2044.105511
2025-09-28 05:57:27,364 Saving model at iteration 420000
2025-09-28 05:57:27,574 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000
2025-09-28 05:57:28,067 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/model.safetensors
2025-09-28 05:57:28,641 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/optimizer.bin
2025-09-28 05:57:28,642 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/scheduler.bin
2025-09-28 05:57:28,642 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/sampler.bin
2025-09-28 05:57:28,643 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/random_states_0.pkl
2025-09-28 05:57:55,161 Stage: Train 0.5 | Epoch: 276 | Iter: 420200 | Total Loss: 0.003257 | Recon Loss: 0.002771 | Commit Loss: 0.000973 | Perplexity: 2044.883699
2025-09-28 05:58:21,840 Stage: Train 0.5 | Epoch: 276 | Iter: 420400 | Total Loss: 0.003349 | Recon Loss: 0.002858 | Commit Loss: 0.000982 | Perplexity: 2043.474447
2025-09-28 05:58:48,425 Stage: Train 0.5 | Epoch: 276 | Iter: 420600 | Total Loss: 0.003508 | Recon Loss: 0.003016 | Commit Loss: 0.000984 | Perplexity: 2039.482822
Trainning Epoch:  84%|████████▍ | 277/330 [23:10:15<2:58:55, 202.56s/it]2025-09-28 05:59:15,176 Stage: Train 0.5 | Epoch: 277 | Iter: 420800 | Total Loss: 0.003257 | Recon Loss: 0.002770 | Commit Loss: 0.000974 | Perplexity: 2040.001525
2025-09-28 05:59:41,761 Stage: Train 0.5 | Epoch: 277 | Iter: 421000 | Total Loss: 0.003284 | Recon Loss: 0.002797 | Commit Loss: 0.000974 | Perplexity: 2040.156558
2025-09-28 06:00:08,276 Stage: Train 0.5 | Epoch: 277 | Iter: 421200 | Total Loss: 0.003372 | Recon Loss: 0.002883 | Commit Loss: 0.000977 | Perplexity: 2043.505828
2025-09-28 06:00:34,802 Stage: Train 0.5 | Epoch: 277 | Iter: 421400 | Total Loss: 0.003265 | Recon Loss: 0.002780 | Commit Loss: 0.000970 | Perplexity: 2039.695030
2025-09-28 06:01:01,451 Stage: Train 0.5 | Epoch: 277 | Iter: 421600 | Total Loss: 0.003332 | Recon Loss: 0.002843 | Commit Loss: 0.000979 | Perplexity: 2041.670286
2025-09-28 06:01:27,988 Stage: Train 0.5 | Epoch: 277 | Iter: 421800 | Total Loss: 0.003359 | Recon Loss: 0.002868 | Commit Loss: 0.000982 | Perplexity: 2045.583748
2025-09-28 06:01:54,534 Stage: Train 0.5 | Epoch: 277 | Iter: 422000 | Total Loss: 0.003325 | Recon Loss: 0.002832 | Commit Loss: 0.000986 | Perplexity: 2042.951237
2025-09-28 06:02:21,047 Stage: Train 0.5 | Epoch: 277 | Iter: 422200 | Total Loss: 0.003339 | Recon Loss: 0.002854 | Commit Loss: 0.000971 | Perplexity: 2037.499023
Trainning Epoch:  84%|████████▍ | 278/330 [23:13:36<2:55:22, 202.35s/it]2025-09-28 06:02:47,876 Stage: Train 0.5 | Epoch: 278 | Iter: 422400 | Total Loss: 0.003317 | Recon Loss: 0.002830 | Commit Loss: 0.000975 | Perplexity: 2043.241871
2025-09-28 06:03:14,528 Stage: Train 0.5 | Epoch: 278 | Iter: 422600 | Total Loss: 0.003297 | Recon Loss: 0.002807 | Commit Loss: 0.000980 | Perplexity: 2045.501504
2025-09-28 06:03:41,115 Stage: Train 0.5 | Epoch: 278 | Iter: 422800 | Total Loss: 0.003299 | Recon Loss: 0.002810 | Commit Loss: 0.000978 | Perplexity: 2045.309061
2025-09-28 06:04:07,680 Stage: Train 0.5 | Epoch: 278 | Iter: 423000 | Total Loss: 0.003349 | Recon Loss: 0.002862 | Commit Loss: 0.000974 | Perplexity: 2043.973694
2025-09-28 06:04:34,205 Stage: Train 0.5 | Epoch: 278 | Iter: 423200 | Total Loss: 0.003349 | Recon Loss: 0.002859 | Commit Loss: 0.000981 | Perplexity: 2046.941126
2025-09-28 06:05:00,623 Stage: Train 0.5 | Epoch: 278 | Iter: 423400 | Total Loss: 0.003324 | Recon Loss: 0.002837 | Commit Loss: 0.000974 | Perplexity: 2039.842599
2025-09-28 06:05:27,109 Stage: Train 0.5 | Epoch: 278 | Iter: 423600 | Total Loss: 0.003320 | Recon Loss: 0.002830 | Commit Loss: 0.000980 | Perplexity: 2047.662659
2025-09-28 06:05:53,740 Stage: Train 0.5 | Epoch: 278 | Iter: 423800 | Total Loss: 0.003314 | Recon Loss: 0.002829 | Commit Loss: 0.000972 | Perplexity: 2042.396852
Trainning Epoch:  85%|████████▍ | 279/330 [23:16:58<2:51:53, 202.22s/it]2025-09-28 06:06:20,537 Stage: Train 0.5 | Epoch: 279 | Iter: 424000 | Total Loss: 0.003276 | Recon Loss: 0.002789 | Commit Loss: 0.000974 | Perplexity: 2042.994891
2025-09-28 06:06:47,152 Stage: Train 0.5 | Epoch: 279 | Iter: 424200 | Total Loss: 0.003277 | Recon Loss: 0.002790 | Commit Loss: 0.000973 | Perplexity: 2042.121892
2025-09-28 06:07:13,781 Stage: Train 0.5 | Epoch: 279 | Iter: 424400 | Total Loss: 0.003408 | Recon Loss: 0.002923 | Commit Loss: 0.000971 | Perplexity: 2040.493903
2025-09-28 06:07:40,363 Stage: Train 0.5 | Epoch: 279 | Iter: 424600 | Total Loss: 0.003268 | Recon Loss: 0.002782 | Commit Loss: 0.000972 | Perplexity: 2032.825582
2025-09-28 06:08:06,943 Stage: Train 0.5 | Epoch: 279 | Iter: 424800 | Total Loss: 0.003383 | Recon Loss: 0.002896 | Commit Loss: 0.000974 | Perplexity: 2046.415912
2025-09-28 06:08:33,483 Stage: Train 0.5 | Epoch: 279 | Iter: 425000 | Total Loss: 0.003471 | Recon Loss: 0.002982 | Commit Loss: 0.000978 | Perplexity: 2045.312633
2025-09-28 06:08:59,990 Stage: Train 0.5 | Epoch: 279 | Iter: 425200 | Total Loss: 0.003262 | Recon Loss: 0.002771 | Commit Loss: 0.000983 | Perplexity: 2046.441444
Trainning Epoch:  85%|████████▍ | 280/330 [23:20:21<2:48:29, 202.18s/it]2025-09-28 06:09:26,780 Stage: Train 0.5 | Epoch: 280 | Iter: 425400 | Total Loss: 0.003470 | Recon Loss: 0.002979 | Commit Loss: 0.000982 | Perplexity: 2048.046831
2025-09-28 06:09:53,414 Stage: Train 0.5 | Epoch: 280 | Iter: 425600 | Total Loss: 0.003211 | Recon Loss: 0.002727 | Commit Loss: 0.000968 | Perplexity: 2041.358231
2025-09-28 06:10:20,015 Stage: Train 0.5 | Epoch: 280 | Iter: 425800 | Total Loss: 0.003324 | Recon Loss: 0.002834 | Commit Loss: 0.000981 | Perplexity: 2053.485513
2025-09-28 06:10:46,616 Stage: Train 0.5 | Epoch: 280 | Iter: 426000 | Total Loss: 0.003352 | Recon Loss: 0.002867 | Commit Loss: 0.000969 | Perplexity: 2042.051302
2025-09-28 06:11:13,243 Stage: Train 0.5 | Epoch: 280 | Iter: 426200 | Total Loss: 0.003292 | Recon Loss: 0.002806 | Commit Loss: 0.000972 | Perplexity: 2044.632266
2025-09-28 06:11:39,833 Stage: Train 0.5 | Epoch: 280 | Iter: 426400 | Total Loss: 0.003298 | Recon Loss: 0.002813 | Commit Loss: 0.000970 | Perplexity: 2039.417072
2025-09-28 06:12:06,365 Stage: Train 0.5 | Epoch: 280 | Iter: 426600 | Total Loss: 0.003320 | Recon Loss: 0.002835 | Commit Loss: 0.000969 | Perplexity: 2044.639972
2025-09-28 06:12:32,984 Stage: Train 0.5 | Epoch: 280 | Iter: 426800 | Total Loss: 0.003326 | Recon Loss: 0.002838 | Commit Loss: 0.000977 | Perplexity: 2050.278671
Trainning Epoch:  85%|████████▌ | 281/330 [23:23:43<2:45:07, 202.18s/it]2025-09-28 06:12:59,686 Stage: Train 0.5 | Epoch: 281 | Iter: 427000 | Total Loss: 0.003312 | Recon Loss: 0.002826 | Commit Loss: 0.000972 | Perplexity: 2040.606400
2025-09-28 06:13:26,348 Stage: Train 0.5 | Epoch: 281 | Iter: 427200 | Total Loss: 0.003307 | Recon Loss: 0.002823 | Commit Loss: 0.000969 | Perplexity: 2043.974417
2025-09-28 06:13:52,782 Stage: Train 0.5 | Epoch: 281 | Iter: 427400 | Total Loss: 0.003326 | Recon Loss: 0.002839 | Commit Loss: 0.000975 | Perplexity: 2047.948645
2025-09-28 06:14:19,426 Stage: Train 0.5 | Epoch: 281 | Iter: 427600 | Total Loss: 0.003294 | Recon Loss: 0.002807 | Commit Loss: 0.000974 | Perplexity: 2041.559728
2025-09-28 06:14:46,084 Stage: Train 0.5 | Epoch: 281 | Iter: 427800 | Total Loss: 0.003270 | Recon Loss: 0.002783 | Commit Loss: 0.000974 | Perplexity: 2044.036105
2025-09-28 06:15:12,752 Stage: Train 0.5 | Epoch: 281 | Iter: 428000 | Total Loss: 0.003309 | Recon Loss: 0.002822 | Commit Loss: 0.000974 | Perplexity: 2041.081025
2025-09-28 06:15:39,369 Stage: Train 0.5 | Epoch: 281 | Iter: 428200 | Total Loss: 0.003307 | Recon Loss: 0.002820 | Commit Loss: 0.000973 | Perplexity: 2044.989013
Trainning Epoch:  85%|████████▌ | 282/330 [23:27:05<2:41:45, 202.20s/it]2025-09-28 06:16:06,195 Stage: Train 0.5 | Epoch: 282 | Iter: 428400 | Total Loss: 0.003339 | Recon Loss: 0.002848 | Commit Loss: 0.000981 | Perplexity: 2048.321732
2025-09-28 06:16:32,751 Stage: Train 0.5 | Epoch: 282 | Iter: 428600 | Total Loss: 0.003331 | Recon Loss: 0.002846 | Commit Loss: 0.000970 | Perplexity: 2042.377982
2025-09-28 06:16:59,275 Stage: Train 0.5 | Epoch: 282 | Iter: 428800 | Total Loss: 0.003279 | Recon Loss: 0.002797 | Commit Loss: 0.000964 | Perplexity: 2040.454936
2025-09-28 06:17:25,416 Stage: Train 0.5 | Epoch: 282 | Iter: 429000 | Total Loss: 0.003274 | Recon Loss: 0.002788 | Commit Loss: 0.000972 | Perplexity: 2046.080919
2025-09-28 06:17:51,891 Stage: Train 0.5 | Epoch: 282 | Iter: 429200 | Total Loss: 0.003324 | Recon Loss: 0.002839 | Commit Loss: 0.000972 | Perplexity: 2044.365375
2025-09-28 06:18:18,479 Stage: Train 0.5 | Epoch: 282 | Iter: 429400 | Total Loss: 0.003376 | Recon Loss: 0.002886 | Commit Loss: 0.000980 | Perplexity: 2042.304766
2025-09-28 06:18:45,048 Stage: Train 0.5 | Epoch: 282 | Iter: 429600 | Total Loss: 0.003263 | Recon Loss: 0.002776 | Commit Loss: 0.000974 | Perplexity: 2040.829587
2025-09-28 06:19:11,540 Stage: Train 0.5 | Epoch: 282 | Iter: 429800 | Total Loss: 0.003313 | Recon Loss: 0.002826 | Commit Loss: 0.000973 | Perplexity: 2041.484434
Trainning Epoch:  86%|████████▌ | 283/330 [23:30:26<2:38:12, 201.96s/it]2025-09-28 06:19:38,368 Stage: Train 0.5 | Epoch: 283 | Iter: 430000 | Total Loss: 0.003261 | Recon Loss: 0.002775 | Commit Loss: 0.000973 | Perplexity: 2041.030474
2025-09-28 06:20:04,848 Stage: Train 0.5 | Epoch: 283 | Iter: 430200 | Total Loss: 0.003240 | Recon Loss: 0.002753 | Commit Loss: 0.000976 | Perplexity: 2047.456536
2025-09-28 06:20:31,476 Stage: Train 0.5 | Epoch: 283 | Iter: 430400 | Total Loss: 0.003298 | Recon Loss: 0.002814 | Commit Loss: 0.000968 | Perplexity: 2040.142917
2025-09-28 06:20:58,159 Stage: Train 0.5 | Epoch: 283 | Iter: 430600 | Total Loss: 0.003347 | Recon Loss: 0.002858 | Commit Loss: 0.000977 | Perplexity: 2049.429553
2025-09-28 06:21:24,807 Stage: Train 0.5 | Epoch: 283 | Iter: 430800 | Total Loss: 0.003361 | Recon Loss: 0.002874 | Commit Loss: 0.000974 | Perplexity: 2042.042213
2025-09-28 06:21:51,512 Stage: Train 0.5 | Epoch: 283 | Iter: 431000 | Total Loss: 0.003387 | Recon Loss: 0.002901 | Commit Loss: 0.000972 | Perplexity: 2046.561265
2025-09-28 06:22:18,073 Stage: Train 0.5 | Epoch: 283 | Iter: 431200 | Total Loss: 0.003234 | Recon Loss: 0.002749 | Commit Loss: 0.000970 | Perplexity: 2047.418762
Trainning Epoch:  86%|████████▌ | 284/330 [23:33:49<2:34:54, 202.06s/it]2025-09-28 06:22:44,816 Stage: Train 0.5 | Epoch: 284 | Iter: 431400 | Total Loss: 0.003342 | Recon Loss: 0.002856 | Commit Loss: 0.000973 | Perplexity: 2042.895988
2025-09-28 06:23:11,476 Stage: Train 0.5 | Epoch: 284 | Iter: 431600 | Total Loss: 0.003291 | Recon Loss: 0.002804 | Commit Loss: 0.000975 | Perplexity: 2045.709570
2025-09-28 06:23:38,058 Stage: Train 0.5 | Epoch: 284 | Iter: 431800 | Total Loss: 0.003285 | Recon Loss: 0.002801 | Commit Loss: 0.000968 | Perplexity: 2043.793563
2025-09-28 06:24:04,699 Stage: Train 0.5 | Epoch: 284 | Iter: 432000 | Total Loss: 0.003335 | Recon Loss: 0.002849 | Commit Loss: 0.000973 | Perplexity: 2047.034360
2025-09-28 06:24:31,398 Stage: Train 0.5 | Epoch: 284 | Iter: 432200 | Total Loss: 0.003260 | Recon Loss: 0.002779 | Commit Loss: 0.000963 | Perplexity: 2045.590238
2025-09-28 06:24:58,010 Stage: Train 0.5 | Epoch: 284 | Iter: 432400 | Total Loss: 0.003278 | Recon Loss: 0.002793 | Commit Loss: 0.000971 | Perplexity: 2046.531861
2025-09-28 06:25:24,664 Stage: Train 0.5 | Epoch: 284 | Iter: 432600 | Total Loss: 0.003259 | Recon Loss: 0.002773 | Commit Loss: 0.000971 | Perplexity: 2045.585451
2025-09-28 06:25:51,327 Stage: Train 0.5 | Epoch: 284 | Iter: 432800 | Total Loss: 0.003273 | Recon Loss: 0.002788 | Commit Loss: 0.000970 | Perplexity: 2042.334462
Trainning Epoch:  86%|████████▋ | 285/330 [23:37:11<2:31:39, 202.20s/it]2025-09-28 06:26:18,125 Stage: Train 0.5 | Epoch: 285 | Iter: 433000 | Total Loss: 0.003296 | Recon Loss: 0.002811 | Commit Loss: 0.000970 | Perplexity: 2042.779164
2025-09-28 06:26:44,806 Stage: Train 0.5 | Epoch: 285 | Iter: 433200 | Total Loss: 0.003361 | Recon Loss: 0.002878 | Commit Loss: 0.000967 | Perplexity: 2044.082407
2025-09-28 06:27:11,375 Stage: Train 0.5 | Epoch: 285 | Iter: 433400 | Total Loss: 0.003244 | Recon Loss: 0.002761 | Commit Loss: 0.000967 | Perplexity: 2044.866055
2025-09-28 06:27:37,790 Stage: Train 0.5 | Epoch: 285 | Iter: 433600 | Total Loss: 0.003329 | Recon Loss: 0.002845 | Commit Loss: 0.000968 | Perplexity: 2043.442772
2025-09-28 06:28:04,309 Stage: Train 0.5 | Epoch: 285 | Iter: 433800 | Total Loss: 0.003293 | Recon Loss: 0.002808 | Commit Loss: 0.000970 | Perplexity: 2042.781715
2025-09-28 06:28:30,913 Stage: Train 0.5 | Epoch: 285 | Iter: 434000 | Total Loss: 0.003487 | Recon Loss: 0.003003 | Commit Loss: 0.000967 | Perplexity: 2037.582699
2025-09-28 06:28:57,446 Stage: Train 0.5 | Epoch: 285 | Iter: 434200 | Total Loss: 0.003239 | Recon Loss: 0.002754 | Commit Loss: 0.000969 | Perplexity: 2048.613894
2025-09-28 06:29:24,057 Stage: Train 0.5 | Epoch: 285 | Iter: 434400 | Total Loss: 0.003362 | Recon Loss: 0.002875 | Commit Loss: 0.000973 | Perplexity: 2044.416353
Trainning Epoch:  87%|████████▋ | 286/330 [23:40:33<2:28:13, 202.12s/it]2025-09-28 06:29:50,796 Stage: Train 0.5 | Epoch: 286 | Iter: 434600 | Total Loss: 0.003349 | Recon Loss: 0.002863 | Commit Loss: 0.000972 | Perplexity: 2050.916841
2025-09-28 06:30:17,388 Stage: Train 0.5 | Epoch: 286 | Iter: 434800 | Total Loss: 0.003234 | Recon Loss: 0.002749 | Commit Loss: 0.000970 | Perplexity: 2043.700413
2025-09-28 06:30:43,932 Stage: Train 0.5 | Epoch: 286 | Iter: 435000 | Total Loss: 0.003333 | Recon Loss: 0.002848 | Commit Loss: 0.000971 | Perplexity: 2046.177162
2025-09-28 06:31:10,526 Stage: Train 0.5 | Epoch: 286 | Iter: 435200 | Total Loss: 0.003246 | Recon Loss: 0.002761 | Commit Loss: 0.000969 | Perplexity: 2050.473207
2025-09-28 06:31:37,230 Stage: Train 0.5 | Epoch: 286 | Iter: 435400 | Total Loss: 0.003236 | Recon Loss: 0.002755 | Commit Loss: 0.000962 | Perplexity: 2038.342333
2025-09-28 06:32:03,852 Stage: Train 0.5 | Epoch: 286 | Iter: 435600 | Total Loss: 0.003321 | Recon Loss: 0.002834 | Commit Loss: 0.000974 | Perplexity: 2044.288573
2025-09-28 06:32:30,407 Stage: Train 0.5 | Epoch: 286 | Iter: 435800 | Total Loss: 0.003283 | Recon Loss: 0.002798 | Commit Loss: 0.000970 | Perplexity: 2043.718269
Trainning Epoch:  87%|████████▋ | 287/330 [23:43:55<2:24:52, 202.15s/it]2025-09-28 06:32:57,220 Stage: Train 0.5 | Epoch: 287 | Iter: 436000 | Total Loss: 0.003352 | Recon Loss: 0.002872 | Commit Loss: 0.000961 | Perplexity: 2040.990977
2025-09-28 06:33:23,910 Stage: Train 0.5 | Epoch: 287 | Iter: 436200 | Total Loss: 0.003349 | Recon Loss: 0.002865 | Commit Loss: 0.000968 | Perplexity: 2048.721568
2025-09-28 06:33:50,608 Stage: Train 0.5 | Epoch: 287 | Iter: 436400 | Total Loss: 0.003183 | Recon Loss: 0.002699 | Commit Loss: 0.000968 | Perplexity: 2045.266712
2025-09-28 06:34:17,189 Stage: Train 0.5 | Epoch: 287 | Iter: 436600 | Total Loss: 0.003329 | Recon Loss: 0.002848 | Commit Loss: 0.000962 | Perplexity: 2041.535769
2025-09-28 06:34:43,654 Stage: Train 0.5 | Epoch: 287 | Iter: 436800 | Total Loss: 0.003232 | Recon Loss: 0.002750 | Commit Loss: 0.000964 | Perplexity: 2043.438950
2025-09-28 06:35:10,232 Stage: Train 0.5 | Epoch: 287 | Iter: 437000 | Total Loss: 0.003351 | Recon Loss: 0.002868 | Commit Loss: 0.000966 | Perplexity: 2041.300797
2025-09-28 06:35:36,865 Stage: Train 0.5 | Epoch: 287 | Iter: 437200 | Total Loss: 0.003234 | Recon Loss: 0.002750 | Commit Loss: 0.000966 | Perplexity: 2046.595623
2025-09-28 06:36:03,534 Stage: Train 0.5 | Epoch: 287 | Iter: 437400 | Total Loss: 0.003339 | Recon Loss: 0.002855 | Commit Loss: 0.000967 | Perplexity: 2042.018483
Trainning Epoch:  87%|████████▋ | 288/330 [23:47:18<2:21:32, 202.20s/it]2025-09-28 06:36:30,342 Stage: Train 0.5 | Epoch: 288 | Iter: 437600 | Total Loss: 0.003293 | Recon Loss: 0.002808 | Commit Loss: 0.000970 | Perplexity: 2046.749842
2025-09-28 06:36:56,902 Stage: Train 0.5 | Epoch: 288 | Iter: 437800 | Total Loss: 0.003293 | Recon Loss: 0.002813 | Commit Loss: 0.000960 | Perplexity: 2044.712364
2025-09-28 06:37:23,339 Stage: Train 0.5 | Epoch: 288 | Iter: 438000 | Total Loss: 0.003306 | Recon Loss: 0.002824 | Commit Loss: 0.000963 | Perplexity: 2040.849096
2025-09-28 06:37:49,877 Stage: Train 0.5 | Epoch: 288 | Iter: 438200 | Total Loss: 0.003275 | Recon Loss: 0.002795 | Commit Loss: 0.000961 | Perplexity: 2046.554137
2025-09-28 06:38:16,273 Stage: Train 0.5 | Epoch: 288 | Iter: 438400 | Total Loss: 0.003307 | Recon Loss: 0.002823 | Commit Loss: 0.000968 | Perplexity: 2046.549266
2025-09-28 06:38:42,899 Stage: Train 0.5 | Epoch: 288 | Iter: 438600 | Total Loss: 0.003271 | Recon Loss: 0.002788 | Commit Loss: 0.000967 | Perplexity: 2044.257629
2025-09-28 06:39:09,484 Stage: Train 0.5 | Epoch: 288 | Iter: 438800 | Total Loss: 0.003276 | Recon Loss: 0.002793 | Commit Loss: 0.000965 | Perplexity: 2039.994937
Trainning Epoch:  88%|████████▊ | 289/330 [23:50:39<2:18:06, 202.10s/it]2025-09-28 06:39:36,393 Stage: Train 0.5 | Epoch: 289 | Iter: 439000 | Total Loss: 0.003257 | Recon Loss: 0.002773 | Commit Loss: 0.000968 | Perplexity: 2044.557943
2025-09-28 06:40:02,897 Stage: Train 0.5 | Epoch: 289 | Iter: 439200 | Total Loss: 0.003297 | Recon Loss: 0.002813 | Commit Loss: 0.000966 | Perplexity: 2050.144815
2025-09-28 06:40:29,476 Stage: Train 0.5 | Epoch: 289 | Iter: 439400 | Total Loss: 0.003232 | Recon Loss: 0.002748 | Commit Loss: 0.000967 | Perplexity: 2048.757736
2025-09-28 06:40:56,019 Stage: Train 0.5 | Epoch: 289 | Iter: 439600 | Total Loss: 0.003353 | Recon Loss: 0.002872 | Commit Loss: 0.000963 | Perplexity: 2043.646471
2025-09-28 06:41:22,525 Stage: Train 0.5 | Epoch: 289 | Iter: 439800 | Total Loss: 0.003214 | Recon Loss: 0.002731 | Commit Loss: 0.000965 | Perplexity: 2041.572155
2025-09-28 06:41:49,171 Stage: Train 0.5 | Epoch: 289 | Iter: 440000 | Total Loss: 0.003282 | Recon Loss: 0.002801 | Commit Loss: 0.000963 | Perplexity: 2043.661589
2025-09-28 06:41:49,171 Saving model at iteration 440000
2025-09-28 06:41:49,525 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000
2025-09-28 06:41:49,976 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/model.safetensors
2025-09-28 06:41:50,452 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/optimizer.bin
2025-09-28 06:41:50,452 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/scheduler.bin
2025-09-28 06:41:50,453 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/sampler.bin
2025-09-28 06:41:50,453 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/random_states_0.pkl
2025-09-28 06:42:17,079 Stage: Train 0.5 | Epoch: 289 | Iter: 440200 | Total Loss: 0.003294 | Recon Loss: 0.002815 | Commit Loss: 0.000958 | Perplexity: 2038.568710
2025-09-28 06:42:43,608 Stage: Train 0.5 | Epoch: 289 | Iter: 440400 | Total Loss: 0.003240 | Recon Loss: 0.002757 | Commit Loss: 0.000967 | Perplexity: 2045.431478
Trainning Epoch:  88%|████████▊ | 290/330 [23:54:03<2:14:58, 202.47s/it]2025-09-28 06:43:10,504 Stage: Train 0.5 | Epoch: 290 | Iter: 440600 | Total Loss: 0.003330 | Recon Loss: 0.002850 | Commit Loss: 0.000958 | Perplexity: 2042.329810
2025-09-28 06:43:37,091 Stage: Train 0.5 | Epoch: 290 | Iter: 440800 | Total Loss: 0.003250 | Recon Loss: 0.002770 | Commit Loss: 0.000961 | Perplexity: 2046.891226
2025-09-28 06:44:03,674 Stage: Train 0.5 | Epoch: 290 | Iter: 441000 | Total Loss: 0.003391 | Recon Loss: 0.002908 | Commit Loss: 0.000965 | Perplexity: 2043.249854
2025-09-28 06:44:30,287 Stage: Train 0.5 | Epoch: 290 | Iter: 441200 | Total Loss: 0.003234 | Recon Loss: 0.002755 | Commit Loss: 0.000957 | Perplexity: 2043.443035
2025-09-28 06:44:56,869 Stage: Train 0.5 | Epoch: 290 | Iter: 441400 | Total Loss: 0.003285 | Recon Loss: 0.002804 | Commit Loss: 0.000962 | Perplexity: 2041.150867
2025-09-28 06:45:23,481 Stage: Train 0.5 | Epoch: 290 | Iter: 441600 | Total Loss: 0.003309 | Recon Loss: 0.002826 | Commit Loss: 0.000966 | Perplexity: 2047.683748
2025-09-28 06:45:50,187 Stage: Train 0.5 | Epoch: 290 | Iter: 441800 | Total Loss: 0.003486 | Recon Loss: 0.003003 | Commit Loss: 0.000966 | Perplexity: 2041.244470
2025-09-28 06:46:16,740 Stage: Train 0.5 | Epoch: 290 | Iter: 442000 | Total Loss: 0.003224 | Recon Loss: 0.002741 | Commit Loss: 0.000965 | Perplexity: 2045.290492
Trainning Epoch:  88%|████████▊ | 291/330 [23:57:25<2:11:34, 202.42s/it]2025-09-28 06:46:43,502 Stage: Train 0.5 | Epoch: 291 | Iter: 442200 | Total Loss: 0.003236 | Recon Loss: 0.002753 | Commit Loss: 0.000965 | Perplexity: 2045.531332
2025-09-28 06:47:10,145 Stage: Train 0.5 | Epoch: 291 | Iter: 442400 | Total Loss: 0.003465 | Recon Loss: 0.002985 | Commit Loss: 0.000960 | Perplexity: 2046.109594
2025-09-28 06:47:36,773 Stage: Train 0.5 | Epoch: 291 | Iter: 442600 | Total Loss: 0.003207 | Recon Loss: 0.002725 | Commit Loss: 0.000964 | Perplexity: 2046.669959
2025-09-28 06:48:03,552 Stage: Train 0.5 | Epoch: 291 | Iter: 442800 | Total Loss: 0.003313 | Recon Loss: 0.002831 | Commit Loss: 0.000964 | Perplexity: 2043.808230
2025-09-28 06:48:30,102 Stage: Train 0.5 | Epoch: 291 | Iter: 443000 | Total Loss: 0.003235 | Recon Loss: 0.002756 | Commit Loss: 0.000957 | Perplexity: 2046.907694
2025-09-28 06:48:56,825 Stage: Train 0.5 | Epoch: 291 | Iter: 443200 | Total Loss: 0.003365 | Recon Loss: 0.002885 | Commit Loss: 0.000960 | Perplexity: 2039.553803
2025-09-28 06:49:23,516 Stage: Train 0.5 | Epoch: 291 | Iter: 443400 | Total Loss: 0.003204 | Recon Loss: 0.002725 | Commit Loss: 0.000958 | Perplexity: 2043.938154
Trainning Epoch:  88%|████████▊ | 292/330 [24:00:48<2:08:14, 202.50s/it]2025-09-28 06:49:50,428 Stage: Train 0.5 | Epoch: 292 | Iter: 443600 | Total Loss: 0.003237 | Recon Loss: 0.002756 | Commit Loss: 0.000962 | Perplexity: 2044.630652
2025-09-28 06:50:17,123 Stage: Train 0.5 | Epoch: 292 | Iter: 443800 | Total Loss: 0.003288 | Recon Loss: 0.002804 | Commit Loss: 0.000968 | Perplexity: 2046.265138
2025-09-28 06:50:43,825 Stage: Train 0.5 | Epoch: 292 | Iter: 444000 | Total Loss: 0.003240 | Recon Loss: 0.002761 | Commit Loss: 0.000958 | Perplexity: 2044.438289
2025-09-28 06:51:10,386 Stage: Train 0.5 | Epoch: 292 | Iter: 444200 | Total Loss: 0.003393 | Recon Loss: 0.002914 | Commit Loss: 0.000958 | Perplexity: 2042.607491
2025-09-28 06:51:37,058 Stage: Train 0.5 | Epoch: 292 | Iter: 444400 | Total Loss: 0.003263 | Recon Loss: 0.002785 | Commit Loss: 0.000956 | Perplexity: 2043.383313
2025-09-28 06:52:03,727 Stage: Train 0.5 | Epoch: 292 | Iter: 444600 | Total Loss: 0.003237 | Recon Loss: 0.002758 | Commit Loss: 0.000958 | Perplexity: 2044.121610
2025-09-28 06:52:30,259 Stage: Train 0.5 | Epoch: 292 | Iter: 444800 | Total Loss: 0.003291 | Recon Loss: 0.002809 | Commit Loss: 0.000964 | Perplexity: 2042.858312
2025-09-28 06:52:56,836 Stage: Train 0.5 | Epoch: 292 | Iter: 445000 | Total Loss: 0.003282 | Recon Loss: 0.002800 | Commit Loss: 0.000964 | Perplexity: 2043.612664
Trainning Epoch:  89%|████████▉ | 293/330 [24:04:10<2:04:52, 202.51s/it]2025-09-28 06:53:23,638 Stage: Train 0.5 | Epoch: 293 | Iter: 445200 | Total Loss: 0.003275 | Recon Loss: 0.002794 | Commit Loss: 0.000962 | Perplexity: 2043.130179
2025-09-28 06:53:50,198 Stage: Train 0.5 | Epoch: 293 | Iter: 445400 | Total Loss: 0.003225 | Recon Loss: 0.002747 | Commit Loss: 0.000955 | Perplexity: 2041.293247
2025-09-28 06:54:16,791 Stage: Train 0.5 | Epoch: 293 | Iter: 445600 | Total Loss: 0.003345 | Recon Loss: 0.002861 | Commit Loss: 0.000967 | Perplexity: 2051.233613
2025-09-28 06:54:43,425 Stage: Train 0.5 | Epoch: 293 | Iter: 445800 | Total Loss: 0.003234 | Recon Loss: 0.002753 | Commit Loss: 0.000962 | Perplexity: 2049.317852
2025-09-28 06:55:10,016 Stage: Train 0.5 | Epoch: 293 | Iter: 446000 | Total Loss: 0.003324 | Recon Loss: 0.002844 | Commit Loss: 0.000960 | Perplexity: 2044.446586
2025-09-28 06:55:36,585 Stage: Train 0.5 | Epoch: 293 | Iter: 446200 | Total Loss: 0.003316 | Recon Loss: 0.002842 | Commit Loss: 0.000947 | Perplexity: 2041.075988
2025-09-28 06:56:03,266 Stage: Train 0.5 | Epoch: 293 | Iter: 446400 | Total Loss: 0.003370 | Recon Loss: 0.002887 | Commit Loss: 0.000966 | Perplexity: 2044.126594
Trainning Epoch:  89%|████████▉ | 294/330 [24:07:33<2:01:26, 202.41s/it]2025-09-28 06:56:30,051 Stage: Train 0.5 | Epoch: 294 | Iter: 446600 | Total Loss: 0.003245 | Recon Loss: 0.002767 | Commit Loss: 0.000956 | Perplexity: 2041.895092
2025-09-28 06:56:56,591 Stage: Train 0.5 | Epoch: 294 | Iter: 446800 | Total Loss: 0.003211 | Recon Loss: 0.002733 | Commit Loss: 0.000955 | Perplexity: 2041.283964
2025-09-28 06:57:23,304 Stage: Train 0.5 | Epoch: 294 | Iter: 447000 | Total Loss: 0.003218 | Recon Loss: 0.002740 | Commit Loss: 0.000955 | Perplexity: 2043.232786
2025-09-28 06:57:49,982 Stage: Train 0.5 | Epoch: 294 | Iter: 447200 | Total Loss: 0.003328 | Recon Loss: 0.002846 | Commit Loss: 0.000963 | Perplexity: 2047.013026
2025-09-28 06:58:16,640 Stage: Train 0.5 | Epoch: 294 | Iter: 447400 | Total Loss: 0.003246 | Recon Loss: 0.002765 | Commit Loss: 0.000962 | Perplexity: 2046.433793
2025-09-28 06:58:43,194 Stage: Train 0.5 | Epoch: 294 | Iter: 447600 | Total Loss: 0.003274 | Recon Loss: 0.002793 | Commit Loss: 0.000962 | Perplexity: 2041.961884
2025-09-28 06:59:09,769 Stage: Train 0.5 | Epoch: 294 | Iter: 447800 | Total Loss: 0.003354 | Recon Loss: 0.002874 | Commit Loss: 0.000961 | Perplexity: 2045.323364
2025-09-28 06:59:36,287 Stage: Train 0.5 | Epoch: 294 | Iter: 448000 | Total Loss: 0.003195 | Recon Loss: 0.002712 | Commit Loss: 0.000967 | Perplexity: 2050.340507
Trainning Epoch:  89%|████████▉ | 295/330 [24:10:55<1:58:01, 202.34s/it]2025-09-28 07:00:03,015 Stage: Train 0.5 | Epoch: 295 | Iter: 448200 | Total Loss: 0.003276 | Recon Loss: 0.002795 | Commit Loss: 0.000962 | Perplexity: 2047.739302
2025-09-28 07:00:29,607 Stage: Train 0.5 | Epoch: 295 | Iter: 448400 | Total Loss: 0.003226 | Recon Loss: 0.002751 | Commit Loss: 0.000951 | Perplexity: 2039.510110
2025-09-28 07:00:56,251 Stage: Train 0.5 | Epoch: 295 | Iter: 448600 | Total Loss: 0.003315 | Recon Loss: 0.002836 | Commit Loss: 0.000956 | Perplexity: 2043.906416
2025-09-28 07:01:22,702 Stage: Train 0.5 | Epoch: 295 | Iter: 448800 | Total Loss: 0.003244 | Recon Loss: 0.002764 | Commit Loss: 0.000960 | Perplexity: 2048.820796
2025-09-28 07:01:49,259 Stage: Train 0.5 | Epoch: 295 | Iter: 449000 | Total Loss: 0.003248 | Recon Loss: 0.002771 | Commit Loss: 0.000956 | Perplexity: 2046.538333
2025-09-28 07:02:15,732 Stage: Train 0.5 | Epoch: 295 | Iter: 449200 | Total Loss: 0.003231 | Recon Loss: 0.002751 | Commit Loss: 0.000959 | Perplexity: 2050.195464
2025-09-28 07:02:42,315 Stage: Train 0.5 | Epoch: 295 | Iter: 449400 | Total Loss: 0.003307 | Recon Loss: 0.002823 | Commit Loss: 0.000967 | Perplexity: 2048.836434
2025-09-28 07:03:08,947 Stage: Train 0.5 | Epoch: 295 | Iter: 449600 | Total Loss: 0.003298 | Recon Loss: 0.002819 | Commit Loss: 0.000958 | Perplexity: 2043.575940
Trainning Epoch:  90%|████████▉ | 296/330 [24:14:17<1:54:36, 202.24s/it]2025-09-28 07:03:35,802 Stage: Train 0.5 | Epoch: 296 | Iter: 449800 | Total Loss: 0.003168 | Recon Loss: 0.002688 | Commit Loss: 0.000960 | Perplexity: 2048.394725
2025-09-28 07:04:02,301 Stage: Train 0.5 | Epoch: 296 | Iter: 450000 | Total Loss: 0.003310 | Recon Loss: 0.002831 | Commit Loss: 0.000958 | Perplexity: 2042.939327
2025-09-28 07:04:28,783 Stage: Train 0.5 | Epoch: 296 | Iter: 450200 | Total Loss: 0.003266 | Recon Loss: 0.002788 | Commit Loss: 0.000955 | Perplexity: 2050.043334
2025-09-28 07:04:55,353 Stage: Train 0.5 | Epoch: 296 | Iter: 450400 | Total Loss: 0.003276 | Recon Loss: 0.002795 | Commit Loss: 0.000961 | Perplexity: 2047.226277
2025-09-28 07:05:21,937 Stage: Train 0.5 | Epoch: 296 | Iter: 450600 | Total Loss: 0.003431 | Recon Loss: 0.002954 | Commit Loss: 0.000954 | Perplexity: 2040.193292
2025-09-28 07:05:48,554 Stage: Train 0.5 | Epoch: 296 | Iter: 450800 | Total Loss: 0.003207 | Recon Loss: 0.002729 | Commit Loss: 0.000956 | Perplexity: 2045.248376
2025-09-28 07:06:15,270 Stage: Train 0.5 | Epoch: 296 | Iter: 451000 | Total Loss: 0.003354 | Recon Loss: 0.002871 | Commit Loss: 0.000965 | Perplexity: 2046.866181
Trainning Epoch:  90%|█████████ | 297/330 [24:17:39<1:51:12, 202.21s/it]2025-09-28 07:06:42,086 Stage: Train 0.5 | Epoch: 297 | Iter: 451200 | Total Loss: 0.003144 | Recon Loss: 0.002668 | Commit Loss: 0.000953 | Perplexity: 2039.468942
2025-09-28 07:07:08,631 Stage: Train 0.5 | Epoch: 297 | Iter: 451400 | Total Loss: 0.003252 | Recon Loss: 0.002777 | Commit Loss: 0.000951 | Perplexity: 2044.350251
2025-09-28 07:07:35,236 Stage: Train 0.5 | Epoch: 297 | Iter: 451600 | Total Loss: 0.003251 | Recon Loss: 0.002773 | Commit Loss: 0.000956 | Perplexity: 2044.344066
2025-09-28 07:08:01,802 Stage: Train 0.5 | Epoch: 297 | Iter: 451800 | Total Loss: 0.003274 | Recon Loss: 0.002795 | Commit Loss: 0.000959 | Perplexity: 2044.014485
2025-09-28 07:08:28,372 Stage: Train 0.5 | Epoch: 297 | Iter: 452000 | Total Loss: 0.003199 | Recon Loss: 0.002721 | Commit Loss: 0.000956 | Perplexity: 2046.389465
2025-09-28 07:08:54,878 Stage: Train 0.5 | Epoch: 297 | Iter: 452200 | Total Loss: 0.003250 | Recon Loss: 0.002771 | Commit Loss: 0.000959 | Perplexity: 2045.687559
2025-09-28 07:09:21,442 Stage: Train 0.5 | Epoch: 297 | Iter: 452400 | Total Loss: 0.003261 | Recon Loss: 0.002783 | Commit Loss: 0.000956 | Perplexity: 2045.693463
2025-09-28 07:09:47,956 Stage: Train 0.5 | Epoch: 297 | Iter: 452600 | Total Loss: 0.003322 | Recon Loss: 0.002841 | Commit Loss: 0.000961 | Perplexity: 2052.083915
Trainning Epoch:  90%|█████████ | 298/330 [24:21:01<1:47:47, 202.12s/it]2025-09-28 07:10:14,821 Stage: Train 0.5 | Epoch: 298 | Iter: 452800 | Total Loss: 0.003172 | Recon Loss: 0.002696 | Commit Loss: 0.000952 | Perplexity: 2042.310476
2025-09-28 07:10:41,389 Stage: Train 0.5 | Epoch: 298 | Iter: 453000 | Total Loss: 0.003251 | Recon Loss: 0.002771 | Commit Loss: 0.000961 | Perplexity: 2047.730613
2025-09-28 07:11:08,057 Stage: Train 0.5 | Epoch: 298 | Iter: 453200 | Total Loss: 0.003245 | Recon Loss: 0.002769 | Commit Loss: 0.000953 | Perplexity: 2045.402396
2025-09-28 07:11:34,543 Stage: Train 0.5 | Epoch: 298 | Iter: 453400 | Total Loss: 0.003391 | Recon Loss: 0.002914 | Commit Loss: 0.000954 | Perplexity: 2036.358452
2025-09-28 07:12:01,110 Stage: Train 0.5 | Epoch: 298 | Iter: 453600 | Total Loss: 0.003541 | Recon Loss: 0.003063 | Commit Loss: 0.000958 | Perplexity: 2044.289464
2025-09-28 07:12:27,742 Stage: Train 0.5 | Epoch: 298 | Iter: 453800 | Total Loss: 0.003178 | Recon Loss: 0.002700 | Commit Loss: 0.000955 | Perplexity: 2043.488776
2025-09-28 07:12:54,186 Stage: Train 0.5 | Epoch: 298 | Iter: 454000 | Total Loss: 0.003261 | Recon Loss: 0.002781 | Commit Loss: 0.000959 | Perplexity: 2052.069651
Trainning Epoch:  91%|█████████ | 299/330 [24:24:23<1:44:24, 202.09s/it]2025-09-28 07:13:20,983 Stage: Train 0.5 | Epoch: 299 | Iter: 454200 | Total Loss: 0.003188 | Recon Loss: 0.002707 | Commit Loss: 0.000961 | Perplexity: 2050.139498
2025-09-28 07:13:47,601 Stage: Train 0.5 | Epoch: 299 | Iter: 454400 | Total Loss: 0.003253 | Recon Loss: 0.002776 | Commit Loss: 0.000955 | Perplexity: 2044.615748
2025-09-28 07:14:14,209 Stage: Train 0.5 | Epoch: 299 | Iter: 454600 | Total Loss: 0.003244 | Recon Loss: 0.002769 | Commit Loss: 0.000951 | Perplexity: 2044.639910
2025-09-28 07:14:40,707 Stage: Train 0.5 | Epoch: 299 | Iter: 454800 | Total Loss: 0.003290 | Recon Loss: 0.002814 | Commit Loss: 0.000952 | Perplexity: 2043.484030
2025-09-28 07:15:07,399 Stage: Train 0.5 | Epoch: 299 | Iter: 455000 | Total Loss: 0.003239 | Recon Loss: 0.002762 | Commit Loss: 0.000954 | Perplexity: 2042.578655
2025-09-28 07:15:34,054 Stage: Train 0.5 | Epoch: 299 | Iter: 455200 | Total Loss: 0.003291 | Recon Loss: 0.002813 | Commit Loss: 0.000956 | Perplexity: 2052.123195
2025-09-28 07:16:00,678 Stage: Train 0.5 | Epoch: 299 | Iter: 455400 | Total Loss: 0.003255 | Recon Loss: 0.002776 | Commit Loss: 0.000958 | Perplexity: 2046.979270
2025-09-28 07:16:27,284 Stage: Train 0.5 | Epoch: 299 | Iter: 455600 | Total Loss: 0.003327 | Recon Loss: 0.002849 | Commit Loss: 0.000956 | Perplexity: 2045.517201
Trainning Epoch:  91%|█████████ | 300/330 [24:27:45<1:41:05, 202.18s/it]2025-09-28 07:16:54,051 Stage: Train 0.5 | Epoch: 300 | Iter: 455800 | Total Loss: 0.003210 | Recon Loss: 0.002736 | Commit Loss: 0.000948 | Perplexity: 2049.269806
2025-09-28 07:17:20,708 Stage: Train 0.5 | Epoch: 300 | Iter: 456000 | Total Loss: 0.003299 | Recon Loss: 0.002821 | Commit Loss: 0.000957 | Perplexity: 2047.169191
2025-09-28 07:17:47,311 Stage: Train 0.5 | Epoch: 300 | Iter: 456200 | Total Loss: 0.003300 | Recon Loss: 0.002824 | Commit Loss: 0.000950 | Perplexity: 2044.456387
2025-09-28 07:18:13,859 Stage: Train 0.5 | Epoch: 300 | Iter: 456400 | Total Loss: 0.003266 | Recon Loss: 0.002789 | Commit Loss: 0.000954 | Perplexity: 2041.892168
2025-09-28 07:18:40,311 Stage: Train 0.5 | Epoch: 300 | Iter: 456600 | Total Loss: 0.003226 | Recon Loss: 0.002749 | Commit Loss: 0.000954 | Perplexity: 2045.482327
2025-09-28 07:19:06,873 Stage: Train 0.5 | Epoch: 300 | Iter: 456800 | Total Loss: 0.003281 | Recon Loss: 0.002805 | Commit Loss: 0.000952 | Perplexity: 2046.189733
2025-09-28 07:19:33,409 Stage: Train 0.5 | Epoch: 300 | Iter: 457000 | Total Loss: 0.003234 | Recon Loss: 0.002757 | Commit Loss: 0.000954 | Perplexity: 2048.378215
2025-09-28 07:20:00,060 Stage: Train 0.5 | Epoch: 300 | Iter: 457200 | Total Loss: 0.003279 | Recon Loss: 0.002804 | Commit Loss: 0.000949 | Perplexity: 2042.340143
Trainning Epoch:  91%|█████████ | 301/330 [24:31:07<1:37:41, 202.12s/it]2025-09-28 07:20:26,905 Stage: Train 0.5 | Epoch: 301 | Iter: 457400 | Total Loss: 0.003245 | Recon Loss: 0.002769 | Commit Loss: 0.000952 | Perplexity: 2045.692209
2025-09-28 07:20:53,405 Stage: Train 0.5 | Epoch: 301 | Iter: 457600 | Total Loss: 0.003209 | Recon Loss: 0.002735 | Commit Loss: 0.000949 | Perplexity: 2046.106669
2025-09-28 07:21:19,888 Stage: Train 0.5 | Epoch: 301 | Iter: 457800 | Total Loss: 0.003272 | Recon Loss: 0.002793 | Commit Loss: 0.000958 | Perplexity: 2047.822511
2025-09-28 07:21:46,317 Stage: Train 0.5 | Epoch: 301 | Iter: 458000 | Total Loss: 0.003239 | Recon Loss: 0.002764 | Commit Loss: 0.000951 | Perplexity: 2041.887621
2025-09-28 07:22:12,920 Stage: Train 0.5 | Epoch: 301 | Iter: 458200 | Total Loss: 0.003194 | Recon Loss: 0.002717 | Commit Loss: 0.000953 | Perplexity: 2041.958442
2025-09-28 07:22:39,554 Stage: Train 0.5 | Epoch: 301 | Iter: 458400 | Total Loss: 0.003382 | Recon Loss: 0.002905 | Commit Loss: 0.000953 | Perplexity: 2044.723869
2025-09-28 07:23:06,138 Stage: Train 0.5 | Epoch: 301 | Iter: 458600 | Total Loss: 0.003214 | Recon Loss: 0.002737 | Commit Loss: 0.000954 | Perplexity: 2045.994356
Trainning Epoch:  92%|█████████▏| 302/330 [24:34:29<1:34:17, 202.06s/it]2025-09-28 07:23:32,992 Stage: Train 0.5 | Epoch: 302 | Iter: 458800 | Total Loss: 0.003307 | Recon Loss: 0.002829 | Commit Loss: 0.000955 | Perplexity: 2045.268937
2025-09-28 07:23:59,614 Stage: Train 0.5 | Epoch: 302 | Iter: 459000 | Total Loss: 0.003162 | Recon Loss: 0.002688 | Commit Loss: 0.000948 | Perplexity: 2043.610137
2025-09-28 07:24:26,202 Stage: Train 0.5 | Epoch: 302 | Iter: 459200 | Total Loss: 0.003281 | Recon Loss: 0.002802 | Commit Loss: 0.000958 | Perplexity: 2049.247783
2025-09-28 07:24:52,699 Stage: Train 0.5 | Epoch: 302 | Iter: 459400 | Total Loss: 0.003212 | Recon Loss: 0.002737 | Commit Loss: 0.000950 | Perplexity: 2044.205526
2025-09-28 07:25:19,314 Stage: Train 0.5 | Epoch: 302 | Iter: 459600 | Total Loss: 0.003238 | Recon Loss: 0.002765 | Commit Loss: 0.000946 | Perplexity: 2043.053348
2025-09-28 07:25:45,956 Stage: Train 0.5 | Epoch: 302 | Iter: 459800 | Total Loss: 0.003239 | Recon Loss: 0.002762 | Commit Loss: 0.000954 | Perplexity: 2048.841426
2025-09-28 07:26:12,652 Stage: Train 0.5 | Epoch: 302 | Iter: 460000 | Total Loss: 0.003257 | Recon Loss: 0.002779 | Commit Loss: 0.000956 | Perplexity: 2048.117733
2025-09-28 07:26:12,652 Saving model at iteration 460000
2025-09-28 07:26:12,849 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000
2025-09-28 07:26:13,354 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/model.safetensors
2025-09-28 07:26:13,886 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/optimizer.bin
2025-09-28 07:26:13,886 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/scheduler.bin
2025-09-28 07:26:13,886 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/sampler.bin
2025-09-28 07:26:13,887 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/random_states_0.pkl
2025-09-28 07:26:40,707 Stage: Train 0.5 | Epoch: 302 | Iter: 460200 | Total Loss: 0.003171 | Recon Loss: 0.002690 | Commit Loss: 0.000961 | Perplexity: 2053.315605
Trainning Epoch:  92%|█████████▏| 303/330 [24:37:53<1:31:09, 202.56s/it]2025-09-28 07:27:07,521 Stage: Train 0.5 | Epoch: 303 | Iter: 460400 | Total Loss: 0.003313 | Recon Loss: 0.002834 | Commit Loss: 0.000957 | Perplexity: 2045.538129
2025-09-28 07:27:34,164 Stage: Train 0.5 | Epoch: 303 | Iter: 460600 | Total Loss: 0.003199 | Recon Loss: 0.002722 | Commit Loss: 0.000956 | Perplexity: 2051.284121
2025-09-28 07:28:00,768 Stage: Train 0.5 | Epoch: 303 | Iter: 460800 | Total Loss: 0.003299 | Recon Loss: 0.002821 | Commit Loss: 0.000955 | Perplexity: 2047.446791
2025-09-28 07:28:27,267 Stage: Train 0.5 | Epoch: 303 | Iter: 461000 | Total Loss: 0.003222 | Recon Loss: 0.002748 | Commit Loss: 0.000948 | Perplexity: 2050.023148
2025-09-28 07:28:53,912 Stage: Train 0.5 | Epoch: 303 | Iter: 461200 | Total Loss: 0.003564 | Recon Loss: 0.003088 | Commit Loss: 0.000951 | Perplexity: 2036.415285
2025-09-28 07:29:20,436 Stage: Train 0.5 | Epoch: 303 | Iter: 461400 | Total Loss: 0.003191 | Recon Loss: 0.002716 | Commit Loss: 0.000951 | Perplexity: 2041.687138
2025-09-28 07:29:46,969 Stage: Train 0.5 | Epoch: 303 | Iter: 461600 | Total Loss: 0.003122 | Recon Loss: 0.002645 | Commit Loss: 0.000954 | Perplexity: 2043.128877
Trainning Epoch:  92%|█████████▏| 304/330 [24:41:15<1:27:43, 202.42s/it]2025-09-28 07:30:13,745 Stage: Train 0.5 | Epoch: 304 | Iter: 461800 | Total Loss: 0.003272 | Recon Loss: 0.002796 | Commit Loss: 0.000952 | Perplexity: 2045.217681
2025-09-28 07:30:40,379 Stage: Train 0.5 | Epoch: 304 | Iter: 462000 | Total Loss: 0.003266 | Recon Loss: 0.002792 | Commit Loss: 0.000948 | Perplexity: 2043.580450
2025-09-28 07:31:07,031 Stage: Train 0.5 | Epoch: 304 | Iter: 462200 | Total Loss: 0.003205 | Recon Loss: 0.002730 | Commit Loss: 0.000950 | Perplexity: 2044.273384
2025-09-28 07:31:33,586 Stage: Train 0.5 | Epoch: 304 | Iter: 462400 | Total Loss: 0.003256 | Recon Loss: 0.002781 | Commit Loss: 0.000951 | Perplexity: 2050.010076
2025-09-28 07:32:00,167 Stage: Train 0.5 | Epoch: 304 | Iter: 462600 | Total Loss: 0.003317 | Recon Loss: 0.002845 | Commit Loss: 0.000945 | Perplexity: 2042.938130
2025-09-28 07:32:26,820 Stage: Train 0.5 | Epoch: 304 | Iter: 462800 | Total Loss: 0.003242 | Recon Loss: 0.002769 | Commit Loss: 0.000947 | Perplexity: 2043.329747
2025-09-28 07:32:53,444 Stage: Train 0.5 | Epoch: 304 | Iter: 463000 | Total Loss: 0.003261 | Recon Loss: 0.002785 | Commit Loss: 0.000952 | Perplexity: 2047.327198
2025-09-28 07:33:20,064 Stage: Train 0.5 | Epoch: 304 | Iter: 463200 | Total Loss: 0.003169 | Recon Loss: 0.002693 | Commit Loss: 0.000952 | Perplexity: 2044.969684
Trainning Epoch:  92%|█████████▏| 305/330 [24:44:37<1:24:19, 202.40s/it]2025-09-28 07:33:46,885 Stage: Train 0.5 | Epoch: 305 | Iter: 463400 | Total Loss: 0.003196 | Recon Loss: 0.002721 | Commit Loss: 0.000950 | Perplexity: 2045.317783
2025-09-28 07:34:13,507 Stage: Train 0.5 | Epoch: 305 | Iter: 463600 | Total Loss: 0.003241 | Recon Loss: 0.002767 | Commit Loss: 0.000947 | Perplexity: 2048.852620
2025-09-28 07:34:40,055 Stage: Train 0.5 | Epoch: 305 | Iter: 463800 | Total Loss: 0.003375 | Recon Loss: 0.002899 | Commit Loss: 0.000953 | Perplexity: 2046.965708
2025-09-28 07:35:06,739 Stage: Train 0.5 | Epoch: 305 | Iter: 464000 | Total Loss: 0.003131 | Recon Loss: 0.002655 | Commit Loss: 0.000953 | Perplexity: 2045.069532
2025-09-28 07:35:33,244 Stage: Train 0.5 | Epoch: 305 | Iter: 464200 | Total Loss: 0.003216 | Recon Loss: 0.002742 | Commit Loss: 0.000949 | Perplexity: 2044.847844
2025-09-28 07:35:59,664 Stage: Train 0.5 | Epoch: 305 | Iter: 464400 | Total Loss: 0.003237 | Recon Loss: 0.002763 | Commit Loss: 0.000949 | Perplexity: 2048.766426
2025-09-28 07:36:26,189 Stage: Train 0.5 | Epoch: 305 | Iter: 464600 | Total Loss: 0.003308 | Recon Loss: 0.002836 | Commit Loss: 0.000945 | Perplexity: 2045.238207
2025-09-28 07:36:52,863 Stage: Train 0.5 | Epoch: 305 | Iter: 464800 | Total Loss: 0.003153 | Recon Loss: 0.002678 | Commit Loss: 0.000950 | Perplexity: 2047.714471
Trainning Epoch:  93%|█████████▎| 306/330 [24:47:59<1:20:54, 202.28s/it]2025-09-28 07:37:19,571 Stage: Train 0.5 | Epoch: 306 | Iter: 465000 | Total Loss: 0.003181 | Recon Loss: 0.002709 | Commit Loss: 0.000944 | Perplexity: 2047.306759
2025-09-28 07:37:46,108 Stage: Train 0.5 | Epoch: 306 | Iter: 465200 | Total Loss: 0.003220 | Recon Loss: 0.002744 | Commit Loss: 0.000951 | Perplexity: 2052.853556
2025-09-28 07:38:12,746 Stage: Train 0.5 | Epoch: 306 | Iter: 465400 | Total Loss: 0.003262 | Recon Loss: 0.002788 | Commit Loss: 0.000948 | Perplexity: 2046.468076
2025-09-28 07:38:39,265 Stage: Train 0.5 | Epoch: 306 | Iter: 465600 | Total Loss: 0.003169 | Recon Loss: 0.002694 | Commit Loss: 0.000950 | Perplexity: 2049.481798
2025-09-28 07:39:05,824 Stage: Train 0.5 | Epoch: 306 | Iter: 465800 | Total Loss: 0.003263 | Recon Loss: 0.002790 | Commit Loss: 0.000948 | Perplexity: 2044.431292
2025-09-28 07:39:32,497 Stage: Train 0.5 | Epoch: 306 | Iter: 466000 | Total Loss: 0.003184 | Recon Loss: 0.002709 | Commit Loss: 0.000951 | Perplexity: 2049.350317
2025-09-28 07:39:58,927 Stage: Train 0.5 | Epoch: 306 | Iter: 466200 | Total Loss: 0.003364 | Recon Loss: 0.002890 | Commit Loss: 0.000948 | Perplexity: 2046.231301
Trainning Epoch:  93%|█████████▎| 307/330 [24:51:21<1:17:30, 202.18s/it]2025-09-28 07:40:25,756 Stage: Train 0.5 | Epoch: 307 | Iter: 466400 | Total Loss: 0.003210 | Recon Loss: 0.002734 | Commit Loss: 0.000953 | Perplexity: 2042.214605
2025-09-28 07:40:52,260 Stage: Train 0.5 | Epoch: 307 | Iter: 466600 | Total Loss: 0.003183 | Recon Loss: 0.002713 | Commit Loss: 0.000939 | Perplexity: 2045.318957
2025-09-28 07:41:18,916 Stage: Train 0.5 | Epoch: 307 | Iter: 466800 | Total Loss: 0.003307 | Recon Loss: 0.002831 | Commit Loss: 0.000952 | Perplexity: 2046.536802
2025-09-28 07:41:45,411 Stage: Train 0.5 | Epoch: 307 | Iter: 467000 | Total Loss: 0.003214 | Recon Loss: 0.002742 | Commit Loss: 0.000944 | Perplexity: 2045.904229
2025-09-28 07:42:11,984 Stage: Train 0.5 | Epoch: 307 | Iter: 467200 | Total Loss: 0.003228 | Recon Loss: 0.002753 | Commit Loss: 0.000949 | Perplexity: 2050.606237
2025-09-28 07:42:38,553 Stage: Train 0.5 | Epoch: 307 | Iter: 467400 | Total Loss: 0.003270 | Recon Loss: 0.002796 | Commit Loss: 0.000948 | Perplexity: 2045.528693
2025-09-28 07:43:05,160 Stage: Train 0.5 | Epoch: 307 | Iter: 467600 | Total Loss: 0.003266 | Recon Loss: 0.002795 | Commit Loss: 0.000944 | Perplexity: 2044.517302
2025-09-28 07:43:31,831 Stage: Train 0.5 | Epoch: 307 | Iter: 467800 | Total Loss: 0.003138 | Recon Loss: 0.002663 | Commit Loss: 0.000951 | Perplexity: 2047.824814
Trainning Epoch:  93%|█████████▎| 308/330 [24:54:43<1:14:07, 202.16s/it]2025-09-28 07:43:58,541 Stage: Train 0.5 | Epoch: 308 | Iter: 468000 | Total Loss: 0.003297 | Recon Loss: 0.002821 | Commit Loss: 0.000953 | Perplexity: 2041.693096
2025-09-28 07:44:25,214 Stage: Train 0.5 | Epoch: 308 | Iter: 468200 | Total Loss: 0.003203 | Recon Loss: 0.002729 | Commit Loss: 0.000948 | Perplexity: 2056.930693
2025-09-28 07:44:51,801 Stage: Train 0.5 | Epoch: 308 | Iter: 468400 | Total Loss: 0.003214 | Recon Loss: 0.002742 | Commit Loss: 0.000944 | Perplexity: 2045.612038
2025-09-28 07:45:18,385 Stage: Train 0.5 | Epoch: 308 | Iter: 468600 | Total Loss: 0.003227 | Recon Loss: 0.002754 | Commit Loss: 0.000947 | Perplexity: 2046.641497
2025-09-28 07:45:44,943 Stage: Train 0.5 | Epoch: 308 | Iter: 468800 | Total Loss: 0.003262 | Recon Loss: 0.002790 | Commit Loss: 0.000944 | Perplexity: 2045.442340
2025-09-28 07:46:11,511 Stage: Train 0.5 | Epoch: 308 | Iter: 469000 | Total Loss: 0.003189 | Recon Loss: 0.002716 | Commit Loss: 0.000946 | Perplexity: 2050.128904
2025-09-28 07:46:38,109 Stage: Train 0.5 | Epoch: 308 | Iter: 469200 | Total Loss: 0.003200 | Recon Loss: 0.002728 | Commit Loss: 0.000944 | Perplexity: 2044.155664
Trainning Epoch:  94%|█████████▎| 309/330 [24:58:05<1:10:45, 202.15s/it]2025-09-28 07:47:04,924 Stage: Train 0.5 | Epoch: 309 | Iter: 469400 | Total Loss: 0.003328 | Recon Loss: 0.002855 | Commit Loss: 0.000945 | Perplexity: 2046.611470
2025-09-28 07:47:31,476 Stage: Train 0.5 | Epoch: 309 | Iter: 469600 | Total Loss: 0.003134 | Recon Loss: 0.002661 | Commit Loss: 0.000945 | Perplexity: 2045.546096
2025-09-28 07:47:58,010 Stage: Train 0.5 | Epoch: 309 | Iter: 469800 | Total Loss: 0.003231 | Recon Loss: 0.002755 | Commit Loss: 0.000952 | Perplexity: 2057.411663
2025-09-28 07:48:24,588 Stage: Train 0.5 | Epoch: 309 | Iter: 470000 | Total Loss: 0.003289 | Recon Loss: 0.002815 | Commit Loss: 0.000949 | Perplexity: 2050.559445
2025-09-28 07:48:51,150 Stage: Train 0.5 | Epoch: 309 | Iter: 470200 | Total Loss: 0.003202 | Recon Loss: 0.002731 | Commit Loss: 0.000941 | Perplexity: 2043.187222
2025-09-28 07:49:17,808 Stage: Train 0.5 | Epoch: 309 | Iter: 470400 | Total Loss: 0.003194 | Recon Loss: 0.002723 | Commit Loss: 0.000942 | Perplexity: 2045.500026
2025-09-28 07:49:44,434 Stage: Train 0.5 | Epoch: 309 | Iter: 470600 | Total Loss: 0.003205 | Recon Loss: 0.002737 | Commit Loss: 0.000936 | Perplexity: 2040.989254
2025-09-28 07:50:11,027 Stage: Train 0.5 | Epoch: 309 | Iter: 470800 | Total Loss: 0.003205 | Recon Loss: 0.002729 | Commit Loss: 0.000952 | Perplexity: 2045.420432
Trainning Epoch:  94%|█████████▍| 310/330 [25:01:27<1:07:21, 202.08s/it]2025-09-28 07:50:37,642 Stage: Train 0.5 | Epoch: 310 | Iter: 471000 | Total Loss: 0.003266 | Recon Loss: 0.002796 | Commit Loss: 0.000940 | Perplexity: 2048.108333
2025-09-28 07:51:04,169 Stage: Train 0.5 | Epoch: 310 | Iter: 471200 | Total Loss: 0.003165 | Recon Loss: 0.002692 | Commit Loss: 0.000946 | Perplexity: 2048.841118
2025-09-28 07:51:30,813 Stage: Train 0.5 | Epoch: 310 | Iter: 471400 | Total Loss: 0.003272 | Recon Loss: 0.002796 | Commit Loss: 0.000953 | Perplexity: 2050.177034
2025-09-28 07:51:57,609 Stage: Train 0.5 | Epoch: 310 | Iter: 471600 | Total Loss: 0.003239 | Recon Loss: 0.002767 | Commit Loss: 0.000945 | Perplexity: 2051.647344
2025-09-28 07:52:24,224 Stage: Train 0.5 | Epoch: 310 | Iter: 471800 | Total Loss: 0.003183 | Recon Loss: 0.002712 | Commit Loss: 0.000943 | Perplexity: 2046.214183
2025-09-28 07:52:50,823 Stage: Train 0.5 | Epoch: 310 | Iter: 472000 | Total Loss: 0.003243 | Recon Loss: 0.002769 | Commit Loss: 0.000948 | Perplexity: 2049.587618
2025-09-28 07:53:17,479 Stage: Train 0.5 | Epoch: 310 | Iter: 472200 | Total Loss: 0.003170 | Recon Loss: 0.002697 | Commit Loss: 0.000946 | Perplexity: 2047.674510
2025-09-28 07:53:44,059 Stage: Train 0.5 | Epoch: 310 | Iter: 472400 | Total Loss: 0.003283 | Recon Loss: 0.002812 | Commit Loss: 0.000943 | Perplexity: 2049.462017
Trainning Epoch:  94%|█████████▍| 311/330 [25:04:50<1:04:01, 202.19s/it]2025-09-28 07:54:10,940 Stage: Train 0.5 | Epoch: 311 | Iter: 472600 | Total Loss: 0.003146 | Recon Loss: 0.002678 | Commit Loss: 0.000937 | Perplexity: 2039.366288
2025-09-28 07:54:37,530 Stage: Train 0.5 | Epoch: 311 | Iter: 472800 | Total Loss: 0.003245 | Recon Loss: 0.002776 | Commit Loss: 0.000939 | Perplexity: 2048.918755
2025-09-28 07:55:04,051 Stage: Train 0.5 | Epoch: 311 | Iter: 473000 | Total Loss: 0.003183 | Recon Loss: 0.002709 | Commit Loss: 0.000949 | Perplexity: 2048.168710
2025-09-28 07:55:30,587 Stage: Train 0.5 | Epoch: 311 | Iter: 473200 | Total Loss: 0.003193 | Recon Loss: 0.002724 | Commit Loss: 0.000938 | Perplexity: 2043.427772
2025-09-28 07:55:57,171 Stage: Train 0.5 | Epoch: 311 | Iter: 473400 | Total Loss: 0.003335 | Recon Loss: 0.002862 | Commit Loss: 0.000945 | Perplexity: 2051.569301
2025-09-28 07:56:23,682 Stage: Train 0.5 | Epoch: 311 | Iter: 473600 | Total Loss: 0.003145 | Recon Loss: 0.002670 | Commit Loss: 0.000950 | Perplexity: 2053.653822
2025-09-28 07:56:50,209 Stage: Train 0.5 | Epoch: 311 | Iter: 473800 | Total Loss: 0.003820 | Recon Loss: 0.003341 | Commit Loss: 0.000958 | Perplexity: 2043.802119
Trainning Epoch:  95%|█████████▍| 312/330 [25:08:12<1:00:38, 202.11s/it]2025-09-28 07:57:16,976 Stage: Train 0.5 | Epoch: 312 | Iter: 474000 | Total Loss: 0.003150 | Recon Loss: 0.002680 | Commit Loss: 0.000941 | Perplexity: 2045.881710
2025-09-28 07:57:43,589 Stage: Train 0.5 | Epoch: 312 | Iter: 474200 | Total Loss: 0.003257 | Recon Loss: 0.002787 | Commit Loss: 0.000941 | Perplexity: 2045.984631
2025-09-28 07:58:10,136 Stage: Train 0.5 | Epoch: 312 | Iter: 474400 | Total Loss: 0.003166 | Recon Loss: 0.002692 | Commit Loss: 0.000949 | Perplexity: 2048.377017
2025-09-28 07:58:36,652 Stage: Train 0.5 | Epoch: 312 | Iter: 474600 | Total Loss: 0.003141 | Recon Loss: 0.002671 | Commit Loss: 0.000940 | Perplexity: 2050.415737
2025-09-28 07:59:03,227 Stage: Train 0.5 | Epoch: 312 | Iter: 474800 | Total Loss: 0.003214 | Recon Loss: 0.002738 | Commit Loss: 0.000951 | Perplexity: 2049.984338
2025-09-28 07:59:29,765 Stage: Train 0.5 | Epoch: 312 | Iter: 475000 | Total Loss: 0.003203 | Recon Loss: 0.002734 | Commit Loss: 0.000939 | Perplexity: 2048.389169
2025-09-28 07:59:56,433 Stage: Train 0.5 | Epoch: 312 | Iter: 475200 | Total Loss: 0.003169 | Recon Loss: 0.002698 | Commit Loss: 0.000940 | Perplexity: 2045.054478
2025-09-28 08:00:23,043 Stage: Train 0.5 | Epoch: 312 | Iter: 475400 | Total Loss: 0.003244 | Recon Loss: 0.002772 | Commit Loss: 0.000943 | Perplexity: 2047.581967
Trainning Epoch:  95%|█████████▍| 313/330 [25:11:34<57:16, 202.12s/it]  2025-09-28 08:00:49,795 Stage: Train 0.5 | Epoch: 313 | Iter: 475600 | Total Loss: 0.003168 | Recon Loss: 0.002698 | Commit Loss: 0.000940 | Perplexity: 2046.830068
2025-09-28 08:01:16,383 Stage: Train 0.5 | Epoch: 313 | Iter: 475800 | Total Loss: 0.003208 | Recon Loss: 0.002737 | Commit Loss: 0.000942 | Perplexity: 2050.080151
2025-09-28 08:01:42,991 Stage: Train 0.5 | Epoch: 313 | Iter: 476000 | Total Loss: 0.003186 | Recon Loss: 0.002718 | Commit Loss: 0.000937 | Perplexity: 2050.704030
2025-09-28 08:02:09,629 Stage: Train 0.5 | Epoch: 313 | Iter: 476200 | Total Loss: 0.003236 | Recon Loss: 0.002764 | Commit Loss: 0.000943 | Perplexity: 2043.467422
2025-09-28 08:02:36,128 Stage: Train 0.5 | Epoch: 313 | Iter: 476400 | Total Loss: 0.003247 | Recon Loss: 0.002775 | Commit Loss: 0.000943 | Perplexity: 2053.647351
2025-09-28 08:03:02,677 Stage: Train 0.5 | Epoch: 313 | Iter: 476600 | Total Loss: 0.003349 | Recon Loss: 0.002877 | Commit Loss: 0.000944 | Perplexity: 2050.165178
2025-09-28 08:03:29,203 Stage: Train 0.5 | Epoch: 313 | Iter: 476800 | Total Loss: 0.003112 | Recon Loss: 0.002641 | Commit Loss: 0.000941 | Perplexity: 2047.538901
Trainning Epoch:  95%|█████████▌| 314/330 [25:14:56<53:53, 202.10s/it]2025-09-28 08:03:56,098 Stage: Train 0.5 | Epoch: 314 | Iter: 477000 | Total Loss: 0.003213 | Recon Loss: 0.002739 | Commit Loss: 0.000948 | Perplexity: 2046.406253
2025-09-28 08:04:22,783 Stage: Train 0.5 | Epoch: 314 | Iter: 477200 | Total Loss: 0.003160 | Recon Loss: 0.002690 | Commit Loss: 0.000940 | Perplexity: 2046.610762
2025-09-28 08:04:49,310 Stage: Train 0.5 | Epoch: 314 | Iter: 477400 | Total Loss: 0.003246 | Recon Loss: 0.002775 | Commit Loss: 0.000943 | Perplexity: 2047.924902
2025-09-28 08:05:15,853 Stage: Train 0.5 | Epoch: 314 | Iter: 477600 | Total Loss: 0.003215 | Recon Loss: 0.002744 | Commit Loss: 0.000941 | Perplexity: 2049.207726
2025-09-28 08:05:42,517 Stage: Train 0.5 | Epoch: 314 | Iter: 477800 | Total Loss: 0.003228 | Recon Loss: 0.002754 | Commit Loss: 0.000948 | Perplexity: 2046.448455
2025-09-28 08:06:09,092 Stage: Train 0.5 | Epoch: 314 | Iter: 478000 | Total Loss: 0.003206 | Recon Loss: 0.002736 | Commit Loss: 0.000940 | Perplexity: 2052.561528
2025-09-28 08:06:35,756 Stage: Train 0.5 | Epoch: 314 | Iter: 478200 | Total Loss: 0.003208 | Recon Loss: 0.002737 | Commit Loss: 0.000942 | Perplexity: 2046.022496
2025-09-28 08:07:02,418 Stage: Train 0.5 | Epoch: 314 | Iter: 478400 | Total Loss: 0.003160 | Recon Loss: 0.002690 | Commit Loss: 0.000941 | Perplexity: 2048.805175
Trainning Epoch:  95%|█████████▌| 315/330 [25:18:18<50:33, 202.20s/it]2025-09-28 08:07:29,238 Stage: Train 0.5 | Epoch: 315 | Iter: 478600 | Total Loss: 0.003204 | Recon Loss: 0.002731 | Commit Loss: 0.000945 | Perplexity: 2050.032512
2025-09-28 08:07:55,721 Stage: Train 0.5 | Epoch: 315 | Iter: 478800 | Total Loss: 0.003300 | Recon Loss: 0.002832 | Commit Loss: 0.000936 | Perplexity: 2046.251516
2025-09-28 08:08:22,274 Stage: Train 0.5 | Epoch: 315 | Iter: 479000 | Total Loss: 0.003222 | Recon Loss: 0.002753 | Commit Loss: 0.000938 | Perplexity: 2051.367961
2025-09-28 08:08:48,895 Stage: Train 0.5 | Epoch: 315 | Iter: 479200 | Total Loss: 0.003182 | Recon Loss: 0.002708 | Commit Loss: 0.000949 | Perplexity: 2054.130353
2025-09-28 08:09:15,501 Stage: Train 0.5 | Epoch: 315 | Iter: 479400 | Total Loss: 0.003221 | Recon Loss: 0.002749 | Commit Loss: 0.000945 | Perplexity: 2049.337252
2025-09-28 08:09:42,117 Stage: Train 0.5 | Epoch: 315 | Iter: 479600 | Total Loss: 0.003260 | Recon Loss: 0.002791 | Commit Loss: 0.000938 | Perplexity: 2049.936168
2025-09-28 08:10:08,702 Stage: Train 0.5 | Epoch: 315 | Iter: 479800 | Total Loss: 0.003193 | Recon Loss: 0.002725 | Commit Loss: 0.000937 | Perplexity: 2047.392558
2025-09-28 08:10:35,247 Stage: Train 0.5 | Epoch: 315 | Iter: 480000 | Total Loss: 0.003291 | Recon Loss: 0.002819 | Commit Loss: 0.000942 | Perplexity: 2044.251652
2025-09-28 08:10:35,248 Saving model at iteration 480000
2025-09-28 08:10:35,436 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000
2025-09-28 08:10:35,925 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/model.safetensors
2025-09-28 08:10:36,494 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/optimizer.bin
2025-09-28 08:10:36,495 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/scheduler.bin
2025-09-28 08:10:36,495 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/sampler.bin
2025-09-28 08:10:36,496 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 316/330 [25:21:42<47:15, 202.51s/it]2025-09-28 08:11:03,295 Stage: Train 0.5 | Epoch: 316 | Iter: 480200 | Total Loss: 0.003124 | Recon Loss: 0.002655 | Commit Loss: 0.000937 | Perplexity: 2052.036868
2025-09-28 08:11:29,900 Stage: Train 0.5 | Epoch: 316 | Iter: 480400 | Total Loss: 0.003203 | Recon Loss: 0.002735 | Commit Loss: 0.000935 | Perplexity: 2053.748356
2025-09-28 08:11:56,515 Stage: Train 0.5 | Epoch: 316 | Iter: 480600 | Total Loss: 0.003103 | Recon Loss: 0.002631 | Commit Loss: 0.000943 | Perplexity: 2053.016935
2025-09-28 08:12:23,095 Stage: Train 0.5 | Epoch: 316 | Iter: 480800 | Total Loss: 0.003253 | Recon Loss: 0.002783 | Commit Loss: 0.000942 | Perplexity: 2045.648438
2025-09-28 08:12:49,657 Stage: Train 0.5 | Epoch: 316 | Iter: 481000 | Total Loss: 0.003234 | Recon Loss: 0.002764 | Commit Loss: 0.000940 | Perplexity: 2047.926512
2025-09-28 08:13:16,321 Stage: Train 0.5 | Epoch: 316 | Iter: 481200 | Total Loss: 0.003178 | Recon Loss: 0.002708 | Commit Loss: 0.000940 | Perplexity: 2050.213409
2025-09-28 08:13:42,943 Stage: Train 0.5 | Epoch: 316 | Iter: 481400 | Total Loss: 0.003211 | Recon Loss: 0.002741 | Commit Loss: 0.000940 | Perplexity: 2049.523690
Trainning Epoch:  96%|█████████▌| 317/330 [25:25:04<43:52, 202.46s/it]2025-09-28 08:14:09,862 Stage: Train 0.5 | Epoch: 317 | Iter: 481600 | Total Loss: 0.003240 | Recon Loss: 0.002771 | Commit Loss: 0.000937 | Perplexity: 2040.368158
2025-09-28 08:14:36,407 Stage: Train 0.5 | Epoch: 317 | Iter: 481800 | Total Loss: 0.003181 | Recon Loss: 0.002713 | Commit Loss: 0.000935 | Perplexity: 2045.359227
2025-09-28 08:15:03,001 Stage: Train 0.5 | Epoch: 317 | Iter: 482000 | Total Loss: 0.003225 | Recon Loss: 0.002753 | Commit Loss: 0.000942 | Perplexity: 2050.591513
2025-09-28 08:15:29,582 Stage: Train 0.5 | Epoch: 317 | Iter: 482200 | Total Loss: 0.003154 | Recon Loss: 0.002689 | Commit Loss: 0.000931 | Perplexity: 2046.408467
2025-09-28 08:15:56,121 Stage: Train 0.5 | Epoch: 317 | Iter: 482400 | Total Loss: 0.003177 | Recon Loss: 0.002707 | Commit Loss: 0.000940 | Perplexity: 2054.686293
2025-09-28 08:16:22,717 Stage: Train 0.5 | Epoch: 317 | Iter: 482600 | Total Loss: 0.003234 | Recon Loss: 0.002763 | Commit Loss: 0.000942 | Perplexity: 2054.613149
2025-09-28 08:16:49,275 Stage: Train 0.5 | Epoch: 317 | Iter: 482800 | Total Loss: 0.003175 | Recon Loss: 0.002706 | Commit Loss: 0.000939 | Perplexity: 2051.113188
2025-09-28 08:17:15,844 Stage: Train 0.5 | Epoch: 317 | Iter: 483000 | Total Loss: 0.003164 | Recon Loss: 0.002694 | Commit Loss: 0.000939 | Perplexity: 2045.871440
Trainning Epoch:  96%|█████████▋| 318/330 [25:28:26<40:28, 202.34s/it]2025-09-28 08:17:42,643 Stage: Train 0.5 | Epoch: 318 | Iter: 483200 | Total Loss: 0.003142 | Recon Loss: 0.002673 | Commit Loss: 0.000938 | Perplexity: 2044.235038
2025-09-28 08:18:09,167 Stage: Train 0.5 | Epoch: 318 | Iter: 483400 | Total Loss: 0.003168 | Recon Loss: 0.002696 | Commit Loss: 0.000944 | Perplexity: 2056.883374
2025-09-28 08:18:35,740 Stage: Train 0.5 | Epoch: 318 | Iter: 483600 | Total Loss: 0.003183 | Recon Loss: 0.002714 | Commit Loss: 0.000938 | Perplexity: 2049.827231
2025-09-28 08:19:02,341 Stage: Train 0.5 | Epoch: 318 | Iter: 483800 | Total Loss: 0.003241 | Recon Loss: 0.002772 | Commit Loss: 0.000939 | Perplexity: 2050.679935
2025-09-28 08:19:28,947 Stage: Train 0.5 | Epoch: 318 | Iter: 484000 | Total Loss: 0.003185 | Recon Loss: 0.002715 | Commit Loss: 0.000938 | Perplexity: 2050.925203
2025-09-28 08:19:55,555 Stage: Train 0.5 | Epoch: 318 | Iter: 484200 | Total Loss: 0.003243 | Recon Loss: 0.002777 | Commit Loss: 0.000933 | Perplexity: 2045.351198
2025-09-28 08:20:22,200 Stage: Train 0.5 | Epoch: 318 | Iter: 484400 | Total Loss: 0.003193 | Recon Loss: 0.002723 | Commit Loss: 0.000941 | Perplexity: 2049.588320
Trainning Epoch:  97%|█████████▋| 319/330 [25:31:48<37:05, 202.30s/it]2025-09-28 08:20:49,032 Stage: Train 0.5 | Epoch: 319 | Iter: 484600 | Total Loss: 0.003146 | Recon Loss: 0.002676 | Commit Loss: 0.000941 | Perplexity: 2052.235897
2025-09-28 08:21:15,571 Stage: Train 0.5 | Epoch: 319 | Iter: 484800 | Total Loss: 0.003204 | Recon Loss: 0.002735 | Commit Loss: 0.000938 | Perplexity: 2057.701938
2025-09-28 08:21:42,176 Stage: Train 0.5 | Epoch: 319 | Iter: 485000 | Total Loss: 0.003263 | Recon Loss: 0.002796 | Commit Loss: 0.000934 | Perplexity: 2051.199460
2025-09-28 08:22:08,795 Stage: Train 0.5 | Epoch: 319 | Iter: 485200 | Total Loss: 0.003051 | Recon Loss: 0.002583 | Commit Loss: 0.000935 | Perplexity: 2046.684631
2025-09-28 08:22:35,358 Stage: Train 0.5 | Epoch: 319 | Iter: 485400 | Total Loss: 0.003229 | Recon Loss: 0.002760 | Commit Loss: 0.000937 | Perplexity: 2055.193191
2025-09-28 08:23:01,873 Stage: Train 0.5 | Epoch: 319 | Iter: 485600 | Total Loss: 0.003182 | Recon Loss: 0.002715 | Commit Loss: 0.000934 | Perplexity: 2044.180257
2025-09-28 08:23:28,497 Stage: Train 0.5 | Epoch: 319 | Iter: 485800 | Total Loss: 0.003249 | Recon Loss: 0.002779 | Commit Loss: 0.000939 | Perplexity: 2055.198201
2025-09-28 08:23:55,031 Stage: Train 0.5 | Epoch: 319 | Iter: 486000 | Total Loss: 0.003252 | Recon Loss: 0.002781 | Commit Loss: 0.000941 | Perplexity: 2052.732604
Trainning Epoch:  97%|█████████▋| 320/330 [25:35:10<33:42, 202.20s/it]2025-09-28 08:24:21,888 Stage: Train 0.5 | Epoch: 320 | Iter: 486200 | Total Loss: 0.003156 | Recon Loss: 0.002690 | Commit Loss: 0.000933 | Perplexity: 2046.872647
2025-09-28 08:24:48,415 Stage: Train 0.5 | Epoch: 320 | Iter: 486400 | Total Loss: 0.003179 | Recon Loss: 0.002713 | Commit Loss: 0.000933 | Perplexity: 2052.312085
2025-09-28 08:25:15,040 Stage: Train 0.5 | Epoch: 320 | Iter: 486600 | Total Loss: 0.003146 | Recon Loss: 0.002680 | Commit Loss: 0.000932 | Perplexity: 2046.488982
2025-09-28 08:25:41,676 Stage: Train 0.5 | Epoch: 320 | Iter: 486800 | Total Loss: 0.003137 | Recon Loss: 0.002671 | Commit Loss: 0.000932 | Perplexity: 2047.882004
2025-09-28 08:26:08,304 Stage: Train 0.5 | Epoch: 320 | Iter: 487000 | Total Loss: 0.003240 | Recon Loss: 0.002771 | Commit Loss: 0.000938 | Perplexity: 2045.768920
2025-09-28 08:26:34,823 Stage: Train 0.5 | Epoch: 320 | Iter: 487200 | Total Loss: 0.003160 | Recon Loss: 0.002689 | Commit Loss: 0.000943 | Perplexity: 2054.335858
2025-09-28 08:27:01,341 Stage: Train 0.5 | Epoch: 320 | Iter: 487400 | Total Loss: 0.003291 | Recon Loss: 0.002824 | Commit Loss: 0.000933 | Perplexity: 2049.378066
Trainning Epoch:  97%|█████████▋| 321/330 [25:38:32<30:19, 202.20s/it]2025-09-28 08:27:28,163 Stage: Train 0.5 | Epoch: 321 | Iter: 487600 | Total Loss: 0.003153 | Recon Loss: 0.002681 | Commit Loss: 0.000943 | Perplexity: 2048.336180
2025-09-28 08:27:54,725 Stage: Train 0.5 | Epoch: 321 | Iter: 487800 | Total Loss: 0.003188 | Recon Loss: 0.002723 | Commit Loss: 0.000931 | Perplexity: 2048.417643
2025-09-28 08:28:21,192 Stage: Train 0.5 | Epoch: 321 | Iter: 488000 | Total Loss: 0.003226 | Recon Loss: 0.002759 | Commit Loss: 0.000934 | Perplexity: 2053.197900
2025-09-28 08:28:47,815 Stage: Train 0.5 | Epoch: 321 | Iter: 488200 | Total Loss: 0.003135 | Recon Loss: 0.002669 | Commit Loss: 0.000931 | Perplexity: 2048.366740
2025-09-28 08:29:14,365 Stage: Train 0.5 | Epoch: 321 | Iter: 488400 | Total Loss: 0.003192 | Recon Loss: 0.002724 | Commit Loss: 0.000936 | Perplexity: 2048.964645
2025-09-28 08:29:40,983 Stage: Train 0.5 | Epoch: 321 | Iter: 488600 | Total Loss: 0.003136 | Recon Loss: 0.002668 | Commit Loss: 0.000937 | Perplexity: 2055.207927
2025-09-28 08:30:07,457 Stage: Train 0.5 | Epoch: 321 | Iter: 488800 | Total Loss: 0.003133 | Recon Loss: 0.002663 | Commit Loss: 0.000941 | Perplexity: 2049.302708
2025-09-28 08:30:33,886 Stage: Train 0.5 | Epoch: 321 | Iter: 489000 | Total Loss: 0.003227 | Recon Loss: 0.002759 | Commit Loss: 0.000937 | Perplexity: 2049.258744
Trainning Epoch:  98%|█████████▊| 322/330 [25:41:54<26:56, 202.04s/it]2025-09-28 08:31:00,607 Stage: Train 0.5 | Epoch: 322 | Iter: 489200 | Total Loss: 0.003276 | Recon Loss: 0.002806 | Commit Loss: 0.000940 | Perplexity: 2048.436578
2025-09-28 08:31:27,365 Stage: Train 0.5 | Epoch: 322 | Iter: 489400 | Total Loss: 0.003360 | Recon Loss: 0.002895 | Commit Loss: 0.000929 | Perplexity: 2041.462504
2025-09-28 08:31:53,899 Stage: Train 0.5 | Epoch: 322 | Iter: 489600 | Total Loss: 0.003060 | Recon Loss: 0.002593 | Commit Loss: 0.000934 | Perplexity: 2052.469896
2025-09-28 08:32:20,499 Stage: Train 0.5 | Epoch: 322 | Iter: 489800 | Total Loss: 0.003181 | Recon Loss: 0.002715 | Commit Loss: 0.000932 | Perplexity: 2052.975953
2025-09-28 08:32:47,016 Stage: Train 0.5 | Epoch: 322 | Iter: 490000 | Total Loss: 0.003185 | Recon Loss: 0.002718 | Commit Loss: 0.000934 | Perplexity: 2046.609770
2025-09-28 08:33:13,666 Stage: Train 0.5 | Epoch: 322 | Iter: 490200 | Total Loss: 0.003170 | Recon Loss: 0.002705 | Commit Loss: 0.000931 | Perplexity: 2048.840652
2025-09-28 08:33:40,254 Stage: Train 0.5 | Epoch: 322 | Iter: 490400 | Total Loss: 0.003225 | Recon Loss: 0.002760 | Commit Loss: 0.000931 | Perplexity: 2052.116083
2025-09-28 08:34:06,877 Stage: Train 0.5 | Epoch: 322 | Iter: 490600 | Total Loss: 0.003258 | Recon Loss: 0.002790 | Commit Loss: 0.000937 | Perplexity: 2054.220550
Trainning Epoch:  98%|█████████▊| 323/330 [25:45:16<23:34, 202.13s/it]2025-09-28 08:34:33,715 Stage: Train 0.5 | Epoch: 323 | Iter: 490800 | Total Loss: 0.003074 | Recon Loss: 0.002611 | Commit Loss: 0.000926 | Perplexity: 2044.804825
2025-09-28 08:35:00,354 Stage: Train 0.5 | Epoch: 323 | Iter: 491000 | Total Loss: 0.003143 | Recon Loss: 0.002678 | Commit Loss: 0.000929 | Perplexity: 2046.965638
2025-09-28 08:35:26,770 Stage: Train 0.5 | Epoch: 323 | Iter: 491200 | Total Loss: 0.003176 | Recon Loss: 0.002707 | Commit Loss: 0.000937 | Perplexity: 2055.049376
2025-09-28 08:35:53,348 Stage: Train 0.5 | Epoch: 323 | Iter: 491400 | Total Loss: 0.003146 | Recon Loss: 0.002677 | Commit Loss: 0.000937 | Perplexity: 2051.282426
2025-09-28 08:36:20,004 Stage: Train 0.5 | Epoch: 323 | Iter: 491600 | Total Loss: 0.003135 | Recon Loss: 0.002667 | Commit Loss: 0.000937 | Perplexity: 2054.362916
2025-09-28 08:36:46,597 Stage: Train 0.5 | Epoch: 323 | Iter: 491800 | Total Loss: 0.003252 | Recon Loss: 0.002786 | Commit Loss: 0.000931 | Perplexity: 2048.635560
2025-09-28 08:37:13,225 Stage: Train 0.5 | Epoch: 323 | Iter: 492000 | Total Loss: 0.003164 | Recon Loss: 0.002694 | Commit Loss: 0.000940 | Perplexity: 2052.086863
Trainning Epoch:  98%|█████████▊| 324/330 [25:48:38<20:12, 202.11s/it]2025-09-28 08:37:39,920 Stage: Train 0.5 | Epoch: 324 | Iter: 492200 | Total Loss: 0.003189 | Recon Loss: 0.002722 | Commit Loss: 0.000933 | Perplexity: 2050.576108
2025-09-28 08:38:06,515 Stage: Train 0.5 | Epoch: 324 | Iter: 492400 | Total Loss: 0.003144 | Recon Loss: 0.002679 | Commit Loss: 0.000931 | Perplexity: 2052.604471
2025-09-28 08:38:33,120 Stage: Train 0.5 | Epoch: 324 | Iter: 492600 | Total Loss: 0.003178 | Recon Loss: 0.002711 | Commit Loss: 0.000934 | Perplexity: 2048.581613
2025-09-28 08:38:59,691 Stage: Train 0.5 | Epoch: 324 | Iter: 492800 | Total Loss: 0.003157 | Recon Loss: 0.002691 | Commit Loss: 0.000932 | Perplexity: 2051.888456
2025-09-28 08:39:26,300 Stage: Train 0.5 | Epoch: 324 | Iter: 493000 | Total Loss: 0.003175 | Recon Loss: 0.002709 | Commit Loss: 0.000933 | Perplexity: 2047.160020
2025-09-28 08:39:52,812 Stage: Train 0.5 | Epoch: 324 | Iter: 493200 | Total Loss: 0.003128 | Recon Loss: 0.002660 | Commit Loss: 0.000935 | Perplexity: 2053.522708
2025-09-28 08:40:19,381 Stage: Train 0.5 | Epoch: 324 | Iter: 493400 | Total Loss: 0.003146 | Recon Loss: 0.002679 | Commit Loss: 0.000934 | Perplexity: 2044.990326
2025-09-28 08:40:45,994 Stage: Train 0.5 | Epoch: 324 | Iter: 493600 | Total Loss: 0.003240 | Recon Loss: 0.002772 | Commit Loss: 0.000936 | Perplexity: 2056.058329
Trainning Epoch:  98%|█████████▊| 325/330 [25:52:00<16:50, 202.08s/it]2025-09-28 08:41:12,635 Stage: Train 0.5 | Epoch: 325 | Iter: 493800 | Total Loss: 0.003132 | Recon Loss: 0.002668 | Commit Loss: 0.000929 | Perplexity: 2042.767980
2025-09-28 08:41:39,196 Stage: Train 0.5 | Epoch: 325 | Iter: 494000 | Total Loss: 0.003169 | Recon Loss: 0.002703 | Commit Loss: 0.000932 | Perplexity: 2057.949152
2025-09-28 08:42:05,628 Stage: Train 0.5 | Epoch: 325 | Iter: 494200 | Total Loss: 0.003223 | Recon Loss: 0.002758 | Commit Loss: 0.000931 | Perplexity: 2051.141953
2025-09-28 08:42:32,389 Stage: Train 0.5 | Epoch: 325 | Iter: 494400 | Total Loss: 0.003143 | Recon Loss: 0.002677 | Commit Loss: 0.000933 | Perplexity: 2054.017857
2025-09-28 08:42:58,931 Stage: Train 0.5 | Epoch: 325 | Iter: 494600 | Total Loss: 0.003351 | Recon Loss: 0.002888 | Commit Loss: 0.000926 | Perplexity: 2045.607891
2025-09-28 08:43:25,534 Stage: Train 0.5 | Epoch: 325 | Iter: 494800 | Total Loss: 0.003210 | Recon Loss: 0.002744 | Commit Loss: 0.000932 | Perplexity: 2050.017963
2025-09-28 08:43:52,124 Stage: Train 0.5 | Epoch: 325 | Iter: 495000 | Total Loss: 0.003076 | Recon Loss: 0.002613 | Commit Loss: 0.000927 | Perplexity: 2052.173119
Trainning Epoch:  99%|█████████▉| 326/330 [25:55:22<13:28, 202.06s/it]2025-09-28 08:44:18,916 Stage: Train 0.5 | Epoch: 326 | Iter: 495200 | Total Loss: 0.003141 | Recon Loss: 0.002675 | Commit Loss: 0.000931 | Perplexity: 2048.066795
2025-09-28 08:44:45,550 Stage: Train 0.5 | Epoch: 326 | Iter: 495400 | Total Loss: 0.003199 | Recon Loss: 0.002734 | Commit Loss: 0.000930 | Perplexity: 2058.745812
2025-09-28 08:45:12,160 Stage: Train 0.5 | Epoch: 326 | Iter: 495600 | Total Loss: 0.003190 | Recon Loss: 0.002724 | Commit Loss: 0.000933 | Perplexity: 2050.300828
2025-09-28 08:45:38,791 Stage: Train 0.5 | Epoch: 326 | Iter: 495800 | Total Loss: 0.003157 | Recon Loss: 0.002691 | Commit Loss: 0.000932 | Perplexity: 2050.267939
2025-09-28 08:46:05,440 Stage: Train 0.5 | Epoch: 326 | Iter: 496000 | Total Loss: 0.003202 | Recon Loss: 0.002738 | Commit Loss: 0.000929 | Perplexity: 2047.729939
2025-09-28 08:46:31,908 Stage: Train 0.5 | Epoch: 326 | Iter: 496200 | Total Loss: 0.003183 | Recon Loss: 0.002716 | Commit Loss: 0.000933 | Perplexity: 2049.532671
2025-09-28 08:46:58,426 Stage: Train 0.5 | Epoch: 326 | Iter: 496400 | Total Loss: 0.003166 | Recon Loss: 0.002700 | Commit Loss: 0.000933 | Perplexity: 2051.775976
2025-09-28 08:47:24,847 Stage: Train 0.5 | Epoch: 326 | Iter: 496600 | Total Loss: 0.003120 | Recon Loss: 0.002654 | Commit Loss: 0.000933 | Perplexity: 2054.690471
Trainning Epoch:  99%|█████████▉| 327/330 [25:58:44<10:06, 202.03s/it]2025-09-28 08:47:51,618 Stage: Train 0.5 | Epoch: 327 | Iter: 496800 | Total Loss: 0.003150 | Recon Loss: 0.002687 | Commit Loss: 0.000927 | Perplexity: 2048.983344
2025-09-28 08:48:18,148 Stage: Train 0.5 | Epoch: 327 | Iter: 497000 | Total Loss: 0.003132 | Recon Loss: 0.002666 | Commit Loss: 0.000931 | Perplexity: 2046.619844
2025-09-28 08:48:44,831 Stage: Train 0.5 | Epoch: 327 | Iter: 497200 | Total Loss: 0.003143 | Recon Loss: 0.002678 | Commit Loss: 0.000930 | Perplexity: 2055.398051
2025-09-28 08:49:11,479 Stage: Train 0.5 | Epoch: 327 | Iter: 497400 | Total Loss: 0.003136 | Recon Loss: 0.002675 | Commit Loss: 0.000921 | Perplexity: 2047.392814
2025-09-28 08:49:38,031 Stage: Train 0.5 | Epoch: 327 | Iter: 497600 | Total Loss: 0.003111 | Recon Loss: 0.002645 | Commit Loss: 0.000931 | Perplexity: 2055.806772
2025-09-28 08:50:04,594 Stage: Train 0.5 | Epoch: 327 | Iter: 497800 | Total Loss: 0.003154 | Recon Loss: 0.002689 | Commit Loss: 0.000929 | Perplexity: 2050.980455
2025-09-28 08:50:31,151 Stage: Train 0.5 | Epoch: 327 | Iter: 498000 | Total Loss: 0.003242 | Recon Loss: 0.002772 | Commit Loss: 0.000939 | Perplexity: 2055.812917
2025-09-28 08:50:57,774 Stage: Train 0.5 | Epoch: 327 | Iter: 498200 | Total Loss: 0.003115 | Recon Loss: 0.002652 | Commit Loss: 0.000927 | Perplexity: 2048.014609
Trainning Epoch:  99%|█████████▉| 328/330 [26:02:07<06:44, 202.06s/it]2025-09-28 08:51:24,380 Stage: Train 0.5 | Epoch: 328 | Iter: 498400 | Total Loss: 0.003134 | Recon Loss: 0.002671 | Commit Loss: 0.000926 | Perplexity: 2055.919951
2025-09-28 08:51:50,932 Stage: Train 0.5 | Epoch: 328 | Iter: 498600 | Total Loss: 0.003174 | Recon Loss: 0.002710 | Commit Loss: 0.000927 | Perplexity: 2050.631199
2025-09-28 08:52:17,466 Stage: Train 0.5 | Epoch: 328 | Iter: 498800 | Total Loss: 0.003160 | Recon Loss: 0.002696 | Commit Loss: 0.000928 | Perplexity: 2050.267009
2025-09-28 08:52:44,060 Stage: Train 0.5 | Epoch: 328 | Iter: 499000 | Total Loss: 0.003249 | Recon Loss: 0.002785 | Commit Loss: 0.000927 | Perplexity: 2051.317326
2025-09-28 08:53:10,689 Stage: Train 0.5 | Epoch: 328 | Iter: 499200 | Total Loss: 0.003077 | Recon Loss: 0.002612 | Commit Loss: 0.000930 | Perplexity: 2049.520576
2025-09-28 08:53:37,245 Stage: Train 0.5 | Epoch: 328 | Iter: 499400 | Total Loss: 0.003137 | Recon Loss: 0.002673 | Commit Loss: 0.000929 | Perplexity: 2053.341679
2025-09-28 08:54:03,748 Stage: Train 0.5 | Epoch: 328 | Iter: 499600 | Total Loss: 0.003202 | Recon Loss: 0.002735 | Commit Loss: 0.000933 | Perplexity: 2057.331375
Trainning Epoch: 100%|█████████▉| 329/330 [26:05:28<03:21, 201.96s/it]2025-09-28 08:54:30,453 Stage: Train 0.5 | Epoch: 329 | Iter: 499800 | Total Loss: 0.003127 | Recon Loss: 0.002663 | Commit Loss: 0.000929 | Perplexity: 2056.238726
2025-09-28 08:54:57,003 Stage: Train 0.5 | Epoch: 329 | Iter: 500000 | Total Loss: 0.003113 | Recon Loss: 0.002651 | Commit Loss: 0.000925 | Perplexity: 2050.504624
2025-09-28 08:54:57,003 Saving model at iteration 500000
2025-09-28 08:54:57,169 Saving current state to vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000
2025-09-28 08:54:57,603 Model weights saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/model.safetensors
2025-09-28 08:54:58,084 Optimizer state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/optimizer.bin
2025-09-28 08:54:58,085 Scheduler state saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/scheduler.bin
2025-09-28 08:54:58,085 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/sampler.bin
2025-09-28 08:54:58,086 Random states saved in vqvae_experiment/joint_only/joint3d_image/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/random_states_0.pkl
Trainning Epoch: 100%|█████████▉| 329/330 [26:06:03<04:45, 285.60s/it]
2025-09-28 08:54:58,133 Training finished
