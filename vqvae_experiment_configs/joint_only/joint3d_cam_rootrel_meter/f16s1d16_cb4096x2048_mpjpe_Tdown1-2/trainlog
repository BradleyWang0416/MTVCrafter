The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-26 14:10:57,703 
python train_vqvae_new.py --batch_size 64 --config vqvae_experiment_configs/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/config.yaml --data_mode null --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2 --not_find_unused_parameters --nb_code 4096 --codebook_dim 2048 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level 3 --vision_guidance_ratio 0 --downsample_time [1,2] --frame_upsample_rate [2.0,1.0] --fix_weights --resume_pth  --joint_data_type joint3d_cam_rootrel_meter
2025-09-26 14:11:07,395 Data loaded with 97196 samples
2025-09-26 14:11:07,853 Trainable parameters: 38,884,867
2025-09-26 14:11:07,853 Non-trainable parameters: 0
2025-09-26 14:11:09,266 Number of trainable parameters: 38.884867 M
2025-09-26 14:11:09,266 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'null', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [[]], 'normalize': 'isotropic', 'filter_invalid_images': False, 'processed_image_shape': None, 'backbone': 'hrnet_32', 'get_item_list': ['joint3d_cam_rootrel_meter', 'slice_id'], 'config': 'vqvae_experiment_configs/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/config.yaml', 'resume_pth': '', 'batch_size': 64, 'commit_ratio': 0.5, 'nb_code': 4096, 'codebook_dim': 2048, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_cam_rootrel_meter', 'hrnet_output_level': 3, 'fix_weights': True, 'vision_guidance_ratio': 0.0, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0]}
Trainning Epoch:   0%|          | 0/330 [00:00<?, ?it/s]2025-09-26 14:12:42,769 current_lr 0.000008 at iteration 200
2025-09-26 14:12:43,233 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.201637 | Recon Loss: 0.197859 | Commit Loss: 0.007555 | Perplexity: 1476.647150
2025-09-26 14:14:17,441 current_lr 0.000016 at iteration 400
2025-09-26 14:14:17,906 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.120697 | Recon Loss: 0.098697 | Commit Loss: 0.044000 | Perplexity: 1094.239324
2025-09-26 14:15:52,030 current_lr 0.000024 at iteration 600
2025-09-26 14:15:52,495 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.118238 | Recon Loss: 0.074749 | Commit Loss: 0.086978 | Perplexity: 1154.374807
2025-09-26 14:17:26,663 current_lr 0.000032 at iteration 800
2025-09-26 14:17:27,126 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.109739 | Recon Loss: 0.063464 | Commit Loss: 0.092549 | Perplexity: 1195.508205
2025-09-26 14:19:01,181 current_lr 0.000040 at iteration 1000
2025-09-26 14:19:01,639 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.097019 | Recon Loss: 0.056612 | Commit Loss: 0.080814 | Perplexity: 1196.548488
2025-09-26 14:20:35,620 current_lr 0.000048 at iteration 1200
2025-09-26 14:20:36,084 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.086435 | Recon Loss: 0.051830 | Commit Loss: 0.069210 | Perplexity: 1193.539821
2025-09-26 14:22:10,232 current_lr 0.000056 at iteration 1400
2025-09-26 14:22:10,698 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.075642 | Recon Loss: 0.047131 | Commit Loss: 0.057022 | Perplexity: 1189.310180
Trainning Epoch:   0%|          | 1/330 [11:57<65:34:44, 717.58s/it]2025-09-26 14:23:44,881 current_lr 0.000064 at iteration 1600
2025-09-26 14:23:45,345 Stage: Warm Up | Epoch: 1 | Iter: 1600 | Total Loss: 0.066695 | Recon Loss: 0.043981 | Commit Loss: 0.045426 | Perplexity: 1175.492781
2025-09-26 14:25:19,303 current_lr 0.000072 at iteration 1800
2025-09-26 14:25:19,765 Stage: Warm Up | Epoch: 1 | Iter: 1800 | Total Loss: 0.059126 | Recon Loss: 0.041012 | Commit Loss: 0.036228 | Perplexity: 1151.429846
2025-09-26 14:26:54,016 current_lr 0.000080 at iteration 2000
2025-09-26 14:26:54,475 Stage: Warm Up | Epoch: 1 | Iter: 2000 | Total Loss: 0.053858 | Recon Loss: 0.039243 | Commit Loss: 0.029231 | Perplexity: 1118.504700
2025-09-26 14:28:28,612 current_lr 0.000088 at iteration 2200
2025-09-26 14:28:29,079 Stage: Warm Up | Epoch: 1 | Iter: 2200 | Total Loss: 0.049820 | Recon Loss: 0.037604 | Commit Loss: 0.024433 | Perplexity: 1093.476160
2025-09-26 14:30:03,259 current_lr 0.000096 at iteration 2400
2025-09-26 14:30:03,727 Stage: Warm Up | Epoch: 1 | Iter: 2400 | Total Loss: 0.046524 | Recon Loss: 0.035829 | Commit Loss: 0.021390 | Perplexity: 1076.836980
2025-09-26 14:31:37,989 current_lr 0.000104 at iteration 2600
2025-09-26 14:31:38,459 Stage: Warm Up | Epoch: 1 | Iter: 2600 | Total Loss: 0.044415 | Recon Loss: 0.035282 | Commit Loss: 0.018266 | Perplexity: 1059.900733
2025-09-26 14:33:05,120 current_lr 0.000112 at iteration 2800
2025-09-26 14:33:05,470 Stage: Warm Up | Epoch: 1 | Iter: 2800 | Total Loss: 0.042272 | Recon Loss: 0.034091 | Commit Loss: 0.016363 | Perplexity: 1049.735623
2025-09-26 14:34:15,533 current_lr 0.000120 at iteration 3000
2025-09-26 14:34:15,881 Stage: Warm Up | Epoch: 1 | Iter: 3000 | Total Loss: 0.040046 | Recon Loss: 0.032549 | Commit Loss: 0.014992 | Perplexity: 1048.137523
Trainning Epoch:   1%|          | 2/330 [23:19<63:29:20, 696.83s/it]2025-09-26 14:35:25,620 current_lr 0.000128 at iteration 3200
2025-09-26 14:35:25,965 Stage: Warm Up | Epoch: 2 | Iter: 3200 | Total Loss: 0.038501 | Recon Loss: 0.031989 | Commit Loss: 0.013024 | Perplexity: 1038.576117
2025-09-26 14:36:35,930 current_lr 0.000136 at iteration 3400
2025-09-26 14:36:36,285 Stage: Warm Up | Epoch: 2 | Iter: 3400 | Total Loss: 0.036459 | Recon Loss: 0.030608 | Commit Loss: 0.011703 | Perplexity: 1039.375948
2025-09-26 14:37:46,201 current_lr 0.000144 at iteration 3600
2025-09-26 14:37:46,545 Stage: Warm Up | Epoch: 2 | Iter: 3600 | Total Loss: 0.034996 | Recon Loss: 0.029593 | Commit Loss: 0.010806 | Perplexity: 1043.493244
2025-09-26 14:38:56,452 current_lr 0.000152 at iteration 3800
2025-09-26 14:38:56,796 Stage: Warm Up | Epoch: 2 | Iter: 3800 | Total Loss: 0.034032 | Recon Loss: 0.029049 | Commit Loss: 0.009966 | Perplexity: 1047.833000
2025-09-26 14:40:06,888 current_lr 0.000160 at iteration 4000
2025-09-26 14:40:07,229 Stage: Warm Up | Epoch: 2 | Iter: 4000 | Total Loss: 0.033445 | Recon Loss: 0.028791 | Commit Loss: 0.009308 | Perplexity: 1043.837397
2025-09-26 14:41:17,133 current_lr 0.000168 at iteration 4200
2025-09-26 14:41:17,475 Stage: Warm Up | Epoch: 2 | Iter: 4200 | Total Loss: 0.032788 | Recon Loss: 0.028444 | Commit Loss: 0.008687 | Perplexity: 1037.162428
2025-09-26 14:42:27,226 current_lr 0.000176 at iteration 4400
2025-09-26 14:42:27,569 Stage: Warm Up | Epoch: 2 | Iter: 4400 | Total Loss: 0.031949 | Recon Loss: 0.027741 | Commit Loss: 0.008418 | Perplexity: 1037.420416
Trainning Epoch:   1%|          | 3/330 [32:13<56:31:23, 622.27s/it]2025-09-26 14:43:37,651 current_lr 0.000184 at iteration 4600
2025-09-26 14:43:37,994 Stage: Warm Up | Epoch: 3 | Iter: 4600 | Total Loss: 0.030934 | Recon Loss: 0.026860 | Commit Loss: 0.008148 | Perplexity: 1032.764851
2025-09-26 14:44:48,000 current_lr 0.000192 at iteration 4800
2025-09-26 14:44:48,348 Stage: Warm Up | Epoch: 3 | Iter: 4800 | Total Loss: 0.030595 | Recon Loss: 0.026661 | Commit Loss: 0.007867 | Perplexity: 1024.810508
2025-09-26 14:45:58,440 current_lr 0.000200 at iteration 5000
2025-09-26 14:45:58,793 Stage: Warm Up | Epoch: 3 | Iter: 5000 | Total Loss: 0.029955 | Recon Loss: 0.026174 | Commit Loss: 0.007561 | Perplexity: 1015.138581
2025-09-26 14:47:09,072 Stage: Train 0.5 | Epoch: 3 | Iter: 5200 | Total Loss: 0.029035 | Recon Loss: 0.025326 | Commit Loss: 0.007417 | Perplexity: 1014.845391
2025-09-26 14:48:02,481 Stage: Train 0.5 | Epoch: 3 | Iter: 5400 | Total Loss: 0.028810 | Recon Loss: 0.025178 | Commit Loss: 0.007263 | Perplexity: 1009.453483
2025-09-26 14:48:47,911 Stage: Train 0.5 | Epoch: 3 | Iter: 5600 | Total Loss: 0.028063 | Recon Loss: 0.024452 | Commit Loss: 0.007221 | Perplexity: 1013.175955
2025-09-26 14:49:33,747 Stage: Train 0.5 | Epoch: 3 | Iter: 5800 | Total Loss: 0.027006 | Recon Loss: 0.023464 | Commit Loss: 0.007085 | Perplexity: 1023.565265
2025-09-26 14:50:19,675 Stage: Train 0.5 | Epoch: 3 | Iter: 6000 | Total Loss: 0.026291 | Recon Loss: 0.022832 | Commit Loss: 0.006916 | Perplexity: 1031.152422
Trainning Epoch:   1%|          | 4/330 [39:27<49:37:43, 548.05s/it]2025-09-26 14:51:05,234 Stage: Train 0.5 | Epoch: 4 | Iter: 6200 | Total Loss: 0.026274 | Recon Loss: 0.022851 | Commit Loss: 0.006846 | Perplexity: 1034.973495
2025-09-26 14:51:50,905 Stage: Train 0.5 | Epoch: 4 | Iter: 6400 | Total Loss: 0.025077 | Recon Loss: 0.021721 | Commit Loss: 0.006712 | Perplexity: 1039.083223
2025-09-26 14:52:36,802 Stage: Train 0.5 | Epoch: 4 | Iter: 6600 | Total Loss: 0.024651 | Recon Loss: 0.021326 | Commit Loss: 0.006650 | Perplexity: 1049.351237
2025-09-26 14:53:22,732 Stage: Train 0.5 | Epoch: 4 | Iter: 6800 | Total Loss: 0.024321 | Recon Loss: 0.021037 | Commit Loss: 0.006568 | Perplexity: 1053.002419
2025-09-26 14:54:08,744 Stage: Train 0.5 | Epoch: 4 | Iter: 7000 | Total Loss: 0.023830 | Recon Loss: 0.020602 | Commit Loss: 0.006455 | Perplexity: 1058.662286
2025-09-26 14:54:54,393 Stage: Train 0.5 | Epoch: 4 | Iter: 7200 | Total Loss: 0.023161 | Recon Loss: 0.019907 | Commit Loss: 0.006508 | Perplexity: 1061.791606
2025-09-26 14:55:40,278 Stage: Train 0.5 | Epoch: 4 | Iter: 7400 | Total Loss: 0.023196 | Recon Loss: 0.019980 | Commit Loss: 0.006432 | Perplexity: 1062.825153
Trainning Epoch:   2%|▏         | 5/330 [45:15<42:57:21, 475.82s/it]2025-09-26 14:56:26,054 Stage: Train 0.5 | Epoch: 5 | Iter: 7600 | Total Loss: 0.022808 | Recon Loss: 0.019566 | Commit Loss: 0.006483 | Perplexity: 1066.562082
2025-09-26 14:57:11,741 Stage: Train 0.5 | Epoch: 5 | Iter: 7800 | Total Loss: 0.022155 | Recon Loss: 0.018960 | Commit Loss: 0.006390 | Perplexity: 1071.913135
2025-09-26 14:57:57,097 Stage: Train 0.5 | Epoch: 5 | Iter: 8000 | Total Loss: 0.022375 | Recon Loss: 0.019185 | Commit Loss: 0.006380 | Perplexity: 1070.904988
2025-09-26 14:58:42,432 Stage: Train 0.5 | Epoch: 5 | Iter: 8200 | Total Loss: 0.021982 | Recon Loss: 0.018790 | Commit Loss: 0.006383 | Perplexity: 1076.830186
2025-09-26 14:59:27,879 Stage: Train 0.5 | Epoch: 5 | Iter: 8400 | Total Loss: 0.021647 | Recon Loss: 0.018495 | Commit Loss: 0.006306 | Perplexity: 1077.575682
2025-09-26 15:00:13,187 Stage: Train 0.5 | Epoch: 5 | Iter: 8600 | Total Loss: 0.021402 | Recon Loss: 0.018245 | Commit Loss: 0.006315 | Perplexity: 1075.629468
2025-09-26 15:00:58,248 Stage: Train 0.5 | Epoch: 5 | Iter: 8800 | Total Loss: 0.021055 | Recon Loss: 0.017892 | Commit Loss: 0.006326 | Perplexity: 1079.644649
2025-09-26 15:01:43,648 Stage: Train 0.5 | Epoch: 5 | Iter: 9000 | Total Loss: 0.021302 | Recon Loss: 0.018151 | Commit Loss: 0.006301 | Perplexity: 1083.286478
Trainning Epoch:   2%|▏         | 6/330 [51:00<38:49:03, 431.31s/it]2025-09-26 15:02:29,420 Stage: Train 0.5 | Epoch: 6 | Iter: 9200 | Total Loss: 0.020818 | Recon Loss: 0.017688 | Commit Loss: 0.006260 | Perplexity: 1083.051728
2025-09-26 15:03:14,909 Stage: Train 0.5 | Epoch: 6 | Iter: 9400 | Total Loss: 0.020535 | Recon Loss: 0.017401 | Commit Loss: 0.006268 | Perplexity: 1089.965862
2025-09-26 15:04:00,225 Stage: Train 0.5 | Epoch: 6 | Iter: 9600 | Total Loss: 0.020164 | Recon Loss: 0.017087 | Commit Loss: 0.006154 | Perplexity: 1089.280098
2025-09-26 15:04:45,508 Stage: Train 0.5 | Epoch: 6 | Iter: 9800 | Total Loss: 0.020294 | Recon Loss: 0.017191 | Commit Loss: 0.006207 | Perplexity: 1094.582753
2025-09-26 15:05:31,076 Stage: Train 0.5 | Epoch: 6 | Iter: 10000 | Total Loss: 0.019871 | Recon Loss: 0.016816 | Commit Loss: 0.006112 | Perplexity: 1097.118873
2025-09-26 15:06:16,620 Stage: Train 0.5 | Epoch: 6 | Iter: 10200 | Total Loss: 0.019917 | Recon Loss: 0.016882 | Commit Loss: 0.006070 | Perplexity: 1099.203091
2025-09-26 15:07:02,098 Stage: Train 0.5 | Epoch: 6 | Iter: 10400 | Total Loss: 0.019534 | Recon Loss: 0.016523 | Commit Loss: 0.006020 | Perplexity: 1105.753562
2025-09-26 15:07:47,464 Stage: Train 0.5 | Epoch: 6 | Iter: 10600 | Total Loss: 0.019314 | Recon Loss: 0.016321 | Commit Loss: 0.005985 | Perplexity: 1107.712084
Trainning Epoch:   2%|▏         | 7/330 [56:45<36:10:32, 403.20s/it]2025-09-26 15:08:33,146 Stage: Train 0.5 | Epoch: 7 | Iter: 10800 | Total Loss: 0.019235 | Recon Loss: 0.016256 | Commit Loss: 0.005957 | Perplexity: 1113.046944
2025-09-26 15:09:18,819 Stage: Train 0.5 | Epoch: 7 | Iter: 11000 | Total Loss: 0.018984 | Recon Loss: 0.016052 | Commit Loss: 0.005865 | Perplexity: 1110.477704
2025-09-26 15:10:04,361 Stage: Train 0.5 | Epoch: 7 | Iter: 11200 | Total Loss: 0.018747 | Recon Loss: 0.015837 | Commit Loss: 0.005819 | Perplexity: 1110.751839
2025-09-26 15:10:49,900 Stage: Train 0.5 | Epoch: 7 | Iter: 11400 | Total Loss: 0.018635 | Recon Loss: 0.015716 | Commit Loss: 0.005838 | Perplexity: 1115.332748
2025-09-26 15:11:35,108 Stage: Train 0.5 | Epoch: 7 | Iter: 11600 | Total Loss: 0.018548 | Recon Loss: 0.015669 | Commit Loss: 0.005758 | Perplexity: 1115.776161
2025-09-26 15:12:18,688 Stage: Train 0.5 | Epoch: 7 | Iter: 11800 | Total Loss: 0.018169 | Recon Loss: 0.015298 | Commit Loss: 0.005743 | Perplexity: 1112.994422
2025-09-26 15:13:04,147 Stage: Train 0.5 | Epoch: 7 | Iter: 12000 | Total Loss: 0.018350 | Recon Loss: 0.015523 | Commit Loss: 0.005653 | Perplexity: 1111.933154
Trainning Epoch:   2%|▏         | 8/330 [1:02:29<34:22:33, 384.33s/it]2025-09-26 15:13:50,062 Stage: Train 0.5 | Epoch: 8 | Iter: 12200 | Total Loss: 0.018078 | Recon Loss: 0.015290 | Commit Loss: 0.005577 | Perplexity: 1112.445461
2025-09-26 15:14:35,062 Stage: Train 0.5 | Epoch: 8 | Iter: 12400 | Total Loss: 0.018001 | Recon Loss: 0.015249 | Commit Loss: 0.005504 | Perplexity: 1108.053466
2025-09-26 15:15:20,410 Stage: Train 0.5 | Epoch: 8 | Iter: 12600 | Total Loss: 0.017696 | Recon Loss: 0.014974 | Commit Loss: 0.005443 | Perplexity: 1106.314908
2025-09-26 15:16:06,047 Stage: Train 0.5 | Epoch: 8 | Iter: 12800 | Total Loss: 0.017453 | Recon Loss: 0.014725 | Commit Loss: 0.005456 | Perplexity: 1108.472184
2025-09-26 15:16:51,527 Stage: Train 0.5 | Epoch: 8 | Iter: 13000 | Total Loss: 0.017378 | Recon Loss: 0.014725 | Commit Loss: 0.005306 | Perplexity: 1098.241000
2025-09-26 15:17:36,919 Stage: Train 0.5 | Epoch: 8 | Iter: 13200 | Total Loss: 0.017368 | Recon Loss: 0.014757 | Commit Loss: 0.005221 | Perplexity: 1096.696964
2025-09-26 15:18:22,228 Stage: Train 0.5 | Epoch: 8 | Iter: 13400 | Total Loss: 0.016853 | Recon Loss: 0.014243 | Commit Loss: 0.005219 | Perplexity: 1097.543870
2025-09-26 15:19:07,682 Stage: Train 0.5 | Epoch: 8 | Iter: 13600 | Total Loss: 0.016762 | Recon Loss: 0.014206 | Commit Loss: 0.005112 | Perplexity: 1090.014602
Trainning Epoch:   3%|▎         | 9/330 [1:08:14<33:10:09, 371.99s/it]2025-09-26 15:19:53,254 Stage: Train 0.5 | Epoch: 9 | Iter: 13800 | Total Loss: 0.016837 | Recon Loss: 0.014323 | Commit Loss: 0.005028 | Perplexity: 1088.731982
2025-09-26 15:20:38,900 Stage: Train 0.5 | Epoch: 9 | Iter: 14000 | Total Loss: 0.016334 | Recon Loss: 0.013874 | Commit Loss: 0.004921 | Perplexity: 1083.952648
2025-09-26 15:21:24,102 Stage: Train 0.5 | Epoch: 9 | Iter: 14200 | Total Loss: 0.016259 | Recon Loss: 0.013834 | Commit Loss: 0.004852 | Perplexity: 1081.737469
2025-09-26 15:22:09,781 Stage: Train 0.5 | Epoch: 9 | Iter: 14400 | Total Loss: 0.016199 | Recon Loss: 0.013835 | Commit Loss: 0.004728 | Perplexity: 1074.893551
2025-09-26 15:22:55,447 Stage: Train 0.5 | Epoch: 9 | Iter: 14600 | Total Loss: 0.015853 | Recon Loss: 0.013549 | Commit Loss: 0.004608 | Perplexity: 1068.154293
2025-09-26 15:23:40,933 Stage: Train 0.5 | Epoch: 9 | Iter: 14800 | Total Loss: 0.015719 | Recon Loss: 0.013425 | Commit Loss: 0.004588 | Perplexity: 1068.312027
2025-09-26 15:24:26,460 Stage: Train 0.5 | Epoch: 9 | Iter: 15000 | Total Loss: 0.015852 | Recon Loss: 0.013609 | Commit Loss: 0.004485 | Perplexity: 1062.136247
Trainning Epoch:   3%|▎         | 10/330 [1:14:00<32:20:49, 363.90s/it]2025-09-26 15:25:12,004 Stage: Train 0.5 | Epoch: 10 | Iter: 15200 | Total Loss: 0.015775 | Recon Loss: 0.013580 | Commit Loss: 0.004390 | Perplexity: 1053.684715
2025-09-26 15:25:57,559 Stage: Train 0.5 | Epoch: 10 | Iter: 15400 | Total Loss: 0.015203 | Recon Loss: 0.013057 | Commit Loss: 0.004292 | Perplexity: 1052.739065
2025-09-26 15:26:43,374 Stage: Train 0.5 | Epoch: 10 | Iter: 15600 | Total Loss: 0.015078 | Recon Loss: 0.012982 | Commit Loss: 0.004193 | Perplexity: 1048.854138
2025-09-26 15:27:29,078 Stage: Train 0.5 | Epoch: 10 | Iter: 15800 | Total Loss: 0.014986 | Recon Loss: 0.012912 | Commit Loss: 0.004148 | Perplexity: 1048.176686
2025-09-26 15:28:14,581 Stage: Train 0.5 | Epoch: 10 | Iter: 16000 | Total Loss: 0.014853 | Recon Loss: 0.012870 | Commit Loss: 0.003967 | Perplexity: 1039.142268
2025-09-26 15:29:00,107 Stage: Train 0.5 | Epoch: 10 | Iter: 16200 | Total Loss: 0.014547 | Recon Loss: 0.012606 | Commit Loss: 0.003883 | Perplexity: 1031.744990
2025-09-26 15:29:45,826 Stage: Train 0.5 | Epoch: 10 | Iter: 16400 | Total Loss: 0.014530 | Recon Loss: 0.012600 | Commit Loss: 0.003859 | Perplexity: 1033.508332
2025-09-26 15:30:31,365 Stage: Train 0.5 | Epoch: 10 | Iter: 16600 | Total Loss: 0.014206 | Recon Loss: 0.012326 | Commit Loss: 0.003759 | Perplexity: 1029.677418
Trainning Epoch:   3%|▎         | 11/330 [1:19:47<31:46:53, 358.66s/it]2025-09-26 15:31:17,086 Stage: Train 0.5 | Epoch: 11 | Iter: 16800 | Total Loss: 0.014131 | Recon Loss: 0.012284 | Commit Loss: 0.003694 | Perplexity: 1030.708253
2025-09-26 15:32:02,791 Stage: Train 0.5 | Epoch: 11 | Iter: 17000 | Total Loss: 0.013879 | Recon Loss: 0.012060 | Commit Loss: 0.003638 | Perplexity: 1028.100901
2025-09-26 15:32:48,419 Stage: Train 0.5 | Epoch: 11 | Iter: 17200 | Total Loss: 0.013684 | Recon Loss: 0.011905 | Commit Loss: 0.003559 | Perplexity: 1022.612324
2025-09-26 15:33:34,147 Stage: Train 0.5 | Epoch: 11 | Iter: 17400 | Total Loss: 0.013887 | Recon Loss: 0.012159 | Commit Loss: 0.003455 | Perplexity: 1019.380328
2025-09-26 15:34:19,719 Stage: Train 0.5 | Epoch: 11 | Iter: 17600 | Total Loss: 0.013491 | Recon Loss: 0.011807 | Commit Loss: 0.003366 | Perplexity: 1018.735984
2025-09-26 15:35:05,130 Stage: Train 0.5 | Epoch: 11 | Iter: 17800 | Total Loss: 0.013555 | Recon Loss: 0.011883 | Commit Loss: 0.003344 | Perplexity: 1020.612020
2025-09-26 15:35:50,682 Stage: Train 0.5 | Epoch: 11 | Iter: 18000 | Total Loss: 0.013444 | Recon Loss: 0.011792 | Commit Loss: 0.003304 | Perplexity: 1024.442222
2025-09-26 15:36:36,032 Stage: Train 0.5 | Epoch: 11 | Iter: 18200 | Total Loss: 0.013284 | Recon Loss: 0.011665 | Commit Loss: 0.003239 | Perplexity: 1026.446327
Trainning Epoch:   4%|▎         | 12/330 [1:25:33<31:20:41, 354.85s/it]2025-09-26 15:37:21,903 Stage: Train 0.5 | Epoch: 12 | Iter: 18400 | Total Loss: 0.013133 | Recon Loss: 0.011562 | Commit Loss: 0.003142 | Perplexity: 1022.846840
2025-09-26 15:38:07,388 Stage: Train 0.5 | Epoch: 12 | Iter: 18600 | Total Loss: 0.012960 | Recon Loss: 0.011380 | Commit Loss: 0.003160 | Perplexity: 1035.538484
2025-09-26 15:38:52,898 Stage: Train 0.5 | Epoch: 12 | Iter: 18800 | Total Loss: 0.012920 | Recon Loss: 0.011384 | Commit Loss: 0.003072 | Perplexity: 1032.861324
2025-09-26 15:39:38,545 Stage: Train 0.5 | Epoch: 12 | Iter: 19000 | Total Loss: 0.012725 | Recon Loss: 0.011242 | Commit Loss: 0.002966 | Perplexity: 1035.670175
2025-09-26 15:40:23,932 Stage: Train 0.5 | Epoch: 12 | Iter: 19200 | Total Loss: 0.012670 | Recon Loss: 0.011220 | Commit Loss: 0.002901 | Perplexity: 1042.047413
2025-09-26 15:41:09,371 Stage: Train 0.5 | Epoch: 12 | Iter: 19400 | Total Loss: 0.012429 | Recon Loss: 0.011000 | Commit Loss: 0.002858 | Perplexity: 1041.227573
2025-09-26 15:41:54,904 Stage: Train 0.5 | Epoch: 12 | Iter: 19600 | Total Loss: 0.012251 | Recon Loss: 0.010833 | Commit Loss: 0.002838 | Perplexity: 1055.862690
Trainning Epoch:   4%|▍         | 13/330 [1:31:19<31:00:39, 352.18s/it]2025-09-26 15:42:40,641 Stage: Train 0.5 | Epoch: 13 | Iter: 19800 | Total Loss: 0.012303 | Recon Loss: 0.010917 | Commit Loss: 0.002771 | Perplexity: 1062.461900
2025-09-26 15:43:26,193 Stage: Train 0.5 | Epoch: 13 | Iter: 20000 | Total Loss: 0.011981 | Recon Loss: 0.010627 | Commit Loss: 0.002708 | Perplexity: 1062.313301
2025-09-26 15:43:26,193 Saving model at iteration 20000
2025-09-26 15:43:26,392 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000
2025-09-26 15:43:26,679 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/model.safetensors
2025-09-26 15:43:27,058 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/optimizer.bin
2025-09-26 15:43:27,059 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/scheduler.bin
2025-09-26 15:43:27,059 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/sampler.bin
2025-09-26 15:43:27,060 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/random_states_0.pkl
2025-09-26 15:44:12,937 Stage: Train 0.5 | Epoch: 13 | Iter: 20200 | Total Loss: 0.011882 | Recon Loss: 0.010565 | Commit Loss: 0.002634 | Perplexity: 1071.133474
2025-09-26 15:44:58,408 Stage: Train 0.5 | Epoch: 13 | Iter: 20400 | Total Loss: 0.011842 | Recon Loss: 0.010531 | Commit Loss: 0.002622 | Perplexity: 1076.633981
2025-09-26 15:45:43,983 Stage: Train 0.5 | Epoch: 13 | Iter: 20600 | Total Loss: 0.011945 | Recon Loss: 0.010687 | Commit Loss: 0.002516 | Perplexity: 1088.511182
2025-09-26 15:46:29,569 Stage: Train 0.5 | Epoch: 13 | Iter: 20800 | Total Loss: 0.011663 | Recon Loss: 0.010412 | Commit Loss: 0.002501 | Perplexity: 1103.214586
2025-09-26 15:47:15,036 Stage: Train 0.5 | Epoch: 13 | Iter: 21000 | Total Loss: 0.011314 | Recon Loss: 0.010084 | Commit Loss: 0.002460 | Perplexity: 1121.186361
2025-09-26 15:48:00,390 Stage: Train 0.5 | Epoch: 13 | Iter: 21200 | Total Loss: 0.011587 | Recon Loss: 0.010397 | Commit Loss: 0.002382 | Perplexity: 1132.803235
Trainning Epoch:   4%|▍         | 14/330 [1:37:05<30:45:58, 350.50s/it]2025-09-26 15:48:45,791 Stage: Train 0.5 | Epoch: 14 | Iter: 21400 | Total Loss: 0.011379 | Recon Loss: 0.010235 | Commit Loss: 0.002289 | Perplexity: 1140.511573
2025-09-26 15:49:31,486 Stage: Train 0.5 | Epoch: 14 | Iter: 21600 | Total Loss: 0.011172 | Recon Loss: 0.010029 | Commit Loss: 0.002287 | Perplexity: 1161.953868
2025-09-26 15:50:17,053 Stage: Train 0.5 | Epoch: 14 | Iter: 21800 | Total Loss: 0.011120 | Recon Loss: 0.009997 | Commit Loss: 0.002245 | Perplexity: 1170.899749
2025-09-26 15:51:02,645 Stage: Train 0.5 | Epoch: 14 | Iter: 22000 | Total Loss: 0.011147 | Recon Loss: 0.010028 | Commit Loss: 0.002238 | Perplexity: 1181.343198
2025-09-26 15:51:47,566 Stage: Train 0.5 | Epoch: 14 | Iter: 22200 | Total Loss: 0.010841 | Recon Loss: 0.009738 | Commit Loss: 0.002206 | Perplexity: 1187.925127
2025-09-26 15:52:33,117 Stage: Train 0.5 | Epoch: 14 | Iter: 22400 | Total Loss: 0.010894 | Recon Loss: 0.009791 | Commit Loss: 0.002206 | Perplexity: 1196.174366
2025-09-26 15:53:18,755 Stage: Train 0.5 | Epoch: 14 | Iter: 22600 | Total Loss: 0.010942 | Recon Loss: 0.009851 | Commit Loss: 0.002182 | Perplexity: 1198.211499
Trainning Epoch:   5%|▍         | 15/330 [1:42:51<30:32:26, 349.04s/it]2025-09-26 15:54:04,318 Stage: Train 0.5 | Epoch: 15 | Iter: 22800 | Total Loss: 0.010948 | Recon Loss: 0.009882 | Commit Loss: 0.002130 | Perplexity: 1195.890255
2025-09-26 15:54:49,552 Stage: Train 0.5 | Epoch: 15 | Iter: 23000 | Total Loss: 0.010635 | Recon Loss: 0.009568 | Commit Loss: 0.002136 | Perplexity: 1199.378175
2025-09-26 15:55:35,462 Stage: Train 0.5 | Epoch: 15 | Iter: 23200 | Total Loss: 0.011181 | Recon Loss: 0.009941 | Commit Loss: 0.002480 | Perplexity: 1219.071535
2025-09-26 15:56:19,803 Stage: Train 0.5 | Epoch: 15 | Iter: 23400 | Total Loss: 0.010640 | Recon Loss: 0.009581 | Commit Loss: 0.002118 | Perplexity: 1201.836876
2025-09-26 15:57:05,144 Stage: Train 0.5 | Epoch: 15 | Iter: 23600 | Total Loss: 0.010659 | Recon Loss: 0.009607 | Commit Loss: 0.002102 | Perplexity: 1203.033406
2025-09-26 15:57:50,654 Stage: Train 0.5 | Epoch: 15 | Iter: 23800 | Total Loss: 0.010552 | Recon Loss: 0.009499 | Commit Loss: 0.002107 | Perplexity: 1207.865747
2025-09-26 15:58:35,965 Stage: Train 0.5 | Epoch: 15 | Iter: 24000 | Total Loss: 0.010521 | Recon Loss: 0.009467 | Commit Loss: 0.002107 | Perplexity: 1210.741949
2025-09-26 15:59:21,418 Stage: Train 0.5 | Epoch: 15 | Iter: 24200 | Total Loss: 0.010611 | Recon Loss: 0.009566 | Commit Loss: 0.002090 | Perplexity: 1206.953950
Trainning Epoch:   5%|▍         | 16/330 [1:48:35<30:19:08, 347.61s/it]2025-09-26 16:00:07,038 Stage: Train 0.5 | Epoch: 16 | Iter: 24400 | Total Loss: 0.010421 | Recon Loss: 0.009373 | Commit Loss: 0.002096 | Perplexity: 1210.960062
2025-09-26 16:00:52,413 Stage: Train 0.5 | Epoch: 16 | Iter: 24600 | Total Loss: 0.010237 | Recon Loss: 0.009210 | Commit Loss: 0.002053 | Perplexity: 1212.892996
2025-09-26 16:01:37,779 Stage: Train 0.5 | Epoch: 16 | Iter: 24800 | Total Loss: 0.010455 | Recon Loss: 0.009407 | Commit Loss: 0.002097 | Perplexity: 1214.734493
2025-09-26 16:02:23,493 Stage: Train 0.5 | Epoch: 16 | Iter: 25000 | Total Loss: 0.010378 | Recon Loss: 0.009329 | Commit Loss: 0.002098 | Perplexity: 1220.264404
2025-09-26 16:03:09,132 Stage: Train 0.5 | Epoch: 16 | Iter: 25200 | Total Loss: 0.010267 | Recon Loss: 0.009232 | Commit Loss: 0.002071 | Perplexity: 1213.684316
2025-09-26 16:03:54,492 Stage: Train 0.5 | Epoch: 16 | Iter: 25400 | Total Loss: 0.010345 | Recon Loss: 0.009303 | Commit Loss: 0.002084 | Perplexity: 1217.932938
2025-09-26 16:04:40,067 Stage: Train 0.5 | Epoch: 16 | Iter: 25600 | Total Loss: 0.010506 | Recon Loss: 0.009333 | Commit Loss: 0.002345 | Perplexity: 1235.433942
2025-09-26 16:05:25,499 Stage: Train 0.5 | Epoch: 16 | Iter: 25800 | Total Loss: 0.010467 | Recon Loss: 0.009449 | Commit Loss: 0.002035 | Perplexity: 1211.705995
Trainning Epoch:   5%|▌         | 17/330 [1:54:21<30:10:29, 347.06s/it]2025-09-26 16:06:11,394 Stage: Train 0.5 | Epoch: 17 | Iter: 26000 | Total Loss: 0.010013 | Recon Loss: 0.008978 | Commit Loss: 0.002069 | Perplexity: 1221.257413
2025-09-26 16:06:56,565 Stage: Train 0.5 | Epoch: 17 | Iter: 26200 | Total Loss: 0.010383 | Recon Loss: 0.009348 | Commit Loss: 0.002070 | Perplexity: 1219.118716
2025-09-26 16:07:42,381 Stage: Train 0.5 | Epoch: 17 | Iter: 26400 | Total Loss: 0.010038 | Recon Loss: 0.009014 | Commit Loss: 0.002049 | Perplexity: 1219.230693
2025-09-26 16:08:27,804 Stage: Train 0.5 | Epoch: 17 | Iter: 26600 | Total Loss: 0.010207 | Recon Loss: 0.009169 | Commit Loss: 0.002076 | Perplexity: 1223.133447
2025-09-26 16:09:13,627 Stage: Train 0.5 | Epoch: 17 | Iter: 26800 | Total Loss: 0.010006 | Recon Loss: 0.008983 | Commit Loss: 0.002047 | Perplexity: 1223.898257
2025-09-26 16:09:59,430 Stage: Train 0.5 | Epoch: 17 | Iter: 27000 | Total Loss: 0.010262 | Recon Loss: 0.009225 | Commit Loss: 0.002073 | Perplexity: 1226.247783
2025-09-26 16:10:44,902 Stage: Train 0.5 | Epoch: 17 | Iter: 27200 | Total Loss: 0.010251 | Recon Loss: 0.009123 | Commit Loss: 0.002256 | Perplexity: 1240.960234
Trainning Epoch:   5%|▌         | 18/330 [2:00:07<30:03:31, 346.83s/it]2025-09-26 16:11:30,481 Stage: Train 0.5 | Epoch: 18 | Iter: 27400 | Total Loss: 0.010343 | Recon Loss: 0.009288 | Commit Loss: 0.002110 | Perplexity: 1228.027726
2025-09-26 16:12:15,755 Stage: Train 0.5 | Epoch: 18 | Iter: 27600 | Total Loss: 0.009864 | Recon Loss: 0.008841 | Commit Loss: 0.002047 | Perplexity: 1229.410703
2025-09-26 16:13:01,219 Stage: Train 0.5 | Epoch: 18 | Iter: 27800 | Total Loss: 0.009998 | Recon Loss: 0.008968 | Commit Loss: 0.002060 | Perplexity: 1229.805834
2025-09-26 16:13:46,832 Stage: Train 0.5 | Epoch: 18 | Iter: 28000 | Total Loss: 0.009835 | Recon Loss: 0.008807 | Commit Loss: 0.002057 | Perplexity: 1230.593541
2025-09-26 16:14:32,445 Stage: Train 0.5 | Epoch: 18 | Iter: 28200 | Total Loss: 0.010039 | Recon Loss: 0.009002 | Commit Loss: 0.002074 | Perplexity: 1232.521382
2025-09-26 16:15:17,445 Stage: Train 0.5 | Epoch: 18 | Iter: 28400 | Total Loss: 0.009980 | Recon Loss: 0.008958 | Commit Loss: 0.002045 | Perplexity: 1230.620744
2025-09-26 16:16:03,149 Stage: Train 0.5 | Epoch: 18 | Iter: 28600 | Total Loss: 0.009859 | Recon Loss: 0.008840 | Commit Loss: 0.002037 | Perplexity: 1234.118172
2025-09-26 16:16:48,673 Stage: Train 0.5 | Epoch: 18 | Iter: 28800 | Total Loss: 0.009849 | Recon Loss: 0.008810 | Commit Loss: 0.002079 | Perplexity: 1235.681968
Trainning Epoch:   6%|▌         | 19/330 [2:05:53<29:55:40, 346.43s/it]2025-09-26 16:17:34,486 Stage: Train 0.5 | Epoch: 19 | Iter: 29000 | Total Loss: 0.009859 | Recon Loss: 0.008834 | Commit Loss: 0.002050 | Perplexity: 1234.759928
2025-09-26 16:18:20,078 Stage: Train 0.5 | Epoch: 19 | Iter: 29200 | Total Loss: 0.009789 | Recon Loss: 0.008754 | Commit Loss: 0.002070 | Perplexity: 1239.105124
2025-09-26 16:19:05,300 Stage: Train 0.5 | Epoch: 19 | Iter: 29400 | Total Loss: 0.009730 | Recon Loss: 0.008704 | Commit Loss: 0.002052 | Perplexity: 1238.785892
2025-09-26 16:19:50,695 Stage: Train 0.5 | Epoch: 19 | Iter: 29600 | Total Loss: 0.009689 | Recon Loss: 0.008658 | Commit Loss: 0.002061 | Perplexity: 1238.405001
2025-09-26 16:20:36,388 Stage: Train 0.5 | Epoch: 19 | Iter: 29800 | Total Loss: 0.009665 | Recon Loss: 0.008637 | Commit Loss: 0.002056 | Perplexity: 1241.103812
2025-09-26 16:21:22,038 Stage: Train 0.5 | Epoch: 19 | Iter: 30000 | Total Loss: 0.009728 | Recon Loss: 0.008713 | Commit Loss: 0.002030 | Perplexity: 1238.528125
2025-09-26 16:22:07,331 Stage: Train 0.5 | Epoch: 19 | Iter: 30200 | Total Loss: 0.009636 | Recon Loss: 0.008606 | Commit Loss: 0.002060 | Perplexity: 1246.841520
Trainning Epoch:   6%|▌         | 20/330 [2:11:39<29:48:51, 346.23s/it]2025-09-26 16:22:53,033 Stage: Train 0.5 | Epoch: 20 | Iter: 30400 | Total Loss: 0.009724 | Recon Loss: 0.008696 | Commit Loss: 0.002054 | Perplexity: 1240.987926
2025-09-26 16:23:38,343 Stage: Train 0.5 | Epoch: 20 | Iter: 30600 | Total Loss: 0.009594 | Recon Loss: 0.008567 | Commit Loss: 0.002054 | Perplexity: 1240.740281
2025-09-26 16:24:23,792 Stage: Train 0.5 | Epoch: 20 | Iter: 30800 | Total Loss: 0.009711 | Recon Loss: 0.008691 | Commit Loss: 0.002041 | Perplexity: 1240.720493
2025-09-26 16:25:09,095 Stage: Train 0.5 | Epoch: 20 | Iter: 31000 | Total Loss: 0.009812 | Recon Loss: 0.008789 | Commit Loss: 0.002046 | Perplexity: 1243.700675
2025-09-26 16:25:54,693 Stage: Train 0.5 | Epoch: 20 | Iter: 31200 | Total Loss: 0.009639 | Recon Loss: 0.008621 | Commit Loss: 0.002035 | Perplexity: 1242.953735
2025-09-26 16:26:40,167 Stage: Train 0.5 | Epoch: 20 | Iter: 31400 | Total Loss: 0.009619 | Recon Loss: 0.008589 | Commit Loss: 0.002060 | Perplexity: 1244.093849
2025-09-26 16:27:25,827 Stage: Train 0.5 | Epoch: 20 | Iter: 31600 | Total Loss: 0.009539 | Recon Loss: 0.008513 | Commit Loss: 0.002053 | Perplexity: 1243.328423
2025-09-26 16:28:11,594 Stage: Train 0.5 | Epoch: 20 | Iter: 31800 | Total Loss: 0.009530 | Recon Loss: 0.008500 | Commit Loss: 0.002060 | Perplexity: 1243.464634
Trainning Epoch:   6%|▋         | 21/330 [2:17:24<29:42:21, 346.09s/it]2025-09-26 16:28:57,399 Stage: Train 0.5 | Epoch: 21 | Iter: 32000 | Total Loss: 0.009541 | Recon Loss: 0.008522 | Commit Loss: 0.002036 | Perplexity: 1246.290457
2025-09-26 16:29:43,028 Stage: Train 0.5 | Epoch: 21 | Iter: 32200 | Total Loss: 0.009409 | Recon Loss: 0.008373 | Commit Loss: 0.002072 | Perplexity: 1248.286340
2025-09-26 16:30:28,691 Stage: Train 0.5 | Epoch: 21 | Iter: 32400 | Total Loss: 0.009597 | Recon Loss: 0.008570 | Commit Loss: 0.002054 | Perplexity: 1249.158381
2025-09-26 16:31:14,450 Stage: Train 0.5 | Epoch: 21 | Iter: 32600 | Total Loss: 0.009610 | Recon Loss: 0.008585 | Commit Loss: 0.002050 | Perplexity: 1245.512197
2025-09-26 16:31:59,909 Stage: Train 0.5 | Epoch: 21 | Iter: 32800 | Total Loss: 0.009446 | Recon Loss: 0.008416 | Commit Loss: 0.002061 | Perplexity: 1244.209324
2025-09-26 16:32:45,500 Stage: Train 0.5 | Epoch: 21 | Iter: 33000 | Total Loss: 0.009488 | Recon Loss: 0.008467 | Commit Loss: 0.002041 | Perplexity: 1244.809168
2025-09-26 16:33:31,204 Stage: Train 0.5 | Epoch: 21 | Iter: 33200 | Total Loss: 0.009403 | Recon Loss: 0.008376 | Commit Loss: 0.002054 | Perplexity: 1247.226439
2025-09-26 16:34:16,522 Stage: Train 0.5 | Epoch: 21 | Iter: 33400 | Total Loss: 0.009580 | Recon Loss: 0.008556 | Commit Loss: 0.002048 | Perplexity: 1248.179745
Trainning Epoch:   7%|▋         | 22/330 [2:23:11<29:37:14, 346.22s/it]2025-09-26 16:35:02,562 Stage: Train 0.5 | Epoch: 22 | Iter: 33600 | Total Loss: 0.009331 | Recon Loss: 0.008308 | Commit Loss: 0.002045 | Perplexity: 1247.071007
2025-09-26 16:35:48,371 Stage: Train 0.5 | Epoch: 22 | Iter: 33800 | Total Loss: 0.009302 | Recon Loss: 0.008280 | Commit Loss: 0.002044 | Perplexity: 1246.843156
2025-09-26 16:36:34,188 Stage: Train 0.5 | Epoch: 22 | Iter: 34000 | Total Loss: 0.009483 | Recon Loss: 0.008455 | Commit Loss: 0.002057 | Perplexity: 1245.718073
2025-09-26 16:37:19,973 Stage: Train 0.5 | Epoch: 22 | Iter: 34200 | Total Loss: 0.009310 | Recon Loss: 0.008286 | Commit Loss: 0.002047 | Perplexity: 1247.048351
2025-09-26 16:38:05,590 Stage: Train 0.5 | Epoch: 22 | Iter: 34400 | Total Loss: 0.009227 | Recon Loss: 0.008191 | Commit Loss: 0.002074 | Perplexity: 1250.723538
2025-09-26 16:38:51,096 Stage: Train 0.5 | Epoch: 22 | Iter: 34600 | Total Loss: 0.009187 | Recon Loss: 0.008158 | Commit Loss: 0.002057 | Perplexity: 1248.862053
2025-09-26 16:39:36,741 Stage: Train 0.5 | Epoch: 22 | Iter: 34800 | Total Loss: 0.009405 | Recon Loss: 0.008367 | Commit Loss: 0.002076 | Perplexity: 1252.755591
Trainning Epoch:   7%|▋         | 23/330 [2:28:58<29:33:11, 346.55s/it]2025-09-26 16:40:20,798 Stage: Train 0.5 | Epoch: 23 | Iter: 35000 | Total Loss: 0.009276 | Recon Loss: 0.008247 | Commit Loss: 0.002059 | Perplexity: 1247.465058
2025-09-26 16:41:06,236 Stage: Train 0.5 | Epoch: 23 | Iter: 35200 | Total Loss: 0.009482 | Recon Loss: 0.008459 | Commit Loss: 0.002046 | Perplexity: 1248.664772
2025-09-26 16:41:52,127 Stage: Train 0.5 | Epoch: 23 | Iter: 35400 | Total Loss: 0.009075 | Recon Loss: 0.008039 | Commit Loss: 0.002072 | Perplexity: 1254.887975
2025-09-26 16:42:37,481 Stage: Train 0.5 | Epoch: 23 | Iter: 35600 | Total Loss: 0.009331 | Recon Loss: 0.008299 | Commit Loss: 0.002063 | Perplexity: 1251.524267
2025-09-26 16:43:23,131 Stage: Train 0.5 | Epoch: 23 | Iter: 35800 | Total Loss: 0.009262 | Recon Loss: 0.008230 | Commit Loss: 0.002066 | Perplexity: 1250.227611
2025-09-26 16:44:08,842 Stage: Train 0.5 | Epoch: 23 | Iter: 36000 | Total Loss: 0.009188 | Recon Loss: 0.008151 | Commit Loss: 0.002074 | Perplexity: 1247.715746
2025-09-26 16:44:54,617 Stage: Train 0.5 | Epoch: 23 | Iter: 36200 | Total Loss: 0.009200 | Recon Loss: 0.008170 | Commit Loss: 0.002058 | Perplexity: 1249.939081
2025-09-26 16:45:40,392 Stage: Train 0.5 | Epoch: 23 | Iter: 36400 | Total Loss: 0.009051 | Recon Loss: 0.008012 | Commit Loss: 0.002079 | Perplexity: 1251.038420
Trainning Epoch:   7%|▋         | 24/330 [2:34:43<29:25:20, 346.15s/it]2025-09-26 16:46:26,253 Stage: Train 0.5 | Epoch: 24 | Iter: 36600 | Total Loss: 0.009076 | Recon Loss: 0.008040 | Commit Loss: 0.002071 | Perplexity: 1253.211644
2025-09-26 16:47:12,013 Stage: Train 0.5 | Epoch: 24 | Iter: 36800 | Total Loss: 0.009281 | Recon Loss: 0.008244 | Commit Loss: 0.002074 | Perplexity: 1250.845483
2025-09-26 16:47:57,813 Stage: Train 0.5 | Epoch: 24 | Iter: 37000 | Total Loss: 0.009080 | Recon Loss: 0.008040 | Commit Loss: 0.002080 | Perplexity: 1252.825684
2025-09-26 16:48:43,182 Stage: Train 0.5 | Epoch: 24 | Iter: 37200 | Total Loss: 0.009196 | Recon Loss: 0.008155 | Commit Loss: 0.002083 | Perplexity: 1251.359196
2025-09-26 16:49:28,783 Stage: Train 0.5 | Epoch: 24 | Iter: 37400 | Total Loss: 0.009158 | Recon Loss: 0.008125 | Commit Loss: 0.002066 | Perplexity: 1250.227316
2025-09-26 16:50:14,426 Stage: Train 0.5 | Epoch: 24 | Iter: 37600 | Total Loss: 0.009039 | Recon Loss: 0.008010 | Commit Loss: 0.002058 | Perplexity: 1252.114464
2025-09-26 16:51:00,026 Stage: Train 0.5 | Epoch: 24 | Iter: 37800 | Total Loss: 0.009105 | Recon Loss: 0.008066 | Commit Loss: 0.002078 | Perplexity: 1253.794996
Trainning Epoch:   8%|▊         | 25/330 [2:40:30<29:20:35, 346.35s/it]2025-09-26 16:51:45,916 Stage: Train 0.5 | Epoch: 25 | Iter: 38000 | Total Loss: 0.008974 | Recon Loss: 0.007942 | Commit Loss: 0.002064 | Perplexity: 1249.450107
2025-09-26 16:52:31,187 Stage: Train 0.5 | Epoch: 25 | Iter: 38200 | Total Loss: 0.009089 | Recon Loss: 0.008054 | Commit Loss: 0.002069 | Perplexity: 1253.073430
2025-09-26 16:53:16,926 Stage: Train 0.5 | Epoch: 25 | Iter: 38400 | Total Loss: 0.009020 | Recon Loss: 0.007977 | Commit Loss: 0.002086 | Perplexity: 1254.252639
2025-09-26 16:54:02,514 Stage: Train 0.5 | Epoch: 25 | Iter: 38600 | Total Loss: 0.008862 | Recon Loss: 0.007829 | Commit Loss: 0.002067 | Perplexity: 1252.378475
2025-09-26 16:54:48,207 Stage: Train 0.5 | Epoch: 25 | Iter: 38800 | Total Loss: 0.009069 | Recon Loss: 0.008028 | Commit Loss: 0.002082 | Perplexity: 1254.289880
2025-09-26 16:55:33,213 Stage: Train 0.5 | Epoch: 25 | Iter: 39000 | Total Loss: 0.009083 | Recon Loss: 0.008050 | Commit Loss: 0.002066 | Perplexity: 1254.610925
2025-09-26 16:56:18,806 Stage: Train 0.5 | Epoch: 25 | Iter: 39200 | Total Loss: 0.008983 | Recon Loss: 0.007943 | Commit Loss: 0.002080 | Perplexity: 1258.368672
2025-09-26 16:57:04,492 Stage: Train 0.5 | Epoch: 25 | Iter: 39400 | Total Loss: 0.008992 | Recon Loss: 0.007955 | Commit Loss: 0.002074 | Perplexity: 1256.703886
Trainning Epoch:   8%|▊         | 26/330 [2:46:16<29:14:13, 346.23s/it]2025-09-26 16:57:50,316 Stage: Train 0.5 | Epoch: 26 | Iter: 39600 | Total Loss: 0.009047 | Recon Loss: 0.008021 | Commit Loss: 0.002053 | Perplexity: 1253.123982
2025-09-26 16:58:35,776 Stage: Train 0.5 | Epoch: 26 | Iter: 39800 | Total Loss: 0.008906 | Recon Loss: 0.007878 | Commit Loss: 0.002056 | Perplexity: 1254.698061
2025-09-26 16:59:21,237 Stage: Train 0.5 | Epoch: 26 | Iter: 40000 | Total Loss: 0.008873 | Recon Loss: 0.007837 | Commit Loss: 0.002073 | Perplexity: 1255.825372
2025-09-26 16:59:21,237 Saving model at iteration 40000
2025-09-26 16:59:21,450 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000
2025-09-26 16:59:21,774 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/model.safetensors
2025-09-26 16:59:22,235 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/optimizer.bin
2025-09-26 16:59:22,236 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/scheduler.bin
2025-09-26 16:59:22,236 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/sampler.bin
2025-09-26 16:59:22,237 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/random_states_0.pkl
2025-09-26 17:00:08,026 Stage: Train 0.5 | Epoch: 26 | Iter: 40200 | Total Loss: 0.008945 | Recon Loss: 0.007911 | Commit Loss: 0.002069 | Perplexity: 1256.293679
2025-09-26 17:00:53,796 Stage: Train 0.5 | Epoch: 26 | Iter: 40400 | Total Loss: 0.009053 | Recon Loss: 0.008014 | Commit Loss: 0.002078 | Perplexity: 1256.627408
2025-09-26 17:01:39,565 Stage: Train 0.5 | Epoch: 26 | Iter: 40600 | Total Loss: 0.008838 | Recon Loss: 0.007800 | Commit Loss: 0.002075 | Perplexity: 1257.936189
2025-09-26 17:02:25,165 Stage: Train 0.5 | Epoch: 26 | Iter: 40800 | Total Loss: 0.008989 | Recon Loss: 0.007954 | Commit Loss: 0.002070 | Perplexity: 1260.028614
2025-09-26 17:03:10,866 Stage: Train 0.5 | Epoch: 26 | Iter: 41000 | Total Loss: 0.008792 | Recon Loss: 0.007759 | Commit Loss: 0.002067 | Perplexity: 1260.830193
Trainning Epoch:   8%|▊         | 27/330 [2:52:04<29:10:59, 346.73s/it]2025-09-26 17:03:56,871 Stage: Train 0.5 | Epoch: 27 | Iter: 41200 | Total Loss: 0.008769 | Recon Loss: 0.007733 | Commit Loss: 0.002072 | Perplexity: 1261.285821
2025-09-26 17:04:42,599 Stage: Train 0.5 | Epoch: 27 | Iter: 41400 | Total Loss: 0.008810 | Recon Loss: 0.007779 | Commit Loss: 0.002064 | Perplexity: 1262.402666
2025-09-26 17:05:28,106 Stage: Train 0.5 | Epoch: 27 | Iter: 41600 | Total Loss: 0.008873 | Recon Loss: 0.007832 | Commit Loss: 0.002083 | Perplexity: 1264.564847
2025-09-26 17:06:13,909 Stage: Train 0.5 | Epoch: 27 | Iter: 41800 | Total Loss: 0.008813 | Recon Loss: 0.007780 | Commit Loss: 0.002065 | Perplexity: 1264.773317
2025-09-26 17:06:59,819 Stage: Train 0.5 | Epoch: 27 | Iter: 42000 | Total Loss: 0.008848 | Recon Loss: 0.007820 | Commit Loss: 0.002055 | Perplexity: 1264.083195
2025-09-26 17:07:45,401 Stage: Train 0.5 | Epoch: 27 | Iter: 42200 | Total Loss: 0.008743 | Recon Loss: 0.007709 | Commit Loss: 0.002069 | Perplexity: 1269.743226
2025-09-26 17:08:30,991 Stage: Train 0.5 | Epoch: 27 | Iter: 42400 | Total Loss: 0.008886 | Recon Loss: 0.007856 | Commit Loss: 0.002061 | Perplexity: 1274.953373
Trainning Epoch:   8%|▊         | 28/330 [2:57:51<29:05:41, 346.83s/it]2025-09-26 17:09:16,664 Stage: Train 0.5 | Epoch: 28 | Iter: 42600 | Total Loss: 0.008614 | Recon Loss: 0.007582 | Commit Loss: 0.002064 | Perplexity: 1275.552076
2025-09-26 17:10:02,374 Stage: Train 0.5 | Epoch: 28 | Iter: 42800 | Total Loss: 0.008760 | Recon Loss: 0.007744 | Commit Loss: 0.002031 | Perplexity: 1279.879506
2025-09-26 17:10:48,046 Stage: Train 0.5 | Epoch: 28 | Iter: 43000 | Total Loss: 0.008697 | Recon Loss: 0.007670 | Commit Loss: 0.002055 | Perplexity: 1284.462209
2025-09-26 17:11:33,859 Stage: Train 0.5 | Epoch: 28 | Iter: 43200 | Total Loss: 0.008667 | Recon Loss: 0.007652 | Commit Loss: 0.002030 | Perplexity: 1288.653123
2025-09-26 17:12:19,257 Stage: Train 0.5 | Epoch: 28 | Iter: 43400 | Total Loss: 0.008652 | Recon Loss: 0.007642 | Commit Loss: 0.002020 | Perplexity: 1300.445848
2025-09-26 17:13:05,053 Stage: Train 0.5 | Epoch: 28 | Iter: 43600 | Total Loss: 0.008599 | Recon Loss: 0.007579 | Commit Loss: 0.002041 | Perplexity: 1313.161885
2025-09-26 17:13:50,798 Stage: Train 0.5 | Epoch: 28 | Iter: 43800 | Total Loss: 0.008575 | Recon Loss: 0.007576 | Commit Loss: 0.001997 | Perplexity: 1315.117400
2025-09-26 17:14:36,477 Stage: Train 0.5 | Epoch: 28 | Iter: 44000 | Total Loss: 0.008484 | Recon Loss: 0.007495 | Commit Loss: 0.001978 | Perplexity: 1315.943233
Trainning Epoch:   9%|▉         | 29/330 [3:03:38<29:00:31, 346.95s/it]2025-09-26 17:15:22,405 Stage: Train 0.5 | Epoch: 29 | Iter: 44200 | Total Loss: 0.008514 | Recon Loss: 0.007519 | Commit Loss: 0.001989 | Perplexity: 1320.207000
2025-09-26 17:16:07,816 Stage: Train 0.5 | Epoch: 29 | Iter: 44400 | Total Loss: 0.008580 | Recon Loss: 0.007594 | Commit Loss: 0.001972 | Perplexity: 1323.195635
2025-09-26 17:16:53,399 Stage: Train 0.5 | Epoch: 29 | Iter: 44600 | Total Loss: 0.008643 | Recon Loss: 0.007660 | Commit Loss: 0.001966 | Perplexity: 1319.194619
2025-09-26 17:17:39,239 Stage: Train 0.5 | Epoch: 29 | Iter: 44800 | Total Loss: 0.008538 | Recon Loss: 0.007559 | Commit Loss: 0.001959 | Perplexity: 1323.388492
2025-09-26 17:18:25,116 Stage: Train 0.5 | Epoch: 29 | Iter: 45000 | Total Loss: 0.008405 | Recon Loss: 0.007439 | Commit Loss: 0.001933 | Perplexity: 1323.651187
2025-09-26 17:19:10,470 Stage: Train 0.5 | Epoch: 29 | Iter: 45200 | Total Loss: 0.008522 | Recon Loss: 0.007538 | Commit Loss: 0.001969 | Perplexity: 1326.352307
2025-09-26 17:19:56,257 Stage: Train 0.5 | Epoch: 29 | Iter: 45400 | Total Loss: 0.008448 | Recon Loss: 0.007468 | Commit Loss: 0.001960 | Perplexity: 1324.176192
Trainning Epoch:   9%|▉         | 30/330 [3:09:25<28:54:41, 346.94s/it]2025-09-26 17:20:42,041 Stage: Train 0.5 | Epoch: 30 | Iter: 45600 | Total Loss: 0.008394 | Recon Loss: 0.007427 | Commit Loss: 0.001933 | Perplexity: 1320.882707
2025-09-26 17:21:27,500 Stage: Train 0.5 | Epoch: 30 | Iter: 45800 | Total Loss: 0.008564 | Recon Loss: 0.007602 | Commit Loss: 0.001923 | Perplexity: 1321.740104
2025-09-26 17:22:12,979 Stage: Train 0.5 | Epoch: 30 | Iter: 46000 | Total Loss: 0.008375 | Recon Loss: 0.007406 | Commit Loss: 0.001937 | Perplexity: 1324.560265
2025-09-26 17:22:58,391 Stage: Train 0.5 | Epoch: 30 | Iter: 46200 | Total Loss: 0.008405 | Recon Loss: 0.007430 | Commit Loss: 0.001948 | Perplexity: 1328.180589
2025-09-26 17:23:44,083 Stage: Train 0.5 | Epoch: 30 | Iter: 46400 | Total Loss: 0.008296 | Recon Loss: 0.007321 | Commit Loss: 0.001952 | Perplexity: 1326.517952
2025-09-26 17:24:29,609 Stage: Train 0.5 | Epoch: 30 | Iter: 46600 | Total Loss: 0.008327 | Recon Loss: 0.007351 | Commit Loss: 0.001953 | Perplexity: 1325.118918
2025-09-26 17:25:13,479 Stage: Train 0.5 | Epoch: 30 | Iter: 46800 | Total Loss: 0.008393 | Recon Loss: 0.007422 | Commit Loss: 0.001942 | Perplexity: 1327.056344
2025-09-26 17:25:58,921 Stage: Train 0.5 | Epoch: 30 | Iter: 47000 | Total Loss: 0.008396 | Recon Loss: 0.007418 | Commit Loss: 0.001956 | Perplexity: 1328.935737
Trainning Epoch:   9%|▉         | 31/330 [3:15:09<28:44:48, 346.12s/it]2025-09-26 17:26:44,702 Stage: Train 0.5 | Epoch: 31 | Iter: 47200 | Total Loss: 0.008328 | Recon Loss: 0.007358 | Commit Loss: 0.001940 | Perplexity: 1326.195167
2025-09-26 17:27:30,333 Stage: Train 0.5 | Epoch: 31 | Iter: 47400 | Total Loss: 0.008371 | Recon Loss: 0.007411 | Commit Loss: 0.001920 | Perplexity: 1325.459946
2025-09-26 17:28:15,979 Stage: Train 0.5 | Epoch: 31 | Iter: 47600 | Total Loss: 0.008426 | Recon Loss: 0.007451 | Commit Loss: 0.001950 | Perplexity: 1330.243533
2025-09-26 17:29:01,343 Stage: Train 0.5 | Epoch: 31 | Iter: 47800 | Total Loss: 0.008236 | Recon Loss: 0.007279 | Commit Loss: 0.001913 | Perplexity: 1328.813187
2025-09-26 17:29:46,997 Stage: Train 0.5 | Epoch: 31 | Iter: 48000 | Total Loss: 0.008406 | Recon Loss: 0.007442 | Commit Loss: 0.001927 | Perplexity: 1328.295009
2025-09-26 17:30:32,702 Stage: Train 0.5 | Epoch: 31 | Iter: 48200 | Total Loss: 0.008095 | Recon Loss: 0.007139 | Commit Loss: 0.001912 | Perplexity: 1330.011835
2025-09-26 17:31:18,395 Stage: Train 0.5 | Epoch: 31 | Iter: 48400 | Total Loss: 0.008278 | Recon Loss: 0.007311 | Commit Loss: 0.001934 | Perplexity: 1328.411562
2025-09-26 17:32:04,260 Stage: Train 0.5 | Epoch: 31 | Iter: 48600 | Total Loss: 0.008242 | Recon Loss: 0.007274 | Commit Loss: 0.001936 | Perplexity: 1332.782889
Trainning Epoch:  10%|▉         | 32/330 [3:20:56<28:40:05, 346.33s/it]2025-09-26 17:32:49,781 Stage: Train 0.5 | Epoch: 32 | Iter: 48800 | Total Loss: 0.008316 | Recon Loss: 0.007353 | Commit Loss: 0.001926 | Perplexity: 1331.631468
2025-09-26 17:33:35,417 Stage: Train 0.5 | Epoch: 32 | Iter: 49000 | Total Loss: 0.008247 | Recon Loss: 0.007282 | Commit Loss: 0.001931 | Perplexity: 1332.667804
2025-09-26 17:34:21,257 Stage: Train 0.5 | Epoch: 32 | Iter: 49200 | Total Loss: 0.008103 | Recon Loss: 0.007152 | Commit Loss: 0.001901 | Perplexity: 1329.770909
2025-09-26 17:35:06,903 Stage: Train 0.5 | Epoch: 32 | Iter: 49400 | Total Loss: 0.008189 | Recon Loss: 0.007221 | Commit Loss: 0.001936 | Perplexity: 1335.303522
2025-09-26 17:35:52,203 Stage: Train 0.5 | Epoch: 32 | Iter: 49600 | Total Loss: 0.008269 | Recon Loss: 0.007312 | Commit Loss: 0.001914 | Perplexity: 1332.038177
2025-09-26 17:36:37,671 Stage: Train 0.5 | Epoch: 32 | Iter: 49800 | Total Loss: 0.008077 | Recon Loss: 0.007119 | Commit Loss: 0.001915 | Perplexity: 1335.010101
2025-09-26 17:37:23,471 Stage: Train 0.5 | Epoch: 32 | Iter: 50000 | Total Loss: 0.008288 | Recon Loss: 0.007337 | Commit Loss: 0.001902 | Perplexity: 1332.124106
Trainning Epoch:  10%|█         | 33/330 [3:26:43<28:34:18, 346.32s/it]2025-09-26 17:38:09,166 Stage: Train 0.5 | Epoch: 33 | Iter: 50200 | Total Loss: 0.008201 | Recon Loss: 0.007242 | Commit Loss: 0.001919 | Perplexity: 1334.497181
2025-09-26 17:38:55,049 Stage: Train 0.5 | Epoch: 33 | Iter: 50400 | Total Loss: 0.008196 | Recon Loss: 0.007240 | Commit Loss: 0.001912 | Perplexity: 1334.126155
2025-09-26 17:39:40,210 Stage: Train 0.5 | Epoch: 33 | Iter: 50600 | Total Loss: 0.008147 | Recon Loss: 0.007185 | Commit Loss: 0.001924 | Perplexity: 1336.231311
2025-09-26 17:40:25,803 Stage: Train 0.5 | Epoch: 33 | Iter: 50800 | Total Loss: 0.008103 | Recon Loss: 0.007156 | Commit Loss: 0.001894 | Perplexity: 1331.713488
2025-09-26 17:41:11,503 Stage: Train 0.5 | Epoch: 33 | Iter: 51000 | Total Loss: 0.008126 | Recon Loss: 0.007167 | Commit Loss: 0.001918 | Perplexity: 1334.523270
2025-09-26 17:41:57,302 Stage: Train 0.5 | Epoch: 33 | Iter: 51200 | Total Loss: 0.008189 | Recon Loss: 0.007231 | Commit Loss: 0.001915 | Perplexity: 1334.852280
2025-09-26 17:42:42,297 Stage: Train 0.5 | Epoch: 33 | Iter: 51400 | Total Loss: 0.008072 | Recon Loss: 0.007120 | Commit Loss: 0.001903 | Perplexity: 1335.339912
2025-09-26 17:43:28,045 Stage: Train 0.5 | Epoch: 33 | Iter: 51600 | Total Loss: 0.008068 | Recon Loss: 0.007119 | Commit Loss: 0.001898 | Perplexity: 1333.081395
Trainning Epoch:  10%|█         | 34/330 [3:32:29<28:28:12, 346.26s/it]2025-09-26 17:44:13,990 Stage: Train 0.5 | Epoch: 34 | Iter: 51800 | Total Loss: 0.008090 | Recon Loss: 0.007137 | Commit Loss: 0.001907 | Perplexity: 1333.701754
2025-09-26 17:44:59,761 Stage: Train 0.5 | Epoch: 34 | Iter: 52000 | Total Loss: 0.007924 | Recon Loss: 0.006967 | Commit Loss: 0.001914 | Perplexity: 1335.585940
2025-09-26 17:45:45,602 Stage: Train 0.5 | Epoch: 34 | Iter: 52200 | Total Loss: 0.008065 | Recon Loss: 0.007110 | Commit Loss: 0.001909 | Perplexity: 1335.194224
2025-09-26 17:46:31,165 Stage: Train 0.5 | Epoch: 34 | Iter: 52400 | Total Loss: 0.008063 | Recon Loss: 0.007111 | Commit Loss: 0.001903 | Perplexity: 1334.741885
2025-09-26 17:47:16,880 Stage: Train 0.5 | Epoch: 34 | Iter: 52600 | Total Loss: 0.007981 | Recon Loss: 0.007026 | Commit Loss: 0.001912 | Perplexity: 1337.844145
2025-09-26 17:48:02,665 Stage: Train 0.5 | Epoch: 34 | Iter: 52800 | Total Loss: 0.007992 | Recon Loss: 0.007029 | Commit Loss: 0.001925 | Perplexity: 1338.413612
2025-09-26 17:48:48,536 Stage: Train 0.5 | Epoch: 34 | Iter: 53000 | Total Loss: 0.008197 | Recon Loss: 0.007238 | Commit Loss: 0.001918 | Perplexity: 1336.589022
Trainning Epoch:  11%|█         | 35/330 [3:38:16<28:24:20, 346.65s/it]2025-09-26 17:49:34,211 Stage: Train 0.5 | Epoch: 35 | Iter: 53200 | Total Loss: 0.008112 | Recon Loss: 0.007157 | Commit Loss: 0.001909 | Perplexity: 1336.394477
2025-09-26 17:50:19,929 Stage: Train 0.5 | Epoch: 35 | Iter: 53400 | Total Loss: 0.008047 | Recon Loss: 0.007096 | Commit Loss: 0.001903 | Perplexity: 1336.044604
2025-09-26 17:51:05,577 Stage: Train 0.5 | Epoch: 35 | Iter: 53600 | Total Loss: 0.007892 | Recon Loss: 0.006947 | Commit Loss: 0.001891 | Perplexity: 1335.629141
2025-09-26 17:51:51,284 Stage: Train 0.5 | Epoch: 35 | Iter: 53800 | Total Loss: 0.007980 | Recon Loss: 0.007020 | Commit Loss: 0.001920 | Perplexity: 1337.081956
2025-09-26 17:52:36,721 Stage: Train 0.5 | Epoch: 35 | Iter: 54000 | Total Loss: 0.007973 | Recon Loss: 0.007025 | Commit Loss: 0.001895 | Perplexity: 1337.248801
2025-09-26 17:53:22,468 Stage: Train 0.5 | Epoch: 35 | Iter: 54200 | Total Loss: 0.007923 | Recon Loss: 0.006959 | Commit Loss: 0.001928 | Perplexity: 1340.565219
2025-09-26 17:54:08,154 Stage: Train 0.5 | Epoch: 35 | Iter: 54400 | Total Loss: 0.008032 | Recon Loss: 0.007080 | Commit Loss: 0.001904 | Perplexity: 1335.154660
2025-09-26 17:54:53,809 Stage: Train 0.5 | Epoch: 35 | Iter: 54600 | Total Loss: 0.007961 | Recon Loss: 0.007005 | Commit Loss: 0.001911 | Perplexity: 1337.656768
Trainning Epoch:  11%|█         | 36/330 [3:44:03<28:19:06, 346.76s/it]2025-09-26 17:55:39,645 Stage: Train 0.5 | Epoch: 36 | Iter: 54800 | Total Loss: 0.007962 | Recon Loss: 0.007007 | Commit Loss: 0.001912 | Perplexity: 1336.750632
2025-09-26 17:56:25,250 Stage: Train 0.5 | Epoch: 36 | Iter: 55000 | Total Loss: 0.007917 | Recon Loss: 0.006960 | Commit Loss: 0.001915 | Perplexity: 1338.140867
2025-09-26 17:57:10,776 Stage: Train 0.5 | Epoch: 36 | Iter: 55200 | Total Loss: 0.007912 | Recon Loss: 0.006960 | Commit Loss: 0.001905 | Perplexity: 1336.158804
2025-09-26 17:57:56,641 Stage: Train 0.5 | Epoch: 36 | Iter: 55400 | Total Loss: 0.007970 | Recon Loss: 0.007005 | Commit Loss: 0.001929 | Perplexity: 1338.611824
2025-09-26 17:58:42,456 Stage: Train 0.5 | Epoch: 36 | Iter: 55600 | Total Loss: 0.007923 | Recon Loss: 0.006976 | Commit Loss: 0.001895 | Perplexity: 1335.554645
2025-09-26 17:59:27,795 Stage: Train 0.5 | Epoch: 36 | Iter: 55800 | Total Loss: 0.007815 | Recon Loss: 0.006856 | Commit Loss: 0.001919 | Perplexity: 1339.513446
2025-09-26 18:00:13,409 Stage: Train 0.5 | Epoch: 36 | Iter: 56000 | Total Loss: 0.007939 | Recon Loss: 0.006982 | Commit Loss: 0.001914 | Perplexity: 1339.479423
2025-09-26 18:00:59,166 Stage: Train 0.5 | Epoch: 36 | Iter: 56200 | Total Loss: 0.007867 | Recon Loss: 0.006916 | Commit Loss: 0.001902 | Perplexity: 1338.546498
Trainning Epoch:  11%|█         | 37/330 [3:49:50<28:13:23, 346.77s/it]2025-09-26 18:01:45,340 Stage: Train 0.5 | Epoch: 37 | Iter: 56400 | Total Loss: 0.007778 | Recon Loss: 0.006824 | Commit Loss: 0.001908 | Perplexity: 1335.843652
2025-09-26 18:02:31,029 Stage: Train 0.5 | Epoch: 37 | Iter: 56600 | Total Loss: 0.008072 | Recon Loss: 0.007109 | Commit Loss: 0.001928 | Perplexity: 1340.238986
2025-09-26 18:03:16,425 Stage: Train 0.5 | Epoch: 37 | Iter: 56800 | Total Loss: 0.007762 | Recon Loss: 0.006817 | Commit Loss: 0.001890 | Perplexity: 1335.886481
2025-09-26 18:04:01,958 Stage: Train 0.5 | Epoch: 37 | Iter: 57000 | Total Loss: 0.007947 | Recon Loss: 0.006993 | Commit Loss: 0.001909 | Perplexity: 1338.991345
2025-09-26 18:04:47,560 Stage: Train 0.5 | Epoch: 37 | Iter: 57200 | Total Loss: 0.007894 | Recon Loss: 0.006935 | Commit Loss: 0.001916 | Perplexity: 1338.453375
2025-09-26 18:05:33,244 Stage: Train 0.5 | Epoch: 37 | Iter: 57400 | Total Loss: 0.007856 | Recon Loss: 0.006898 | Commit Loss: 0.001917 | Perplexity: 1341.679182
2025-09-26 18:06:18,587 Stage: Train 0.5 | Epoch: 37 | Iter: 57600 | Total Loss: 0.007771 | Recon Loss: 0.006818 | Commit Loss: 0.001906 | Perplexity: 1339.011998
Trainning Epoch:  12%|█▏        | 38/330 [3:55:37<28:07:21, 346.72s/it]2025-09-26 18:07:04,388 Stage: Train 0.5 | Epoch: 38 | Iter: 57800 | Total Loss: 0.007876 | Recon Loss: 0.006924 | Commit Loss: 0.001906 | Perplexity: 1339.892697
2025-09-26 18:07:50,048 Stage: Train 0.5 | Epoch: 38 | Iter: 58000 | Total Loss: 0.007848 | Recon Loss: 0.006897 | Commit Loss: 0.001904 | Perplexity: 1340.109065
2025-09-26 18:08:35,784 Stage: Train 0.5 | Epoch: 38 | Iter: 58200 | Total Loss: 0.007701 | Recon Loss: 0.006747 | Commit Loss: 0.001908 | Perplexity: 1339.937653
2025-09-26 18:09:20,528 Stage: Train 0.5 | Epoch: 38 | Iter: 58400 | Total Loss: 0.007822 | Recon Loss: 0.006867 | Commit Loss: 0.001911 | Perplexity: 1340.201669
2025-09-26 18:10:06,069 Stage: Train 0.5 | Epoch: 38 | Iter: 58600 | Total Loss: 0.007861 | Recon Loss: 0.006896 | Commit Loss: 0.001929 | Perplexity: 1343.351296
2025-09-26 18:10:51,825 Stage: Train 0.5 | Epoch: 38 | Iter: 58800 | Total Loss: 0.007861 | Recon Loss: 0.006895 | Commit Loss: 0.001930 | Perplexity: 1344.096301
2025-09-26 18:11:37,737 Stage: Train 0.5 | Epoch: 38 | Iter: 59000 | Total Loss: 0.007749 | Recon Loss: 0.006799 | Commit Loss: 0.001899 | Perplexity: 1336.965266
2025-09-26 18:12:23,468 Stage: Train 0.5 | Epoch: 38 | Iter: 59200 | Total Loss: 0.007644 | Recon Loss: 0.006691 | Commit Loss: 0.001907 | Perplexity: 1339.612732
Trainning Epoch:  12%|█▏        | 39/330 [4:01:23<28:00:57, 346.59s/it]2025-09-26 18:13:08,898 Stage: Train 0.5 | Epoch: 39 | Iter: 59400 | Total Loss: 0.007775 | Recon Loss: 0.006818 | Commit Loss: 0.001914 | Perplexity: 1340.373331
2025-09-26 18:13:54,546 Stage: Train 0.5 | Epoch: 39 | Iter: 59600 | Total Loss: 0.007696 | Recon Loss: 0.006745 | Commit Loss: 0.001901 | Perplexity: 1339.850625
2025-09-26 18:14:40,176 Stage: Train 0.5 | Epoch: 39 | Iter: 59800 | Total Loss: 0.007809 | Recon Loss: 0.006851 | Commit Loss: 0.001916 | Perplexity: 1341.494233
2025-09-26 18:15:25,792 Stage: Train 0.5 | Epoch: 39 | Iter: 60000 | Total Loss: 0.007702 | Recon Loss: 0.006741 | Commit Loss: 0.001923 | Perplexity: 1340.867554
2025-09-26 18:15:25,793 Saving model at iteration 60000
2025-09-26 18:15:25,995 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000
2025-09-26 18:15:26,274 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/model.safetensors
2025-09-26 18:15:26,655 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/optimizer.bin
2025-09-26 18:15:26,655 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/scheduler.bin
2025-09-26 18:15:26,655 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/sampler.bin
2025-09-26 18:15:26,656 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/random_states_0.pkl
2025-09-26 18:16:12,214 Stage: Train 0.5 | Epoch: 39 | Iter: 60200 | Total Loss: 0.007844 | Recon Loss: 0.006886 | Commit Loss: 0.001915 | Perplexity: 1342.652114
2025-09-26 18:16:58,030 Stage: Train 0.5 | Epoch: 39 | Iter: 60400 | Total Loss: 0.007699 | Recon Loss: 0.006746 | Commit Loss: 0.001905 | Perplexity: 1338.753136
2025-09-26 18:17:43,856 Stage: Train 0.5 | Epoch: 39 | Iter: 60600 | Total Loss: 0.007829 | Recon Loss: 0.006872 | Commit Loss: 0.001913 | Perplexity: 1338.567258
Trainning Epoch:  12%|█▏        | 40/330 [4:07:10<27:56:31, 346.87s/it]2025-09-26 18:18:29,612 Stage: Train 0.5 | Epoch: 40 | Iter: 60800 | Total Loss: 0.007612 | Recon Loss: 0.006643 | Commit Loss: 0.001938 | Perplexity: 1344.435675
2025-09-26 18:19:15,337 Stage: Train 0.5 | Epoch: 40 | Iter: 61000 | Total Loss: 0.007671 | Recon Loss: 0.006714 | Commit Loss: 0.001915 | Perplexity: 1340.792986
2025-09-26 18:20:00,733 Stage: Train 0.5 | Epoch: 40 | Iter: 61200 | Total Loss: 0.007827 | Recon Loss: 0.006865 | Commit Loss: 0.001924 | Perplexity: 1346.191820
2025-09-26 18:20:46,434 Stage: Train 0.5 | Epoch: 40 | Iter: 61400 | Total Loss: 0.007673 | Recon Loss: 0.006713 | Commit Loss: 0.001919 | Perplexity: 1341.144097
2025-09-26 18:21:32,231 Stage: Train 0.5 | Epoch: 40 | Iter: 61600 | Total Loss: 0.007770 | Recon Loss: 0.006814 | Commit Loss: 0.001912 | Perplexity: 1340.485087
2025-09-26 18:22:18,065 Stage: Train 0.5 | Epoch: 40 | Iter: 61800 | Total Loss: 0.007641 | Recon Loss: 0.006685 | Commit Loss: 0.001912 | Perplexity: 1340.563939
2025-09-26 18:23:03,653 Stage: Train 0.5 | Epoch: 40 | Iter: 62000 | Total Loss: 0.007654 | Recon Loss: 0.006693 | Commit Loss: 0.001923 | Perplexity: 1343.355318
2025-09-26 18:23:49,522 Stage: Train 0.5 | Epoch: 40 | Iter: 62200 | Total Loss: 0.007589 | Recon Loss: 0.006630 | Commit Loss: 0.001917 | Perplexity: 1340.868864
Trainning Epoch:  12%|█▏        | 41/330 [4:12:58<27:51:18, 346.98s/it]2025-09-26 18:24:35,292 Stage: Train 0.5 | Epoch: 41 | Iter: 62400 | Total Loss: 0.007612 | Recon Loss: 0.006656 | Commit Loss: 0.001913 | Perplexity: 1340.197314
2025-09-26 18:25:20,914 Stage: Train 0.5 | Epoch: 41 | Iter: 62600 | Total Loss: 0.007668 | Recon Loss: 0.006713 | Commit Loss: 0.001910 | Perplexity: 1339.951261
2025-09-26 18:26:06,609 Stage: Train 0.5 | Epoch: 41 | Iter: 62800 | Total Loss: 0.007743 | Recon Loss: 0.006778 | Commit Loss: 0.001930 | Perplexity: 1341.014349
2025-09-26 18:26:52,257 Stage: Train 0.5 | Epoch: 41 | Iter: 63000 | Total Loss: 0.007597 | Recon Loss: 0.006638 | Commit Loss: 0.001918 | Perplexity: 1339.367186
2025-09-26 18:27:37,943 Stage: Train 0.5 | Epoch: 41 | Iter: 63200 | Total Loss: 0.007569 | Recon Loss: 0.006608 | Commit Loss: 0.001922 | Perplexity: 1342.676899
2025-09-26 18:28:23,656 Stage: Train 0.5 | Epoch: 41 | Iter: 63400 | Total Loss: 0.007714 | Recon Loss: 0.006750 | Commit Loss: 0.001927 | Perplexity: 1342.395126
2025-09-26 18:29:09,348 Stage: Train 0.5 | Epoch: 41 | Iter: 63600 | Total Loss: 0.007665 | Recon Loss: 0.006707 | Commit Loss: 0.001917 | Perplexity: 1341.032609
Trainning Epoch:  13%|█▎        | 42/330 [4:18:45<27:45:16, 346.93s/it]2025-09-26 18:29:54,953 Stage: Train 0.5 | Epoch: 42 | Iter: 63800 | Total Loss: 0.007579 | Recon Loss: 0.006623 | Commit Loss: 0.001911 | Perplexity: 1339.681158
2025-09-26 18:30:40,700 Stage: Train 0.5 | Epoch: 42 | Iter: 64000 | Total Loss: 0.007598 | Recon Loss: 0.006632 | Commit Loss: 0.001931 | Perplexity: 1341.322266
2025-09-26 18:31:26,427 Stage: Train 0.5 | Epoch: 42 | Iter: 64200 | Total Loss: 0.007599 | Recon Loss: 0.006651 | Commit Loss: 0.001895 | Perplexity: 1337.678156
2025-09-26 18:32:11,980 Stage: Train 0.5 | Epoch: 42 | Iter: 64400 | Total Loss: 0.007634 | Recon Loss: 0.006669 | Commit Loss: 0.001930 | Perplexity: 1339.661631
2025-09-26 18:32:57,590 Stage: Train 0.5 | Epoch: 42 | Iter: 64600 | Total Loss: 0.007653 | Recon Loss: 0.006695 | Commit Loss: 0.001915 | Perplexity: 1341.413954
2025-09-26 18:33:43,406 Stage: Train 0.5 | Epoch: 42 | Iter: 64800 | Total Loss: 0.007589 | Recon Loss: 0.006626 | Commit Loss: 0.001926 | Perplexity: 1344.315261
2025-09-26 18:34:29,254 Stage: Train 0.5 | Epoch: 42 | Iter: 65000 | Total Loss: 0.007565 | Recon Loss: 0.006602 | Commit Loss: 0.001925 | Perplexity: 1342.478328
2025-09-26 18:35:14,935 Stage: Train 0.5 | Epoch: 42 | Iter: 65200 | Total Loss: 0.007503 | Recon Loss: 0.006540 | Commit Loss: 0.001926 | Perplexity: 1342.344622
Trainning Epoch:  13%|█▎        | 43/330 [4:24:32<27:40:08, 347.07s/it]2025-09-26 18:36:00,842 Stage: Train 0.5 | Epoch: 43 | Iter: 65400 | Total Loss: 0.007665 | Recon Loss: 0.006703 | Commit Loss: 0.001924 | Perplexity: 1340.703049
2025-09-26 18:36:46,376 Stage: Train 0.5 | Epoch: 43 | Iter: 65600 | Total Loss: 0.007502 | Recon Loss: 0.006539 | Commit Loss: 0.001927 | Perplexity: 1342.429001
2025-09-26 18:37:32,158 Stage: Train 0.5 | Epoch: 43 | Iter: 65800 | Total Loss: 0.007562 | Recon Loss: 0.006597 | Commit Loss: 0.001929 | Perplexity: 1345.285804
2025-09-26 18:38:17,788 Stage: Train 0.5 | Epoch: 43 | Iter: 66000 | Total Loss: 0.007537 | Recon Loss: 0.006569 | Commit Loss: 0.001936 | Perplexity: 1343.053151
2025-09-26 18:39:03,296 Stage: Train 0.5 | Epoch: 43 | Iter: 66200 | Total Loss: 0.007493 | Recon Loss: 0.006533 | Commit Loss: 0.001921 | Perplexity: 1340.493139
2025-09-26 18:39:48,578 Stage: Train 0.5 | Epoch: 43 | Iter: 66400 | Total Loss: 0.007766 | Recon Loss: 0.006809 | Commit Loss: 0.001913 | Perplexity: 1339.669781
2025-09-26 18:40:34,285 Stage: Train 0.5 | Epoch: 43 | Iter: 66600 | Total Loss: 0.007528 | Recon Loss: 0.006567 | Commit Loss: 0.001922 | Perplexity: 1340.229889
2025-09-26 18:41:20,090 Stage: Train 0.5 | Epoch: 43 | Iter: 66800 | Total Loss: 0.007558 | Recon Loss: 0.006592 | Commit Loss: 0.001932 | Perplexity: 1341.323965
Trainning Epoch:  13%|█▎        | 44/330 [4:30:19<27:33:44, 346.94s/it]2025-09-26 18:42:06,069 Stage: Train 0.5 | Epoch: 44 | Iter: 67000 | Total Loss: 0.007530 | Recon Loss: 0.006570 | Commit Loss: 0.001920 | Perplexity: 1342.648846
2025-09-26 18:42:51,870 Stage: Train 0.5 | Epoch: 44 | Iter: 67200 | Total Loss: 0.007451 | Recon Loss: 0.006486 | Commit Loss: 0.001929 | Perplexity: 1341.112877
2025-09-26 18:43:37,561 Stage: Train 0.5 | Epoch: 44 | Iter: 67400 | Total Loss: 0.007487 | Recon Loss: 0.006523 | Commit Loss: 0.001928 | Perplexity: 1341.067589
2025-09-26 18:44:23,360 Stage: Train 0.5 | Epoch: 44 | Iter: 67600 | Total Loss: 0.007567 | Recon Loss: 0.006596 | Commit Loss: 0.001940 | Perplexity: 1343.124911
2025-09-26 18:45:09,145 Stage: Train 0.5 | Epoch: 44 | Iter: 67800 | Total Loss: 0.007463 | Recon Loss: 0.006490 | Commit Loss: 0.001947 | Perplexity: 1343.869531
2025-09-26 18:45:54,832 Stage: Train 0.5 | Epoch: 44 | Iter: 68000 | Total Loss: 0.007449 | Recon Loss: 0.006482 | Commit Loss: 0.001934 | Perplexity: 1343.210003
2025-09-26 18:46:40,089 Stage: Train 0.5 | Epoch: 44 | Iter: 68200 | Total Loss: 0.007555 | Recon Loss: 0.006594 | Commit Loss: 0.001922 | Perplexity: 1340.200316
Trainning Epoch:  14%|█▎        | 45/330 [4:36:06<27:28:15, 347.00s/it]2025-09-26 18:47:25,929 Stage: Train 0.5 | Epoch: 45 | Iter: 68400 | Total Loss: 0.007556 | Recon Loss: 0.006587 | Commit Loss: 0.001938 | Perplexity: 1340.217654
2025-09-26 18:48:11,646 Stage: Train 0.5 | Epoch: 45 | Iter: 68600 | Total Loss: 0.007437 | Recon Loss: 0.006483 | Commit Loss: 0.001908 | Perplexity: 1338.846326
2025-09-26 18:48:57,392 Stage: Train 0.5 | Epoch: 45 | Iter: 68800 | Total Loss: 0.007339 | Recon Loss: 0.006377 | Commit Loss: 0.001925 | Perplexity: 1340.163768
2025-09-26 18:49:42,975 Stage: Train 0.5 | Epoch: 45 | Iter: 69000 | Total Loss: 0.007542 | Recon Loss: 0.006567 | Commit Loss: 0.001950 | Perplexity: 1342.106957
2025-09-26 18:50:28,866 Stage: Train 0.5 | Epoch: 45 | Iter: 69200 | Total Loss: 0.007568 | Recon Loss: 0.006597 | Commit Loss: 0.001942 | Perplexity: 1341.777390
2025-09-26 18:51:14,669 Stage: Train 0.5 | Epoch: 45 | Iter: 69400 | Total Loss: 0.007533 | Recon Loss: 0.006566 | Commit Loss: 0.001933 | Perplexity: 1340.942125
2025-09-26 18:52:00,670 Stage: Train 0.5 | Epoch: 45 | Iter: 69600 | Total Loss: 0.007439 | Recon Loss: 0.006480 | Commit Loss: 0.001919 | Perplexity: 1339.476988
2025-09-26 18:52:46,369 Stage: Train 0.5 | Epoch: 45 | Iter: 69800 | Total Loss: 0.007427 | Recon Loss: 0.006443 | Commit Loss: 0.001968 | Perplexity: 1348.575488
Trainning Epoch:  14%|█▍        | 46/330 [4:41:52<27:21:05, 346.71s/it]2025-09-26 18:53:30,396 Stage: Train 0.5 | Epoch: 46 | Iter: 70000 | Total Loss: 0.007467 | Recon Loss: 0.006489 | Commit Loss: 0.001956 | Perplexity: 1343.939418
2025-09-26 18:54:16,058 Stage: Train 0.5 | Epoch: 46 | Iter: 70200 | Total Loss: 0.007418 | Recon Loss: 0.006455 | Commit Loss: 0.001925 | Perplexity: 1341.341059
2025-09-26 18:55:01,820 Stage: Train 0.5 | Epoch: 46 | Iter: 70400 | Total Loss: 0.007444 | Recon Loss: 0.006478 | Commit Loss: 0.001932 | Perplexity: 1341.491747
2025-09-26 18:55:47,262 Stage: Train 0.5 | Epoch: 46 | Iter: 70600 | Total Loss: 0.007383 | Recon Loss: 0.006417 | Commit Loss: 0.001931 | Perplexity: 1338.604362
2025-09-26 18:56:32,658 Stage: Train 0.5 | Epoch: 46 | Iter: 70800 | Total Loss: 0.007373 | Recon Loss: 0.006405 | Commit Loss: 0.001936 | Perplexity: 1342.831346
2025-09-26 18:57:18,008 Stage: Train 0.5 | Epoch: 46 | Iter: 71000 | Total Loss: 0.007457 | Recon Loss: 0.006494 | Commit Loss: 0.001926 | Perplexity: 1339.460547
2025-09-26 18:58:03,589 Stage: Train 0.5 | Epoch: 46 | Iter: 71200 | Total Loss: 0.007411 | Recon Loss: 0.006436 | Commit Loss: 0.001948 | Perplexity: 1342.349312
Trainning Epoch:  14%|█▍        | 47/330 [4:47:38<27:14:17, 346.49s/it]2025-09-26 18:58:49,265 Stage: Train 0.5 | Epoch: 47 | Iter: 71400 | Total Loss: 0.007422 | Recon Loss: 0.006457 | Commit Loss: 0.001932 | Perplexity: 1341.789508
2025-09-26 18:59:34,877 Stage: Train 0.5 | Epoch: 47 | Iter: 71600 | Total Loss: 0.007423 | Recon Loss: 0.006454 | Commit Loss: 0.001939 | Perplexity: 1342.561758
2025-09-26 19:00:20,514 Stage: Train 0.5 | Epoch: 47 | Iter: 71800 | Total Loss: 0.007388 | Recon Loss: 0.006415 | Commit Loss: 0.001945 | Perplexity: 1342.532250
2025-09-26 19:01:06,332 Stage: Train 0.5 | Epoch: 47 | Iter: 72000 | Total Loss: 0.007467 | Recon Loss: 0.006498 | Commit Loss: 0.001938 | Perplexity: 1340.345657
2025-09-26 19:01:52,058 Stage: Train 0.5 | Epoch: 47 | Iter: 72200 | Total Loss: 0.007411 | Recon Loss: 0.006432 | Commit Loss: 0.001957 | Perplexity: 1343.579280
2025-09-26 19:02:37,787 Stage: Train 0.5 | Epoch: 47 | Iter: 72400 | Total Loss: 0.007282 | Recon Loss: 0.006322 | Commit Loss: 0.001920 | Perplexity: 1340.946281
2025-09-26 19:03:23,355 Stage: Train 0.5 | Epoch: 47 | Iter: 72600 | Total Loss: 0.007389 | Recon Loss: 0.006421 | Commit Loss: 0.001936 | Perplexity: 1341.416700
2025-09-26 19:04:08,975 Stage: Train 0.5 | Epoch: 47 | Iter: 72800 | Total Loss: 0.007409 | Recon Loss: 0.006435 | Commit Loss: 0.001946 | Perplexity: 1343.773965
Trainning Epoch:  15%|█▍        | 48/330 [4:53:25<27:09:25, 346.69s/it]2025-09-26 19:04:54,940 Stage: Train 0.5 | Epoch: 48 | Iter: 73000 | Total Loss: 0.007347 | Recon Loss: 0.006379 | Commit Loss: 0.001936 | Perplexity: 1343.433585
2025-09-26 19:05:40,732 Stage: Train 0.5 | Epoch: 48 | Iter: 73200 | Total Loss: 0.007319 | Recon Loss: 0.006348 | Commit Loss: 0.001941 | Perplexity: 1341.679387
2025-09-26 19:06:26,238 Stage: Train 0.5 | Epoch: 48 | Iter: 73400 | Total Loss: 0.007474 | Recon Loss: 0.006488 | Commit Loss: 0.001971 | Perplexity: 1342.722534
2025-09-26 19:07:11,771 Stage: Train 0.5 | Epoch: 48 | Iter: 73600 | Total Loss: 0.007345 | Recon Loss: 0.006374 | Commit Loss: 0.001941 | Perplexity: 1341.236784
2025-09-26 19:07:57,576 Stage: Train 0.5 | Epoch: 48 | Iter: 73800 | Total Loss: 0.007311 | Recon Loss: 0.006346 | Commit Loss: 0.001930 | Perplexity: 1339.583967
2025-09-26 19:08:43,198 Stage: Train 0.5 | Epoch: 48 | Iter: 74000 | Total Loss: 0.007390 | Recon Loss: 0.006410 | Commit Loss: 0.001959 | Perplexity: 1343.225482
2025-09-26 19:09:28,819 Stage: Train 0.5 | Epoch: 48 | Iter: 74200 | Total Loss: 0.007261 | Recon Loss: 0.006282 | Commit Loss: 0.001957 | Perplexity: 1344.492999
2025-09-26 19:10:14,057 Stage: Train 0.5 | Epoch: 48 | Iter: 74400 | Total Loss: 0.007371 | Recon Loss: 0.006392 | Commit Loss: 0.001958 | Perplexity: 1344.345504
Trainning Epoch:  15%|█▍        | 49/330 [4:59:11<27:03:22, 346.63s/it]2025-09-26 19:10:59,954 Stage: Train 0.5 | Epoch: 49 | Iter: 74600 | Total Loss: 0.007326 | Recon Loss: 0.006346 | Commit Loss: 0.001960 | Perplexity: 1343.791913
2025-09-26 19:11:45,559 Stage: Train 0.5 | Epoch: 49 | Iter: 74800 | Total Loss: 0.007373 | Recon Loss: 0.006400 | Commit Loss: 0.001946 | Perplexity: 1342.971934
2025-09-26 19:12:30,975 Stage: Train 0.5 | Epoch: 49 | Iter: 75000 | Total Loss: 0.007197 | Recon Loss: 0.006226 | Commit Loss: 0.001942 | Perplexity: 1341.282742
2025-09-26 19:13:16,290 Stage: Train 0.5 | Epoch: 49 | Iter: 75200 | Total Loss: 0.007314 | Recon Loss: 0.006342 | Commit Loss: 0.001944 | Perplexity: 1339.737414
2025-09-26 19:14:02,174 Stage: Train 0.5 | Epoch: 49 | Iter: 75400 | Total Loss: 0.007360 | Recon Loss: 0.006375 | Commit Loss: 0.001969 | Perplexity: 1342.972870
2025-09-26 19:14:48,011 Stage: Train 0.5 | Epoch: 49 | Iter: 75600 | Total Loss: 0.007314 | Recon Loss: 0.006338 | Commit Loss: 0.001952 | Perplexity: 1342.218259
2025-09-26 19:15:33,936 Stage: Train 0.5 | Epoch: 49 | Iter: 75800 | Total Loss: 0.007428 | Recon Loss: 0.006448 | Commit Loss: 0.001959 | Perplexity: 1342.407532
Trainning Epoch:  15%|█▌        | 50/330 [5:04:58<26:58:15, 346.77s/it]2025-09-26 19:16:19,776 Stage: Train 0.5 | Epoch: 50 | Iter: 76000 | Total Loss: 0.007238 | Recon Loss: 0.006260 | Commit Loss: 0.001955 | Perplexity: 1342.327929
2025-09-26 19:17:05,386 Stage: Train 0.5 | Epoch: 50 | Iter: 76200 | Total Loss: 0.007181 | Recon Loss: 0.006202 | Commit Loss: 0.001957 | Perplexity: 1342.579309
2025-09-26 19:17:51,092 Stage: Train 0.5 | Epoch: 50 | Iter: 76400 | Total Loss: 0.007309 | Recon Loss: 0.006330 | Commit Loss: 0.001957 | Perplexity: 1343.426605
2025-09-26 19:18:36,541 Stage: Train 0.5 | Epoch: 50 | Iter: 76600 | Total Loss: 0.007302 | Recon Loss: 0.006331 | Commit Loss: 0.001943 | Perplexity: 1339.810283
2025-09-26 19:19:22,408 Stage: Train 0.5 | Epoch: 50 | Iter: 76800 | Total Loss: 0.007261 | Recon Loss: 0.006291 | Commit Loss: 0.001941 | Perplexity: 1340.658461
2025-09-26 19:20:07,769 Stage: Train 0.5 | Epoch: 50 | Iter: 77000 | Total Loss: 0.007302 | Recon Loss: 0.006314 | Commit Loss: 0.001975 | Perplexity: 1347.107880
2025-09-26 19:20:53,640 Stage: Train 0.5 | Epoch: 50 | Iter: 77200 | Total Loss: 0.007217 | Recon Loss: 0.006238 | Commit Loss: 0.001959 | Perplexity: 1342.995963
2025-09-26 19:21:39,518 Stage: Train 0.5 | Epoch: 50 | Iter: 77400 | Total Loss: 0.007288 | Recon Loss: 0.006303 | Commit Loss: 0.001972 | Perplexity: 1342.750095
Trainning Epoch:  15%|█▌        | 51/330 [5:10:45<26:52:44, 346.83s/it]2025-09-26 19:22:25,301 Stage: Train 0.5 | Epoch: 51 | Iter: 77600 | Total Loss: 0.007335 | Recon Loss: 0.006355 | Commit Loss: 0.001960 | Perplexity: 1339.808208
2025-09-26 19:23:11,132 Stage: Train 0.5 | Epoch: 51 | Iter: 77800 | Total Loss: 0.007336 | Recon Loss: 0.006356 | Commit Loss: 0.001959 | Perplexity: 1341.758530
2025-09-26 19:23:56,451 Stage: Train 0.5 | Epoch: 51 | Iter: 78000 | Total Loss: 0.007178 | Recon Loss: 0.006204 | Commit Loss: 0.001949 | Perplexity: 1342.046750
2025-09-26 19:24:42,234 Stage: Train 0.5 | Epoch: 51 | Iter: 78200 | Total Loss: 0.007326 | Recon Loss: 0.006344 | Commit Loss: 0.001965 | Perplexity: 1343.156028
2025-09-26 19:25:27,765 Stage: Train 0.5 | Epoch: 51 | Iter: 78400 | Total Loss: 0.007267 | Recon Loss: 0.006286 | Commit Loss: 0.001961 | Perplexity: 1342.434100
2025-09-26 19:26:13,173 Stage: Train 0.5 | Epoch: 51 | Iter: 78600 | Total Loss: 0.007190 | Recon Loss: 0.006215 | Commit Loss: 0.001950 | Perplexity: 1342.696714
2025-09-26 19:26:58,427 Stage: Train 0.5 | Epoch: 51 | Iter: 78800 | Total Loss: 0.007216 | Recon Loss: 0.006227 | Commit Loss: 0.001977 | Perplexity: 1343.827358
Trainning Epoch:  16%|█▌        | 52/330 [5:16:31<26:45:50, 346.58s/it]2025-09-26 19:27:44,138 Stage: Train 0.5 | Epoch: 52 | Iter: 79000 | Total Loss: 0.007245 | Recon Loss: 0.006260 | Commit Loss: 0.001971 | Perplexity: 1341.037341
2025-09-26 19:28:29,769 Stage: Train 0.5 | Epoch: 52 | Iter: 79200 | Total Loss: 0.007246 | Recon Loss: 0.006266 | Commit Loss: 0.001961 | Perplexity: 1342.444788
2025-09-26 19:29:15,478 Stage: Train 0.5 | Epoch: 52 | Iter: 79400 | Total Loss: 0.007231 | Recon Loss: 0.006251 | Commit Loss: 0.001961 | Perplexity: 1342.918214
2025-09-26 19:30:00,826 Stage: Train 0.5 | Epoch: 52 | Iter: 79600 | Total Loss: 0.007238 | Recon Loss: 0.006255 | Commit Loss: 0.001965 | Perplexity: 1343.074857
2025-09-26 19:30:46,489 Stage: Train 0.5 | Epoch: 52 | Iter: 79800 | Total Loss: 0.007252 | Recon Loss: 0.006269 | Commit Loss: 0.001966 | Perplexity: 1342.067481
2025-09-26 19:31:32,347 Stage: Train 0.5 | Epoch: 52 | Iter: 80000 | Total Loss: 0.007209 | Recon Loss: 0.006225 | Commit Loss: 0.001969 | Perplexity: 1344.065278
2025-09-26 19:31:32,347 Saving model at iteration 80000
2025-09-26 19:31:32,547 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000
2025-09-26 19:31:32,835 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/model.safetensors
2025-09-26 19:31:33,205 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/optimizer.bin
2025-09-26 19:31:33,206 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/scheduler.bin
2025-09-26 19:31:33,206 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/sampler.bin
2025-09-26 19:31:33,207 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/random_states_0.pkl
2025-09-26 19:32:19,202 Stage: Train 0.5 | Epoch: 52 | Iter: 80200 | Total Loss: 0.007223 | Recon Loss: 0.006244 | Commit Loss: 0.001959 | Perplexity: 1342.235445
2025-09-26 19:33:05,023 Stage: Train 0.5 | Epoch: 52 | Iter: 80400 | Total Loss: 0.007272 | Recon Loss: 0.006287 | Commit Loss: 0.001970 | Perplexity: 1341.856894
Trainning Epoch:  16%|█▌        | 53/330 [5:22:19<26:42:00, 347.01s/it]2025-09-26 19:33:50,632 Stage: Train 0.5 | Epoch: 53 | Iter: 80600 | Total Loss: 0.007164 | Recon Loss: 0.006177 | Commit Loss: 0.001973 | Perplexity: 1342.906604
2025-09-26 19:34:36,387 Stage: Train 0.5 | Epoch: 53 | Iter: 80800 | Total Loss: 0.007251 | Recon Loss: 0.006265 | Commit Loss: 0.001972 | Perplexity: 1341.328255
2025-09-26 19:35:22,117 Stage: Train 0.5 | Epoch: 53 | Iter: 81000 | Total Loss: 0.007127 | Recon Loss: 0.006148 | Commit Loss: 0.001957 | Perplexity: 1339.759842
2025-09-26 19:36:07,704 Stage: Train 0.5 | Epoch: 53 | Iter: 81200 | Total Loss: 0.007184 | Recon Loss: 0.006194 | Commit Loss: 0.001980 | Perplexity: 1345.079952
2025-09-26 19:36:52,929 Stage: Train 0.5 | Epoch: 53 | Iter: 81400 | Total Loss: 0.007240 | Recon Loss: 0.006252 | Commit Loss: 0.001975 | Perplexity: 1342.035591
2025-09-26 19:37:36,749 Stage: Train 0.5 | Epoch: 53 | Iter: 81600 | Total Loss: 0.007152 | Recon Loss: 0.006163 | Commit Loss: 0.001977 | Perplexity: 1342.820412
2025-09-26 19:38:22,514 Stage: Train 0.5 | Epoch: 53 | Iter: 81800 | Total Loss: 0.007179 | Recon Loss: 0.006188 | Commit Loss: 0.001982 | Perplexity: 1342.763994
2025-09-26 19:39:08,155 Stage: Train 0.5 | Epoch: 53 | Iter: 82000 | Total Loss: 0.007138 | Recon Loss: 0.006157 | Commit Loss: 0.001961 | Perplexity: 1338.765682
Trainning Epoch:  16%|█▋        | 54/330 [5:28:04<26:33:19, 346.37s/it]2025-09-26 19:39:53,951 Stage: Train 0.5 | Epoch: 54 | Iter: 82200 | Total Loss: 0.007184 | Recon Loss: 0.006205 | Commit Loss: 0.001959 | Perplexity: 1342.092658
2025-09-26 19:40:39,175 Stage: Train 0.5 | Epoch: 54 | Iter: 82400 | Total Loss: 0.007184 | Recon Loss: 0.006202 | Commit Loss: 0.001963 | Perplexity: 1340.952036
2025-09-26 19:41:24,955 Stage: Train 0.5 | Epoch: 54 | Iter: 82600 | Total Loss: 0.007172 | Recon Loss: 0.006187 | Commit Loss: 0.001969 | Perplexity: 1340.288542
2025-09-26 19:42:10,994 Stage: Train 0.5 | Epoch: 54 | Iter: 82800 | Total Loss: 0.007103 | Recon Loss: 0.006114 | Commit Loss: 0.001978 | Perplexity: 1343.901398
2025-09-26 19:42:56,780 Stage: Train 0.5 | Epoch: 54 | Iter: 83000 | Total Loss: 0.007168 | Recon Loss: 0.006183 | Commit Loss: 0.001971 | Perplexity: 1342.262375
2025-09-26 19:43:42,364 Stage: Train 0.5 | Epoch: 54 | Iter: 83200 | Total Loss: 0.007140 | Recon Loss: 0.006147 | Commit Loss: 0.001985 | Perplexity: 1342.316912
2025-09-26 19:44:28,173 Stage: Train 0.5 | Epoch: 54 | Iter: 83400 | Total Loss: 0.007101 | Recon Loss: 0.006116 | Commit Loss: 0.001969 | Perplexity: 1342.969309
Trainning Epoch:  17%|█▋        | 55/330 [5:33:52<26:28:50, 346.66s/it]2025-09-26 19:45:14,219 Stage: Train 0.5 | Epoch: 55 | Iter: 83600 | Total Loss: 0.007177 | Recon Loss: 0.006187 | Commit Loss: 0.001981 | Perplexity: 1342.224179
2025-09-26 19:45:59,934 Stage: Train 0.5 | Epoch: 55 | Iter: 83800 | Total Loss: 0.007184 | Recon Loss: 0.006202 | Commit Loss: 0.001963 | Perplexity: 1339.350262
2025-09-26 19:46:45,812 Stage: Train 0.5 | Epoch: 55 | Iter: 84000 | Total Loss: 0.007098 | Recon Loss: 0.006116 | Commit Loss: 0.001964 | Perplexity: 1342.482888
2025-09-26 19:47:31,561 Stage: Train 0.5 | Epoch: 55 | Iter: 84200 | Total Loss: 0.007155 | Recon Loss: 0.006167 | Commit Loss: 0.001976 | Perplexity: 1342.970325
2025-09-26 19:48:17,741 Stage: Train 0.5 | Epoch: 55 | Iter: 84400 | Total Loss: 0.007103 | Recon Loss: 0.006107 | Commit Loss: 0.001991 | Perplexity: 1344.485604
2025-09-26 19:49:03,780 Stage: Train 0.5 | Epoch: 55 | Iter: 84600 | Total Loss: 0.007131 | Recon Loss: 0.006143 | Commit Loss: 0.001975 | Perplexity: 1340.270493
2025-09-26 19:49:49,510 Stage: Train 0.5 | Epoch: 55 | Iter: 84800 | Total Loss: 0.007121 | Recon Loss: 0.006133 | Commit Loss: 0.001977 | Perplexity: 1340.346480
2025-09-26 19:50:35,220 Stage: Train 0.5 | Epoch: 55 | Iter: 85000 | Total Loss: 0.007198 | Recon Loss: 0.006210 | Commit Loss: 0.001977 | Perplexity: 1341.447462
Trainning Epoch:  17%|█▋        | 56/330 [5:39:40<26:25:23, 347.17s/it]2025-09-26 19:51:21,013 Stage: Train 0.5 | Epoch: 56 | Iter: 85200 | Total Loss: 0.007172 | Recon Loss: 0.006187 | Commit Loss: 0.001970 | Perplexity: 1341.129210
2025-09-26 19:52:06,562 Stage: Train 0.5 | Epoch: 56 | Iter: 85400 | Total Loss: 0.007094 | Recon Loss: 0.006096 | Commit Loss: 0.001997 | Perplexity: 1343.117618
2025-09-26 19:52:52,262 Stage: Train 0.5 | Epoch: 56 | Iter: 85600 | Total Loss: 0.007155 | Recon Loss: 0.006165 | Commit Loss: 0.001981 | Perplexity: 1340.535281
2025-09-26 19:53:37,367 Stage: Train 0.5 | Epoch: 56 | Iter: 85800 | Total Loss: 0.007105 | Recon Loss: 0.006116 | Commit Loss: 0.001979 | Perplexity: 1341.191628
2025-09-26 19:54:22,903 Stage: Train 0.5 | Epoch: 56 | Iter: 86000 | Total Loss: 0.007000 | Recon Loss: 0.006016 | Commit Loss: 0.001968 | Perplexity: 1341.228060
2025-09-26 19:55:08,605 Stage: Train 0.5 | Epoch: 56 | Iter: 86200 | Total Loss: 0.007240 | Recon Loss: 0.006247 | Commit Loss: 0.001987 | Perplexity: 1341.031102
2025-09-26 19:55:54,404 Stage: Train 0.5 | Epoch: 56 | Iter: 86400 | Total Loss: 0.007135 | Recon Loss: 0.006148 | Commit Loss: 0.001974 | Perplexity: 1337.573189
Trainning Epoch:  17%|█▋        | 57/330 [5:45:26<26:18:27, 346.91s/it]2025-09-26 19:56:40,155 Stage: Train 0.5 | Epoch: 57 | Iter: 86600 | Total Loss: 0.007150 | Recon Loss: 0.006158 | Commit Loss: 0.001983 | Perplexity: 1342.953359
2025-09-26 19:57:25,617 Stage: Train 0.5 | Epoch: 57 | Iter: 86800 | Total Loss: 0.007060 | Recon Loss: 0.006073 | Commit Loss: 0.001973 | Perplexity: 1344.904008
2025-09-26 19:58:11,225 Stage: Train 0.5 | Epoch: 57 | Iter: 87000 | Total Loss: 0.007037 | Recon Loss: 0.006048 | Commit Loss: 0.001977 | Perplexity: 1342.176895
2025-09-26 19:58:56,829 Stage: Train 0.5 | Epoch: 57 | Iter: 87200 | Total Loss: 0.007036 | Recon Loss: 0.006042 | Commit Loss: 0.001989 | Perplexity: 1343.500798
2025-09-26 19:59:42,234 Stage: Train 0.5 | Epoch: 57 | Iter: 87400 | Total Loss: 0.007092 | Recon Loss: 0.006096 | Commit Loss: 0.001992 | Perplexity: 1342.717642
2025-09-26 20:00:27,480 Stage: Train 0.5 | Epoch: 57 | Iter: 87600 | Total Loss: 0.007025 | Recon Loss: 0.006039 | Commit Loss: 0.001972 | Perplexity: 1340.383358
2025-09-26 20:01:13,300 Stage: Train 0.5 | Epoch: 57 | Iter: 87800 | Total Loss: 0.007141 | Recon Loss: 0.006150 | Commit Loss: 0.001981 | Perplexity: 1343.962186
2025-09-26 20:01:59,098 Stage: Train 0.5 | Epoch: 57 | Iter: 88000 | Total Loss: 0.007027 | Recon Loss: 0.006036 | Commit Loss: 0.001982 | Perplexity: 1341.120266
Trainning Epoch:  18%|█▊        | 58/330 [5:51:13<26:11:46, 346.72s/it]2025-09-26 20:02:44,934 Stage: Train 0.5 | Epoch: 58 | Iter: 88200 | Total Loss: 0.007029 | Recon Loss: 0.006046 | Commit Loss: 0.001966 | Perplexity: 1337.368452
2025-09-26 20:03:30,508 Stage: Train 0.5 | Epoch: 58 | Iter: 88400 | Total Loss: 0.006903 | Recon Loss: 0.005925 | Commit Loss: 0.001956 | Perplexity: 1339.811484
2025-09-26 20:04:15,865 Stage: Train 0.5 | Epoch: 58 | Iter: 88600 | Total Loss: 0.007172 | Recon Loss: 0.006171 | Commit Loss: 0.002001 | Perplexity: 1341.629524
2025-09-26 20:05:01,571 Stage: Train 0.5 | Epoch: 58 | Iter: 88800 | Total Loss: 0.007116 | Recon Loss: 0.006116 | Commit Loss: 0.002000 | Perplexity: 1343.982104
2025-09-26 20:05:47,216 Stage: Train 0.5 | Epoch: 58 | Iter: 89000 | Total Loss: 0.007094 | Recon Loss: 0.006104 | Commit Loss: 0.001980 | Perplexity: 1340.312017
2025-09-26 20:06:33,005 Stage: Train 0.5 | Epoch: 58 | Iter: 89200 | Total Loss: 0.006988 | Recon Loss: 0.005994 | Commit Loss: 0.001989 | Perplexity: 1341.948696
2025-09-26 20:07:18,422 Stage: Train 0.5 | Epoch: 58 | Iter: 89400 | Total Loss: 0.007080 | Recon Loss: 0.006089 | Commit Loss: 0.001982 | Perplexity: 1339.695138
2025-09-26 20:08:04,260 Stage: Train 0.5 | Epoch: 58 | Iter: 89600 | Total Loss: 0.007040 | Recon Loss: 0.006041 | Commit Loss: 0.001998 | Perplexity: 1341.545521
Trainning Epoch:  18%|█▊        | 59/330 [5:56:59<26:05:54, 346.70s/it]2025-09-26 20:08:50,170 Stage: Train 0.5 | Epoch: 59 | Iter: 89800 | Total Loss: 0.006959 | Recon Loss: 0.005971 | Commit Loss: 0.001977 | Perplexity: 1341.424346
2025-09-26 20:09:35,804 Stage: Train 0.5 | Epoch: 59 | Iter: 90000 | Total Loss: 0.007003 | Recon Loss: 0.006008 | Commit Loss: 0.001990 | Perplexity: 1341.617864
2025-09-26 20:10:21,572 Stage: Train 0.5 | Epoch: 59 | Iter: 90200 | Total Loss: 0.006982 | Recon Loss: 0.005986 | Commit Loss: 0.001991 | Perplexity: 1342.188182
2025-09-26 20:11:07,131 Stage: Train 0.5 | Epoch: 59 | Iter: 90400 | Total Loss: 0.007030 | Recon Loss: 0.006040 | Commit Loss: 0.001980 | Perplexity: 1340.234496
2025-09-26 20:11:52,811 Stage: Train 0.5 | Epoch: 59 | Iter: 90600 | Total Loss: 0.007071 | Recon Loss: 0.006073 | Commit Loss: 0.001996 | Perplexity: 1342.421700
2025-09-26 20:12:38,444 Stage: Train 0.5 | Epoch: 59 | Iter: 90800 | Total Loss: 0.006956 | Recon Loss: 0.005955 | Commit Loss: 0.002001 | Perplexity: 1344.927061
2025-09-26 20:13:24,378 Stage: Train 0.5 | Epoch: 59 | Iter: 91000 | Total Loss: 0.007022 | Recon Loss: 0.006027 | Commit Loss: 0.001991 | Perplexity: 1341.091417
Trainning Epoch:  18%|█▊        | 60/330 [6:02:46<26:00:44, 346.83s/it]2025-09-26 20:14:10,097 Stage: Train 0.5 | Epoch: 60 | Iter: 91200 | Total Loss: 0.006908 | Recon Loss: 0.005913 | Commit Loss: 0.001991 | Perplexity: 1342.192679
2025-09-26 20:14:55,797 Stage: Train 0.5 | Epoch: 60 | Iter: 91400 | Total Loss: 0.007022 | Recon Loss: 0.006030 | Commit Loss: 0.001984 | Perplexity: 1338.028216
2025-09-26 20:15:41,426 Stage: Train 0.5 | Epoch: 60 | Iter: 91600 | Total Loss: 0.007010 | Recon Loss: 0.006008 | Commit Loss: 0.002004 | Perplexity: 1343.945679
2025-09-26 20:16:27,275 Stage: Train 0.5 | Epoch: 60 | Iter: 91800 | Total Loss: 0.006956 | Recon Loss: 0.005962 | Commit Loss: 0.001988 | Perplexity: 1340.980534
2025-09-26 20:17:13,010 Stage: Train 0.5 | Epoch: 60 | Iter: 92000 | Total Loss: 0.007046 | Recon Loss: 0.006048 | Commit Loss: 0.001995 | Perplexity: 1342.225060
2025-09-26 20:17:58,720 Stage: Train 0.5 | Epoch: 60 | Iter: 92200 | Total Loss: 0.006939 | Recon Loss: 0.005947 | Commit Loss: 0.001985 | Perplexity: 1339.206987
2025-09-26 20:18:44,734 Stage: Train 0.5 | Epoch: 60 | Iter: 92400 | Total Loss: 0.007009 | Recon Loss: 0.006008 | Commit Loss: 0.002003 | Perplexity: 1345.689452
2025-09-26 20:19:30,540 Stage: Train 0.5 | Epoch: 60 | Iter: 92600 | Total Loss: 0.007036 | Recon Loss: 0.006045 | Commit Loss: 0.001983 | Perplexity: 1340.411285
Trainning Epoch:  18%|█▊        | 61/330 [6:08:34<25:56:22, 347.15s/it]2025-09-26 20:20:16,543 Stage: Train 0.5 | Epoch: 61 | Iter: 92800 | Total Loss: 0.006995 | Recon Loss: 0.005993 | Commit Loss: 0.002002 | Perplexity: 1341.750291
2025-09-26 20:21:01,720 Stage: Train 0.5 | Epoch: 61 | Iter: 93000 | Total Loss: 0.006977 | Recon Loss: 0.005980 | Commit Loss: 0.001994 | Perplexity: 1338.656260
2025-09-26 20:21:46,068 Stage: Train 0.5 | Epoch: 61 | Iter: 93200 | Total Loss: 0.006996 | Recon Loss: 0.005999 | Commit Loss: 0.001994 | Perplexity: 1343.003934
2025-09-26 20:22:31,504 Stage: Train 0.5 | Epoch: 61 | Iter: 93400 | Total Loss: 0.007031 | Recon Loss: 0.006038 | Commit Loss: 0.001987 | Perplexity: 1340.514234
2025-09-26 20:23:17,289 Stage: Train 0.5 | Epoch: 61 | Iter: 93600 | Total Loss: 0.006983 | Recon Loss: 0.005975 | Commit Loss: 0.002016 | Perplexity: 1342.389826
2025-09-26 20:24:02,703 Stage: Train 0.5 | Epoch: 61 | Iter: 93800 | Total Loss: 0.006974 | Recon Loss: 0.005986 | Commit Loss: 0.001975 | Perplexity: 1338.839224
2025-09-26 20:24:48,403 Stage: Train 0.5 | Epoch: 61 | Iter: 94000 | Total Loss: 0.006930 | Recon Loss: 0.005932 | Commit Loss: 0.001996 | Perplexity: 1343.031292
Trainning Epoch:  19%|█▉        | 62/330 [6:14:19<25:47:40, 346.50s/it]2025-09-26 20:25:34,282 Stage: Train 0.5 | Epoch: 62 | Iter: 94200 | Total Loss: 0.006919 | Recon Loss: 0.005927 | Commit Loss: 0.001984 | Perplexity: 1339.824349
2025-09-26 20:26:19,891 Stage: Train 0.5 | Epoch: 62 | Iter: 94400 | Total Loss: 0.006961 | Recon Loss: 0.005961 | Commit Loss: 0.002002 | Perplexity: 1342.771091
2025-09-26 20:27:05,251 Stage: Train 0.5 | Epoch: 62 | Iter: 94600 | Total Loss: 0.006864 | Recon Loss: 0.005871 | Commit Loss: 0.001985 | Perplexity: 1338.138107
2025-09-26 20:27:50,590 Stage: Train 0.5 | Epoch: 62 | Iter: 94800 | Total Loss: 0.006888 | Recon Loss: 0.005893 | Commit Loss: 0.001991 | Perplexity: 1339.741772
2025-09-26 20:28:36,255 Stage: Train 0.5 | Epoch: 62 | Iter: 95000 | Total Loss: 0.006913 | Recon Loss: 0.005913 | Commit Loss: 0.001999 | Perplexity: 1339.802809
2025-09-26 20:29:21,966 Stage: Train 0.5 | Epoch: 62 | Iter: 95200 | Total Loss: 0.006971 | Recon Loss: 0.005970 | Commit Loss: 0.002002 | Perplexity: 1340.959741
2025-09-26 20:30:07,696 Stage: Train 0.5 | Epoch: 62 | Iter: 95400 | Total Loss: 0.006884 | Recon Loss: 0.005882 | Commit Loss: 0.002004 | Perplexity: 1343.773517
2025-09-26 20:30:52,959 Stage: Train 0.5 | Epoch: 62 | Iter: 95600 | Total Loss: 0.007041 | Recon Loss: 0.006037 | Commit Loss: 0.002008 | Perplexity: 1338.249467
Trainning Epoch:  19%|█▉        | 63/330 [6:20:05<25:41:25, 346.39s/it]2025-09-26 20:31:38,895 Stage: Train 0.5 | Epoch: 63 | Iter: 95800 | Total Loss: 0.006882 | Recon Loss: 0.005888 | Commit Loss: 0.001988 | Perplexity: 1339.204139
2025-09-26 20:32:24,798 Stage: Train 0.5 | Epoch: 63 | Iter: 96000 | Total Loss: 0.006901 | Recon Loss: 0.005897 | Commit Loss: 0.002009 | Perplexity: 1339.303139
2025-09-26 20:33:10,693 Stage: Train 0.5 | Epoch: 63 | Iter: 96200 | Total Loss: 0.006956 | Recon Loss: 0.005951 | Commit Loss: 0.002010 | Perplexity: 1342.155612
2025-09-26 20:33:56,077 Stage: Train 0.5 | Epoch: 63 | Iter: 96400 | Total Loss: 0.006887 | Recon Loss: 0.005890 | Commit Loss: 0.001993 | Perplexity: 1337.244472
2025-09-26 20:34:42,032 Stage: Train 0.5 | Epoch: 63 | Iter: 96600 | Total Loss: 0.007005 | Recon Loss: 0.006002 | Commit Loss: 0.002006 | Perplexity: 1340.168893
2025-09-26 20:35:28,067 Stage: Train 0.5 | Epoch: 63 | Iter: 96800 | Total Loss: 0.006843 | Recon Loss: 0.005840 | Commit Loss: 0.002007 | Perplexity: 1342.760473
2025-09-26 20:36:13,737 Stage: Train 0.5 | Epoch: 63 | Iter: 97000 | Total Loss: 0.006903 | Recon Loss: 0.005903 | Commit Loss: 0.002001 | Perplexity: 1339.991077
2025-09-26 20:36:59,505 Stage: Train 0.5 | Epoch: 63 | Iter: 97200 | Total Loss: 0.006922 | Recon Loss: 0.005921 | Commit Loss: 0.002001 | Perplexity: 1341.643550
Trainning Epoch:  19%|█▉        | 64/330 [6:25:53<25:37:47, 346.87s/it]2025-09-26 20:37:44,989 Stage: Train 0.5 | Epoch: 64 | Iter: 97400 | Total Loss: 0.006871 | Recon Loss: 0.005870 | Commit Loss: 0.002002 | Perplexity: 1341.312095
2025-09-26 20:38:30,709 Stage: Train 0.5 | Epoch: 64 | Iter: 97600 | Total Loss: 0.006868 | Recon Loss: 0.005867 | Commit Loss: 0.002002 | Perplexity: 1338.884539
2025-09-26 20:39:16,545 Stage: Train 0.5 | Epoch: 64 | Iter: 97800 | Total Loss: 0.006922 | Recon Loss: 0.005926 | Commit Loss: 0.001991 | Perplexity: 1339.781923
2025-09-26 20:40:02,414 Stage: Train 0.5 | Epoch: 64 | Iter: 98000 | Total Loss: 0.006910 | Recon Loss: 0.005902 | Commit Loss: 0.002015 | Perplexity: 1341.684549
2025-09-26 20:40:48,147 Stage: Train 0.5 | Epoch: 64 | Iter: 98200 | Total Loss: 0.006863 | Recon Loss: 0.005853 | Commit Loss: 0.002020 | Perplexity: 1341.368956
2025-09-26 20:41:33,887 Stage: Train 0.5 | Epoch: 64 | Iter: 98400 | Total Loss: 0.006864 | Recon Loss: 0.005868 | Commit Loss: 0.001993 | Perplexity: 1337.212539
2025-09-26 20:42:19,657 Stage: Train 0.5 | Epoch: 64 | Iter: 98600 | Total Loss: 0.006825 | Recon Loss: 0.005812 | Commit Loss: 0.002026 | Perplexity: 1342.397908
Trainning Epoch:  20%|█▉        | 65/330 [6:31:41<25:32:41, 347.02s/it]2025-09-26 20:43:05,598 Stage: Train 0.5 | Epoch: 65 | Iter: 98800 | Total Loss: 0.006936 | Recon Loss: 0.005932 | Commit Loss: 0.002008 | Perplexity: 1341.264059
2025-09-26 20:43:51,508 Stage: Train 0.5 | Epoch: 65 | Iter: 99000 | Total Loss: 0.006841 | Recon Loss: 0.005839 | Commit Loss: 0.002005 | Perplexity: 1344.093159
2025-09-26 20:44:37,166 Stage: Train 0.5 | Epoch: 65 | Iter: 99200 | Total Loss: 0.006810 | Recon Loss: 0.005801 | Commit Loss: 0.002017 | Perplexity: 1344.800599
2025-09-26 20:45:22,990 Stage: Train 0.5 | Epoch: 65 | Iter: 99400 | Total Loss: 0.006854 | Recon Loss: 0.005851 | Commit Loss: 0.002006 | Perplexity: 1338.772679
2025-09-26 20:46:08,704 Stage: Train 0.5 | Epoch: 65 | Iter: 99600 | Total Loss: 0.006894 | Recon Loss: 0.005882 | Commit Loss: 0.002024 | Perplexity: 1343.375611
2025-09-26 20:46:54,483 Stage: Train 0.5 | Epoch: 65 | Iter: 99800 | Total Loss: 0.006828 | Recon Loss: 0.005827 | Commit Loss: 0.002002 | Perplexity: 1338.966790
2025-09-26 20:47:39,960 Stage: Train 0.5 | Epoch: 65 | Iter: 100000 | Total Loss: 0.006917 | Recon Loss: 0.005914 | Commit Loss: 0.002006 | Perplexity: 1337.729677
2025-09-26 20:47:39,960 Saving model at iteration 100000
2025-09-26 20:47:40,487 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000
2025-09-26 20:47:40,807 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/model.safetensors
2025-09-26 20:47:41,236 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/optimizer.bin
2025-09-26 20:47:41,236 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/scheduler.bin
2025-09-26 20:47:41,236 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/sampler.bin
2025-09-26 20:47:41,237 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/random_states_0.pkl
2025-09-26 20:48:27,291 Stage: Train 0.5 | Epoch: 65 | Iter: 100200 | Total Loss: 0.006884 | Recon Loss: 0.005873 | Commit Loss: 0.002022 | Perplexity: 1340.184340
Trainning Epoch:  20%|██        | 66/330 [6:37:30<25:29:47, 347.68s/it]2025-09-26 20:49:13,439 Stage: Train 0.5 | Epoch: 66 | Iter: 100400 | Total Loss: 0.006821 | Recon Loss: 0.005809 | Commit Loss: 0.002023 | Perplexity: 1340.519887
2025-09-26 20:49:58,900 Stage: Train 0.5 | Epoch: 66 | Iter: 100600 | Total Loss: 0.006862 | Recon Loss: 0.005849 | Commit Loss: 0.002026 | Perplexity: 1342.486849
2025-09-26 20:50:44,719 Stage: Train 0.5 | Epoch: 66 | Iter: 100800 | Total Loss: 0.006923 | Recon Loss: 0.005914 | Commit Loss: 0.002017 | Perplexity: 1342.694514
2025-09-26 20:51:30,611 Stage: Train 0.5 | Epoch: 66 | Iter: 101000 | Total Loss: 0.006796 | Recon Loss: 0.005791 | Commit Loss: 0.002010 | Perplexity: 1342.179974
2025-09-26 20:52:16,461 Stage: Train 0.5 | Epoch: 66 | Iter: 101200 | Total Loss: 0.006864 | Recon Loss: 0.005862 | Commit Loss: 0.002005 | Perplexity: 1339.627399
2025-09-26 20:53:02,253 Stage: Train 0.5 | Epoch: 66 | Iter: 101400 | Total Loss: 0.007001 | Recon Loss: 0.005998 | Commit Loss: 0.002006 | Perplexity: 1339.328925
2025-09-26 20:53:48,034 Stage: Train 0.5 | Epoch: 66 | Iter: 101600 | Total Loss: 0.006876 | Recon Loss: 0.005873 | Commit Loss: 0.002006 | Perplexity: 1339.270059
Trainning Epoch:  20%|██        | 67/330 [6:43:18<25:23:56, 347.67s/it]2025-09-26 20:54:33,723 Stage: Train 0.5 | Epoch: 67 | Iter: 101800 | Total Loss: 0.006860 | Recon Loss: 0.005858 | Commit Loss: 0.002004 | Perplexity: 1336.921530
2025-09-26 20:55:19,543 Stage: Train 0.5 | Epoch: 67 | Iter: 102000 | Total Loss: 0.006793 | Recon Loss: 0.005785 | Commit Loss: 0.002014 | Perplexity: 1339.423491
2025-09-26 20:56:05,440 Stage: Train 0.5 | Epoch: 67 | Iter: 102200 | Total Loss: 0.006894 | Recon Loss: 0.005892 | Commit Loss: 0.002003 | Perplexity: 1339.707125
2025-09-26 20:56:51,362 Stage: Train 0.5 | Epoch: 67 | Iter: 102400 | Total Loss: 0.006780 | Recon Loss: 0.005778 | Commit Loss: 0.002003 | Perplexity: 1342.694501
2025-09-26 20:57:37,029 Stage: Train 0.5 | Epoch: 67 | Iter: 102600 | Total Loss: 0.006810 | Recon Loss: 0.005805 | Commit Loss: 0.002009 | Perplexity: 1339.000829
2025-09-26 20:58:22,703 Stage: Train 0.5 | Epoch: 67 | Iter: 102800 | Total Loss: 0.006841 | Recon Loss: 0.005829 | Commit Loss: 0.002023 | Perplexity: 1339.143311
2025-09-26 20:59:08,276 Stage: Train 0.5 | Epoch: 67 | Iter: 103000 | Total Loss: 0.006839 | Recon Loss: 0.005829 | Commit Loss: 0.002021 | Perplexity: 1341.518699
2025-09-26 20:59:53,972 Stage: Train 0.5 | Epoch: 67 | Iter: 103200 | Total Loss: 0.006858 | Recon Loss: 0.005839 | Commit Loss: 0.002037 | Perplexity: 1342.041976
Trainning Epoch:  21%|██        | 68/330 [6:49:05<25:17:55, 347.62s/it]2025-09-26 21:00:39,693 Stage: Train 0.5 | Epoch: 68 | Iter: 103400 | Total Loss: 0.006825 | Recon Loss: 0.005814 | Commit Loss: 0.002021 | Perplexity: 1339.598101
2025-09-26 21:01:25,112 Stage: Train 0.5 | Epoch: 68 | Iter: 103600 | Total Loss: 0.006858 | Recon Loss: 0.005851 | Commit Loss: 0.002013 | Perplexity: 1339.362026
2025-09-26 21:02:10,978 Stage: Train 0.5 | Epoch: 68 | Iter: 103800 | Total Loss: 0.006727 | Recon Loss: 0.005720 | Commit Loss: 0.002014 | Perplexity: 1338.078182
2025-09-26 21:02:56,938 Stage: Train 0.5 | Epoch: 68 | Iter: 104000 | Total Loss: 0.006768 | Recon Loss: 0.005760 | Commit Loss: 0.002016 | Perplexity: 1339.065167
2025-09-26 21:03:42,739 Stage: Train 0.5 | Epoch: 68 | Iter: 104200 | Total Loss: 0.006833 | Recon Loss: 0.005829 | Commit Loss: 0.002007 | Perplexity: 1339.852265
2025-09-26 21:04:28,004 Stage: Train 0.5 | Epoch: 68 | Iter: 104400 | Total Loss: 0.006770 | Recon Loss: 0.005759 | Commit Loss: 0.002023 | Perplexity: 1339.142241
2025-09-26 21:05:13,862 Stage: Train 0.5 | Epoch: 68 | Iter: 104600 | Total Loss: 0.006797 | Recon Loss: 0.005785 | Commit Loss: 0.002025 | Perplexity: 1343.985660
2025-09-26 21:05:58,646 Stage: Train 0.5 | Epoch: 68 | Iter: 104800 | Total Loss: 0.006862 | Recon Loss: 0.005854 | Commit Loss: 0.002017 | Perplexity: 1340.962044
Trainning Epoch:  21%|██        | 69/330 [6:54:51<25:10:21, 347.21s/it]2025-09-26 21:06:44,687 Stage: Train 0.5 | Epoch: 69 | Iter: 105000 | Total Loss: 0.006761 | Recon Loss: 0.005749 | Commit Loss: 0.002025 | Perplexity: 1342.375177
2025-09-26 21:07:30,124 Stage: Train 0.5 | Epoch: 69 | Iter: 105200 | Total Loss: 0.006858 | Recon Loss: 0.005845 | Commit Loss: 0.002026 | Perplexity: 1341.754319
2025-09-26 21:08:15,670 Stage: Train 0.5 | Epoch: 69 | Iter: 105400 | Total Loss: 0.006747 | Recon Loss: 0.005739 | Commit Loss: 0.002017 | Perplexity: 1341.441205
2025-09-26 21:09:01,087 Stage: Train 0.5 | Epoch: 69 | Iter: 105600 | Total Loss: 0.006859 | Recon Loss: 0.005849 | Commit Loss: 0.002020 | Perplexity: 1343.181125
2025-09-26 21:09:46,888 Stage: Train 0.5 | Epoch: 69 | Iter: 105800 | Total Loss: 0.006789 | Recon Loss: 0.005782 | Commit Loss: 0.002015 | Perplexity: 1339.348000
2025-09-26 21:10:32,672 Stage: Train 0.5 | Epoch: 69 | Iter: 106000 | Total Loss: 0.006748 | Recon Loss: 0.005734 | Commit Loss: 0.002028 | Perplexity: 1338.373400
2025-09-26 21:11:18,079 Stage: Train 0.5 | Epoch: 69 | Iter: 106200 | Total Loss: 0.006740 | Recon Loss: 0.005737 | Commit Loss: 0.002004 | Perplexity: 1339.696821
Trainning Epoch:  21%|██        | 70/330 [7:00:38<25:03:55, 347.06s/it]2025-09-26 21:12:03,970 Stage: Train 0.5 | Epoch: 70 | Iter: 106400 | Total Loss: 0.006850 | Recon Loss: 0.005838 | Commit Loss: 0.002023 | Perplexity: 1338.090869
2025-09-26 21:12:49,891 Stage: Train 0.5 | Epoch: 70 | Iter: 106600 | Total Loss: 0.006717 | Recon Loss: 0.005705 | Commit Loss: 0.002025 | Perplexity: 1340.491271
2025-09-26 21:13:35,696 Stage: Train 0.5 | Epoch: 70 | Iter: 106800 | Total Loss: 0.006837 | Recon Loss: 0.005826 | Commit Loss: 0.002022 | Perplexity: 1341.397562
2025-09-26 21:14:21,267 Stage: Train 0.5 | Epoch: 70 | Iter: 107000 | Total Loss: 0.006733 | Recon Loss: 0.005729 | Commit Loss: 0.002009 | Perplexity: 1339.778161
2025-09-26 21:15:07,121 Stage: Train 0.5 | Epoch: 70 | Iter: 107200 | Total Loss: 0.006829 | Recon Loss: 0.005816 | Commit Loss: 0.002025 | Perplexity: 1340.969149
2025-09-26 21:15:53,149 Stage: Train 0.5 | Epoch: 70 | Iter: 107400 | Total Loss: 0.006739 | Recon Loss: 0.005726 | Commit Loss: 0.002026 | Perplexity: 1340.259672
2025-09-26 21:16:38,575 Stage: Train 0.5 | Epoch: 70 | Iter: 107600 | Total Loss: 0.006739 | Recon Loss: 0.005723 | Commit Loss: 0.002031 | Perplexity: 1342.531233
2025-09-26 21:17:24,425 Stage: Train 0.5 | Epoch: 70 | Iter: 107800 | Total Loss: 0.006738 | Recon Loss: 0.005722 | Commit Loss: 0.002032 | Perplexity: 1338.352855
Trainning Epoch:  22%|██▏       | 71/330 [7:06:26<24:58:46, 347.21s/it]2025-09-26 21:18:10,267 Stage: Train 0.5 | Epoch: 71 | Iter: 108000 | Total Loss: 0.006764 | Recon Loss: 0.005746 | Commit Loss: 0.002037 | Perplexity: 1340.354081
2025-09-26 21:18:56,190 Stage: Train 0.5 | Epoch: 71 | Iter: 108200 | Total Loss: 0.006688 | Recon Loss: 0.005677 | Commit Loss: 0.002022 | Perplexity: 1342.617171
2025-09-26 21:19:41,899 Stage: Train 0.5 | Epoch: 71 | Iter: 108400 | Total Loss: 0.006758 | Recon Loss: 0.005742 | Commit Loss: 0.002032 | Perplexity: 1342.484866
2025-09-26 21:20:27,715 Stage: Train 0.5 | Epoch: 71 | Iter: 108600 | Total Loss: 0.006714 | Recon Loss: 0.005702 | Commit Loss: 0.002024 | Perplexity: 1344.871078
2025-09-26 21:21:13,327 Stage: Train 0.5 | Epoch: 71 | Iter: 108800 | Total Loss: 0.006733 | Recon Loss: 0.005721 | Commit Loss: 0.002024 | Perplexity: 1341.214760
2025-09-26 21:21:59,099 Stage: Train 0.5 | Epoch: 71 | Iter: 109000 | Total Loss: 0.006742 | Recon Loss: 0.005722 | Commit Loss: 0.002041 | Perplexity: 1342.560343
2025-09-26 21:22:44,942 Stage: Train 0.5 | Epoch: 71 | Iter: 109200 | Total Loss: 0.006798 | Recon Loss: 0.005778 | Commit Loss: 0.002039 | Perplexity: 1340.506327
Trainning Epoch:  22%|██▏       | 72/330 [7:12:13<24:53:46, 347.39s/it]2025-09-26 21:23:30,635 Stage: Train 0.5 | Epoch: 72 | Iter: 109400 | Total Loss: 0.006716 | Recon Loss: 0.005701 | Commit Loss: 0.002030 | Perplexity: 1341.785953
2025-09-26 21:24:15,887 Stage: Train 0.5 | Epoch: 72 | Iter: 109600 | Total Loss: 0.006751 | Recon Loss: 0.005740 | Commit Loss: 0.002022 | Perplexity: 1342.759216
2025-09-26 21:25:01,792 Stage: Train 0.5 | Epoch: 72 | Iter: 109800 | Total Loss: 0.006792 | Recon Loss: 0.005775 | Commit Loss: 0.002033 | Perplexity: 1341.156129
2025-09-26 21:25:47,649 Stage: Train 0.5 | Epoch: 72 | Iter: 110000 | Total Loss: 0.006670 | Recon Loss: 0.005659 | Commit Loss: 0.002022 | Perplexity: 1340.227380
2025-09-26 21:26:33,588 Stage: Train 0.5 | Epoch: 72 | Iter: 110200 | Total Loss: 0.006637 | Recon Loss: 0.005625 | Commit Loss: 0.002024 | Perplexity: 1340.065653
2025-09-26 21:27:19,510 Stage: Train 0.5 | Epoch: 72 | Iter: 110400 | Total Loss: 0.006697 | Recon Loss: 0.005689 | Commit Loss: 0.002016 | Perplexity: 1336.414321
2025-09-26 21:28:05,200 Stage: Train 0.5 | Epoch: 72 | Iter: 110600 | Total Loss: 0.006798 | Recon Loss: 0.005771 | Commit Loss: 0.002055 | Perplexity: 1344.023407
2025-09-26 21:28:50,792 Stage: Train 0.5 | Epoch: 72 | Iter: 110800 | Total Loss: 0.006760 | Recon Loss: 0.005737 | Commit Loss: 0.002045 | Perplexity: 1342.884932
Trainning Epoch:  22%|██▏       | 73/330 [7:18:01<24:48:00, 347.40s/it]2025-09-26 21:29:36,646 Stage: Train 0.5 | Epoch: 73 | Iter: 111000 | Total Loss: 0.006737 | Recon Loss: 0.005709 | Commit Loss: 0.002055 | Perplexity: 1343.841732
2025-09-26 21:30:22,354 Stage: Train 0.5 | Epoch: 73 | Iter: 111200 | Total Loss: 0.006864 | Recon Loss: 0.005840 | Commit Loss: 0.002047 | Perplexity: 1345.703954
2025-09-26 21:31:07,776 Stage: Train 0.5 | Epoch: 73 | Iter: 111400 | Total Loss: 0.006603 | Recon Loss: 0.005592 | Commit Loss: 0.002023 | Perplexity: 1340.926576
2025-09-26 21:31:53,412 Stage: Train 0.5 | Epoch: 73 | Iter: 111600 | Total Loss: 0.006746 | Recon Loss: 0.005733 | Commit Loss: 0.002025 | Perplexity: 1339.834548
2025-09-26 21:32:39,020 Stage: Train 0.5 | Epoch: 73 | Iter: 111800 | Total Loss: 0.006650 | Recon Loss: 0.005623 | Commit Loss: 0.002053 | Perplexity: 1344.264138
2025-09-26 21:33:24,784 Stage: Train 0.5 | Epoch: 73 | Iter: 112000 | Total Loss: 0.006662 | Recon Loss: 0.005644 | Commit Loss: 0.002037 | Perplexity: 1340.704897
2025-09-26 21:34:10,309 Stage: Train 0.5 | Epoch: 73 | Iter: 112200 | Total Loss: 0.006644 | Recon Loss: 0.005623 | Commit Loss: 0.002043 | Perplexity: 1341.532379
2025-09-26 21:34:55,671 Stage: Train 0.5 | Epoch: 73 | Iter: 112400 | Total Loss: 0.006786 | Recon Loss: 0.005772 | Commit Loss: 0.002029 | Perplexity: 1342.989859
Trainning Epoch:  22%|██▏       | 74/330 [7:23:47<24:40:57, 347.10s/it]2025-09-26 21:35:41,699 Stage: Train 0.5 | Epoch: 74 | Iter: 112600 | Total Loss: 0.006668 | Recon Loss: 0.005650 | Commit Loss: 0.002036 | Perplexity: 1340.185578
2025-09-26 21:36:27,574 Stage: Train 0.5 | Epoch: 74 | Iter: 112800 | Total Loss: 0.006904 | Recon Loss: 0.005881 | Commit Loss: 0.002045 | Perplexity: 1345.899223
2025-09-26 21:37:13,176 Stage: Train 0.5 | Epoch: 74 | Iter: 113000 | Total Loss: 0.006609 | Recon Loss: 0.005598 | Commit Loss: 0.002021 | Perplexity: 1336.668211
2025-09-26 21:37:58,631 Stage: Train 0.5 | Epoch: 74 | Iter: 113200 | Total Loss: 0.006626 | Recon Loss: 0.005604 | Commit Loss: 0.002042 | Perplexity: 1344.542811
2025-09-26 21:38:44,472 Stage: Train 0.5 | Epoch: 74 | Iter: 113400 | Total Loss: 0.006661 | Recon Loss: 0.005649 | Commit Loss: 0.002024 | Perplexity: 1338.344021
2025-09-26 21:39:30,341 Stage: Train 0.5 | Epoch: 74 | Iter: 113600 | Total Loss: 0.006676 | Recon Loss: 0.005654 | Commit Loss: 0.002043 | Perplexity: 1342.102697
2025-09-26 21:40:16,061 Stage: Train 0.5 | Epoch: 74 | Iter: 113800 | Total Loss: 0.006682 | Recon Loss: 0.005668 | Commit Loss: 0.002029 | Perplexity: 1340.762182
Trainning Epoch:  23%|██▎       | 75/330 [7:29:35<24:35:56, 347.28s/it]2025-09-26 21:41:01,830 Stage: Train 0.5 | Epoch: 75 | Iter: 114000 | Total Loss: 0.006778 | Recon Loss: 0.005752 | Commit Loss: 0.002052 | Perplexity: 1339.897123
2025-09-26 21:41:47,510 Stage: Train 0.5 | Epoch: 75 | Iter: 114200 | Total Loss: 0.006661 | Recon Loss: 0.005644 | Commit Loss: 0.002034 | Perplexity: 1340.216099
2025-09-26 21:42:33,167 Stage: Train 0.5 | Epoch: 75 | Iter: 114400 | Total Loss: 0.006657 | Recon Loss: 0.005636 | Commit Loss: 0.002042 | Perplexity: 1341.869171
2025-09-26 21:43:18,849 Stage: Train 0.5 | Epoch: 75 | Iter: 114600 | Total Loss: 0.006633 | Recon Loss: 0.005614 | Commit Loss: 0.002039 | Perplexity: 1343.386182
2025-09-26 21:44:04,746 Stage: Train 0.5 | Epoch: 75 | Iter: 114800 | Total Loss: 0.006762 | Recon Loss: 0.005743 | Commit Loss: 0.002037 | Perplexity: 1340.353856
2025-09-26 21:44:50,273 Stage: Train 0.5 | Epoch: 75 | Iter: 115000 | Total Loss: 0.006576 | Recon Loss: 0.005555 | Commit Loss: 0.002043 | Perplexity: 1341.900463
2025-09-26 21:45:36,026 Stage: Train 0.5 | Epoch: 75 | Iter: 115200 | Total Loss: 0.006552 | Recon Loss: 0.005532 | Commit Loss: 0.002040 | Perplexity: 1342.240620
2025-09-26 21:46:21,964 Stage: Train 0.5 | Epoch: 75 | Iter: 115400 | Total Loss: 0.006680 | Recon Loss: 0.005655 | Commit Loss: 0.002051 | Perplexity: 1344.156011
Trainning Epoch:  23%|██▎       | 76/330 [7:35:22<24:30:14, 347.30s/it]2025-09-26 21:47:08,123 Stage: Train 0.5 | Epoch: 76 | Iter: 115600 | Total Loss: 0.006608 | Recon Loss: 0.005589 | Commit Loss: 0.002039 | Perplexity: 1342.893441
2025-09-26 21:47:53,649 Stage: Train 0.5 | Epoch: 76 | Iter: 115800 | Total Loss: 0.006641 | Recon Loss: 0.005621 | Commit Loss: 0.002039 | Perplexity: 1341.819489
2025-09-26 21:48:39,516 Stage: Train 0.5 | Epoch: 76 | Iter: 116000 | Total Loss: 0.006621 | Recon Loss: 0.005603 | Commit Loss: 0.002036 | Perplexity: 1340.586185
2025-09-26 21:49:25,207 Stage: Train 0.5 | Epoch: 76 | Iter: 116200 | Total Loss: 0.006612 | Recon Loss: 0.005595 | Commit Loss: 0.002035 | Perplexity: 1338.952731
2025-09-26 21:50:09,972 Stage: Train 0.5 | Epoch: 76 | Iter: 116400 | Total Loss: 0.006739 | Recon Loss: 0.005712 | Commit Loss: 0.002054 | Perplexity: 1339.950627
2025-09-26 21:50:55,652 Stage: Train 0.5 | Epoch: 76 | Iter: 116600 | Total Loss: 0.006658 | Recon Loss: 0.005635 | Commit Loss: 0.002047 | Perplexity: 1339.582308
2025-09-26 21:51:41,048 Stage: Train 0.5 | Epoch: 76 | Iter: 116800 | Total Loss: 0.006689 | Recon Loss: 0.005658 | Commit Loss: 0.002062 | Perplexity: 1343.767657
Trainning Epoch:  23%|██▎       | 77/330 [7:41:08<24:23:00, 346.96s/it]2025-09-26 21:52:26,955 Stage: Train 0.5 | Epoch: 77 | Iter: 117000 | Total Loss: 0.006595 | Recon Loss: 0.005575 | Commit Loss: 0.002039 | Perplexity: 1338.528571
2025-09-26 21:53:12,897 Stage: Train 0.5 | Epoch: 77 | Iter: 117200 | Total Loss: 0.006626 | Recon Loss: 0.005612 | Commit Loss: 0.002027 | Perplexity: 1337.879732
2025-09-26 21:53:58,735 Stage: Train 0.5 | Epoch: 77 | Iter: 117400 | Total Loss: 0.006618 | Recon Loss: 0.005595 | Commit Loss: 0.002045 | Perplexity: 1341.444621
2025-09-26 21:54:44,166 Stage: Train 0.5 | Epoch: 77 | Iter: 117600 | Total Loss: 0.006586 | Recon Loss: 0.005568 | Commit Loss: 0.002035 | Perplexity: 1339.034883
2025-09-26 21:55:29,724 Stage: Train 0.5 | Epoch: 77 | Iter: 117800 | Total Loss: 0.006634 | Recon Loss: 0.005614 | Commit Loss: 0.002040 | Perplexity: 1341.163922
2025-09-26 21:56:15,224 Stage: Train 0.5 | Epoch: 77 | Iter: 118000 | Total Loss: 0.006625 | Recon Loss: 0.005603 | Commit Loss: 0.002045 | Perplexity: 1340.621964
2025-09-26 21:57:01,240 Stage: Train 0.5 | Epoch: 77 | Iter: 118200 | Total Loss: 0.006691 | Recon Loss: 0.005661 | Commit Loss: 0.002059 | Perplexity: 1343.599934
2025-09-26 21:57:47,071 Stage: Train 0.5 | Epoch: 77 | Iter: 118400 | Total Loss: 0.006625 | Recon Loss: 0.005591 | Commit Loss: 0.002068 | Perplexity: 1339.517357
Trainning Epoch:  24%|██▎       | 78/330 [7:46:56<24:17:46, 347.09s/it]2025-09-26 21:58:32,987 Stage: Train 0.5 | Epoch: 78 | Iter: 118600 | Total Loss: 0.006656 | Recon Loss: 0.005629 | Commit Loss: 0.002054 | Perplexity: 1340.442010
2025-09-26 21:59:18,823 Stage: Train 0.5 | Epoch: 78 | Iter: 118800 | Total Loss: 0.006638 | Recon Loss: 0.005617 | Commit Loss: 0.002042 | Perplexity: 1341.294216
2025-09-26 22:00:04,344 Stage: Train 0.5 | Epoch: 78 | Iter: 119000 | Total Loss: 0.006612 | Recon Loss: 0.005594 | Commit Loss: 0.002036 | Perplexity: 1339.194539
2025-09-26 22:00:49,935 Stage: Train 0.5 | Epoch: 78 | Iter: 119200 | Total Loss: 0.006543 | Recon Loss: 0.005524 | Commit Loss: 0.002038 | Perplexity: 1338.447388
2025-09-26 22:01:35,422 Stage: Train 0.5 | Epoch: 78 | Iter: 119400 | Total Loss: 0.006616 | Recon Loss: 0.005594 | Commit Loss: 0.002045 | Perplexity: 1339.041362
2025-09-26 22:02:21,494 Stage: Train 0.5 | Epoch: 78 | Iter: 119600 | Total Loss: 0.006558 | Recon Loss: 0.005531 | Commit Loss: 0.002053 | Perplexity: 1342.235748
2025-09-26 22:03:07,065 Stage: Train 0.5 | Epoch: 78 | Iter: 119800 | Total Loss: 0.006598 | Recon Loss: 0.005568 | Commit Loss: 0.002061 | Perplexity: 1341.979368
2025-09-26 22:03:52,737 Stage: Train 0.5 | Epoch: 78 | Iter: 120000 | Total Loss: 0.006643 | Recon Loss: 0.005619 | Commit Loss: 0.002048 | Perplexity: 1341.480559
2025-09-26 22:03:52,738 Saving model at iteration 120000
2025-09-26 22:03:53,093 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000
2025-09-26 22:03:53,409 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/model.safetensors
2025-09-26 22:03:53,845 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/optimizer.bin
2025-09-26 22:03:53,845 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/scheduler.bin
2025-09-26 22:03:53,845 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/sampler.bin
2025-09-26 22:03:53,846 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/random_states_0.pkl
Trainning Epoch:  24%|██▍       | 79/330 [7:52:44<24:13:53, 347.54s/it]2025-09-26 22:04:39,800 Stage: Train 0.5 | Epoch: 79 | Iter: 120200 | Total Loss: 0.006653 | Recon Loss: 0.005622 | Commit Loss: 0.002062 | Perplexity: 1341.347607
2025-09-26 22:05:25,337 Stage: Train 0.5 | Epoch: 79 | Iter: 120400 | Total Loss: 0.006605 | Recon Loss: 0.005582 | Commit Loss: 0.002047 | Perplexity: 1339.437191
2025-09-26 22:06:11,023 Stage: Train 0.5 | Epoch: 79 | Iter: 120600 | Total Loss: 0.006536 | Recon Loss: 0.005517 | Commit Loss: 0.002037 | Perplexity: 1337.617556
2025-09-26 22:06:56,591 Stage: Train 0.5 | Epoch: 79 | Iter: 120800 | Total Loss: 0.006550 | Recon Loss: 0.005523 | Commit Loss: 0.002054 | Perplexity: 1341.941064
2025-09-26 22:07:42,546 Stage: Train 0.5 | Epoch: 79 | Iter: 121000 | Total Loss: 0.006594 | Recon Loss: 0.005567 | Commit Loss: 0.002054 | Perplexity: 1343.880427
2025-09-26 22:08:28,175 Stage: Train 0.5 | Epoch: 79 | Iter: 121200 | Total Loss: 0.006618 | Recon Loss: 0.005592 | Commit Loss: 0.002051 | Perplexity: 1339.307834
2025-09-26 22:09:13,937 Stage: Train 0.5 | Epoch: 79 | Iter: 121400 | Total Loss: 0.006597 | Recon Loss: 0.005577 | Commit Loss: 0.002040 | Perplexity: 1338.049372
Trainning Epoch:  24%|██▍       | 80/330 [7:58:32<24:07:37, 347.43s/it]2025-09-26 22:09:59,923 Stage: Train 0.5 | Epoch: 80 | Iter: 121600 | Total Loss: 0.006566 | Recon Loss: 0.005537 | Commit Loss: 0.002059 | Perplexity: 1342.309043
2025-09-26 22:10:45,848 Stage: Train 0.5 | Epoch: 80 | Iter: 121800 | Total Loss: 0.006563 | Recon Loss: 0.005541 | Commit Loss: 0.002042 | Perplexity: 1341.589921
2025-09-26 22:11:31,424 Stage: Train 0.5 | Epoch: 80 | Iter: 122000 | Total Loss: 0.006610 | Recon Loss: 0.005579 | Commit Loss: 0.002062 | Perplexity: 1341.238523
2025-09-26 22:12:17,236 Stage: Train 0.5 | Epoch: 80 | Iter: 122200 | Total Loss: 0.006580 | Recon Loss: 0.005552 | Commit Loss: 0.002057 | Perplexity: 1342.277083
2025-09-26 22:13:03,168 Stage: Train 0.5 | Epoch: 80 | Iter: 122400 | Total Loss: 0.006512 | Recon Loss: 0.005490 | Commit Loss: 0.002045 | Perplexity: 1341.509472
2025-09-26 22:13:48,992 Stage: Train 0.5 | Epoch: 80 | Iter: 122600 | Total Loss: 0.006599 | Recon Loss: 0.005568 | Commit Loss: 0.002061 | Perplexity: 1341.274923
2025-09-26 22:14:34,694 Stage: Train 0.5 | Epoch: 80 | Iter: 122800 | Total Loss: 0.006515 | Recon Loss: 0.005487 | Commit Loss: 0.002055 | Perplexity: 1342.704026
2025-09-26 22:15:20,478 Stage: Train 0.5 | Epoch: 80 | Iter: 123000 | Total Loss: 0.006562 | Recon Loss: 0.005534 | Commit Loss: 0.002055 | Perplexity: 1341.210181
Trainning Epoch:  25%|██▍       | 81/330 [8:04:20<24:02:29, 347.59s/it]2025-09-26 22:16:06,798 Stage: Train 0.5 | Epoch: 81 | Iter: 123200 | Total Loss: 0.006520 | Recon Loss: 0.005487 | Commit Loss: 0.002066 | Perplexity: 1341.187642
2025-09-26 22:16:52,675 Stage: Train 0.5 | Epoch: 81 | Iter: 123400 | Total Loss: 0.006543 | Recon Loss: 0.005513 | Commit Loss: 0.002060 | Perplexity: 1341.325055
2025-09-26 22:17:38,352 Stage: Train 0.5 | Epoch: 81 | Iter: 123600 | Total Loss: 0.006596 | Recon Loss: 0.005571 | Commit Loss: 0.002049 | Perplexity: 1339.987476
2025-09-26 22:18:23,612 Stage: Train 0.5 | Epoch: 81 | Iter: 123800 | Total Loss: 0.006538 | Recon Loss: 0.005515 | Commit Loss: 0.002046 | Perplexity: 1340.984883
2025-09-26 22:19:09,467 Stage: Train 0.5 | Epoch: 81 | Iter: 124000 | Total Loss: 0.006620 | Recon Loss: 0.005590 | Commit Loss: 0.002059 | Perplexity: 1341.102883
2025-09-26 22:19:55,269 Stage: Train 0.5 | Epoch: 81 | Iter: 124200 | Total Loss: 0.006576 | Recon Loss: 0.005547 | Commit Loss: 0.002059 | Perplexity: 1340.019230
2025-09-26 22:20:40,863 Stage: Train 0.5 | Epoch: 81 | Iter: 124400 | Total Loss: 0.006629 | Recon Loss: 0.005596 | Commit Loss: 0.002066 | Perplexity: 1340.643196
Trainning Epoch:  25%|██▍       | 82/330 [8:10:07<23:56:15, 347.48s/it]2025-09-26 22:21:26,421 Stage: Train 0.5 | Epoch: 82 | Iter: 124600 | Total Loss: 0.006514 | Recon Loss: 0.005487 | Commit Loss: 0.002054 | Perplexity: 1338.858830
2025-09-26 22:22:12,259 Stage: Train 0.5 | Epoch: 82 | Iter: 124800 | Total Loss: 0.006497 | Recon Loss: 0.005472 | Commit Loss: 0.002050 | Perplexity: 1340.659716
2025-09-26 22:22:57,933 Stage: Train 0.5 | Epoch: 82 | Iter: 125000 | Total Loss: 0.006562 | Recon Loss: 0.005538 | Commit Loss: 0.002047 | Perplexity: 1340.073277
2025-09-26 22:23:43,599 Stage: Train 0.5 | Epoch: 82 | Iter: 125200 | Total Loss: 0.006518 | Recon Loss: 0.005481 | Commit Loss: 0.002074 | Perplexity: 1343.035027
2025-09-26 22:24:29,472 Stage: Train 0.5 | Epoch: 82 | Iter: 125400 | Total Loss: 0.006542 | Recon Loss: 0.005510 | Commit Loss: 0.002064 | Perplexity: 1341.575074
2025-09-26 22:25:15,028 Stage: Train 0.5 | Epoch: 82 | Iter: 125600 | Total Loss: 0.006541 | Recon Loss: 0.005513 | Commit Loss: 0.002056 | Perplexity: 1341.526954
2025-09-26 22:26:00,937 Stage: Train 0.5 | Epoch: 82 | Iter: 125800 | Total Loss: 0.006503 | Recon Loss: 0.005474 | Commit Loss: 0.002057 | Perplexity: 1341.113017
2025-09-26 22:26:46,778 Stage: Train 0.5 | Epoch: 82 | Iter: 126000 | Total Loss: 0.006640 | Recon Loss: 0.005607 | Commit Loss: 0.002066 | Perplexity: 1340.916174
Trainning Epoch:  25%|██▌       | 83/330 [8:15:55<23:50:59, 347.61s/it]2025-09-26 22:27:32,645 Stage: Train 0.5 | Epoch: 83 | Iter: 126200 | Total Loss: 0.006505 | Recon Loss: 0.005484 | Commit Loss: 0.002043 | Perplexity: 1337.885912
2025-09-26 22:28:18,034 Stage: Train 0.5 | Epoch: 83 | Iter: 126400 | Total Loss: 0.006485 | Recon Loss: 0.005453 | Commit Loss: 0.002064 | Perplexity: 1341.625768
2025-09-26 22:29:03,775 Stage: Train 0.5 | Epoch: 83 | Iter: 126600 | Total Loss: 0.006565 | Recon Loss: 0.005541 | Commit Loss: 0.002049 | Perplexity: 1339.024924
2025-09-26 22:29:49,694 Stage: Train 0.5 | Epoch: 83 | Iter: 126800 | Total Loss: 0.006590 | Recon Loss: 0.005561 | Commit Loss: 0.002058 | Perplexity: 1339.455138
2025-09-26 22:30:35,566 Stage: Train 0.5 | Epoch: 83 | Iter: 127000 | Total Loss: 0.006530 | Recon Loss: 0.005502 | Commit Loss: 0.002056 | Perplexity: 1339.910394
2025-09-26 22:31:20,852 Stage: Train 0.5 | Epoch: 83 | Iter: 127200 | Total Loss: 0.006525 | Recon Loss: 0.005492 | Commit Loss: 0.002065 | Perplexity: 1342.662891
2025-09-26 22:32:06,350 Stage: Train 0.5 | Epoch: 83 | Iter: 127400 | Total Loss: 0.006530 | Recon Loss: 0.005495 | Commit Loss: 0.002070 | Perplexity: 1343.362228
Trainning Epoch:  25%|██▌       | 84/330 [8:21:41<23:43:46, 347.26s/it]2025-09-26 22:32:52,062 Stage: Train 0.5 | Epoch: 84 | Iter: 127600 | Total Loss: 0.006532 | Recon Loss: 0.005500 | Commit Loss: 0.002063 | Perplexity: 1341.688622
2025-09-26 22:33:37,855 Stage: Train 0.5 | Epoch: 84 | Iter: 127800 | Total Loss: 0.006497 | Recon Loss: 0.005472 | Commit Loss: 0.002050 | Perplexity: 1341.855792
2025-09-26 22:34:22,399 Stage: Train 0.5 | Epoch: 84 | Iter: 128000 | Total Loss: 0.006517 | Recon Loss: 0.005487 | Commit Loss: 0.002059 | Perplexity: 1340.243506
2025-09-26 22:35:07,614 Stage: Train 0.5 | Epoch: 84 | Iter: 128200 | Total Loss: 0.006529 | Recon Loss: 0.005494 | Commit Loss: 0.002070 | Perplexity: 1342.011387
2025-09-26 22:35:53,151 Stage: Train 0.5 | Epoch: 84 | Iter: 128400 | Total Loss: 0.006522 | Recon Loss: 0.005483 | Commit Loss: 0.002079 | Perplexity: 1345.670461
2025-09-26 22:36:38,856 Stage: Train 0.5 | Epoch: 84 | Iter: 128600 | Total Loss: 0.006455 | Recon Loss: 0.005430 | Commit Loss: 0.002051 | Perplexity: 1339.006250
2025-09-26 22:37:24,521 Stage: Train 0.5 | Epoch: 84 | Iter: 128800 | Total Loss: 0.006503 | Recon Loss: 0.005463 | Commit Loss: 0.002079 | Perplexity: 1341.735026
2025-09-26 22:38:10,322 Stage: Train 0.5 | Epoch: 84 | Iter: 129000 | Total Loss: 0.006453 | Recon Loss: 0.005417 | Commit Loss: 0.002073 | Perplexity: 1342.989781
Trainning Epoch:  26%|██▌       | 85/330 [8:27:27<23:35:57, 346.76s/it]2025-09-26 22:38:56,187 Stage: Train 0.5 | Epoch: 85 | Iter: 129200 | Total Loss: 0.006589 | Recon Loss: 0.005558 | Commit Loss: 0.002062 | Perplexity: 1341.346248
2025-09-26 22:39:42,006 Stage: Train 0.5 | Epoch: 85 | Iter: 129400 | Total Loss: 0.006463 | Recon Loss: 0.005430 | Commit Loss: 0.002066 | Perplexity: 1345.027639
2025-09-26 22:40:27,841 Stage: Train 0.5 | Epoch: 85 | Iter: 129600 | Total Loss: 0.006460 | Recon Loss: 0.005429 | Commit Loss: 0.002064 | Perplexity: 1341.946336
2025-09-26 22:41:13,597 Stage: Train 0.5 | Epoch: 85 | Iter: 129800 | Total Loss: 0.006477 | Recon Loss: 0.005445 | Commit Loss: 0.002063 | Perplexity: 1340.382886
2025-09-26 22:41:58,929 Stage: Train 0.5 | Epoch: 85 | Iter: 130000 | Total Loss: 0.006545 | Recon Loss: 0.005504 | Commit Loss: 0.002082 | Perplexity: 1342.344437
2025-09-26 22:42:44,343 Stage: Train 0.5 | Epoch: 85 | Iter: 130200 | Total Loss: 0.006402 | Recon Loss: 0.005376 | Commit Loss: 0.002051 | Perplexity: 1338.612991
2025-09-26 22:43:29,761 Stage: Train 0.5 | Epoch: 85 | Iter: 130400 | Total Loss: 0.006579 | Recon Loss: 0.005548 | Commit Loss: 0.002062 | Perplexity: 1342.220649
2025-09-26 22:44:15,613 Stage: Train 0.5 | Epoch: 85 | Iter: 130600 | Total Loss: 0.006467 | Recon Loss: 0.005435 | Commit Loss: 0.002063 | Perplexity: 1341.068830
Trainning Epoch:  26%|██▌       | 86/330 [8:33:14<23:30:15, 346.78s/it]2025-09-26 22:45:01,300 Stage: Train 0.5 | Epoch: 86 | Iter: 130800 | Total Loss: 0.006474 | Recon Loss: 0.005444 | Commit Loss: 0.002061 | Perplexity: 1339.775356
2025-09-26 22:45:46,976 Stage: Train 0.5 | Epoch: 86 | Iter: 131000 | Total Loss: 0.006478 | Recon Loss: 0.005446 | Commit Loss: 0.002065 | Perplexity: 1342.494077
2025-09-26 22:46:32,799 Stage: Train 0.5 | Epoch: 86 | Iter: 131200 | Total Loss: 0.006463 | Recon Loss: 0.005424 | Commit Loss: 0.002079 | Perplexity: 1342.803613
2025-09-26 22:47:18,568 Stage: Train 0.5 | Epoch: 86 | Iter: 131400 | Total Loss: 0.006424 | Recon Loss: 0.005391 | Commit Loss: 0.002066 | Perplexity: 1342.229712
2025-09-26 22:48:04,266 Stage: Train 0.5 | Epoch: 86 | Iter: 131600 | Total Loss: 0.006486 | Recon Loss: 0.005448 | Commit Loss: 0.002075 | Perplexity: 1343.838108
2025-09-26 22:48:49,738 Stage: Train 0.5 | Epoch: 86 | Iter: 131800 | Total Loss: 0.006518 | Recon Loss: 0.005478 | Commit Loss: 0.002080 | Perplexity: 1345.432405
2025-09-26 22:49:35,499 Stage: Train 0.5 | Epoch: 86 | Iter: 132000 | Total Loss: 0.006507 | Recon Loss: 0.005468 | Commit Loss: 0.002077 | Perplexity: 1342.492512
Trainning Epoch:  26%|██▋       | 87/330 [8:39:01<23:24:49, 346.87s/it]2025-09-26 22:50:21,307 Stage: Train 0.5 | Epoch: 87 | Iter: 132200 | Total Loss: 0.006407 | Recon Loss: 0.005373 | Commit Loss: 0.002067 | Perplexity: 1341.714625
2025-09-26 22:51:06,971 Stage: Train 0.5 | Epoch: 87 | Iter: 132400 | Total Loss: 0.006515 | Recon Loss: 0.005479 | Commit Loss: 0.002072 | Perplexity: 1342.438597
2025-09-26 22:51:52,359 Stage: Train 0.5 | Epoch: 87 | Iter: 132600 | Total Loss: 0.006449 | Recon Loss: 0.005417 | Commit Loss: 0.002065 | Perplexity: 1343.371826
2025-09-26 22:52:37,852 Stage: Train 0.5 | Epoch: 87 | Iter: 132800 | Total Loss: 0.006432 | Recon Loss: 0.005402 | Commit Loss: 0.002058 | Perplexity: 1340.750406
2025-09-26 22:53:23,357 Stage: Train 0.5 | Epoch: 87 | Iter: 133000 | Total Loss: 0.006460 | Recon Loss: 0.005419 | Commit Loss: 0.002082 | Perplexity: 1345.705687
2025-09-26 22:54:09,004 Stage: Train 0.5 | Epoch: 87 | Iter: 133200 | Total Loss: 0.006437 | Recon Loss: 0.005409 | Commit Loss: 0.002056 | Perplexity: 1342.898250
2025-09-26 22:54:54,416 Stage: Train 0.5 | Epoch: 87 | Iter: 133400 | Total Loss: 0.006430 | Recon Loss: 0.005400 | Commit Loss: 0.002061 | Perplexity: 1339.157784
2025-09-26 22:55:40,274 Stage: Train 0.5 | Epoch: 87 | Iter: 133600 | Total Loss: 0.006558 | Recon Loss: 0.005519 | Commit Loss: 0.002078 | Perplexity: 1342.568276
Trainning Epoch:  27%|██▋       | 88/330 [8:44:47<23:18:20, 346.70s/it]2025-09-26 22:56:26,107 Stage: Train 0.5 | Epoch: 88 | Iter: 133800 | Total Loss: 0.006460 | Recon Loss: 0.005419 | Commit Loss: 0.002083 | Perplexity: 1342.171707
2025-09-26 22:57:11,727 Stage: Train 0.5 | Epoch: 88 | Iter: 134000 | Total Loss: 0.006391 | Recon Loss: 0.005357 | Commit Loss: 0.002069 | Perplexity: 1341.746638
2025-09-26 22:57:57,290 Stage: Train 0.5 | Epoch: 88 | Iter: 134200 | Total Loss: 0.006515 | Recon Loss: 0.005485 | Commit Loss: 0.002060 | Perplexity: 1340.716601
2025-09-26 22:58:42,901 Stage: Train 0.5 | Epoch: 88 | Iter: 134400 | Total Loss: 0.006453 | Recon Loss: 0.005416 | Commit Loss: 0.002074 | Perplexity: 1343.590460
2025-09-26 22:59:28,678 Stage: Train 0.5 | Epoch: 88 | Iter: 134600 | Total Loss: 0.006484 | Recon Loss: 0.005445 | Commit Loss: 0.002079 | Perplexity: 1340.581202
2025-09-26 23:00:14,328 Stage: Train 0.5 | Epoch: 88 | Iter: 134800 | Total Loss: 0.006409 | Recon Loss: 0.005384 | Commit Loss: 0.002051 | Perplexity: 1341.591172
2025-09-26 23:00:59,795 Stage: Train 0.5 | Epoch: 88 | Iter: 135000 | Total Loss: 0.006421 | Recon Loss: 0.005384 | Commit Loss: 0.002074 | Perplexity: 1347.136902
Trainning Epoch:  27%|██▋       | 89/330 [8:50:33<23:11:49, 346.51s/it]2025-09-26 23:01:45,082 Stage: Train 0.5 | Epoch: 89 | Iter: 135200 | Total Loss: 0.006518 | Recon Loss: 0.005475 | Commit Loss: 0.002086 | Perplexity: 1342.576896
2025-09-26 23:02:30,679 Stage: Train 0.5 | Epoch: 89 | Iter: 135400 | Total Loss: 0.006433 | Recon Loss: 0.005398 | Commit Loss: 0.002069 | Perplexity: 1340.371232
2025-09-26 23:03:16,151 Stage: Train 0.5 | Epoch: 89 | Iter: 135600 | Total Loss: 0.006387 | Recon Loss: 0.005349 | Commit Loss: 0.002077 | Perplexity: 1346.074394
2025-09-26 23:04:01,574 Stage: Train 0.5 | Epoch: 89 | Iter: 135800 | Total Loss: 0.006441 | Recon Loss: 0.005404 | Commit Loss: 0.002076 | Perplexity: 1342.591525
2025-09-26 23:04:47,192 Stage: Train 0.5 | Epoch: 89 | Iter: 136000 | Total Loss: 0.006501 | Recon Loss: 0.005461 | Commit Loss: 0.002080 | Perplexity: 1344.079615
2025-09-26 23:05:32,516 Stage: Train 0.5 | Epoch: 89 | Iter: 136200 | Total Loss: 0.006435 | Recon Loss: 0.005401 | Commit Loss: 0.002069 | Perplexity: 1342.134780
2025-09-26 23:06:18,031 Stage: Train 0.5 | Epoch: 89 | Iter: 136400 | Total Loss: 0.006409 | Recon Loss: 0.005375 | Commit Loss: 0.002066 | Perplexity: 1341.341226
2025-09-26 23:07:03,785 Stage: Train 0.5 | Epoch: 89 | Iter: 136600 | Total Loss: 0.006334 | Recon Loss: 0.005295 | Commit Loss: 0.002078 | Perplexity: 1345.423418
Trainning Epoch:  27%|██▋       | 90/330 [8:56:19<23:05:53, 346.47s/it]2025-09-26 23:07:49,939 Stage: Train 0.5 | Epoch: 90 | Iter: 136800 | Total Loss: 0.006480 | Recon Loss: 0.005444 | Commit Loss: 0.002073 | Perplexity: 1342.204537
2025-09-26 23:08:35,300 Stage: Train 0.5 | Epoch: 90 | Iter: 137000 | Total Loss: 0.006385 | Recon Loss: 0.005353 | Commit Loss: 0.002064 | Perplexity: 1340.744496
2025-09-26 23:09:20,914 Stage: Train 0.5 | Epoch: 90 | Iter: 137200 | Total Loss: 0.006404 | Recon Loss: 0.005363 | Commit Loss: 0.002082 | Perplexity: 1347.360739
2025-09-26 23:10:06,481 Stage: Train 0.5 | Epoch: 90 | Iter: 137400 | Total Loss: 0.006436 | Recon Loss: 0.005405 | Commit Loss: 0.002063 | Perplexity: 1342.988546
2025-09-26 23:10:52,159 Stage: Train 0.5 | Epoch: 90 | Iter: 137600 | Total Loss: 0.006404 | Recon Loss: 0.005362 | Commit Loss: 0.002084 | Perplexity: 1342.903265
2025-09-26 23:11:37,967 Stage: Train 0.5 | Epoch: 90 | Iter: 137800 | Total Loss: 0.006460 | Recon Loss: 0.005418 | Commit Loss: 0.002086 | Perplexity: 1343.454787
2025-09-26 23:12:23,566 Stage: Train 0.5 | Epoch: 90 | Iter: 138000 | Total Loss: 0.006425 | Recon Loss: 0.005383 | Commit Loss: 0.002084 | Perplexity: 1342.335966
2025-09-26 23:13:09,551 Stage: Train 0.5 | Epoch: 90 | Iter: 138200 | Total Loss: 0.006375 | Recon Loss: 0.005342 | Commit Loss: 0.002065 | Perplexity: 1339.796505
Trainning Epoch:  28%|██▊       | 91/330 [9:02:06<23:00:43, 346.63s/it]2025-09-26 23:13:55,489 Stage: Train 0.5 | Epoch: 91 | Iter: 138400 | Total Loss: 0.006452 | Recon Loss: 0.005417 | Commit Loss: 0.002070 | Perplexity: 1343.324130
2025-09-26 23:14:41,103 Stage: Train 0.5 | Epoch: 91 | Iter: 138600 | Total Loss: 0.006366 | Recon Loss: 0.005325 | Commit Loss: 0.002081 | Perplexity: 1345.739145
2025-09-26 23:15:26,758 Stage: Train 0.5 | Epoch: 91 | Iter: 138800 | Total Loss: 0.006406 | Recon Loss: 0.005360 | Commit Loss: 0.002090 | Perplexity: 1343.848109
2025-09-26 23:16:12,639 Stage: Train 0.5 | Epoch: 91 | Iter: 139000 | Total Loss: 0.006345 | Recon Loss: 0.005311 | Commit Loss: 0.002067 | Perplexity: 1339.906978
2025-09-26 23:16:58,307 Stage: Train 0.5 | Epoch: 91 | Iter: 139200 | Total Loss: 0.006479 | Recon Loss: 0.005431 | Commit Loss: 0.002097 | Perplexity: 1348.337800
2025-09-26 23:17:43,936 Stage: Train 0.5 | Epoch: 91 | Iter: 139400 | Total Loss: 0.006415 | Recon Loss: 0.005378 | Commit Loss: 0.002074 | Perplexity: 1342.987694
2025-09-26 23:18:28,789 Stage: Train 0.5 | Epoch: 91 | Iter: 139600 | Total Loss: 0.006424 | Recon Loss: 0.005389 | Commit Loss: 0.002069 | Perplexity: 1343.196696
Trainning Epoch:  28%|██▊       | 92/330 [9:07:52<22:53:22, 346.23s/it]2025-09-26 23:19:13,666 Stage: Train 0.5 | Epoch: 92 | Iter: 139800 | Total Loss: 0.006405 | Recon Loss: 0.005372 | Commit Loss: 0.002066 | Perplexity: 1338.157311
2025-09-26 23:19:59,492 Stage: Train 0.5 | Epoch: 92 | Iter: 140000 | Total Loss: 0.006351 | Recon Loss: 0.005320 | Commit Loss: 0.002062 | Perplexity: 1340.762186
2025-09-26 23:19:59,492 Saving model at iteration 140000
2025-09-26 23:19:59,999 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000
2025-09-26 23:20:00,302 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/model.safetensors
2025-09-26 23:20:00,709 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/optimizer.bin
2025-09-26 23:20:00,709 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/scheduler.bin
2025-09-26 23:20:00,709 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/sampler.bin
2025-09-26 23:20:00,710 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/random_states_0.pkl
2025-09-26 23:20:46,781 Stage: Train 0.5 | Epoch: 92 | Iter: 140200 | Total Loss: 0.006425 | Recon Loss: 0.005392 | Commit Loss: 0.002066 | Perplexity: 1342.631581
2025-09-26 23:21:32,533 Stage: Train 0.5 | Epoch: 92 | Iter: 140400 | Total Loss: 0.006409 | Recon Loss: 0.005370 | Commit Loss: 0.002078 | Perplexity: 1341.470655
2025-09-26 23:22:18,248 Stage: Train 0.5 | Epoch: 92 | Iter: 140600 | Total Loss: 0.006391 | Recon Loss: 0.005358 | Commit Loss: 0.002067 | Perplexity: 1343.587785
2025-09-26 23:23:03,892 Stage: Train 0.5 | Epoch: 92 | Iter: 140800 | Total Loss: 0.006333 | Recon Loss: 0.005291 | Commit Loss: 0.002084 | Perplexity: 1341.127803
2025-09-26 23:23:49,521 Stage: Train 0.5 | Epoch: 92 | Iter: 141000 | Total Loss: 0.006444 | Recon Loss: 0.005402 | Commit Loss: 0.002083 | Perplexity: 1343.098896
2025-09-26 23:24:35,261 Stage: Train 0.5 | Epoch: 92 | Iter: 141200 | Total Loss: 0.006387 | Recon Loss: 0.005349 | Commit Loss: 0.002077 | Perplexity: 1343.434921
Trainning Epoch:  28%|██▊       | 93/330 [9:13:41<22:51:03, 347.10s/it]2025-09-26 23:25:21,070 Stage: Train 0.5 | Epoch: 93 | Iter: 141400 | Total Loss: 0.006352 | Recon Loss: 0.005312 | Commit Loss: 0.002080 | Perplexity: 1343.454263
2025-09-26 23:26:06,863 Stage: Train 0.5 | Epoch: 93 | Iter: 141600 | Total Loss: 0.006422 | Recon Loss: 0.005381 | Commit Loss: 0.002082 | Perplexity: 1344.863030
2025-09-26 23:26:52,512 Stage: Train 0.5 | Epoch: 93 | Iter: 141800 | Total Loss: 0.006348 | Recon Loss: 0.005313 | Commit Loss: 0.002069 | Perplexity: 1342.375118
2025-09-26 23:27:38,296 Stage: Train 0.5 | Epoch: 93 | Iter: 142000 | Total Loss: 0.006363 | Recon Loss: 0.005324 | Commit Loss: 0.002078 | Perplexity: 1344.131465
2025-09-26 23:28:24,143 Stage: Train 0.5 | Epoch: 93 | Iter: 142200 | Total Loss: 0.006365 | Recon Loss: 0.005328 | Commit Loss: 0.002073 | Perplexity: 1342.186102
2025-09-26 23:29:09,434 Stage: Train 0.5 | Epoch: 93 | Iter: 142400 | Total Loss: 0.006362 | Recon Loss: 0.005320 | Commit Loss: 0.002084 | Perplexity: 1340.775331
2025-09-26 23:29:55,165 Stage: Train 0.5 | Epoch: 93 | Iter: 142600 | Total Loss: 0.006334 | Recon Loss: 0.005296 | Commit Loss: 0.002076 | Perplexity: 1342.746512
Trainning Epoch:  28%|██▊       | 94/330 [9:19:28<22:45:15, 347.10s/it]2025-09-26 23:30:41,177 Stage: Train 0.5 | Epoch: 94 | Iter: 142800 | Total Loss: 0.006392 | Recon Loss: 0.005349 | Commit Loss: 0.002086 | Perplexity: 1342.171188
2025-09-26 23:31:26,849 Stage: Train 0.5 | Epoch: 94 | Iter: 143000 | Total Loss: 0.006370 | Recon Loss: 0.005335 | Commit Loss: 0.002071 | Perplexity: 1343.569282
2025-09-26 23:32:12,268 Stage: Train 0.5 | Epoch: 94 | Iter: 143200 | Total Loss: 0.006414 | Recon Loss: 0.005371 | Commit Loss: 0.002084 | Perplexity: 1344.631360
2025-09-26 23:32:58,085 Stage: Train 0.5 | Epoch: 94 | Iter: 143400 | Total Loss: 0.006405 | Recon Loss: 0.005364 | Commit Loss: 0.002081 | Perplexity: 1343.480876
2025-09-26 23:33:43,866 Stage: Train 0.5 | Epoch: 94 | Iter: 143600 | Total Loss: 0.006379 | Recon Loss: 0.005338 | Commit Loss: 0.002084 | Perplexity: 1344.675510
2025-09-26 23:34:29,599 Stage: Train 0.5 | Epoch: 94 | Iter: 143800 | Total Loss: 0.006367 | Recon Loss: 0.005329 | Commit Loss: 0.002076 | Perplexity: 1341.678187
2025-09-26 23:35:15,116 Stage: Train 0.5 | Epoch: 94 | Iter: 144000 | Total Loss: 0.006378 | Recon Loss: 0.005344 | Commit Loss: 0.002068 | Perplexity: 1340.897791
2025-09-26 23:36:00,780 Stage: Train 0.5 | Epoch: 94 | Iter: 144200 | Total Loss: 0.006338 | Recon Loss: 0.005298 | Commit Loss: 0.002080 | Perplexity: 1341.184281
Trainning Epoch:  29%|██▉       | 95/330 [9:25:15<22:39:22, 347.07s/it]2025-09-26 23:36:46,805 Stage: Train 0.5 | Epoch: 95 | Iter: 144400 | Total Loss: 0.006303 | Recon Loss: 0.005266 | Commit Loss: 0.002074 | Perplexity: 1343.087485
2025-09-26 23:37:32,499 Stage: Train 0.5 | Epoch: 95 | Iter: 144600 | Total Loss: 0.006365 | Recon Loss: 0.005323 | Commit Loss: 0.002085 | Perplexity: 1344.644415
2025-09-26 23:38:18,096 Stage: Train 0.5 | Epoch: 95 | Iter: 144800 | Total Loss: 0.006361 | Recon Loss: 0.005327 | Commit Loss: 0.002069 | Perplexity: 1342.188779
2025-09-26 23:39:03,616 Stage: Train 0.5 | Epoch: 95 | Iter: 145000 | Total Loss: 0.006384 | Recon Loss: 0.005349 | Commit Loss: 0.002069 | Perplexity: 1341.673892
2025-09-26 23:39:49,363 Stage: Train 0.5 | Epoch: 95 | Iter: 145200 | Total Loss: 0.006330 | Recon Loss: 0.005285 | Commit Loss: 0.002089 | Perplexity: 1346.087406
2025-09-26 23:40:35,108 Stage: Train 0.5 | Epoch: 95 | Iter: 145400 | Total Loss: 0.006330 | Recon Loss: 0.005294 | Commit Loss: 0.002072 | Perplexity: 1342.441121
2025-09-26 23:41:20,839 Stage: Train 0.5 | Epoch: 95 | Iter: 145600 | Total Loss: 0.006426 | Recon Loss: 0.005379 | Commit Loss: 0.002095 | Perplexity: 1345.451040
2025-09-26 23:42:06,113 Stage: Train 0.5 | Epoch: 95 | Iter: 145800 | Total Loss: 0.006409 | Recon Loss: 0.005372 | Commit Loss: 0.002075 | Perplexity: 1341.269187
Trainning Epoch:  29%|██▉       | 96/330 [9:31:02<22:33:18, 347.00s/it]2025-09-26 23:42:51,926 Stage: Train 0.5 | Epoch: 96 | Iter: 146000 | Total Loss: 0.006312 | Recon Loss: 0.005275 | Commit Loss: 0.002075 | Perplexity: 1343.008799
2025-09-26 23:43:37,690 Stage: Train 0.5 | Epoch: 96 | Iter: 146200 | Total Loss: 0.006310 | Recon Loss: 0.005274 | Commit Loss: 0.002072 | Perplexity: 1345.043116
2025-09-26 23:44:23,432 Stage: Train 0.5 | Epoch: 96 | Iter: 146400 | Total Loss: 0.006393 | Recon Loss: 0.005347 | Commit Loss: 0.002093 | Perplexity: 1346.741526
2025-09-26 23:45:09,176 Stage: Train 0.5 | Epoch: 96 | Iter: 146600 | Total Loss: 0.006253 | Recon Loss: 0.005217 | Commit Loss: 0.002072 | Perplexity: 1341.165896
2025-09-26 23:45:54,695 Stage: Train 0.5 | Epoch: 96 | Iter: 146800 | Total Loss: 0.006335 | Recon Loss: 0.005291 | Commit Loss: 0.002089 | Perplexity: 1346.734290
2025-09-26 23:46:40,421 Stage: Train 0.5 | Epoch: 96 | Iter: 147000 | Total Loss: 0.006318 | Recon Loss: 0.005271 | Commit Loss: 0.002093 | Perplexity: 1344.585793
2025-09-26 23:47:26,211 Stage: Train 0.5 | Epoch: 96 | Iter: 147200 | Total Loss: 0.006330 | Recon Loss: 0.005286 | Commit Loss: 0.002089 | Perplexity: 1346.299471
Trainning Epoch:  29%|██▉       | 97/330 [9:36:49<22:28:04, 347.14s/it]2025-09-26 23:48:12,234 Stage: Train 0.5 | Epoch: 97 | Iter: 147400 | Total Loss: 0.006344 | Recon Loss: 0.005303 | Commit Loss: 0.002081 | Perplexity: 1340.354745
2025-09-26 23:48:57,586 Stage: Train 0.5 | Epoch: 97 | Iter: 147600 | Total Loss: 0.006276 | Recon Loss: 0.005234 | Commit Loss: 0.002084 | Perplexity: 1342.964576
2025-09-26 23:49:43,182 Stage: Train 0.5 | Epoch: 97 | Iter: 147800 | Total Loss: 0.006323 | Recon Loss: 0.005277 | Commit Loss: 0.002092 | Perplexity: 1346.284901
2025-09-26 23:50:28,763 Stage: Train 0.5 | Epoch: 97 | Iter: 148000 | Total Loss: 0.006313 | Recon Loss: 0.005275 | Commit Loss: 0.002076 | Perplexity: 1341.987576
2025-09-26 23:51:14,357 Stage: Train 0.5 | Epoch: 97 | Iter: 148200 | Total Loss: 0.006300 | Recon Loss: 0.005252 | Commit Loss: 0.002095 | Perplexity: 1346.174163
2025-09-26 23:52:00,128 Stage: Train 0.5 | Epoch: 97 | Iter: 148400 | Total Loss: 0.006383 | Recon Loss: 0.005338 | Commit Loss: 0.002090 | Perplexity: 1344.630177
2025-09-26 23:52:45,618 Stage: Train 0.5 | Epoch: 97 | Iter: 148600 | Total Loss: 0.006286 | Recon Loss: 0.005241 | Commit Loss: 0.002090 | Perplexity: 1344.548704
2025-09-26 23:53:31,382 Stage: Train 0.5 | Epoch: 97 | Iter: 148800 | Total Loss: 0.006303 | Recon Loss: 0.005261 | Commit Loss: 0.002085 | Perplexity: 1344.264545
Trainning Epoch:  30%|██▉       | 98/330 [9:42:36<22:21:33, 346.95s/it]2025-09-26 23:54:17,302 Stage: Train 0.5 | Epoch: 98 | Iter: 149000 | Total Loss: 0.006344 | Recon Loss: 0.005297 | Commit Loss: 0.002093 | Perplexity: 1345.330840
2025-09-26 23:55:02,984 Stage: Train 0.5 | Epoch: 98 | Iter: 149200 | Total Loss: 0.006264 | Recon Loss: 0.005221 | Commit Loss: 0.002087 | Perplexity: 1343.073340
2025-09-26 23:55:48,296 Stage: Train 0.5 | Epoch: 98 | Iter: 149400 | Total Loss: 0.006257 | Recon Loss: 0.005209 | Commit Loss: 0.002095 | Perplexity: 1343.880287
2025-09-26 23:56:34,145 Stage: Train 0.5 | Epoch: 98 | Iter: 149600 | Total Loss: 0.006350 | Recon Loss: 0.005312 | Commit Loss: 0.002078 | Perplexity: 1340.346189
2025-09-26 23:57:19,841 Stage: Train 0.5 | Epoch: 98 | Iter: 149800 | Total Loss: 0.006311 | Recon Loss: 0.005263 | Commit Loss: 0.002096 | Perplexity: 1344.552459
2025-09-26 23:58:05,548 Stage: Train 0.5 | Epoch: 98 | Iter: 150000 | Total Loss: 0.006349 | Recon Loss: 0.005305 | Commit Loss: 0.002088 | Perplexity: 1344.438052
2025-09-26 23:58:51,033 Stage: Train 0.5 | Epoch: 98 | Iter: 150200 | Total Loss: 0.006231 | Recon Loss: 0.005191 | Commit Loss: 0.002080 | Perplexity: 1339.570333
Trainning Epoch:  30%|███       | 99/330 [9:48:23<22:15:41, 346.93s/it]2025-09-26 23:59:37,033 Stage: Train 0.5 | Epoch: 99 | Iter: 150400 | Total Loss: 0.006325 | Recon Loss: 0.005281 | Commit Loss: 0.002087 | Perplexity: 1343.126684
2025-09-27 00:00:22,778 Stage: Train 0.5 | Epoch: 99 | Iter: 150600 | Total Loss: 0.006294 | Recon Loss: 0.005253 | Commit Loss: 0.002082 | Perplexity: 1339.915308
2025-09-27 00:01:08,542 Stage: Train 0.5 | Epoch: 99 | Iter: 150800 | Total Loss: 0.006286 | Recon Loss: 0.005237 | Commit Loss: 0.002098 | Perplexity: 1345.967192
2025-09-27 00:01:54,061 Stage: Train 0.5 | Epoch: 99 | Iter: 151000 | Total Loss: 0.006243 | Recon Loss: 0.005201 | Commit Loss: 0.002085 | Perplexity: 1343.081069
2025-09-27 00:02:38,832 Stage: Train 0.5 | Epoch: 99 | Iter: 151200 | Total Loss: 0.006418 | Recon Loss: 0.005374 | Commit Loss: 0.002090 | Perplexity: 1345.058620
2025-09-27 00:03:22,359 Stage: Train 0.5 | Epoch: 99 | Iter: 151400 | Total Loss: 0.006302 | Recon Loss: 0.005259 | Commit Loss: 0.002085 | Perplexity: 1345.266249
2025-09-27 00:04:07,471 Stage: Train 0.5 | Epoch: 99 | Iter: 151600 | Total Loss: 0.006271 | Recon Loss: 0.005228 | Commit Loss: 0.002086 | Perplexity: 1343.914867
2025-09-27 00:04:52,659 Stage: Train 0.5 | Epoch: 99 | Iter: 151800 | Total Loss: 0.006236 | Recon Loss: 0.005193 | Commit Loss: 0.002086 | Perplexity: 1342.916175
Trainning Epoch:  30%|███       | 100/330 [9:54:05<22:05:06, 345.68s/it]2025-09-27 00:05:37,708 Stage: Train 0.5 | Epoch: 100 | Iter: 152000 | Total Loss: 0.006327 | Recon Loss: 0.005278 | Commit Loss: 0.002097 | Perplexity: 1344.920801
2025-09-27 00:06:22,851 Stage: Train 0.5 | Epoch: 100 | Iter: 152200 | Total Loss: 0.006274 | Recon Loss: 0.005238 | Commit Loss: 0.002072 | Perplexity: 1340.963687
2025-09-27 00:07:08,297 Stage: Train 0.5 | Epoch: 100 | Iter: 152400 | Total Loss: 0.006314 | Recon Loss: 0.005271 | Commit Loss: 0.002088 | Perplexity: 1343.326097
2025-09-27 00:07:53,640 Stage: Train 0.5 | Epoch: 100 | Iter: 152600 | Total Loss: 0.006280 | Recon Loss: 0.005233 | Commit Loss: 0.002093 | Perplexity: 1346.510869
2025-09-27 00:08:38,950 Stage: Train 0.5 | Epoch: 100 | Iter: 152800 | Total Loss: 0.006275 | Recon Loss: 0.005228 | Commit Loss: 0.002094 | Perplexity: 1345.204413
2025-09-27 00:09:23,991 Stage: Train 0.5 | Epoch: 100 | Iter: 153000 | Total Loss: 0.006288 | Recon Loss: 0.005243 | Commit Loss: 0.002089 | Perplexity: 1344.290608
2025-09-27 00:10:09,624 Stage: Train 0.5 | Epoch: 100 | Iter: 153200 | Total Loss: 0.006298 | Recon Loss: 0.005253 | Commit Loss: 0.002091 | Perplexity: 1343.533506
2025-09-27 00:10:55,118 Stage: Train 0.5 | Epoch: 100 | Iter: 153400 | Total Loss: 0.006206 | Recon Loss: 0.005164 | Commit Loss: 0.002084 | Perplexity: 1343.377905
Trainning Epoch:  31%|███       | 101/330 [9:59:50<21:57:40, 345.24s/it]2025-09-27 00:11:40,921 Stage: Train 0.5 | Epoch: 101 | Iter: 153600 | Total Loss: 0.006252 | Recon Loss: 0.005207 | Commit Loss: 0.002090 | Perplexity: 1343.578787
2025-09-27 00:12:26,135 Stage: Train 0.5 | Epoch: 101 | Iter: 153800 | Total Loss: 0.006325 | Recon Loss: 0.005275 | Commit Loss: 0.002101 | Perplexity: 1344.217787
2025-09-27 00:13:11,646 Stage: Train 0.5 | Epoch: 101 | Iter: 154000 | Total Loss: 0.006220 | Recon Loss: 0.005176 | Commit Loss: 0.002088 | Perplexity: 1344.712108
2025-09-27 00:13:57,165 Stage: Train 0.5 | Epoch: 101 | Iter: 154200 | Total Loss: 0.006335 | Recon Loss: 0.005288 | Commit Loss: 0.002094 | Perplexity: 1345.078690
2025-09-27 00:14:42,768 Stage: Train 0.5 | Epoch: 101 | Iter: 154400 | Total Loss: 0.006257 | Recon Loss: 0.005215 | Commit Loss: 0.002084 | Perplexity: 1344.925834
2025-09-27 00:15:28,085 Stage: Train 0.5 | Epoch: 101 | Iter: 154600 | Total Loss: 0.006349 | Recon Loss: 0.005301 | Commit Loss: 0.002095 | Perplexity: 1345.155532
2025-09-27 00:16:13,210 Stage: Train 0.5 | Epoch: 101 | Iter: 154800 | Total Loss: 0.006198 | Recon Loss: 0.005150 | Commit Loss: 0.002095 | Perplexity: 1341.176354
Trainning Epoch:  31%|███       | 102/330 [10:05:35<21:51:46, 345.20s/it]2025-09-27 00:16:58,860 Stage: Train 0.5 | Epoch: 102 | Iter: 155000 | Total Loss: 0.006309 | Recon Loss: 0.005262 | Commit Loss: 0.002096 | Perplexity: 1343.433272
2025-09-27 00:17:44,587 Stage: Train 0.5 | Epoch: 102 | Iter: 155200 | Total Loss: 0.006289 | Recon Loss: 0.005239 | Commit Loss: 0.002099 | Perplexity: 1344.677193
2025-09-27 00:18:29,966 Stage: Train 0.5 | Epoch: 102 | Iter: 155400 | Total Loss: 0.006224 | Recon Loss: 0.005180 | Commit Loss: 0.002089 | Perplexity: 1345.388188
2025-09-27 00:19:15,347 Stage: Train 0.5 | Epoch: 102 | Iter: 155600 | Total Loss: 0.006244 | Recon Loss: 0.005200 | Commit Loss: 0.002088 | Perplexity: 1343.515989
2025-09-27 00:20:00,735 Stage: Train 0.5 | Epoch: 102 | Iter: 155800 | Total Loss: 0.006287 | Recon Loss: 0.005239 | Commit Loss: 0.002095 | Perplexity: 1342.877493
2025-09-27 00:20:46,239 Stage: Train 0.5 | Epoch: 102 | Iter: 156000 | Total Loss: 0.006229 | Recon Loss: 0.005187 | Commit Loss: 0.002084 | Perplexity: 1340.697814
2025-09-27 00:21:31,768 Stage: Train 0.5 | Epoch: 102 | Iter: 156200 | Total Loss: 0.006260 | Recon Loss: 0.005211 | Commit Loss: 0.002098 | Perplexity: 1344.721613
2025-09-27 00:22:17,386 Stage: Train 0.5 | Epoch: 102 | Iter: 156400 | Total Loss: 0.006321 | Recon Loss: 0.005267 | Commit Loss: 0.002109 | Perplexity: 1348.296920
Trainning Epoch:  31%|███       | 103/330 [10:11:20<21:46:24, 345.30s/it]2025-09-27 00:23:02,896 Stage: Train 0.5 | Epoch: 103 | Iter: 156600 | Total Loss: 0.006290 | Recon Loss: 0.005243 | Commit Loss: 0.002094 | Perplexity: 1344.837640
2025-09-27 00:23:48,450 Stage: Train 0.5 | Epoch: 103 | Iter: 156800 | Total Loss: 0.006220 | Recon Loss: 0.005172 | Commit Loss: 0.002094 | Perplexity: 1346.325900
2025-09-27 00:24:34,306 Stage: Train 0.5 | Epoch: 103 | Iter: 157000 | Total Loss: 0.006245 | Recon Loss: 0.005197 | Commit Loss: 0.002096 | Perplexity: 1347.285911
2025-09-27 00:25:19,881 Stage: Train 0.5 | Epoch: 103 | Iter: 157200 | Total Loss: 0.006300 | Recon Loss: 0.005254 | Commit Loss: 0.002092 | Perplexity: 1344.052295
2025-09-27 00:26:04,944 Stage: Train 0.5 | Epoch: 103 | Iter: 157400 | Total Loss: 0.006241 | Recon Loss: 0.005195 | Commit Loss: 0.002093 | Perplexity: 1344.656110
2025-09-27 00:26:50,582 Stage: Train 0.5 | Epoch: 103 | Iter: 157600 | Total Loss: 0.006273 | Recon Loss: 0.005228 | Commit Loss: 0.002091 | Perplexity: 1342.544285
2025-09-27 00:27:36,287 Stage: Train 0.5 | Epoch: 103 | Iter: 157800 | Total Loss: 0.006179 | Recon Loss: 0.005133 | Commit Loss: 0.002092 | Perplexity: 1341.155787
Trainning Epoch:  32%|███▏      | 104/330 [10:17:07<21:41:47, 345.61s/it]2025-09-27 00:28:22,080 Stage: Train 0.5 | Epoch: 104 | Iter: 158000 | Total Loss: 0.006205 | Recon Loss: 0.005157 | Commit Loss: 0.002095 | Perplexity: 1345.526622
2025-09-27 00:29:07,716 Stage: Train 0.5 | Epoch: 104 | Iter: 158200 | Total Loss: 0.006295 | Recon Loss: 0.005245 | Commit Loss: 0.002101 | Perplexity: 1346.238094
2025-09-27 00:29:52,954 Stage: Train 0.5 | Epoch: 104 | Iter: 158400 | Total Loss: 0.006200 | Recon Loss: 0.005155 | Commit Loss: 0.002090 | Perplexity: 1346.284202
2025-09-27 00:30:38,404 Stage: Train 0.5 | Epoch: 104 | Iter: 158600 | Total Loss: 0.006212 | Recon Loss: 0.005168 | Commit Loss: 0.002088 | Perplexity: 1344.211262
2025-09-27 00:31:23,901 Stage: Train 0.5 | Epoch: 104 | Iter: 158800 | Total Loss: 0.006192 | Recon Loss: 0.005148 | Commit Loss: 0.002089 | Perplexity: 1346.128763
2025-09-27 00:32:09,408 Stage: Train 0.5 | Epoch: 104 | Iter: 159000 | Total Loss: 0.006204 | Recon Loss: 0.005154 | Commit Loss: 0.002100 | Perplexity: 1345.666285
2025-09-27 00:32:54,739 Stage: Train 0.5 | Epoch: 104 | Iter: 159200 | Total Loss: 0.006235 | Recon Loss: 0.005185 | Commit Loss: 0.002099 | Perplexity: 1343.996542
2025-09-27 00:33:40,571 Stage: Train 0.5 | Epoch: 104 | Iter: 159400 | Total Loss: 0.006277 | Recon Loss: 0.005227 | Commit Loss: 0.002100 | Perplexity: 1345.070179
Trainning Epoch:  32%|███▏      | 105/330 [10:22:52<21:36:12, 345.66s/it]2025-09-27 00:34:26,327 Stage: Train 0.5 | Epoch: 105 | Iter: 159600 | Total Loss: 0.006255 | Recon Loss: 0.005206 | Commit Loss: 0.002097 | Perplexity: 1346.666320
2025-09-27 00:35:11,942 Stage: Train 0.5 | Epoch: 105 | Iter: 159800 | Total Loss: 0.006229 | Recon Loss: 0.005186 | Commit Loss: 0.002087 | Perplexity: 1343.876814
2025-09-27 00:35:57,549 Stage: Train 0.5 | Epoch: 105 | Iter: 160000 | Total Loss: 0.006236 | Recon Loss: 0.005191 | Commit Loss: 0.002092 | Perplexity: 1344.086993
2025-09-27 00:35:57,549 Saving model at iteration 160000
2025-09-27 00:35:57,759 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000
2025-09-27 00:35:58,067 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/model.safetensors
2025-09-27 00:35:58,461 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/optimizer.bin
2025-09-27 00:35:58,461 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/scheduler.bin
2025-09-27 00:35:58,461 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/sampler.bin
2025-09-27 00:35:58,462 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/random_states_0.pkl
2025-09-27 00:36:43,807 Stage: Train 0.5 | Epoch: 105 | Iter: 160200 | Total Loss: 0.006210 | Recon Loss: 0.005163 | Commit Loss: 0.002094 | Perplexity: 1345.748041
2025-09-27 00:37:29,576 Stage: Train 0.5 | Epoch: 105 | Iter: 160400 | Total Loss: 0.006203 | Recon Loss: 0.005157 | Commit Loss: 0.002091 | Perplexity: 1345.477460
2025-09-27 00:38:15,070 Stage: Train 0.5 | Epoch: 105 | Iter: 160600 | Total Loss: 0.006200 | Recon Loss: 0.005146 | Commit Loss: 0.002107 | Perplexity: 1347.603232
2025-09-27 00:39:00,743 Stage: Train 0.5 | Epoch: 105 | Iter: 160800 | Total Loss: 0.006247 | Recon Loss: 0.005196 | Commit Loss: 0.002103 | Perplexity: 1344.218863
2025-09-27 00:39:46,160 Stage: Train 0.5 | Epoch: 105 | Iter: 161000 | Total Loss: 0.006286 | Recon Loss: 0.005232 | Commit Loss: 0.002107 | Perplexity: 1347.069757
Trainning Epoch:  32%|███▏      | 106/330 [10:28:40<21:32:07, 346.10s/it]2025-09-27 00:40:32,088 Stage: Train 0.5 | Epoch: 106 | Iter: 161200 | Total Loss: 0.006227 | Recon Loss: 0.005177 | Commit Loss: 0.002102 | Perplexity: 1344.518688
2025-09-27 00:41:17,577 Stage: Train 0.5 | Epoch: 106 | Iter: 161400 | Total Loss: 0.006237 | Recon Loss: 0.005191 | Commit Loss: 0.002093 | Perplexity: 1345.454998
2025-09-27 00:42:03,045 Stage: Train 0.5 | Epoch: 106 | Iter: 161600 | Total Loss: 0.006193 | Recon Loss: 0.005142 | Commit Loss: 0.002103 | Perplexity: 1348.552267
2025-09-27 00:42:48,288 Stage: Train 0.5 | Epoch: 106 | Iter: 161800 | Total Loss: 0.006261 | Recon Loss: 0.005210 | Commit Loss: 0.002101 | Perplexity: 1346.406893
2025-09-27 00:43:33,998 Stage: Train 0.5 | Epoch: 106 | Iter: 162000 | Total Loss: 0.006236 | Recon Loss: 0.005188 | Commit Loss: 0.002096 | Perplexity: 1343.489559
2025-09-27 00:44:19,742 Stage: Train 0.5 | Epoch: 106 | Iter: 162200 | Total Loss: 0.006240 | Recon Loss: 0.005193 | Commit Loss: 0.002094 | Perplexity: 1342.693400
2025-09-27 00:45:05,698 Stage: Train 0.5 | Epoch: 106 | Iter: 162400 | Total Loss: 0.006196 | Recon Loss: 0.005155 | Commit Loss: 0.002084 | Perplexity: 1343.524650
Trainning Epoch:  32%|███▏      | 107/330 [10:34:26<21:27:05, 346.30s/it]2025-09-27 00:45:51,614 Stage: Train 0.5 | Epoch: 107 | Iter: 162600 | Total Loss: 0.006170 | Recon Loss: 0.005121 | Commit Loss: 0.002098 | Perplexity: 1345.598738
2025-09-27 00:46:36,877 Stage: Train 0.5 | Epoch: 107 | Iter: 162800 | Total Loss: 0.006225 | Recon Loss: 0.005179 | Commit Loss: 0.002093 | Perplexity: 1345.496939
2025-09-27 00:47:21,356 Stage: Train 0.5 | Epoch: 107 | Iter: 163000 | Total Loss: 0.006164 | Recon Loss: 0.005120 | Commit Loss: 0.002089 | Perplexity: 1344.357758
2025-09-27 00:48:06,887 Stage: Train 0.5 | Epoch: 107 | Iter: 163200 | Total Loss: 0.006214 | Recon Loss: 0.005163 | Commit Loss: 0.002102 | Perplexity: 1344.909626
2025-09-27 00:48:52,399 Stage: Train 0.5 | Epoch: 107 | Iter: 163400 | Total Loss: 0.006166 | Recon Loss: 0.005118 | Commit Loss: 0.002095 | Perplexity: 1344.180671
2025-09-27 00:49:37,863 Stage: Train 0.5 | Epoch: 107 | Iter: 163600 | Total Loss: 0.006201 | Recon Loss: 0.005153 | Commit Loss: 0.002095 | Perplexity: 1345.904605
2025-09-27 00:50:23,244 Stage: Train 0.5 | Epoch: 107 | Iter: 163800 | Total Loss: 0.006174 | Recon Loss: 0.005126 | Commit Loss: 0.002096 | Perplexity: 1345.798281
2025-09-27 00:51:08,880 Stage: Train 0.5 | Epoch: 107 | Iter: 164000 | Total Loss: 0.006201 | Recon Loss: 0.005149 | Commit Loss: 0.002104 | Perplexity: 1346.796207
Trainning Epoch:  33%|███▎      | 108/330 [10:40:11<21:19:32, 345.82s/it]2025-09-27 00:51:54,603 Stage: Train 0.5 | Epoch: 108 | Iter: 164200 | Total Loss: 0.006181 | Recon Loss: 0.005130 | Commit Loss: 0.002101 | Perplexity: 1343.603651
2025-09-27 00:52:40,160 Stage: Train 0.5 | Epoch: 108 | Iter: 164400 | Total Loss: 0.006239 | Recon Loss: 0.005194 | Commit Loss: 0.002089 | Perplexity: 1344.646259
2025-09-27 00:53:25,552 Stage: Train 0.5 | Epoch: 108 | Iter: 164600 | Total Loss: 0.006179 | Recon Loss: 0.005133 | Commit Loss: 0.002093 | Perplexity: 1345.142028
2025-09-27 00:54:11,250 Stage: Train 0.5 | Epoch: 108 | Iter: 164800 | Total Loss: 0.006259 | Recon Loss: 0.005200 | Commit Loss: 0.002118 | Perplexity: 1348.301108
2025-09-27 00:54:56,883 Stage: Train 0.5 | Epoch: 108 | Iter: 165000 | Total Loss: 0.006165 | Recon Loss: 0.005114 | Commit Loss: 0.002102 | Perplexity: 1344.635209
2025-09-27 00:55:42,515 Stage: Train 0.5 | Epoch: 108 | Iter: 165200 | Total Loss: 0.006164 | Recon Loss: 0.005120 | Commit Loss: 0.002088 | Perplexity: 1344.738295
2025-09-27 00:56:27,862 Stage: Train 0.5 | Epoch: 108 | Iter: 165400 | Total Loss: 0.006211 | Recon Loss: 0.005158 | Commit Loss: 0.002106 | Perplexity: 1347.846854
Trainning Epoch:  33%|███▎      | 109/330 [10:45:57<21:14:07, 345.92s/it]2025-09-27 00:57:13,773 Stage: Train 0.5 | Epoch: 109 | Iter: 165600 | Total Loss: 0.006160 | Recon Loss: 0.005117 | Commit Loss: 0.002086 | Perplexity: 1341.466760
2025-09-27 00:57:59,228 Stage: Train 0.5 | Epoch: 109 | Iter: 165800 | Total Loss: 0.006136 | Recon Loss: 0.005086 | Commit Loss: 0.002100 | Perplexity: 1344.426351
2025-09-27 00:58:44,546 Stage: Train 0.5 | Epoch: 109 | Iter: 166000 | Total Loss: 0.006250 | Recon Loss: 0.005198 | Commit Loss: 0.002103 | Perplexity: 1343.636384
2025-09-27 00:59:30,295 Stage: Train 0.5 | Epoch: 109 | Iter: 166200 | Total Loss: 0.006157 | Recon Loss: 0.005109 | Commit Loss: 0.002095 | Perplexity: 1346.457201
2025-09-27 01:00:15,712 Stage: Train 0.5 | Epoch: 109 | Iter: 166400 | Total Loss: 0.006212 | Recon Loss: 0.005156 | Commit Loss: 0.002111 | Perplexity: 1346.996566
2025-09-27 01:01:01,194 Stage: Train 0.5 | Epoch: 109 | Iter: 166600 | Total Loss: 0.006228 | Recon Loss: 0.005177 | Commit Loss: 0.002102 | Perplexity: 1348.153535
2025-09-27 01:01:46,984 Stage: Train 0.5 | Epoch: 109 | Iter: 166800 | Total Loss: 0.006197 | Recon Loss: 0.005148 | Commit Loss: 0.002097 | Perplexity: 1344.451398
2025-09-27 01:02:32,712 Stage: Train 0.5 | Epoch: 109 | Iter: 167000 | Total Loss: 0.006171 | Recon Loss: 0.005124 | Commit Loss: 0.002094 | Perplexity: 1347.026525
Trainning Epoch:  33%|███▎      | 110/330 [10:51:43<21:08:50, 346.05s/it]2025-09-27 01:03:18,392 Stage: Train 0.5 | Epoch: 110 | Iter: 167200 | Total Loss: 0.006151 | Recon Loss: 0.005100 | Commit Loss: 0.002103 | Perplexity: 1346.739380
2025-09-27 01:04:03,999 Stage: Train 0.5 | Epoch: 110 | Iter: 167400 | Total Loss: 0.006160 | Recon Loss: 0.005107 | Commit Loss: 0.002105 | Perplexity: 1346.339856
2025-09-27 01:04:49,659 Stage: Train 0.5 | Epoch: 110 | Iter: 167600 | Total Loss: 0.006168 | Recon Loss: 0.005124 | Commit Loss: 0.002088 | Perplexity: 1345.703320
2025-09-27 01:05:35,369 Stage: Train 0.5 | Epoch: 110 | Iter: 167800 | Total Loss: 0.006126 | Recon Loss: 0.005079 | Commit Loss: 0.002094 | Perplexity: 1346.347632
2025-09-27 01:06:20,601 Stage: Train 0.5 | Epoch: 110 | Iter: 168000 | Total Loss: 0.006177 | Recon Loss: 0.005118 | Commit Loss: 0.002118 | Perplexity: 1347.035497
2025-09-27 01:07:05,815 Stage: Train 0.5 | Epoch: 110 | Iter: 168200 | Total Loss: 0.006179 | Recon Loss: 0.005126 | Commit Loss: 0.002105 | Perplexity: 1347.414353
2025-09-27 01:07:51,280 Stage: Train 0.5 | Epoch: 110 | Iter: 168400 | Total Loss: 0.006178 | Recon Loss: 0.005124 | Commit Loss: 0.002108 | Perplexity: 1349.867120
2025-09-27 01:08:36,880 Stage: Train 0.5 | Epoch: 110 | Iter: 168600 | Total Loss: 0.006208 | Recon Loss: 0.005151 | Commit Loss: 0.002116 | Perplexity: 1347.379203
Trainning Epoch:  34%|███▎      | 111/330 [10:57:29<21:02:40, 345.94s/it]2025-09-27 01:09:22,535 Stage: Train 0.5 | Epoch: 111 | Iter: 168800 | Total Loss: 0.006154 | Recon Loss: 0.005108 | Commit Loss: 0.002092 | Perplexity: 1342.137286
2025-09-27 01:10:07,775 Stage: Train 0.5 | Epoch: 111 | Iter: 169000 | Total Loss: 0.006193 | Recon Loss: 0.005139 | Commit Loss: 0.002106 | Perplexity: 1346.028820
2025-09-27 01:10:53,385 Stage: Train 0.5 | Epoch: 111 | Iter: 169200 | Total Loss: 0.006162 | Recon Loss: 0.005109 | Commit Loss: 0.002106 | Perplexity: 1348.017360
2025-09-27 01:11:38,590 Stage: Train 0.5 | Epoch: 111 | Iter: 169400 | Total Loss: 0.006206 | Recon Loss: 0.005153 | Commit Loss: 0.002105 | Perplexity: 1345.592610
2025-09-27 01:12:24,029 Stage: Train 0.5 | Epoch: 111 | Iter: 169600 | Total Loss: 0.006082 | Recon Loss: 0.005029 | Commit Loss: 0.002106 | Perplexity: 1347.883932
2025-09-27 01:13:09,546 Stage: Train 0.5 | Epoch: 111 | Iter: 169800 | Total Loss: 0.006169 | Recon Loss: 0.005118 | Commit Loss: 0.002101 | Perplexity: 1348.250513
2025-09-27 01:13:54,927 Stage: Train 0.5 | Epoch: 111 | Iter: 170000 | Total Loss: 0.006157 | Recon Loss: 0.005106 | Commit Loss: 0.002101 | Perplexity: 1344.489991
Trainning Epoch:  34%|███▍      | 112/330 [11:03:14<20:55:56, 345.67s/it]2025-09-27 01:14:40,634 Stage: Train 0.5 | Epoch: 112 | Iter: 170200 | Total Loss: 0.006113 | Recon Loss: 0.005054 | Commit Loss: 0.002117 | Perplexity: 1345.653276
2025-09-27 01:15:26,136 Stage: Train 0.5 | Epoch: 112 | Iter: 170400 | Total Loss: 0.006114 | Recon Loss: 0.005069 | Commit Loss: 0.002090 | Perplexity: 1346.298091
2025-09-27 01:16:11,709 Stage: Train 0.5 | Epoch: 112 | Iter: 170600 | Total Loss: 0.006103 | Recon Loss: 0.005048 | Commit Loss: 0.002108 | Perplexity: 1346.300638
2025-09-27 01:16:56,782 Stage: Train 0.5 | Epoch: 112 | Iter: 170800 | Total Loss: 0.006141 | Recon Loss: 0.005086 | Commit Loss: 0.002109 | Perplexity: 1350.465493
2025-09-27 01:17:42,051 Stage: Train 0.5 | Epoch: 112 | Iter: 171000 | Total Loss: 0.006154 | Recon Loss: 0.005095 | Commit Loss: 0.002119 | Perplexity: 1350.586579
2025-09-27 01:18:27,573 Stage: Train 0.5 | Epoch: 112 | Iter: 171200 | Total Loss: 0.006139 | Recon Loss: 0.005084 | Commit Loss: 0.002110 | Perplexity: 1346.623572
2025-09-27 01:19:13,068 Stage: Train 0.5 | Epoch: 112 | Iter: 171400 | Total Loss: 0.006243 | Recon Loss: 0.005190 | Commit Loss: 0.002105 | Perplexity: 1346.042348
2025-09-27 01:19:58,659 Stage: Train 0.5 | Epoch: 112 | Iter: 171600 | Total Loss: 0.006147 | Recon Loss: 0.005095 | Commit Loss: 0.002103 | Perplexity: 1344.837088
Trainning Epoch:  34%|███▍      | 113/330 [11:08:59<20:49:31, 345.49s/it]2025-09-27 01:20:44,272 Stage: Train 0.5 | Epoch: 113 | Iter: 171800 | Total Loss: 0.006141 | Recon Loss: 0.005094 | Commit Loss: 0.002094 | Perplexity: 1343.369773
2025-09-27 01:21:29,969 Stage: Train 0.5 | Epoch: 113 | Iter: 172000 | Total Loss: 0.006125 | Recon Loss: 0.005067 | Commit Loss: 0.002116 | Perplexity: 1349.563630
2025-09-27 01:22:15,250 Stage: Train 0.5 | Epoch: 113 | Iter: 172200 | Total Loss: 0.006113 | Recon Loss: 0.005063 | Commit Loss: 0.002100 | Perplexity: 1345.777438
2025-09-27 01:23:00,525 Stage: Train 0.5 | Epoch: 113 | Iter: 172400 | Total Loss: 0.006112 | Recon Loss: 0.005066 | Commit Loss: 0.002093 | Perplexity: 1345.679769
2025-09-27 01:23:45,865 Stage: Train 0.5 | Epoch: 113 | Iter: 172600 | Total Loss: 0.006132 | Recon Loss: 0.005080 | Commit Loss: 0.002104 | Perplexity: 1346.323444
2025-09-27 01:24:31,339 Stage: Train 0.5 | Epoch: 113 | Iter: 172800 | Total Loss: 0.006168 | Recon Loss: 0.005110 | Commit Loss: 0.002115 | Perplexity: 1346.635012
2025-09-27 01:25:16,770 Stage: Train 0.5 | Epoch: 113 | Iter: 173000 | Total Loss: 0.006116 | Recon Loss: 0.005067 | Commit Loss: 0.002099 | Perplexity: 1347.212498
Trainning Epoch:  35%|███▍      | 114/330 [11:14:44<20:43:23, 345.38s/it]2025-09-27 01:26:02,136 Stage: Train 0.5 | Epoch: 114 | Iter: 173200 | Total Loss: 0.006156 | Recon Loss: 0.005101 | Commit Loss: 0.002109 | Perplexity: 1347.175591
2025-09-27 01:26:47,420 Stage: Train 0.5 | Epoch: 114 | Iter: 173400 | Total Loss: 0.006052 | Recon Loss: 0.005001 | Commit Loss: 0.002103 | Perplexity: 1346.069514
2025-09-27 01:27:32,764 Stage: Train 0.5 | Epoch: 114 | Iter: 173600 | Total Loss: 0.006165 | Recon Loss: 0.005112 | Commit Loss: 0.002106 | Perplexity: 1348.284107
2025-09-27 01:28:18,390 Stage: Train 0.5 | Epoch: 114 | Iter: 173800 | Total Loss: 0.006152 | Recon Loss: 0.005100 | Commit Loss: 0.002104 | Perplexity: 1346.965684
2025-09-27 01:29:03,937 Stage: Train 0.5 | Epoch: 114 | Iter: 174000 | Total Loss: 0.006124 | Recon Loss: 0.005074 | Commit Loss: 0.002099 | Perplexity: 1348.081465
2025-09-27 01:29:49,196 Stage: Train 0.5 | Epoch: 114 | Iter: 174200 | Total Loss: 0.006169 | Recon Loss: 0.005117 | Commit Loss: 0.002105 | Perplexity: 1345.697490
2025-09-27 01:30:34,349 Stage: Train 0.5 | Epoch: 114 | Iter: 174400 | Total Loss: 0.006052 | Recon Loss: 0.005000 | Commit Loss: 0.002105 | Perplexity: 1348.392318
2025-09-27 01:31:19,909 Stage: Train 0.5 | Epoch: 114 | Iter: 174600 | Total Loss: 0.006173 | Recon Loss: 0.005121 | Commit Loss: 0.002104 | Perplexity: 1346.836236
Trainning Epoch:  35%|███▍      | 115/330 [11:20:29<20:37:16, 345.29s/it]2025-09-27 01:32:04,299 Stage: Train 0.5 | Epoch: 115 | Iter: 174800 | Total Loss: 0.006169 | Recon Loss: 0.005113 | Commit Loss: 0.002112 | Perplexity: 1346.194379
2025-09-27 01:32:49,541 Stage: Train 0.5 | Epoch: 115 | Iter: 175000 | Total Loss: 0.006060 | Recon Loss: 0.005010 | Commit Loss: 0.002101 | Perplexity: 1346.689109
2025-09-27 01:33:34,620 Stage: Train 0.5 | Epoch: 115 | Iter: 175200 | Total Loss: 0.006085 | Recon Loss: 0.005037 | Commit Loss: 0.002095 | Perplexity: 1347.415959
2025-09-27 01:34:20,159 Stage: Train 0.5 | Epoch: 115 | Iter: 175400 | Total Loss: 0.006073 | Recon Loss: 0.005020 | Commit Loss: 0.002104 | Perplexity: 1347.119268
2025-09-27 01:35:05,648 Stage: Train 0.5 | Epoch: 115 | Iter: 175600 | Total Loss: 0.006102 | Recon Loss: 0.005048 | Commit Loss: 0.002108 | Perplexity: 1346.322488
2025-09-27 01:35:50,795 Stage: Train 0.5 | Epoch: 115 | Iter: 175800 | Total Loss: 0.006173 | Recon Loss: 0.005118 | Commit Loss: 0.002111 | Perplexity: 1348.497072
2025-09-27 01:36:36,204 Stage: Train 0.5 | Epoch: 115 | Iter: 176000 | Total Loss: 0.006136 | Recon Loss: 0.005080 | Commit Loss: 0.002110 | Perplexity: 1343.794094
2025-09-27 01:37:21,222 Stage: Train 0.5 | Epoch: 115 | Iter: 176200 | Total Loss: 0.006084 | Recon Loss: 0.005033 | Commit Loss: 0.002102 | Perplexity: 1344.952947
Trainning Epoch:  35%|███▌      | 116/330 [11:26:12<20:28:55, 344.56s/it]2025-09-27 01:38:06,763 Stage: Train 0.5 | Epoch: 116 | Iter: 176400 | Total Loss: 0.006099 | Recon Loss: 0.005048 | Commit Loss: 0.002102 | Perplexity: 1350.141528
2025-09-27 01:38:52,188 Stage: Train 0.5 | Epoch: 116 | Iter: 176600 | Total Loss: 0.006155 | Recon Loss: 0.005106 | Commit Loss: 0.002100 | Perplexity: 1347.263812
2025-09-27 01:39:37,741 Stage: Train 0.5 | Epoch: 116 | Iter: 176800 | Total Loss: 0.006112 | Recon Loss: 0.005057 | Commit Loss: 0.002111 | Perplexity: 1345.820350
2025-09-27 01:40:22,938 Stage: Train 0.5 | Epoch: 116 | Iter: 177000 | Total Loss: 0.006086 | Recon Loss: 0.005033 | Commit Loss: 0.002105 | Perplexity: 1348.486280
2025-09-27 01:41:08,149 Stage: Train 0.5 | Epoch: 116 | Iter: 177200 | Total Loss: 0.006110 | Recon Loss: 0.005056 | Commit Loss: 0.002109 | Perplexity: 1347.868464
2025-09-27 01:41:53,802 Stage: Train 0.5 | Epoch: 116 | Iter: 177400 | Total Loss: 0.006127 | Recon Loss: 0.005069 | Commit Loss: 0.002116 | Perplexity: 1349.638045
2025-09-27 01:42:39,389 Stage: Train 0.5 | Epoch: 116 | Iter: 177600 | Total Loss: 0.006079 | Recon Loss: 0.005023 | Commit Loss: 0.002113 | Perplexity: 1349.209969
Trainning Epoch:  35%|███▌      | 117/330 [11:31:58<20:23:54, 344.76s/it]2025-09-27 01:43:25,139 Stage: Train 0.5 | Epoch: 117 | Iter: 177800 | Total Loss: 0.006124 | Recon Loss: 0.005070 | Commit Loss: 0.002108 | Perplexity: 1344.739307
2025-09-27 01:44:10,162 Stage: Train 0.5 | Epoch: 117 | Iter: 178000 | Total Loss: 0.006064 | Recon Loss: 0.005016 | Commit Loss: 0.002095 | Perplexity: 1346.810735
2025-09-27 01:44:55,686 Stage: Train 0.5 | Epoch: 117 | Iter: 178200 | Total Loss: 0.006161 | Recon Loss: 0.005108 | Commit Loss: 0.002105 | Perplexity: 1344.728533
2025-09-27 01:45:41,167 Stage: Train 0.5 | Epoch: 117 | Iter: 178400 | Total Loss: 0.006102 | Recon Loss: 0.005049 | Commit Loss: 0.002106 | Perplexity: 1351.101309
2025-09-27 01:46:26,501 Stage: Train 0.5 | Epoch: 117 | Iter: 178600 | Total Loss: 0.006126 | Recon Loss: 0.005074 | Commit Loss: 0.002106 | Perplexity: 1348.073733
2025-09-27 01:47:11,654 Stage: Train 0.5 | Epoch: 117 | Iter: 178800 | Total Loss: 0.006080 | Recon Loss: 0.005025 | Commit Loss: 0.002110 | Perplexity: 1348.713607
2025-09-27 01:47:57,110 Stage: Train 0.5 | Epoch: 117 | Iter: 179000 | Total Loss: 0.006083 | Recon Loss: 0.005026 | Commit Loss: 0.002116 | Perplexity: 1350.326831
2025-09-27 01:48:42,396 Stage: Train 0.5 | Epoch: 117 | Iter: 179200 | Total Loss: 0.006194 | Recon Loss: 0.005131 | Commit Loss: 0.002126 | Perplexity: 1348.054331
Trainning Epoch:  36%|███▌      | 118/330 [11:37:42<20:17:52, 344.68s/it]2025-09-27 01:49:27,981 Stage: Train 0.5 | Epoch: 118 | Iter: 179400 | Total Loss: 0.006077 | Recon Loss: 0.005027 | Commit Loss: 0.002101 | Perplexity: 1345.420669
2025-09-27 01:50:13,213 Stage: Train 0.5 | Epoch: 118 | Iter: 179600 | Total Loss: 0.006091 | Recon Loss: 0.005036 | Commit Loss: 0.002109 | Perplexity: 1347.056883
2025-09-27 01:50:57,980 Stage: Train 0.5 | Epoch: 118 | Iter: 179800 | Total Loss: 0.006034 | Recon Loss: 0.004984 | Commit Loss: 0.002101 | Perplexity: 1348.027226
2025-09-27 01:51:43,425 Stage: Train 0.5 | Epoch: 118 | Iter: 180000 | Total Loss: 0.006119 | Recon Loss: 0.005065 | Commit Loss: 0.002108 | Perplexity: 1347.990579
2025-09-27 01:51:43,425 Saving model at iteration 180000
2025-09-27 01:51:43,628 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000
2025-09-27 01:51:43,939 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/model.safetensors
2025-09-27 01:51:44,337 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/optimizer.bin
2025-09-27 01:51:44,337 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/scheduler.bin
2025-09-27 01:51:44,337 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/sampler.bin
2025-09-27 01:51:44,338 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/random_states_0.pkl
2025-09-27 01:52:29,842 Stage: Train 0.5 | Epoch: 118 | Iter: 180200 | Total Loss: 0.006131 | Recon Loss: 0.005070 | Commit Loss: 0.002121 | Perplexity: 1348.815336
2025-09-27 01:53:15,436 Stage: Train 0.5 | Epoch: 118 | Iter: 180400 | Total Loss: 0.006081 | Recon Loss: 0.005024 | Commit Loss: 0.002114 | Perplexity: 1348.939320
2025-09-27 01:54:00,240 Stage: Train 0.5 | Epoch: 118 | Iter: 180600 | Total Loss: 0.006121 | Recon Loss: 0.005060 | Commit Loss: 0.002122 | Perplexity: 1347.385311
Trainning Epoch:  36%|███▌      | 119/330 [11:43:27<20:12:03, 344.66s/it]2025-09-27 01:54:45,500 Stage: Train 0.5 | Epoch: 119 | Iter: 180800 | Total Loss: 0.006087 | Recon Loss: 0.005031 | Commit Loss: 0.002112 | Perplexity: 1348.224169
2025-09-27 01:55:30,779 Stage: Train 0.5 | Epoch: 119 | Iter: 181000 | Total Loss: 0.006068 | Recon Loss: 0.005008 | Commit Loss: 0.002121 | Perplexity: 1351.631328
2025-09-27 01:56:15,992 Stage: Train 0.5 | Epoch: 119 | Iter: 181200 | Total Loss: 0.006113 | Recon Loss: 0.005058 | Commit Loss: 0.002111 | Perplexity: 1349.668018
2025-09-27 01:57:01,224 Stage: Train 0.5 | Epoch: 119 | Iter: 181400 | Total Loss: 0.006056 | Recon Loss: 0.004999 | Commit Loss: 0.002113 | Perplexity: 1350.552831
2025-09-27 01:57:46,066 Stage: Train 0.5 | Epoch: 119 | Iter: 181600 | Total Loss: 0.006086 | Recon Loss: 0.005037 | Commit Loss: 0.002100 | Perplexity: 1347.733148
2025-09-27 01:58:31,369 Stage: Train 0.5 | Epoch: 119 | Iter: 181800 | Total Loss: 0.006098 | Recon Loss: 0.005042 | Commit Loss: 0.002111 | Perplexity: 1346.150604
2025-09-27 01:59:16,388 Stage: Train 0.5 | Epoch: 119 | Iter: 182000 | Total Loss: 0.006080 | Recon Loss: 0.005025 | Commit Loss: 0.002109 | Perplexity: 1347.456139
2025-09-27 02:00:01,440 Stage: Train 0.5 | Epoch: 119 | Iter: 182200 | Total Loss: 0.006130 | Recon Loss: 0.005080 | Commit Loss: 0.002101 | Perplexity: 1346.896394
Trainning Epoch:  36%|███▋      | 120/330 [11:49:10<20:04:43, 344.21s/it]2025-09-27 02:00:46,674 Stage: Train 0.5 | Epoch: 120 | Iter: 182400 | Total Loss: 0.006036 | Recon Loss: 0.004982 | Commit Loss: 0.002108 | Perplexity: 1347.723751
2025-09-27 02:01:31,787 Stage: Train 0.5 | Epoch: 120 | Iter: 182600 | Total Loss: 0.006105 | Recon Loss: 0.005052 | Commit Loss: 0.002107 | Perplexity: 1346.662596
2025-09-27 02:02:17,095 Stage: Train 0.5 | Epoch: 120 | Iter: 182800 | Total Loss: 0.006046 | Recon Loss: 0.004993 | Commit Loss: 0.002106 | Perplexity: 1348.201154
2025-09-27 02:03:02,595 Stage: Train 0.5 | Epoch: 120 | Iter: 183000 | Total Loss: 0.006020 | Recon Loss: 0.004965 | Commit Loss: 0.002110 | Perplexity: 1351.043362
2025-09-27 02:03:48,116 Stage: Train 0.5 | Epoch: 120 | Iter: 183200 | Total Loss: 0.006064 | Recon Loss: 0.005008 | Commit Loss: 0.002112 | Perplexity: 1346.953370
2025-09-27 02:04:33,259 Stage: Train 0.5 | Epoch: 120 | Iter: 183400 | Total Loss: 0.006056 | Recon Loss: 0.004997 | Commit Loss: 0.002118 | Perplexity: 1351.082359
2025-09-27 02:05:18,472 Stage: Train 0.5 | Epoch: 120 | Iter: 183600 | Total Loss: 0.006177 | Recon Loss: 0.005126 | Commit Loss: 0.002104 | Perplexity: 1348.631624
Trainning Epoch:  37%|███▋      | 121/330 [11:54:54<19:58:44, 344.13s/it]2025-09-27 02:06:04,039 Stage: Train 0.5 | Epoch: 121 | Iter: 183800 | Total Loss: 0.006075 | Recon Loss: 0.005022 | Commit Loss: 0.002106 | Perplexity: 1346.037963
2025-09-27 02:06:49,316 Stage: Train 0.5 | Epoch: 121 | Iter: 184000 | Total Loss: 0.006036 | Recon Loss: 0.004975 | Commit Loss: 0.002121 | Perplexity: 1350.995531
2025-09-27 02:07:34,388 Stage: Train 0.5 | Epoch: 121 | Iter: 184200 | Total Loss: 0.006076 | Recon Loss: 0.005022 | Commit Loss: 0.002107 | Perplexity: 1347.846080
2025-09-27 02:08:20,035 Stage: Train 0.5 | Epoch: 121 | Iter: 184400 | Total Loss: 0.006087 | Recon Loss: 0.005029 | Commit Loss: 0.002115 | Perplexity: 1347.886868
2025-09-27 02:09:05,405 Stage: Train 0.5 | Epoch: 121 | Iter: 184600 | Total Loss: 0.006013 | Recon Loss: 0.004957 | Commit Loss: 0.002113 | Perplexity: 1350.502323
2025-09-27 02:09:50,545 Stage: Train 0.5 | Epoch: 121 | Iter: 184800 | Total Loss: 0.006098 | Recon Loss: 0.005044 | Commit Loss: 0.002107 | Perplexity: 1347.701131
2025-09-27 02:10:35,888 Stage: Train 0.5 | Epoch: 121 | Iter: 185000 | Total Loss: 0.006087 | Recon Loss: 0.005031 | Commit Loss: 0.002111 | Perplexity: 1348.967473
2025-09-27 02:11:20,889 Stage: Train 0.5 | Epoch: 121 | Iter: 185200 | Total Loss: 0.006042 | Recon Loss: 0.004989 | Commit Loss: 0.002106 | Perplexity: 1347.681633
Trainning Epoch:  37%|███▋      | 122/330 [12:00:38<19:53:00, 344.14s/it]2025-09-27 02:12:06,636 Stage: Train 0.5 | Epoch: 122 | Iter: 185400 | Total Loss: 0.006034 | Recon Loss: 0.004987 | Commit Loss: 0.002093 | Perplexity: 1347.057617
2025-09-27 02:12:52,141 Stage: Train 0.5 | Epoch: 122 | Iter: 185600 | Total Loss: 0.006030 | Recon Loss: 0.004975 | Commit Loss: 0.002111 | Perplexity: 1350.698935
2025-09-27 02:13:37,584 Stage: Train 0.5 | Epoch: 122 | Iter: 185800 | Total Loss: 0.006096 | Recon Loss: 0.005041 | Commit Loss: 0.002110 | Perplexity: 1347.469009
2025-09-27 02:14:22,549 Stage: Train 0.5 | Epoch: 122 | Iter: 186000 | Total Loss: 0.006038 | Recon Loss: 0.004989 | Commit Loss: 0.002099 | Perplexity: 1347.001273
2025-09-27 02:15:07,965 Stage: Train 0.5 | Epoch: 122 | Iter: 186200 | Total Loss: 0.006070 | Recon Loss: 0.005011 | Commit Loss: 0.002118 | Perplexity: 1349.568980
2025-09-27 02:15:53,303 Stage: Train 0.5 | Epoch: 122 | Iter: 186400 | Total Loss: 0.006087 | Recon Loss: 0.005031 | Commit Loss: 0.002112 | Perplexity: 1352.791609
2025-09-27 02:16:37,257 Stage: Train 0.5 | Epoch: 122 | Iter: 186600 | Total Loss: 0.006018 | Recon Loss: 0.004966 | Commit Loss: 0.002104 | Perplexity: 1347.145082
2025-09-27 02:17:22,605 Stage: Train 0.5 | Epoch: 122 | Iter: 186800 | Total Loss: 0.006001 | Recon Loss: 0.004951 | Commit Loss: 0.002101 | Perplexity: 1346.125483
Trainning Epoch:  37%|███▋      | 123/330 [12:06:21<19:46:23, 343.88s/it]2025-09-27 02:18:08,024 Stage: Train 0.5 | Epoch: 123 | Iter: 187000 | Total Loss: 0.006035 | Recon Loss: 0.004975 | Commit Loss: 0.002120 | Perplexity: 1346.627269
2025-09-27 02:18:53,373 Stage: Train 0.5 | Epoch: 123 | Iter: 187200 | Total Loss: 0.006074 | Recon Loss: 0.005021 | Commit Loss: 0.002106 | Perplexity: 1348.318058
2025-09-27 02:19:38,632 Stage: Train 0.5 | Epoch: 123 | Iter: 187400 | Total Loss: 0.005970 | Recon Loss: 0.004917 | Commit Loss: 0.002105 | Perplexity: 1350.164938
2025-09-27 02:20:24,026 Stage: Train 0.5 | Epoch: 123 | Iter: 187600 | Total Loss: 0.005989 | Recon Loss: 0.004936 | Commit Loss: 0.002106 | Perplexity: 1348.719721
2025-09-27 02:21:09,316 Stage: Train 0.5 | Epoch: 123 | Iter: 187800 | Total Loss: 0.006078 | Recon Loss: 0.005024 | Commit Loss: 0.002109 | Perplexity: 1349.736036
2025-09-27 02:21:54,785 Stage: Train 0.5 | Epoch: 123 | Iter: 188000 | Total Loss: 0.006057 | Recon Loss: 0.004997 | Commit Loss: 0.002120 | Perplexity: 1352.785344
2025-09-27 02:22:40,088 Stage: Train 0.5 | Epoch: 123 | Iter: 188200 | Total Loss: 0.006041 | Recon Loss: 0.004986 | Commit Loss: 0.002110 | Perplexity: 1348.557969
Trainning Epoch:  38%|███▊      | 124/330 [12:12:06<19:41:24, 344.10s/it]2025-09-27 02:23:25,917 Stage: Train 0.5 | Epoch: 124 | Iter: 188400 | Total Loss: 0.006106 | Recon Loss: 0.005050 | Commit Loss: 0.002112 | Perplexity: 1349.188401
2025-09-27 02:24:11,767 Stage: Train 0.5 | Epoch: 124 | Iter: 188600 | Total Loss: 0.006001 | Recon Loss: 0.004941 | Commit Loss: 0.002119 | Perplexity: 1350.177670
2025-09-27 02:24:56,904 Stage: Train 0.5 | Epoch: 124 | Iter: 188800 | Total Loss: 0.006032 | Recon Loss: 0.004977 | Commit Loss: 0.002109 | Perplexity: 1352.218593
2025-09-27 02:25:42,357 Stage: Train 0.5 | Epoch: 124 | Iter: 189000 | Total Loss: 0.006009 | Recon Loss: 0.004952 | Commit Loss: 0.002115 | Perplexity: 1348.637835
2025-09-27 02:26:27,792 Stage: Train 0.5 | Epoch: 124 | Iter: 189200 | Total Loss: 0.006071 | Recon Loss: 0.005017 | Commit Loss: 0.002109 | Perplexity: 1349.637513
2025-09-27 02:27:13,309 Stage: Train 0.5 | Epoch: 124 | Iter: 189400 | Total Loss: 0.005993 | Recon Loss: 0.004940 | Commit Loss: 0.002106 | Perplexity: 1347.642386
2025-09-27 02:27:58,442 Stage: Train 0.5 | Epoch: 124 | Iter: 189600 | Total Loss: 0.005980 | Recon Loss: 0.004922 | Commit Loss: 0.002117 | Perplexity: 1350.536566
2025-09-27 02:28:43,992 Stage: Train 0.5 | Epoch: 124 | Iter: 189800 | Total Loss: 0.006076 | Recon Loss: 0.005024 | Commit Loss: 0.002104 | Perplexity: 1347.101162
Trainning Epoch:  38%|███▊      | 125/330 [12:17:51<19:37:01, 344.50s/it]2025-09-27 02:29:29,719 Stage: Train 0.5 | Epoch: 125 | Iter: 190000 | Total Loss: 0.006018 | Recon Loss: 0.004963 | Commit Loss: 0.002110 | Perplexity: 1351.020677
2025-09-27 02:30:15,336 Stage: Train 0.5 | Epoch: 125 | Iter: 190200 | Total Loss: 0.006032 | Recon Loss: 0.004975 | Commit Loss: 0.002114 | Perplexity: 1347.442065
2025-09-27 02:31:00,879 Stage: Train 0.5 | Epoch: 125 | Iter: 190400 | Total Loss: 0.006025 | Recon Loss: 0.004962 | Commit Loss: 0.002125 | Perplexity: 1352.492537
2025-09-27 02:31:45,925 Stage: Train 0.5 | Epoch: 125 | Iter: 190600 | Total Loss: 0.006000 | Recon Loss: 0.004948 | Commit Loss: 0.002104 | Perplexity: 1349.600148
2025-09-27 02:32:31,218 Stage: Train 0.5 | Epoch: 125 | Iter: 190800 | Total Loss: 0.006044 | Recon Loss: 0.004988 | Commit Loss: 0.002114 | Perplexity: 1352.837186
2025-09-27 02:33:16,597 Stage: Train 0.5 | Epoch: 125 | Iter: 191000 | Total Loss: 0.006047 | Recon Loss: 0.004986 | Commit Loss: 0.002122 | Perplexity: 1350.827149
2025-09-27 02:34:02,006 Stage: Train 0.5 | Epoch: 125 | Iter: 191200 | Total Loss: 0.006001 | Recon Loss: 0.004945 | Commit Loss: 0.002111 | Perplexity: 1352.630663
Trainning Epoch:  38%|███▊      | 126/330 [12:23:36<19:31:35, 344.58s/it]2025-09-27 02:34:47,440 Stage: Train 0.5 | Epoch: 126 | Iter: 191400 | Total Loss: 0.006071 | Recon Loss: 0.005015 | Commit Loss: 0.002112 | Perplexity: 1351.371815
2025-09-27 02:35:32,952 Stage: Train 0.5 | Epoch: 126 | Iter: 191600 | Total Loss: 0.005947 | Recon Loss: 0.004893 | Commit Loss: 0.002109 | Perplexity: 1350.441389
2025-09-27 02:36:18,742 Stage: Train 0.5 | Epoch: 126 | Iter: 191800 | Total Loss: 0.005993 | Recon Loss: 0.004934 | Commit Loss: 0.002118 | Perplexity: 1352.117986
2025-09-27 02:37:04,174 Stage: Train 0.5 | Epoch: 126 | Iter: 192000 | Total Loss: 0.006061 | Recon Loss: 0.004999 | Commit Loss: 0.002123 | Perplexity: 1353.087327
2025-09-27 02:37:49,679 Stage: Train 0.5 | Epoch: 126 | Iter: 192200 | Total Loss: 0.006031 | Recon Loss: 0.004973 | Commit Loss: 0.002117 | Perplexity: 1351.537588
2025-09-27 02:38:34,756 Stage: Train 0.5 | Epoch: 126 | Iter: 192400 | Total Loss: 0.006046 | Recon Loss: 0.004989 | Commit Loss: 0.002113 | Perplexity: 1351.856738
2025-09-27 02:39:20,097 Stage: Train 0.5 | Epoch: 126 | Iter: 192600 | Total Loss: 0.005934 | Recon Loss: 0.004878 | Commit Loss: 0.002112 | Perplexity: 1349.334719
2025-09-27 02:40:05,876 Stage: Train 0.5 | Epoch: 126 | Iter: 192800 | Total Loss: 0.005977 | Recon Loss: 0.004917 | Commit Loss: 0.002119 | Perplexity: 1349.266379
Trainning Epoch:  38%|███▊      | 127/330 [12:29:22<19:27:02, 344.94s/it]2025-09-27 02:40:51,622 Stage: Train 0.5 | Epoch: 127 | Iter: 193000 | Total Loss: 0.006061 | Recon Loss: 0.005008 | Commit Loss: 0.002106 | Perplexity: 1347.615726
2025-09-27 02:41:36,843 Stage: Train 0.5 | Epoch: 127 | Iter: 193200 | Total Loss: 0.005981 | Recon Loss: 0.004926 | Commit Loss: 0.002110 | Perplexity: 1348.375337
2025-09-27 02:42:22,180 Stage: Train 0.5 | Epoch: 127 | Iter: 193400 | Total Loss: 0.006064 | Recon Loss: 0.005007 | Commit Loss: 0.002113 | Perplexity: 1350.955269
2025-09-27 02:43:07,657 Stage: Train 0.5 | Epoch: 127 | Iter: 193600 | Total Loss: 0.005986 | Recon Loss: 0.004925 | Commit Loss: 0.002124 | Perplexity: 1355.638343
2025-09-27 02:43:53,198 Stage: Train 0.5 | Epoch: 127 | Iter: 193800 | Total Loss: 0.005984 | Recon Loss: 0.004927 | Commit Loss: 0.002115 | Perplexity: 1351.318709
2025-09-27 02:44:38,442 Stage: Train 0.5 | Epoch: 127 | Iter: 194000 | Total Loss: 0.005941 | Recon Loss: 0.004891 | Commit Loss: 0.002099 | Perplexity: 1347.483102
2025-09-27 02:45:23,816 Stage: Train 0.5 | Epoch: 127 | Iter: 194200 | Total Loss: 0.006085 | Recon Loss: 0.005030 | Commit Loss: 0.002108 | Perplexity: 1352.293950
2025-09-27 02:46:09,259 Stage: Train 0.5 | Epoch: 127 | Iter: 194400 | Total Loss: 0.006029 | Recon Loss: 0.004974 | Commit Loss: 0.002110 | Perplexity: 1351.690413
Trainning Epoch:  39%|███▉      | 128/330 [12:35:07<19:21:17, 344.94s/it]2025-09-27 02:46:55,013 Stage: Train 0.5 | Epoch: 128 | Iter: 194600 | Total Loss: 0.005949 | Recon Loss: 0.004895 | Commit Loss: 0.002107 | Perplexity: 1347.661733
2025-09-27 02:47:40,402 Stage: Train 0.5 | Epoch: 128 | Iter: 194800 | Total Loss: 0.005999 | Recon Loss: 0.004939 | Commit Loss: 0.002119 | Perplexity: 1352.887097
2025-09-27 02:48:25,483 Stage: Train 0.5 | Epoch: 128 | Iter: 195000 | Total Loss: 0.005945 | Recon Loss: 0.004891 | Commit Loss: 0.002110 | Perplexity: 1350.612474
2025-09-27 02:49:11,041 Stage: Train 0.5 | Epoch: 128 | Iter: 195200 | Total Loss: 0.006018 | Recon Loss: 0.004961 | Commit Loss: 0.002114 | Perplexity: 1352.659303
2025-09-27 02:49:56,474 Stage: Train 0.5 | Epoch: 128 | Iter: 195400 | Total Loss: 0.006035 | Recon Loss: 0.004975 | Commit Loss: 0.002119 | Perplexity: 1349.258826
2025-09-27 02:50:41,936 Stage: Train 0.5 | Epoch: 128 | Iter: 195600 | Total Loss: 0.005950 | Recon Loss: 0.004896 | Commit Loss: 0.002109 | Perplexity: 1351.488969
2025-09-27 02:51:27,364 Stage: Train 0.5 | Epoch: 128 | Iter: 195800 | Total Loss: 0.006109 | Recon Loss: 0.005044 | Commit Loss: 0.002130 | Perplexity: 1354.998247
Trainning Epoch:  39%|███▉      | 129/330 [12:40:52<19:15:53, 345.04s/it]2025-09-27 02:52:13,265 Stage: Train 0.5 | Epoch: 129 | Iter: 196000 | Total Loss: 0.005958 | Recon Loss: 0.004900 | Commit Loss: 0.002114 | Perplexity: 1349.226767
2025-09-27 02:52:58,934 Stage: Train 0.5 | Epoch: 129 | Iter: 196200 | Total Loss: 0.005973 | Recon Loss: 0.004915 | Commit Loss: 0.002115 | Perplexity: 1351.570298
2025-09-27 02:53:44,261 Stage: Train 0.5 | Epoch: 129 | Iter: 196400 | Total Loss: 0.005918 | Recon Loss: 0.004866 | Commit Loss: 0.002105 | Perplexity: 1350.665314
2025-09-27 02:54:29,815 Stage: Train 0.5 | Epoch: 129 | Iter: 196600 | Total Loss: 0.006070 | Recon Loss: 0.005011 | Commit Loss: 0.002119 | Perplexity: 1352.655621
2025-09-27 02:55:15,009 Stage: Train 0.5 | Epoch: 129 | Iter: 196800 | Total Loss: 0.005975 | Recon Loss: 0.004919 | Commit Loss: 0.002113 | Perplexity: 1350.802160
2025-09-27 02:56:00,935 Stage: Train 0.5 | Epoch: 129 | Iter: 197000 | Total Loss: 0.005961 | Recon Loss: 0.004910 | Commit Loss: 0.002103 | Perplexity: 1349.853757
2025-09-27 02:56:46,632 Stage: Train 0.5 | Epoch: 129 | Iter: 197200 | Total Loss: 0.005984 | Recon Loss: 0.004927 | Commit Loss: 0.002115 | Perplexity: 1353.412524
2025-09-27 02:57:32,469 Stage: Train 0.5 | Epoch: 129 | Iter: 197400 | Total Loss: 0.005943 | Recon Loss: 0.004886 | Commit Loss: 0.002113 | Perplexity: 1353.113858
Trainning Epoch:  39%|███▉      | 130/330 [12:46:39<19:11:48, 345.54s/it]2025-09-27 02:58:18,049 Stage: Train 0.5 | Epoch: 130 | Iter: 197600 | Total Loss: 0.005956 | Recon Loss: 0.004902 | Commit Loss: 0.002107 | Perplexity: 1349.574072
2025-09-27 02:59:03,622 Stage: Train 0.5 | Epoch: 130 | Iter: 197800 | Total Loss: 0.005987 | Recon Loss: 0.004930 | Commit Loss: 0.002115 | Perplexity: 1352.205443
2025-09-27 02:59:49,109 Stage: Train 0.5 | Epoch: 130 | Iter: 198000 | Total Loss: 0.005984 | Recon Loss: 0.004931 | Commit Loss: 0.002105 | Perplexity: 1354.734901
2025-09-27 03:00:34,844 Stage: Train 0.5 | Epoch: 130 | Iter: 198200 | Total Loss: 0.005993 | Recon Loss: 0.004940 | Commit Loss: 0.002106 | Perplexity: 1351.163022
2025-09-27 03:01:18,872 Stage: Train 0.5 | Epoch: 130 | Iter: 198400 | Total Loss: 0.005923 | Recon Loss: 0.004865 | Commit Loss: 0.002116 | Perplexity: 1350.185682
2025-09-27 03:02:04,122 Stage: Train 0.5 | Epoch: 130 | Iter: 198600 | Total Loss: 0.006064 | Recon Loss: 0.005006 | Commit Loss: 0.002117 | Perplexity: 1352.762970
2025-09-27 03:02:49,716 Stage: Train 0.5 | Epoch: 130 | Iter: 198800 | Total Loss: 0.005934 | Recon Loss: 0.004872 | Commit Loss: 0.002126 | Perplexity: 1353.828592
Trainning Epoch:  40%|███▉      | 131/330 [12:52:23<19:04:34, 345.10s/it]2025-09-27 03:03:35,375 Stage: Train 0.5 | Epoch: 131 | Iter: 199000 | Total Loss: 0.006026 | Recon Loss: 0.004970 | Commit Loss: 0.002112 | Perplexity: 1350.782105
2025-09-27 03:04:20,929 Stage: Train 0.5 | Epoch: 131 | Iter: 199200 | Total Loss: 0.005946 | Recon Loss: 0.004894 | Commit Loss: 0.002104 | Perplexity: 1349.335289
2025-09-27 03:05:06,235 Stage: Train 0.5 | Epoch: 131 | Iter: 199400 | Total Loss: 0.005965 | Recon Loss: 0.004909 | Commit Loss: 0.002111 | Perplexity: 1351.754390
2025-09-27 03:05:51,688 Stage: Train 0.5 | Epoch: 131 | Iter: 199600 | Total Loss: 0.006069 | Recon Loss: 0.005004 | Commit Loss: 0.002130 | Perplexity: 1354.031747
2025-09-27 03:06:37,444 Stage: Train 0.5 | Epoch: 131 | Iter: 199800 | Total Loss: 0.005925 | Recon Loss: 0.004871 | Commit Loss: 0.002107 | Perplexity: 1353.705120
2025-09-27 03:07:23,004 Stage: Train 0.5 | Epoch: 131 | Iter: 200000 | Total Loss: 0.005949 | Recon Loss: 0.004889 | Commit Loss: 0.002120 | Perplexity: 1351.371714
2025-09-27 03:07:23,004 Saving model at iteration 200000
2025-09-27 03:07:23,209 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000
2025-09-27 03:07:23,503 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/model.safetensors
2025-09-27 03:07:23,902 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/optimizer.bin
2025-09-27 03:07:23,902 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/scheduler.bin
2025-09-27 03:07:23,902 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/sampler.bin
2025-09-27 03:07:23,903 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/random_states_0.pkl
2025-09-27 03:08:09,525 Stage: Train 0.5 | Epoch: 131 | Iter: 200200 | Total Loss: 0.005988 | Recon Loss: 0.004923 | Commit Loss: 0.002131 | Perplexity: 1356.194174
2025-09-27 03:08:54,995 Stage: Train 0.5 | Epoch: 131 | Iter: 200400 | Total Loss: 0.005932 | Recon Loss: 0.004876 | Commit Loss: 0.002112 | Perplexity: 1350.260667
Trainning Epoch:  40%|████      | 132/330 [12:58:10<19:00:46, 345.69s/it]2025-09-27 03:09:40,986 Stage: Train 0.5 | Epoch: 132 | Iter: 200600 | Total Loss: 0.005951 | Recon Loss: 0.004894 | Commit Loss: 0.002114 | Perplexity: 1349.952396
2025-09-27 03:10:26,509 Stage: Train 0.5 | Epoch: 132 | Iter: 200800 | Total Loss: 0.005917 | Recon Loss: 0.004867 | Commit Loss: 0.002100 | Perplexity: 1350.955157
2025-09-27 03:11:12,191 Stage: Train 0.5 | Epoch: 132 | Iter: 201000 | Total Loss: 0.005971 | Recon Loss: 0.004917 | Commit Loss: 0.002107 | Perplexity: 1350.732219
2025-09-27 03:11:57,932 Stage: Train 0.5 | Epoch: 132 | Iter: 201200 | Total Loss: 0.005946 | Recon Loss: 0.004891 | Commit Loss: 0.002110 | Perplexity: 1350.872761
2025-09-27 03:12:43,644 Stage: Train 0.5 | Epoch: 132 | Iter: 201400 | Total Loss: 0.005941 | Recon Loss: 0.004877 | Commit Loss: 0.002128 | Perplexity: 1353.921982
2025-09-27 03:13:29,247 Stage: Train 0.5 | Epoch: 132 | Iter: 201600 | Total Loss: 0.005955 | Recon Loss: 0.004893 | Commit Loss: 0.002124 | Perplexity: 1355.314356
2025-09-27 03:14:14,760 Stage: Train 0.5 | Epoch: 132 | Iter: 201800 | Total Loss: 0.005946 | Recon Loss: 0.004895 | Commit Loss: 0.002104 | Perplexity: 1350.323458
2025-09-27 03:15:00,559 Stage: Train 0.5 | Epoch: 132 | Iter: 202000 | Total Loss: 0.005994 | Recon Loss: 0.004934 | Commit Loss: 0.002120 | Perplexity: 1355.064624
Trainning Epoch:  40%|████      | 133/330 [13:03:57<18:56:07, 346.03s/it]2025-09-27 03:15:46,107 Stage: Train 0.5 | Epoch: 133 | Iter: 202200 | Total Loss: 0.005954 | Recon Loss: 0.004899 | Commit Loss: 0.002109 | Perplexity: 1350.494111
2025-09-27 03:16:31,835 Stage: Train 0.5 | Epoch: 133 | Iter: 202400 | Total Loss: 0.005933 | Recon Loss: 0.004877 | Commit Loss: 0.002111 | Perplexity: 1354.035143
2025-09-27 03:17:17,442 Stage: Train 0.5 | Epoch: 133 | Iter: 202600 | Total Loss: 0.005914 | Recon Loss: 0.004859 | Commit Loss: 0.002109 | Perplexity: 1350.569487
2025-09-27 03:18:02,807 Stage: Train 0.5 | Epoch: 133 | Iter: 202800 | Total Loss: 0.005941 | Recon Loss: 0.004882 | Commit Loss: 0.002117 | Perplexity: 1351.609193
2025-09-27 03:18:47,872 Stage: Train 0.5 | Epoch: 133 | Iter: 203000 | Total Loss: 0.005995 | Recon Loss: 0.004934 | Commit Loss: 0.002122 | Perplexity: 1352.682718
2025-09-27 03:19:33,279 Stage: Train 0.5 | Epoch: 133 | Iter: 203200 | Total Loss: 0.005953 | Recon Loss: 0.004896 | Commit Loss: 0.002113 | Perplexity: 1353.196523
2025-09-27 03:20:18,919 Stage: Train 0.5 | Epoch: 133 | Iter: 203400 | Total Loss: 0.005983 | Recon Loss: 0.004923 | Commit Loss: 0.002121 | Perplexity: 1353.482002
Trainning Epoch:  41%|████      | 134/330 [13:09:42<18:50:03, 345.94s/it]2025-09-27 03:21:04,730 Stage: Train 0.5 | Epoch: 134 | Iter: 203600 | Total Loss: 0.005902 | Recon Loss: 0.004849 | Commit Loss: 0.002106 | Perplexity: 1351.290345
2025-09-27 03:21:49,748 Stage: Train 0.5 | Epoch: 134 | Iter: 203800 | Total Loss: 0.005895 | Recon Loss: 0.004845 | Commit Loss: 0.002101 | Perplexity: 1350.289332
2025-09-27 03:22:35,451 Stage: Train 0.5 | Epoch: 134 | Iter: 204000 | Total Loss: 0.005960 | Recon Loss: 0.004903 | Commit Loss: 0.002114 | Perplexity: 1352.528433
2025-09-27 03:23:21,126 Stage: Train 0.5 | Epoch: 134 | Iter: 204200 | Total Loss: 0.005945 | Recon Loss: 0.004881 | Commit Loss: 0.002128 | Perplexity: 1353.075171
2025-09-27 03:24:06,802 Stage: Train 0.5 | Epoch: 134 | Iter: 204400 | Total Loss: 0.005956 | Recon Loss: 0.004899 | Commit Loss: 0.002115 | Perplexity: 1353.788373
2025-09-27 03:24:52,465 Stage: Train 0.5 | Epoch: 134 | Iter: 204600 | Total Loss: 0.005929 | Recon Loss: 0.004879 | Commit Loss: 0.002099 | Perplexity: 1349.370906
2025-09-27 03:25:37,852 Stage: Train 0.5 | Epoch: 134 | Iter: 204800 | Total Loss: 0.005899 | Recon Loss: 0.004837 | Commit Loss: 0.002125 | Perplexity: 1353.413219
2025-09-27 03:26:23,637 Stage: Train 0.5 | Epoch: 134 | Iter: 205000 | Total Loss: 0.006041 | Recon Loss: 0.004982 | Commit Loss: 0.002117 | Perplexity: 1353.382875
Trainning Epoch:  41%|████      | 135/330 [13:15:29<18:44:43, 346.07s/it]2025-09-27 03:27:09,898 Stage: Train 0.5 | Epoch: 135 | Iter: 205200 | Total Loss: 0.005885 | Recon Loss: 0.004833 | Commit Loss: 0.002104 | Perplexity: 1353.507535
2025-09-27 03:27:55,885 Stage: Train 0.5 | Epoch: 135 | Iter: 205400 | Total Loss: 0.005920 | Recon Loss: 0.004868 | Commit Loss: 0.002104 | Perplexity: 1351.526832
2025-09-27 03:28:41,558 Stage: Train 0.5 | Epoch: 135 | Iter: 205600 | Total Loss: 0.005952 | Recon Loss: 0.004891 | Commit Loss: 0.002123 | Perplexity: 1358.386905
2025-09-27 03:29:27,366 Stage: Train 0.5 | Epoch: 135 | Iter: 205800 | Total Loss: 0.005934 | Recon Loss: 0.004875 | Commit Loss: 0.002117 | Perplexity: 1353.879309
2025-09-27 03:30:13,596 Stage: Train 0.5 | Epoch: 135 | Iter: 206000 | Total Loss: 0.005942 | Recon Loss: 0.004886 | Commit Loss: 0.002113 | Perplexity: 1353.457574
2025-09-27 03:30:59,572 Stage: Train 0.5 | Epoch: 135 | Iter: 206200 | Total Loss: 0.005934 | Recon Loss: 0.004880 | Commit Loss: 0.002108 | Perplexity: 1353.658967
2025-09-27 03:31:45,599 Stage: Train 0.5 | Epoch: 135 | Iter: 206400 | Total Loss: 0.005890 | Recon Loss: 0.004833 | Commit Loss: 0.002113 | Perplexity: 1352.643118
Trainning Epoch:  41%|████      | 136/330 [13:21:18<18:41:53, 346.97s/it]2025-09-27 03:32:31,642 Stage: Train 0.5 | Epoch: 136 | Iter: 206600 | Total Loss: 0.005939 | Recon Loss: 0.004874 | Commit Loss: 0.002129 | Perplexity: 1352.943479
2025-09-27 03:33:17,567 Stage: Train 0.5 | Epoch: 136 | Iter: 206800 | Total Loss: 0.005914 | Recon Loss: 0.004861 | Commit Loss: 0.002108 | Perplexity: 1353.429663
2025-09-27 03:34:03,324 Stage: Train 0.5 | Epoch: 136 | Iter: 207000 | Total Loss: 0.005923 | Recon Loss: 0.004868 | Commit Loss: 0.002111 | Perplexity: 1352.541976
2025-09-27 03:34:49,144 Stage: Train 0.5 | Epoch: 136 | Iter: 207200 | Total Loss: 0.005874 | Recon Loss: 0.004818 | Commit Loss: 0.002113 | Perplexity: 1350.187590
2025-09-27 03:35:34,855 Stage: Train 0.5 | Epoch: 136 | Iter: 207400 | Total Loss: 0.005895 | Recon Loss: 0.004837 | Commit Loss: 0.002117 | Perplexity: 1355.059838
2025-09-27 03:36:20,742 Stage: Train 0.5 | Epoch: 136 | Iter: 207600 | Total Loss: 0.005947 | Recon Loss: 0.004887 | Commit Loss: 0.002120 | Perplexity: 1355.785967
2025-09-27 03:37:06,739 Stage: Train 0.5 | Epoch: 136 | Iter: 207800 | Total Loss: 0.005929 | Recon Loss: 0.004864 | Commit Loss: 0.002129 | Perplexity: 1357.365873
2025-09-27 03:37:52,641 Stage: Train 0.5 | Epoch: 136 | Iter: 208000 | Total Loss: 0.005906 | Recon Loss: 0.004854 | Commit Loss: 0.002105 | Perplexity: 1353.631196
Trainning Epoch:  42%|████▏     | 137/330 [13:27:07<18:37:43, 347.48s/it]2025-09-27 03:38:38,963 Stage: Train 0.5 | Epoch: 137 | Iter: 208200 | Total Loss: 0.005915 | Recon Loss: 0.004857 | Commit Loss: 0.002116 | Perplexity: 1354.413882
2025-09-27 03:39:24,472 Stage: Train 0.5 | Epoch: 137 | Iter: 208400 | Total Loss: 0.005911 | Recon Loss: 0.004856 | Commit Loss: 0.002110 | Perplexity: 1354.372092
2025-09-27 03:40:10,396 Stage: Train 0.5 | Epoch: 137 | Iter: 208600 | Total Loss: 0.005890 | Recon Loss: 0.004840 | Commit Loss: 0.002100 | Perplexity: 1351.722000
2025-09-27 03:40:56,413 Stage: Train 0.5 | Epoch: 137 | Iter: 208800 | Total Loss: 0.005917 | Recon Loss: 0.004864 | Commit Loss: 0.002106 | Perplexity: 1352.215174
2025-09-27 03:41:42,245 Stage: Train 0.5 | Epoch: 137 | Iter: 209000 | Total Loss: 0.005908 | Recon Loss: 0.004848 | Commit Loss: 0.002119 | Perplexity: 1354.759858
2025-09-27 03:42:27,783 Stage: Train 0.5 | Epoch: 137 | Iter: 209200 | Total Loss: 0.005871 | Recon Loss: 0.004815 | Commit Loss: 0.002113 | Perplexity: 1356.709298
2025-09-27 03:43:13,473 Stage: Train 0.5 | Epoch: 137 | Iter: 209400 | Total Loss: 0.005988 | Recon Loss: 0.004933 | Commit Loss: 0.002111 | Perplexity: 1352.991122
2025-09-27 03:43:59,284 Stage: Train 0.5 | Epoch: 137 | Iter: 209600 | Total Loss: 0.005872 | Recon Loss: 0.004812 | Commit Loss: 0.002119 | Perplexity: 1353.519698
Trainning Epoch:  42%|████▏     | 138/330 [13:32:55<18:32:24, 347.63s/it]2025-09-27 03:44:45,329 Stage: Train 0.5 | Epoch: 138 | Iter: 209800 | Total Loss: 0.005917 | Recon Loss: 0.004857 | Commit Loss: 0.002120 | Perplexity: 1355.266777
2025-09-27 03:45:29,699 Stage: Train 0.5 | Epoch: 138 | Iter: 210000 | Total Loss: 0.005842 | Recon Loss: 0.004789 | Commit Loss: 0.002106 | Perplexity: 1354.334219
2025-09-27 03:46:15,373 Stage: Train 0.5 | Epoch: 138 | Iter: 210200 | Total Loss: 0.005895 | Recon Loss: 0.004837 | Commit Loss: 0.002116 | Perplexity: 1350.648920
2025-09-27 03:47:00,960 Stage: Train 0.5 | Epoch: 138 | Iter: 210400 | Total Loss: 0.005890 | Recon Loss: 0.004839 | Commit Loss: 0.002102 | Perplexity: 1352.388594
2025-09-27 03:47:46,728 Stage: Train 0.5 | Epoch: 138 | Iter: 210600 | Total Loss: 0.005931 | Recon Loss: 0.004874 | Commit Loss: 0.002114 | Perplexity: 1356.596339
2025-09-27 03:48:32,365 Stage: Train 0.5 | Epoch: 138 | Iter: 210800 | Total Loss: 0.005882 | Recon Loss: 0.004826 | Commit Loss: 0.002113 | Perplexity: 1356.003287
2025-09-27 03:49:18,004 Stage: Train 0.5 | Epoch: 138 | Iter: 211000 | Total Loss: 0.005895 | Recon Loss: 0.004831 | Commit Loss: 0.002126 | Perplexity: 1359.835920
Trainning Epoch:  42%|████▏     | 139/330 [13:38:40<18:25:01, 347.13s/it]2025-09-27 03:50:04,118 Stage: Train 0.5 | Epoch: 139 | Iter: 211200 | Total Loss: 0.005896 | Recon Loss: 0.004839 | Commit Loss: 0.002114 | Perplexity: 1352.152390
2025-09-27 03:50:49,870 Stage: Train 0.5 | Epoch: 139 | Iter: 211400 | Total Loss: 0.005847 | Recon Loss: 0.004795 | Commit Loss: 0.002105 | Perplexity: 1356.131448
2025-09-27 03:51:35,750 Stage: Train 0.5 | Epoch: 139 | Iter: 211600 | Total Loss: 0.006027 | Recon Loss: 0.004966 | Commit Loss: 0.002123 | Perplexity: 1355.300686
2025-09-27 03:52:21,363 Stage: Train 0.5 | Epoch: 139 | Iter: 211800 | Total Loss: 0.005914 | Recon Loss: 0.004861 | Commit Loss: 0.002107 | Perplexity: 1355.242346
2025-09-27 03:53:07,027 Stage: Train 0.5 | Epoch: 139 | Iter: 212000 | Total Loss: 0.005913 | Recon Loss: 0.004857 | Commit Loss: 0.002112 | Perplexity: 1350.854476
2025-09-27 03:53:52,779 Stage: Train 0.5 | Epoch: 139 | Iter: 212200 | Total Loss: 0.005888 | Recon Loss: 0.004833 | Commit Loss: 0.002111 | Perplexity: 1353.358974
2025-09-27 03:54:38,677 Stage: Train 0.5 | Epoch: 139 | Iter: 212400 | Total Loss: 0.005913 | Recon Loss: 0.004851 | Commit Loss: 0.002125 | Perplexity: 1357.615059
2025-09-27 03:55:24,506 Stage: Train 0.5 | Epoch: 139 | Iter: 212600 | Total Loss: 0.005896 | Recon Loss: 0.004839 | Commit Loss: 0.002114 | Perplexity: 1355.705309
Trainning Epoch:  42%|████▏     | 140/330 [13:44:28<18:19:52, 347.33s/it]2025-09-27 03:56:10,313 Stage: Train 0.5 | Epoch: 140 | Iter: 212800 | Total Loss: 0.005884 | Recon Loss: 0.004831 | Commit Loss: 0.002107 | Perplexity: 1351.157944
2025-09-27 03:56:56,111 Stage: Train 0.5 | Epoch: 140 | Iter: 213000 | Total Loss: 0.005876 | Recon Loss: 0.004822 | Commit Loss: 0.002107 | Perplexity: 1353.661638
2025-09-27 03:57:41,920 Stage: Train 0.5 | Epoch: 140 | Iter: 213200 | Total Loss: 0.005886 | Recon Loss: 0.004834 | Commit Loss: 0.002104 | Perplexity: 1355.664042
2025-09-27 03:58:27,497 Stage: Train 0.5 | Epoch: 140 | Iter: 213400 | Total Loss: 0.005853 | Recon Loss: 0.004797 | Commit Loss: 0.002113 | Perplexity: 1354.999704
2025-09-27 03:59:13,030 Stage: Train 0.5 | Epoch: 140 | Iter: 213600 | Total Loss: 0.005946 | Recon Loss: 0.004891 | Commit Loss: 0.002109 | Perplexity: 1356.021625
2025-09-27 03:59:58,936 Stage: Train 0.5 | Epoch: 140 | Iter: 213800 | Total Loss: 0.005880 | Recon Loss: 0.004824 | Commit Loss: 0.002113 | Perplexity: 1354.134611
2025-09-27 04:00:44,563 Stage: Train 0.5 | Epoch: 140 | Iter: 214000 | Total Loss: 0.005876 | Recon Loss: 0.004822 | Commit Loss: 0.002108 | Perplexity: 1355.608825
Trainning Epoch:  43%|████▎     | 141/330 [13:50:16<18:14:05, 347.33s/it]2025-09-27 04:01:30,436 Stage: Train 0.5 | Epoch: 141 | Iter: 214200 | Total Loss: 0.005881 | Recon Loss: 0.004821 | Commit Loss: 0.002119 | Perplexity: 1357.637628
2025-09-27 04:02:16,018 Stage: Train 0.5 | Epoch: 141 | Iter: 214400 | Total Loss: 0.005914 | Recon Loss: 0.004860 | Commit Loss: 0.002108 | Perplexity: 1355.181520
2025-09-27 04:03:01,568 Stage: Train 0.5 | Epoch: 141 | Iter: 214600 | Total Loss: 0.005881 | Recon Loss: 0.004830 | Commit Loss: 0.002102 | Perplexity: 1354.627465
2025-09-27 04:03:47,555 Stage: Train 0.5 | Epoch: 141 | Iter: 214800 | Total Loss: 0.005866 | Recon Loss: 0.004817 | Commit Loss: 0.002097 | Perplexity: 1353.001233
2025-09-27 04:04:33,588 Stage: Train 0.5 | Epoch: 141 | Iter: 215000 | Total Loss: 0.005849 | Recon Loss: 0.004798 | Commit Loss: 0.002101 | Perplexity: 1353.690909
2025-09-27 04:05:19,655 Stage: Train 0.5 | Epoch: 141 | Iter: 215200 | Total Loss: 0.005918 | Recon Loss: 0.004861 | Commit Loss: 0.002113 | Perplexity: 1355.637451
2025-09-27 04:06:05,588 Stage: Train 0.5 | Epoch: 141 | Iter: 215400 | Total Loss: 0.005880 | Recon Loss: 0.004823 | Commit Loss: 0.002114 | Perplexity: 1356.176911
2025-09-27 04:06:51,521 Stage: Train 0.5 | Epoch: 141 | Iter: 215600 | Total Loss: 0.005883 | Recon Loss: 0.004822 | Commit Loss: 0.002122 | Perplexity: 1356.760098
Trainning Epoch:  43%|████▎     | 142/330 [13:56:04<18:09:27, 347.70s/it]2025-09-27 04:07:37,638 Stage: Train 0.5 | Epoch: 142 | Iter: 215800 | Total Loss: 0.005919 | Recon Loss: 0.004866 | Commit Loss: 0.002106 | Perplexity: 1354.963644
2025-09-27 04:08:23,524 Stage: Train 0.5 | Epoch: 142 | Iter: 216000 | Total Loss: 0.005878 | Recon Loss: 0.004826 | Commit Loss: 0.002105 | Perplexity: 1355.249189
2025-09-27 04:09:09,002 Stage: Train 0.5 | Epoch: 142 | Iter: 216200 | Total Loss: 0.005841 | Recon Loss: 0.004793 | Commit Loss: 0.002095 | Perplexity: 1351.762399
2025-09-27 04:09:54,751 Stage: Train 0.5 | Epoch: 142 | Iter: 216400 | Total Loss: 0.005941 | Recon Loss: 0.004884 | Commit Loss: 0.002114 | Perplexity: 1353.587495
2025-09-27 04:10:40,946 Stage: Train 0.5 | Epoch: 142 | Iter: 216600 | Total Loss: 0.005832 | Recon Loss: 0.004777 | Commit Loss: 0.002110 | Perplexity: 1355.713172
2025-09-27 04:11:26,995 Stage: Train 0.5 | Epoch: 142 | Iter: 216800 | Total Loss: 0.005860 | Recon Loss: 0.004805 | Commit Loss: 0.002109 | Perplexity: 1354.804535
2025-09-27 04:12:12,726 Stage: Train 0.5 | Epoch: 142 | Iter: 217000 | Total Loss: 0.005903 | Recon Loss: 0.004842 | Commit Loss: 0.002123 | Perplexity: 1355.073725
2025-09-27 04:12:58,378 Stage: Train 0.5 | Epoch: 142 | Iter: 217200 | Total Loss: 0.005892 | Recon Loss: 0.004835 | Commit Loss: 0.002113 | Perplexity: 1355.410387
Trainning Epoch:  43%|████▎     | 143/330 [14:01:53<18:04:15, 347.89s/it]2025-09-27 04:13:44,613 Stage: Train 0.5 | Epoch: 143 | Iter: 217400 | Total Loss: 0.005831 | Recon Loss: 0.004782 | Commit Loss: 0.002097 | Perplexity: 1352.835079
2025-09-27 04:14:30,624 Stage: Train 0.5 | Epoch: 143 | Iter: 217600 | Total Loss: 0.005882 | Recon Loss: 0.004831 | Commit Loss: 0.002103 | Perplexity: 1356.664839
2025-09-27 04:15:16,422 Stage: Train 0.5 | Epoch: 143 | Iter: 217800 | Total Loss: 0.005852 | Recon Loss: 0.004795 | Commit Loss: 0.002114 | Perplexity: 1357.390701
2025-09-27 04:16:02,296 Stage: Train 0.5 | Epoch: 143 | Iter: 218000 | Total Loss: 0.005840 | Recon Loss: 0.004788 | Commit Loss: 0.002103 | Perplexity: 1353.650343
2025-09-27 04:16:48,096 Stage: Train 0.5 | Epoch: 143 | Iter: 218200 | Total Loss: 0.005943 | Recon Loss: 0.004888 | Commit Loss: 0.002111 | Perplexity: 1355.930704
2025-09-27 04:17:33,895 Stage: Train 0.5 | Epoch: 143 | Iter: 218400 | Total Loss: 0.005929 | Recon Loss: 0.004871 | Commit Loss: 0.002116 | Perplexity: 1355.512047
2025-09-27 04:18:19,808 Stage: Train 0.5 | Epoch: 143 | Iter: 218600 | Total Loss: 0.005847 | Recon Loss: 0.004795 | Commit Loss: 0.002104 | Perplexity: 1354.980247
Trainning Epoch:  44%|████▎     | 144/330 [14:07:41<17:59:20, 348.17s/it]2025-09-27 04:19:05,788 Stage: Train 0.5 | Epoch: 144 | Iter: 218800 | Total Loss: 0.005792 | Recon Loss: 0.004739 | Commit Loss: 0.002106 | Perplexity: 1354.502442
2025-09-27 04:19:51,725 Stage: Train 0.5 | Epoch: 144 | Iter: 219000 | Total Loss: 0.005843 | Recon Loss: 0.004794 | Commit Loss: 0.002098 | Perplexity: 1353.539616
2025-09-27 04:20:37,863 Stage: Train 0.5 | Epoch: 144 | Iter: 219200 | Total Loss: 0.005853 | Recon Loss: 0.004800 | Commit Loss: 0.002106 | Perplexity: 1353.875094
2025-09-27 04:21:23,889 Stage: Train 0.5 | Epoch: 144 | Iter: 219400 | Total Loss: 0.005862 | Recon Loss: 0.004804 | Commit Loss: 0.002115 | Perplexity: 1356.334749
2025-09-27 04:22:09,942 Stage: Train 0.5 | Epoch: 144 | Iter: 219600 | Total Loss: 0.005862 | Recon Loss: 0.004807 | Commit Loss: 0.002110 | Perplexity: 1354.763960
2025-09-27 04:22:55,527 Stage: Train 0.5 | Epoch: 144 | Iter: 219800 | Total Loss: 0.005807 | Recon Loss: 0.004750 | Commit Loss: 0.002114 | Perplexity: 1354.956620
2025-09-27 04:23:41,229 Stage: Train 0.5 | Epoch: 144 | Iter: 220000 | Total Loss: 0.005866 | Recon Loss: 0.004810 | Commit Loss: 0.002112 | Perplexity: 1357.713972
2025-09-27 04:23:41,230 Saving model at iteration 220000
2025-09-27 04:23:41,521 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000
2025-09-27 04:23:41,800 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/model.safetensors
2025-09-27 04:23:42,188 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/optimizer.bin
2025-09-27 04:23:42,188 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/scheduler.bin
2025-09-27 04:23:42,189 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/sampler.bin
2025-09-27 04:23:42,189 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/random_states_0.pkl
2025-09-27 04:24:28,270 Stage: Train 0.5 | Epoch: 144 | Iter: 220200 | Total Loss: 0.005864 | Recon Loss: 0.004808 | Commit Loss: 0.002112 | Perplexity: 1358.024287
Trainning Epoch:  44%|████▍     | 145/330 [14:13:31<17:54:58, 348.64s/it]2025-09-27 04:25:14,247 Stage: Train 0.5 | Epoch: 145 | Iter: 220400 | Total Loss: 0.005834 | Recon Loss: 0.004784 | Commit Loss: 0.002100 | Perplexity: 1353.527749
2025-09-27 04:25:59,706 Stage: Train 0.5 | Epoch: 145 | Iter: 220600 | Total Loss: 0.005822 | Recon Loss: 0.004766 | Commit Loss: 0.002111 | Perplexity: 1356.295259
2025-09-27 04:26:45,498 Stage: Train 0.5 | Epoch: 145 | Iter: 220800 | Total Loss: 0.005829 | Recon Loss: 0.004780 | Commit Loss: 0.002098 | Perplexity: 1355.634725
2025-09-27 04:27:31,131 Stage: Train 0.5 | Epoch: 145 | Iter: 221000 | Total Loss: 0.005875 | Recon Loss: 0.004817 | Commit Loss: 0.002116 | Perplexity: 1357.957773
2025-09-27 04:28:16,976 Stage: Train 0.5 | Epoch: 145 | Iter: 221200 | Total Loss: 0.005824 | Recon Loss: 0.004774 | Commit Loss: 0.002099 | Perplexity: 1353.586431
2025-09-27 04:29:02,805 Stage: Train 0.5 | Epoch: 145 | Iter: 221400 | Total Loss: 0.005889 | Recon Loss: 0.004834 | Commit Loss: 0.002108 | Perplexity: 1356.460745
2025-09-27 04:29:46,668 Stage: Train 0.5 | Epoch: 145 | Iter: 221600 | Total Loss: 0.005904 | Recon Loss: 0.004845 | Commit Loss: 0.002118 | Perplexity: 1357.721448
Trainning Epoch:  44%|████▍     | 146/330 [14:19:17<17:46:15, 347.69s/it]2025-09-27 04:30:32,535 Stage: Train 0.5 | Epoch: 146 | Iter: 221800 | Total Loss: 0.005805 | Recon Loss: 0.004751 | Commit Loss: 0.002108 | Perplexity: 1357.043651
2025-09-27 04:31:18,318 Stage: Train 0.5 | Epoch: 146 | Iter: 222000 | Total Loss: 0.005820 | Recon Loss: 0.004769 | Commit Loss: 0.002100 | Perplexity: 1354.651387
2025-09-27 04:32:04,048 Stage: Train 0.5 | Epoch: 146 | Iter: 222200 | Total Loss: 0.005760 | Recon Loss: 0.004710 | Commit Loss: 0.002099 | Perplexity: 1356.468022
2025-09-27 04:32:49,353 Stage: Train 0.5 | Epoch: 146 | Iter: 222400 | Total Loss: 0.005840 | Recon Loss: 0.004783 | Commit Loss: 0.002114 | Perplexity: 1354.740626
2025-09-27 04:33:34,848 Stage: Train 0.5 | Epoch: 146 | Iter: 222600 | Total Loss: 0.005811 | Recon Loss: 0.004758 | Commit Loss: 0.002106 | Perplexity: 1354.995837
2025-09-27 04:34:20,709 Stage: Train 0.5 | Epoch: 146 | Iter: 222800 | Total Loss: 0.005822 | Recon Loss: 0.004769 | Commit Loss: 0.002105 | Perplexity: 1354.265983
2025-09-27 04:35:06,232 Stage: Train 0.5 | Epoch: 146 | Iter: 223000 | Total Loss: 0.005854 | Recon Loss: 0.004794 | Commit Loss: 0.002120 | Perplexity: 1360.639868
2025-09-27 04:35:51,651 Stage: Train 0.5 | Epoch: 146 | Iter: 223200 | Total Loss: 0.005793 | Recon Loss: 0.004737 | Commit Loss: 0.002111 | Perplexity: 1358.858273
Trainning Epoch:  45%|████▍     | 147/330 [14:25:03<17:39:28, 347.37s/it]2025-09-27 04:36:37,672 Stage: Train 0.5 | Epoch: 147 | Iter: 223400 | Total Loss: 0.005847 | Recon Loss: 0.004781 | Commit Loss: 0.002131 | Perplexity: 1359.179635
2025-09-27 04:37:23,392 Stage: Train 0.5 | Epoch: 147 | Iter: 223600 | Total Loss: 0.005861 | Recon Loss: 0.004809 | Commit Loss: 0.002103 | Perplexity: 1356.262843
2025-09-27 04:38:09,034 Stage: Train 0.5 | Epoch: 147 | Iter: 223800 | Total Loss: 0.005881 | Recon Loss: 0.004824 | Commit Loss: 0.002114 | Perplexity: 1358.062620
2025-09-27 04:38:54,874 Stage: Train 0.5 | Epoch: 147 | Iter: 224000 | Total Loss: 0.005800 | Recon Loss: 0.004750 | Commit Loss: 0.002101 | Perplexity: 1355.448476
2025-09-27 04:39:40,264 Stage: Train 0.5 | Epoch: 147 | Iter: 224200 | Total Loss: 0.005827 | Recon Loss: 0.004775 | Commit Loss: 0.002104 | Perplexity: 1355.543425
2025-09-27 04:40:25,905 Stage: Train 0.5 | Epoch: 147 | Iter: 224400 | Total Loss: 0.005819 | Recon Loss: 0.004769 | Commit Loss: 0.002100 | Perplexity: 1356.663319
2025-09-27 04:41:11,601 Stage: Train 0.5 | Epoch: 147 | Iter: 224600 | Total Loss: 0.005835 | Recon Loss: 0.004780 | Commit Loss: 0.002110 | Perplexity: 1356.553924
2025-09-27 04:41:57,373 Stage: Train 0.5 | Epoch: 147 | Iter: 224800 | Total Loss: 0.005871 | Recon Loss: 0.004817 | Commit Loss: 0.002109 | Perplexity: 1355.630799
Trainning Epoch:  45%|████▍     | 148/330 [14:30:50<17:33:28, 347.30s/it]2025-09-27 04:42:42,953 Stage: Train 0.5 | Epoch: 148 | Iter: 225000 | Total Loss: 0.005839 | Recon Loss: 0.004784 | Commit Loss: 0.002110 | Perplexity: 1356.258773
2025-09-27 04:43:28,582 Stage: Train 0.5 | Epoch: 148 | Iter: 225200 | Total Loss: 0.005795 | Recon Loss: 0.004746 | Commit Loss: 0.002099 | Perplexity: 1356.024906
2025-09-27 04:44:14,521 Stage: Train 0.5 | Epoch: 148 | Iter: 225400 | Total Loss: 0.005771 | Recon Loss: 0.004711 | Commit Loss: 0.002118 | Perplexity: 1357.837368
2025-09-27 04:45:00,220 Stage: Train 0.5 | Epoch: 148 | Iter: 225600 | Total Loss: 0.005846 | Recon Loss: 0.004790 | Commit Loss: 0.002112 | Perplexity: 1358.122847
2025-09-27 04:45:46,001 Stage: Train 0.5 | Epoch: 148 | Iter: 225800 | Total Loss: 0.005786 | Recon Loss: 0.004735 | Commit Loss: 0.002102 | Perplexity: 1354.212997
2025-09-27 04:46:31,533 Stage: Train 0.5 | Epoch: 148 | Iter: 226000 | Total Loss: 0.005919 | Recon Loss: 0.004867 | Commit Loss: 0.002104 | Perplexity: 1356.570317
2025-09-27 04:47:17,291 Stage: Train 0.5 | Epoch: 148 | Iter: 226200 | Total Loss: 0.005848 | Recon Loss: 0.004792 | Commit Loss: 0.002111 | Perplexity: 1355.934162
Trainning Epoch:  45%|████▌     | 149/330 [14:36:38<17:27:36, 347.27s/it]2025-09-27 04:48:03,366 Stage: Train 0.5 | Epoch: 149 | Iter: 226400 | Total Loss: 0.005754 | Recon Loss: 0.004704 | Commit Loss: 0.002099 | Perplexity: 1356.769715
2025-09-27 04:48:49,090 Stage: Train 0.5 | Epoch: 149 | Iter: 226600 | Total Loss: 0.005781 | Recon Loss: 0.004730 | Commit Loss: 0.002103 | Perplexity: 1358.009291
2025-09-27 04:49:34,610 Stage: Train 0.5 | Epoch: 149 | Iter: 226800 | Total Loss: 0.005838 | Recon Loss: 0.004786 | Commit Loss: 0.002103 | Perplexity: 1354.428658
2025-09-27 04:50:20,232 Stage: Train 0.5 | Epoch: 149 | Iter: 227000 | Total Loss: 0.005842 | Recon Loss: 0.004789 | Commit Loss: 0.002106 | Perplexity: 1357.402281
2025-09-27 04:51:06,220 Stage: Train 0.5 | Epoch: 149 | Iter: 227200 | Total Loss: 0.005794 | Recon Loss: 0.004740 | Commit Loss: 0.002108 | Perplexity: 1361.072067
2025-09-27 04:51:51,931 Stage: Train 0.5 | Epoch: 149 | Iter: 227400 | Total Loss: 0.005877 | Recon Loss: 0.004820 | Commit Loss: 0.002114 | Perplexity: 1359.555617
2025-09-27 04:52:37,543 Stage: Train 0.5 | Epoch: 149 | Iter: 227600 | Total Loss: 0.005835 | Recon Loss: 0.004779 | Commit Loss: 0.002112 | Perplexity: 1356.972404
2025-09-27 04:53:22,950 Stage: Train 0.5 | Epoch: 149 | Iter: 227800 | Total Loss: 0.005814 | Recon Loss: 0.004764 | Commit Loss: 0.002100 | Perplexity: 1354.897369
Trainning Epoch:  45%|████▌     | 150/330 [14:42:25<17:21:37, 347.21s/it]2025-09-27 04:54:08,846 Stage: Train 0.5 | Epoch: 150 | Iter: 228000 | Total Loss: 0.005764 | Recon Loss: 0.004714 | Commit Loss: 0.002100 | Perplexity: 1357.085253
2025-09-27 04:54:54,576 Stage: Train 0.5 | Epoch: 150 | Iter: 228200 | Total Loss: 0.005838 | Recon Loss: 0.004789 | Commit Loss: 0.002099 | Perplexity: 1356.953275
2025-09-27 04:55:40,534 Stage: Train 0.5 | Epoch: 150 | Iter: 228400 | Total Loss: 0.005830 | Recon Loss: 0.004777 | Commit Loss: 0.002106 | Perplexity: 1357.195125
2025-09-27 04:56:26,202 Stage: Train 0.5 | Epoch: 150 | Iter: 228600 | Total Loss: 0.005782 | Recon Loss: 0.004733 | Commit Loss: 0.002098 | Perplexity: 1357.954578
2025-09-27 04:57:11,929 Stage: Train 0.5 | Epoch: 150 | Iter: 228800 | Total Loss: 0.005845 | Recon Loss: 0.004787 | Commit Loss: 0.002115 | Perplexity: 1359.784182
2025-09-27 04:57:57,516 Stage: Train 0.5 | Epoch: 150 | Iter: 229000 | Total Loss: 0.005811 | Recon Loss: 0.004756 | Commit Loss: 0.002110 | Perplexity: 1356.914687
2025-09-27 04:58:43,109 Stage: Train 0.5 | Epoch: 150 | Iter: 229200 | Total Loss: 0.005809 | Recon Loss: 0.004755 | Commit Loss: 0.002109 | Perplexity: 1357.955351
Trainning Epoch:  46%|████▌     | 151/330 [14:48:12<17:15:41, 347.16s/it]2025-09-27 04:59:28,770 Stage: Train 0.5 | Epoch: 151 | Iter: 229400 | Total Loss: 0.005852 | Recon Loss: 0.004791 | Commit Loss: 0.002121 | Perplexity: 1361.216945
2025-09-27 05:00:14,582 Stage: Train 0.5 | Epoch: 151 | Iter: 229600 | Total Loss: 0.005809 | Recon Loss: 0.004760 | Commit Loss: 0.002098 | Perplexity: 1357.097493
2025-09-27 05:01:00,219 Stage: Train 0.5 | Epoch: 151 | Iter: 229800 | Total Loss: 0.005804 | Recon Loss: 0.004755 | Commit Loss: 0.002098 | Perplexity: 1357.253077
2025-09-27 05:01:45,761 Stage: Train 0.5 | Epoch: 151 | Iter: 230000 | Total Loss: 0.005739 | Recon Loss: 0.004691 | Commit Loss: 0.002096 | Perplexity: 1355.929438
2025-09-27 05:02:31,741 Stage: Train 0.5 | Epoch: 151 | Iter: 230200 | Total Loss: 0.005847 | Recon Loss: 0.004794 | Commit Loss: 0.002107 | Perplexity: 1358.478741
2025-09-27 05:03:17,360 Stage: Train 0.5 | Epoch: 151 | Iter: 230400 | Total Loss: 0.005751 | Recon Loss: 0.004697 | Commit Loss: 0.002109 | Perplexity: 1359.974850
2025-09-27 05:04:02,818 Stage: Train 0.5 | Epoch: 151 | Iter: 230600 | Total Loss: 0.005814 | Recon Loss: 0.004760 | Commit Loss: 0.002107 | Perplexity: 1359.378884
2025-09-27 05:04:48,515 Stage: Train 0.5 | Epoch: 151 | Iter: 230800 | Total Loss: 0.005790 | Recon Loss: 0.004735 | Commit Loss: 0.002109 | Perplexity: 1357.269257
Trainning Epoch:  46%|████▌     | 152/330 [14:53:59<17:09:59, 347.19s/it]2025-09-27 05:05:34,475 Stage: Train 0.5 | Epoch: 152 | Iter: 231000 | Total Loss: 0.005806 | Recon Loss: 0.004751 | Commit Loss: 0.002110 | Perplexity: 1359.046813
2025-09-27 05:06:19,851 Stage: Train 0.5 | Epoch: 152 | Iter: 231200 | Total Loss: 0.005805 | Recon Loss: 0.004749 | Commit Loss: 0.002112 | Perplexity: 1359.899932
2025-09-27 05:07:05,628 Stage: Train 0.5 | Epoch: 152 | Iter: 231400 | Total Loss: 0.005811 | Recon Loss: 0.004766 | Commit Loss: 0.002090 | Perplexity: 1354.705403
2025-09-27 05:07:51,402 Stage: Train 0.5 | Epoch: 152 | Iter: 231600 | Total Loss: 0.005763 | Recon Loss: 0.004714 | Commit Loss: 0.002098 | Perplexity: 1360.949897
2025-09-27 05:08:37,262 Stage: Train 0.5 | Epoch: 152 | Iter: 231800 | Total Loss: 0.005775 | Recon Loss: 0.004722 | Commit Loss: 0.002105 | Perplexity: 1356.185609
2025-09-27 05:09:23,160 Stage: Train 0.5 | Epoch: 152 | Iter: 232000 | Total Loss: 0.005820 | Recon Loss: 0.004767 | Commit Loss: 0.002105 | Perplexity: 1357.125835
2025-09-27 05:10:08,707 Stage: Train 0.5 | Epoch: 152 | Iter: 232200 | Total Loss: 0.005805 | Recon Loss: 0.004751 | Commit Loss: 0.002109 | Perplexity: 1359.854501
2025-09-27 05:10:54,541 Stage: Train 0.5 | Epoch: 152 | Iter: 232400 | Total Loss: 0.005756 | Recon Loss: 0.004703 | Commit Loss: 0.002105 | Perplexity: 1356.866777
Trainning Epoch:  46%|████▋     | 153/330 [14:59:46<17:04:26, 347.27s/it]2025-09-27 05:11:40,389 Stage: Train 0.5 | Epoch: 153 | Iter: 232600 | Total Loss: 0.005791 | Recon Loss: 0.004736 | Commit Loss: 0.002111 | Perplexity: 1358.847247
2025-09-27 05:12:26,025 Stage: Train 0.5 | Epoch: 153 | Iter: 232800 | Total Loss: 0.005742 | Recon Loss: 0.004693 | Commit Loss: 0.002097 | Perplexity: 1359.369963
2025-09-27 05:13:11,754 Stage: Train 0.5 | Epoch: 153 | Iter: 233000 | Total Loss: 0.005788 | Recon Loss: 0.004737 | Commit Loss: 0.002102 | Perplexity: 1357.984825
2025-09-27 05:13:56,393 Stage: Train 0.5 | Epoch: 153 | Iter: 233200 | Total Loss: 0.005758 | Recon Loss: 0.004709 | Commit Loss: 0.002099 | Perplexity: 1358.148405
2025-09-27 05:14:42,011 Stage: Train 0.5 | Epoch: 153 | Iter: 233400 | Total Loss: 0.005792 | Recon Loss: 0.004739 | Commit Loss: 0.002106 | Perplexity: 1360.599659
2025-09-27 05:15:27,714 Stage: Train 0.5 | Epoch: 153 | Iter: 233600 | Total Loss: 0.005813 | Recon Loss: 0.004759 | Commit Loss: 0.002109 | Perplexity: 1358.346702
2025-09-27 05:16:13,311 Stage: Train 0.5 | Epoch: 153 | Iter: 233800 | Total Loss: 0.005745 | Recon Loss: 0.004692 | Commit Loss: 0.002106 | Perplexity: 1359.568579
Trainning Epoch:  47%|████▋     | 154/330 [15:05:33<16:57:47, 346.98s/it]2025-09-27 05:16:59,522 Stage: Train 0.5 | Epoch: 154 | Iter: 234000 | Total Loss: 0.005832 | Recon Loss: 0.004778 | Commit Loss: 0.002109 | Perplexity: 1359.503729
2025-09-27 05:17:45,234 Stage: Train 0.5 | Epoch: 154 | Iter: 234200 | Total Loss: 0.005754 | Recon Loss: 0.004704 | Commit Loss: 0.002100 | Perplexity: 1358.579613
2025-09-27 05:18:30,941 Stage: Train 0.5 | Epoch: 154 | Iter: 234400 | Total Loss: 0.005807 | Recon Loss: 0.004756 | Commit Loss: 0.002101 | Perplexity: 1359.544239
2025-09-27 05:19:16,638 Stage: Train 0.5 | Epoch: 154 | Iter: 234600 | Total Loss: 0.005782 | Recon Loss: 0.004724 | Commit Loss: 0.002115 | Perplexity: 1362.338179
2025-09-27 05:20:02,035 Stage: Train 0.5 | Epoch: 154 | Iter: 234800 | Total Loss: 0.005881 | Recon Loss: 0.004824 | Commit Loss: 0.002113 | Perplexity: 1360.747206
2025-09-27 05:20:47,919 Stage: Train 0.5 | Epoch: 154 | Iter: 235000 | Total Loss: 0.005710 | Recon Loss: 0.004663 | Commit Loss: 0.002094 | Perplexity: 1358.305378
2025-09-27 05:21:33,743 Stage: Train 0.5 | Epoch: 154 | Iter: 235200 | Total Loss: 0.005736 | Recon Loss: 0.004685 | Commit Loss: 0.002103 | Perplexity: 1356.169372
2025-09-27 05:22:19,500 Stage: Train 0.5 | Epoch: 154 | Iter: 235400 | Total Loss: 0.005793 | Recon Loss: 0.004738 | Commit Loss: 0.002111 | Perplexity: 1359.767719
Trainning Epoch:  47%|████▋     | 155/330 [15:11:20<16:52:20, 347.09s/it]2025-09-27 05:23:05,093 Stage: Train 0.5 | Epoch: 155 | Iter: 235600 | Total Loss: 0.005803 | Recon Loss: 0.004748 | Commit Loss: 0.002110 | Perplexity: 1359.596403
2025-09-27 05:23:50,874 Stage: Train 0.5 | Epoch: 155 | Iter: 235800 | Total Loss: 0.005727 | Recon Loss: 0.004675 | Commit Loss: 0.002102 | Perplexity: 1358.627852
2025-09-27 05:24:36,602 Stage: Train 0.5 | Epoch: 155 | Iter: 236000 | Total Loss: 0.005795 | Recon Loss: 0.004737 | Commit Loss: 0.002117 | Perplexity: 1362.840951
2025-09-27 05:25:22,312 Stage: Train 0.5 | Epoch: 155 | Iter: 236200 | Total Loss: 0.005768 | Recon Loss: 0.004715 | Commit Loss: 0.002107 | Perplexity: 1359.264510
2025-09-27 05:26:08,099 Stage: Train 0.5 | Epoch: 155 | Iter: 236400 | Total Loss: 0.005756 | Recon Loss: 0.004704 | Commit Loss: 0.002105 | Perplexity: 1359.473631
2025-09-27 05:26:53,587 Stage: Train 0.5 | Epoch: 155 | Iter: 236600 | Total Loss: 0.005814 | Recon Loss: 0.004762 | Commit Loss: 0.002104 | Perplexity: 1357.015883
2025-09-27 05:27:39,513 Stage: Train 0.5 | Epoch: 155 | Iter: 236800 | Total Loss: 0.005734 | Recon Loss: 0.004687 | Commit Loss: 0.002094 | Perplexity: 1356.443765
Trainning Epoch:  47%|████▋     | 156/330 [15:17:07<16:46:51, 347.19s/it]2025-09-27 05:28:25,685 Stage: Train 0.5 | Epoch: 156 | Iter: 237000 | Total Loss: 0.005766 | Recon Loss: 0.004714 | Commit Loss: 0.002104 | Perplexity: 1360.487614
2025-09-27 05:29:11,542 Stage: Train 0.5 | Epoch: 156 | Iter: 237200 | Total Loss: 0.005760 | Recon Loss: 0.004711 | Commit Loss: 0.002098 | Perplexity: 1359.381528
2025-09-27 05:29:57,051 Stage: Train 0.5 | Epoch: 156 | Iter: 237400 | Total Loss: 0.005693 | Recon Loss: 0.004642 | Commit Loss: 0.002101 | Perplexity: 1358.972946
2025-09-27 05:30:42,712 Stage: Train 0.5 | Epoch: 156 | Iter: 237600 | Total Loss: 0.005745 | Recon Loss: 0.004696 | Commit Loss: 0.002099 | Perplexity: 1358.365955
2025-09-27 05:31:28,340 Stage: Train 0.5 | Epoch: 156 | Iter: 237800 | Total Loss: 0.005820 | Recon Loss: 0.004767 | Commit Loss: 0.002107 | Perplexity: 1358.674963
2025-09-27 05:32:14,111 Stage: Train 0.5 | Epoch: 156 | Iter: 238000 | Total Loss: 0.005822 | Recon Loss: 0.004766 | Commit Loss: 0.002113 | Perplexity: 1361.022119
2025-09-27 05:32:59,579 Stage: Train 0.5 | Epoch: 156 | Iter: 238200 | Total Loss: 0.005804 | Recon Loss: 0.004751 | Commit Loss: 0.002105 | Perplexity: 1361.488733
2025-09-27 05:33:45,315 Stage: Train 0.5 | Epoch: 156 | Iter: 238400 | Total Loss: 0.005742 | Recon Loss: 0.004690 | Commit Loss: 0.002105 | Perplexity: 1360.114117
Trainning Epoch:  48%|████▊     | 157/330 [15:22:55<16:41:05, 347.20s/it]2025-09-27 05:34:31,355 Stage: Train 0.5 | Epoch: 157 | Iter: 238600 | Total Loss: 0.005803 | Recon Loss: 0.004755 | Commit Loss: 0.002097 | Perplexity: 1359.371479
2025-09-27 05:35:16,934 Stage: Train 0.5 | Epoch: 157 | Iter: 238800 | Total Loss: 0.005767 | Recon Loss: 0.004718 | Commit Loss: 0.002098 | Perplexity: 1358.250170
2025-09-27 05:36:02,710 Stage: Train 0.5 | Epoch: 157 | Iter: 239000 | Total Loss: 0.005766 | Recon Loss: 0.004716 | Commit Loss: 0.002100 | Perplexity: 1360.372789
2025-09-27 05:36:48,238 Stage: Train 0.5 | Epoch: 157 | Iter: 239200 | Total Loss: 0.005708 | Recon Loss: 0.004658 | Commit Loss: 0.002101 | Perplexity: 1361.031970
2025-09-27 05:37:33,959 Stage: Train 0.5 | Epoch: 157 | Iter: 239400 | Total Loss: 0.005693 | Recon Loss: 0.004642 | Commit Loss: 0.002103 | Perplexity: 1359.918157
2025-09-27 05:38:19,742 Stage: Train 0.5 | Epoch: 157 | Iter: 239600 | Total Loss: 0.005831 | Recon Loss: 0.004776 | Commit Loss: 0.002110 | Perplexity: 1360.971909
2025-09-27 05:39:05,483 Stage: Train 0.5 | Epoch: 157 | Iter: 239800 | Total Loss: 0.005750 | Recon Loss: 0.004696 | Commit Loss: 0.002108 | Perplexity: 1358.339329
2025-09-27 05:39:50,850 Stage: Train 0.5 | Epoch: 157 | Iter: 240000 | Total Loss: 0.005807 | Recon Loss: 0.004755 | Commit Loss: 0.002104 | Perplexity: 1359.403063
2025-09-27 05:39:50,850 Saving model at iteration 240000
2025-09-27 05:39:51,068 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000
2025-09-27 05:39:51,374 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/model.safetensors
2025-09-27 05:39:51,799 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/optimizer.bin
2025-09-27 05:39:51,800 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/scheduler.bin
2025-09-27 05:39:51,800 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/sampler.bin
2025-09-27 05:39:51,801 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 158/330 [15:28:43<16:36:05, 347.47s/it]2025-09-27 05:40:38,131 Stage: Train 0.5 | Epoch: 158 | Iter: 240200 | Total Loss: 0.005765 | Recon Loss: 0.004712 | Commit Loss: 0.002105 | Perplexity: 1361.122412
2025-09-27 05:41:23,810 Stage: Train 0.5 | Epoch: 158 | Iter: 240400 | Total Loss: 0.005704 | Recon Loss: 0.004652 | Commit Loss: 0.002104 | Perplexity: 1361.597317
2025-09-27 05:42:09,448 Stage: Train 0.5 | Epoch: 158 | Iter: 240600 | Total Loss: 0.005706 | Recon Loss: 0.004655 | Commit Loss: 0.002103 | Perplexity: 1361.764901
2025-09-27 05:42:55,273 Stage: Train 0.5 | Epoch: 158 | Iter: 240800 | Total Loss: 0.005722 | Recon Loss: 0.004673 | Commit Loss: 0.002098 | Perplexity: 1358.916171
2025-09-27 05:43:40,805 Stage: Train 0.5 | Epoch: 158 | Iter: 241000 | Total Loss: 0.005720 | Recon Loss: 0.004670 | Commit Loss: 0.002101 | Perplexity: 1356.485128
2025-09-27 05:44:26,512 Stage: Train 0.5 | Epoch: 158 | Iter: 241200 | Total Loss: 0.005744 | Recon Loss: 0.004698 | Commit Loss: 0.002092 | Perplexity: 1357.457351
2025-09-27 05:45:12,414 Stage: Train 0.5 | Epoch: 158 | Iter: 241400 | Total Loss: 0.005726 | Recon Loss: 0.004678 | Commit Loss: 0.002096 | Perplexity: 1360.093373
Trainning Epoch:  48%|████▊     | 159/330 [15:34:30<16:30:13, 347.45s/it]2025-09-27 05:45:58,178 Stage: Train 0.5 | Epoch: 159 | Iter: 241600 | Total Loss: 0.005776 | Recon Loss: 0.004723 | Commit Loss: 0.002105 | Perplexity: 1359.102458
2025-09-27 05:46:43,841 Stage: Train 0.5 | Epoch: 159 | Iter: 241800 | Total Loss: 0.005724 | Recon Loss: 0.004671 | Commit Loss: 0.002106 | Perplexity: 1358.880143
2025-09-27 05:47:29,493 Stage: Train 0.5 | Epoch: 159 | Iter: 242000 | Total Loss: 0.005786 | Recon Loss: 0.004734 | Commit Loss: 0.002104 | Perplexity: 1357.973750
2025-09-27 05:48:15,173 Stage: Train 0.5 | Epoch: 159 | Iter: 242200 | Total Loss: 0.005717 | Recon Loss: 0.004673 | Commit Loss: 0.002088 | Perplexity: 1359.473815
2025-09-27 05:49:01,128 Stage: Train 0.5 | Epoch: 159 | Iter: 242400 | Total Loss: 0.005785 | Recon Loss: 0.004738 | Commit Loss: 0.002094 | Perplexity: 1359.368350
2025-09-27 05:49:46,821 Stage: Train 0.5 | Epoch: 159 | Iter: 242600 | Total Loss: 0.005703 | Recon Loss: 0.004653 | Commit Loss: 0.002100 | Perplexity: 1358.749879
2025-09-27 05:50:32,861 Stage: Train 0.5 | Epoch: 159 | Iter: 242800 | Total Loss: 0.005768 | Recon Loss: 0.004714 | Commit Loss: 0.002106 | Perplexity: 1363.840154
2025-09-27 05:51:18,829 Stage: Train 0.5 | Epoch: 159 | Iter: 243000 | Total Loss: 0.005767 | Recon Loss: 0.004715 | Commit Loss: 0.002105 | Perplexity: 1360.855708
Trainning Epoch:  48%|████▊     | 160/330 [15:40:18<16:25:01, 347.66s/it]2025-09-27 05:52:05,071 Stage: Train 0.5 | Epoch: 160 | Iter: 243200 | Total Loss: 0.005739 | Recon Loss: 0.004693 | Commit Loss: 0.002092 | Perplexity: 1358.029694
2025-09-27 05:52:51,004 Stage: Train 0.5 | Epoch: 160 | Iter: 243400 | Total Loss: 0.005714 | Recon Loss: 0.004667 | Commit Loss: 0.002093 | Perplexity: 1359.719327
2025-09-27 05:53:36,760 Stage: Train 0.5 | Epoch: 160 | Iter: 243600 | Total Loss: 0.005686 | Recon Loss: 0.004637 | Commit Loss: 0.002098 | Perplexity: 1360.601556
2025-09-27 05:54:22,527 Stage: Train 0.5 | Epoch: 160 | Iter: 243800 | Total Loss: 0.005779 | Recon Loss: 0.004728 | Commit Loss: 0.002101 | Perplexity: 1360.522585
2025-09-27 05:55:08,370 Stage: Train 0.5 | Epoch: 160 | Iter: 244000 | Total Loss: 0.005771 | Recon Loss: 0.004719 | Commit Loss: 0.002104 | Perplexity: 1360.259281
2025-09-27 05:55:53,908 Stage: Train 0.5 | Epoch: 160 | Iter: 244200 | Total Loss: 0.005749 | Recon Loss: 0.004695 | Commit Loss: 0.002108 | Perplexity: 1361.540839
2025-09-27 05:56:39,298 Stage: Train 0.5 | Epoch: 160 | Iter: 244400 | Total Loss: 0.005751 | Recon Loss: 0.004702 | Commit Loss: 0.002099 | Perplexity: 1360.865917
Trainning Epoch:  49%|████▉     | 161/330 [15:46:06<16:19:04, 347.60s/it]2025-09-27 05:57:25,049 Stage: Train 0.5 | Epoch: 161 | Iter: 244600 | Total Loss: 0.005787 | Recon Loss: 0.004735 | Commit Loss: 0.002105 | Perplexity: 1360.843779
2025-09-27 05:58:09,203 Stage: Train 0.5 | Epoch: 161 | Iter: 244800 | Total Loss: 0.005704 | Recon Loss: 0.004656 | Commit Loss: 0.002095 | Perplexity: 1361.882176
2025-09-27 05:58:54,875 Stage: Train 0.5 | Epoch: 161 | Iter: 245000 | Total Loss: 0.005760 | Recon Loss: 0.004719 | Commit Loss: 0.002083 | Perplexity: 1357.735803
2025-09-27 05:59:40,620 Stage: Train 0.5 | Epoch: 161 | Iter: 245200 | Total Loss: 0.005697 | Recon Loss: 0.004646 | Commit Loss: 0.002102 | Perplexity: 1359.032390
2025-09-27 06:00:26,193 Stage: Train 0.5 | Epoch: 161 | Iter: 245400 | Total Loss: 0.005725 | Recon Loss: 0.004676 | Commit Loss: 0.002098 | Perplexity: 1360.945068
2025-09-27 06:01:12,000 Stage: Train 0.5 | Epoch: 161 | Iter: 245600 | Total Loss: 0.005697 | Recon Loss: 0.004648 | Commit Loss: 0.002098 | Perplexity: 1360.491700
2025-09-27 06:01:57,776 Stage: Train 0.5 | Epoch: 161 | Iter: 245800 | Total Loss: 0.005744 | Recon Loss: 0.004693 | Commit Loss: 0.002102 | Perplexity: 1359.711353
2025-09-27 06:02:43,555 Stage: Train 0.5 | Epoch: 161 | Iter: 246000 | Total Loss: 0.005740 | Recon Loss: 0.004691 | Commit Loss: 0.002098 | Perplexity: 1361.854925
Trainning Epoch:  49%|████▉     | 162/330 [15:51:52<16:11:52, 347.10s/it]2025-09-27 06:03:29,345 Stage: Train 0.5 | Epoch: 162 | Iter: 246200 | Total Loss: 0.005772 | Recon Loss: 0.004720 | Commit Loss: 0.002105 | Perplexity: 1360.648162
2025-09-27 06:04:15,009 Stage: Train 0.5 | Epoch: 162 | Iter: 246400 | Total Loss: 0.005753 | Recon Loss: 0.004697 | Commit Loss: 0.002112 | Perplexity: 1362.928656
2025-09-27 06:05:00,663 Stage: Train 0.5 | Epoch: 162 | Iter: 246600 | Total Loss: 0.005698 | Recon Loss: 0.004652 | Commit Loss: 0.002093 | Perplexity: 1356.375475
2025-09-27 06:05:46,298 Stage: Train 0.5 | Epoch: 162 | Iter: 246800 | Total Loss: 0.005651 | Recon Loss: 0.004606 | Commit Loss: 0.002089 | Perplexity: 1361.596292
2025-09-27 06:06:31,773 Stage: Train 0.5 | Epoch: 162 | Iter: 247000 | Total Loss: 0.005692 | Recon Loss: 0.004640 | Commit Loss: 0.002103 | Perplexity: 1360.652737
2025-09-27 06:07:17,509 Stage: Train 0.5 | Epoch: 162 | Iter: 247200 | Total Loss: 0.005772 | Recon Loss: 0.004719 | Commit Loss: 0.002106 | Perplexity: 1360.074777
2025-09-27 06:08:03,289 Stage: Train 0.5 | Epoch: 162 | Iter: 247400 | Total Loss: 0.005684 | Recon Loss: 0.004635 | Commit Loss: 0.002098 | Perplexity: 1361.656837
Trainning Epoch:  49%|████▉     | 163/330 [15:57:59<16:23:10, 353.24s/it]2025-09-27 06:09:10,247 Stage: Train 0.5 | Epoch: 163 | Iter: 247600 | Total Loss: 0.005800 | Recon Loss: 0.004746 | Commit Loss: 0.002109 | Perplexity: 1360.343541
2025-09-27 06:10:07,682 Stage: Train 0.5 | Epoch: 163 | Iter: 247800 | Total Loss: 0.005621 | Recon Loss: 0.004578 | Commit Loss: 0.002085 | Perplexity: 1356.667418
2025-09-27 06:10:53,166 Stage: Train 0.5 | Epoch: 163 | Iter: 248000 | Total Loss: 0.005795 | Recon Loss: 0.004746 | Commit Loss: 0.002098 | Perplexity: 1360.175327
2025-09-27 06:11:38,972 Stage: Train 0.5 | Epoch: 163 | Iter: 248200 | Total Loss: 0.005711 | Recon Loss: 0.004659 | Commit Loss: 0.002103 | Perplexity: 1360.995792
2025-09-27 06:12:24,670 Stage: Train 0.5 | Epoch: 163 | Iter: 248400 | Total Loss: 0.005733 | Recon Loss: 0.004682 | Commit Loss: 0.002100 | Perplexity: 1362.041431
2025-09-27 06:13:10,391 Stage: Train 0.5 | Epoch: 163 | Iter: 248600 | Total Loss: 0.005725 | Recon Loss: 0.004673 | Commit Loss: 0.002103 | Perplexity: 1360.566055
2025-09-27 06:13:55,848 Stage: Train 0.5 | Epoch: 163 | Iter: 248800 | Total Loss: 0.005705 | Recon Loss: 0.004656 | Commit Loss: 0.002098 | Perplexity: 1360.328723
2025-09-27 06:14:41,696 Stage: Train 0.5 | Epoch: 163 | Iter: 249000 | Total Loss: 0.005712 | Recon Loss: 0.004661 | Commit Loss: 0.002102 | Perplexity: 1361.800522
Trainning Epoch:  50%|████▉     | 164/330 [16:03:58<16:22:12, 355.01s/it]2025-09-27 06:15:27,620 Stage: Train 0.5 | Epoch: 164 | Iter: 249200 | Total Loss: 0.005709 | Recon Loss: 0.004660 | Commit Loss: 0.002098 | Perplexity: 1360.446215
2025-09-27 06:16:13,403 Stage: Train 0.5 | Epoch: 164 | Iter: 249400 | Total Loss: 0.005666 | Recon Loss: 0.004618 | Commit Loss: 0.002096 | Perplexity: 1362.419595
2025-09-27 06:16:59,107 Stage: Train 0.5 | Epoch: 164 | Iter: 249600 | Total Loss: 0.005713 | Recon Loss: 0.004663 | Commit Loss: 0.002100 | Perplexity: 1363.431596
2025-09-27 06:17:44,710 Stage: Train 0.5 | Epoch: 164 | Iter: 249800 | Total Loss: 0.005713 | Recon Loss: 0.004668 | Commit Loss: 0.002091 | Perplexity: 1361.921735
2025-09-27 06:18:30,379 Stage: Train 0.5 | Epoch: 164 | Iter: 250000 | Total Loss: 0.005673 | Recon Loss: 0.004622 | Commit Loss: 0.002101 | Perplexity: 1360.526688
2025-09-27 06:19:15,820 Stage: Train 0.5 | Epoch: 164 | Iter: 250200 | Total Loss: 0.005764 | Recon Loss: 0.004709 | Commit Loss: 0.002109 | Perplexity: 1363.289864
2025-09-27 06:20:01,444 Stage: Train 0.5 | Epoch: 164 | Iter: 250400 | Total Loss: 0.005683 | Recon Loss: 0.004637 | Commit Loss: 0.002092 | Perplexity: 1360.127296
2025-09-27 06:20:46,978 Stage: Train 0.5 | Epoch: 164 | Iter: 250600 | Total Loss: 0.005675 | Recon Loss: 0.004627 | Commit Loss: 0.002096 | Perplexity: 1360.698645
Trainning Epoch:  50%|█████     | 165/330 [16:09:45<16:09:31, 352.56s/it]2025-09-27 06:21:32,947 Stage: Train 0.5 | Epoch: 165 | Iter: 250800 | Total Loss: 0.005726 | Recon Loss: 0.004674 | Commit Loss: 0.002105 | Perplexity: 1365.114315
2025-09-27 06:22:18,776 Stage: Train 0.5 | Epoch: 165 | Iter: 251000 | Total Loss: 0.005701 | Recon Loss: 0.004653 | Commit Loss: 0.002094 | Perplexity: 1361.516660
2025-09-27 06:23:04,651 Stage: Train 0.5 | Epoch: 165 | Iter: 251200 | Total Loss: 0.005659 | Recon Loss: 0.004612 | Commit Loss: 0.002095 | Perplexity: 1361.862706
2025-09-27 06:23:50,502 Stage: Train 0.5 | Epoch: 165 | Iter: 251400 | Total Loss: 0.005731 | Recon Loss: 0.004686 | Commit Loss: 0.002090 | Perplexity: 1361.293661
2025-09-27 06:24:36,042 Stage: Train 0.5 | Epoch: 165 | Iter: 251600 | Total Loss: 0.005675 | Recon Loss: 0.004629 | Commit Loss: 0.002092 | Perplexity: 1360.655919
2025-09-27 06:25:21,713 Stage: Train 0.5 | Epoch: 165 | Iter: 251800 | Total Loss: 0.005707 | Recon Loss: 0.004658 | Commit Loss: 0.002097 | Perplexity: 1361.139057
2025-09-27 06:26:07,466 Stage: Train 0.5 | Epoch: 165 | Iter: 252000 | Total Loss: 0.005738 | Recon Loss: 0.004691 | Commit Loss: 0.002095 | Perplexity: 1361.238804
Trainning Epoch:  50%|█████     | 166/330 [16:15:33<15:59:40, 351.10s/it]2025-09-27 06:26:53,471 Stage: Train 0.5 | Epoch: 166 | Iter: 252200 | Total Loss: 0.005699 | Recon Loss: 0.004653 | Commit Loss: 0.002091 | Perplexity: 1358.688166
2025-09-27 06:27:20,583 Stage: Train 0.5 | Epoch: 166 | Iter: 252400 | Total Loss: 0.005612 | Recon Loss: 0.004568 | Commit Loss: 0.002087 | Perplexity: 1363.328795
2025-09-27 06:27:41,425 Stage: Train 0.5 | Epoch: 166 | Iter: 252600 | Total Loss: 0.005711 | Recon Loss: 0.004668 | Commit Loss: 0.002085 | Perplexity: 1360.932042
2025-09-27 06:28:02,262 Stage: Train 0.5 | Epoch: 166 | Iter: 252800 | Total Loss: 0.005692 | Recon Loss: 0.004643 | Commit Loss: 0.002099 | Perplexity: 1363.318702
2025-09-27 06:28:23,054 Stage: Train 0.5 | Epoch: 166 | Iter: 253000 | Total Loss: 0.005704 | Recon Loss: 0.004653 | Commit Loss: 0.002102 | Perplexity: 1363.577618
2025-09-27 06:28:43,848 Stage: Train 0.5 | Epoch: 166 | Iter: 253200 | Total Loss: 0.005664 | Recon Loss: 0.004616 | Commit Loss: 0.002095 | Perplexity: 1362.147330
2025-09-27 06:29:04,901 Stage: Train 0.5 | Epoch: 166 | Iter: 253400 | Total Loss: 0.005728 | Recon Loss: 0.004677 | Commit Loss: 0.002102 | Perplexity: 1361.556392
2025-09-27 06:29:25,908 Stage: Train 0.5 | Epoch: 166 | Iter: 253600 | Total Loss: 0.005745 | Recon Loss: 0.004701 | Commit Loss: 0.002088 | Perplexity: 1360.384968
Trainning Epoch:  51%|█████     | 167/330 [16:18:24<13:26:55, 297.03s/it]2025-09-27 06:29:47,052 Stage: Train 0.5 | Epoch: 167 | Iter: 253800 | Total Loss: 0.005630 | Recon Loss: 0.004589 | Commit Loss: 0.002081 | Perplexity: 1357.824042
2025-09-27 06:30:07,721 Stage: Train 0.5 | Epoch: 167 | Iter: 254000 | Total Loss: 0.005708 | Recon Loss: 0.004665 | Commit Loss: 0.002085 | Perplexity: 1358.835614
2025-09-27 06:30:28,942 Stage: Train 0.5 | Epoch: 167 | Iter: 254200 | Total Loss: 0.005723 | Recon Loss: 0.004677 | Commit Loss: 0.002091 | Perplexity: 1363.479644
2025-09-27 06:30:49,834 Stage: Train 0.5 | Epoch: 167 | Iter: 254400 | Total Loss: 0.005598 | Recon Loss: 0.004551 | Commit Loss: 0.002096 | Perplexity: 1362.792697
2025-09-27 06:31:11,010 Stage: Train 0.5 | Epoch: 167 | Iter: 254600 | Total Loss: 0.005698 | Recon Loss: 0.004653 | Commit Loss: 0.002088 | Perplexity: 1359.918711
2025-09-27 06:31:32,129 Stage: Train 0.5 | Epoch: 167 | Iter: 254800 | Total Loss: 0.005676 | Recon Loss: 0.004625 | Commit Loss: 0.002103 | Perplexity: 1362.102172
2025-09-27 06:31:53,227 Stage: Train 0.5 | Epoch: 167 | Iter: 255000 | Total Loss: 0.005767 | Recon Loss: 0.004720 | Commit Loss: 0.002095 | Perplexity: 1361.132168
Trainning Epoch:  51%|█████     | 168/330 [16:21:04<11:31:05, 255.96s/it]2025-09-27 06:32:14,779 Stage: Train 0.5 | Epoch: 168 | Iter: 255200 | Total Loss: 0.005706 | Recon Loss: 0.004655 | Commit Loss: 0.002102 | Perplexity: 1365.565105
2025-09-27 06:32:35,817 Stage: Train 0.5 | Epoch: 168 | Iter: 255400 | Total Loss: 0.005662 | Recon Loss: 0.004618 | Commit Loss: 0.002088 | Perplexity: 1364.237643
2025-09-27 06:32:56,896 Stage: Train 0.5 | Epoch: 168 | Iter: 255600 | Total Loss: 0.005707 | Recon Loss: 0.004661 | Commit Loss: 0.002092 | Perplexity: 1364.303249
2025-09-27 06:33:18,016 Stage: Train 0.5 | Epoch: 168 | Iter: 255800 | Total Loss: 0.005656 | Recon Loss: 0.004610 | Commit Loss: 0.002092 | Perplexity: 1363.605181
2025-09-27 06:33:39,128 Stage: Train 0.5 | Epoch: 168 | Iter: 256000 | Total Loss: 0.005700 | Recon Loss: 0.004651 | Commit Loss: 0.002097 | Perplexity: 1363.651462
2025-09-27 06:34:00,137 Stage: Train 0.5 | Epoch: 168 | Iter: 256200 | Total Loss: 0.005673 | Recon Loss: 0.004627 | Commit Loss: 0.002090 | Perplexity: 1361.131725
2025-09-27 06:34:21,287 Stage: Train 0.5 | Epoch: 168 | Iter: 256400 | Total Loss: 0.005643 | Recon Loss: 0.004598 | Commit Loss: 0.002091 | Perplexity: 1360.685966
2025-09-27 06:34:42,347 Stage: Train 0.5 | Epoch: 168 | Iter: 256600 | Total Loss: 0.005683 | Recon Loss: 0.004636 | Commit Loss: 0.002093 | Perplexity: 1363.093693
Trainning Epoch:  51%|█████     | 169/330 [16:23:44<10:09:53, 227.29s/it]2025-09-27 06:35:03,663 Stage: Train 0.5 | Epoch: 169 | Iter: 256800 | Total Loss: 0.005672 | Recon Loss: 0.004627 | Commit Loss: 0.002090 | Perplexity: 1359.614006
2025-09-27 06:35:24,744 Stage: Train 0.5 | Epoch: 169 | Iter: 257000 | Total Loss: 0.005668 | Recon Loss: 0.004624 | Commit Loss: 0.002089 | Perplexity: 1363.055325
2025-09-27 06:35:45,698 Stage: Train 0.5 | Epoch: 169 | Iter: 257200 | Total Loss: 0.005692 | Recon Loss: 0.004645 | Commit Loss: 0.002094 | Perplexity: 1363.078684
2025-09-27 06:36:06,606 Stage: Train 0.5 | Epoch: 169 | Iter: 257400 | Total Loss: 0.005632 | Recon Loss: 0.004588 | Commit Loss: 0.002088 | Perplexity: 1364.921210
2025-09-27 06:36:27,782 Stage: Train 0.5 | Epoch: 169 | Iter: 257600 | Total Loss: 0.005662 | Recon Loss: 0.004616 | Commit Loss: 0.002092 | Perplexity: 1361.941213
2025-09-27 06:36:48,744 Stage: Train 0.5 | Epoch: 169 | Iter: 257800 | Total Loss: 0.005648 | Recon Loss: 0.004598 | Commit Loss: 0.002100 | Perplexity: 1361.112444
2025-09-27 06:37:09,756 Stage: Train 0.5 | Epoch: 169 | Iter: 258000 | Total Loss: 0.005643 | Recon Loss: 0.004600 | Commit Loss: 0.002084 | Perplexity: 1362.694811
2025-09-27 06:37:30,762 Stage: Train 0.5 | Epoch: 169 | Iter: 258200 | Total Loss: 0.005704 | Recon Loss: 0.004652 | Commit Loss: 0.002105 | Perplexity: 1365.403908
Trainning Epoch:  52%|█████▏    | 170/330 [16:26:24<9:12:10, 207.07s/it] 2025-09-27 06:37:52,214 Stage: Train 0.5 | Epoch: 170 | Iter: 258400 | Total Loss: 0.005652 | Recon Loss: 0.004608 | Commit Loss: 0.002087 | Perplexity: 1364.296379
2025-09-27 06:38:13,412 Stage: Train 0.5 | Epoch: 170 | Iter: 258600 | Total Loss: 0.005655 | Recon Loss: 0.004613 | Commit Loss: 0.002085 | Perplexity: 1361.701029
2025-09-27 06:38:34,585 Stage: Train 0.5 | Epoch: 170 | Iter: 258800 | Total Loss: 0.005616 | Recon Loss: 0.004569 | Commit Loss: 0.002095 | Perplexity: 1362.096811
2025-09-27 06:38:55,661 Stage: Train 0.5 | Epoch: 170 | Iter: 259000 | Total Loss: 0.005668 | Recon Loss: 0.004619 | Commit Loss: 0.002098 | Perplexity: 1362.829237
2025-09-27 06:39:16,792 Stage: Train 0.5 | Epoch: 170 | Iter: 259200 | Total Loss: 0.005701 | Recon Loss: 0.004653 | Commit Loss: 0.002095 | Perplexity: 1364.558375
2025-09-27 06:39:37,802 Stage: Train 0.5 | Epoch: 170 | Iter: 259400 | Total Loss: 0.005653 | Recon Loss: 0.004608 | Commit Loss: 0.002090 | Perplexity: 1362.984928
2025-09-27 06:39:59,026 Stage: Train 0.5 | Epoch: 170 | Iter: 259600 | Total Loss: 0.005673 | Recon Loss: 0.004620 | Commit Loss: 0.002105 | Perplexity: 1365.773584
Trainning Epoch:  52%|█████▏    | 171/330 [16:29:05<8:32:02, 193.22s/it]2025-09-27 06:40:20,512 Stage: Train 0.5 | Epoch: 171 | Iter: 259800 | Total Loss: 0.005642 | Recon Loss: 0.004596 | Commit Loss: 0.002092 | Perplexity: 1361.509907
2025-09-27 06:40:41,675 Stage: Train 0.5 | Epoch: 171 | Iter: 260000 | Total Loss: 0.005664 | Recon Loss: 0.004616 | Commit Loss: 0.002096 | Perplexity: 1364.702588
2025-09-27 06:40:41,675 Saving model at iteration 260000
2025-09-27 06:40:42,110 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000
2025-09-27 06:40:42,406 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/model.safetensors
2025-09-27 06:40:42,804 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/optimizer.bin
2025-09-27 06:40:42,804 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/scheduler.bin
2025-09-27 06:40:42,804 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/sampler.bin
2025-09-27 06:40:42,805 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/random_states_0.pkl
2025-09-27 06:41:04,028 Stage: Train 0.5 | Epoch: 171 | Iter: 260200 | Total Loss: 0.005699 | Recon Loss: 0.004653 | Commit Loss: 0.002093 | Perplexity: 1363.978898
2025-09-27 06:41:25,126 Stage: Train 0.5 | Epoch: 171 | Iter: 260400 | Total Loss: 0.005679 | Recon Loss: 0.004632 | Commit Loss: 0.002095 | Perplexity: 1363.757213
2025-09-27 06:41:46,176 Stage: Train 0.5 | Epoch: 171 | Iter: 260600 | Total Loss: 0.005664 | Recon Loss: 0.004617 | Commit Loss: 0.002093 | Perplexity: 1363.433249
2025-09-27 06:42:07,261 Stage: Train 0.5 | Epoch: 171 | Iter: 260800 | Total Loss: 0.005620 | Recon Loss: 0.004576 | Commit Loss: 0.002087 | Perplexity: 1361.943354
2025-09-27 06:42:28,260 Stage: Train 0.5 | Epoch: 171 | Iter: 261000 | Total Loss: 0.005692 | Recon Loss: 0.004642 | Commit Loss: 0.002100 | Perplexity: 1365.458758
2025-09-27 06:42:49,164 Stage: Train 0.5 | Epoch: 171 | Iter: 261200 | Total Loss: 0.005617 | Recon Loss: 0.004573 | Commit Loss: 0.002088 | Perplexity: 1363.455220
Trainning Epoch:  52%|█████▏    | 172/330 [16:31:47<8:03:50, 183.73s/it]2025-09-27 06:43:10,675 Stage: Train 0.5 | Epoch: 172 | Iter: 261400 | Total Loss: 0.005678 | Recon Loss: 0.004632 | Commit Loss: 0.002093 | Perplexity: 1364.892648
2025-09-27 06:43:31,823 Stage: Train 0.5 | Epoch: 172 | Iter: 261600 | Total Loss: 0.005649 | Recon Loss: 0.004610 | Commit Loss: 0.002077 | Perplexity: 1359.102336
2025-09-27 06:43:52,759 Stage: Train 0.5 | Epoch: 172 | Iter: 261800 | Total Loss: 0.005658 | Recon Loss: 0.004610 | Commit Loss: 0.002097 | Perplexity: 1362.968102
2025-09-27 06:44:13,539 Stage: Train 0.5 | Epoch: 172 | Iter: 262000 | Total Loss: 0.005694 | Recon Loss: 0.004657 | Commit Loss: 0.002074 | Perplexity: 1361.805952
2025-09-27 06:44:34,301 Stage: Train 0.5 | Epoch: 172 | Iter: 262200 | Total Loss: 0.005653 | Recon Loss: 0.004609 | Commit Loss: 0.002089 | Perplexity: 1365.343805
2025-09-27 06:44:55,205 Stage: Train 0.5 | Epoch: 172 | Iter: 262400 | Total Loss: 0.005665 | Recon Loss: 0.004620 | Commit Loss: 0.002091 | Perplexity: 1362.909996
2025-09-27 06:45:16,055 Stage: Train 0.5 | Epoch: 172 | Iter: 262600 | Total Loss: 0.005687 | Recon Loss: 0.004638 | Commit Loss: 0.002097 | Perplexity: 1364.860065
Trainning Epoch:  52%|█████▏    | 173/330 [16:34:26<7:41:22, 176.32s/it]2025-09-27 06:45:37,080 Stage: Train 0.5 | Epoch: 173 | Iter: 262800 | Total Loss: 0.005634 | Recon Loss: 0.004589 | Commit Loss: 0.002089 | Perplexity: 1364.443953
2025-09-27 06:45:58,029 Stage: Train 0.5 | Epoch: 173 | Iter: 263000 | Total Loss: 0.005643 | Recon Loss: 0.004600 | Commit Loss: 0.002086 | Perplexity: 1363.873986
2025-09-27 06:46:19,234 Stage: Train 0.5 | Epoch: 173 | Iter: 263200 | Total Loss: 0.005669 | Recon Loss: 0.004623 | Commit Loss: 0.002092 | Perplexity: 1363.169166
2025-09-27 06:46:40,140 Stage: Train 0.5 | Epoch: 173 | Iter: 263400 | Total Loss: 0.005593 | Recon Loss: 0.004551 | Commit Loss: 0.002084 | Perplexity: 1365.988482
2025-09-27 06:47:01,073 Stage: Train 0.5 | Epoch: 173 | Iter: 263600 | Total Loss: 0.005667 | Recon Loss: 0.004620 | Commit Loss: 0.002093 | Perplexity: 1364.391050
2025-09-27 06:47:22,091 Stage: Train 0.5 | Epoch: 173 | Iter: 263800 | Total Loss: 0.005688 | Recon Loss: 0.004644 | Commit Loss: 0.002088 | Perplexity: 1364.602037
2025-09-27 06:47:42,863 Stage: Train 0.5 | Epoch: 173 | Iter: 264000 | Total Loss: 0.005627 | Recon Loss: 0.004584 | Commit Loss: 0.002086 | Perplexity: 1363.211592
2025-09-27 06:48:17,699 Stage: Train 0.5 | Epoch: 173 | Iter: 264200 | Total Loss: 0.005666 | Recon Loss: 0.004618 | Commit Loss: 0.002096 | Perplexity: 1363.392139
Trainning Epoch:  53%|█████▎    | 174/330 [16:37:33<7:46:52, 179.57s/it]2025-09-27 06:49:04,972 Stage: Train 0.5 | Epoch: 174 | Iter: 264400 | Total Loss: 0.005618 | Recon Loss: 0.004575 | Commit Loss: 0.002085 | Perplexity: 1362.410143
2025-09-27 06:49:51,995 Stage: Train 0.5 | Epoch: 174 | Iter: 264600 | Total Loss: 0.005667 | Recon Loss: 0.004622 | Commit Loss: 0.002089 | Perplexity: 1365.845389
2025-09-27 06:50:39,137 Stage: Train 0.5 | Epoch: 174 | Iter: 264800 | Total Loss: 0.005603 | Recon Loss: 0.004558 | Commit Loss: 0.002090 | Perplexity: 1362.740225
2025-09-27 06:51:26,100 Stage: Train 0.5 | Epoch: 174 | Iter: 265000 | Total Loss: 0.005643 | Recon Loss: 0.004597 | Commit Loss: 0.002092 | Perplexity: 1365.397560
2025-09-27 06:52:13,199 Stage: Train 0.5 | Epoch: 174 | Iter: 265200 | Total Loss: 0.005608 | Recon Loss: 0.004564 | Commit Loss: 0.002087 | Perplexity: 1362.466054
2025-09-27 06:53:00,313 Stage: Train 0.5 | Epoch: 174 | Iter: 265400 | Total Loss: 0.005640 | Recon Loss: 0.004592 | Commit Loss: 0.002096 | Perplexity: 1364.113951
2025-09-27 06:53:47,317 Stage: Train 0.5 | Epoch: 174 | Iter: 265600 | Total Loss: 0.005661 | Recon Loss: 0.004614 | Commit Loss: 0.002093 | Perplexity: 1364.712159
2025-09-27 06:54:34,434 Stage: Train 0.5 | Epoch: 174 | Iter: 265800 | Total Loss: 0.005597 | Recon Loss: 0.004554 | Commit Loss: 0.002084 | Perplexity: 1363.008159
Trainning Epoch:  53%|█████▎    | 175/330 [16:43:31<10:01:55, 233.00s/it]2025-09-27 06:55:21,419 Stage: Train 0.5 | Epoch: 175 | Iter: 266000 | Total Loss: 0.005602 | Recon Loss: 0.004560 | Commit Loss: 0.002085 | Perplexity: 1364.218768
2025-09-27 06:56:08,364 Stage: Train 0.5 | Epoch: 175 | Iter: 266200 | Total Loss: 0.005600 | Recon Loss: 0.004556 | Commit Loss: 0.002089 | Perplexity: 1365.360481
2025-09-27 06:56:55,286 Stage: Train 0.5 | Epoch: 175 | Iter: 266400 | Total Loss: 0.005605 | Recon Loss: 0.004567 | Commit Loss: 0.002077 | Perplexity: 1361.700778
2025-09-27 06:57:42,531 Stage: Train 0.5 | Epoch: 175 | Iter: 266600 | Total Loss: 0.005628 | Recon Loss: 0.004585 | Commit Loss: 0.002086 | Perplexity: 1364.772009
2025-09-27 06:58:29,439 Stage: Train 0.5 | Epoch: 175 | Iter: 266800 | Total Loss: 0.005625 | Recon Loss: 0.004576 | Commit Loss: 0.002098 | Perplexity: 1363.936204
2025-09-27 06:59:16,609 Stage: Train 0.5 | Epoch: 175 | Iter: 267000 | Total Loss: 0.005635 | Recon Loss: 0.004586 | Commit Loss: 0.002099 | Perplexity: 1366.957280
2025-09-27 07:00:03,634 Stage: Train 0.5 | Epoch: 175 | Iter: 267200 | Total Loss: 0.005611 | Recon Loss: 0.004563 | Commit Loss: 0.002096 | Perplexity: 1364.415588
Trainning Epoch:  53%|█████▎    | 176/330 [16:49:28<11:33:38, 270.25s/it]2025-09-27 07:00:50,964 Stage: Train 0.5 | Epoch: 176 | Iter: 267400 | Total Loss: 0.005683 | Recon Loss: 0.004635 | Commit Loss: 0.002096 | Perplexity: 1365.183854
2025-09-27 07:01:37,918 Stage: Train 0.5 | Epoch: 176 | Iter: 267600 | Total Loss: 0.005562 | Recon Loss: 0.004523 | Commit Loss: 0.002078 | Perplexity: 1363.410767
2025-09-27 07:02:24,716 Stage: Train 0.5 | Epoch: 176 | Iter: 267800 | Total Loss: 0.005681 | Recon Loss: 0.004633 | Commit Loss: 0.002097 | Perplexity: 1365.909428
2025-09-27 07:03:11,641 Stage: Train 0.5 | Epoch: 176 | Iter: 268000 | Total Loss: 0.005625 | Recon Loss: 0.004583 | Commit Loss: 0.002084 | Perplexity: 1362.428641
2025-09-27 07:03:58,860 Stage: Train 0.5 | Epoch: 176 | Iter: 268200 | Total Loss: 0.005587 | Recon Loss: 0.004546 | Commit Loss: 0.002082 | Perplexity: 1363.687418
2025-09-27 07:04:45,903 Stage: Train 0.5 | Epoch: 176 | Iter: 268400 | Total Loss: 0.005636 | Recon Loss: 0.004588 | Commit Loss: 0.002095 | Perplexity: 1366.332682
2025-09-27 07:05:32,963 Stage: Train 0.5 | Epoch: 176 | Iter: 268600 | Total Loss: 0.005655 | Recon Loss: 0.004609 | Commit Loss: 0.002092 | Perplexity: 1367.903060
2025-09-27 07:06:19,816 Stage: Train 0.5 | Epoch: 176 | Iter: 268800 | Total Loss: 0.005597 | Recon Loss: 0.004551 | Commit Loss: 0.002091 | Perplexity: 1364.063370
Trainning Epoch:  54%|█████▎    | 177/330 [16:55:25<12:35:38, 296.33s/it]2025-09-27 07:07:06,986 Stage: Train 0.5 | Epoch: 177 | Iter: 269000 | Total Loss: 0.005679 | Recon Loss: 0.004636 | Commit Loss: 0.002086 | Perplexity: 1363.789415
2025-09-27 07:07:53,609 Stage: Train 0.5 | Epoch: 177 | Iter: 269200 | Total Loss: 0.005583 | Recon Loss: 0.004537 | Commit Loss: 0.002092 | Perplexity: 1365.501201
2025-09-27 07:08:40,576 Stage: Train 0.5 | Epoch: 177 | Iter: 269400 | Total Loss: 0.005590 | Recon Loss: 0.004546 | Commit Loss: 0.002089 | Perplexity: 1364.178741
2025-09-27 07:09:27,657 Stage: Train 0.5 | Epoch: 177 | Iter: 269600 | Total Loss: 0.005652 | Recon Loss: 0.004612 | Commit Loss: 0.002081 | Perplexity: 1364.060087
2025-09-27 07:10:14,306 Stage: Train 0.5 | Epoch: 177 | Iter: 269800 | Total Loss: 0.005665 | Recon Loss: 0.004624 | Commit Loss: 0.002081 | Perplexity: 1365.197464
2025-09-27 07:11:01,624 Stage: Train 0.5 | Epoch: 177 | Iter: 270000 | Total Loss: 0.005585 | Recon Loss: 0.004547 | Commit Loss: 0.002077 | Perplexity: 1364.290137
2025-09-27 07:11:49,033 Stage: Train 0.5 | Epoch: 177 | Iter: 270200 | Total Loss: 0.005607 | Recon Loss: 0.004557 | Commit Loss: 0.002100 | Perplexity: 1367.136454
Trainning Epoch:  54%|█████▍    | 178/330 [17:01:22<13:17:06, 314.65s/it]2025-09-27 07:12:36,549 Stage: Train 0.5 | Epoch: 178 | Iter: 270400 | Total Loss: 0.005593 | Recon Loss: 0.004550 | Commit Loss: 0.002085 | Perplexity: 1364.033590
2025-09-27 07:13:23,823 Stage: Train 0.5 | Epoch: 178 | Iter: 270600 | Total Loss: 0.005641 | Recon Loss: 0.004593 | Commit Loss: 0.002096 | Perplexity: 1366.094374
2025-09-27 07:14:11,130 Stage: Train 0.5 | Epoch: 178 | Iter: 270800 | Total Loss: 0.005595 | Recon Loss: 0.004553 | Commit Loss: 0.002085 | Perplexity: 1363.903839
2025-09-27 07:14:58,380 Stage: Train 0.5 | Epoch: 178 | Iter: 271000 | Total Loss: 0.005641 | Recon Loss: 0.004599 | Commit Loss: 0.002083 | Perplexity: 1363.790872
2025-09-27 07:15:45,648 Stage: Train 0.5 | Epoch: 178 | Iter: 271200 | Total Loss: 0.005592 | Recon Loss: 0.004549 | Commit Loss: 0.002087 | Perplexity: 1364.342344
2025-09-27 07:16:32,632 Stage: Train 0.5 | Epoch: 178 | Iter: 271400 | Total Loss: 0.005593 | Recon Loss: 0.004557 | Commit Loss: 0.002073 | Perplexity: 1361.978813
2025-09-27 07:17:19,310 Stage: Train 0.5 | Epoch: 178 | Iter: 271600 | Total Loss: 0.005675 | Recon Loss: 0.004629 | Commit Loss: 0.002091 | Perplexity: 1364.717917
2025-09-27 07:18:06,230 Stage: Train 0.5 | Epoch: 178 | Iter: 271800 | Total Loss: 0.005619 | Recon Loss: 0.004579 | Commit Loss: 0.002079 | Perplexity: 1362.107657
Trainning Epoch:  54%|█████▍    | 179/330 [17:07:20<13:44:34, 327.64s/it]2025-09-27 07:18:53,747 Stage: Train 0.5 | Epoch: 179 | Iter: 272000 | Total Loss: 0.005559 | Recon Loss: 0.004519 | Commit Loss: 0.002079 | Perplexity: 1363.674986
2025-09-27 07:19:40,962 Stage: Train 0.5 | Epoch: 179 | Iter: 272200 | Total Loss: 0.005554 | Recon Loss: 0.004514 | Commit Loss: 0.002080 | Perplexity: 1364.632498
2025-09-27 07:20:28,089 Stage: Train 0.5 | Epoch: 179 | Iter: 272400 | Total Loss: 0.005681 | Recon Loss: 0.004642 | Commit Loss: 0.002079 | Perplexity: 1365.105261
2025-09-27 07:21:15,008 Stage: Train 0.5 | Epoch: 179 | Iter: 272600 | Total Loss: 0.005567 | Recon Loss: 0.004521 | Commit Loss: 0.002092 | Perplexity: 1366.327355
2025-09-27 07:22:02,102 Stage: Train 0.5 | Epoch: 179 | Iter: 272800 | Total Loss: 0.005624 | Recon Loss: 0.004580 | Commit Loss: 0.002088 | Perplexity: 1366.564307
2025-09-27 07:22:49,463 Stage: Train 0.5 | Epoch: 179 | Iter: 273000 | Total Loss: 0.005570 | Recon Loss: 0.004527 | Commit Loss: 0.002086 | Perplexity: 1365.820825
2025-09-27 07:23:36,747 Stage: Train 0.5 | Epoch: 179 | Iter: 273200 | Total Loss: 0.005616 | Recon Loss: 0.004573 | Commit Loss: 0.002086 | Perplexity: 1365.802953
2025-09-27 07:24:23,691 Stage: Train 0.5 | Epoch: 179 | Iter: 273400 | Total Loss: 0.005586 | Recon Loss: 0.004541 | Commit Loss: 0.002090 | Perplexity: 1365.697463
Trainning Epoch:  55%|█████▍    | 180/330 [17:13:19<14:02:11, 336.88s/it]2025-09-27 07:25:11,227 Stage: Train 0.5 | Epoch: 180 | Iter: 273600 | Total Loss: 0.005627 | Recon Loss: 0.004584 | Commit Loss: 0.002086 | Perplexity: 1366.422704
2025-09-27 07:25:58,664 Stage: Train 0.5 | Epoch: 180 | Iter: 273800 | Total Loss: 0.005613 | Recon Loss: 0.004569 | Commit Loss: 0.002087 | Perplexity: 1367.272231
2025-09-27 07:26:46,097 Stage: Train 0.5 | Epoch: 180 | Iter: 274000 | Total Loss: 0.005591 | Recon Loss: 0.004549 | Commit Loss: 0.002085 | Perplexity: 1366.483862
2025-09-27 07:27:33,282 Stage: Train 0.5 | Epoch: 180 | Iter: 274200 | Total Loss: 0.005595 | Recon Loss: 0.004556 | Commit Loss: 0.002079 | Perplexity: 1366.716277
2025-09-27 07:28:20,285 Stage: Train 0.5 | Epoch: 180 | Iter: 274400 | Total Loss: 0.005577 | Recon Loss: 0.004538 | Commit Loss: 0.002079 | Perplexity: 1363.862127
2025-09-27 07:29:07,373 Stage: Train 0.5 | Epoch: 180 | Iter: 274600 | Total Loss: 0.005628 | Recon Loss: 0.004586 | Commit Loss: 0.002084 | Perplexity: 1366.387215
2025-09-27 07:29:54,503 Stage: Train 0.5 | Epoch: 180 | Iter: 274800 | Total Loss: 0.005610 | Recon Loss: 0.004568 | Commit Loss: 0.002084 | Perplexity: 1364.587424
Trainning Epoch:  55%|█████▍    | 181/330 [17:19:18<14:12:57, 343.47s/it]2025-09-27 07:30:41,897 Stage: Train 0.5 | Epoch: 181 | Iter: 275000 | Total Loss: 0.005600 | Recon Loss: 0.004559 | Commit Loss: 0.002082 | Perplexity: 1362.446866
2025-09-27 07:31:28,772 Stage: Train 0.5 | Epoch: 181 | Iter: 275200 | Total Loss: 0.005589 | Recon Loss: 0.004549 | Commit Loss: 0.002082 | Perplexity: 1363.127854
2025-09-27 07:32:16,156 Stage: Train 0.5 | Epoch: 181 | Iter: 275400 | Total Loss: 0.005565 | Recon Loss: 0.004525 | Commit Loss: 0.002078 | Perplexity: 1364.480443
2025-09-27 07:33:03,225 Stage: Train 0.5 | Epoch: 181 | Iter: 275600 | Total Loss: 0.005567 | Recon Loss: 0.004524 | Commit Loss: 0.002087 | Perplexity: 1367.529897
2025-09-27 07:33:50,388 Stage: Train 0.5 | Epoch: 181 | Iter: 275800 | Total Loss: 0.005604 | Recon Loss: 0.004565 | Commit Loss: 0.002079 | Perplexity: 1368.193367
2025-09-27 07:34:37,564 Stage: Train 0.5 | Epoch: 181 | Iter: 276000 | Total Loss: 0.005587 | Recon Loss: 0.004543 | Commit Loss: 0.002089 | Perplexity: 1368.782649
2025-09-27 07:35:24,871 Stage: Train 0.5 | Epoch: 181 | Iter: 276200 | Total Loss: 0.005611 | Recon Loss: 0.004567 | Commit Loss: 0.002087 | Perplexity: 1367.619320
2025-09-27 07:36:12,069 Stage: Train 0.5 | Epoch: 181 | Iter: 276400 | Total Loss: 0.005603 | Recon Loss: 0.004556 | Commit Loss: 0.002094 | Perplexity: 1367.878970
Trainning Epoch:  55%|█████▌    | 182/330 [17:25:16<14:18:19, 347.97s/it]2025-09-27 07:36:59,627 Stage: Train 0.5 | Epoch: 182 | Iter: 276600 | Total Loss: 0.005604 | Recon Loss: 0.004564 | Commit Loss: 0.002079 | Perplexity: 1365.849139
2025-09-27 07:37:46,651 Stage: Train 0.5 | Epoch: 182 | Iter: 276800 | Total Loss: 0.005556 | Recon Loss: 0.004515 | Commit Loss: 0.002081 | Perplexity: 1366.064596
2025-09-27 07:38:33,623 Stage: Train 0.5 | Epoch: 182 | Iter: 277000 | Total Loss: 0.005579 | Recon Loss: 0.004540 | Commit Loss: 0.002079 | Perplexity: 1366.803751
2025-09-27 07:39:20,253 Stage: Train 0.5 | Epoch: 182 | Iter: 277200 | Total Loss: 0.005565 | Recon Loss: 0.004520 | Commit Loss: 0.002089 | Perplexity: 1367.517634
2025-09-27 07:40:07,437 Stage: Train 0.5 | Epoch: 182 | Iter: 277400 | Total Loss: 0.005581 | Recon Loss: 0.004536 | Commit Loss: 0.002090 | Perplexity: 1368.196512
2025-09-27 07:40:54,923 Stage: Train 0.5 | Epoch: 182 | Iter: 277600 | Total Loss: 0.005607 | Recon Loss: 0.004561 | Commit Loss: 0.002091 | Perplexity: 1367.778480
2025-09-27 07:41:42,168 Stage: Train 0.5 | Epoch: 182 | Iter: 277800 | Total Loss: 0.005584 | Recon Loss: 0.004541 | Commit Loss: 0.002086 | Perplexity: 1366.541207
Trainning Epoch:  55%|█████▌    | 183/330 [17:31:14<14:20:02, 351.04s/it]2025-09-27 07:42:29,660 Stage: Train 0.5 | Epoch: 183 | Iter: 278000 | Total Loss: 0.005596 | Recon Loss: 0.004555 | Commit Loss: 0.002083 | Perplexity: 1364.570501
2025-09-27 07:43:16,798 Stage: Train 0.5 | Epoch: 183 | Iter: 278200 | Total Loss: 0.005561 | Recon Loss: 0.004522 | Commit Loss: 0.002079 | Perplexity: 1365.081831
2025-09-27 07:44:04,162 Stage: Train 0.5 | Epoch: 183 | Iter: 278400 | Total Loss: 0.005592 | Recon Loss: 0.004549 | Commit Loss: 0.002086 | Perplexity: 1367.101668
2025-09-27 07:44:51,178 Stage: Train 0.5 | Epoch: 183 | Iter: 278600 | Total Loss: 0.005538 | Recon Loss: 0.004497 | Commit Loss: 0.002082 | Perplexity: 1366.649109
2025-09-27 07:45:38,334 Stage: Train 0.5 | Epoch: 183 | Iter: 278800 | Total Loss: 0.005599 | Recon Loss: 0.004555 | Commit Loss: 0.002088 | Perplexity: 1369.397548
2025-09-27 07:46:25,156 Stage: Train 0.5 | Epoch: 183 | Iter: 279000 | Total Loss: 0.005599 | Recon Loss: 0.004560 | Commit Loss: 0.002077 | Perplexity: 1365.494772
2025-09-27 07:47:12,315 Stage: Train 0.5 | Epoch: 183 | Iter: 279200 | Total Loss: 0.005626 | Recon Loss: 0.004583 | Commit Loss: 0.002087 | Perplexity: 1366.188495
2025-09-27 07:47:59,275 Stage: Train 0.5 | Epoch: 183 | Iter: 279400 | Total Loss: 0.005618 | Recon Loss: 0.004579 | Commit Loss: 0.002078 | Perplexity: 1364.594268
Trainning Epoch:  56%|█████▌    | 184/330 [17:37:12<14:19:10, 353.09s/it]2025-09-27 07:48:46,675 Stage: Train 0.5 | Epoch: 184 | Iter: 279600 | Total Loss: 0.005475 | Recon Loss: 0.004440 | Commit Loss: 0.002070 | Perplexity: 1364.934825
2025-09-27 07:49:33,782 Stage: Train 0.5 | Epoch: 184 | Iter: 279800 | Total Loss: 0.005652 | Recon Loss: 0.004612 | Commit Loss: 0.002081 | Perplexity: 1366.711642
2025-09-27 07:50:20,962 Stage: Train 0.5 | Epoch: 184 | Iter: 280000 | Total Loss: 0.005570 | Recon Loss: 0.004530 | Commit Loss: 0.002080 | Perplexity: 1367.374308
2025-09-27 07:50:20,962 Saving model at iteration 280000
2025-09-27 07:50:21,406 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000
2025-09-27 07:50:21,693 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/model.safetensors
2025-09-27 07:50:22,090 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/optimizer.bin
2025-09-27 07:50:22,090 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/scheduler.bin
2025-09-27 07:50:22,091 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/sampler.bin
2025-09-27 07:50:22,092 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/random_states_0.pkl
2025-09-27 07:51:09,449 Stage: Train 0.5 | Epoch: 184 | Iter: 280200 | Total Loss: 0.005595 | Recon Loss: 0.004558 | Commit Loss: 0.002073 | Perplexity: 1366.176193
2025-09-27 07:51:56,730 Stage: Train 0.5 | Epoch: 184 | Iter: 280400 | Total Loss: 0.005561 | Recon Loss: 0.004519 | Commit Loss: 0.002086 | Perplexity: 1366.122304
2025-09-27 07:52:44,216 Stage: Train 0.5 | Epoch: 184 | Iter: 280600 | Total Loss: 0.005576 | Recon Loss: 0.004535 | Commit Loss: 0.002082 | Perplexity: 1367.319705
2025-09-27 07:53:31,479 Stage: Train 0.5 | Epoch: 184 | Iter: 280800 | Total Loss: 0.005579 | Recon Loss: 0.004536 | Commit Loss: 0.002085 | Perplexity: 1364.252558
2025-09-27 07:54:18,759 Stage: Train 0.5 | Epoch: 184 | Iter: 281000 | Total Loss: 0.005594 | Recon Loss: 0.004553 | Commit Loss: 0.002081 | Perplexity: 1366.498155
Trainning Epoch:  56%|█████▌    | 185/330 [17:43:13<14:18:41, 355.32s/it]2025-09-27 07:55:06,397 Stage: Train 0.5 | Epoch: 185 | Iter: 281200 | Total Loss: 0.005598 | Recon Loss: 0.004562 | Commit Loss: 0.002072 | Perplexity: 1367.602293
2025-09-27 07:55:53,757 Stage: Train 0.5 | Epoch: 185 | Iter: 281400 | Total Loss: 0.005569 | Recon Loss: 0.004530 | Commit Loss: 0.002077 | Perplexity: 1364.621398
2025-09-27 07:56:41,041 Stage: Train 0.5 | Epoch: 185 | Iter: 281600 | Total Loss: 0.005506 | Recon Loss: 0.004469 | Commit Loss: 0.002073 | Perplexity: 1364.644023
2025-09-27 07:57:28,374 Stage: Train 0.5 | Epoch: 185 | Iter: 281800 | Total Loss: 0.005638 | Recon Loss: 0.004594 | Commit Loss: 0.002087 | Perplexity: 1368.227108
2025-09-27 07:58:15,760 Stage: Train 0.5 | Epoch: 185 | Iter: 282000 | Total Loss: 0.005565 | Recon Loss: 0.004523 | Commit Loss: 0.002083 | Perplexity: 1367.692751
2025-09-27 07:59:02,817 Stage: Train 0.5 | Epoch: 185 | Iter: 282200 | Total Loss: 0.005568 | Recon Loss: 0.004528 | Commit Loss: 0.002080 | Perplexity: 1370.176618
2025-09-27 07:59:50,005 Stage: Train 0.5 | Epoch: 185 | Iter: 282400 | Total Loss: 0.005591 | Recon Loss: 0.004552 | Commit Loss: 0.002077 | Perplexity: 1366.971594
Trainning Epoch:  56%|█████▋    | 186/330 [17:49:12<14:15:33, 356.48s/it]2025-09-27 08:00:37,415 Stage: Train 0.5 | Epoch: 186 | Iter: 282600 | Total Loss: 0.005554 | Recon Loss: 0.004512 | Commit Loss: 0.002084 | Perplexity: 1365.649678
2025-09-27 08:01:24,684 Stage: Train 0.5 | Epoch: 186 | Iter: 282800 | Total Loss: 0.005535 | Recon Loss: 0.004494 | Commit Loss: 0.002083 | Perplexity: 1368.645857
2025-09-27 08:02:11,771 Stage: Train 0.5 | Epoch: 186 | Iter: 283000 | Total Loss: 0.005556 | Recon Loss: 0.004519 | Commit Loss: 0.002074 | Perplexity: 1366.875038
2025-09-27 08:02:59,235 Stage: Train 0.5 | Epoch: 186 | Iter: 283200 | Total Loss: 0.005606 | Recon Loss: 0.004565 | Commit Loss: 0.002084 | Perplexity: 1370.450421
2025-09-27 08:03:46,487 Stage: Train 0.5 | Epoch: 186 | Iter: 283400 | Total Loss: 0.005576 | Recon Loss: 0.004536 | Commit Loss: 0.002081 | Perplexity: 1366.307770
2025-09-27 08:04:33,573 Stage: Train 0.5 | Epoch: 186 | Iter: 283600 | Total Loss: 0.005571 | Recon Loss: 0.004529 | Commit Loss: 0.002084 | Perplexity: 1368.708304
2025-09-27 08:05:20,925 Stage: Train 0.5 | Epoch: 186 | Iter: 283800 | Total Loss: 0.005552 | Recon Loss: 0.004514 | Commit Loss: 0.002076 | Perplexity: 1367.300970
2025-09-27 08:06:08,227 Stage: Train 0.5 | Epoch: 186 | Iter: 284000 | Total Loss: 0.005524 | Recon Loss: 0.004482 | Commit Loss: 0.002084 | Perplexity: 1368.200234
Trainning Epoch:  57%|█████▋    | 187/330 [17:55:11<14:11:31, 357.28s/it]2025-09-27 08:06:55,718 Stage: Train 0.5 | Epoch: 187 | Iter: 284200 | Total Loss: 0.005557 | Recon Loss: 0.004518 | Commit Loss: 0.002077 | Perplexity: 1366.204818
2025-09-27 08:07:42,695 Stage: Train 0.5 | Epoch: 187 | Iter: 284400 | Total Loss: 0.005596 | Recon Loss: 0.004557 | Commit Loss: 0.002077 | Perplexity: 1369.565334
2025-09-27 08:08:30,025 Stage: Train 0.5 | Epoch: 187 | Iter: 284600 | Total Loss: 0.005601 | Recon Loss: 0.004563 | Commit Loss: 0.002075 | Perplexity: 1369.446093
2025-09-27 08:09:17,341 Stage: Train 0.5 | Epoch: 187 | Iter: 284800 | Total Loss: 0.005535 | Recon Loss: 0.004498 | Commit Loss: 0.002073 | Perplexity: 1368.950355
2025-09-27 08:10:04,364 Stage: Train 0.5 | Epoch: 187 | Iter: 285000 | Total Loss: 0.005519 | Recon Loss: 0.004484 | Commit Loss: 0.002071 | Perplexity: 1365.595566
2025-09-27 08:10:51,441 Stage: Train 0.5 | Epoch: 187 | Iter: 285200 | Total Loss: 0.005620 | Recon Loss: 0.004577 | Commit Loss: 0.002087 | Perplexity: 1367.885966
2025-09-27 08:11:38,620 Stage: Train 0.5 | Epoch: 187 | Iter: 285400 | Total Loss: 0.005587 | Recon Loss: 0.004545 | Commit Loss: 0.002083 | Perplexity: 1367.864576
Trainning Epoch:  57%|█████▋    | 188/330 [18:01:09<14:06:19, 357.60s/it]2025-09-27 08:12:25,917 Stage: Train 0.5 | Epoch: 188 | Iter: 285600 | Total Loss: 0.005519 | Recon Loss: 0.004479 | Commit Loss: 0.002079 | Perplexity: 1365.142042
2025-09-27 08:13:13,241 Stage: Train 0.5 | Epoch: 188 | Iter: 285800 | Total Loss: 0.005602 | Recon Loss: 0.004562 | Commit Loss: 0.002079 | Perplexity: 1371.436888
2025-09-27 08:14:00,528 Stage: Train 0.5 | Epoch: 188 | Iter: 286000 | Total Loss: 0.005525 | Recon Loss: 0.004489 | Commit Loss: 0.002072 | Perplexity: 1367.717367
2025-09-27 08:14:47,378 Stage: Train 0.5 | Epoch: 188 | Iter: 286200 | Total Loss: 0.005539 | Recon Loss: 0.004501 | Commit Loss: 0.002075 | Perplexity: 1366.307120
2025-09-27 08:15:34,472 Stage: Train 0.5 | Epoch: 188 | Iter: 286400 | Total Loss: 0.005536 | Recon Loss: 0.004496 | Commit Loss: 0.002080 | Perplexity: 1370.083997
2025-09-27 08:16:21,830 Stage: Train 0.5 | Epoch: 188 | Iter: 286600 | Total Loss: 0.005597 | Recon Loss: 0.004558 | Commit Loss: 0.002078 | Perplexity: 1370.839424
2025-09-27 08:17:09,017 Stage: Train 0.5 | Epoch: 188 | Iter: 286800 | Total Loss: 0.005537 | Recon Loss: 0.004500 | Commit Loss: 0.002074 | Perplexity: 1366.248597
2025-09-27 08:17:56,191 Stage: Train 0.5 | Epoch: 188 | Iter: 287000 | Total Loss: 0.005585 | Recon Loss: 0.004545 | Commit Loss: 0.002079 | Perplexity: 1369.353176
Trainning Epoch:  57%|█████▋    | 189/330 [18:07:08<14:00:59, 357.87s/it]2025-09-27 08:18:43,358 Stage: Train 0.5 | Epoch: 189 | Iter: 287200 | Total Loss: 0.005569 | Recon Loss: 0.004536 | Commit Loss: 0.002067 | Perplexity: 1366.169658
2025-09-27 08:19:30,454 Stage: Train 0.5 | Epoch: 189 | Iter: 287400 | Total Loss: 0.005557 | Recon Loss: 0.004522 | Commit Loss: 0.002070 | Perplexity: 1369.128395
2025-09-27 08:20:17,580 Stage: Train 0.5 | Epoch: 189 | Iter: 287600 | Total Loss: 0.005553 | Recon Loss: 0.004515 | Commit Loss: 0.002075 | Perplexity: 1369.494136
2025-09-27 08:21:04,865 Stage: Train 0.5 | Epoch: 189 | Iter: 287800 | Total Loss: 0.005513 | Recon Loss: 0.004474 | Commit Loss: 0.002079 | Perplexity: 1369.326712
2025-09-27 08:21:52,260 Stage: Train 0.5 | Epoch: 189 | Iter: 288000 | Total Loss: 0.005550 | Recon Loss: 0.004511 | Commit Loss: 0.002077 | Perplexity: 1368.601028
2025-09-27 08:22:39,271 Stage: Train 0.5 | Epoch: 189 | Iter: 288200 | Total Loss: 0.005552 | Recon Loss: 0.004512 | Commit Loss: 0.002081 | Perplexity: 1368.972970
2025-09-27 08:23:24,934 Stage: Train 0.5 | Epoch: 189 | Iter: 288400 | Total Loss: 0.005517 | Recon Loss: 0.004478 | Commit Loss: 0.002077 | Perplexity: 1368.668855
2025-09-27 08:24:12,193 Stage: Train 0.5 | Epoch: 189 | Iter: 288600 | Total Loss: 0.005641 | Recon Loss: 0.004601 | Commit Loss: 0.002081 | Perplexity: 1368.902052
Trainning Epoch:  58%|█████▊    | 190/330 [18:13:05<13:54:24, 357.60s/it]2025-09-27 08:24:59,535 Stage: Train 0.5 | Epoch: 190 | Iter: 288800 | Total Loss: 0.005491 | Recon Loss: 0.004456 | Commit Loss: 0.002071 | Perplexity: 1366.763907
2025-09-27 08:25:46,674 Stage: Train 0.5 | Epoch: 190 | Iter: 289000 | Total Loss: 0.005525 | Recon Loss: 0.004489 | Commit Loss: 0.002073 | Perplexity: 1367.919893
2025-09-27 08:26:33,902 Stage: Train 0.5 | Epoch: 190 | Iter: 289200 | Total Loss: 0.005595 | Recon Loss: 0.004561 | Commit Loss: 0.002068 | Perplexity: 1365.917373
2025-09-27 08:27:20,956 Stage: Train 0.5 | Epoch: 190 | Iter: 289400 | Total Loss: 0.005539 | Recon Loss: 0.004497 | Commit Loss: 0.002085 | Perplexity: 1369.786089
2025-09-27 08:28:08,311 Stage: Train 0.5 | Epoch: 190 | Iter: 289600 | Total Loss: 0.005530 | Recon Loss: 0.004496 | Commit Loss: 0.002069 | Perplexity: 1368.121184
2025-09-27 08:28:55,564 Stage: Train 0.5 | Epoch: 190 | Iter: 289800 | Total Loss: 0.005546 | Recon Loss: 0.004505 | Commit Loss: 0.002082 | Perplexity: 1371.796194
2025-09-27 08:29:42,191 Stage: Train 0.5 | Epoch: 190 | Iter: 290000 | Total Loss: 0.005542 | Recon Loss: 0.004503 | Commit Loss: 0.002077 | Perplexity: 1369.309567
Trainning Epoch:  58%|█████▊    | 191/330 [18:19:03<13:48:52, 357.79s/it]2025-09-27 08:30:29,795 Stage: Train 0.5 | Epoch: 191 | Iter: 290200 | Total Loss: 0.005523 | Recon Loss: 0.004490 | Commit Loss: 0.002066 | Perplexity: 1369.052668
2025-09-27 08:31:17,040 Stage: Train 0.5 | Epoch: 191 | Iter: 290400 | Total Loss: 0.005519 | Recon Loss: 0.004485 | Commit Loss: 0.002068 | Perplexity: 1368.814143
2025-09-27 08:32:04,092 Stage: Train 0.5 | Epoch: 191 | Iter: 290600 | Total Loss: 0.005527 | Recon Loss: 0.004491 | Commit Loss: 0.002072 | Perplexity: 1368.111228
2025-09-27 08:32:51,109 Stage: Train 0.5 | Epoch: 191 | Iter: 290800 | Total Loss: 0.005491 | Recon Loss: 0.004454 | Commit Loss: 0.002073 | Perplexity: 1369.914101
2025-09-27 08:33:38,126 Stage: Train 0.5 | Epoch: 191 | Iter: 291000 | Total Loss: 0.005564 | Recon Loss: 0.004527 | Commit Loss: 0.002074 | Perplexity: 1370.539921
2025-09-27 08:34:25,147 Stage: Train 0.5 | Epoch: 191 | Iter: 291200 | Total Loss: 0.005523 | Recon Loss: 0.004484 | Commit Loss: 0.002078 | Perplexity: 1367.636219
2025-09-27 08:35:12,391 Stage: Train 0.5 | Epoch: 191 | Iter: 291400 | Total Loss: 0.005564 | Recon Loss: 0.004522 | Commit Loss: 0.002084 | Perplexity: 1369.654526
2025-09-27 08:35:59,586 Stage: Train 0.5 | Epoch: 191 | Iter: 291600 | Total Loss: 0.005571 | Recon Loss: 0.004535 | Commit Loss: 0.002073 | Perplexity: 1367.663942
Trainning Epoch:  58%|█████▊    | 192/330 [18:25:01<13:43:10, 357.91s/it]2025-09-27 08:36:46,605 Stage: Train 0.5 | Epoch: 192 | Iter: 291800 | Total Loss: 0.005514 | Recon Loss: 0.004480 | Commit Loss: 0.002067 | Perplexity: 1366.922489
2025-09-27 08:37:33,729 Stage: Train 0.5 | Epoch: 192 | Iter: 292000 | Total Loss: 0.005516 | Recon Loss: 0.004480 | Commit Loss: 0.002072 | Perplexity: 1368.628326
2025-09-27 08:38:20,958 Stage: Train 0.5 | Epoch: 192 | Iter: 292200 | Total Loss: 0.005553 | Recon Loss: 0.004516 | Commit Loss: 0.002074 | Perplexity: 1370.219873
2025-09-27 08:39:08,161 Stage: Train 0.5 | Epoch: 192 | Iter: 292400 | Total Loss: 0.005516 | Recon Loss: 0.004480 | Commit Loss: 0.002071 | Perplexity: 1368.675581
2025-09-27 08:39:55,359 Stage: Train 0.5 | Epoch: 192 | Iter: 292600 | Total Loss: 0.005514 | Recon Loss: 0.004477 | Commit Loss: 0.002074 | Perplexity: 1371.551484
2025-09-27 08:40:42,718 Stage: Train 0.5 | Epoch: 192 | Iter: 292800 | Total Loss: 0.005565 | Recon Loss: 0.004527 | Commit Loss: 0.002076 | Perplexity: 1366.993550
2025-09-27 08:41:30,007 Stage: Train 0.5 | Epoch: 192 | Iter: 293000 | Total Loss: 0.005521 | Recon Loss: 0.004482 | Commit Loss: 0.002079 | Perplexity: 1371.527432
Trainning Epoch:  58%|█████▊    | 193/330 [18:31:00<13:37:46, 358.15s/it]2025-09-27 08:42:17,628 Stage: Train 0.5 | Epoch: 193 | Iter: 293200 | Total Loss: 0.005509 | Recon Loss: 0.004475 | Commit Loss: 0.002067 | Perplexity: 1370.961660
2025-09-27 08:43:04,911 Stage: Train 0.5 | Epoch: 193 | Iter: 293400 | Total Loss: 0.005483 | Recon Loss: 0.004448 | Commit Loss: 0.002068 | Perplexity: 1372.319321
2025-09-27 08:43:51,935 Stage: Train 0.5 | Epoch: 193 | Iter: 293600 | Total Loss: 0.005490 | Recon Loss: 0.004455 | Commit Loss: 0.002069 | Perplexity: 1369.606602
2025-09-27 08:44:39,149 Stage: Train 0.5 | Epoch: 193 | Iter: 293800 | Total Loss: 0.005553 | Recon Loss: 0.004512 | Commit Loss: 0.002081 | Perplexity: 1369.309946
2025-09-27 08:45:26,250 Stage: Train 0.5 | Epoch: 193 | Iter: 294000 | Total Loss: 0.005522 | Recon Loss: 0.004482 | Commit Loss: 0.002080 | Perplexity: 1370.420316
2025-09-27 08:46:13,498 Stage: Train 0.5 | Epoch: 193 | Iter: 294200 | Total Loss: 0.005557 | Recon Loss: 0.004524 | Commit Loss: 0.002066 | Perplexity: 1367.195459
2025-09-27 08:47:00,852 Stage: Train 0.5 | Epoch: 193 | Iter: 294400 | Total Loss: 0.005555 | Recon Loss: 0.004515 | Commit Loss: 0.002082 | Perplexity: 1368.872200
2025-09-27 08:47:48,203 Stage: Train 0.5 | Epoch: 193 | Iter: 294600 | Total Loss: 0.005457 | Recon Loss: 0.004423 | Commit Loss: 0.002069 | Perplexity: 1368.914374
Trainning Epoch:  59%|█████▉    | 194/330 [18:36:59<13:32:26, 358.43s/it]2025-09-27 08:48:35,881 Stage: Train 0.5 | Epoch: 194 | Iter: 294800 | Total Loss: 0.005543 | Recon Loss: 0.004507 | Commit Loss: 0.002071 | Perplexity: 1368.270151
2025-09-27 08:49:23,236 Stage: Train 0.5 | Epoch: 194 | Iter: 295000 | Total Loss: 0.005519 | Recon Loss: 0.004482 | Commit Loss: 0.002073 | Perplexity: 1368.673261
2025-09-27 08:50:10,451 Stage: Train 0.5 | Epoch: 194 | Iter: 295200 | Total Loss: 0.005495 | Recon Loss: 0.004460 | Commit Loss: 0.002071 | Perplexity: 1370.099987
2025-09-27 08:50:57,257 Stage: Train 0.5 | Epoch: 194 | Iter: 295400 | Total Loss: 0.005541 | Recon Loss: 0.004504 | Commit Loss: 0.002074 | Perplexity: 1371.934337
2025-09-27 08:51:44,704 Stage: Train 0.5 | Epoch: 194 | Iter: 295600 | Total Loss: 0.005487 | Recon Loss: 0.004452 | Commit Loss: 0.002071 | Perplexity: 1370.114574
2025-09-27 08:52:32,009 Stage: Train 0.5 | Epoch: 194 | Iter: 295800 | Total Loss: 0.005533 | Recon Loss: 0.004498 | Commit Loss: 0.002071 | Perplexity: 1373.057852
2025-09-27 08:53:19,332 Stage: Train 0.5 | Epoch: 194 | Iter: 296000 | Total Loss: 0.005482 | Recon Loss: 0.004445 | Commit Loss: 0.002075 | Perplexity: 1369.041166
2025-09-27 08:54:06,663 Stage: Train 0.5 | Epoch: 194 | Iter: 296200 | Total Loss: 0.005507 | Recon Loss: 0.004468 | Commit Loss: 0.002077 | Perplexity: 1369.728703
Trainning Epoch:  59%|█████▉    | 195/330 [18:42:58<13:26:55, 358.63s/it]2025-09-27 08:54:54,633 Stage: Train 0.5 | Epoch: 195 | Iter: 296400 | Total Loss: 0.005516 | Recon Loss: 0.004484 | Commit Loss: 0.002065 | Perplexity: 1368.946323
2025-09-27 08:55:41,918 Stage: Train 0.5 | Epoch: 195 | Iter: 296600 | Total Loss: 0.005538 | Recon Loss: 0.004503 | Commit Loss: 0.002071 | Perplexity: 1370.347509
2025-09-27 08:56:29,270 Stage: Train 0.5 | Epoch: 195 | Iter: 296800 | Total Loss: 0.005469 | Recon Loss: 0.004435 | Commit Loss: 0.002068 | Perplexity: 1369.163769
2025-09-27 08:57:16,508 Stage: Train 0.5 | Epoch: 195 | Iter: 297000 | Total Loss: 0.005516 | Recon Loss: 0.004476 | Commit Loss: 0.002080 | Perplexity: 1372.362534
2025-09-27 08:58:03,508 Stage: Train 0.5 | Epoch: 195 | Iter: 297200 | Total Loss: 0.005532 | Recon Loss: 0.004496 | Commit Loss: 0.002072 | Perplexity: 1371.820996
2025-09-27 08:58:50,736 Stage: Train 0.5 | Epoch: 195 | Iter: 297400 | Total Loss: 0.005528 | Recon Loss: 0.004491 | Commit Loss: 0.002075 | Perplexity: 1372.938010
2025-09-27 08:59:38,153 Stage: Train 0.5 | Epoch: 195 | Iter: 297600 | Total Loss: 0.005500 | Recon Loss: 0.004463 | Commit Loss: 0.002074 | Perplexity: 1369.151112
Trainning Epoch:  59%|█████▉    | 196/330 [18:48:58<13:21:42, 358.98s/it]2025-09-27 09:00:25,863 Stage: Train 0.5 | Epoch: 196 | Iter: 297800 | Total Loss: 0.005467 | Recon Loss: 0.004428 | Commit Loss: 0.002079 | Perplexity: 1370.786356
2025-09-27 09:01:13,007 Stage: Train 0.5 | Epoch: 196 | Iter: 298000 | Total Loss: 0.005521 | Recon Loss: 0.004485 | Commit Loss: 0.002072 | Perplexity: 1372.247412
2025-09-27 09:02:00,197 Stage: Train 0.5 | Epoch: 196 | Iter: 298200 | Total Loss: 0.005526 | Recon Loss: 0.004490 | Commit Loss: 0.002071 | Perplexity: 1372.612887
2025-09-27 09:02:47,374 Stage: Train 0.5 | Epoch: 196 | Iter: 298400 | Total Loss: 0.005534 | Recon Loss: 0.004494 | Commit Loss: 0.002079 | Perplexity: 1371.237068
2025-09-27 09:03:34,637 Stage: Train 0.5 | Epoch: 196 | Iter: 298600 | Total Loss: 0.005512 | Recon Loss: 0.004479 | Commit Loss: 0.002066 | Perplexity: 1371.963543
2025-09-27 09:04:21,831 Stage: Train 0.5 | Epoch: 196 | Iter: 298800 | Total Loss: 0.005492 | Recon Loss: 0.004462 | Commit Loss: 0.002060 | Perplexity: 1367.226282
2025-09-27 09:05:09,128 Stage: Train 0.5 | Epoch: 196 | Iter: 299000 | Total Loss: 0.005493 | Recon Loss: 0.004451 | Commit Loss: 0.002085 | Perplexity: 1371.541236
2025-09-27 09:05:56,087 Stage: Train 0.5 | Epoch: 196 | Iter: 299200 | Total Loss: 0.005520 | Recon Loss: 0.004482 | Commit Loss: 0.002075 | Perplexity: 1371.569438
Trainning Epoch:  60%|█████▉    | 197/330 [18:54:56<13:15:29, 358.87s/it]2025-09-27 09:06:43,497 Stage: Train 0.5 | Epoch: 197 | Iter: 299400 | Total Loss: 0.005509 | Recon Loss: 0.004477 | Commit Loss: 0.002064 | Perplexity: 1370.283730
2025-09-27 09:07:30,765 Stage: Train 0.5 | Epoch: 197 | Iter: 299600 | Total Loss: 0.005465 | Recon Loss: 0.004432 | Commit Loss: 0.002066 | Perplexity: 1372.110060
2025-09-27 09:08:18,239 Stage: Train 0.5 | Epoch: 197 | Iter: 299800 | Total Loss: 0.005481 | Recon Loss: 0.004448 | Commit Loss: 0.002064 | Perplexity: 1369.430240
2025-09-27 09:09:05,659 Stage: Train 0.5 | Epoch: 197 | Iter: 300000 | Total Loss: 0.005472 | Recon Loss: 0.004437 | Commit Loss: 0.002071 | Perplexity: 1370.325146
2025-09-27 09:09:05,659 Saving model at iteration 300000
2025-09-27 09:09:06,143 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000
2025-09-27 09:09:06,441 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/model.safetensors
2025-09-27 09:09:06,846 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/optimizer.bin
2025-09-27 09:09:06,846 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/scheduler.bin
2025-09-27 09:09:06,846 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/sampler.bin
2025-09-27 09:09:06,847 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/random_states_0.pkl
2025-09-27 09:09:54,415 Stage: Train 0.5 | Epoch: 197 | Iter: 300200 | Total Loss: 0.005529 | Recon Loss: 0.004497 | Commit Loss: 0.002063 | Perplexity: 1370.668699
2025-09-27 09:10:41,598 Stage: Train 0.5 | Epoch: 197 | Iter: 300400 | Total Loss: 0.005560 | Recon Loss: 0.004523 | Commit Loss: 0.002074 | Perplexity: 1371.573329
2025-09-27 09:11:28,916 Stage: Train 0.5 | Epoch: 197 | Iter: 300600 | Total Loss: 0.005443 | Recon Loss: 0.004408 | Commit Loss: 0.002071 | Perplexity: 1369.616561
Trainning Epoch:  60%|██████    | 198/330 [19:00:58<13:11:03, 359.57s/it]2025-09-27 09:12:16,666 Stage: Train 0.5 | Epoch: 198 | Iter: 300800 | Total Loss: 0.005577 | Recon Loss: 0.004537 | Commit Loss: 0.002079 | Perplexity: 1370.544022
2025-09-27 09:13:03,595 Stage: Train 0.5 | Epoch: 198 | Iter: 301000 | Total Loss: 0.005518 | Recon Loss: 0.004486 | Commit Loss: 0.002064 | Perplexity: 1370.877421
2025-09-27 09:13:50,854 Stage: Train 0.5 | Epoch: 198 | Iter: 301200 | Total Loss: 0.005487 | Recon Loss: 0.004451 | Commit Loss: 0.002071 | Perplexity: 1373.957213
2025-09-27 09:14:38,065 Stage: Train 0.5 | Epoch: 198 | Iter: 301400 | Total Loss: 0.005496 | Recon Loss: 0.004460 | Commit Loss: 0.002072 | Perplexity: 1369.998468
2025-09-27 09:15:25,071 Stage: Train 0.5 | Epoch: 198 | Iter: 301600 | Total Loss: 0.005463 | Recon Loss: 0.004430 | Commit Loss: 0.002066 | Perplexity: 1368.042138
2025-09-27 09:16:12,252 Stage: Train 0.5 | Epoch: 198 | Iter: 301800 | Total Loss: 0.005486 | Recon Loss: 0.004454 | Commit Loss: 0.002064 | Perplexity: 1370.317521
2025-09-27 09:16:59,455 Stage: Train 0.5 | Epoch: 198 | Iter: 302000 | Total Loss: 0.005516 | Recon Loss: 0.004477 | Commit Loss: 0.002078 | Perplexity: 1373.926547
2025-09-27 09:17:46,689 Stage: Train 0.5 | Epoch: 198 | Iter: 302200 | Total Loss: 0.005518 | Recon Loss: 0.004481 | Commit Loss: 0.002075 | Perplexity: 1371.564042
Trainning Epoch:  60%|██████    | 199/330 [19:06:56<13:04:08, 359.15s/it]2025-09-27 09:18:34,040 Stage: Train 0.5 | Epoch: 199 | Iter: 302400 | Total Loss: 0.005425 | Recon Loss: 0.004395 | Commit Loss: 0.002059 | Perplexity: 1370.271915
2025-09-27 09:19:21,293 Stage: Train 0.5 | Epoch: 199 | Iter: 302600 | Total Loss: 0.005486 | Recon Loss: 0.004451 | Commit Loss: 0.002069 | Perplexity: 1373.493409
2025-09-27 09:20:08,251 Stage: Train 0.5 | Epoch: 199 | Iter: 302800 | Total Loss: 0.005477 | Recon Loss: 0.004445 | Commit Loss: 0.002065 | Perplexity: 1372.925261
2025-09-27 09:20:55,320 Stage: Train 0.5 | Epoch: 199 | Iter: 303000 | Total Loss: 0.005515 | Recon Loss: 0.004475 | Commit Loss: 0.002080 | Perplexity: 1372.184611
2025-09-27 09:21:42,364 Stage: Train 0.5 | Epoch: 199 | Iter: 303200 | Total Loss: 0.005443 | Recon Loss: 0.004408 | Commit Loss: 0.002069 | Perplexity: 1370.046311
2025-09-27 09:22:29,487 Stage: Train 0.5 | Epoch: 199 | Iter: 303400 | Total Loss: 0.005499 | Recon Loss: 0.004467 | Commit Loss: 0.002064 | Perplexity: 1370.387449
2025-09-27 09:23:16,509 Stage: Train 0.5 | Epoch: 199 | Iter: 303600 | Total Loss: 0.005472 | Recon Loss: 0.004433 | Commit Loss: 0.002078 | Perplexity: 1372.243553
2025-09-27 09:24:03,717 Stage: Train 0.5 | Epoch: 199 | Iter: 303800 | Total Loss: 0.005487 | Recon Loss: 0.004451 | Commit Loss: 0.002073 | Perplexity: 1370.536964
Trainning Epoch:  61%|██████    | 200/330 [19:12:54<12:57:29, 358.84s/it]2025-09-27 09:24:51,066 Stage: Train 0.5 | Epoch: 200 | Iter: 304000 | Total Loss: 0.005494 | Recon Loss: 0.004461 | Commit Loss: 0.002066 | Perplexity: 1371.119730
2025-09-27 09:25:38,154 Stage: Train 0.5 | Epoch: 200 | Iter: 304200 | Total Loss: 0.005478 | Recon Loss: 0.004443 | Commit Loss: 0.002069 | Perplexity: 1369.362138
2025-09-27 09:26:25,292 Stage: Train 0.5 | Epoch: 200 | Iter: 304400 | Total Loss: 0.005510 | Recon Loss: 0.004477 | Commit Loss: 0.002065 | Perplexity: 1369.205178
2025-09-27 09:27:12,464 Stage: Train 0.5 | Epoch: 200 | Iter: 304600 | Total Loss: 0.005468 | Recon Loss: 0.004436 | Commit Loss: 0.002063 | Perplexity: 1368.429111
2025-09-27 09:27:59,630 Stage: Train 0.5 | Epoch: 200 | Iter: 304800 | Total Loss: 0.005489 | Recon Loss: 0.004454 | Commit Loss: 0.002072 | Perplexity: 1374.740239
2025-09-27 09:28:46,801 Stage: Train 0.5 | Epoch: 200 | Iter: 305000 | Total Loss: 0.005480 | Recon Loss: 0.004446 | Commit Loss: 0.002069 | Perplexity: 1370.452378
2025-09-27 09:29:33,906 Stage: Train 0.5 | Epoch: 200 | Iter: 305200 | Total Loss: 0.005482 | Recon Loss: 0.004446 | Commit Loss: 0.002072 | Perplexity: 1373.264202
Trainning Epoch:  61%|██████    | 201/330 [19:18:52<12:51:07, 358.67s/it]2025-09-27 09:30:21,329 Stage: Train 0.5 | Epoch: 201 | Iter: 305400 | Total Loss: 0.005443 | Recon Loss: 0.004411 | Commit Loss: 0.002064 | Perplexity: 1369.512209
2025-09-27 09:31:08,356 Stage: Train 0.5 | Epoch: 201 | Iter: 305600 | Total Loss: 0.005497 | Recon Loss: 0.004464 | Commit Loss: 0.002066 | Perplexity: 1372.061894
2025-09-27 09:31:55,507 Stage: Train 0.5 | Epoch: 201 | Iter: 305800 | Total Loss: 0.005465 | Recon Loss: 0.004432 | Commit Loss: 0.002065 | Perplexity: 1370.422366
2025-09-27 09:32:42,495 Stage: Train 0.5 | Epoch: 201 | Iter: 306000 | Total Loss: 0.005466 | Recon Loss: 0.004438 | Commit Loss: 0.002056 | Perplexity: 1372.352766
2025-09-27 09:33:29,580 Stage: Train 0.5 | Epoch: 201 | Iter: 306200 | Total Loss: 0.005507 | Recon Loss: 0.004470 | Commit Loss: 0.002074 | Perplexity: 1371.457774
2025-09-27 09:34:16,443 Stage: Train 0.5 | Epoch: 201 | Iter: 306400 | Total Loss: 0.005463 | Recon Loss: 0.004427 | Commit Loss: 0.002071 | Perplexity: 1375.098890
2025-09-27 09:35:03,544 Stage: Train 0.5 | Epoch: 201 | Iter: 306600 | Total Loss: 0.005447 | Recon Loss: 0.004419 | Commit Loss: 0.002056 | Perplexity: 1369.035682
2025-09-27 09:35:50,702 Stage: Train 0.5 | Epoch: 201 | Iter: 306800 | Total Loss: 0.005457 | Recon Loss: 0.004425 | Commit Loss: 0.002065 | Perplexity: 1371.274439
Trainning Epoch:  61%|██████    | 202/330 [19:24:50<12:44:33, 358.39s/it]2025-09-27 09:36:37,816 Stage: Train 0.5 | Epoch: 202 | Iter: 307000 | Total Loss: 0.005473 | Recon Loss: 0.004440 | Commit Loss: 0.002066 | Perplexity: 1370.232921
2025-09-27 09:37:24,859 Stage: Train 0.5 | Epoch: 202 | Iter: 307200 | Total Loss: 0.005442 | Recon Loss: 0.004412 | Commit Loss: 0.002060 | Perplexity: 1372.556848
2025-09-27 09:38:12,017 Stage: Train 0.5 | Epoch: 202 | Iter: 307400 | Total Loss: 0.005428 | Recon Loss: 0.004395 | Commit Loss: 0.002067 | Perplexity: 1372.840637
2025-09-27 09:38:59,402 Stage: Train 0.5 | Epoch: 202 | Iter: 307600 | Total Loss: 0.005475 | Recon Loss: 0.004444 | Commit Loss: 0.002062 | Perplexity: 1373.978582
2025-09-27 09:39:46,673 Stage: Train 0.5 | Epoch: 202 | Iter: 307800 | Total Loss: 0.005503 | Recon Loss: 0.004467 | Commit Loss: 0.002074 | Perplexity: 1372.710291
2025-09-27 09:40:33,749 Stage: Train 0.5 | Epoch: 202 | Iter: 308000 | Total Loss: 0.005418 | Recon Loss: 0.004387 | Commit Loss: 0.002063 | Perplexity: 1370.451584
2025-09-27 09:41:20,583 Stage: Train 0.5 | Epoch: 202 | Iter: 308200 | Total Loss: 0.005520 | Recon Loss: 0.004488 | Commit Loss: 0.002064 | Perplexity: 1373.489483
Trainning Epoch:  62%|██████▏   | 203/330 [19:30:48<12:38:11, 358.20s/it]2025-09-27 09:42:07,949 Stage: Train 0.5 | Epoch: 203 | Iter: 308400 | Total Loss: 0.005489 | Recon Loss: 0.004455 | Commit Loss: 0.002069 | Perplexity: 1370.374058
2025-09-27 09:42:55,170 Stage: Train 0.5 | Epoch: 203 | Iter: 308600 | Total Loss: 0.005462 | Recon Loss: 0.004433 | Commit Loss: 0.002059 | Perplexity: 1372.022635
2025-09-27 09:43:42,470 Stage: Train 0.5 | Epoch: 203 | Iter: 308800 | Total Loss: 0.005446 | Recon Loss: 0.004413 | Commit Loss: 0.002066 | Perplexity: 1372.611025
2025-09-27 09:44:29,391 Stage: Train 0.5 | Epoch: 203 | Iter: 309000 | Total Loss: 0.005510 | Recon Loss: 0.004474 | Commit Loss: 0.002071 | Perplexity: 1372.015354
2025-09-27 09:45:16,745 Stage: Train 0.5 | Epoch: 203 | Iter: 309200 | Total Loss: 0.005499 | Recon Loss: 0.004468 | Commit Loss: 0.002062 | Perplexity: 1372.716732
2025-09-27 09:46:03,914 Stage: Train 0.5 | Epoch: 203 | Iter: 309400 | Total Loss: 0.005479 | Recon Loss: 0.004448 | Commit Loss: 0.002063 | Perplexity: 1370.405485
2025-09-27 09:46:50,999 Stage: Train 0.5 | Epoch: 203 | Iter: 309600 | Total Loss: 0.005441 | Recon Loss: 0.004412 | Commit Loss: 0.002057 | Perplexity: 1370.875735
2025-09-27 09:47:38,158 Stage: Train 0.5 | Epoch: 203 | Iter: 309800 | Total Loss: 0.005453 | Recon Loss: 0.004422 | Commit Loss: 0.002062 | Perplexity: 1371.317864
Trainning Epoch:  62%|██████▏   | 204/330 [19:36:46<12:32:26, 358.30s/it]2025-09-27 09:48:25,573 Stage: Train 0.5 | Epoch: 204 | Iter: 310000 | Total Loss: 0.005454 | Recon Loss: 0.004427 | Commit Loss: 0.002054 | Perplexity: 1368.963293
2025-09-27 09:49:12,359 Stage: Train 0.5 | Epoch: 204 | Iter: 310200 | Total Loss: 0.005497 | Recon Loss: 0.004465 | Commit Loss: 0.002065 | Perplexity: 1370.626384
2025-09-27 09:49:59,463 Stage: Train 0.5 | Epoch: 204 | Iter: 310400 | Total Loss: 0.005406 | Recon Loss: 0.004378 | Commit Loss: 0.002056 | Perplexity: 1370.684704
2025-09-27 09:50:46,485 Stage: Train 0.5 | Epoch: 204 | Iter: 310600 | Total Loss: 0.005445 | Recon Loss: 0.004415 | Commit Loss: 0.002061 | Perplexity: 1369.656321
2025-09-27 09:51:33,775 Stage: Train 0.5 | Epoch: 204 | Iter: 310800 | Total Loss: 0.005436 | Recon Loss: 0.004406 | Commit Loss: 0.002060 | Perplexity: 1371.965596
2025-09-27 09:52:20,990 Stage: Train 0.5 | Epoch: 204 | Iter: 311000 | Total Loss: 0.005505 | Recon Loss: 0.004475 | Commit Loss: 0.002060 | Perplexity: 1370.151306
2025-09-27 09:53:08,077 Stage: Train 0.5 | Epoch: 204 | Iter: 311200 | Total Loss: 0.005492 | Recon Loss: 0.004460 | Commit Loss: 0.002066 | Perplexity: 1373.061152
Trainning Epoch:  62%|██████▏   | 205/330 [19:42:44<12:26:19, 358.23s/it]2025-09-27 09:53:55,544 Stage: Train 0.5 | Epoch: 205 | Iter: 311400 | Total Loss: 0.005451 | Recon Loss: 0.004417 | Commit Loss: 0.002068 | Perplexity: 1374.717523
2025-09-27 09:54:42,710 Stage: Train 0.5 | Epoch: 205 | Iter: 311600 | Total Loss: 0.005429 | Recon Loss: 0.004400 | Commit Loss: 0.002058 | Perplexity: 1373.805197
2025-09-27 09:55:29,725 Stage: Train 0.5 | Epoch: 205 | Iter: 311800 | Total Loss: 0.005438 | Recon Loss: 0.004410 | Commit Loss: 0.002056 | Perplexity: 1372.036301
2025-09-27 09:56:16,634 Stage: Train 0.5 | Epoch: 205 | Iter: 312000 | Total Loss: 0.005478 | Recon Loss: 0.004445 | Commit Loss: 0.002065 | Perplexity: 1373.315402
2025-09-27 09:57:03,570 Stage: Train 0.5 | Epoch: 205 | Iter: 312200 | Total Loss: 0.005406 | Recon Loss: 0.004377 | Commit Loss: 0.002058 | Perplexity: 1371.972813
2025-09-27 09:57:50,814 Stage: Train 0.5 | Epoch: 205 | Iter: 312400 | Total Loss: 0.005438 | Recon Loss: 0.004404 | Commit Loss: 0.002068 | Perplexity: 1371.750309
2025-09-27 09:58:35,938 Stage: Train 0.5 | Epoch: 205 | Iter: 312600 | Total Loss: 0.005550 | Recon Loss: 0.004516 | Commit Loss: 0.002068 | Perplexity: 1371.493584
2025-09-27 09:59:22,994 Stage: Train 0.5 | Epoch: 205 | Iter: 312800 | Total Loss: 0.005423 | Recon Loss: 0.004393 | Commit Loss: 0.002060 | Perplexity: 1371.163322
Trainning Epoch:  62%|██████▏   | 206/330 [19:48:40<12:18:46, 357.47s/it]2025-09-27 10:00:10,365 Stage: Train 0.5 | Epoch: 206 | Iter: 313000 | Total Loss: 0.005464 | Recon Loss: 0.004432 | Commit Loss: 0.002065 | Perplexity: 1374.819456
2025-09-27 10:00:57,569 Stage: Train 0.5 | Epoch: 206 | Iter: 313200 | Total Loss: 0.005428 | Recon Loss: 0.004398 | Commit Loss: 0.002059 | Perplexity: 1370.523701
2025-09-27 10:01:44,767 Stage: Train 0.5 | Epoch: 206 | Iter: 313400 | Total Loss: 0.005445 | Recon Loss: 0.004413 | Commit Loss: 0.002064 | Perplexity: 1373.840648
2025-09-27 10:02:31,753 Stage: Train 0.5 | Epoch: 206 | Iter: 313600 | Total Loss: 0.005456 | Recon Loss: 0.004424 | Commit Loss: 0.002063 | Perplexity: 1372.549692
2025-09-27 10:03:18,553 Stage: Train 0.5 | Epoch: 206 | Iter: 313800 | Total Loss: 0.005422 | Recon Loss: 0.004394 | Commit Loss: 0.002055 | Perplexity: 1372.122305
2025-09-27 10:04:05,658 Stage: Train 0.5 | Epoch: 206 | Iter: 314000 | Total Loss: 0.005444 | Recon Loss: 0.004410 | Commit Loss: 0.002066 | Perplexity: 1371.825455
2025-09-27 10:04:52,598 Stage: Train 0.5 | Epoch: 206 | Iter: 314200 | Total Loss: 0.005479 | Recon Loss: 0.004449 | Commit Loss: 0.002060 | Perplexity: 1372.408353
2025-09-27 10:05:39,769 Stage: Train 0.5 | Epoch: 206 | Iter: 314400 | Total Loss: 0.005440 | Recon Loss: 0.004409 | Commit Loss: 0.002061 | Perplexity: 1374.791361
Trainning Epoch:  63%|██████▎   | 207/330 [19:54:38<12:13:00, 357.57s/it]2025-09-27 10:06:27,065 Stage: Train 0.5 | Epoch: 207 | Iter: 314600 | Total Loss: 0.005418 | Recon Loss: 0.004388 | Commit Loss: 0.002059 | Perplexity: 1374.177109
2025-09-27 10:07:14,169 Stage: Train 0.5 | Epoch: 207 | Iter: 314800 | Total Loss: 0.005462 | Recon Loss: 0.004435 | Commit Loss: 0.002056 | Perplexity: 1371.029666
2025-09-27 10:08:01,366 Stage: Train 0.5 | Epoch: 207 | Iter: 315000 | Total Loss: 0.005413 | Recon Loss: 0.004385 | Commit Loss: 0.002056 | Perplexity: 1372.103051
2025-09-27 10:08:48,514 Stage: Train 0.5 | Epoch: 207 | Iter: 315200 | Total Loss: 0.005432 | Recon Loss: 0.004399 | Commit Loss: 0.002068 | Perplexity: 1375.957347
2025-09-27 10:09:35,777 Stage: Train 0.5 | Epoch: 207 | Iter: 315400 | Total Loss: 0.005509 | Recon Loss: 0.004475 | Commit Loss: 0.002069 | Perplexity: 1373.594144
2025-09-27 10:10:22,832 Stage: Train 0.5 | Epoch: 207 | Iter: 315600 | Total Loss: 0.005408 | Recon Loss: 0.004379 | Commit Loss: 0.002058 | Perplexity: 1373.167189
2025-09-27 10:11:10,018 Stage: Train 0.5 | Epoch: 207 | Iter: 315800 | Total Loss: 0.005450 | Recon Loss: 0.004421 | Commit Loss: 0.002058 | Perplexity: 1373.455627
Trainning Epoch:  63%|██████▎   | 208/330 [20:00:36<12:07:28, 357.78s/it]2025-09-27 10:11:57,440 Stage: Train 0.5 | Epoch: 208 | Iter: 316000 | Total Loss: 0.005428 | Recon Loss: 0.004399 | Commit Loss: 0.002058 | Perplexity: 1372.316167
2025-09-27 10:12:44,539 Stage: Train 0.5 | Epoch: 208 | Iter: 316200 | Total Loss: 0.005451 | Recon Loss: 0.004421 | Commit Loss: 0.002060 | Perplexity: 1374.449380
2025-09-27 10:13:31,695 Stage: Train 0.5 | Epoch: 208 | Iter: 316400 | Total Loss: 0.005427 | Recon Loss: 0.004398 | Commit Loss: 0.002059 | Perplexity: 1373.507064
2025-09-27 10:14:18,723 Stage: Train 0.5 | Epoch: 208 | Iter: 316600 | Total Loss: 0.005443 | Recon Loss: 0.004414 | Commit Loss: 0.002058 | Perplexity: 1373.925889
2025-09-27 10:15:05,823 Stage: Train 0.5 | Epoch: 208 | Iter: 316800 | Total Loss: 0.005462 | Recon Loss: 0.004433 | Commit Loss: 0.002058 | Perplexity: 1373.264124
2025-09-27 10:15:53,139 Stage: Train 0.5 | Epoch: 208 | Iter: 317000 | Total Loss: 0.005466 | Recon Loss: 0.004434 | Commit Loss: 0.002063 | Perplexity: 1372.669643
2025-09-27 10:16:40,267 Stage: Train 0.5 | Epoch: 208 | Iter: 317200 | Total Loss: 0.005472 | Recon Loss: 0.004441 | Commit Loss: 0.002062 | Perplexity: 1374.966288
2025-09-27 10:17:27,356 Stage: Train 0.5 | Epoch: 208 | Iter: 317400 | Total Loss: 0.005469 | Recon Loss: 0.004442 | Commit Loss: 0.002053 | Perplexity: 1374.076330
Trainning Epoch:  63%|██████▎   | 209/330 [20:06:34<12:01:35, 357.81s/it]2025-09-27 10:18:14,289 Stage: Train 0.5 | Epoch: 209 | Iter: 317600 | Total Loss: 0.005430 | Recon Loss: 0.004403 | Commit Loss: 0.002056 | Perplexity: 1374.067430
2025-09-27 10:19:01,619 Stage: Train 0.5 | Epoch: 209 | Iter: 317800 | Total Loss: 0.005493 | Recon Loss: 0.004461 | Commit Loss: 0.002063 | Perplexity: 1374.793594
2025-09-27 10:19:48,711 Stage: Train 0.5 | Epoch: 209 | Iter: 318000 | Total Loss: 0.005432 | Recon Loss: 0.004404 | Commit Loss: 0.002056 | Perplexity: 1374.106289
2025-09-27 10:20:35,840 Stage: Train 0.5 | Epoch: 209 | Iter: 318200 | Total Loss: 0.005402 | Recon Loss: 0.004372 | Commit Loss: 0.002059 | Perplexity: 1374.683735
2025-09-27 10:21:23,112 Stage: Train 0.5 | Epoch: 209 | Iter: 318400 | Total Loss: 0.005392 | Recon Loss: 0.004368 | Commit Loss: 0.002048 | Perplexity: 1370.245063
2025-09-27 10:22:10,287 Stage: Train 0.5 | Epoch: 209 | Iter: 318600 | Total Loss: 0.005467 | Recon Loss: 0.004438 | Commit Loss: 0.002057 | Perplexity: 1373.685022
2025-09-27 10:22:57,524 Stage: Train 0.5 | Epoch: 209 | Iter: 318800 | Total Loss: 0.005428 | Recon Loss: 0.004402 | Commit Loss: 0.002051 | Perplexity: 1376.138058
Trainning Epoch:  64%|██████▎   | 210/330 [20:12:33<11:56:09, 358.08s/it]2025-09-27 10:23:45,053 Stage: Train 0.5 | Epoch: 210 | Iter: 319000 | Total Loss: 0.005427 | Recon Loss: 0.004397 | Commit Loss: 0.002061 | Perplexity: 1374.973577
2025-09-27 10:24:32,176 Stage: Train 0.5 | Epoch: 210 | Iter: 319200 | Total Loss: 0.005452 | Recon Loss: 0.004427 | Commit Loss: 0.002050 | Perplexity: 1373.349619
2025-09-27 10:25:18,884 Stage: Train 0.5 | Epoch: 210 | Iter: 319400 | Total Loss: 0.005378 | Recon Loss: 0.004349 | Commit Loss: 0.002059 | Perplexity: 1375.573856
2025-09-27 10:26:06,057 Stage: Train 0.5 | Epoch: 210 | Iter: 319600 | Total Loss: 0.005456 | Recon Loss: 0.004428 | Commit Loss: 0.002054 | Perplexity: 1371.096015
2025-09-27 10:26:53,197 Stage: Train 0.5 | Epoch: 210 | Iter: 319800 | Total Loss: 0.005436 | Recon Loss: 0.004407 | Commit Loss: 0.002058 | Perplexity: 1375.896755
2025-09-27 10:27:40,153 Stage: Train 0.5 | Epoch: 210 | Iter: 320000 | Total Loss: 0.005414 | Recon Loss: 0.004381 | Commit Loss: 0.002066 | Perplexity: 1376.963416
2025-09-27 10:27:40,153 Saving model at iteration 320000
2025-09-27 10:27:40,791 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000
2025-09-27 10:27:41,095 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/model.safetensors
2025-09-27 10:27:41,496 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/optimizer.bin
2025-09-27 10:27:41,497 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/scheduler.bin
2025-09-27 10:27:41,497 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/sampler.bin
2025-09-27 10:27:41,498 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/random_states_0.pkl
2025-09-27 10:28:29,348 Stage: Train 0.5 | Epoch: 210 | Iter: 320200 | Total Loss: 0.005433 | Recon Loss: 0.004405 | Commit Loss: 0.002057 | Perplexity: 1375.180662
2025-09-27 10:29:16,649 Stage: Train 0.5 | Epoch: 210 | Iter: 320400 | Total Loss: 0.005506 | Recon Loss: 0.004477 | Commit Loss: 0.002058 | Perplexity: 1372.825327
Trainning Epoch:  64%|██████▍   | 211/330 [20:18:33<11:51:17, 358.63s/it]2025-09-27 10:30:04,229 Stage: Train 0.5 | Epoch: 211 | Iter: 320600 | Total Loss: 0.005355 | Recon Loss: 0.004332 | Commit Loss: 0.002046 | Perplexity: 1370.495458
2025-09-27 10:30:51,570 Stage: Train 0.5 | Epoch: 211 | Iter: 320800 | Total Loss: 0.005448 | Recon Loss: 0.004418 | Commit Loss: 0.002060 | Perplexity: 1375.794601
2025-09-27 10:31:38,824 Stage: Train 0.5 | Epoch: 211 | Iter: 321000 | Total Loss: 0.005432 | Recon Loss: 0.004404 | Commit Loss: 0.002056 | Perplexity: 1373.436091
2025-09-27 10:32:25,710 Stage: Train 0.5 | Epoch: 211 | Iter: 321200 | Total Loss: 0.005374 | Recon Loss: 0.004346 | Commit Loss: 0.002056 | Perplexity: 1374.800167
2025-09-27 10:33:13,079 Stage: Train 0.5 | Epoch: 211 | Iter: 321400 | Total Loss: 0.005415 | Recon Loss: 0.004387 | Commit Loss: 0.002056 | Perplexity: 1373.165607
2025-09-27 10:34:00,413 Stage: Train 0.5 | Epoch: 211 | Iter: 321600 | Total Loss: 0.005435 | Recon Loss: 0.004407 | Commit Loss: 0.002056 | Perplexity: 1374.895721
2025-09-27 10:34:47,783 Stage: Train 0.5 | Epoch: 211 | Iter: 321800 | Total Loss: 0.005424 | Recon Loss: 0.004396 | Commit Loss: 0.002057 | Perplexity: 1373.060247
2025-09-27 10:35:35,047 Stage: Train 0.5 | Epoch: 211 | Iter: 322000 | Total Loss: 0.005437 | Recon Loss: 0.004405 | Commit Loss: 0.002064 | Perplexity: 1374.342631
Trainning Epoch:  64%|██████▍   | 212/330 [20:24:32<11:45:42, 358.83s/it]2025-09-27 10:36:22,540 Stage: Train 0.5 | Epoch: 212 | Iter: 322200 | Total Loss: 0.005416 | Recon Loss: 0.004395 | Commit Loss: 0.002043 | Perplexity: 1372.018082
2025-09-27 10:37:09,779 Stage: Train 0.5 | Epoch: 212 | Iter: 322400 | Total Loss: 0.005405 | Recon Loss: 0.004380 | Commit Loss: 0.002051 | Perplexity: 1374.719368
2025-09-27 10:37:56,943 Stage: Train 0.5 | Epoch: 212 | Iter: 322600 | Total Loss: 0.005423 | Recon Loss: 0.004395 | Commit Loss: 0.002056 | Perplexity: 1374.885453
2025-09-27 10:38:44,203 Stage: Train 0.5 | Epoch: 212 | Iter: 322800 | Total Loss: 0.005429 | Recon Loss: 0.004397 | Commit Loss: 0.002064 | Perplexity: 1377.272907
2025-09-27 10:39:31,254 Stage: Train 0.5 | Epoch: 212 | Iter: 323000 | Total Loss: 0.005378 | Recon Loss: 0.004356 | Commit Loss: 0.002043 | Perplexity: 1370.563918
2025-09-27 10:40:18,524 Stage: Train 0.5 | Epoch: 212 | Iter: 323200 | Total Loss: 0.005444 | Recon Loss: 0.004417 | Commit Loss: 0.002054 | Perplexity: 1374.129894
2025-09-27 10:41:05,774 Stage: Train 0.5 | Epoch: 212 | Iter: 323400 | Total Loss: 0.005482 | Recon Loss: 0.004449 | Commit Loss: 0.002068 | Perplexity: 1374.366600
Trainning Epoch:  65%|██████▍   | 213/330 [20:30:31<11:39:49, 358.89s/it]2025-09-27 10:41:53,308 Stage: Train 0.5 | Epoch: 213 | Iter: 323600 | Total Loss: 0.005438 | Recon Loss: 0.004406 | Commit Loss: 0.002064 | Perplexity: 1374.071490
2025-09-27 10:42:40,509 Stage: Train 0.5 | Epoch: 213 | Iter: 323800 | Total Loss: 0.005404 | Recon Loss: 0.004382 | Commit Loss: 0.002046 | Perplexity: 1375.897611
2025-09-27 10:43:27,726 Stage: Train 0.5 | Epoch: 213 | Iter: 324000 | Total Loss: 0.005423 | Recon Loss: 0.004397 | Commit Loss: 0.002052 | Perplexity: 1373.615545
2025-09-27 10:44:15,024 Stage: Train 0.5 | Epoch: 213 | Iter: 324200 | Total Loss: 0.005391 | Recon Loss: 0.004367 | Commit Loss: 0.002049 | Perplexity: 1372.319986
2025-09-27 10:45:02,278 Stage: Train 0.5 | Epoch: 213 | Iter: 324400 | Total Loss: 0.005436 | Recon Loss: 0.004409 | Commit Loss: 0.002055 | Perplexity: 1374.044048
2025-09-27 10:45:49,588 Stage: Train 0.5 | Epoch: 213 | Iter: 324600 | Total Loss: 0.005471 | Recon Loss: 0.004446 | Commit Loss: 0.002050 | Perplexity: 1374.534602
2025-09-27 10:46:36,651 Stage: Train 0.5 | Epoch: 213 | Iter: 324800 | Total Loss: 0.005396 | Recon Loss: 0.004370 | Commit Loss: 0.002052 | Perplexity: 1373.847598
2025-09-27 10:47:24,087 Stage: Train 0.5 | Epoch: 213 | Iter: 325000 | Total Loss: 0.005400 | Recon Loss: 0.004371 | Commit Loss: 0.002058 | Perplexity: 1376.052116
Trainning Epoch:  65%|██████▍   | 214/330 [20:36:30<11:33:58, 358.95s/it]2025-09-27 10:48:11,789 Stage: Train 0.5 | Epoch: 214 | Iter: 325200 | Total Loss: 0.005370 | Recon Loss: 0.004346 | Commit Loss: 0.002049 | Perplexity: 1371.854093
2025-09-27 10:48:59,106 Stage: Train 0.5 | Epoch: 214 | Iter: 325400 | Total Loss: 0.005446 | Recon Loss: 0.004417 | Commit Loss: 0.002059 | Perplexity: 1374.145456
2025-09-27 10:49:46,458 Stage: Train 0.5 | Epoch: 214 | Iter: 325600 | Total Loss: 0.005429 | Recon Loss: 0.004405 | Commit Loss: 0.002046 | Perplexity: 1373.755584
2025-09-27 10:50:33,734 Stage: Train 0.5 | Epoch: 214 | Iter: 325800 | Total Loss: 0.005372 | Recon Loss: 0.004343 | Commit Loss: 0.002057 | Perplexity: 1374.933150
2025-09-27 10:51:20,965 Stage: Train 0.5 | Epoch: 214 | Iter: 326000 | Total Loss: 0.005399 | Recon Loss: 0.004374 | Commit Loss: 0.002050 | Perplexity: 1374.144828
2025-09-27 10:52:08,050 Stage: Train 0.5 | Epoch: 214 | Iter: 326200 | Total Loss: 0.005422 | Recon Loss: 0.004390 | Commit Loss: 0.002064 | Perplexity: 1379.032001
2025-09-27 10:52:55,308 Stage: Train 0.5 | Epoch: 214 | Iter: 326400 | Total Loss: 0.005427 | Recon Loss: 0.004399 | Commit Loss: 0.002056 | Perplexity: 1374.919661
Trainning Epoch:  65%|██████▌   | 215/330 [20:42:29<11:27:51, 358.89s/it]2025-09-27 10:53:42,295 Stage: Train 0.5 | Epoch: 215 | Iter: 326600 | Total Loss: 0.005460 | Recon Loss: 0.004436 | Commit Loss: 0.002048 | Perplexity: 1374.600873
2025-09-27 10:54:29,445 Stage: Train 0.5 | Epoch: 215 | Iter: 326800 | Total Loss: 0.005410 | Recon Loss: 0.004385 | Commit Loss: 0.002050 | Perplexity: 1376.642510
2025-09-27 10:55:16,337 Stage: Train 0.5 | Epoch: 215 | Iter: 327000 | Total Loss: 0.005382 | Recon Loss: 0.004358 | Commit Loss: 0.002048 | Perplexity: 1374.737751
2025-09-27 10:56:03,450 Stage: Train 0.5 | Epoch: 215 | Iter: 327200 | Total Loss: 0.005401 | Recon Loss: 0.004374 | Commit Loss: 0.002054 | Perplexity: 1375.900850
2025-09-27 10:56:50,555 Stage: Train 0.5 | Epoch: 215 | Iter: 327400 | Total Loss: 0.005395 | Recon Loss: 0.004366 | Commit Loss: 0.002058 | Perplexity: 1375.367407
2025-09-27 10:57:37,766 Stage: Train 0.5 | Epoch: 215 | Iter: 327600 | Total Loss: 0.005399 | Recon Loss: 0.004376 | Commit Loss: 0.002046 | Perplexity: 1372.978978
2025-09-27 10:58:25,138 Stage: Train 0.5 | Epoch: 215 | Iter: 327800 | Total Loss: 0.005365 | Recon Loss: 0.004339 | Commit Loss: 0.002053 | Perplexity: 1375.648445
2025-09-27 10:59:12,350 Stage: Train 0.5 | Epoch: 215 | Iter: 328000 | Total Loss: 0.005403 | Recon Loss: 0.004377 | Commit Loss: 0.002052 | Perplexity: 1375.429683
Trainning Epoch:  65%|██████▌   | 216/330 [20:48:27<11:21:35, 358.73s/it]2025-09-27 10:59:59,761 Stage: Train 0.5 | Epoch: 216 | Iter: 328200 | Total Loss: 0.005397 | Recon Loss: 0.004376 | Commit Loss: 0.002043 | Perplexity: 1372.051852
2025-09-27 11:00:46,844 Stage: Train 0.5 | Epoch: 216 | Iter: 328400 | Total Loss: 0.005414 | Recon Loss: 0.004389 | Commit Loss: 0.002049 | Perplexity: 1373.595497
2025-09-27 11:01:33,485 Stage: Train 0.5 | Epoch: 216 | Iter: 328600 | Total Loss: 0.005392 | Recon Loss: 0.004368 | Commit Loss: 0.002048 | Perplexity: 1376.756921
2025-09-27 11:02:20,650 Stage: Train 0.5 | Epoch: 216 | Iter: 328800 | Total Loss: 0.005411 | Recon Loss: 0.004384 | Commit Loss: 0.002054 | Perplexity: 1377.501599
2025-09-27 11:03:07,768 Stage: Train 0.5 | Epoch: 216 | Iter: 329000 | Total Loss: 0.005417 | Recon Loss: 0.004386 | Commit Loss: 0.002061 | Perplexity: 1377.983269
2025-09-27 11:03:54,945 Stage: Train 0.5 | Epoch: 216 | Iter: 329200 | Total Loss: 0.005397 | Recon Loss: 0.004365 | Commit Loss: 0.002063 | Perplexity: 1375.953378
2025-09-27 11:04:42,075 Stage: Train 0.5 | Epoch: 216 | Iter: 329400 | Total Loss: 0.005356 | Recon Loss: 0.004330 | Commit Loss: 0.002052 | Perplexity: 1374.197363
2025-09-27 11:05:29,099 Stage: Train 0.5 | Epoch: 216 | Iter: 329600 | Total Loss: 0.005432 | Recon Loss: 0.004405 | Commit Loss: 0.002054 | Perplexity: 1375.767042
Trainning Epoch:  66%|██████▌   | 217/330 [20:54:25<11:14:59, 358.40s/it]2025-09-27 11:06:16,458 Stage: Train 0.5 | Epoch: 217 | Iter: 329800 | Total Loss: 0.005345 | Recon Loss: 0.004321 | Commit Loss: 0.002048 | Perplexity: 1373.193876
2025-09-27 11:07:03,742 Stage: Train 0.5 | Epoch: 217 | Iter: 330000 | Total Loss: 0.005380 | Recon Loss: 0.004357 | Commit Loss: 0.002046 | Perplexity: 1374.415725
2025-09-27 11:07:51,118 Stage: Train 0.5 | Epoch: 217 | Iter: 330200 | Total Loss: 0.005372 | Recon Loss: 0.004346 | Commit Loss: 0.002052 | Perplexity: 1375.435557
2025-09-27 11:08:37,969 Stage: Train 0.5 | Epoch: 217 | Iter: 330400 | Total Loss: 0.005393 | Recon Loss: 0.004363 | Commit Loss: 0.002059 | Perplexity: 1378.262403
2025-09-27 11:09:25,059 Stage: Train 0.5 | Epoch: 217 | Iter: 330600 | Total Loss: 0.005396 | Recon Loss: 0.004370 | Commit Loss: 0.002053 | Perplexity: 1375.133987
2025-09-27 11:10:12,067 Stage: Train 0.5 | Epoch: 217 | Iter: 330800 | Total Loss: 0.005410 | Recon Loss: 0.004382 | Commit Loss: 0.002057 | Perplexity: 1376.122919
2025-09-27 11:10:58,967 Stage: Train 0.5 | Epoch: 217 | Iter: 331000 | Total Loss: 0.005450 | Recon Loss: 0.004419 | Commit Loss: 0.002061 | Perplexity: 1378.017877
Trainning Epoch:  66%|██████▌   | 218/330 [21:00:23<11:08:42, 358.24s/it]2025-09-27 11:11:46,192 Stage: Train 0.5 | Epoch: 218 | Iter: 331200 | Total Loss: 0.005372 | Recon Loss: 0.004347 | Commit Loss: 0.002050 | Perplexity: 1374.466460
2025-09-27 11:12:33,457 Stage: Train 0.5 | Epoch: 218 | Iter: 331400 | Total Loss: 0.005409 | Recon Loss: 0.004383 | Commit Loss: 0.002051 | Perplexity: 1377.562562
2025-09-27 11:13:20,692 Stage: Train 0.5 | Epoch: 218 | Iter: 331600 | Total Loss: 0.005356 | Recon Loss: 0.004332 | Commit Loss: 0.002048 | Perplexity: 1375.391518
2025-09-27 11:14:07,756 Stage: Train 0.5 | Epoch: 218 | Iter: 331800 | Total Loss: 0.005406 | Recon Loss: 0.004384 | Commit Loss: 0.002045 | Perplexity: 1374.341646
2025-09-27 11:14:54,836 Stage: Train 0.5 | Epoch: 218 | Iter: 332000 | Total Loss: 0.005358 | Recon Loss: 0.004330 | Commit Loss: 0.002057 | Perplexity: 1375.697648
2025-09-27 11:15:41,816 Stage: Train 0.5 | Epoch: 218 | Iter: 332200 | Total Loss: 0.005447 | Recon Loss: 0.004418 | Commit Loss: 0.002057 | Perplexity: 1376.096404
2025-09-27 11:16:29,191 Stage: Train 0.5 | Epoch: 218 | Iter: 332400 | Total Loss: 0.005358 | Recon Loss: 0.004336 | Commit Loss: 0.002045 | Perplexity: 1374.717062
2025-09-27 11:17:16,619 Stage: Train 0.5 | Epoch: 218 | Iter: 332600 | Total Loss: 0.005377 | Recon Loss: 0.004350 | Commit Loss: 0.002056 | Perplexity: 1379.272120
Trainning Epoch:  66%|██████▋   | 219/330 [21:06:21<11:02:57, 358.36s/it]2025-09-27 11:18:04,002 Stage: Train 0.5 | Epoch: 219 | Iter: 332800 | Total Loss: 0.005392 | Recon Loss: 0.004368 | Commit Loss: 0.002050 | Perplexity: 1376.064922
2025-09-27 11:18:51,175 Stage: Train 0.5 | Epoch: 219 | Iter: 333000 | Total Loss: 0.005370 | Recon Loss: 0.004349 | Commit Loss: 0.002042 | Perplexity: 1376.406727
2025-09-27 11:19:38,354 Stage: Train 0.5 | Epoch: 219 | Iter: 333200 | Total Loss: 0.005356 | Recon Loss: 0.004329 | Commit Loss: 0.002055 | Perplexity: 1375.388854
2025-09-27 11:20:25,495 Stage: Train 0.5 | Epoch: 219 | Iter: 333400 | Total Loss: 0.005341 | Recon Loss: 0.004317 | Commit Loss: 0.002050 | Perplexity: 1375.506423
2025-09-27 11:21:12,908 Stage: Train 0.5 | Epoch: 219 | Iter: 333600 | Total Loss: 0.005444 | Recon Loss: 0.004418 | Commit Loss: 0.002054 | Perplexity: 1375.393490
2025-09-27 11:22:00,283 Stage: Train 0.5 | Epoch: 219 | Iter: 333800 | Total Loss: 0.005352 | Recon Loss: 0.004327 | Commit Loss: 0.002050 | Perplexity: 1378.624797
2025-09-27 11:22:47,179 Stage: Train 0.5 | Epoch: 219 | Iter: 334000 | Total Loss: 0.005418 | Recon Loss: 0.004394 | Commit Loss: 0.002049 | Perplexity: 1376.651761
Trainning Epoch:  67%|██████▋   | 220/330 [21:12:20<10:57:08, 358.44s/it]2025-09-27 11:23:34,622 Stage: Train 0.5 | Epoch: 220 | Iter: 334200 | Total Loss: 0.005374 | Recon Loss: 0.004350 | Commit Loss: 0.002048 | Perplexity: 1377.196559
2025-09-27 11:24:21,931 Stage: Train 0.5 | Epoch: 220 | Iter: 334400 | Total Loss: 0.005414 | Recon Loss: 0.004390 | Commit Loss: 0.002047 | Perplexity: 1376.441424
2025-09-27 11:25:09,111 Stage: Train 0.5 | Epoch: 220 | Iter: 334600 | Total Loss: 0.005310 | Recon Loss: 0.004288 | Commit Loss: 0.002044 | Perplexity: 1377.100761
2025-09-27 11:25:56,368 Stage: Train 0.5 | Epoch: 220 | Iter: 334800 | Total Loss: 0.005360 | Recon Loss: 0.004341 | Commit Loss: 0.002038 | Perplexity: 1376.526978
2025-09-27 11:26:43,622 Stage: Train 0.5 | Epoch: 220 | Iter: 335000 | Total Loss: 0.005412 | Recon Loss: 0.004387 | Commit Loss: 0.002050 | Perplexity: 1377.663168
2025-09-27 11:27:30,743 Stage: Train 0.5 | Epoch: 220 | Iter: 335200 | Total Loss: 0.005329 | Recon Loss: 0.004307 | Commit Loss: 0.002044 | Perplexity: 1374.062781
2025-09-27 11:28:17,946 Stage: Train 0.5 | Epoch: 220 | Iter: 335400 | Total Loss: 0.005390 | Recon Loss: 0.004361 | Commit Loss: 0.002060 | Perplexity: 1376.695742
2025-09-27 11:29:04,939 Stage: Train 0.5 | Epoch: 220 | Iter: 335600 | Total Loss: 0.005375 | Recon Loss: 0.004347 | Commit Loss: 0.002056 | Perplexity: 1376.740745
Trainning Epoch:  67%|██████▋   | 221/330 [21:18:18<10:51:15, 358.49s/it]2025-09-27 11:29:52,003 Stage: Train 0.5 | Epoch: 221 | Iter: 335800 | Total Loss: 0.005378 | Recon Loss: 0.004352 | Commit Loss: 0.002051 | Perplexity: 1374.966802
2025-09-27 11:30:39,009 Stage: Train 0.5 | Epoch: 221 | Iter: 336000 | Total Loss: 0.005331 | Recon Loss: 0.004311 | Commit Loss: 0.002040 | Perplexity: 1374.650513
2025-09-27 11:31:26,168 Stage: Train 0.5 | Epoch: 221 | Iter: 336200 | Total Loss: 0.005352 | Recon Loss: 0.004328 | Commit Loss: 0.002048 | Perplexity: 1374.646352
2025-09-27 11:32:13,373 Stage: Train 0.5 | Epoch: 221 | Iter: 336400 | Total Loss: 0.005401 | Recon Loss: 0.004381 | Commit Loss: 0.002040 | Perplexity: 1375.912268
2025-09-27 11:33:00,491 Stage: Train 0.5 | Epoch: 221 | Iter: 336600 | Total Loss: 0.005336 | Recon Loss: 0.004313 | Commit Loss: 0.002045 | Perplexity: 1372.914246
2025-09-27 11:33:46,208 Stage: Train 0.5 | Epoch: 221 | Iter: 336800 | Total Loss: 0.005416 | Recon Loss: 0.004392 | Commit Loss: 0.002048 | Perplexity: 1377.922459
2025-09-27 11:34:33,450 Stage: Train 0.5 | Epoch: 221 | Iter: 337000 | Total Loss: 0.005399 | Recon Loss: 0.004372 | Commit Loss: 0.002053 | Perplexity: 1378.049067
2025-09-27 11:35:20,721 Stage: Train 0.5 | Epoch: 221 | Iter: 337200 | Total Loss: 0.005359 | Recon Loss: 0.004337 | Commit Loss: 0.002045 | Perplexity: 1375.683236
Trainning Epoch:  67%|██████▋   | 222/330 [21:24:15<10:44:19, 357.96s/it]2025-09-27 11:36:08,293 Stage: Train 0.5 | Epoch: 222 | Iter: 337400 | Total Loss: 0.005363 | Recon Loss: 0.004345 | Commit Loss: 0.002036 | Perplexity: 1374.483015
2025-09-27 11:36:55,560 Stage: Train 0.5 | Epoch: 222 | Iter: 337600 | Total Loss: 0.005349 | Recon Loss: 0.004326 | Commit Loss: 0.002046 | Perplexity: 1377.261664
2025-09-27 11:37:42,624 Stage: Train 0.5 | Epoch: 222 | Iter: 337800 | Total Loss: 0.005379 | Recon Loss: 0.004356 | Commit Loss: 0.002046 | Perplexity: 1376.176883
2025-09-27 11:38:29,947 Stage: Train 0.5 | Epoch: 222 | Iter: 338000 | Total Loss: 0.005417 | Recon Loss: 0.004391 | Commit Loss: 0.002051 | Perplexity: 1376.847928
2025-09-27 11:39:17,099 Stage: Train 0.5 | Epoch: 222 | Iter: 338200 | Total Loss: 0.005386 | Recon Loss: 0.004368 | Commit Loss: 0.002037 | Perplexity: 1377.233322
2025-09-27 11:40:04,529 Stage: Train 0.5 | Epoch: 222 | Iter: 338400 | Total Loss: 0.005380 | Recon Loss: 0.004352 | Commit Loss: 0.002055 | Perplexity: 1378.166516
2025-09-27 11:40:51,841 Stage: Train 0.5 | Epoch: 222 | Iter: 338600 | Total Loss: 0.005402 | Recon Loss: 0.004378 | Commit Loss: 0.002050 | Perplexity: 1377.226206
Trainning Epoch:  68%|██████▊   | 223/330 [21:30:14<10:39:03, 358.35s/it]2025-09-27 11:41:39,469 Stage: Train 0.5 | Epoch: 223 | Iter: 338800 | Total Loss: 0.005330 | Recon Loss: 0.004308 | Commit Loss: 0.002044 | Perplexity: 1377.224841
2025-09-27 11:42:26,776 Stage: Train 0.5 | Epoch: 223 | Iter: 339000 | Total Loss: 0.005373 | Recon Loss: 0.004351 | Commit Loss: 0.002044 | Perplexity: 1373.963629
2025-09-27 11:43:14,177 Stage: Train 0.5 | Epoch: 223 | Iter: 339200 | Total Loss: 0.005355 | Recon Loss: 0.004336 | Commit Loss: 0.002038 | Perplexity: 1377.598347
2025-09-27 11:44:01,518 Stage: Train 0.5 | Epoch: 223 | Iter: 339400 | Total Loss: 0.005429 | Recon Loss: 0.004401 | Commit Loss: 0.002056 | Perplexity: 1378.590918
2025-09-27 11:44:48,556 Stage: Train 0.5 | Epoch: 223 | Iter: 339600 | Total Loss: 0.005363 | Recon Loss: 0.004344 | Commit Loss: 0.002040 | Perplexity: 1377.650984
2025-09-27 11:45:35,702 Stage: Train 0.5 | Epoch: 223 | Iter: 339800 | Total Loss: 0.005373 | Recon Loss: 0.004349 | Commit Loss: 0.002047 | Perplexity: 1376.727758
2025-09-27 11:46:22,853 Stage: Train 0.5 | Epoch: 223 | Iter: 340000 | Total Loss: 0.005354 | Recon Loss: 0.004330 | Commit Loss: 0.002048 | Perplexity: 1379.926166
2025-09-27 11:46:22,853 Saving model at iteration 340000
2025-09-27 11:46:23,339 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000
2025-09-27 11:46:23,701 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/model.safetensors
2025-09-27 11:46:24,150 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/optimizer.bin
2025-09-27 11:46:24,151 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/scheduler.bin
2025-09-27 11:46:24,151 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/sampler.bin
2025-09-27 11:46:24,152 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/random_states_0.pkl
2025-09-27 11:47:11,625 Stage: Train 0.5 | Epoch: 223 | Iter: 340200 | Total Loss: 0.005413 | Recon Loss: 0.004388 | Commit Loss: 0.002052 | Perplexity: 1378.324731
Trainning Epoch:  68%|██████▊   | 224/330 [21:36:15<10:34:17, 359.04s/it]2025-09-27 11:47:59,245 Stage: Train 0.5 | Epoch: 224 | Iter: 340400 | Total Loss: 0.005285 | Recon Loss: 0.004262 | Commit Loss: 0.002048 | Perplexity: 1378.970391
2025-09-27 11:48:46,414 Stage: Train 0.5 | Epoch: 224 | Iter: 340600 | Total Loss: 0.005389 | Recon Loss: 0.004363 | Commit Loss: 0.002052 | Perplexity: 1379.532571
2025-09-27 11:49:33,517 Stage: Train 0.5 | Epoch: 224 | Iter: 340800 | Total Loss: 0.005350 | Recon Loss: 0.004332 | Commit Loss: 0.002037 | Perplexity: 1375.726890
2025-09-27 11:50:20,779 Stage: Train 0.5 | Epoch: 224 | Iter: 341000 | Total Loss: 0.005378 | Recon Loss: 0.004353 | Commit Loss: 0.002050 | Perplexity: 1378.046659
2025-09-27 11:51:08,013 Stage: Train 0.5 | Epoch: 224 | Iter: 341200 | Total Loss: 0.005344 | Recon Loss: 0.004320 | Commit Loss: 0.002048 | Perplexity: 1378.871318
2025-09-27 11:51:54,977 Stage: Train 0.5 | Epoch: 224 | Iter: 341400 | Total Loss: 0.005346 | Recon Loss: 0.004324 | Commit Loss: 0.002044 | Perplexity: 1379.405892
2025-09-27 11:52:42,122 Stage: Train 0.5 | Epoch: 224 | Iter: 341600 | Total Loss: 0.005400 | Recon Loss: 0.004376 | Commit Loss: 0.002049 | Perplexity: 1378.875582
Trainning Epoch:  68%|██████▊   | 225/330 [21:42:14<10:28:09, 358.94s/it]2025-09-27 11:53:29,810 Stage: Train 0.5 | Epoch: 225 | Iter: 341800 | Total Loss: 0.005327 | Recon Loss: 0.004304 | Commit Loss: 0.002047 | Perplexity: 1377.996467
2025-09-27 11:54:17,052 Stage: Train 0.5 | Epoch: 225 | Iter: 342000 | Total Loss: 0.005307 | Recon Loss: 0.004285 | Commit Loss: 0.002045 | Perplexity: 1378.016319
2025-09-27 11:55:04,382 Stage: Train 0.5 | Epoch: 225 | Iter: 342200 | Total Loss: 0.005360 | Recon Loss: 0.004341 | Commit Loss: 0.002036 | Perplexity: 1376.610364
2025-09-27 11:55:51,639 Stage: Train 0.5 | Epoch: 225 | Iter: 342400 | Total Loss: 0.005327 | Recon Loss: 0.004310 | Commit Loss: 0.002035 | Perplexity: 1374.443886
2025-09-27 11:56:38,865 Stage: Train 0.5 | Epoch: 225 | Iter: 342600 | Total Loss: 0.005387 | Recon Loss: 0.004362 | Commit Loss: 0.002051 | Perplexity: 1379.540682
2025-09-27 11:57:26,138 Stage: Train 0.5 | Epoch: 225 | Iter: 342800 | Total Loss: 0.005365 | Recon Loss: 0.004341 | Commit Loss: 0.002047 | Perplexity: 1379.195665
2025-09-27 11:58:13,428 Stage: Train 0.5 | Epoch: 225 | Iter: 343000 | Total Loss: 0.005332 | Recon Loss: 0.004304 | Commit Loss: 0.002056 | Perplexity: 1378.658191
2025-09-27 11:59:00,471 Stage: Train 0.5 | Epoch: 225 | Iter: 343200 | Total Loss: 0.005330 | Recon Loss: 0.004306 | Commit Loss: 0.002048 | Perplexity: 1379.216882
Trainning Epoch:  68%|██████▊   | 226/330 [21:48:13<10:22:13, 358.97s/it]2025-09-27 11:59:47,781 Stage: Train 0.5 | Epoch: 226 | Iter: 343400 | Total Loss: 0.005402 | Recon Loss: 0.004376 | Commit Loss: 0.002051 | Perplexity: 1375.194481
2025-09-27 12:00:34,933 Stage: Train 0.5 | Epoch: 226 | Iter: 343600 | Total Loss: 0.005336 | Recon Loss: 0.004317 | Commit Loss: 0.002037 | Perplexity: 1375.662761
2025-09-27 12:01:22,168 Stage: Train 0.5 | Epoch: 226 | Iter: 343800 | Total Loss: 0.005289 | Recon Loss: 0.004266 | Commit Loss: 0.002047 | Perplexity: 1378.827148
2025-09-27 12:02:09,380 Stage: Train 0.5 | Epoch: 226 | Iter: 344000 | Total Loss: 0.005412 | Recon Loss: 0.004386 | Commit Loss: 0.002052 | Perplexity: 1378.822507
2025-09-27 12:02:56,578 Stage: Train 0.5 | Epoch: 226 | Iter: 344200 | Total Loss: 0.005374 | Recon Loss: 0.004352 | Commit Loss: 0.002045 | Perplexity: 1379.045955
2025-09-27 12:03:43,724 Stage: Train 0.5 | Epoch: 226 | Iter: 344400 | Total Loss: 0.005303 | Recon Loss: 0.004280 | Commit Loss: 0.002045 | Perplexity: 1377.361656
2025-09-27 12:04:30,839 Stage: Train 0.5 | Epoch: 226 | Iter: 344600 | Total Loss: 0.005359 | Recon Loss: 0.004338 | Commit Loss: 0.002040 | Perplexity: 1374.018333
2025-09-27 12:05:17,972 Stage: Train 0.5 | Epoch: 226 | Iter: 344800 | Total Loss: 0.005359 | Recon Loss: 0.004333 | Commit Loss: 0.002052 | Perplexity: 1381.968607
Trainning Epoch:  69%|██████▉   | 227/330 [21:54:11<10:15:57, 358.81s/it]2025-09-27 12:06:05,137 Stage: Train 0.5 | Epoch: 227 | Iter: 345000 | Total Loss: 0.005303 | Recon Loss: 0.004282 | Commit Loss: 0.002042 | Perplexity: 1378.547161
2025-09-27 12:06:52,318 Stage: Train 0.5 | Epoch: 227 | Iter: 345200 | Total Loss: 0.005356 | Recon Loss: 0.004335 | Commit Loss: 0.002042 | Perplexity: 1377.152839
2025-09-27 12:07:39,492 Stage: Train 0.5 | Epoch: 227 | Iter: 345400 | Total Loss: 0.005355 | Recon Loss: 0.004335 | Commit Loss: 0.002040 | Perplexity: 1377.096724
2025-09-27 12:08:26,823 Stage: Train 0.5 | Epoch: 227 | Iter: 345600 | Total Loss: 0.005349 | Recon Loss: 0.004328 | Commit Loss: 0.002042 | Perplexity: 1381.860581
2025-09-27 12:09:14,082 Stage: Train 0.5 | Epoch: 227 | Iter: 345800 | Total Loss: 0.005359 | Recon Loss: 0.004336 | Commit Loss: 0.002046 | Perplexity: 1378.101581
2025-09-27 12:10:01,419 Stage: Train 0.5 | Epoch: 227 | Iter: 346000 | Total Loss: 0.005321 | Recon Loss: 0.004300 | Commit Loss: 0.002042 | Perplexity: 1378.822734
2025-09-27 12:10:48,684 Stage: Train 0.5 | Epoch: 227 | Iter: 346200 | Total Loss: 0.005340 | Recon Loss: 0.004319 | Commit Loss: 0.002043 | Perplexity: 1378.066508
Trainning Epoch:  69%|██████▉   | 228/330 [22:00:10<10:09:58, 358.81s/it]2025-09-27 12:11:36,227 Stage: Train 0.5 | Epoch: 228 | Iter: 346400 | Total Loss: 0.005325 | Recon Loss: 0.004305 | Commit Loss: 0.002040 | Perplexity: 1374.895042
2025-09-27 12:12:23,460 Stage: Train 0.5 | Epoch: 228 | Iter: 346600 | Total Loss: 0.005351 | Recon Loss: 0.004325 | Commit Loss: 0.002052 | Perplexity: 1379.068676
2025-09-27 12:13:10,326 Stage: Train 0.5 | Epoch: 228 | Iter: 346800 | Total Loss: 0.005377 | Recon Loss: 0.004349 | Commit Loss: 0.002055 | Perplexity: 1378.718391
2025-09-27 12:13:57,603 Stage: Train 0.5 | Epoch: 228 | Iter: 347000 | Total Loss: 0.005333 | Recon Loss: 0.004314 | Commit Loss: 0.002039 | Perplexity: 1379.607838
2025-09-27 12:14:44,960 Stage: Train 0.5 | Epoch: 228 | Iter: 347200 | Total Loss: 0.005328 | Recon Loss: 0.004304 | Commit Loss: 0.002049 | Perplexity: 1379.726340
2025-09-27 12:15:32,499 Stage: Train 0.5 | Epoch: 228 | Iter: 347400 | Total Loss: 0.005347 | Recon Loss: 0.004327 | Commit Loss: 0.002041 | Perplexity: 1379.068202
2025-09-27 12:16:19,755 Stage: Train 0.5 | Epoch: 228 | Iter: 347600 | Total Loss: 0.005307 | Recon Loss: 0.004287 | Commit Loss: 0.002040 | Perplexity: 1377.751023
2025-09-27 12:17:07,141 Stage: Train 0.5 | Epoch: 228 | Iter: 347800 | Total Loss: 0.005341 | Recon Loss: 0.004320 | Commit Loss: 0.002042 | Perplexity: 1376.995484
Trainning Epoch:  69%|██████▉   | 229/330 [22:06:09<10:04:14, 358.96s/it]2025-09-27 12:17:54,729 Stage: Train 0.5 | Epoch: 229 | Iter: 348000 | Total Loss: 0.005284 | Recon Loss: 0.004262 | Commit Loss: 0.002044 | Perplexity: 1377.745099
2025-09-27 12:18:42,054 Stage: Train 0.5 | Epoch: 229 | Iter: 348200 | Total Loss: 0.005346 | Recon Loss: 0.004320 | Commit Loss: 0.002052 | Perplexity: 1380.610648
2025-09-27 12:19:29,520 Stage: Train 0.5 | Epoch: 229 | Iter: 348400 | Total Loss: 0.005334 | Recon Loss: 0.004313 | Commit Loss: 0.002041 | Perplexity: 1376.999302
2025-09-27 12:20:16,876 Stage: Train 0.5 | Epoch: 229 | Iter: 348600 | Total Loss: 0.005338 | Recon Loss: 0.004316 | Commit Loss: 0.002044 | Perplexity: 1380.880042
2025-09-27 12:21:04,162 Stage: Train 0.5 | Epoch: 229 | Iter: 348800 | Total Loss: 0.005318 | Recon Loss: 0.004297 | Commit Loss: 0.002041 | Perplexity: 1380.182286
2025-09-27 12:21:51,530 Stage: Train 0.5 | Epoch: 229 | Iter: 349000 | Total Loss: 0.005333 | Recon Loss: 0.004317 | Commit Loss: 0.002032 | Perplexity: 1377.422052
2025-09-27 12:22:38,893 Stage: Train 0.5 | Epoch: 229 | Iter: 349200 | Total Loss: 0.005325 | Recon Loss: 0.004306 | Commit Loss: 0.002040 | Perplexity: 1376.344509
Trainning Epoch:  70%|██████▉   | 230/330 [22:12:09<9:58:44, 359.25s/it] 2025-09-27 12:23:26,345 Stage: Train 0.5 | Epoch: 230 | Iter: 349400 | Total Loss: 0.005377 | Recon Loss: 0.004355 | Commit Loss: 0.002043 | Perplexity: 1379.830845
2025-09-27 12:24:13,543 Stage: Train 0.5 | Epoch: 230 | Iter: 349600 | Total Loss: 0.005303 | Recon Loss: 0.004284 | Commit Loss: 0.002037 | Perplexity: 1378.320185
2025-09-27 12:25:00,791 Stage: Train 0.5 | Epoch: 230 | Iter: 349800 | Total Loss: 0.005300 | Recon Loss: 0.004285 | Commit Loss: 0.002031 | Perplexity: 1376.105186
2025-09-27 12:25:47,963 Stage: Train 0.5 | Epoch: 230 | Iter: 350000 | Total Loss: 0.005370 | Recon Loss: 0.004350 | Commit Loss: 0.002038 | Perplexity: 1379.598516
2025-09-27 12:26:35,348 Stage: Train 0.5 | Epoch: 230 | Iter: 350200 | Total Loss: 0.005353 | Recon Loss: 0.004333 | Commit Loss: 0.002040 | Perplexity: 1376.661954
2025-09-27 12:27:22,411 Stage: Train 0.5 | Epoch: 230 | Iter: 350400 | Total Loss: 0.005308 | Recon Loss: 0.004288 | Commit Loss: 0.002040 | Perplexity: 1378.700427
2025-09-27 12:28:09,365 Stage: Train 0.5 | Epoch: 230 | Iter: 350600 | Total Loss: 0.005309 | Recon Loss: 0.004290 | Commit Loss: 0.002040 | Perplexity: 1376.882459
2025-09-27 12:28:56,641 Stage: Train 0.5 | Epoch: 230 | Iter: 350800 | Total Loss: 0.005400 | Recon Loss: 0.004375 | Commit Loss: 0.002049 | Perplexity: 1380.122501
Trainning Epoch:  70%|███████   | 231/330 [22:18:08<9:52:22, 359.01s/it]2025-09-27 12:29:44,125 Stage: Train 0.5 | Epoch: 231 | Iter: 351000 | Total Loss: 0.005315 | Recon Loss: 0.004296 | Commit Loss: 0.002038 | Perplexity: 1376.820338
2025-09-27 12:30:31,448 Stage: Train 0.5 | Epoch: 231 | Iter: 351200 | Total Loss: 0.005344 | Recon Loss: 0.004327 | Commit Loss: 0.002035 | Perplexity: 1377.250263
2025-09-27 12:31:18,751 Stage: Train 0.5 | Epoch: 231 | Iter: 351400 | Total Loss: 0.005287 | Recon Loss: 0.004272 | Commit Loss: 0.002031 | Perplexity: 1378.733134
2025-09-27 12:32:06,080 Stage: Train 0.5 | Epoch: 231 | Iter: 351600 | Total Loss: 0.005391 | Recon Loss: 0.004369 | Commit Loss: 0.002042 | Perplexity: 1379.517704
2025-09-27 12:32:53,498 Stage: Train 0.5 | Epoch: 231 | Iter: 351800 | Total Loss: 0.005357 | Recon Loss: 0.004338 | Commit Loss: 0.002038 | Perplexity: 1381.148262
2025-09-27 12:33:40,788 Stage: Train 0.5 | Epoch: 231 | Iter: 352000 | Total Loss: 0.005333 | Recon Loss: 0.004315 | Commit Loss: 0.002035 | Perplexity: 1377.044359
2025-09-27 12:34:27,991 Stage: Train 0.5 | Epoch: 231 | Iter: 352200 | Total Loss: 0.005388 | Recon Loss: 0.004366 | Commit Loss: 0.002043 | Perplexity: 1379.875594
2025-09-27 12:35:14,788 Stage: Train 0.5 | Epoch: 231 | Iter: 352400 | Total Loss: 0.005265 | Recon Loss: 0.004246 | Commit Loss: 0.002037 | Perplexity: 1376.939174
Trainning Epoch:  70%|███████   | 232/330 [22:24:07<9:46:26, 359.04s/it]2025-09-27 12:36:02,411 Stage: Train 0.5 | Epoch: 232 | Iter: 352600 | Total Loss: 0.005309 | Recon Loss: 0.004289 | Commit Loss: 0.002039 | Perplexity: 1380.301453
2025-09-27 12:36:49,552 Stage: Train 0.5 | Epoch: 232 | Iter: 352800 | Total Loss: 0.005319 | Recon Loss: 0.004299 | Commit Loss: 0.002039 | Perplexity: 1381.514908
2025-09-27 12:37:36,799 Stage: Train 0.5 | Epoch: 232 | Iter: 353000 | Total Loss: 0.005367 | Recon Loss: 0.004349 | Commit Loss: 0.002037 | Perplexity: 1379.657565
2025-09-27 12:38:24,118 Stage: Train 0.5 | Epoch: 232 | Iter: 353200 | Total Loss: 0.005301 | Recon Loss: 0.004284 | Commit Loss: 0.002035 | Perplexity: 1376.881736
2025-09-27 12:39:11,353 Stage: Train 0.5 | Epoch: 232 | Iter: 353400 | Total Loss: 0.005302 | Recon Loss: 0.004288 | Commit Loss: 0.002027 | Perplexity: 1378.815833
2025-09-27 12:39:58,676 Stage: Train 0.5 | Epoch: 232 | Iter: 353600 | Total Loss: 0.005322 | Recon Loss: 0.004305 | Commit Loss: 0.002033 | Perplexity: 1378.862263
2025-09-27 12:40:46,116 Stage: Train 0.5 | Epoch: 232 | Iter: 353800 | Total Loss: 0.005311 | Recon Loss: 0.004290 | Commit Loss: 0.002043 | Perplexity: 1379.664952
Trainning Epoch:  71%|███████   | 233/330 [22:30:06<9:40:41, 359.20s/it]2025-09-27 12:41:33,757 Stage: Train 0.5 | Epoch: 233 | Iter: 354000 | Total Loss: 0.005373 | Recon Loss: 0.004353 | Commit Loss: 0.002041 | Perplexity: 1377.288279
2025-09-27 12:42:20,859 Stage: Train 0.5 | Epoch: 233 | Iter: 354200 | Total Loss: 0.005305 | Recon Loss: 0.004280 | Commit Loss: 0.002049 | Perplexity: 1383.077277
2025-09-27 12:43:08,269 Stage: Train 0.5 | Epoch: 233 | Iter: 354400 | Total Loss: 0.005287 | Recon Loss: 0.004268 | Commit Loss: 0.002038 | Perplexity: 1378.181369
2025-09-27 12:43:55,626 Stage: Train 0.5 | Epoch: 233 | Iter: 354600 | Total Loss: 0.005338 | Recon Loss: 0.004321 | Commit Loss: 0.002034 | Perplexity: 1378.804048
2025-09-27 12:44:42,912 Stage: Train 0.5 | Epoch: 233 | Iter: 354800 | Total Loss: 0.005274 | Recon Loss: 0.004256 | Commit Loss: 0.002038 | Perplexity: 1379.889840
2025-09-27 12:45:30,239 Stage: Train 0.5 | Epoch: 233 | Iter: 355000 | Total Loss: 0.005324 | Recon Loss: 0.004308 | Commit Loss: 0.002033 | Perplexity: 1380.527472
2025-09-27 12:46:17,452 Stage: Train 0.5 | Epoch: 233 | Iter: 355200 | Total Loss: 0.005323 | Recon Loss: 0.004302 | Commit Loss: 0.002042 | Perplexity: 1379.447631
2025-09-27 12:47:04,727 Stage: Train 0.5 | Epoch: 233 | Iter: 355400 | Total Loss: 0.005284 | Recon Loss: 0.004263 | Commit Loss: 0.002041 | Perplexity: 1378.956018
Trainning Epoch:  71%|███████   | 234/330 [22:36:06<9:34:48, 359.25s/it]2025-09-27 12:47:52,371 Stage: Train 0.5 | Epoch: 234 | Iter: 355600 | Total Loss: 0.005332 | Recon Loss: 0.004315 | Commit Loss: 0.002036 | Perplexity: 1378.509676
2025-09-27 12:48:39,646 Stage: Train 0.5 | Epoch: 234 | Iter: 355800 | Total Loss: 0.005303 | Recon Loss: 0.004289 | Commit Loss: 0.002028 | Perplexity: 1377.784785
2025-09-27 12:49:26,576 Stage: Train 0.5 | Epoch: 234 | Iter: 356000 | Total Loss: 0.005274 | Recon Loss: 0.004253 | Commit Loss: 0.002042 | Perplexity: 1380.942170
2025-09-27 12:50:13,798 Stage: Train 0.5 | Epoch: 234 | Iter: 356200 | Total Loss: 0.005333 | Recon Loss: 0.004313 | Commit Loss: 0.002039 | Perplexity: 1380.654309
2025-09-27 12:51:01,130 Stage: Train 0.5 | Epoch: 234 | Iter: 356400 | Total Loss: 0.005303 | Recon Loss: 0.004285 | Commit Loss: 0.002036 | Perplexity: 1381.112173
2025-09-27 12:51:48,521 Stage: Train 0.5 | Epoch: 234 | Iter: 356600 | Total Loss: 0.005306 | Recon Loss: 0.004276 | Commit Loss: 0.002059 | Perplexity: 1380.587101
2025-09-27 12:52:35,907 Stage: Train 0.5 | Epoch: 234 | Iter: 356800 | Total Loss: 0.005343 | Recon Loss: 0.004318 | Commit Loss: 0.002049 | Perplexity: 1379.464163
Trainning Epoch:  71%|███████   | 235/330 [22:42:05<9:28:52, 359.29s/it]2025-09-27 12:53:23,505 Stage: Train 0.5 | Epoch: 235 | Iter: 357000 | Total Loss: 0.005281 | Recon Loss: 0.004260 | Commit Loss: 0.002042 | Perplexity: 1377.625703
2025-09-27 12:54:10,859 Stage: Train 0.5 | Epoch: 235 | Iter: 357200 | Total Loss: 0.005279 | Recon Loss: 0.004264 | Commit Loss: 0.002030 | Perplexity: 1378.723868
2025-09-27 12:54:58,140 Stage: Train 0.5 | Epoch: 235 | Iter: 357400 | Total Loss: 0.005301 | Recon Loss: 0.004281 | Commit Loss: 0.002041 | Perplexity: 1381.895919
2025-09-27 12:55:45,171 Stage: Train 0.5 | Epoch: 235 | Iter: 357600 | Total Loss: 0.005361 | Recon Loss: 0.004337 | Commit Loss: 0.002047 | Perplexity: 1380.256259
2025-09-27 12:56:32,065 Stage: Train 0.5 | Epoch: 235 | Iter: 357800 | Total Loss: 0.005329 | Recon Loss: 0.004307 | Commit Loss: 0.002044 | Perplexity: 1380.714684
2025-09-27 12:57:19,365 Stage: Train 0.5 | Epoch: 235 | Iter: 358000 | Total Loss: 0.005307 | Recon Loss: 0.004292 | Commit Loss: 0.002029 | Perplexity: 1379.496010
2025-09-27 12:58:06,448 Stage: Train 0.5 | Epoch: 235 | Iter: 358200 | Total Loss: 0.005293 | Recon Loss: 0.004274 | Commit Loss: 0.002037 | Perplexity: 1381.799439
2025-09-27 12:58:53,692 Stage: Train 0.5 | Epoch: 235 | Iter: 358400 | Total Loss: 0.005353 | Recon Loss: 0.004333 | Commit Loss: 0.002039 | Perplexity: 1379.915059
Trainning Epoch:  72%|███████▏  | 236/330 [22:48:04<9:22:30, 359.05s/it]2025-09-27 12:59:41,131 Stage: Train 0.5 | Epoch: 236 | Iter: 358600 | Total Loss: 0.005307 | Recon Loss: 0.004287 | Commit Loss: 0.002039 | Perplexity: 1379.755818
2025-09-27 13:00:28,266 Stage: Train 0.5 | Epoch: 236 | Iter: 358800 | Total Loss: 0.005297 | Recon Loss: 0.004284 | Commit Loss: 0.002026 | Perplexity: 1379.216984
2025-09-27 13:01:15,493 Stage: Train 0.5 | Epoch: 236 | Iter: 359000 | Total Loss: 0.005288 | Recon Loss: 0.004270 | Commit Loss: 0.002035 | Perplexity: 1380.377883
2025-09-27 13:02:02,575 Stage: Train 0.5 | Epoch: 236 | Iter: 359200 | Total Loss: 0.005328 | Recon Loss: 0.004311 | Commit Loss: 0.002034 | Perplexity: 1378.750823
2025-09-27 13:02:49,870 Stage: Train 0.5 | Epoch: 236 | Iter: 359400 | Total Loss: 0.005268 | Recon Loss: 0.004248 | Commit Loss: 0.002040 | Perplexity: 1382.944398
2025-09-27 13:03:37,159 Stage: Train 0.5 | Epoch: 236 | Iter: 359600 | Total Loss: 0.005284 | Recon Loss: 0.004263 | Commit Loss: 0.002040 | Perplexity: 1379.851882
2025-09-27 13:04:24,054 Stage: Train 0.5 | Epoch: 236 | Iter: 359800 | Total Loss: 0.005324 | Recon Loss: 0.004306 | Commit Loss: 0.002037 | Perplexity: 1379.365739
2025-09-27 13:05:11,188 Stage: Train 0.5 | Epoch: 236 | Iter: 360000 | Total Loss: 0.005274 | Recon Loss: 0.004257 | Commit Loss: 0.002033 | Perplexity: 1379.944185
2025-09-27 13:05:11,188 Saving model at iteration 360000
2025-09-27 13:05:11,420 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000
2025-09-27 13:05:11,713 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/model.safetensors
2025-09-27 13:05:12,080 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/optimizer.bin
2025-09-27 13:05:12,080 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/scheduler.bin
2025-09-27 13:05:12,080 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/sampler.bin
2025-09-27 13:05:12,081 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/random_states_0.pkl
Trainning Epoch:  72%|███████▏  | 237/330 [22:54:03<9:16:46, 359.21s/it]2025-09-27 13:06:00,044 Stage: Train 0.5 | Epoch: 237 | Iter: 360200 | Total Loss: 0.005330 | Recon Loss: 0.004312 | Commit Loss: 0.002035 | Perplexity: 1383.284685
2025-09-27 13:06:47,257 Stage: Train 0.5 | Epoch: 237 | Iter: 360400 | Total Loss: 0.005282 | Recon Loss: 0.004265 | Commit Loss: 0.002034 | Perplexity: 1380.276099
2025-09-27 13:07:34,555 Stage: Train 0.5 | Epoch: 237 | Iter: 360600 | Total Loss: 0.005289 | Recon Loss: 0.004273 | Commit Loss: 0.002033 | Perplexity: 1380.707964
2025-09-27 13:08:21,940 Stage: Train 0.5 | Epoch: 237 | Iter: 360800 | Total Loss: 0.005254 | Recon Loss: 0.004234 | Commit Loss: 0.002040 | Perplexity: 1378.384169
2025-09-27 13:09:07,572 Stage: Train 0.5 | Epoch: 237 | Iter: 361000 | Total Loss: 0.005321 | Recon Loss: 0.004300 | Commit Loss: 0.002042 | Perplexity: 1379.540538
2025-09-27 13:09:55,098 Stage: Train 0.5 | Epoch: 237 | Iter: 361200 | Total Loss: 0.005284 | Recon Loss: 0.004266 | Commit Loss: 0.002037 | Perplexity: 1379.664055
2025-09-27 13:10:42,544 Stage: Train 0.5 | Epoch: 237 | Iter: 361400 | Total Loss: 0.005281 | Recon Loss: 0.004261 | Commit Loss: 0.002040 | Perplexity: 1381.142771
Trainning Epoch:  72%|███████▏  | 238/330 [23:00:01<9:10:12, 358.84s/it]2025-09-27 13:11:29,788 Stage: Train 0.5 | Epoch: 238 | Iter: 361600 | Total Loss: 0.005312 | Recon Loss: 0.004298 | Commit Loss: 0.002028 | Perplexity: 1381.177391
2025-09-27 13:12:16,961 Stage: Train 0.5 | Epoch: 238 | Iter: 361800 | Total Loss: 0.005282 | Recon Loss: 0.004268 | Commit Loss: 0.002028 | Perplexity: 1379.006425
2025-09-27 13:13:04,215 Stage: Train 0.5 | Epoch: 238 | Iter: 362000 | Total Loss: 0.005295 | Recon Loss: 0.004271 | Commit Loss: 0.002048 | Perplexity: 1381.202812
2025-09-27 13:13:51,350 Stage: Train 0.5 | Epoch: 238 | Iter: 362200 | Total Loss: 0.005325 | Recon Loss: 0.004306 | Commit Loss: 0.002037 | Perplexity: 1380.934821
2025-09-27 13:14:38,518 Stage: Train 0.5 | Epoch: 238 | Iter: 362400 | Total Loss: 0.005276 | Recon Loss: 0.004259 | Commit Loss: 0.002035 | Perplexity: 1383.593300
2025-09-27 13:15:25,784 Stage: Train 0.5 | Epoch: 238 | Iter: 362600 | Total Loss: 0.005294 | Recon Loss: 0.004276 | Commit Loss: 0.002036 | Perplexity: 1380.236251
2025-09-27 13:16:12,914 Stage: Train 0.5 | Epoch: 238 | Iter: 362800 | Total Loss: 0.005279 | Recon Loss: 0.004262 | Commit Loss: 0.002033 | Perplexity: 1379.751296
2025-09-27 13:17:00,072 Stage: Train 0.5 | Epoch: 238 | Iter: 363000 | Total Loss: 0.005270 | Recon Loss: 0.004248 | Commit Loss: 0.002044 | Perplexity: 1382.696358
Trainning Epoch:  72%|███████▏  | 239/330 [23:06:00<9:04:10, 358.80s/it]2025-09-27 13:17:47,618 Stage: Train 0.5 | Epoch: 239 | Iter: 363200 | Total Loss: 0.005290 | Recon Loss: 0.004275 | Commit Loss: 0.002029 | Perplexity: 1380.850518
2025-09-27 13:18:34,454 Stage: Train 0.5 | Epoch: 239 | Iter: 363400 | Total Loss: 0.005254 | Recon Loss: 0.004240 | Commit Loss: 0.002029 | Perplexity: 1379.173319
2025-09-27 13:19:21,376 Stage: Train 0.5 | Epoch: 239 | Iter: 363600 | Total Loss: 0.005349 | Recon Loss: 0.004334 | Commit Loss: 0.002030 | Perplexity: 1380.845279
2025-09-27 13:20:08,522 Stage: Train 0.5 | Epoch: 239 | Iter: 363800 | Total Loss: 0.005281 | Recon Loss: 0.004261 | Commit Loss: 0.002040 | Perplexity: 1382.487639
2025-09-27 13:20:55,688 Stage: Train 0.5 | Epoch: 239 | Iter: 364000 | Total Loss: 0.005281 | Recon Loss: 0.004264 | Commit Loss: 0.002035 | Perplexity: 1380.010370
2025-09-27 13:21:42,950 Stage: Train 0.5 | Epoch: 239 | Iter: 364200 | Total Loss: 0.005286 | Recon Loss: 0.004269 | Commit Loss: 0.002034 | Perplexity: 1384.074984
2025-09-27 13:22:30,326 Stage: Train 0.5 | Epoch: 239 | Iter: 364400 | Total Loss: 0.005267 | Recon Loss: 0.004252 | Commit Loss: 0.002031 | Perplexity: 1382.497300
Trainning Epoch:  73%|███████▎  | 240/330 [23:11:58<8:58:01, 358.68s/it]2025-09-27 13:23:17,789 Stage: Train 0.5 | Epoch: 240 | Iter: 364600 | Total Loss: 0.005315 | Recon Loss: 0.004296 | Commit Loss: 0.002038 | Perplexity: 1381.980254
2025-09-27 13:24:05,129 Stage: Train 0.5 | Epoch: 240 | Iter: 364800 | Total Loss: 0.005244 | Recon Loss: 0.004230 | Commit Loss: 0.002029 | Perplexity: 1381.091744
2025-09-27 13:24:52,339 Stage: Train 0.5 | Epoch: 240 | Iter: 365000 | Total Loss: 0.005290 | Recon Loss: 0.004273 | Commit Loss: 0.002035 | Perplexity: 1381.969668
2025-09-27 13:25:39,163 Stage: Train 0.5 | Epoch: 240 | Iter: 365200 | Total Loss: 0.005322 | Recon Loss: 0.004305 | Commit Loss: 0.002032 | Perplexity: 1382.752230
2025-09-27 13:26:26,256 Stage: Train 0.5 | Epoch: 240 | Iter: 365400 | Total Loss: 0.005254 | Recon Loss: 0.004239 | Commit Loss: 0.002030 | Perplexity: 1379.564084
2025-09-27 13:27:13,441 Stage: Train 0.5 | Epoch: 240 | Iter: 365600 | Total Loss: 0.005279 | Recon Loss: 0.004257 | Commit Loss: 0.002043 | Perplexity: 1385.299057
2025-09-27 13:28:00,587 Stage: Train 0.5 | Epoch: 240 | Iter: 365800 | Total Loss: 0.005320 | Recon Loss: 0.004303 | Commit Loss: 0.002034 | Perplexity: 1383.598120
2025-09-27 13:28:47,892 Stage: Train 0.5 | Epoch: 240 | Iter: 366000 | Total Loss: 0.005277 | Recon Loss: 0.004262 | Commit Loss: 0.002030 | Perplexity: 1382.250576
Trainning Epoch:  73%|███████▎  | 241/330 [23:17:57<8:51:56, 358.61s/it]2025-09-27 13:29:35,333 Stage: Train 0.5 | Epoch: 241 | Iter: 366200 | Total Loss: 0.005267 | Recon Loss: 0.004248 | Commit Loss: 0.002038 | Perplexity: 1381.268630
2025-09-27 13:30:22,507 Stage: Train 0.5 | Epoch: 241 | Iter: 366400 | Total Loss: 0.005265 | Recon Loss: 0.004249 | Commit Loss: 0.002034 | Perplexity: 1381.273049
2025-09-27 13:31:09,719 Stage: Train 0.5 | Epoch: 241 | Iter: 366600 | Total Loss: 0.005281 | Recon Loss: 0.004265 | Commit Loss: 0.002032 | Perplexity: 1384.412935
2025-09-27 13:31:56,882 Stage: Train 0.5 | Epoch: 241 | Iter: 366800 | Total Loss: 0.005294 | Recon Loss: 0.004276 | Commit Loss: 0.002037 | Perplexity: 1383.105009
2025-09-27 13:32:43,736 Stage: Train 0.5 | Epoch: 241 | Iter: 367000 | Total Loss: 0.005254 | Recon Loss: 0.004237 | Commit Loss: 0.002034 | Perplexity: 1382.738338
2025-09-27 13:33:30,932 Stage: Train 0.5 | Epoch: 241 | Iter: 367200 | Total Loss: 0.005236 | Recon Loss: 0.004223 | Commit Loss: 0.002025 | Perplexity: 1384.428489
2025-09-27 13:34:18,220 Stage: Train 0.5 | Epoch: 241 | Iter: 367400 | Total Loss: 0.005305 | Recon Loss: 0.004286 | Commit Loss: 0.002038 | Perplexity: 1383.140241
Trainning Epoch:  73%|███████▎  | 242/330 [23:23:55<8:45:53, 358.56s/it]2025-09-27 13:35:05,780 Stage: Train 0.5 | Epoch: 242 | Iter: 367600 | Total Loss: 0.005258 | Recon Loss: 0.004240 | Commit Loss: 0.002036 | Perplexity: 1383.497272
2025-09-27 13:35:53,067 Stage: Train 0.5 | Epoch: 242 | Iter: 367800 | Total Loss: 0.005279 | Recon Loss: 0.004265 | Commit Loss: 0.002028 | Perplexity: 1381.082328
2025-09-27 13:36:40,444 Stage: Train 0.5 | Epoch: 242 | Iter: 368000 | Total Loss: 0.005304 | Recon Loss: 0.004287 | Commit Loss: 0.002035 | Perplexity: 1382.223586
2025-09-27 13:37:27,547 Stage: Train 0.5 | Epoch: 242 | Iter: 368200 | Total Loss: 0.005273 | Recon Loss: 0.004256 | Commit Loss: 0.002034 | Perplexity: 1383.466556
2025-09-27 13:38:14,565 Stage: Train 0.5 | Epoch: 242 | Iter: 368400 | Total Loss: 0.005334 | Recon Loss: 0.004324 | Commit Loss: 0.002020 | Perplexity: 1381.126912
2025-09-27 13:39:01,782 Stage: Train 0.5 | Epoch: 242 | Iter: 368600 | Total Loss: 0.005228 | Recon Loss: 0.004213 | Commit Loss: 0.002028 | Perplexity: 1383.986632
2025-09-27 13:39:48,990 Stage: Train 0.5 | Epoch: 242 | Iter: 368800 | Total Loss: 0.005257 | Recon Loss: 0.004240 | Commit Loss: 0.002033 | Perplexity: 1383.252673
2025-09-27 13:40:35,847 Stage: Train 0.5 | Epoch: 242 | Iter: 369000 | Total Loss: 0.005289 | Recon Loss: 0.004269 | Commit Loss: 0.002039 | Perplexity: 1384.683575
Trainning Epoch:  74%|███████▎  | 243/330 [23:29:54<8:39:50, 358.51s/it]2025-09-27 13:41:23,279 Stage: Train 0.5 | Epoch: 243 | Iter: 369200 | Total Loss: 0.005218 | Recon Loss: 0.004202 | Commit Loss: 0.002032 | Perplexity: 1381.958665
2025-09-27 13:42:10,401 Stage: Train 0.5 | Epoch: 243 | Iter: 369400 | Total Loss: 0.005272 | Recon Loss: 0.004255 | Commit Loss: 0.002034 | Perplexity: 1383.547852
2025-09-27 13:42:57,697 Stage: Train 0.5 | Epoch: 243 | Iter: 369600 | Total Loss: 0.005263 | Recon Loss: 0.004250 | Commit Loss: 0.002026 | Perplexity: 1383.143967
2025-09-27 13:43:44,938 Stage: Train 0.5 | Epoch: 243 | Iter: 369800 | Total Loss: 0.005248 | Recon Loss: 0.004235 | Commit Loss: 0.002026 | Perplexity: 1381.272233
2025-09-27 13:44:32,122 Stage: Train 0.5 | Epoch: 243 | Iter: 370000 | Total Loss: 0.005303 | Recon Loss: 0.004287 | Commit Loss: 0.002033 | Perplexity: 1383.169768
2025-09-27 13:45:19,067 Stage: Train 0.5 | Epoch: 243 | Iter: 370200 | Total Loss: 0.005281 | Recon Loss: 0.004262 | Commit Loss: 0.002038 | Perplexity: 1385.811390
2025-09-27 13:46:06,145 Stage: Train 0.5 | Epoch: 243 | Iter: 370400 | Total Loss: 0.005269 | Recon Loss: 0.004253 | Commit Loss: 0.002032 | Perplexity: 1382.329841
2025-09-27 13:46:53,216 Stage: Train 0.5 | Epoch: 243 | Iter: 370600 | Total Loss: 0.005221 | Recon Loss: 0.004204 | Commit Loss: 0.002034 | Perplexity: 1384.828262
Trainning Epoch:  74%|███████▍  | 244/330 [23:35:52<8:33:47, 358.46s/it]2025-09-27 13:47:40,490 Stage: Train 0.5 | Epoch: 244 | Iter: 370800 | Total Loss: 0.005224 | Recon Loss: 0.004215 | Commit Loss: 0.002018 | Perplexity: 1380.657064
2025-09-27 13:48:27,749 Stage: Train 0.5 | Epoch: 244 | Iter: 371000 | Total Loss: 0.005325 | Recon Loss: 0.004312 | Commit Loss: 0.002025 | Perplexity: 1381.711417
2025-09-27 13:49:14,782 Stage: Train 0.5 | Epoch: 244 | Iter: 371200 | Total Loss: 0.005254 | Recon Loss: 0.004244 | Commit Loss: 0.002021 | Perplexity: 1379.480313
2025-09-27 13:50:01,838 Stage: Train 0.5 | Epoch: 244 | Iter: 371400 | Total Loss: 0.005258 | Recon Loss: 0.004242 | Commit Loss: 0.002032 | Perplexity: 1383.927542
2025-09-27 13:50:48,961 Stage: Train 0.5 | Epoch: 244 | Iter: 371600 | Total Loss: 0.005231 | Recon Loss: 0.004218 | Commit Loss: 0.002026 | Perplexity: 1383.439074
2025-09-27 13:51:36,114 Stage: Train 0.5 | Epoch: 244 | Iter: 371800 | Total Loss: 0.005264 | Recon Loss: 0.004249 | Commit Loss: 0.002030 | Perplexity: 1384.679605
2025-09-27 13:52:23,248 Stage: Train 0.5 | Epoch: 244 | Iter: 372000 | Total Loss: 0.005271 | Recon Loss: 0.004252 | Commit Loss: 0.002038 | Perplexity: 1384.495352
Trainning Epoch:  74%|███████▍  | 245/330 [23:41:50<8:27:38, 358.34s/it]2025-09-27 13:53:10,686 Stage: Train 0.5 | Epoch: 245 | Iter: 372200 | Total Loss: 0.005244 | Recon Loss: 0.004231 | Commit Loss: 0.002027 | Perplexity: 1383.249574
2025-09-27 13:53:57,983 Stage: Train 0.5 | Epoch: 245 | Iter: 372400 | Total Loss: 0.005254 | Recon Loss: 0.004238 | Commit Loss: 0.002033 | Perplexity: 1385.905067
2025-09-27 13:54:44,871 Stage: Train 0.5 | Epoch: 245 | Iter: 372600 | Total Loss: 0.005268 | Recon Loss: 0.004257 | Commit Loss: 0.002023 | Perplexity: 1382.738112
2025-09-27 13:55:32,330 Stage: Train 0.5 | Epoch: 245 | Iter: 372800 | Total Loss: 0.005247 | Recon Loss: 0.004236 | Commit Loss: 0.002024 | Perplexity: 1381.745408
2025-09-27 13:56:19,532 Stage: Train 0.5 | Epoch: 245 | Iter: 373000 | Total Loss: 0.005246 | Recon Loss: 0.004232 | Commit Loss: 0.002028 | Perplexity: 1385.543538
2025-09-27 13:57:06,961 Stage: Train 0.5 | Epoch: 245 | Iter: 373200 | Total Loss: 0.005283 | Recon Loss: 0.004268 | Commit Loss: 0.002029 | Perplexity: 1383.023010
2025-09-27 13:57:54,225 Stage: Train 0.5 | Epoch: 245 | Iter: 373400 | Total Loss: 0.005233 | Recon Loss: 0.004220 | Commit Loss: 0.002025 | Perplexity: 1382.046555
2025-09-27 13:58:41,519 Stage: Train 0.5 | Epoch: 245 | Iter: 373600 | Total Loss: 0.005259 | Recon Loss: 0.004247 | Commit Loss: 0.002024 | Perplexity: 1381.928553
Trainning Epoch:  75%|███████▍  | 246/330 [23:47:49<8:22:01, 358.59s/it]2025-09-27 13:59:28,972 Stage: Train 0.5 | Epoch: 246 | Iter: 373800 | Total Loss: 0.005266 | Recon Loss: 0.004254 | Commit Loss: 0.002025 | Perplexity: 1383.080001
2025-09-27 14:00:16,468 Stage: Train 0.5 | Epoch: 246 | Iter: 374000 | Total Loss: 0.005270 | Recon Loss: 0.004257 | Commit Loss: 0.002026 | Perplexity: 1383.638405
2025-09-27 14:01:03,649 Stage: Train 0.5 | Epoch: 246 | Iter: 374200 | Total Loss: 0.005242 | Recon Loss: 0.004226 | Commit Loss: 0.002031 | Perplexity: 1387.588975
2025-09-27 14:01:50,310 Stage: Train 0.5 | Epoch: 246 | Iter: 374400 | Total Loss: 0.005269 | Recon Loss: 0.004254 | Commit Loss: 0.002029 | Perplexity: 1381.043363
2025-09-27 14:02:37,544 Stage: Train 0.5 | Epoch: 246 | Iter: 374600 | Total Loss: 0.005250 | Recon Loss: 0.004237 | Commit Loss: 0.002026 | Perplexity: 1382.077759
2025-09-27 14:03:24,760 Stage: Train 0.5 | Epoch: 246 | Iter: 374800 | Total Loss: 0.005294 | Recon Loss: 0.004280 | Commit Loss: 0.002027 | Perplexity: 1382.751663
2025-09-27 14:04:11,961 Stage: Train 0.5 | Epoch: 246 | Iter: 375000 | Total Loss: 0.005302 | Recon Loss: 0.004288 | Commit Loss: 0.002028 | Perplexity: 1384.169776
Trainning Epoch:  75%|███████▍  | 247/330 [23:53:48<8:15:54, 358.49s/it]2025-09-27 14:04:59,224 Stage: Train 0.5 | Epoch: 247 | Iter: 375200 | Total Loss: 0.005254 | Recon Loss: 0.004238 | Commit Loss: 0.002031 | Perplexity: 1382.326108
2025-09-27 14:05:46,323 Stage: Train 0.5 | Epoch: 247 | Iter: 375400 | Total Loss: 0.005242 | Recon Loss: 0.004230 | Commit Loss: 0.002024 | Perplexity: 1383.882513
2025-09-27 14:06:33,441 Stage: Train 0.5 | Epoch: 247 | Iter: 375600 | Total Loss: 0.005285 | Recon Loss: 0.004273 | Commit Loss: 0.002026 | Perplexity: 1382.721110
2025-09-27 14:07:20,510 Stage: Train 0.5 | Epoch: 247 | Iter: 375800 | Total Loss: 0.005237 | Recon Loss: 0.004225 | Commit Loss: 0.002024 | Perplexity: 1382.413970
2025-09-27 14:08:07,745 Stage: Train 0.5 | Epoch: 247 | Iter: 376000 | Total Loss: 0.005266 | Recon Loss: 0.004255 | Commit Loss: 0.002021 | Perplexity: 1380.245400
2025-09-27 14:08:54,723 Stage: Train 0.5 | Epoch: 247 | Iter: 376200 | Total Loss: 0.005223 | Recon Loss: 0.004211 | Commit Loss: 0.002024 | Perplexity: 1383.842839
2025-09-27 14:09:41,764 Stage: Train 0.5 | Epoch: 247 | Iter: 376400 | Total Loss: 0.005241 | Recon Loss: 0.004227 | Commit Loss: 0.002029 | Perplexity: 1382.756219
2025-09-27 14:10:29,013 Stage: Train 0.5 | Epoch: 247 | Iter: 376600 | Total Loss: 0.005313 | Recon Loss: 0.004296 | Commit Loss: 0.002034 | Perplexity: 1383.038535
Trainning Epoch:  75%|███████▌  | 248/330 [23:59:46<8:09:46, 358.37s/it]2025-09-27 14:11:16,418 Stage: Train 0.5 | Epoch: 248 | Iter: 376800 | Total Loss: 0.005253 | Recon Loss: 0.004241 | Commit Loss: 0.002024 | Perplexity: 1383.679883
2025-09-27 14:12:03,546 Stage: Train 0.5 | Epoch: 248 | Iter: 377000 | Total Loss: 0.005217 | Recon Loss: 0.004206 | Commit Loss: 0.002021 | Perplexity: 1384.202640
2025-09-27 14:12:50,746 Stage: Train 0.5 | Epoch: 248 | Iter: 377200 | Total Loss: 0.005299 | Recon Loss: 0.004287 | Commit Loss: 0.002024 | Perplexity: 1383.230245
2025-09-27 14:13:37,848 Stage: Train 0.5 | Epoch: 248 | Iter: 377400 | Total Loss: 0.005207 | Recon Loss: 0.004198 | Commit Loss: 0.002019 | Perplexity: 1381.917457
2025-09-27 14:14:25,329 Stage: Train 0.5 | Epoch: 248 | Iter: 377600 | Total Loss: 0.005307 | Recon Loss: 0.004293 | Commit Loss: 0.002028 | Perplexity: 1384.653649
2025-09-27 14:15:12,474 Stage: Train 0.5 | Epoch: 248 | Iter: 377800 | Total Loss: 0.005217 | Recon Loss: 0.004211 | Commit Loss: 0.002012 | Perplexity: 1381.960151
2025-09-27 14:15:59,662 Stage: Train 0.5 | Epoch: 248 | Iter: 378000 | Total Loss: 0.005272 | Recon Loss: 0.004257 | Commit Loss: 0.002029 | Perplexity: 1385.699548
2025-09-27 14:16:46,465 Stage: Train 0.5 | Epoch: 248 | Iter: 378200 | Total Loss: 0.005252 | Recon Loss: 0.004235 | Commit Loss: 0.002034 | Perplexity: 1385.403007
Trainning Epoch:  75%|███████▌  | 249/330 [24:05:44<8:03:48, 358.38s/it]2025-09-27 14:17:33,876 Stage: Train 0.5 | Epoch: 249 | Iter: 378400 | Total Loss: 0.005211 | Recon Loss: 0.004202 | Commit Loss: 0.002019 | Perplexity: 1381.066487
2025-09-27 14:18:21,014 Stage: Train 0.5 | Epoch: 249 | Iter: 378600 | Total Loss: 0.005263 | Recon Loss: 0.004253 | Commit Loss: 0.002021 | Perplexity: 1382.448183
2025-09-27 14:19:08,202 Stage: Train 0.5 | Epoch: 249 | Iter: 378800 | Total Loss: 0.005265 | Recon Loss: 0.004251 | Commit Loss: 0.002028 | Perplexity: 1384.789996
2025-09-27 14:19:55,474 Stage: Train 0.5 | Epoch: 249 | Iter: 379000 | Total Loss: 0.005283 | Recon Loss: 0.004267 | Commit Loss: 0.002032 | Perplexity: 1385.824228
2025-09-27 14:20:42,802 Stage: Train 0.5 | Epoch: 249 | Iter: 379200 | Total Loss: 0.005190 | Recon Loss: 0.004178 | Commit Loss: 0.002024 | Perplexity: 1385.317693
2025-09-27 14:21:30,095 Stage: Train 0.5 | Epoch: 249 | Iter: 379400 | Total Loss: 0.005250 | Recon Loss: 0.004237 | Commit Loss: 0.002026 | Perplexity: 1385.438182
2025-09-27 14:22:17,202 Stage: Train 0.5 | Epoch: 249 | Iter: 379600 | Total Loss: 0.005233 | Recon Loss: 0.004222 | Commit Loss: 0.002023 | Perplexity: 1384.016456
Trainning Epoch:  76%|███████▌  | 250/330 [24:11:43<7:57:59, 358.49s/it]2025-09-27 14:23:04,577 Stage: Train 0.5 | Epoch: 250 | Iter: 379800 | Total Loss: 0.005245 | Recon Loss: 0.004231 | Commit Loss: 0.002028 | Perplexity: 1383.703323
2025-09-27 14:23:51,346 Stage: Train 0.5 | Epoch: 250 | Iter: 380000 | Total Loss: 0.005206 | Recon Loss: 0.004194 | Commit Loss: 0.002024 | Perplexity: 1383.147955
2025-09-27 14:23:51,346 Saving model at iteration 380000
2025-09-27 14:23:51,706 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000
2025-09-27 14:23:51,993 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/model.safetensors
2025-09-27 14:23:52,392 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/optimizer.bin
2025-09-27 14:23:52,392 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/scheduler.bin
2025-09-27 14:23:52,392 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/sampler.bin
2025-09-27 14:23:52,393 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/random_states_0.pkl
2025-09-27 14:24:40,113 Stage: Train 0.5 | Epoch: 250 | Iter: 380200 | Total Loss: 0.005224 | Recon Loss: 0.004214 | Commit Loss: 0.002019 | Perplexity: 1382.684529
2025-09-27 14:25:27,244 Stage: Train 0.5 | Epoch: 250 | Iter: 380400 | Total Loss: 0.005264 | Recon Loss: 0.004251 | Commit Loss: 0.002027 | Perplexity: 1384.661075
2025-09-27 14:26:14,347 Stage: Train 0.5 | Epoch: 250 | Iter: 380600 | Total Loss: 0.005210 | Recon Loss: 0.004195 | Commit Loss: 0.002029 | Perplexity: 1386.027561
2025-09-27 14:27:01,641 Stage: Train 0.5 | Epoch: 250 | Iter: 380800 | Total Loss: 0.005252 | Recon Loss: 0.004236 | Commit Loss: 0.002032 | Perplexity: 1383.344688
2025-09-27 14:27:48,974 Stage: Train 0.5 | Epoch: 250 | Iter: 381000 | Total Loss: 0.005220 | Recon Loss: 0.004213 | Commit Loss: 0.002015 | Perplexity: 1384.487626
2025-09-27 14:28:36,302 Stage: Train 0.5 | Epoch: 250 | Iter: 381200 | Total Loss: 0.005230 | Recon Loss: 0.004219 | Commit Loss: 0.002022 | Perplexity: 1381.137377
Trainning Epoch:  76%|███████▌  | 251/330 [24:17:43<7:52:38, 358.97s/it]2025-09-27 14:29:23,811 Stage: Train 0.5 | Epoch: 251 | Iter: 381400 | Total Loss: 0.005242 | Recon Loss: 0.004229 | Commit Loss: 0.002025 | Perplexity: 1380.792396
2025-09-27 14:30:10,975 Stage: Train 0.5 | Epoch: 251 | Iter: 381600 | Total Loss: 0.005245 | Recon Loss: 0.004235 | Commit Loss: 0.002019 | Perplexity: 1383.990997
2025-09-27 14:30:57,881 Stage: Train 0.5 | Epoch: 251 | Iter: 381800 | Total Loss: 0.005232 | Recon Loss: 0.004221 | Commit Loss: 0.002022 | Perplexity: 1384.507986
2025-09-27 14:31:45,023 Stage: Train 0.5 | Epoch: 251 | Iter: 382000 | Total Loss: 0.005220 | Recon Loss: 0.004211 | Commit Loss: 0.002018 | Perplexity: 1382.376342
2025-09-27 14:32:32,259 Stage: Train 0.5 | Epoch: 251 | Iter: 382200 | Total Loss: 0.005237 | Recon Loss: 0.004225 | Commit Loss: 0.002024 | Perplexity: 1384.363746
2025-09-27 14:33:19,649 Stage: Train 0.5 | Epoch: 251 | Iter: 382400 | Total Loss: 0.005248 | Recon Loss: 0.004234 | Commit Loss: 0.002028 | Perplexity: 1385.582739
2025-09-27 14:34:06,922 Stage: Train 0.5 | Epoch: 251 | Iter: 382600 | Total Loss: 0.005239 | Recon Loss: 0.004230 | Commit Loss: 0.002017 | Perplexity: 1382.296036
Trainning Epoch:  76%|███████▋  | 252/330 [24:23:42<7:46:33, 358.89s/it]2025-09-27 14:34:54,423 Stage: Train 0.5 | Epoch: 252 | Iter: 382800 | Total Loss: 0.005257 | Recon Loss: 0.004243 | Commit Loss: 0.002027 | Perplexity: 1385.260054
2025-09-27 14:35:41,727 Stage: Train 0.5 | Epoch: 252 | Iter: 383000 | Total Loss: 0.005197 | Recon Loss: 0.004188 | Commit Loss: 0.002019 | Perplexity: 1383.878227
2025-09-27 14:36:29,085 Stage: Train 0.5 | Epoch: 252 | Iter: 383200 | Total Loss: 0.005249 | Recon Loss: 0.004236 | Commit Loss: 0.002025 | Perplexity: 1383.044352
2025-09-27 14:37:16,441 Stage: Train 0.5 | Epoch: 252 | Iter: 383400 | Total Loss: 0.005174 | Recon Loss: 0.004166 | Commit Loss: 0.002015 | Perplexity: 1381.962410
2025-09-27 14:38:03,489 Stage: Train 0.5 | Epoch: 252 | Iter: 383600 | Total Loss: 0.005287 | Recon Loss: 0.004275 | Commit Loss: 0.002024 | Perplexity: 1386.024981
2025-09-27 14:38:50,840 Stage: Train 0.5 | Epoch: 252 | Iter: 383800 | Total Loss: 0.005194 | Recon Loss: 0.004186 | Commit Loss: 0.002014 | Perplexity: 1385.126745
2025-09-27 14:39:38,208 Stage: Train 0.5 | Epoch: 252 | Iter: 384000 | Total Loss: 0.005240 | Recon Loss: 0.004226 | Commit Loss: 0.002027 | Perplexity: 1384.924849
2025-09-27 14:40:25,491 Stage: Train 0.5 | Epoch: 252 | Iter: 384200 | Total Loss: 0.005218 | Recon Loss: 0.004203 | Commit Loss: 0.002030 | Perplexity: 1386.332008
Trainning Epoch:  77%|███████▋  | 253/330 [24:29:41<7:40:49, 359.08s/it]2025-09-27 14:41:13,168 Stage: Train 0.5 | Epoch: 253 | Iter: 384400 | Total Loss: 0.005264 | Recon Loss: 0.004253 | Commit Loss: 0.002024 | Perplexity: 1384.713276
2025-09-27 14:42:00,507 Stage: Train 0.5 | Epoch: 253 | Iter: 384600 | Total Loss: 0.005206 | Recon Loss: 0.004196 | Commit Loss: 0.002020 | Perplexity: 1383.268474
2025-09-27 14:42:47,744 Stage: Train 0.5 | Epoch: 253 | Iter: 384800 | Total Loss: 0.005226 | Recon Loss: 0.004220 | Commit Loss: 0.002012 | Perplexity: 1385.581709
2025-09-27 14:43:34,909 Stage: Train 0.5 | Epoch: 253 | Iter: 385000 | Total Loss: 0.005221 | Recon Loss: 0.004212 | Commit Loss: 0.002018 | Perplexity: 1383.743066
2025-09-27 14:44:20,182 Stage: Train 0.5 | Epoch: 253 | Iter: 385200 | Total Loss: 0.005192 | Recon Loss: 0.004186 | Commit Loss: 0.002011 | Perplexity: 1381.843491
2025-09-27 14:45:07,105 Stage: Train 0.5 | Epoch: 253 | Iter: 385400 | Total Loss: 0.005259 | Recon Loss: 0.004245 | Commit Loss: 0.002028 | Perplexity: 1388.188842
2025-09-27 14:45:54,326 Stage: Train 0.5 | Epoch: 253 | Iter: 385600 | Total Loss: 0.005243 | Recon Loss: 0.004236 | Commit Loss: 0.002015 | Perplexity: 1383.604949
2025-09-27 14:46:41,602 Stage: Train 0.5 | Epoch: 253 | Iter: 385800 | Total Loss: 0.005244 | Recon Loss: 0.004229 | Commit Loss: 0.002030 | Perplexity: 1386.227742
Trainning Epoch:  77%|███████▋  | 254/330 [24:35:38<7:34:01, 358.44s/it]2025-09-27 14:47:29,193 Stage: Train 0.5 | Epoch: 254 | Iter: 386000 | Total Loss: 0.005235 | Recon Loss: 0.004228 | Commit Loss: 0.002014 | Perplexity: 1384.802821
2025-09-27 14:48:16,481 Stage: Train 0.5 | Epoch: 254 | Iter: 386200 | Total Loss: 0.005186 | Recon Loss: 0.004179 | Commit Loss: 0.002013 | Perplexity: 1382.021807
2025-09-27 14:49:03,808 Stage: Train 0.5 | Epoch: 254 | Iter: 386400 | Total Loss: 0.005212 | Recon Loss: 0.004202 | Commit Loss: 0.002020 | Perplexity: 1384.754970
2025-09-27 14:49:51,113 Stage: Train 0.5 | Epoch: 254 | Iter: 386600 | Total Loss: 0.005201 | Recon Loss: 0.004193 | Commit Loss: 0.002015 | Perplexity: 1383.647870
2025-09-27 14:50:38,334 Stage: Train 0.5 | Epoch: 254 | Iter: 386800 | Total Loss: 0.005244 | Recon Loss: 0.004231 | Commit Loss: 0.002026 | Perplexity: 1383.712054
2025-09-27 14:51:25,637 Stage: Train 0.5 | Epoch: 254 | Iter: 387000 | Total Loss: 0.005211 | Recon Loss: 0.004203 | Commit Loss: 0.002015 | Perplexity: 1385.327032
2025-09-27 14:52:12,745 Stage: Train 0.5 | Epoch: 254 | Iter: 387200 | Total Loss: 0.005273 | Recon Loss: 0.004259 | Commit Loss: 0.002027 | Perplexity: 1385.948001
Trainning Epoch:  77%|███████▋  | 255/330 [24:41:37<7:28:21, 358.69s/it]2025-09-27 14:53:00,298 Stage: Train 0.5 | Epoch: 255 | Iter: 387400 | Total Loss: 0.005249 | Recon Loss: 0.004236 | Commit Loss: 0.002027 | Perplexity: 1383.627913
2025-09-27 14:53:47,544 Stage: Train 0.5 | Epoch: 255 | Iter: 387600 | Total Loss: 0.005232 | Recon Loss: 0.004221 | Commit Loss: 0.002021 | Perplexity: 1386.471334
2025-09-27 14:54:34,893 Stage: Train 0.5 | Epoch: 255 | Iter: 387800 | Total Loss: 0.005215 | Recon Loss: 0.004205 | Commit Loss: 0.002019 | Perplexity: 1384.144870
2025-09-27 14:55:22,442 Stage: Train 0.5 | Epoch: 255 | Iter: 388000 | Total Loss: 0.005226 | Recon Loss: 0.004215 | Commit Loss: 0.002023 | Perplexity: 1387.234015
2025-09-27 14:56:09,768 Stage: Train 0.5 | Epoch: 255 | Iter: 388200 | Total Loss: 0.005192 | Recon Loss: 0.004186 | Commit Loss: 0.002012 | Perplexity: 1385.877157
2025-09-27 14:56:57,083 Stage: Train 0.5 | Epoch: 255 | Iter: 388400 | Total Loss: 0.005198 | Recon Loss: 0.004195 | Commit Loss: 0.002006 | Perplexity: 1381.781426
2025-09-27 14:57:44,154 Stage: Train 0.5 | Epoch: 255 | Iter: 388600 | Total Loss: 0.005216 | Recon Loss: 0.004203 | Commit Loss: 0.002026 | Perplexity: 1386.092318
2025-09-27 14:58:31,382 Stage: Train 0.5 | Epoch: 255 | Iter: 388800 | Total Loss: 0.005215 | Recon Loss: 0.004206 | Commit Loss: 0.002018 | Perplexity: 1386.288927
Trainning Epoch:  78%|███████▊  | 256/330 [24:47:37<7:22:39, 358.92s/it]2025-09-27 14:59:18,917 Stage: Train 0.5 | Epoch: 256 | Iter: 389000 | Total Loss: 0.005210 | Recon Loss: 0.004204 | Commit Loss: 0.002013 | Perplexity: 1385.997065
2025-09-27 15:00:06,146 Stage: Train 0.5 | Epoch: 256 | Iter: 389200 | Total Loss: 0.005196 | Recon Loss: 0.004184 | Commit Loss: 0.002023 | Perplexity: 1384.872589
2025-09-27 15:00:53,426 Stage: Train 0.5 | Epoch: 256 | Iter: 389400 | Total Loss: 0.005237 | Recon Loss: 0.004227 | Commit Loss: 0.002020 | Perplexity: 1384.884905
2025-09-27 15:01:40,731 Stage: Train 0.5 | Epoch: 256 | Iter: 389600 | Total Loss: 0.005220 | Recon Loss: 0.004215 | Commit Loss: 0.002011 | Perplexity: 1382.828215
2025-09-27 15:02:28,082 Stage: Train 0.5 | Epoch: 256 | Iter: 389800 | Total Loss: 0.005195 | Recon Loss: 0.004187 | Commit Loss: 0.002015 | Perplexity: 1384.064432
2025-09-27 15:03:15,329 Stage: Train 0.5 | Epoch: 256 | Iter: 390000 | Total Loss: 0.005237 | Recon Loss: 0.004223 | Commit Loss: 0.002026 | Perplexity: 1385.682023
2025-09-27 15:04:02,678 Stage: Train 0.5 | Epoch: 256 | Iter: 390200 | Total Loss: 0.005188 | Recon Loss: 0.004177 | Commit Loss: 0.002021 | Perplexity: 1385.014615
Trainning Epoch:  78%|███████▊  | 257/330 [24:53:36<7:16:50, 359.04s/it]2025-09-27 15:04:50,119 Stage: Train 0.5 | Epoch: 257 | Iter: 390400 | Total Loss: 0.005234 | Recon Loss: 0.004224 | Commit Loss: 0.002019 | Perplexity: 1388.124844
2025-09-27 15:05:37,540 Stage: Train 0.5 | Epoch: 257 | Iter: 390600 | Total Loss: 0.005169 | Recon Loss: 0.004161 | Commit Loss: 0.002015 | Perplexity: 1386.715263
2025-09-27 15:06:24,882 Stage: Train 0.5 | Epoch: 257 | Iter: 390800 | Total Loss: 0.005198 | Recon Loss: 0.004193 | Commit Loss: 0.002010 | Perplexity: 1382.412749
2025-09-27 15:07:11,826 Stage: Train 0.5 | Epoch: 257 | Iter: 391000 | Total Loss: 0.005239 | Recon Loss: 0.004232 | Commit Loss: 0.002014 | Perplexity: 1385.473466
2025-09-27 15:07:59,016 Stage: Train 0.5 | Epoch: 257 | Iter: 391200 | Total Loss: 0.005228 | Recon Loss: 0.004219 | Commit Loss: 0.002017 | Perplexity: 1386.853140
2025-09-27 15:08:46,232 Stage: Train 0.5 | Epoch: 257 | Iter: 391400 | Total Loss: 0.005256 | Recon Loss: 0.004245 | Commit Loss: 0.002021 | Perplexity: 1384.407413
2025-09-27 15:09:33,403 Stage: Train 0.5 | Epoch: 257 | Iter: 391600 | Total Loss: 0.005206 | Recon Loss: 0.004196 | Commit Loss: 0.002020 | Perplexity: 1385.087360
2025-09-27 15:10:20,744 Stage: Train 0.5 | Epoch: 257 | Iter: 391800 | Total Loss: 0.005225 | Recon Loss: 0.004217 | Commit Loss: 0.002017 | Perplexity: 1384.193406
Trainning Epoch:  78%|███████▊  | 258/330 [24:59:35<7:10:49, 359.02s/it]2025-09-27 15:11:08,254 Stage: Train 0.5 | Epoch: 258 | Iter: 392000 | Total Loss: 0.005237 | Recon Loss: 0.004229 | Commit Loss: 0.002016 | Perplexity: 1384.739601
2025-09-27 15:11:55,586 Stage: Train 0.5 | Epoch: 258 | Iter: 392200 | Total Loss: 0.005229 | Recon Loss: 0.004221 | Commit Loss: 0.002017 | Perplexity: 1385.579294
2025-09-27 15:12:42,819 Stage: Train 0.5 | Epoch: 258 | Iter: 392400 | Total Loss: 0.005212 | Recon Loss: 0.004204 | Commit Loss: 0.002016 | Perplexity: 1386.438677
2025-09-27 15:13:30,180 Stage: Train 0.5 | Epoch: 258 | Iter: 392600 | Total Loss: 0.005192 | Recon Loss: 0.004188 | Commit Loss: 0.002007 | Perplexity: 1382.984152
2025-09-27 15:14:17,104 Stage: Train 0.5 | Epoch: 258 | Iter: 392800 | Total Loss: 0.005193 | Recon Loss: 0.004185 | Commit Loss: 0.002017 | Perplexity: 1386.192695
2025-09-27 15:15:04,570 Stage: Train 0.5 | Epoch: 258 | Iter: 393000 | Total Loss: 0.005196 | Recon Loss: 0.004190 | Commit Loss: 0.002012 | Perplexity: 1384.309277
2025-09-27 15:15:51,897 Stage: Train 0.5 | Epoch: 258 | Iter: 393200 | Total Loss: 0.005170 | Recon Loss: 0.004165 | Commit Loss: 0.002009 | Perplexity: 1385.631545
2025-09-27 15:16:39,117 Stage: Train 0.5 | Epoch: 258 | Iter: 393400 | Total Loss: 0.005237 | Recon Loss: 0.004223 | Commit Loss: 0.002029 | Perplexity: 1389.604395
Trainning Epoch:  78%|███████▊  | 259/330 [25:05:34<7:04:55, 359.09s/it]2025-09-27 15:17:26,464 Stage: Train 0.5 | Epoch: 259 | Iter: 393600 | Total Loss: 0.005151 | Recon Loss: 0.004149 | Commit Loss: 0.002005 | Perplexity: 1384.861006
2025-09-27 15:18:13,661 Stage: Train 0.5 | Epoch: 259 | Iter: 393800 | Total Loss: 0.005236 | Recon Loss: 0.004228 | Commit Loss: 0.002015 | Perplexity: 1385.734072
2025-09-27 15:19:00,840 Stage: Train 0.5 | Epoch: 259 | Iter: 394000 | Total Loss: 0.005169 | Recon Loss: 0.004160 | Commit Loss: 0.002018 | Perplexity: 1387.123389
2025-09-27 15:19:48,222 Stage: Train 0.5 | Epoch: 259 | Iter: 394200 | Total Loss: 0.005179 | Recon Loss: 0.004171 | Commit Loss: 0.002015 | Perplexity: 1384.517178
2025-09-27 15:20:35,373 Stage: Train 0.5 | Epoch: 259 | Iter: 394400 | Total Loss: 0.005174 | Recon Loss: 0.004169 | Commit Loss: 0.002009 | Perplexity: 1383.647686
2025-09-27 15:21:22,356 Stage: Train 0.5 | Epoch: 259 | Iter: 394600 | Total Loss: 0.005264 | Recon Loss: 0.004252 | Commit Loss: 0.002024 | Perplexity: 1386.613870
2025-09-27 15:22:09,652 Stage: Train 0.5 | Epoch: 259 | Iter: 394800 | Total Loss: 0.005245 | Recon Loss: 0.004236 | Commit Loss: 0.002018 | Perplexity: 1386.954573
Trainning Epoch:  79%|███████▉  | 260/330 [25:11:33<6:58:46, 358.95s/it]2025-09-27 15:22:57,114 Stage: Train 0.5 | Epoch: 260 | Iter: 395000 | Total Loss: 0.005220 | Recon Loss: 0.004207 | Commit Loss: 0.002026 | Perplexity: 1386.876795
2025-09-27 15:23:44,320 Stage: Train 0.5 | Epoch: 260 | Iter: 395200 | Total Loss: 0.005224 | Recon Loss: 0.004217 | Commit Loss: 0.002014 | Perplexity: 1386.374428
2025-09-27 15:24:31,616 Stage: Train 0.5 | Epoch: 260 | Iter: 395400 | Total Loss: 0.005190 | Recon Loss: 0.004184 | Commit Loss: 0.002012 | Perplexity: 1387.047781
2025-09-27 15:25:18,729 Stage: Train 0.5 | Epoch: 260 | Iter: 395600 | Total Loss: 0.005198 | Recon Loss: 0.004188 | Commit Loss: 0.002019 | Perplexity: 1386.484887
2025-09-27 15:26:05,683 Stage: Train 0.5 | Epoch: 260 | Iter: 395800 | Total Loss: 0.005183 | Recon Loss: 0.004173 | Commit Loss: 0.002019 | Perplexity: 1384.628104
2025-09-27 15:26:52,992 Stage: Train 0.5 | Epoch: 260 | Iter: 396000 | Total Loss: 0.005196 | Recon Loss: 0.004188 | Commit Loss: 0.002015 | Perplexity: 1384.148395
2025-09-27 15:27:40,206 Stage: Train 0.5 | Epoch: 260 | Iter: 396200 | Total Loss: 0.005182 | Recon Loss: 0.004174 | Commit Loss: 0.002015 | Perplexity: 1386.447613
2025-09-27 15:28:27,167 Stage: Train 0.5 | Epoch: 260 | Iter: 396400 | Total Loss: 0.005197 | Recon Loss: 0.004190 | Commit Loss: 0.002014 | Perplexity: 1389.147553
Trainning Epoch:  79%|███████▉  | 261/330 [25:17:31<6:52:36, 358.79s/it]2025-09-27 15:29:14,549 Stage: Train 0.5 | Epoch: 261 | Iter: 396600 | Total Loss: 0.005190 | Recon Loss: 0.004183 | Commit Loss: 0.002013 | Perplexity: 1386.847031
2025-09-27 15:30:01,740 Stage: Train 0.5 | Epoch: 261 | Iter: 396800 | Total Loss: 0.005190 | Recon Loss: 0.004186 | Commit Loss: 0.002007 | Perplexity: 1386.025159
2025-09-27 15:30:48,885 Stage: Train 0.5 | Epoch: 261 | Iter: 397000 | Total Loss: 0.005183 | Recon Loss: 0.004171 | Commit Loss: 0.002023 | Perplexity: 1390.777080
2025-09-27 15:31:36,286 Stage: Train 0.5 | Epoch: 261 | Iter: 397200 | Total Loss: 0.005196 | Recon Loss: 0.004191 | Commit Loss: 0.002011 | Perplexity: 1385.734844
2025-09-27 15:32:23,571 Stage: Train 0.5 | Epoch: 261 | Iter: 397400 | Total Loss: 0.005197 | Recon Loss: 0.004189 | Commit Loss: 0.002016 | Perplexity: 1386.215928
2025-09-27 15:33:10,802 Stage: Train 0.5 | Epoch: 261 | Iter: 397600 | Total Loss: 0.005234 | Recon Loss: 0.004225 | Commit Loss: 0.002018 | Perplexity: 1385.924960
2025-09-27 15:33:58,016 Stage: Train 0.5 | Epoch: 261 | Iter: 397800 | Total Loss: 0.005186 | Recon Loss: 0.004176 | Commit Loss: 0.002019 | Perplexity: 1387.446578
Trainning Epoch:  79%|███████▉  | 262/330 [25:23:30<6:46:39, 358.82s/it]2025-09-27 15:34:45,417 Stage: Train 0.5 | Epoch: 262 | Iter: 398000 | Total Loss: 0.005192 | Recon Loss: 0.004185 | Commit Loss: 0.002014 | Perplexity: 1385.422003
2025-09-27 15:35:32,607 Stage: Train 0.5 | Epoch: 262 | Iter: 398200 | Total Loss: 0.005198 | Recon Loss: 0.004193 | Commit Loss: 0.002009 | Perplexity: 1386.000233
2025-09-27 15:36:19,517 Stage: Train 0.5 | Epoch: 262 | Iter: 398400 | Total Loss: 0.005174 | Recon Loss: 0.004172 | Commit Loss: 0.002004 | Perplexity: 1383.884626
2025-09-27 15:37:06,472 Stage: Train 0.5 | Epoch: 262 | Iter: 398600 | Total Loss: 0.005157 | Recon Loss: 0.004150 | Commit Loss: 0.002012 | Perplexity: 1386.716708
2025-09-27 15:37:53,736 Stage: Train 0.5 | Epoch: 262 | Iter: 398800 | Total Loss: 0.005225 | Recon Loss: 0.004214 | Commit Loss: 0.002021 | Perplexity: 1388.004609
2025-09-27 15:38:40,948 Stage: Train 0.5 | Epoch: 262 | Iter: 399000 | Total Loss: 0.005212 | Recon Loss: 0.004205 | Commit Loss: 0.002015 | Perplexity: 1386.473718
2025-09-27 15:39:28,218 Stage: Train 0.5 | Epoch: 262 | Iter: 399200 | Total Loss: 0.005164 | Recon Loss: 0.004157 | Commit Loss: 0.002014 | Perplexity: 1387.284761
2025-09-27 15:40:15,552 Stage: Train 0.5 | Epoch: 262 | Iter: 399400 | Total Loss: 0.005169 | Recon Loss: 0.004166 | Commit Loss: 0.002007 | Perplexity: 1384.375010
Trainning Epoch:  80%|███████▉  | 263/330 [25:29:29<6:40:33, 358.72s/it]2025-09-27 15:41:03,166 Stage: Train 0.5 | Epoch: 263 | Iter: 399600 | Total Loss: 0.005209 | Recon Loss: 0.004201 | Commit Loss: 0.002015 | Perplexity: 1386.924411
2025-09-27 15:41:50,256 Stage: Train 0.5 | Epoch: 263 | Iter: 399800 | Total Loss: 0.005196 | Recon Loss: 0.004194 | Commit Loss: 0.002006 | Perplexity: 1385.516161
2025-09-27 15:42:37,484 Stage: Train 0.5 | Epoch: 263 | Iter: 400000 | Total Loss: 0.005135 | Recon Loss: 0.004133 | Commit Loss: 0.002003 | Perplexity: 1384.853814
2025-09-27 15:42:37,484 Saving model at iteration 400000
2025-09-27 15:42:37,858 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000
2025-09-27 15:42:38,171 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/model.safetensors
2025-09-27 15:42:38,604 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/optimizer.bin
2025-09-27 15:42:38,605 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/scheduler.bin
2025-09-27 15:42:38,605 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/sampler.bin
2025-09-27 15:42:38,606 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/random_states_0.pkl
2025-09-27 15:43:25,810 Stage: Train 0.5 | Epoch: 263 | Iter: 400200 | Total Loss: 0.005179 | Recon Loss: 0.004177 | Commit Loss: 0.002005 | Perplexity: 1386.570994
2025-09-27 15:44:12,697 Stage: Train 0.5 | Epoch: 263 | Iter: 400400 | Total Loss: 0.005217 | Recon Loss: 0.004211 | Commit Loss: 0.002012 | Perplexity: 1386.398766
2025-09-27 15:44:59,962 Stage: Train 0.5 | Epoch: 263 | Iter: 400600 | Total Loss: 0.005171 | Recon Loss: 0.004166 | Commit Loss: 0.002009 | Perplexity: 1384.417043
2025-09-27 15:45:47,186 Stage: Train 0.5 | Epoch: 263 | Iter: 400800 | Total Loss: 0.005216 | Recon Loss: 0.004211 | Commit Loss: 0.002009 | Perplexity: 1385.782369
2025-09-27 15:46:34,436 Stage: Train 0.5 | Epoch: 263 | Iter: 401000 | Total Loss: 0.005222 | Recon Loss: 0.004213 | Commit Loss: 0.002018 | Perplexity: 1386.433248
Trainning Epoch:  80%|████████  | 264/330 [25:35:28<6:34:54, 359.01s/it]2025-09-27 15:47:21,887 Stage: Train 0.5 | Epoch: 264 | Iter: 401200 | Total Loss: 0.005153 | Recon Loss: 0.004150 | Commit Loss: 0.002006 | Perplexity: 1387.703572
2025-09-27 15:48:09,017 Stage: Train 0.5 | Epoch: 264 | Iter: 401400 | Total Loss: 0.005153 | Recon Loss: 0.004148 | Commit Loss: 0.002010 | Perplexity: 1386.649294
2025-09-27 15:48:56,315 Stage: Train 0.5 | Epoch: 264 | Iter: 401600 | Total Loss: 0.005186 | Recon Loss: 0.004181 | Commit Loss: 0.002010 | Perplexity: 1385.861548
2025-09-27 15:49:43,619 Stage: Train 0.5 | Epoch: 264 | Iter: 401800 | Total Loss: 0.005192 | Recon Loss: 0.004185 | Commit Loss: 0.002015 | Perplexity: 1387.775874
2025-09-27 15:50:30,469 Stage: Train 0.5 | Epoch: 264 | Iter: 402000 | Total Loss: 0.005204 | Recon Loss: 0.004197 | Commit Loss: 0.002014 | Perplexity: 1387.903268
2025-09-27 15:51:17,672 Stage: Train 0.5 | Epoch: 264 | Iter: 402200 | Total Loss: 0.005179 | Recon Loss: 0.004174 | Commit Loss: 0.002010 | Perplexity: 1386.098113
2025-09-27 15:52:05,101 Stage: Train 0.5 | Epoch: 264 | Iter: 402400 | Total Loss: 0.005196 | Recon Loss: 0.004190 | Commit Loss: 0.002012 | Perplexity: 1385.880125
Trainning Epoch:  80%|████████  | 265/330 [25:41:27<6:28:51, 358.94s/it]2025-09-27 15:52:52,589 Stage: Train 0.5 | Epoch: 265 | Iter: 402600 | Total Loss: 0.005171 | Recon Loss: 0.004168 | Commit Loss: 0.002006 | Perplexity: 1382.686826
2025-09-27 15:53:39,859 Stage: Train 0.5 | Epoch: 265 | Iter: 402800 | Total Loss: 0.005158 | Recon Loss: 0.004153 | Commit Loss: 0.002009 | Perplexity: 1385.104070
2025-09-27 15:54:26,985 Stage: Train 0.5 | Epoch: 265 | Iter: 403000 | Total Loss: 0.005156 | Recon Loss: 0.004152 | Commit Loss: 0.002009 | Perplexity: 1385.735422
2025-09-27 15:55:14,186 Stage: Train 0.5 | Epoch: 265 | Iter: 403200 | Total Loss: 0.005207 | Recon Loss: 0.004207 | Commit Loss: 0.002001 | Perplexity: 1384.434023
2025-09-27 15:56:01,117 Stage: Train 0.5 | Epoch: 265 | Iter: 403400 | Total Loss: 0.005270 | Recon Loss: 0.004262 | Commit Loss: 0.002015 | Perplexity: 1387.751166
2025-09-27 15:56:48,423 Stage: Train 0.5 | Epoch: 265 | Iter: 403600 | Total Loss: 0.005110 | Recon Loss: 0.004108 | Commit Loss: 0.002004 | Perplexity: 1384.233655
2025-09-27 15:57:35,176 Stage: Train 0.5 | Epoch: 265 | Iter: 403800 | Total Loss: 0.005207 | Recon Loss: 0.004206 | Commit Loss: 0.002003 | Perplexity: 1384.113096
2025-09-27 15:58:22,488 Stage: Train 0.5 | Epoch: 265 | Iter: 404000 | Total Loss: 0.005258 | Recon Loss: 0.004249 | Commit Loss: 0.002018 | Perplexity: 1390.683190
Trainning Epoch:  81%|████████  | 266/330 [25:47:25<6:22:39, 358.74s/it]2025-09-27 15:59:09,988 Stage: Train 0.5 | Epoch: 266 | Iter: 404200 | Total Loss: 0.005163 | Recon Loss: 0.004161 | Commit Loss: 0.002004 | Perplexity: 1384.482341
2025-09-27 15:59:57,079 Stage: Train 0.5 | Epoch: 266 | Iter: 404400 | Total Loss: 0.005122 | Recon Loss: 0.004124 | Commit Loss: 0.001997 | Perplexity: 1385.078821
2025-09-27 16:00:44,243 Stage: Train 0.5 | Epoch: 266 | Iter: 404600 | Total Loss: 0.005149 | Recon Loss: 0.004148 | Commit Loss: 0.002002 | Perplexity: 1387.030577
2025-09-27 16:01:31,516 Stage: Train 0.5 | Epoch: 266 | Iter: 404800 | Total Loss: 0.005140 | Recon Loss: 0.004132 | Commit Loss: 0.002015 | Perplexity: 1386.195991
2025-09-27 16:02:18,555 Stage: Train 0.5 | Epoch: 266 | Iter: 405000 | Total Loss: 0.005206 | Recon Loss: 0.004202 | Commit Loss: 0.002009 | Perplexity: 1386.568883
2025-09-27 16:03:06,001 Stage: Train 0.5 | Epoch: 266 | Iter: 405200 | Total Loss: 0.005151 | Recon Loss: 0.004148 | Commit Loss: 0.002005 | Perplexity: 1388.518898
2025-09-27 16:03:53,263 Stage: Train 0.5 | Epoch: 266 | Iter: 405400 | Total Loss: 0.005212 | Recon Loss: 0.004204 | Commit Loss: 0.002015 | Perplexity: 1385.226818
Trainning Epoch:  81%|████████  | 267/330 [25:53:24<6:16:39, 358.72s/it]2025-09-27 16:04:40,508 Stage: Train 0.5 | Epoch: 267 | Iter: 405600 | Total Loss: 0.005201 | Recon Loss: 0.004194 | Commit Loss: 0.002014 | Perplexity: 1386.293935
2025-09-27 16:05:27,731 Stage: Train 0.5 | Epoch: 267 | Iter: 405800 | Total Loss: 0.005146 | Recon Loss: 0.004144 | Commit Loss: 0.002003 | Perplexity: 1383.862540
2025-09-27 16:06:14,811 Stage: Train 0.5 | Epoch: 267 | Iter: 406000 | Total Loss: 0.005150 | Recon Loss: 0.004147 | Commit Loss: 0.002005 | Perplexity: 1387.674119
2025-09-27 16:07:01,817 Stage: Train 0.5 | Epoch: 267 | Iter: 406200 | Total Loss: 0.005174 | Recon Loss: 0.004170 | Commit Loss: 0.002009 | Perplexity: 1389.228664
2025-09-27 16:07:48,963 Stage: Train 0.5 | Epoch: 267 | Iter: 406400 | Total Loss: 0.005174 | Recon Loss: 0.004167 | Commit Loss: 0.002014 | Perplexity: 1388.319425
2025-09-27 16:08:36,079 Stage: Train 0.5 | Epoch: 267 | Iter: 406600 | Total Loss: 0.005159 | Recon Loss: 0.004152 | Commit Loss: 0.002014 | Perplexity: 1386.601947
2025-09-27 16:09:23,218 Stage: Train 0.5 | Epoch: 267 | Iter: 406800 | Total Loss: 0.005145 | Recon Loss: 0.004142 | Commit Loss: 0.002005 | Perplexity: 1386.736403
2025-09-27 16:10:10,449 Stage: Train 0.5 | Epoch: 267 | Iter: 407000 | Total Loss: 0.005178 | Recon Loss: 0.004175 | Commit Loss: 0.002006 | Perplexity: 1386.885556
Trainning Epoch:  81%|████████  | 268/330 [25:59:22<6:10:32, 358.59s/it]2025-09-27 16:10:57,927 Stage: Train 0.5 | Epoch: 268 | Iter: 407200 | Total Loss: 0.005182 | Recon Loss: 0.004180 | Commit Loss: 0.002002 | Perplexity: 1384.310067
2025-09-27 16:11:44,791 Stage: Train 0.5 | Epoch: 268 | Iter: 407400 | Total Loss: 0.005181 | Recon Loss: 0.004177 | Commit Loss: 0.002007 | Perplexity: 1387.418415
2025-09-27 16:12:31,849 Stage: Train 0.5 | Epoch: 268 | Iter: 407600 | Total Loss: 0.005172 | Recon Loss: 0.004170 | Commit Loss: 0.002003 | Perplexity: 1385.727977
2025-09-27 16:13:18,964 Stage: Train 0.5 | Epoch: 268 | Iter: 407800 | Total Loss: 0.005192 | Recon Loss: 0.004189 | Commit Loss: 0.002007 | Perplexity: 1389.093977
2025-09-27 16:14:06,140 Stage: Train 0.5 | Epoch: 268 | Iter: 408000 | Total Loss: 0.005143 | Recon Loss: 0.004138 | Commit Loss: 0.002010 | Perplexity: 1388.175482
2025-09-27 16:14:53,372 Stage: Train 0.5 | Epoch: 268 | Iter: 408200 | Total Loss: 0.005188 | Recon Loss: 0.004183 | Commit Loss: 0.002011 | Perplexity: 1388.306417
2025-09-27 16:15:40,487 Stage: Train 0.5 | Epoch: 268 | Iter: 408400 | Total Loss: 0.005168 | Recon Loss: 0.004163 | Commit Loss: 0.002009 | Perplexity: 1388.358770
2025-09-27 16:16:27,728 Stage: Train 0.5 | Epoch: 268 | Iter: 408600 | Total Loss: 0.005185 | Recon Loss: 0.004183 | Commit Loss: 0.002005 | Perplexity: 1386.774010
Trainning Epoch:  82%|████████▏ | 269/330 [26:05:21<6:04:25, 358.45s/it]2025-09-27 16:17:15,072 Stage: Train 0.5 | Epoch: 269 | Iter: 408800 | Total Loss: 0.005163 | Recon Loss: 0.004163 | Commit Loss: 0.002000 | Perplexity: 1387.231147
2025-09-27 16:18:02,464 Stage: Train 0.5 | Epoch: 269 | Iter: 409000 | Total Loss: 0.005130 | Recon Loss: 0.004129 | Commit Loss: 0.002001 | Perplexity: 1386.824059
2025-09-27 16:18:49,617 Stage: Train 0.5 | Epoch: 269 | Iter: 409200 | Total Loss: 0.005151 | Recon Loss: 0.004152 | Commit Loss: 0.001998 | Perplexity: 1387.219924
2025-09-27 16:19:34,343 Stage: Train 0.5 | Epoch: 269 | Iter: 409400 | Total Loss: 0.005201 | Recon Loss: 0.004198 | Commit Loss: 0.002007 | Perplexity: 1385.822776
2025-09-27 16:20:21,408 Stage: Train 0.5 | Epoch: 269 | Iter: 409600 | Total Loss: 0.005200 | Recon Loss: 0.004197 | Commit Loss: 0.002005 | Perplexity: 1386.792236
2025-09-27 16:21:08,515 Stage: Train 0.5 | Epoch: 269 | Iter: 409800 | Total Loss: 0.005169 | Recon Loss: 0.004166 | Commit Loss: 0.002005 | Perplexity: 1385.617654
2025-09-27 16:21:55,552 Stage: Train 0.5 | Epoch: 269 | Iter: 410000 | Total Loss: 0.005186 | Recon Loss: 0.004183 | Commit Loss: 0.002005 | Perplexity: 1388.515016
Trainning Epoch:  82%|████████▏ | 270/330 [26:11:17<5:57:42, 357.72s/it]2025-09-27 16:22:43,102 Stage: Train 0.5 | Epoch: 270 | Iter: 410200 | Total Loss: 0.005180 | Recon Loss: 0.004175 | Commit Loss: 0.002010 | Perplexity: 1389.393356
2025-09-27 16:23:30,357 Stage: Train 0.5 | Epoch: 270 | Iter: 410400 | Total Loss: 0.005157 | Recon Loss: 0.004154 | Commit Loss: 0.002007 | Perplexity: 1387.598605
2025-09-27 16:24:17,592 Stage: Train 0.5 | Epoch: 270 | Iter: 410600 | Total Loss: 0.005197 | Recon Loss: 0.004197 | Commit Loss: 0.001999 | Perplexity: 1388.247822
2025-09-27 16:25:04,717 Stage: Train 0.5 | Epoch: 270 | Iter: 410800 | Total Loss: 0.005146 | Recon Loss: 0.004145 | Commit Loss: 0.002003 | Perplexity: 1387.041743
2025-09-27 16:25:51,744 Stage: Train 0.5 | Epoch: 270 | Iter: 411000 | Total Loss: 0.005142 | Recon Loss: 0.004139 | Commit Loss: 0.002007 | Perplexity: 1386.526452
2025-09-27 16:26:38,653 Stage: Train 0.5 | Epoch: 270 | Iter: 411200 | Total Loss: 0.005192 | Recon Loss: 0.004192 | Commit Loss: 0.002001 | Perplexity: 1386.580747
2025-09-27 16:27:25,738 Stage: Train 0.5 | Epoch: 270 | Iter: 411400 | Total Loss: 0.005174 | Recon Loss: 0.004170 | Commit Loss: 0.002008 | Perplexity: 1387.304374
2025-09-27 16:28:13,035 Stage: Train 0.5 | Epoch: 270 | Iter: 411600 | Total Loss: 0.005143 | Recon Loss: 0.004140 | Commit Loss: 0.002007 | Perplexity: 1386.214956
Trainning Epoch:  82%|████████▏ | 271/330 [26:17:15<5:51:55, 357.89s/it]2025-09-27 16:29:00,598 Stage: Train 0.5 | Epoch: 271 | Iter: 411800 | Total Loss: 0.005189 | Recon Loss: 0.004184 | Commit Loss: 0.002011 | Perplexity: 1387.369229
2025-09-27 16:29:47,920 Stage: Train 0.5 | Epoch: 271 | Iter: 412000 | Total Loss: 0.005155 | Recon Loss: 0.004154 | Commit Loss: 0.002001 | Perplexity: 1387.391030
2025-09-27 16:30:34,999 Stage: Train 0.5 | Epoch: 271 | Iter: 412200 | Total Loss: 0.005220 | Recon Loss: 0.004217 | Commit Loss: 0.002007 | Perplexity: 1388.581878
2025-09-27 16:31:22,122 Stage: Train 0.5 | Epoch: 271 | Iter: 412400 | Total Loss: 0.005110 | Recon Loss: 0.004108 | Commit Loss: 0.002004 | Perplexity: 1387.354183
2025-09-27 16:32:09,296 Stage: Train 0.5 | Epoch: 271 | Iter: 412600 | Total Loss: 0.005183 | Recon Loss: 0.004182 | Commit Loss: 0.002003 | Perplexity: 1387.752844
2025-09-27 16:32:56,360 Stage: Train 0.5 | Epoch: 271 | Iter: 412800 | Total Loss: 0.005152 | Recon Loss: 0.004149 | Commit Loss: 0.002006 | Perplexity: 1388.666769
2025-09-27 16:33:43,142 Stage: Train 0.5 | Epoch: 271 | Iter: 413000 | Total Loss: 0.005161 | Recon Loss: 0.004156 | Commit Loss: 0.002011 | Perplexity: 1390.223681
Trainning Epoch:  82%|████████▏ | 272/330 [26:23:13<5:46:02, 357.97s/it]2025-09-27 16:34:30,544 Stage: Train 0.5 | Epoch: 272 | Iter: 413200 | Total Loss: 0.005198 | Recon Loss: 0.004193 | Commit Loss: 0.002010 | Perplexity: 1385.312938
2025-09-27 16:35:17,653 Stage: Train 0.5 | Epoch: 272 | Iter: 413400 | Total Loss: 0.005104 | Recon Loss: 0.004107 | Commit Loss: 0.001995 | Perplexity: 1385.038630
2025-09-27 16:36:04,754 Stage: Train 0.5 | Epoch: 272 | Iter: 413600 | Total Loss: 0.005151 | Recon Loss: 0.004149 | Commit Loss: 0.002004 | Perplexity: 1388.519393
2025-09-27 16:36:51,895 Stage: Train 0.5 | Epoch: 272 | Iter: 413800 | Total Loss: 0.005169 | Recon Loss: 0.004168 | Commit Loss: 0.002002 | Perplexity: 1386.252856
2025-09-27 16:37:39,117 Stage: Train 0.5 | Epoch: 272 | Iter: 414000 | Total Loss: 0.005167 | Recon Loss: 0.004165 | Commit Loss: 0.002004 | Perplexity: 1387.948823
2025-09-27 16:38:26,283 Stage: Train 0.5 | Epoch: 272 | Iter: 414200 | Total Loss: 0.005168 | Recon Loss: 0.004165 | Commit Loss: 0.002006 | Perplexity: 1389.132372
2025-09-27 16:39:13,476 Stage: Train 0.5 | Epoch: 272 | Iter: 414400 | Total Loss: 0.005137 | Recon Loss: 0.004133 | Commit Loss: 0.002008 | Perplexity: 1387.517275
2025-09-27 16:40:00,680 Stage: Train 0.5 | Epoch: 272 | Iter: 414600 | Total Loss: 0.005190 | Recon Loss: 0.004185 | Commit Loss: 0.002009 | Perplexity: 1391.134161
Trainning Epoch:  83%|████████▎ | 273/330 [26:29:12<5:40:13, 358.14s/it]2025-09-27 16:40:47,911 Stage: Train 0.5 | Epoch: 273 | Iter: 414800 | Total Loss: 0.005158 | Recon Loss: 0.004157 | Commit Loss: 0.002002 | Perplexity: 1385.380767
2025-09-27 16:41:35,089 Stage: Train 0.5 | Epoch: 273 | Iter: 415000 | Total Loss: 0.005137 | Recon Loss: 0.004133 | Commit Loss: 0.002007 | Perplexity: 1389.770884
2025-09-27 16:42:22,243 Stage: Train 0.5 | Epoch: 273 | Iter: 415200 | Total Loss: 0.005145 | Recon Loss: 0.004144 | Commit Loss: 0.002000 | Perplexity: 1386.061607
2025-09-27 16:43:09,427 Stage: Train 0.5 | Epoch: 273 | Iter: 415400 | Total Loss: 0.005137 | Recon Loss: 0.004139 | Commit Loss: 0.001997 | Perplexity: 1390.170415
2025-09-27 16:43:56,593 Stage: Train 0.5 | Epoch: 273 | Iter: 415600 | Total Loss: 0.005169 | Recon Loss: 0.004164 | Commit Loss: 0.002010 | Perplexity: 1390.910875
2025-09-27 16:44:43,733 Stage: Train 0.5 | Epoch: 273 | Iter: 415800 | Total Loss: 0.005182 | Recon Loss: 0.004180 | Commit Loss: 0.002004 | Perplexity: 1388.538988
2025-09-27 16:45:30,915 Stage: Train 0.5 | Epoch: 273 | Iter: 416000 | Total Loss: 0.005161 | Recon Loss: 0.004157 | Commit Loss: 0.002009 | Perplexity: 1386.316868
2025-09-27 16:46:18,186 Stage: Train 0.5 | Epoch: 273 | Iter: 416200 | Total Loss: 0.005081 | Recon Loss: 0.004084 | Commit Loss: 0.001995 | Perplexity: 1385.190658
Trainning Epoch:  83%|████████▎ | 274/330 [26:35:10<5:34:18, 358.19s/it]2025-09-27 16:47:05,744 Stage: Train 0.5 | Epoch: 274 | Iter: 416400 | Total Loss: 0.005139 | Recon Loss: 0.004142 | Commit Loss: 0.001993 | Perplexity: 1387.016411
2025-09-27 16:47:52,745 Stage: Train 0.5 | Epoch: 274 | Iter: 416600 | Total Loss: 0.005115 | Recon Loss: 0.004117 | Commit Loss: 0.001996 | Perplexity: 1387.195495
2025-09-27 16:48:40,061 Stage: Train 0.5 | Epoch: 274 | Iter: 416800 | Total Loss: 0.005164 | Recon Loss: 0.004161 | Commit Loss: 0.002005 | Perplexity: 1388.508699
2025-09-27 16:49:27,352 Stage: Train 0.5 | Epoch: 274 | Iter: 417000 | Total Loss: 0.005159 | Recon Loss: 0.004156 | Commit Loss: 0.002007 | Perplexity: 1389.945467
2025-09-27 16:50:14,540 Stage: Train 0.5 | Epoch: 274 | Iter: 417200 | Total Loss: 0.005190 | Recon Loss: 0.004185 | Commit Loss: 0.002010 | Perplexity: 1389.295981
2025-09-27 16:51:01,904 Stage: Train 0.5 | Epoch: 274 | Iter: 417400 | Total Loss: 0.005124 | Recon Loss: 0.004126 | Commit Loss: 0.001997 | Perplexity: 1388.512283
2025-09-27 16:51:48,937 Stage: Train 0.5 | Epoch: 274 | Iter: 417600 | Total Loss: 0.005130 | Recon Loss: 0.004130 | Commit Loss: 0.001999 | Perplexity: 1387.736764
Trainning Epoch:  83%|████████▎ | 275/330 [26:41:09<5:28:29, 358.35s/it]2025-09-27 16:52:36,322 Stage: Train 0.5 | Epoch: 275 | Iter: 417800 | Total Loss: 0.005139 | Recon Loss: 0.004133 | Commit Loss: 0.002012 | Perplexity: 1388.119557
2025-09-27 16:53:23,418 Stage: Train 0.5 | Epoch: 275 | Iter: 418000 | Total Loss: 0.005172 | Recon Loss: 0.004168 | Commit Loss: 0.002009 | Perplexity: 1392.127676
2025-09-27 16:54:10,644 Stage: Train 0.5 | Epoch: 275 | Iter: 418200 | Total Loss: 0.005145 | Recon Loss: 0.004148 | Commit Loss: 0.001993 | Perplexity: 1389.062015
2025-09-27 16:54:57,947 Stage: Train 0.5 | Epoch: 275 | Iter: 418400 | Total Loss: 0.005157 | Recon Loss: 0.004155 | Commit Loss: 0.002003 | Perplexity: 1387.950242
2025-09-27 16:55:44,799 Stage: Train 0.5 | Epoch: 275 | Iter: 418600 | Total Loss: 0.005124 | Recon Loss: 0.004122 | Commit Loss: 0.002004 | Perplexity: 1387.166146
2025-09-27 16:56:31,917 Stage: Train 0.5 | Epoch: 275 | Iter: 418800 | Total Loss: 0.005186 | Recon Loss: 0.004185 | Commit Loss: 0.002003 | Perplexity: 1390.224546
2025-09-27 16:57:18,834 Stage: Train 0.5 | Epoch: 275 | Iter: 419000 | Total Loss: 0.005119 | Recon Loss: 0.004119 | Commit Loss: 0.002000 | Perplexity: 1387.305085
2025-09-27 16:58:06,039 Stage: Train 0.5 | Epoch: 275 | Iter: 419200 | Total Loss: 0.005154 | Recon Loss: 0.004152 | Commit Loss: 0.002004 | Perplexity: 1390.426773
Trainning Epoch:  84%|████████▎ | 276/330 [26:47:07<5:22:27, 358.29s/it]2025-09-27 16:58:53,645 Stage: Train 0.5 | Epoch: 276 | Iter: 419400 | Total Loss: 0.005159 | Recon Loss: 0.004159 | Commit Loss: 0.002001 | Perplexity: 1388.983148
2025-09-27 16:59:40,869 Stage: Train 0.5 | Epoch: 276 | Iter: 419600 | Total Loss: 0.005096 | Recon Loss: 0.004095 | Commit Loss: 0.002001 | Perplexity: 1386.963747
2025-09-27 17:00:28,113 Stage: Train 0.5 | Epoch: 276 | Iter: 419800 | Total Loss: 0.005169 | Recon Loss: 0.004164 | Commit Loss: 0.002011 | Perplexity: 1390.212818
2025-09-27 17:01:15,289 Stage: Train 0.5 | Epoch: 276 | Iter: 420000 | Total Loss: 0.005152 | Recon Loss: 0.004150 | Commit Loss: 0.002005 | Perplexity: 1388.093333
2025-09-27 17:01:15,289 Saving model at iteration 420000
2025-09-27 17:01:15,683 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000
2025-09-27 17:01:15,991 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/model.safetensors
2025-09-27 17:01:16,417 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/optimizer.bin
2025-09-27 17:01:16,417 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/scheduler.bin
2025-09-27 17:01:16,418 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/sampler.bin
2025-09-27 17:01:16,419 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/random_states_0.pkl
2025-09-27 17:02:04,137 Stage: Train 0.5 | Epoch: 276 | Iter: 420200 | Total Loss: 0.005111 | Recon Loss: 0.004108 | Commit Loss: 0.002006 | Perplexity: 1387.497610
2025-09-27 17:02:51,090 Stage: Train 0.5 | Epoch: 276 | Iter: 420400 | Total Loss: 0.005144 | Recon Loss: 0.004146 | Commit Loss: 0.001997 | Perplexity: 1388.627775
2025-09-27 17:03:38,388 Stage: Train 0.5 | Epoch: 276 | Iter: 420600 | Total Loss: 0.005177 | Recon Loss: 0.004174 | Commit Loss: 0.002004 | Perplexity: 1388.841905
Trainning Epoch:  84%|████████▍ | 277/330 [26:53:07<5:17:05, 358.97s/it]2025-09-27 17:04:26,054 Stage: Train 0.5 | Epoch: 277 | Iter: 420800 | Total Loss: 0.005121 | Recon Loss: 0.004121 | Commit Loss: 0.001999 | Perplexity: 1390.378831
2025-09-27 17:05:13,337 Stage: Train 0.5 | Epoch: 277 | Iter: 421000 | Total Loss: 0.005078 | Recon Loss: 0.004080 | Commit Loss: 0.001997 | Perplexity: 1389.306324
2025-09-27 17:06:00,660 Stage: Train 0.5 | Epoch: 277 | Iter: 421200 | Total Loss: 0.005128 | Recon Loss: 0.004131 | Commit Loss: 0.001994 | Perplexity: 1388.258546
2025-09-27 17:06:47,717 Stage: Train 0.5 | Epoch: 277 | Iter: 421400 | Total Loss: 0.005124 | Recon Loss: 0.004123 | Commit Loss: 0.002002 | Perplexity: 1391.397176
2025-09-27 17:07:34,935 Stage: Train 0.5 | Epoch: 277 | Iter: 421600 | Total Loss: 0.005086 | Recon Loss: 0.004085 | Commit Loss: 0.002001 | Perplexity: 1389.262206
2025-09-27 17:08:22,360 Stage: Train 0.5 | Epoch: 277 | Iter: 421800 | Total Loss: 0.005141 | Recon Loss: 0.004142 | Commit Loss: 0.001999 | Perplexity: 1392.201957
2025-09-27 17:09:09,729 Stage: Train 0.5 | Epoch: 277 | Iter: 422000 | Total Loss: 0.005122 | Recon Loss: 0.004118 | Commit Loss: 0.002009 | Perplexity: 1390.101334
2025-09-27 17:09:56,700 Stage: Train 0.5 | Epoch: 277 | Iter: 422200 | Total Loss: 0.005163 | Recon Loss: 0.004159 | Commit Loss: 0.002007 | Perplexity: 1389.494996
Trainning Epoch:  84%|████████▍ | 278/330 [26:59:06<5:11:08, 359.01s/it]2025-09-27 17:10:44,239 Stage: Train 0.5 | Epoch: 278 | Iter: 422400 | Total Loss: 0.005128 | Recon Loss: 0.004131 | Commit Loss: 0.001995 | Perplexity: 1386.548341
2025-09-27 17:11:31,485 Stage: Train 0.5 | Epoch: 278 | Iter: 422600 | Total Loss: 0.005099 | Recon Loss: 0.004100 | Commit Loss: 0.001996 | Perplexity: 1390.176324
2025-09-27 17:12:18,817 Stage: Train 0.5 | Epoch: 278 | Iter: 422800 | Total Loss: 0.005114 | Recon Loss: 0.004114 | Commit Loss: 0.001999 | Perplexity: 1389.282476
2025-09-27 17:13:06,128 Stage: Train 0.5 | Epoch: 278 | Iter: 423000 | Total Loss: 0.005132 | Recon Loss: 0.004129 | Commit Loss: 0.002004 | Perplexity: 1391.466018
2025-09-27 17:13:53,421 Stage: Train 0.5 | Epoch: 278 | Iter: 423200 | Total Loss: 0.005120 | Recon Loss: 0.004123 | Commit Loss: 0.001995 | Perplexity: 1391.088504
2025-09-27 17:14:40,746 Stage: Train 0.5 | Epoch: 278 | Iter: 423400 | Total Loss: 0.005147 | Recon Loss: 0.004144 | Commit Loss: 0.002006 | Perplexity: 1390.430079
2025-09-27 17:15:28,058 Stage: Train 0.5 | Epoch: 278 | Iter: 423600 | Total Loss: 0.005144 | Recon Loss: 0.004143 | Commit Loss: 0.002001 | Perplexity: 1391.024868
2025-09-27 17:16:15,343 Stage: Train 0.5 | Epoch: 278 | Iter: 423800 | Total Loss: 0.005134 | Recon Loss: 0.004133 | Commit Loss: 0.002002 | Perplexity: 1387.739919
Trainning Epoch:  85%|████████▍ | 279/330 [27:05:06<5:05:16, 359.14s/it]2025-09-27 17:17:02,499 Stage: Train 0.5 | Epoch: 279 | Iter: 424000 | Total Loss: 0.005148 | Recon Loss: 0.004150 | Commit Loss: 0.001997 | Perplexity: 1388.775482
2025-09-27 17:17:49,804 Stage: Train 0.5 | Epoch: 279 | Iter: 424200 | Total Loss: 0.005144 | Recon Loss: 0.004145 | Commit Loss: 0.001997 | Perplexity: 1388.840056
2025-09-27 17:18:37,143 Stage: Train 0.5 | Epoch: 279 | Iter: 424400 | Total Loss: 0.005065 | Recon Loss: 0.004065 | Commit Loss: 0.002000 | Perplexity: 1389.791241
2025-09-27 17:19:24,401 Stage: Train 0.5 | Epoch: 279 | Iter: 424600 | Total Loss: 0.005131 | Recon Loss: 0.004131 | Commit Loss: 0.002000 | Perplexity: 1390.860762
2025-09-27 17:20:11,725 Stage: Train 0.5 | Epoch: 279 | Iter: 424800 | Total Loss: 0.005128 | Recon Loss: 0.004129 | Commit Loss: 0.001998 | Perplexity: 1387.128860
2025-09-27 17:20:59,021 Stage: Train 0.5 | Epoch: 279 | Iter: 425000 | Total Loss: 0.005148 | Recon Loss: 0.004142 | Commit Loss: 0.002011 | Perplexity: 1392.607349
2025-09-27 17:21:46,415 Stage: Train 0.5 | Epoch: 279 | Iter: 425200 | Total Loss: 0.005122 | Recon Loss: 0.004123 | Commit Loss: 0.001997 | Perplexity: 1389.965421
Trainning Epoch:  85%|████████▍ | 280/330 [27:11:05<4:59:15, 359.11s/it]2025-09-27 17:22:33,837 Stage: Train 0.5 | Epoch: 280 | Iter: 425400 | Total Loss: 0.005070 | Recon Loss: 0.004070 | Commit Loss: 0.001999 | Perplexity: 1390.035527
2025-09-27 17:23:21,041 Stage: Train 0.5 | Epoch: 280 | Iter: 425600 | Total Loss: 0.005138 | Recon Loss: 0.004142 | Commit Loss: 0.001992 | Perplexity: 1390.124314
2025-09-27 17:24:08,012 Stage: Train 0.5 | Epoch: 280 | Iter: 425800 | Total Loss: 0.005102 | Recon Loss: 0.004105 | Commit Loss: 0.001994 | Perplexity: 1391.032078
2025-09-27 17:24:55,725 Stage: Train 0.5 | Epoch: 280 | Iter: 426000 | Total Loss: 0.005175 | Recon Loss: 0.004175 | Commit Loss: 0.001999 | Perplexity: 1390.133518
2025-09-27 17:25:42,948 Stage: Train 0.5 | Epoch: 280 | Iter: 426200 | Total Loss: 0.005119 | Recon Loss: 0.004121 | Commit Loss: 0.001996 | Perplexity: 1388.644715
2025-09-27 17:26:30,060 Stage: Train 0.5 | Epoch: 280 | Iter: 426400 | Total Loss: 0.005118 | Recon Loss: 0.004118 | Commit Loss: 0.001999 | Perplexity: 1391.200467
2025-09-27 17:27:17,129 Stage: Train 0.5 | Epoch: 280 | Iter: 426600 | Total Loss: 0.005122 | Recon Loss: 0.004123 | Commit Loss: 0.001999 | Perplexity: 1389.852290
2025-09-27 17:28:04,205 Stage: Train 0.5 | Epoch: 280 | Iter: 426800 | Total Loss: 0.005131 | Recon Loss: 0.004132 | Commit Loss: 0.001999 | Perplexity: 1390.608899
Trainning Epoch:  85%|████████▌ | 281/330 [27:17:04<4:53:11, 359.01s/it]2025-09-27 17:28:51,819 Stage: Train 0.5 | Epoch: 281 | Iter: 427000 | Total Loss: 0.005092 | Recon Loss: 0.004094 | Commit Loss: 0.001997 | Perplexity: 1389.179413
2025-09-27 17:29:39,057 Stage: Train 0.5 | Epoch: 281 | Iter: 427200 | Total Loss: 0.005130 | Recon Loss: 0.004136 | Commit Loss: 0.001989 | Perplexity: 1390.616810
2025-09-27 17:30:26,348 Stage: Train 0.5 | Epoch: 281 | Iter: 427400 | Total Loss: 0.005106 | Recon Loss: 0.004110 | Commit Loss: 0.001992 | Perplexity: 1390.350397
2025-09-27 17:31:13,580 Stage: Train 0.5 | Epoch: 281 | Iter: 427600 | Total Loss: 0.005116 | Recon Loss: 0.004118 | Commit Loss: 0.001996 | Perplexity: 1389.368563
2025-09-27 17:32:00,647 Stage: Train 0.5 | Epoch: 281 | Iter: 427800 | Total Loss: 0.005112 | Recon Loss: 0.004114 | Commit Loss: 0.001998 | Perplexity: 1390.074655
2025-09-27 17:32:47,902 Stage: Train 0.5 | Epoch: 281 | Iter: 428000 | Total Loss: 0.005135 | Recon Loss: 0.004138 | Commit Loss: 0.001993 | Perplexity: 1390.099362
2025-09-27 17:33:35,126 Stage: Train 0.5 | Epoch: 281 | Iter: 428200 | Total Loss: 0.005107 | Recon Loss: 0.004108 | Commit Loss: 0.001998 | Perplexity: 1389.739258
Trainning Epoch:  85%|████████▌ | 282/330 [27:23:03<4:47:12, 359.00s/it]2025-09-27 17:34:22,652 Stage: Train 0.5 | Epoch: 282 | Iter: 428400 | Total Loss: 0.005123 | Recon Loss: 0.004129 | Commit Loss: 0.001987 | Perplexity: 1388.516392
2025-09-27 17:35:09,964 Stage: Train 0.5 | Epoch: 282 | Iter: 428600 | Total Loss: 0.005159 | Recon Loss: 0.004162 | Commit Loss: 0.001995 | Perplexity: 1391.933803
2025-09-27 17:35:57,339 Stage: Train 0.5 | Epoch: 282 | Iter: 428800 | Total Loss: 0.005070 | Recon Loss: 0.004071 | Commit Loss: 0.001998 | Perplexity: 1390.934150
2025-09-27 17:36:44,611 Stage: Train 0.5 | Epoch: 282 | Iter: 429000 | Total Loss: 0.005079 | Recon Loss: 0.004086 | Commit Loss: 0.001987 | Perplexity: 1389.061583
2025-09-27 17:37:31,900 Stage: Train 0.5 | Epoch: 282 | Iter: 429200 | Total Loss: 0.005092 | Recon Loss: 0.004094 | Commit Loss: 0.001996 | Perplexity: 1390.078287
2025-09-27 17:38:19,164 Stage: Train 0.5 | Epoch: 282 | Iter: 429400 | Total Loss: 0.005081 | Recon Loss: 0.004084 | Commit Loss: 0.001993 | Perplexity: 1390.624500
2025-09-27 17:39:06,111 Stage: Train 0.5 | Epoch: 282 | Iter: 429600 | Total Loss: 0.005175 | Recon Loss: 0.004175 | Commit Loss: 0.002001 | Perplexity: 1391.029267
2025-09-27 17:39:53,406 Stage: Train 0.5 | Epoch: 282 | Iter: 429800 | Total Loss: 0.005103 | Recon Loss: 0.004103 | Commit Loss: 0.002001 | Perplexity: 1394.421342
Trainning Epoch:  86%|████████▌ | 283/330 [27:29:02<4:41:15, 359.06s/it]2025-09-27 17:40:41,028 Stage: Train 0.5 | Epoch: 283 | Iter: 430000 | Total Loss: 0.005099 | Recon Loss: 0.004100 | Commit Loss: 0.001997 | Perplexity: 1387.318683
2025-09-27 17:41:28,414 Stage: Train 0.5 | Epoch: 283 | Iter: 430200 | Total Loss: 0.005058 | Recon Loss: 0.004066 | Commit Loss: 0.001985 | Perplexity: 1390.226509
2025-09-27 17:42:15,684 Stage: Train 0.5 | Epoch: 283 | Iter: 430400 | Total Loss: 0.005162 | Recon Loss: 0.004171 | Commit Loss: 0.001983 | Perplexity: 1387.929421
2025-09-27 17:43:03,236 Stage: Train 0.5 | Epoch: 283 | Iter: 430600 | Total Loss: 0.005096 | Recon Loss: 0.004100 | Commit Loss: 0.001993 | Perplexity: 1390.913397
2025-09-27 17:43:50,385 Stage: Train 0.5 | Epoch: 283 | Iter: 430800 | Total Loss: 0.005100 | Recon Loss: 0.004100 | Commit Loss: 0.002000 | Perplexity: 1390.246758
2025-09-27 17:44:37,690 Stage: Train 0.5 | Epoch: 283 | Iter: 431000 | Total Loss: 0.005110 | Recon Loss: 0.004110 | Commit Loss: 0.002001 | Perplexity: 1391.407078
2025-09-27 17:45:25,125 Stage: Train 0.5 | Epoch: 283 | Iter: 431200 | Total Loss: 0.005109 | Recon Loss: 0.004114 | Commit Loss: 0.001991 | Perplexity: 1391.812335
Trainning Epoch:  86%|████████▌ | 284/330 [27:35:01<4:35:25, 359.25s/it]2025-09-27 17:46:12,502 Stage: Train 0.5 | Epoch: 284 | Iter: 431400 | Total Loss: 0.005155 | Recon Loss: 0.004155 | Commit Loss: 0.002000 | Perplexity: 1390.252362
2025-09-27 17:46:59,810 Stage: Train 0.5 | Epoch: 284 | Iter: 431600 | Total Loss: 0.005057 | Recon Loss: 0.004060 | Commit Loss: 0.001994 | Perplexity: 1391.594247
2025-09-27 17:47:46,936 Stage: Train 0.5 | Epoch: 284 | Iter: 431800 | Total Loss: 0.005110 | Recon Loss: 0.004112 | Commit Loss: 0.001996 | Perplexity: 1391.177632
2025-09-27 17:48:34,221 Stage: Train 0.5 | Epoch: 284 | Iter: 432000 | Total Loss: 0.005117 | Recon Loss: 0.004115 | Commit Loss: 0.002004 | Perplexity: 1394.215541
2025-09-27 17:49:21,366 Stage: Train 0.5 | Epoch: 284 | Iter: 432200 | Total Loss: 0.005134 | Recon Loss: 0.004137 | Commit Loss: 0.001995 | Perplexity: 1390.799938
2025-09-27 17:50:08,596 Stage: Train 0.5 | Epoch: 284 | Iter: 432400 | Total Loss: 0.005102 | Recon Loss: 0.004107 | Commit Loss: 0.001991 | Perplexity: 1390.051752
2025-09-27 17:50:55,807 Stage: Train 0.5 | Epoch: 284 | Iter: 432600 | Total Loss: 0.005102 | Recon Loss: 0.004105 | Commit Loss: 0.001994 | Perplexity: 1389.457740
2025-09-27 17:51:43,080 Stage: Train 0.5 | Epoch: 284 | Iter: 432800 | Total Loss: 0.005105 | Recon Loss: 0.004106 | Commit Loss: 0.001998 | Perplexity: 1390.237324
Trainning Epoch:  86%|████████▋ | 285/330 [27:41:00<4:29:22, 359.17s/it]2025-09-27 17:52:30,551 Stage: Train 0.5 | Epoch: 285 | Iter: 433000 | Total Loss: 0.005084 | Recon Loss: 0.004086 | Commit Loss: 0.001995 | Perplexity: 1387.459980
2025-09-27 17:53:17,211 Stage: Train 0.5 | Epoch: 285 | Iter: 433200 | Total Loss: 0.005087 | Recon Loss: 0.004093 | Commit Loss: 0.001989 | Perplexity: 1390.452555
2025-09-27 17:54:04,374 Stage: Train 0.5 | Epoch: 285 | Iter: 433400 | Total Loss: 0.005074 | Recon Loss: 0.004077 | Commit Loss: 0.001993 | Perplexity: 1390.226716
2025-09-27 17:54:49,964 Stage: Train 0.5 | Epoch: 285 | Iter: 433600 | Total Loss: 0.005155 | Recon Loss: 0.004155 | Commit Loss: 0.002000 | Perplexity: 1393.710923
2025-09-27 17:55:37,135 Stage: Train 0.5 | Epoch: 285 | Iter: 433800 | Total Loss: 0.005084 | Recon Loss: 0.004087 | Commit Loss: 0.001994 | Perplexity: 1391.228325
2025-09-27 17:56:24,381 Stage: Train 0.5 | Epoch: 285 | Iter: 434000 | Total Loss: 0.005152 | Recon Loss: 0.004154 | Commit Loss: 0.001996 | Perplexity: 1391.163128
2025-09-27 17:57:11,764 Stage: Train 0.5 | Epoch: 285 | Iter: 434200 | Total Loss: 0.005038 | Recon Loss: 0.004045 | Commit Loss: 0.001986 | Perplexity: 1388.551636
2025-09-27 17:57:58,843 Stage: Train 0.5 | Epoch: 285 | Iter: 434400 | Total Loss: 0.005087 | Recon Loss: 0.004089 | Commit Loss: 0.001996 | Perplexity: 1391.298712
Trainning Epoch:  87%|████████▋ | 286/330 [27:46:57<4:22:49, 358.41s/it]2025-09-27 17:58:46,421 Stage: Train 0.5 | Epoch: 286 | Iter: 434600 | Total Loss: 0.005177 | Recon Loss: 0.004182 | Commit Loss: 0.001991 | Perplexity: 1391.546606
2025-09-27 17:59:33,741 Stage: Train 0.5 | Epoch: 286 | Iter: 434800 | Total Loss: 0.005062 | Recon Loss: 0.004068 | Commit Loss: 0.001987 | Perplexity: 1392.659956
2025-09-27 18:00:20,803 Stage: Train 0.5 | Epoch: 286 | Iter: 435000 | Total Loss: 0.005071 | Recon Loss: 0.004075 | Commit Loss: 0.001993 | Perplexity: 1391.061387
2025-09-27 18:01:08,126 Stage: Train 0.5 | Epoch: 286 | Iter: 435200 | Total Loss: 0.005086 | Recon Loss: 0.004087 | Commit Loss: 0.001999 | Perplexity: 1390.861271
2025-09-27 18:01:55,417 Stage: Train 0.5 | Epoch: 286 | Iter: 435400 | Total Loss: 0.005110 | Recon Loss: 0.004115 | Commit Loss: 0.001990 | Perplexity: 1390.317911
2025-09-27 18:02:42,781 Stage: Train 0.5 | Epoch: 286 | Iter: 435600 | Total Loss: 0.005103 | Recon Loss: 0.004110 | Commit Loss: 0.001987 | Perplexity: 1390.597940
2025-09-27 18:03:30,135 Stage: Train 0.5 | Epoch: 286 | Iter: 435800 | Total Loss: 0.005113 | Recon Loss: 0.004113 | Commit Loss: 0.002002 | Perplexity: 1393.510398
Trainning Epoch:  87%|████████▋ | 287/330 [27:52:57<4:17:04, 358.71s/it]2025-09-27 18:04:17,721 Stage: Train 0.5 | Epoch: 287 | Iter: 436000 | Total Loss: 0.005094 | Recon Loss: 0.004096 | Commit Loss: 0.001996 | Perplexity: 1390.245212
2025-09-27 18:05:04,891 Stage: Train 0.5 | Epoch: 287 | Iter: 436200 | Total Loss: 0.005149 | Recon Loss: 0.004154 | Commit Loss: 0.001989 | Perplexity: 1391.289236
2025-09-27 18:05:52,107 Stage: Train 0.5 | Epoch: 287 | Iter: 436400 | Total Loss: 0.005074 | Recon Loss: 0.004078 | Commit Loss: 0.001992 | Perplexity: 1393.628503
2025-09-27 18:06:39,361 Stage: Train 0.5 | Epoch: 287 | Iter: 436600 | Total Loss: 0.005070 | Recon Loss: 0.004074 | Commit Loss: 0.001992 | Perplexity: 1390.350649
2025-09-27 18:07:26,621 Stage: Train 0.5 | Epoch: 287 | Iter: 436800 | Total Loss: 0.005097 | Recon Loss: 0.004105 | Commit Loss: 0.001984 | Perplexity: 1388.073307
2025-09-27 18:08:13,614 Stage: Train 0.5 | Epoch: 287 | Iter: 437000 | Total Loss: 0.005106 | Recon Loss: 0.004111 | Commit Loss: 0.001989 | Perplexity: 1390.943448
2025-09-27 18:09:01,162 Stage: Train 0.5 | Epoch: 287 | Iter: 437200 | Total Loss: 0.005102 | Recon Loss: 0.004107 | Commit Loss: 0.001991 | Perplexity: 1393.899439
2025-09-27 18:09:48,237 Stage: Train 0.5 | Epoch: 287 | Iter: 437400 | Total Loss: 0.005107 | Recon Loss: 0.004110 | Commit Loss: 0.001994 | Perplexity: 1390.395651
Trainning Epoch:  87%|████████▋ | 288/330 [27:58:55<4:11:09, 358.79s/it]2025-09-27 18:10:35,741 Stage: Train 0.5 | Epoch: 288 | Iter: 437600 | Total Loss: 0.005082 | Recon Loss: 0.004091 | Commit Loss: 0.001981 | Perplexity: 1391.205627
2025-09-27 18:11:23,167 Stage: Train 0.5 | Epoch: 288 | Iter: 437800 | Total Loss: 0.005071 | Recon Loss: 0.004081 | Commit Loss: 0.001979 | Perplexity: 1389.250106
2025-09-27 18:12:10,390 Stage: Train 0.5 | Epoch: 288 | Iter: 438000 | Total Loss: 0.005098 | Recon Loss: 0.004102 | Commit Loss: 0.001991 | Perplexity: 1392.718490
2025-09-27 18:12:57,668 Stage: Train 0.5 | Epoch: 288 | Iter: 438200 | Total Loss: 0.005095 | Recon Loss: 0.004099 | Commit Loss: 0.001992 | Perplexity: 1393.235694
2025-09-27 18:13:45,005 Stage: Train 0.5 | Epoch: 288 | Iter: 438400 | Total Loss: 0.005071 | Recon Loss: 0.004075 | Commit Loss: 0.001992 | Perplexity: 1392.837822
2025-09-27 18:14:32,239 Stage: Train 0.5 | Epoch: 288 | Iter: 438600 | Total Loss: 0.005093 | Recon Loss: 0.004094 | Commit Loss: 0.001999 | Perplexity: 1390.835357
2025-09-27 18:15:19,243 Stage: Train 0.5 | Epoch: 288 | Iter: 438800 | Total Loss: 0.005101 | Recon Loss: 0.004107 | Commit Loss: 0.001987 | Perplexity: 1391.851264
Trainning Epoch:  88%|████████▊ | 289/330 [28:04:55<4:05:15, 358.92s/it]2025-09-27 18:16:06,906 Stage: Train 0.5 | Epoch: 289 | Iter: 439000 | Total Loss: 0.005107 | Recon Loss: 0.004107 | Commit Loss: 0.002000 | Perplexity: 1391.065118
2025-09-27 18:16:54,217 Stage: Train 0.5 | Epoch: 289 | Iter: 439200 | Total Loss: 0.005062 | Recon Loss: 0.004069 | Commit Loss: 0.001984 | Perplexity: 1389.735115
2025-09-27 18:17:41,442 Stage: Train 0.5 | Epoch: 289 | Iter: 439400 | Total Loss: 0.005040 | Recon Loss: 0.004046 | Commit Loss: 0.001987 | Perplexity: 1391.893131
2025-09-27 18:18:28,608 Stage: Train 0.5 | Epoch: 289 | Iter: 439600 | Total Loss: 0.005141 | Recon Loss: 0.004148 | Commit Loss: 0.001987 | Perplexity: 1391.185365
2025-09-27 18:19:15,760 Stage: Train 0.5 | Epoch: 289 | Iter: 439800 | Total Loss: 0.005064 | Recon Loss: 0.004071 | Commit Loss: 0.001986 | Perplexity: 1391.738477
2025-09-27 18:20:03,104 Stage: Train 0.5 | Epoch: 289 | Iter: 440000 | Total Loss: 0.005071 | Recon Loss: 0.004073 | Commit Loss: 0.001996 | Perplexity: 1392.586049
2025-09-27 18:20:03,104 Saving model at iteration 440000
2025-09-27 18:20:03,319 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000
2025-09-27 18:20:03,598 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/model.safetensors
2025-09-27 18:20:04,005 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/optimizer.bin
2025-09-27 18:20:04,006 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/scheduler.bin
2025-09-27 18:20:04,006 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/sampler.bin
2025-09-27 18:20:04,007 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/random_states_0.pkl
2025-09-27 18:20:51,058 Stage: Train 0.5 | Epoch: 289 | Iter: 440200 | Total Loss: 0.005079 | Recon Loss: 0.004086 | Commit Loss: 0.001987 | Perplexity: 1391.470437
2025-09-27 18:21:38,138 Stage: Train 0.5 | Epoch: 289 | Iter: 440400 | Total Loss: 0.005136 | Recon Loss: 0.004140 | Commit Loss: 0.001991 | Perplexity: 1391.024917
Trainning Epoch:  88%|████████▊ | 290/330 [28:10:54<3:59:21, 359.05s/it]2025-09-27 18:22:25,416 Stage: Train 0.5 | Epoch: 290 | Iter: 440600 | Total Loss: 0.005073 | Recon Loss: 0.004078 | Commit Loss: 0.001989 | Perplexity: 1391.923304
2025-09-27 18:23:12,607 Stage: Train 0.5 | Epoch: 290 | Iter: 440800 | Total Loss: 0.005057 | Recon Loss: 0.004059 | Commit Loss: 0.001995 | Perplexity: 1393.307949
2025-09-27 18:23:59,802 Stage: Train 0.5 | Epoch: 290 | Iter: 441000 | Total Loss: 0.005052 | Recon Loss: 0.004058 | Commit Loss: 0.001988 | Perplexity: 1391.842509
2025-09-27 18:24:47,084 Stage: Train 0.5 | Epoch: 290 | Iter: 441200 | Total Loss: 0.005086 | Recon Loss: 0.004090 | Commit Loss: 0.001991 | Perplexity: 1388.474453
2025-09-27 18:25:34,318 Stage: Train 0.5 | Epoch: 290 | Iter: 441400 | Total Loss: 0.005064 | Recon Loss: 0.004070 | Commit Loss: 0.001989 | Perplexity: 1390.626977
2025-09-27 18:26:21,739 Stage: Train 0.5 | Epoch: 290 | Iter: 441600 | Total Loss: 0.005099 | Recon Loss: 0.004101 | Commit Loss: 0.001997 | Perplexity: 1393.493568
2025-09-27 18:27:09,081 Stage: Train 0.5 | Epoch: 290 | Iter: 441800 | Total Loss: 0.005063 | Recon Loss: 0.004072 | Commit Loss: 0.001983 | Perplexity: 1390.498260
2025-09-27 18:27:56,578 Stage: Train 0.5 | Epoch: 290 | Iter: 442000 | Total Loss: 0.005124 | Recon Loss: 0.004128 | Commit Loss: 0.001992 | Perplexity: 1391.663315
Trainning Epoch:  88%|████████▊ | 291/330 [28:16:54<3:53:29, 359.21s/it]2025-09-27 18:28:44,108 Stage: Train 0.5 | Epoch: 291 | Iter: 442200 | Total Loss: 0.005077 | Recon Loss: 0.004087 | Commit Loss: 0.001979 | Perplexity: 1390.838518
2025-09-27 18:29:31,016 Stage: Train 0.5 | Epoch: 291 | Iter: 442400 | Total Loss: 0.005068 | Recon Loss: 0.004075 | Commit Loss: 0.001986 | Perplexity: 1392.997620
2025-09-27 18:30:18,412 Stage: Train 0.5 | Epoch: 291 | Iter: 442600 | Total Loss: 0.005076 | Recon Loss: 0.004083 | Commit Loss: 0.001985 | Perplexity: 1389.685680
2025-09-27 18:31:05,863 Stage: Train 0.5 | Epoch: 291 | Iter: 442800 | Total Loss: 0.005131 | Recon Loss: 0.004134 | Commit Loss: 0.001993 | Perplexity: 1391.785602
2025-09-27 18:31:53,274 Stage: Train 0.5 | Epoch: 291 | Iter: 443000 | Total Loss: 0.005097 | Recon Loss: 0.004105 | Commit Loss: 0.001984 | Perplexity: 1388.633414
2025-09-27 18:32:40,544 Stage: Train 0.5 | Epoch: 291 | Iter: 443200 | Total Loss: 0.005087 | Recon Loss: 0.004092 | Commit Loss: 0.001991 | Perplexity: 1393.457027
2025-09-27 18:33:27,882 Stage: Train 0.5 | Epoch: 291 | Iter: 443400 | Total Loss: 0.005064 | Recon Loss: 0.004071 | Commit Loss: 0.001986 | Perplexity: 1390.444422
Trainning Epoch:  88%|████████▊ | 292/330 [28:22:53<3:47:34, 359.33s/it]2025-09-27 18:34:15,629 Stage: Train 0.5 | Epoch: 292 | Iter: 443600 | Total Loss: 0.005092 | Recon Loss: 0.004095 | Commit Loss: 0.001994 | Perplexity: 1391.333347
2025-09-27 18:35:03,024 Stage: Train 0.5 | Epoch: 292 | Iter: 443800 | Total Loss: 0.005062 | Recon Loss: 0.004068 | Commit Loss: 0.001987 | Perplexity: 1395.482003
2025-09-27 18:35:50,327 Stage: Train 0.5 | Epoch: 292 | Iter: 444000 | Total Loss: 0.005086 | Recon Loss: 0.004094 | Commit Loss: 0.001984 | Perplexity: 1392.949410
2025-09-27 18:36:37,307 Stage: Train 0.5 | Epoch: 292 | Iter: 444200 | Total Loss: 0.005042 | Recon Loss: 0.004049 | Commit Loss: 0.001986 | Perplexity: 1391.876780
2025-09-27 18:37:24,554 Stage: Train 0.5 | Epoch: 292 | Iter: 444400 | Total Loss: 0.005061 | Recon Loss: 0.004069 | Commit Loss: 0.001985 | Perplexity: 1392.588582
2025-09-27 18:38:11,780 Stage: Train 0.5 | Epoch: 292 | Iter: 444600 | Total Loss: 0.005079 | Recon Loss: 0.004084 | Commit Loss: 0.001991 | Perplexity: 1393.276846
2025-09-27 18:38:59,117 Stage: Train 0.5 | Epoch: 292 | Iter: 444800 | Total Loss: 0.005061 | Recon Loss: 0.004067 | Commit Loss: 0.001988 | Perplexity: 1390.126401
2025-09-27 18:39:46,465 Stage: Train 0.5 | Epoch: 292 | Iter: 445000 | Total Loss: 0.005085 | Recon Loss: 0.004091 | Commit Loss: 0.001987 | Perplexity: 1390.450990
Trainning Epoch:  89%|████████▉ | 293/330 [28:28:53<3:41:34, 359.31s/it]2025-09-27 18:40:34,024 Stage: Train 0.5 | Epoch: 293 | Iter: 445200 | Total Loss: 0.005083 | Recon Loss: 0.004087 | Commit Loss: 0.001992 | Perplexity: 1392.706008
2025-09-27 18:41:21,315 Stage: Train 0.5 | Epoch: 293 | Iter: 445400 | Total Loss: 0.005069 | Recon Loss: 0.004079 | Commit Loss: 0.001981 | Perplexity: 1389.635181
2025-09-27 18:42:08,626 Stage: Train 0.5 | Epoch: 293 | Iter: 445600 | Total Loss: 0.005090 | Recon Loss: 0.004094 | Commit Loss: 0.001992 | Perplexity: 1392.113842
2025-09-27 18:42:56,059 Stage: Train 0.5 | Epoch: 293 | Iter: 445800 | Total Loss: 0.005073 | Recon Loss: 0.004080 | Commit Loss: 0.001985 | Perplexity: 1391.965197
2025-09-27 18:43:43,200 Stage: Train 0.5 | Epoch: 293 | Iter: 446000 | Total Loss: 0.005060 | Recon Loss: 0.004071 | Commit Loss: 0.001979 | Perplexity: 1389.752789
2025-09-27 18:44:30,539 Stage: Train 0.5 | Epoch: 293 | Iter: 446200 | Total Loss: 0.005067 | Recon Loss: 0.004071 | Commit Loss: 0.001991 | Perplexity: 1391.879492
2025-09-27 18:45:17,898 Stage: Train 0.5 | Epoch: 293 | Iter: 446400 | Total Loss: 0.005075 | Recon Loss: 0.004085 | Commit Loss: 0.001980 | Perplexity: 1390.865898
Trainning Epoch:  89%|████████▉ | 294/330 [28:34:52<3:35:38, 359.39s/it]2025-09-27 18:46:05,488 Stage: Train 0.5 | Epoch: 294 | Iter: 446600 | Total Loss: 0.005115 | Recon Loss: 0.004118 | Commit Loss: 0.001994 | Perplexity: 1392.366882
2025-09-27 18:46:52,928 Stage: Train 0.5 | Epoch: 294 | Iter: 446800 | Total Loss: 0.005041 | Recon Loss: 0.004050 | Commit Loss: 0.001981 | Perplexity: 1392.472976
2025-09-27 18:47:40,302 Stage: Train 0.5 | Epoch: 294 | Iter: 447000 | Total Loss: 0.005098 | Recon Loss: 0.004100 | Commit Loss: 0.001995 | Perplexity: 1394.140281
2025-09-27 18:48:27,639 Stage: Train 0.5 | Epoch: 294 | Iter: 447200 | Total Loss: 0.005050 | Recon Loss: 0.004057 | Commit Loss: 0.001985 | Perplexity: 1393.640379
2025-09-27 18:49:14,969 Stage: Train 0.5 | Epoch: 294 | Iter: 447400 | Total Loss: 0.005090 | Recon Loss: 0.004098 | Commit Loss: 0.001983 | Perplexity: 1389.774873
2025-09-27 18:50:02,569 Stage: Train 0.5 | Epoch: 294 | Iter: 447600 | Total Loss: 0.005094 | Recon Loss: 0.004098 | Commit Loss: 0.001992 | Perplexity: 1392.864144
2025-09-27 18:50:50,135 Stage: Train 0.5 | Epoch: 294 | Iter: 447800 | Total Loss: 0.005044 | Recon Loss: 0.004047 | Commit Loss: 0.001994 | Perplexity: 1393.220320
2025-09-27 18:51:37,015 Stage: Train 0.5 | Epoch: 294 | Iter: 448000 | Total Loss: 0.005050 | Recon Loss: 0.004058 | Commit Loss: 0.001983 | Perplexity: 1393.595612
Trainning Epoch:  89%|████████▉ | 295/330 [28:40:52<3:29:44, 359.56s/it]2025-09-27 18:52:24,509 Stage: Train 0.5 | Epoch: 295 | Iter: 448200 | Total Loss: 0.005119 | Recon Loss: 0.004126 | Commit Loss: 0.001986 | Perplexity: 1391.513461
2025-09-27 18:53:11,619 Stage: Train 0.5 | Epoch: 295 | Iter: 448400 | Total Loss: 0.005044 | Recon Loss: 0.004056 | Commit Loss: 0.001977 | Perplexity: 1390.354432
2025-09-27 18:53:58,796 Stage: Train 0.5 | Epoch: 295 | Iter: 448600 | Total Loss: 0.005072 | Recon Loss: 0.004081 | Commit Loss: 0.001982 | Perplexity: 1393.161536
2025-09-27 18:54:45,985 Stage: Train 0.5 | Epoch: 295 | Iter: 448800 | Total Loss: 0.005065 | Recon Loss: 0.004078 | Commit Loss: 0.001974 | Perplexity: 1388.469224
2025-09-27 18:55:33,209 Stage: Train 0.5 | Epoch: 295 | Iter: 449000 | Total Loss: 0.005066 | Recon Loss: 0.004073 | Commit Loss: 0.001987 | Perplexity: 1394.591428
2025-09-27 18:56:20,259 Stage: Train 0.5 | Epoch: 295 | Iter: 449200 | Total Loss: 0.005089 | Recon Loss: 0.004096 | Commit Loss: 0.001985 | Perplexity: 1394.852456
2025-09-27 18:57:07,512 Stage: Train 0.5 | Epoch: 295 | Iter: 449400 | Total Loss: 0.005104 | Recon Loss: 0.004109 | Commit Loss: 0.001989 | Perplexity: 1393.154879
2025-09-27 18:57:54,557 Stage: Train 0.5 | Epoch: 295 | Iter: 449600 | Total Loss: 0.005089 | Recon Loss: 0.004096 | Commit Loss: 0.001985 | Perplexity: 1393.472166
Trainning Epoch:  90%|████████▉ | 296/330 [28:46:50<3:23:32, 359.19s/it]2025-09-27 18:58:41,896 Stage: Train 0.5 | Epoch: 296 | Iter: 449800 | Total Loss: 0.005012 | Recon Loss: 0.004026 | Commit Loss: 0.001973 | Perplexity: 1394.054110
2025-09-27 18:59:29,321 Stage: Train 0.5 | Epoch: 296 | Iter: 450000 | Total Loss: 0.005106 | Recon Loss: 0.004114 | Commit Loss: 0.001984 | Perplexity: 1395.193417
2025-09-27 19:00:16,720 Stage: Train 0.5 | Epoch: 296 | Iter: 450200 | Total Loss: 0.005056 | Recon Loss: 0.004064 | Commit Loss: 0.001983 | Perplexity: 1393.083589
2025-09-27 19:01:04,128 Stage: Train 0.5 | Epoch: 296 | Iter: 450400 | Total Loss: 0.005038 | Recon Loss: 0.004049 | Commit Loss: 0.001979 | Perplexity: 1392.556046
2025-09-27 19:01:51,501 Stage: Train 0.5 | Epoch: 296 | Iter: 450600 | Total Loss: 0.005077 | Recon Loss: 0.004085 | Commit Loss: 0.001985 | Perplexity: 1394.431632
2025-09-27 19:02:38,939 Stage: Train 0.5 | Epoch: 296 | Iter: 450800 | Total Loss: 0.005047 | Recon Loss: 0.004054 | Commit Loss: 0.001987 | Perplexity: 1393.267331
2025-09-27 19:03:26,257 Stage: Train 0.5 | Epoch: 296 | Iter: 451000 | Total Loss: 0.005094 | Recon Loss: 0.004102 | Commit Loss: 0.001985 | Perplexity: 1394.789476
Trainning Epoch:  90%|█████████ | 297/330 [28:52:50<3:17:40, 359.41s/it]2025-09-27 19:04:13,842 Stage: Train 0.5 | Epoch: 297 | Iter: 451200 | Total Loss: 0.005023 | Recon Loss: 0.004035 | Commit Loss: 0.001977 | Perplexity: 1394.419586
2025-09-27 19:05:01,242 Stage: Train 0.5 | Epoch: 297 | Iter: 451400 | Total Loss: 0.005015 | Recon Loss: 0.004025 | Commit Loss: 0.001981 | Perplexity: 1394.188674
2025-09-27 19:05:48,344 Stage: Train 0.5 | Epoch: 297 | Iter: 451600 | Total Loss: 0.005070 | Recon Loss: 0.004079 | Commit Loss: 0.001982 | Perplexity: 1395.403576
2025-09-27 19:06:35,537 Stage: Train 0.5 | Epoch: 297 | Iter: 451800 | Total Loss: 0.005046 | Recon Loss: 0.004051 | Commit Loss: 0.001988 | Perplexity: 1393.949094
2025-09-27 19:07:22,960 Stage: Train 0.5 | Epoch: 297 | Iter: 452000 | Total Loss: 0.005097 | Recon Loss: 0.004107 | Commit Loss: 0.001981 | Perplexity: 1393.639180
2025-09-27 19:08:10,227 Stage: Train 0.5 | Epoch: 297 | Iter: 452200 | Total Loss: 0.005050 | Recon Loss: 0.004054 | Commit Loss: 0.001992 | Perplexity: 1394.851656
2025-09-27 19:08:57,546 Stage: Train 0.5 | Epoch: 297 | Iter: 452400 | Total Loss: 0.005045 | Recon Loss: 0.004053 | Commit Loss: 0.001983 | Perplexity: 1393.025900
2025-09-27 19:09:44,798 Stage: Train 0.5 | Epoch: 297 | Iter: 452600 | Total Loss: 0.005071 | Recon Loss: 0.004076 | Commit Loss: 0.001989 | Perplexity: 1393.756601
Trainning Epoch:  90%|█████████ | 298/330 [28:58:50<3:11:40, 359.40s/it]2025-09-27 19:10:32,351 Stage: Train 0.5 | Epoch: 298 | Iter: 452800 | Total Loss: 0.004992 | Recon Loss: 0.004005 | Commit Loss: 0.001974 | Perplexity: 1393.217347
2025-09-27 19:11:19,668 Stage: Train 0.5 | Epoch: 298 | Iter: 453000 | Total Loss: 0.005085 | Recon Loss: 0.004095 | Commit Loss: 0.001981 | Perplexity: 1394.956577
2025-09-27 19:12:06,920 Stage: Train 0.5 | Epoch: 298 | Iter: 453200 | Total Loss: 0.005069 | Recon Loss: 0.004081 | Commit Loss: 0.001976 | Perplexity: 1392.658723
2025-09-27 19:12:53,851 Stage: Train 0.5 | Epoch: 298 | Iter: 453400 | Total Loss: 0.005030 | Recon Loss: 0.004037 | Commit Loss: 0.001985 | Perplexity: 1396.200934
2025-09-27 19:13:41,224 Stage: Train 0.5 | Epoch: 298 | Iter: 453600 | Total Loss: 0.005067 | Recon Loss: 0.004073 | Commit Loss: 0.001986 | Perplexity: 1395.307846
2025-09-27 19:14:28,552 Stage: Train 0.5 | Epoch: 298 | Iter: 453800 | Total Loss: 0.005098 | Recon Loss: 0.004107 | Commit Loss: 0.001983 | Perplexity: 1393.494704
2025-09-27 19:15:15,806 Stage: Train 0.5 | Epoch: 298 | Iter: 454000 | Total Loss: 0.005014 | Recon Loss: 0.004020 | Commit Loss: 0.001988 | Perplexity: 1394.101479
Trainning Epoch:  91%|█████████ | 299/330 [29:04:49<3:05:39, 359.32s/it]2025-09-27 19:16:03,374 Stage: Train 0.5 | Epoch: 299 | Iter: 454200 | Total Loss: 0.005071 | Recon Loss: 0.004079 | Commit Loss: 0.001983 | Perplexity: 1394.070065
2025-09-27 19:16:50,682 Stage: Train 0.5 | Epoch: 299 | Iter: 454400 | Total Loss: 0.005035 | Recon Loss: 0.004044 | Commit Loss: 0.001982 | Perplexity: 1396.363781
2025-09-27 19:17:37,918 Stage: Train 0.5 | Epoch: 299 | Iter: 454600 | Total Loss: 0.005041 | Recon Loss: 0.004047 | Commit Loss: 0.001986 | Perplexity: 1396.313633
2025-09-27 19:18:25,302 Stage: Train 0.5 | Epoch: 299 | Iter: 454800 | Total Loss: 0.005082 | Recon Loss: 0.004092 | Commit Loss: 0.001980 | Perplexity: 1392.971734
2025-09-27 19:19:12,546 Stage: Train 0.5 | Epoch: 299 | Iter: 455000 | Total Loss: 0.005054 | Recon Loss: 0.004066 | Commit Loss: 0.001976 | Perplexity: 1392.393542
2025-09-27 19:19:59,397 Stage: Train 0.5 | Epoch: 299 | Iter: 455200 | Total Loss: 0.005056 | Recon Loss: 0.004064 | Commit Loss: 0.001985 | Perplexity: 1395.508296
2025-09-27 19:20:46,690 Stage: Train 0.5 | Epoch: 299 | Iter: 455400 | Total Loss: 0.005033 | Recon Loss: 0.004043 | Commit Loss: 0.001980 | Perplexity: 1393.003736
2025-09-27 19:21:34,106 Stage: Train 0.5 | Epoch: 299 | Iter: 455600 | Total Loss: 0.005074 | Recon Loss: 0.004085 | Commit Loss: 0.001976 | Perplexity: 1395.522859
Trainning Epoch:  91%|█████████ | 300/330 [29:10:48<2:59:37, 359.25s/it]2025-09-27 19:22:21,631 Stage: Train 0.5 | Epoch: 300 | Iter: 455800 | Total Loss: 0.005053 | Recon Loss: 0.004060 | Commit Loss: 0.001986 | Perplexity: 1395.291029
2025-09-27 19:23:08,837 Stage: Train 0.5 | Epoch: 300 | Iter: 456000 | Total Loss: 0.005076 | Recon Loss: 0.004086 | Commit Loss: 0.001978 | Perplexity: 1396.468372
2025-09-27 19:23:56,120 Stage: Train 0.5 | Epoch: 300 | Iter: 456200 | Total Loss: 0.005061 | Recon Loss: 0.004069 | Commit Loss: 0.001986 | Perplexity: 1395.288492
2025-09-27 19:24:43,276 Stage: Train 0.5 | Epoch: 300 | Iter: 456400 | Total Loss: 0.004981 | Recon Loss: 0.003994 | Commit Loss: 0.001975 | Perplexity: 1393.920388
2025-09-27 19:25:30,655 Stage: Train 0.5 | Epoch: 300 | Iter: 456600 | Total Loss: 0.005047 | Recon Loss: 0.004056 | Commit Loss: 0.001982 | Perplexity: 1396.133323
2025-09-27 19:26:18,010 Stage: Train 0.5 | Epoch: 300 | Iter: 456800 | Total Loss: 0.005058 | Recon Loss: 0.004068 | Commit Loss: 0.001980 | Perplexity: 1396.382795
2025-09-27 19:27:05,344 Stage: Train 0.5 | Epoch: 300 | Iter: 457000 | Total Loss: 0.005083 | Recon Loss: 0.004095 | Commit Loss: 0.001975 | Perplexity: 1395.128853
2025-09-27 19:27:52,234 Stage: Train 0.5 | Epoch: 300 | Iter: 457200 | Total Loss: 0.004999 | Recon Loss: 0.004009 | Commit Loss: 0.001981 | Perplexity: 1396.290465
Trainning Epoch:  91%|█████████ | 301/330 [29:16:47<2:53:36, 359.19s/it]2025-09-27 19:28:39,752 Stage: Train 0.5 | Epoch: 301 | Iter: 457400 | Total Loss: 0.005057 | Recon Loss: 0.004069 | Commit Loss: 0.001977 | Perplexity: 1392.674200
2025-09-27 19:29:26,908 Stage: Train 0.5 | Epoch: 301 | Iter: 457600 | Total Loss: 0.005028 | Recon Loss: 0.004039 | Commit Loss: 0.001977 | Perplexity: 1394.932566
2025-09-27 19:30:12,596 Stage: Train 0.5 | Epoch: 301 | Iter: 457800 | Total Loss: 0.005009 | Recon Loss: 0.004019 | Commit Loss: 0.001979 | Perplexity: 1396.860288
2025-09-27 19:30:59,794 Stage: Train 0.5 | Epoch: 301 | Iter: 458000 | Total Loss: 0.005051 | Recon Loss: 0.004061 | Commit Loss: 0.001980 | Perplexity: 1394.296620
2025-09-27 19:31:46,638 Stage: Train 0.5 | Epoch: 301 | Iter: 458200 | Total Loss: 0.005076 | Recon Loss: 0.004086 | Commit Loss: 0.001979 | Perplexity: 1393.597111
2025-09-27 19:32:33,734 Stage: Train 0.5 | Epoch: 301 | Iter: 458400 | Total Loss: 0.005060 | Recon Loss: 0.004073 | Commit Loss: 0.001976 | Perplexity: 1394.981273
2025-09-27 19:33:20,875 Stage: Train 0.5 | Epoch: 301 | Iter: 458600 | Total Loss: 0.005034 | Recon Loss: 0.004043 | Commit Loss: 0.001983 | Perplexity: 1394.606171
Trainning Epoch:  92%|█████████▏| 302/330 [29:22:44<2:47:16, 358.43s/it]2025-09-27 19:34:08,286 Stage: Train 0.5 | Epoch: 302 | Iter: 458800 | Total Loss: 0.005058 | Recon Loss: 0.004068 | Commit Loss: 0.001981 | Perplexity: 1393.997922
2025-09-27 19:34:55,145 Stage: Train 0.5 | Epoch: 302 | Iter: 459000 | Total Loss: 0.005016 | Recon Loss: 0.004033 | Commit Loss: 0.001965 | Perplexity: 1394.390802
2025-09-27 19:35:42,224 Stage: Train 0.5 | Epoch: 302 | Iter: 459200 | Total Loss: 0.005043 | Recon Loss: 0.004056 | Commit Loss: 0.001974 | Perplexity: 1395.324820
2025-09-27 19:36:29,470 Stage: Train 0.5 | Epoch: 302 | Iter: 459400 | Total Loss: 0.005057 | Recon Loss: 0.004068 | Commit Loss: 0.001978 | Perplexity: 1395.157211
2025-09-27 19:37:16,670 Stage: Train 0.5 | Epoch: 302 | Iter: 459600 | Total Loss: 0.005086 | Recon Loss: 0.004096 | Commit Loss: 0.001980 | Perplexity: 1395.120505
2025-09-27 19:38:03,720 Stage: Train 0.5 | Epoch: 302 | Iter: 459800 | Total Loss: 0.005010 | Recon Loss: 0.004021 | Commit Loss: 0.001979 | Perplexity: 1396.564114
2025-09-27 19:38:51,069 Stage: Train 0.5 | Epoch: 302 | Iter: 460000 | Total Loss: 0.005048 | Recon Loss: 0.004056 | Commit Loss: 0.001985 | Perplexity: 1395.021807
2025-09-27 19:38:51,069 Saving model at iteration 460000
2025-09-27 19:38:51,279 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000
2025-09-27 19:38:51,557 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/model.safetensors
2025-09-27 19:38:51,938 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/optimizer.bin
2025-09-27 19:38:51,939 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/scheduler.bin
2025-09-27 19:38:51,939 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/sampler.bin
2025-09-27 19:38:51,940 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/random_states_0.pkl
2025-09-27 19:39:39,436 Stage: Train 0.5 | Epoch: 302 | Iter: 460200 | Total Loss: 0.005034 | Recon Loss: 0.004046 | Commit Loss: 0.001976 | Perplexity: 1395.446810
Trainning Epoch:  92%|█████████▏| 303/330 [29:28:43<2:41:25, 358.74s/it]2025-09-27 19:40:26,841 Stage: Train 0.5 | Epoch: 303 | Iter: 460400 | Total Loss: 0.005026 | Recon Loss: 0.004035 | Commit Loss: 0.001983 | Perplexity: 1393.094864
2025-09-27 19:41:13,931 Stage: Train 0.5 | Epoch: 303 | Iter: 460600 | Total Loss: 0.005066 | Recon Loss: 0.004077 | Commit Loss: 0.001979 | Perplexity: 1393.476723
2025-09-27 19:42:00,993 Stage: Train 0.5 | Epoch: 303 | Iter: 460800 | Total Loss: 0.005019 | Recon Loss: 0.004034 | Commit Loss: 0.001970 | Perplexity: 1393.740160
2025-09-27 19:42:48,350 Stage: Train 0.5 | Epoch: 303 | Iter: 461000 | Total Loss: 0.005051 | Recon Loss: 0.004060 | Commit Loss: 0.001982 | Perplexity: 1395.176155
2025-09-27 19:43:35,715 Stage: Train 0.5 | Epoch: 303 | Iter: 461200 | Total Loss: 0.005066 | Recon Loss: 0.004072 | Commit Loss: 0.001988 | Perplexity: 1394.020618
2025-09-27 19:44:22,924 Stage: Train 0.5 | Epoch: 303 | Iter: 461400 | Total Loss: 0.005014 | Recon Loss: 0.004028 | Commit Loss: 0.001972 | Perplexity: 1393.565191
2025-09-27 19:45:10,044 Stage: Train 0.5 | Epoch: 303 | Iter: 461600 | Total Loss: 0.005032 | Recon Loss: 0.004040 | Commit Loss: 0.001984 | Perplexity: 1396.703503
Trainning Epoch:  92%|█████████▏| 304/330 [29:34:42<2:35:27, 358.75s/it]2025-09-27 19:45:57,588 Stage: Train 0.5 | Epoch: 304 | Iter: 461800 | Total Loss: 0.005052 | Recon Loss: 0.004063 | Commit Loss: 0.001977 | Perplexity: 1394.280659
2025-09-27 19:46:44,855 Stage: Train 0.5 | Epoch: 304 | Iter: 462000 | Total Loss: 0.005030 | Recon Loss: 0.004044 | Commit Loss: 0.001972 | Perplexity: 1393.991622
2025-09-27 19:47:32,179 Stage: Train 0.5 | Epoch: 304 | Iter: 462200 | Total Loss: 0.005037 | Recon Loss: 0.004049 | Commit Loss: 0.001976 | Perplexity: 1395.974211
2025-09-27 19:48:19,562 Stage: Train 0.5 | Epoch: 304 | Iter: 462400 | Total Loss: 0.005034 | Recon Loss: 0.004049 | Commit Loss: 0.001972 | Perplexity: 1394.134246
2025-09-27 19:49:06,496 Stage: Train 0.5 | Epoch: 304 | Iter: 462600 | Total Loss: 0.005041 | Recon Loss: 0.004052 | Commit Loss: 0.001978 | Perplexity: 1394.794121
2025-09-27 19:49:53,733 Stage: Train 0.5 | Epoch: 304 | Iter: 462800 | Total Loss: 0.005026 | Recon Loss: 0.004041 | Commit Loss: 0.001970 | Perplexity: 1396.797113
2025-09-27 19:50:41,086 Stage: Train 0.5 | Epoch: 304 | Iter: 463000 | Total Loss: 0.005028 | Recon Loss: 0.004042 | Commit Loss: 0.001972 | Perplexity: 1393.643812
2025-09-27 19:51:28,502 Stage: Train 0.5 | Epoch: 304 | Iter: 463200 | Total Loss: 0.005040 | Recon Loss: 0.004052 | Commit Loss: 0.001976 | Perplexity: 1393.589772
Trainning Epoch:  92%|█████████▏| 305/330 [29:40:41<2:29:33, 358.92s/it]2025-09-27 19:52:16,038 Stage: Train 0.5 | Epoch: 305 | Iter: 463400 | Total Loss: 0.005081 | Recon Loss: 0.004090 | Commit Loss: 0.001982 | Perplexity: 1394.277588
2025-09-27 19:53:03,352 Stage: Train 0.5 | Epoch: 305 | Iter: 463600 | Total Loss: 0.005017 | Recon Loss: 0.004030 | Commit Loss: 0.001973 | Perplexity: 1394.420498
2025-09-27 19:53:50,701 Stage: Train 0.5 | Epoch: 305 | Iter: 463800 | Total Loss: 0.004990 | Recon Loss: 0.004006 | Commit Loss: 0.001970 | Perplexity: 1394.924084
2025-09-27 19:54:38,168 Stage: Train 0.5 | Epoch: 305 | Iter: 464000 | Total Loss: 0.004980 | Recon Loss: 0.003994 | Commit Loss: 0.001972 | Perplexity: 1394.984494
2025-09-27 19:55:25,551 Stage: Train 0.5 | Epoch: 305 | Iter: 464200 | Total Loss: 0.005032 | Recon Loss: 0.004038 | Commit Loss: 0.001989 | Perplexity: 1396.174142
2025-09-27 19:56:12,646 Stage: Train 0.5 | Epoch: 305 | Iter: 464400 | Total Loss: 0.005011 | Recon Loss: 0.004024 | Commit Loss: 0.001974 | Perplexity: 1395.657496
2025-09-27 19:57:00,048 Stage: Train 0.5 | Epoch: 305 | Iter: 464600 | Total Loss: 0.005029 | Recon Loss: 0.004042 | Commit Loss: 0.001975 | Perplexity: 1393.979727
2025-09-27 19:57:47,433 Stage: Train 0.5 | Epoch: 305 | Iter: 464800 | Total Loss: 0.005047 | Recon Loss: 0.004060 | Commit Loss: 0.001974 | Perplexity: 1396.368062
Trainning Epoch:  93%|█████████▎| 306/330 [29:46:41<2:23:40, 359.18s/it]2025-09-27 19:58:34,379 Stage: Train 0.5 | Epoch: 306 | Iter: 465000 | Total Loss: 0.005031 | Recon Loss: 0.004042 | Commit Loss: 0.001978 | Perplexity: 1395.577960
2025-09-27 19:59:21,588 Stage: Train 0.5 | Epoch: 306 | Iter: 465200 | Total Loss: 0.005047 | Recon Loss: 0.004058 | Commit Loss: 0.001977 | Perplexity: 1395.501475
2025-09-27 20:00:08,649 Stage: Train 0.5 | Epoch: 306 | Iter: 465400 | Total Loss: 0.005004 | Recon Loss: 0.004019 | Commit Loss: 0.001969 | Perplexity: 1392.051232
2025-09-27 20:00:55,686 Stage: Train 0.5 | Epoch: 306 | Iter: 465600 | Total Loss: 0.005009 | Recon Loss: 0.004023 | Commit Loss: 0.001971 | Perplexity: 1392.796208
2025-09-27 20:01:43,008 Stage: Train 0.5 | Epoch: 306 | Iter: 465800 | Total Loss: 0.005055 | Recon Loss: 0.004067 | Commit Loss: 0.001976 | Perplexity: 1394.560006
2025-09-27 20:02:30,385 Stage: Train 0.5 | Epoch: 306 | Iter: 466000 | Total Loss: 0.005016 | Recon Loss: 0.004029 | Commit Loss: 0.001973 | Perplexity: 1395.405052
2025-09-27 20:03:17,613 Stage: Train 0.5 | Epoch: 306 | Iter: 466200 | Total Loss: 0.005022 | Recon Loss: 0.004029 | Commit Loss: 0.001985 | Perplexity: 1395.383290
Trainning Epoch:  93%|█████████▎| 307/330 [29:52:39<2:17:34, 358.91s/it]2025-09-27 20:04:05,066 Stage: Train 0.5 | Epoch: 307 | Iter: 466400 | Total Loss: 0.005015 | Recon Loss: 0.004031 | Commit Loss: 0.001968 | Perplexity: 1394.260604
2025-09-27 20:04:52,082 Stage: Train 0.5 | Epoch: 307 | Iter: 466600 | Total Loss: 0.005018 | Recon Loss: 0.004034 | Commit Loss: 0.001968 | Perplexity: 1395.626236
2025-09-27 20:05:39,184 Stage: Train 0.5 | Epoch: 307 | Iter: 466800 | Total Loss: 0.005091 | Recon Loss: 0.004106 | Commit Loss: 0.001971 | Perplexity: 1395.196566
2025-09-27 20:06:26,294 Stage: Train 0.5 | Epoch: 307 | Iter: 467000 | Total Loss: 0.004995 | Recon Loss: 0.004005 | Commit Loss: 0.001980 | Perplexity: 1396.647181
2025-09-27 20:07:13,390 Stage: Train 0.5 | Epoch: 307 | Iter: 467200 | Total Loss: 0.005025 | Recon Loss: 0.004040 | Commit Loss: 0.001970 | Perplexity: 1394.987776
2025-09-27 20:08:00,643 Stage: Train 0.5 | Epoch: 307 | Iter: 467400 | Total Loss: 0.005012 | Recon Loss: 0.004023 | Commit Loss: 0.001978 | Perplexity: 1395.604558
2025-09-27 20:08:47,858 Stage: Train 0.5 | Epoch: 307 | Iter: 467600 | Total Loss: 0.004997 | Recon Loss: 0.004011 | Commit Loss: 0.001971 | Perplexity: 1393.491818
2025-09-27 20:09:35,128 Stage: Train 0.5 | Epoch: 307 | Iter: 467800 | Total Loss: 0.005006 | Recon Loss: 0.004018 | Commit Loss: 0.001976 | Perplexity: 1396.211328
Trainning Epoch:  93%|█████████▎| 308/330 [29:58:38<2:11:32, 358.76s/it]2025-09-27 20:10:22,595 Stage: Train 0.5 | Epoch: 308 | Iter: 468000 | Total Loss: 0.005071 | Recon Loss: 0.004086 | Commit Loss: 0.001971 | Perplexity: 1395.280679
2025-09-27 20:11:08,833 Stage: Train 0.5 | Epoch: 308 | Iter: 468200 | Total Loss: 0.004996 | Recon Loss: 0.004012 | Commit Loss: 0.001968 | Perplexity: 1392.908450
2025-09-27 20:11:56,078 Stage: Train 0.5 | Epoch: 308 | Iter: 468400 | Total Loss: 0.005010 | Recon Loss: 0.004023 | Commit Loss: 0.001974 | Perplexity: 1394.976525
2025-09-27 20:12:43,370 Stage: Train 0.5 | Epoch: 308 | Iter: 468600 | Total Loss: 0.004996 | Recon Loss: 0.004008 | Commit Loss: 0.001976 | Perplexity: 1395.387960
2025-09-27 20:13:30,527 Stage: Train 0.5 | Epoch: 308 | Iter: 468800 | Total Loss: 0.005052 | Recon Loss: 0.004066 | Commit Loss: 0.001972 | Perplexity: 1398.399861
2025-09-27 20:14:17,931 Stage: Train 0.5 | Epoch: 308 | Iter: 469000 | Total Loss: 0.005003 | Recon Loss: 0.004018 | Commit Loss: 0.001971 | Perplexity: 1397.150945
2025-09-27 20:15:05,356 Stage: Train 0.5 | Epoch: 308 | Iter: 469200 | Total Loss: 0.005019 | Recon Loss: 0.004034 | Commit Loss: 0.001969 | Perplexity: 1394.044839
Trainning Epoch:  94%|█████████▎| 309/330 [30:04:36<2:05:31, 358.65s/it]2025-09-27 20:15:52,930 Stage: Train 0.5 | Epoch: 309 | Iter: 469400 | Total Loss: 0.005051 | Recon Loss: 0.004066 | Commit Loss: 0.001970 | Perplexity: 1393.402706
2025-09-27 20:16:40,209 Stage: Train 0.5 | Epoch: 309 | Iter: 469600 | Total Loss: 0.004976 | Recon Loss: 0.003990 | Commit Loss: 0.001972 | Perplexity: 1392.612089
2025-09-27 20:17:27,581 Stage: Train 0.5 | Epoch: 309 | Iter: 469800 | Total Loss: 0.005030 | Recon Loss: 0.004047 | Commit Loss: 0.001967 | Perplexity: 1394.564444
2025-09-27 20:18:14,512 Stage: Train 0.5 | Epoch: 309 | Iter: 470000 | Total Loss: 0.005027 | Recon Loss: 0.004039 | Commit Loss: 0.001976 | Perplexity: 1397.075153
2025-09-27 20:19:01,887 Stage: Train 0.5 | Epoch: 309 | Iter: 470200 | Total Loss: 0.004987 | Recon Loss: 0.004005 | Commit Loss: 0.001966 | Perplexity: 1394.965074
2025-09-27 20:19:49,249 Stage: Train 0.5 | Epoch: 309 | Iter: 470400 | Total Loss: 0.005024 | Recon Loss: 0.004036 | Commit Loss: 0.001975 | Perplexity: 1397.352856
2025-09-27 20:20:36,511 Stage: Train 0.5 | Epoch: 309 | Iter: 470600 | Total Loss: 0.005072 | Recon Loss: 0.004082 | Commit Loss: 0.001980 | Perplexity: 1396.997237
2025-09-27 20:21:23,806 Stage: Train 0.5 | Epoch: 309 | Iter: 470800 | Total Loss: 0.005012 | Recon Loss: 0.004026 | Commit Loss: 0.001973 | Perplexity: 1393.795259
Trainning Epoch:  94%|█████████▍| 310/330 [30:10:35<1:59:36, 358.84s/it]2025-09-27 20:22:11,414 Stage: Train 0.5 | Epoch: 310 | Iter: 471000 | Total Loss: 0.005035 | Recon Loss: 0.004055 | Commit Loss: 0.001961 | Perplexity: 1394.947575
2025-09-27 20:22:58,682 Stage: Train 0.5 | Epoch: 310 | Iter: 471200 | Total Loss: 0.005012 | Recon Loss: 0.004030 | Commit Loss: 0.001963 | Perplexity: 1393.806556
2025-09-27 20:23:45,938 Stage: Train 0.5 | Epoch: 310 | Iter: 471400 | Total Loss: 0.005017 | Recon Loss: 0.004029 | Commit Loss: 0.001976 | Perplexity: 1395.387587
2025-09-27 20:24:33,257 Stage: Train 0.5 | Epoch: 310 | Iter: 471600 | Total Loss: 0.005003 | Recon Loss: 0.004018 | Commit Loss: 0.001971 | Perplexity: 1394.179801
2025-09-27 20:25:20,291 Stage: Train 0.5 | Epoch: 310 | Iter: 471800 | Total Loss: 0.005007 | Recon Loss: 0.004017 | Commit Loss: 0.001979 | Perplexity: 1397.074332
2025-09-27 20:26:07,492 Stage: Train 0.5 | Epoch: 310 | Iter: 472000 | Total Loss: 0.004985 | Recon Loss: 0.004002 | Commit Loss: 0.001966 | Perplexity: 1394.595736
2025-09-27 20:26:54,871 Stage: Train 0.5 | Epoch: 310 | Iter: 472200 | Total Loss: 0.005055 | Recon Loss: 0.004070 | Commit Loss: 0.001970 | Perplexity: 1396.291795
2025-09-27 20:27:42,021 Stage: Train 0.5 | Epoch: 310 | Iter: 472400 | Total Loss: 0.005005 | Recon Loss: 0.004018 | Commit Loss: 0.001975 | Perplexity: 1395.536982
Trainning Epoch:  94%|█████████▍| 311/330 [30:16:34<1:53:39, 358.91s/it]2025-09-27 20:28:29,399 Stage: Train 0.5 | Epoch: 311 | Iter: 472600 | Total Loss: 0.005011 | Recon Loss: 0.004026 | Commit Loss: 0.001970 | Perplexity: 1396.138757
2025-09-27 20:29:16,631 Stage: Train 0.5 | Epoch: 311 | Iter: 472800 | Total Loss: 0.004996 | Recon Loss: 0.004017 | Commit Loss: 0.001958 | Perplexity: 1394.478032
2025-09-27 20:30:03,842 Stage: Train 0.5 | Epoch: 311 | Iter: 473000 | Total Loss: 0.005013 | Recon Loss: 0.004029 | Commit Loss: 0.001969 | Perplexity: 1396.655311
2025-09-27 20:30:50,858 Stage: Train 0.5 | Epoch: 311 | Iter: 473200 | Total Loss: 0.004982 | Recon Loss: 0.003998 | Commit Loss: 0.001968 | Perplexity: 1395.166240
2025-09-27 20:31:38,178 Stage: Train 0.5 | Epoch: 311 | Iter: 473400 | Total Loss: 0.005001 | Recon Loss: 0.004014 | Commit Loss: 0.001976 | Perplexity: 1397.852187
2025-09-27 20:32:25,172 Stage: Train 0.5 | Epoch: 311 | Iter: 473600 | Total Loss: 0.005055 | Recon Loss: 0.004071 | Commit Loss: 0.001970 | Perplexity: 1394.247031
2025-09-27 20:33:12,481 Stage: Train 0.5 | Epoch: 311 | Iter: 473800 | Total Loss: 0.004986 | Recon Loss: 0.004002 | Commit Loss: 0.001968 | Perplexity: 1395.584131
Trainning Epoch:  95%|█████████▍| 312/330 [30:22:33<1:47:38, 358.78s/it]2025-09-27 20:33:59,960 Stage: Train 0.5 | Epoch: 312 | Iter: 474000 | Total Loss: 0.005006 | Recon Loss: 0.004020 | Commit Loss: 0.001972 | Perplexity: 1396.262475
2025-09-27 20:34:47,303 Stage: Train 0.5 | Epoch: 312 | Iter: 474200 | Total Loss: 0.004977 | Recon Loss: 0.003994 | Commit Loss: 0.001966 | Perplexity: 1395.533521
2025-09-27 20:35:34,618 Stage: Train 0.5 | Epoch: 312 | Iter: 474400 | Total Loss: 0.005012 | Recon Loss: 0.004025 | Commit Loss: 0.001973 | Perplexity: 1398.893961
2025-09-27 20:36:21,760 Stage: Train 0.5 | Epoch: 312 | Iter: 474600 | Total Loss: 0.005000 | Recon Loss: 0.004019 | Commit Loss: 0.001962 | Perplexity: 1394.645010
2025-09-27 20:37:08,985 Stage: Train 0.5 | Epoch: 312 | Iter: 474800 | Total Loss: 0.004991 | Recon Loss: 0.004004 | Commit Loss: 0.001973 | Perplexity: 1394.931476
2025-09-27 20:37:56,199 Stage: Train 0.5 | Epoch: 312 | Iter: 475000 | Total Loss: 0.005057 | Recon Loss: 0.004072 | Commit Loss: 0.001970 | Perplexity: 1398.173234
2025-09-27 20:38:43,177 Stage: Train 0.5 | Epoch: 312 | Iter: 475200 | Total Loss: 0.004999 | Recon Loss: 0.004014 | Commit Loss: 0.001971 | Perplexity: 1397.873917
2025-09-27 20:39:30,077 Stage: Train 0.5 | Epoch: 312 | Iter: 475400 | Total Loss: 0.005007 | Recon Loss: 0.004021 | Commit Loss: 0.001971 | Perplexity: 1396.164178
Trainning Epoch:  95%|█████████▍| 313/330 [30:28:31<1:41:38, 358.71s/it]2025-09-27 20:40:17,503 Stage: Train 0.5 | Epoch: 313 | Iter: 475600 | Total Loss: 0.004976 | Recon Loss: 0.003993 | Commit Loss: 0.001964 | Perplexity: 1395.374667
2025-09-27 20:41:04,880 Stage: Train 0.5 | Epoch: 313 | Iter: 475800 | Total Loss: 0.005022 | Recon Loss: 0.004038 | Commit Loss: 0.001968 | Perplexity: 1397.208293
2025-09-27 20:41:51,955 Stage: Train 0.5 | Epoch: 313 | Iter: 476000 | Total Loss: 0.004984 | Recon Loss: 0.004002 | Commit Loss: 0.001965 | Perplexity: 1394.736658
2025-09-27 20:42:39,260 Stage: Train 0.5 | Epoch: 313 | Iter: 476200 | Total Loss: 0.004995 | Recon Loss: 0.004011 | Commit Loss: 0.001968 | Perplexity: 1395.834117
2025-09-27 20:43:26,369 Stage: Train 0.5 | Epoch: 313 | Iter: 476400 | Total Loss: 0.005029 | Recon Loss: 0.004048 | Commit Loss: 0.001963 | Perplexity: 1394.060457
2025-09-27 20:44:13,606 Stage: Train 0.5 | Epoch: 313 | Iter: 476600 | Total Loss: 0.005038 | Recon Loss: 0.004050 | Commit Loss: 0.001975 | Perplexity: 1397.370833
2025-09-27 20:45:00,882 Stage: Train 0.5 | Epoch: 313 | Iter: 476800 | Total Loss: 0.004955 | Recon Loss: 0.003975 | Commit Loss: 0.001961 | Perplexity: 1392.910168
Trainning Epoch:  95%|█████████▌| 314/330 [30:34:30<1:35:39, 358.73s/it]2025-09-27 20:45:48,297 Stage: Train 0.5 | Epoch: 314 | Iter: 477000 | Total Loss: 0.005014 | Recon Loss: 0.004027 | Commit Loss: 0.001974 | Perplexity: 1400.629516
2025-09-27 20:46:35,616 Stage: Train 0.5 | Epoch: 314 | Iter: 477200 | Total Loss: 0.004984 | Recon Loss: 0.004001 | Commit Loss: 0.001966 | Perplexity: 1396.122802
2025-09-27 20:47:22,555 Stage: Train 0.5 | Epoch: 314 | Iter: 477400 | Total Loss: 0.005018 | Recon Loss: 0.004035 | Commit Loss: 0.001967 | Perplexity: 1395.784138
2025-09-27 20:48:09,935 Stage: Train 0.5 | Epoch: 314 | Iter: 477600 | Total Loss: 0.004985 | Recon Loss: 0.004001 | Commit Loss: 0.001967 | Perplexity: 1395.558497
2025-09-27 20:48:57,068 Stage: Train 0.5 | Epoch: 314 | Iter: 477800 | Total Loss: 0.005022 | Recon Loss: 0.004039 | Commit Loss: 0.001966 | Perplexity: 1397.857382
2025-09-27 20:49:44,350 Stage: Train 0.5 | Epoch: 314 | Iter: 478000 | Total Loss: 0.004984 | Recon Loss: 0.003999 | Commit Loss: 0.001971 | Perplexity: 1396.188216
2025-09-27 20:50:31,587 Stage: Train 0.5 | Epoch: 314 | Iter: 478200 | Total Loss: 0.005015 | Recon Loss: 0.004027 | Commit Loss: 0.001975 | Perplexity: 1395.444983
2025-09-27 20:51:18,947 Stage: Train 0.5 | Epoch: 314 | Iter: 478400 | Total Loss: 0.005015 | Recon Loss: 0.004033 | Commit Loss: 0.001964 | Perplexity: 1395.848020
Trainning Epoch:  95%|█████████▌| 315/330 [30:40:29<1:29:42, 358.83s/it]2025-09-27 20:52:06,598 Stage: Train 0.5 | Epoch: 315 | Iter: 478600 | Total Loss: 0.004990 | Recon Loss: 0.004010 | Commit Loss: 0.001961 | Perplexity: 1394.473130
2025-09-27 20:52:53,755 Stage: Train 0.5 | Epoch: 315 | Iter: 478800 | Total Loss: 0.005009 | Recon Loss: 0.004023 | Commit Loss: 0.001971 | Perplexity: 1397.442976
2025-09-27 20:53:40,937 Stage: Train 0.5 | Epoch: 315 | Iter: 479000 | Total Loss: 0.004981 | Recon Loss: 0.004000 | Commit Loss: 0.001963 | Perplexity: 1397.841746
2025-09-27 20:54:27,953 Stage: Train 0.5 | Epoch: 315 | Iter: 479200 | Total Loss: 0.004990 | Recon Loss: 0.004009 | Commit Loss: 0.001962 | Perplexity: 1393.869558
2025-09-27 20:55:15,092 Stage: Train 0.5 | Epoch: 315 | Iter: 479400 | Total Loss: 0.005026 | Recon Loss: 0.004044 | Commit Loss: 0.001965 | Perplexity: 1397.175458
2025-09-27 20:56:02,245 Stage: Train 0.5 | Epoch: 315 | Iter: 479600 | Total Loss: 0.004952 | Recon Loss: 0.003969 | Commit Loss: 0.001967 | Perplexity: 1396.836698
2025-09-27 20:56:49,609 Stage: Train 0.5 | Epoch: 315 | Iter: 479800 | Total Loss: 0.004996 | Recon Loss: 0.004010 | Commit Loss: 0.001971 | Perplexity: 1396.282900
2025-09-27 20:57:36,927 Stage: Train 0.5 | Epoch: 315 | Iter: 480000 | Total Loss: 0.004952 | Recon Loss: 0.003965 | Commit Loss: 0.001973 | Perplexity: 1396.110991
2025-09-27 20:57:36,927 Saving model at iteration 480000
2025-09-27 20:57:37,162 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000
2025-09-27 20:57:37,461 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/model.safetensors
2025-09-27 20:57:37,840 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/optimizer.bin
2025-09-27 20:57:37,840 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/scheduler.bin
2025-09-27 20:57:37,840 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/sampler.bin
2025-09-27 20:57:37,841 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 316/330 [30:46:29<1:23:47, 359.13s/it]2025-09-27 20:58:25,509 Stage: Train 0.5 | Epoch: 316 | Iter: 480200 | Total Loss: 0.005003 | Recon Loss: 0.004024 | Commit Loss: 0.001959 | Perplexity: 1395.849938
2025-09-27 20:59:12,885 Stage: Train 0.5 | Epoch: 316 | Iter: 480400 | Total Loss: 0.004982 | Recon Loss: 0.004003 | Commit Loss: 0.001959 | Perplexity: 1395.452377
2025-09-27 21:00:00,300 Stage: Train 0.5 | Epoch: 316 | Iter: 480600 | Total Loss: 0.004973 | Recon Loss: 0.003992 | Commit Loss: 0.001963 | Perplexity: 1397.785977
2025-09-27 21:00:47,575 Stage: Train 0.5 | Epoch: 316 | Iter: 480800 | Total Loss: 0.005025 | Recon Loss: 0.004042 | Commit Loss: 0.001966 | Perplexity: 1397.845670
2025-09-27 21:01:34,633 Stage: Train 0.5 | Epoch: 316 | Iter: 481000 | Total Loss: 0.005044 | Recon Loss: 0.004058 | Commit Loss: 0.001972 | Perplexity: 1398.967335
2025-09-27 21:02:22,062 Stage: Train 0.5 | Epoch: 316 | Iter: 481200 | Total Loss: 0.004961 | Recon Loss: 0.003980 | Commit Loss: 0.001964 | Perplexity: 1395.715037
2025-09-27 21:03:09,501 Stage: Train 0.5 | Epoch: 316 | Iter: 481400 | Total Loss: 0.004986 | Recon Loss: 0.004002 | Commit Loss: 0.001967 | Perplexity: 1397.393853
Trainning Epoch:  96%|█████████▌| 317/330 [30:52:29<1:17:51, 359.35s/it]2025-09-27 21:03:57,229 Stage: Train 0.5 | Epoch: 317 | Iter: 481600 | Total Loss: 0.005012 | Recon Loss: 0.004029 | Commit Loss: 0.001965 | Perplexity: 1398.121224
2025-09-27 21:04:44,656 Stage: Train 0.5 | Epoch: 317 | Iter: 481800 | Total Loss: 0.004983 | Recon Loss: 0.004000 | Commit Loss: 0.001966 | Perplexity: 1397.779770
2025-09-27 21:05:30,700 Stage: Train 0.5 | Epoch: 317 | Iter: 482000 | Total Loss: 0.004979 | Recon Loss: 0.003999 | Commit Loss: 0.001961 | Perplexity: 1395.766529
2025-09-27 21:06:18,076 Stage: Train 0.5 | Epoch: 317 | Iter: 482200 | Total Loss: 0.004990 | Recon Loss: 0.004003 | Commit Loss: 0.001974 | Perplexity: 1397.735922
2025-09-27 21:07:05,421 Stage: Train 0.5 | Epoch: 317 | Iter: 482400 | Total Loss: 0.004999 | Recon Loss: 0.004017 | Commit Loss: 0.001964 | Perplexity: 1394.883135
2025-09-27 21:07:52,848 Stage: Train 0.5 | Epoch: 317 | Iter: 482600 | Total Loss: 0.005024 | Recon Loss: 0.004040 | Commit Loss: 0.001969 | Perplexity: 1396.922568
2025-09-27 21:08:39,642 Stage: Train 0.5 | Epoch: 317 | Iter: 482800 | Total Loss: 0.004937 | Recon Loss: 0.003956 | Commit Loss: 0.001964 | Perplexity: 1398.046326
2025-09-27 21:09:26,870 Stage: Train 0.5 | Epoch: 317 | Iter: 483000 | Total Loss: 0.004982 | Recon Loss: 0.003995 | Commit Loss: 0.001974 | Perplexity: 1396.306508
Trainning Epoch:  96%|█████████▋| 318/330 [30:58:27<1:11:47, 358.99s/it]2025-09-27 21:10:14,613 Stage: Train 0.5 | Epoch: 318 | Iter: 483200 | Total Loss: 0.004963 | Recon Loss: 0.003981 | Commit Loss: 0.001964 | Perplexity: 1396.505466
2025-09-27 21:11:01,863 Stage: Train 0.5 | Epoch: 318 | Iter: 483400 | Total Loss: 0.004955 | Recon Loss: 0.003971 | Commit Loss: 0.001968 | Perplexity: 1395.598728
2025-09-27 21:11:49,149 Stage: Train 0.5 | Epoch: 318 | Iter: 483600 | Total Loss: 0.004999 | Recon Loss: 0.004017 | Commit Loss: 0.001964 | Perplexity: 1397.900580
2025-09-27 21:12:36,533 Stage: Train 0.5 | Epoch: 318 | Iter: 483800 | Total Loss: 0.004960 | Recon Loss: 0.003976 | Commit Loss: 0.001968 | Perplexity: 1399.780557
2025-09-27 21:13:23,865 Stage: Train 0.5 | Epoch: 318 | Iter: 484000 | Total Loss: 0.004987 | Recon Loss: 0.004003 | Commit Loss: 0.001966 | Perplexity: 1398.104122
2025-09-27 21:14:11,363 Stage: Train 0.5 | Epoch: 318 | Iter: 484200 | Total Loss: 0.005011 | Recon Loss: 0.004023 | Commit Loss: 0.001976 | Perplexity: 1399.331133
2025-09-27 21:14:58,778 Stage: Train 0.5 | Epoch: 318 | Iter: 484400 | Total Loss: 0.004981 | Recon Loss: 0.003998 | Commit Loss: 0.001966 | Perplexity: 1394.847897
Trainning Epoch:  97%|█████████▋| 319/330 [31:04:27<1:05:51, 359.25s/it]2025-09-27 21:15:45,901 Stage: Train 0.5 | Epoch: 319 | Iter: 484600 | Total Loss: 0.004984 | Recon Loss: 0.004000 | Commit Loss: 0.001967 | Perplexity: 1396.135518
2025-09-27 21:16:33,125 Stage: Train 0.5 | Epoch: 319 | Iter: 484800 | Total Loss: 0.004988 | Recon Loss: 0.004008 | Commit Loss: 0.001959 | Perplexity: 1396.620183
2025-09-27 21:17:20,475 Stage: Train 0.5 | Epoch: 319 | Iter: 485000 | Total Loss: 0.004944 | Recon Loss: 0.003964 | Commit Loss: 0.001960 | Perplexity: 1396.669282
2025-09-27 21:18:07,773 Stage: Train 0.5 | Epoch: 319 | Iter: 485200 | Total Loss: 0.004974 | Recon Loss: 0.003993 | Commit Loss: 0.001962 | Perplexity: 1395.078419
2025-09-27 21:18:55,163 Stage: Train 0.5 | Epoch: 319 | Iter: 485400 | Total Loss: 0.005019 | Recon Loss: 0.004032 | Commit Loss: 0.001973 | Perplexity: 1400.648349
2025-09-27 21:19:42,380 Stage: Train 0.5 | Epoch: 319 | Iter: 485600 | Total Loss: 0.005055 | Recon Loss: 0.004067 | Commit Loss: 0.001975 | Perplexity: 1402.988997
2025-09-27 21:20:29,690 Stage: Train 0.5 | Epoch: 319 | Iter: 485800 | Total Loss: 0.004945 | Recon Loss: 0.003963 | Commit Loss: 0.001965 | Perplexity: 1396.995059
2025-09-27 21:21:16,957 Stage: Train 0.5 | Epoch: 319 | Iter: 486000 | Total Loss: 0.005013 | Recon Loss: 0.004030 | Commit Loss: 0.001967 | Perplexity: 1397.275673
Trainning Epoch:  97%|█████████▋| 320/330 [31:10:26<59:52, 359.23s/it]  2025-09-27 21:22:04,528 Stage: Train 0.5 | Epoch: 320 | Iter: 486200 | Total Loss: 0.004918 | Recon Loss: 0.003940 | Commit Loss: 0.001957 | Perplexity: 1393.929438
2025-09-27 21:22:51,712 Stage: Train 0.5 | Epoch: 320 | Iter: 486400 | Total Loss: 0.004979 | Recon Loss: 0.003998 | Commit Loss: 0.001962 | Perplexity: 1398.664580
2025-09-27 21:23:38,822 Stage: Train 0.5 | Epoch: 320 | Iter: 486600 | Total Loss: 0.004985 | Recon Loss: 0.004002 | Commit Loss: 0.001966 | Perplexity: 1399.393061
2025-09-27 21:24:26,052 Stage: Train 0.5 | Epoch: 320 | Iter: 486800 | Total Loss: 0.004968 | Recon Loss: 0.003989 | Commit Loss: 0.001959 | Perplexity: 1397.042505
2025-09-27 21:25:13,295 Stage: Train 0.5 | Epoch: 320 | Iter: 487000 | Total Loss: 0.004971 | Recon Loss: 0.003987 | Commit Loss: 0.001969 | Perplexity: 1399.709380
2025-09-27 21:26:00,689 Stage: Train 0.5 | Epoch: 320 | Iter: 487200 | Total Loss: 0.005025 | Recon Loss: 0.004044 | Commit Loss: 0.001962 | Perplexity: 1398.968384
2025-09-27 21:26:48,068 Stage: Train 0.5 | Epoch: 320 | Iter: 487400 | Total Loss: 0.005000 | Recon Loss: 0.004014 | Commit Loss: 0.001972 | Perplexity: 1396.974204
Trainning Epoch:  97%|█████████▋| 321/330 [31:16:25<53:52, 359.21s/it]2025-09-27 21:27:35,571 Stage: Train 0.5 | Epoch: 321 | Iter: 487600 | Total Loss: 0.004974 | Recon Loss: 0.003996 | Commit Loss: 0.001956 | Perplexity: 1395.722122
2025-09-27 21:28:22,903 Stage: Train 0.5 | Epoch: 321 | Iter: 487800 | Total Loss: 0.005009 | Recon Loss: 0.004031 | Commit Loss: 0.001957 | Perplexity: 1398.187389
2025-09-27 21:29:10,235 Stage: Train 0.5 | Epoch: 321 | Iter: 488000 | Total Loss: 0.004956 | Recon Loss: 0.003978 | Commit Loss: 0.001956 | Perplexity: 1396.344200
2025-09-27 21:29:57,583 Stage: Train 0.5 | Epoch: 321 | Iter: 488200 | Total Loss: 0.005010 | Recon Loss: 0.004028 | Commit Loss: 0.001964 | Perplexity: 1397.205900
2025-09-27 21:30:44,562 Stage: Train 0.5 | Epoch: 321 | Iter: 488400 | Total Loss: 0.004968 | Recon Loss: 0.003988 | Commit Loss: 0.001961 | Perplexity: 1398.398646
2025-09-27 21:31:31,939 Stage: Train 0.5 | Epoch: 321 | Iter: 488600 | Total Loss: 0.004950 | Recon Loss: 0.003967 | Commit Loss: 0.001967 | Perplexity: 1398.664237
2025-09-27 21:32:19,602 Stage: Train 0.5 | Epoch: 321 | Iter: 488800 | Total Loss: 0.005013 | Recon Loss: 0.004030 | Commit Loss: 0.001966 | Perplexity: 1397.823119
2025-09-27 21:33:06,905 Stage: Train 0.5 | Epoch: 321 | Iter: 489000 | Total Loss: 0.004942 | Recon Loss: 0.003960 | Commit Loss: 0.001965 | Perplexity: 1399.613147
Trainning Epoch:  98%|█████████▊| 322/330 [31:22:25<47:55, 359.38s/it]2025-09-27 21:33:54,384 Stage: Train 0.5 | Epoch: 322 | Iter: 489200 | Total Loss: 0.004969 | Recon Loss: 0.003989 | Commit Loss: 0.001961 | Perplexity: 1399.059732
2025-09-27 21:34:41,804 Stage: Train 0.5 | Epoch: 322 | Iter: 489400 | Total Loss: 0.004944 | Recon Loss: 0.003964 | Commit Loss: 0.001961 | Perplexity: 1396.722150
2025-09-27 21:35:29,095 Stage: Train 0.5 | Epoch: 322 | Iter: 489600 | Total Loss: 0.004985 | Recon Loss: 0.004003 | Commit Loss: 0.001965 | Perplexity: 1397.445145
2025-09-27 21:36:16,525 Stage: Train 0.5 | Epoch: 322 | Iter: 489800 | Total Loss: 0.004999 | Recon Loss: 0.004015 | Commit Loss: 0.001967 | Perplexity: 1399.302670
2025-09-27 21:37:03,831 Stage: Train 0.5 | Epoch: 322 | Iter: 490000 | Total Loss: 0.004967 | Recon Loss: 0.003988 | Commit Loss: 0.001958 | Perplexity: 1397.238160
2025-09-27 21:37:50,806 Stage: Train 0.5 | Epoch: 322 | Iter: 490200 | Total Loss: 0.005006 | Recon Loss: 0.004024 | Commit Loss: 0.001964 | Perplexity: 1397.181852
2025-09-27 21:38:38,072 Stage: Train 0.5 | Epoch: 322 | Iter: 490400 | Total Loss: 0.004986 | Recon Loss: 0.004005 | Commit Loss: 0.001963 | Perplexity: 1398.206592
2025-09-27 21:39:25,401 Stage: Train 0.5 | Epoch: 322 | Iter: 490600 | Total Loss: 0.005001 | Recon Loss: 0.004022 | Commit Loss: 0.001957 | Perplexity: 1396.879531
Trainning Epoch:  98%|█████████▊| 323/330 [31:28:24<41:55, 359.36s/it]2025-09-27 21:40:13,122 Stage: Train 0.5 | Epoch: 323 | Iter: 490800 | Total Loss: 0.004956 | Recon Loss: 0.003976 | Commit Loss: 0.001960 | Perplexity: 1396.904285
2025-09-27 21:41:00,445 Stage: Train 0.5 | Epoch: 323 | Iter: 491000 | Total Loss: 0.004999 | Recon Loss: 0.004021 | Commit Loss: 0.001956 | Perplexity: 1397.419583
2025-09-27 21:41:47,618 Stage: Train 0.5 | Epoch: 323 | Iter: 491200 | Total Loss: 0.004977 | Recon Loss: 0.003996 | Commit Loss: 0.001963 | Perplexity: 1396.596392
2025-09-27 21:42:35,022 Stage: Train 0.5 | Epoch: 323 | Iter: 491400 | Total Loss: 0.004948 | Recon Loss: 0.003969 | Commit Loss: 0.001959 | Perplexity: 1398.097889
2025-09-27 21:43:22,238 Stage: Train 0.5 | Epoch: 323 | Iter: 491600 | Total Loss: 0.004953 | Recon Loss: 0.003973 | Commit Loss: 0.001960 | Perplexity: 1397.824435
2025-09-27 21:44:09,756 Stage: Train 0.5 | Epoch: 323 | Iter: 491800 | Total Loss: 0.004990 | Recon Loss: 0.004009 | Commit Loss: 0.001963 | Perplexity: 1399.811383
2025-09-27 21:44:56,775 Stage: Train 0.5 | Epoch: 323 | Iter: 492000 | Total Loss: 0.005017 | Recon Loss: 0.004037 | Commit Loss: 0.001960 | Perplexity: 1398.729272
Trainning Epoch:  98%|█████████▊| 324/330 [31:34:24<35:56, 359.40s/it]2025-09-27 21:45:44,347 Stage: Train 0.5 | Epoch: 324 | Iter: 492200 | Total Loss: 0.004946 | Recon Loss: 0.003967 | Commit Loss: 0.001959 | Perplexity: 1398.031963
2025-09-27 21:46:31,595 Stage: Train 0.5 | Epoch: 324 | Iter: 492400 | Total Loss: 0.004947 | Recon Loss: 0.003967 | Commit Loss: 0.001959 | Perplexity: 1396.234372
2025-09-27 21:47:18,950 Stage: Train 0.5 | Epoch: 324 | Iter: 492600 | Total Loss: 0.004971 | Recon Loss: 0.003994 | Commit Loss: 0.001954 | Perplexity: 1396.701599
2025-09-27 21:48:06,069 Stage: Train 0.5 | Epoch: 324 | Iter: 492800 | Total Loss: 0.004982 | Recon Loss: 0.004004 | Commit Loss: 0.001956 | Perplexity: 1398.075638
2025-09-27 21:48:53,263 Stage: Train 0.5 | Epoch: 324 | Iter: 493000 | Total Loss: 0.004979 | Recon Loss: 0.004000 | Commit Loss: 0.001957 | Perplexity: 1397.605915
2025-09-27 21:49:40,561 Stage: Train 0.5 | Epoch: 324 | Iter: 493200 | Total Loss: 0.004959 | Recon Loss: 0.003978 | Commit Loss: 0.001962 | Perplexity: 1400.764664
2025-09-27 21:50:27,668 Stage: Train 0.5 | Epoch: 324 | Iter: 493400 | Total Loss: 0.004984 | Recon Loss: 0.004002 | Commit Loss: 0.001963 | Perplexity: 1396.387455
2025-09-27 21:51:14,757 Stage: Train 0.5 | Epoch: 324 | Iter: 493600 | Total Loss: 0.004941 | Recon Loss: 0.003963 | Commit Loss: 0.001957 | Perplexity: 1398.234398
Trainning Epoch:  98%|█████████▊| 325/330 [31:40:23<29:56, 359.23s/it]2025-09-27 21:52:01,998 Stage: Train 0.5 | Epoch: 325 | Iter: 493800 | Total Loss: 0.004995 | Recon Loss: 0.004019 | Commit Loss: 0.001953 | Perplexity: 1397.014221
2025-09-27 21:52:49,236 Stage: Train 0.5 | Epoch: 325 | Iter: 494000 | Total Loss: 0.004969 | Recon Loss: 0.003990 | Commit Loss: 0.001958 | Perplexity: 1401.037929
2025-09-27 21:53:36,622 Stage: Train 0.5 | Epoch: 325 | Iter: 494200 | Total Loss: 0.004998 | Recon Loss: 0.004022 | Commit Loss: 0.001952 | Perplexity: 1395.801510
2025-09-27 21:54:23,913 Stage: Train 0.5 | Epoch: 325 | Iter: 494400 | Total Loss: 0.004931 | Recon Loss: 0.003952 | Commit Loss: 0.001958 | Perplexity: 1399.542742
2025-09-27 21:55:11,196 Stage: Train 0.5 | Epoch: 325 | Iter: 494600 | Total Loss: 0.004986 | Recon Loss: 0.004004 | Commit Loss: 0.001964 | Perplexity: 1398.441518
2025-09-27 21:55:58,612 Stage: Train 0.5 | Epoch: 325 | Iter: 494800 | Total Loss: 0.004972 | Recon Loss: 0.003993 | Commit Loss: 0.001956 | Perplexity: 1399.055638
2025-09-27 21:56:45,953 Stage: Train 0.5 | Epoch: 325 | Iter: 495000 | Total Loss: 0.004979 | Recon Loss: 0.003998 | Commit Loss: 0.001960 | Perplexity: 1396.980995
Trainning Epoch:  99%|█████████▉| 326/330 [31:46:22<23:57, 359.28s/it]2025-09-27 21:57:33,589 Stage: Train 0.5 | Epoch: 326 | Iter: 495200 | Total Loss: 0.004961 | Recon Loss: 0.003979 | Commit Loss: 0.001964 | Perplexity: 1399.607138
2025-09-27 21:58:20,701 Stage: Train 0.5 | Epoch: 326 | Iter: 495400 | Total Loss: 0.004954 | Recon Loss: 0.003979 | Commit Loss: 0.001948 | Perplexity: 1398.598607
2025-09-27 21:59:07,732 Stage: Train 0.5 | Epoch: 326 | Iter: 495600 | Total Loss: 0.004967 | Recon Loss: 0.003989 | Commit Loss: 0.001954 | Perplexity: 1399.676508
2025-09-27 21:59:55,038 Stage: Train 0.5 | Epoch: 326 | Iter: 495800 | Total Loss: 0.004982 | Recon Loss: 0.004002 | Commit Loss: 0.001961 | Perplexity: 1399.776093
2025-09-27 22:00:42,420 Stage: Train 0.5 | Epoch: 326 | Iter: 496000 | Total Loss: 0.004942 | Recon Loss: 0.003962 | Commit Loss: 0.001958 | Perplexity: 1397.926287
2025-09-27 22:01:29,682 Stage: Train 0.5 | Epoch: 326 | Iter: 496200 | Total Loss: 0.004999 | Recon Loss: 0.004019 | Commit Loss: 0.001960 | Perplexity: 1400.244357
2025-09-27 22:02:16,983 Stage: Train 0.5 | Epoch: 326 | Iter: 496400 | Total Loss: 0.004955 | Recon Loss: 0.003972 | Commit Loss: 0.001967 | Perplexity: 1402.246002
2025-09-27 22:03:04,369 Stage: Train 0.5 | Epoch: 326 | Iter: 496600 | Total Loss: 0.004996 | Recon Loss: 0.004017 | Commit Loss: 0.001958 | Perplexity: 1398.498091
Trainning Epoch:  99%|█████████▉| 327/330 [31:52:21<17:57, 359.27s/it]2025-09-27 22:03:52,075 Stage: Train 0.5 | Epoch: 327 | Iter: 496800 | Total Loss: 0.004920 | Recon Loss: 0.003941 | Commit Loss: 0.001957 | Perplexity: 1398.883452
2025-09-27 22:04:39,504 Stage: Train 0.5 | Epoch: 327 | Iter: 497000 | Total Loss: 0.004963 | Recon Loss: 0.003985 | Commit Loss: 0.001957 | Perplexity: 1401.286411
2025-09-27 22:05:26,775 Stage: Train 0.5 | Epoch: 327 | Iter: 497200 | Total Loss: 0.004972 | Recon Loss: 0.003994 | Commit Loss: 0.001956 | Perplexity: 1402.085004
2025-09-27 22:06:14,089 Stage: Train 0.5 | Epoch: 327 | Iter: 497400 | Total Loss: 0.004964 | Recon Loss: 0.003986 | Commit Loss: 0.001957 | Perplexity: 1399.674937
2025-09-27 22:07:01,275 Stage: Train 0.5 | Epoch: 327 | Iter: 497600 | Total Loss: 0.004986 | Recon Loss: 0.004007 | Commit Loss: 0.001958 | Perplexity: 1397.918989
2025-09-27 22:07:48,623 Stage: Train 0.5 | Epoch: 327 | Iter: 497800 | Total Loss: 0.004926 | Recon Loss: 0.003947 | Commit Loss: 0.001957 | Perplexity: 1399.352300
2025-09-27 22:08:36,029 Stage: Train 0.5 | Epoch: 327 | Iter: 498000 | Total Loss: 0.004956 | Recon Loss: 0.003973 | Commit Loss: 0.001966 | Perplexity: 1403.072924
2025-09-27 22:09:23,362 Stage: Train 0.5 | Epoch: 327 | Iter: 498200 | Total Loss: 0.004997 | Recon Loss: 0.004020 | Commit Loss: 0.001954 | Perplexity: 1399.026525
Trainning Epoch:  99%|█████████▉| 328/330 [31:58:21<11:58, 359.43s/it]2025-09-27 22:10:10,846 Stage: Train 0.5 | Epoch: 328 | Iter: 498400 | Total Loss: 0.004953 | Recon Loss: 0.003979 | Commit Loss: 0.001948 | Perplexity: 1396.467872
2025-09-27 22:10:58,034 Stage: Train 0.5 | Epoch: 328 | Iter: 498600 | Total Loss: 0.004965 | Recon Loss: 0.003988 | Commit Loss: 0.001954 | Perplexity: 1400.758066
2025-09-27 22:11:45,157 Stage: Train 0.5 | Epoch: 328 | Iter: 498800 | Total Loss: 0.004935 | Recon Loss: 0.003959 | Commit Loss: 0.001950 | Perplexity: 1398.243522
2025-09-27 22:12:32,315 Stage: Train 0.5 | Epoch: 328 | Iter: 499000 | Total Loss: 0.004920 | Recon Loss: 0.003943 | Commit Loss: 0.001954 | Perplexity: 1401.485489
2025-09-27 22:13:19,500 Stage: Train 0.5 | Epoch: 328 | Iter: 499200 | Total Loss: 0.004996 | Recon Loss: 0.004016 | Commit Loss: 0.001960 | Perplexity: 1401.552428
2025-09-27 22:14:06,469 Stage: Train 0.5 | Epoch: 328 | Iter: 499400 | Total Loss: 0.004932 | Recon Loss: 0.003953 | Commit Loss: 0.001957 | Perplexity: 1400.342024
2025-09-27 22:14:53,808 Stage: Train 0.5 | Epoch: 328 | Iter: 499600 | Total Loss: 0.004973 | Recon Loss: 0.003991 | Commit Loss: 0.001964 | Perplexity: 1400.054355
Trainning Epoch: 100%|█████████▉| 329/330 [32:04:20<05:59, 359.14s/it]2025-09-27 22:15:41,247 Stage: Train 0.5 | Epoch: 329 | Iter: 499800 | Total Loss: 0.004987 | Recon Loss: 0.004006 | Commit Loss: 0.001962 | Perplexity: 1399.842545
2025-09-27 22:16:28,349 Stage: Train 0.5 | Epoch: 329 | Iter: 500000 | Total Loss: 0.004922 | Recon Loss: 0.003947 | Commit Loss: 0.001950 | Perplexity: 1398.504776
2025-09-27 22:16:28,350 Saving model at iteration 500000
2025-09-27 22:16:28,551 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000
2025-09-27 22:16:28,858 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/model.safetensors
2025-09-27 22:16:29,278 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/optimizer.bin
2025-09-27 22:16:29,278 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/scheduler.bin
2025-09-27 22:16:29,278 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/sampler.bin
2025-09-27 22:16:29,279 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb4096x2048_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/random_states_0.pkl
Trainning Epoch: 100%|█████████▉| 329/330 [32:05:20<05:51, 351.12s/it]
2025-09-27 22:16:29,343 Training finished
