The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-27 06:47:39,120 
python train_vqvae_new.py --batch_size 64 --config vqvae_experiment_configs/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/config.yaml --data_mode null --num_frames 16 --sample_stride 1 --data_stride 16 --project_dir vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2 --not_find_unused_parameters --nb_code 8192 --codebook_dim 3072 --loss_type mpjpe --vqvae_type hybrid --hrnet_output_level 3 --vision_guidance_ratio 0 --fix_weights --resume_pth  --joint_data_type joint3d_cam_rootrel_meter --downsample_time [1,2] --frame_upsample_rate [2.0,1.0]
--- Logging error ---
Traceback (most recent call last):
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 1100, in emit
    msg = self.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 943, in format
    return fmt.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 678, in format
    record.message = record.getMessage()
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 368, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/wxs/MTVCrafter/train_vqvae_new.py", line 318, in <module>
    logger.info('\nPID ', os.getpid())
Message: '\nPID '
Arguments: (3105506,)
--- Logging error ---
Traceback (most recent call last):
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 1100, in emit
    msg = self.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 943, in format
    return fmt.format(record)
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 678, in format
    record.message = record.getMessage()
  File "/home/wxs/anaconda3/envs/llama_factory/lib/python3.10/logging/__init__.py", line 368, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/wxs/MTVCrafter/train_vqvae_new.py", line 318, in <module>
    logger.info('\nPID ', os.getpid())
Message: '\nPID '
Arguments: (3105506,)
2025-09-27 06:47:49,234 Data loaded with 97196 samples
2025-09-27 06:47:50,060 Trainable parameters: 48,323,075
2025-09-27 06:47:50,060 Non-trainable parameters: 0
2025-09-27 06:47:51,289 Number of trainable parameters: 48.323075 M
2025-09-27 06:47:51,289 Args: {'num_frames': 16, 'sample_stride': 1, 'data_stride': 16, 'data_mode': 'null', 'load_data_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final_wImgPath_wJ3dCam_wJ2dCpn.pkl', 'load_image_source_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/images_source.pkl', 'load_bbox_file': '/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/bboxes_xyxy.pkl', 'load_text_source_file': '', 'return_extra': [[]], 'normalize': 'isotropic', 'filter_invalid_images': False, 'processed_image_shape': None, 'backbone': 'hrnet_32', 'get_item_list': ['joint3d_cam_rootrel_meter', 'slice_id'], 'config': 'vqvae_experiment_configs/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/config.yaml', 'resume_pth': '', 'batch_size': 64, 'commit_ratio': 0.5, 'nb_code': 8192, 'codebook_dim': 3072, 'max_epoch': 1000000000.0, 'total_iter': 500000, 'world_size': 1, 'rank': 0, 'save_interval': 20000, 'warm_up_iter': 5000, 'print_iter': 200, 'learning_rate': 0.0002, 'lr_schedule': [300000], 'gamma': 0.05, 'weight_decay': 0.0001, 'device': 'cuda', 'project_config': '', 'allow_tf32': False, 'project_dir': 'vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2', 'seed': 6666, 'not_find_unused_parameters': True, 'loss_type': 'mpjpe', 'vqvae_type': 'hybrid', 'joint_data_type': 'joint3d_cam_rootrel_meter', 'hrnet_output_level': 3, 'fix_weights': True, 'vision_guidance_ratio': 0.0, 'downsample_time': [1, 2], 'frame_upsample_rate': [2.0, 1.0]}
Trainning Epoch:   0%|          | 0/330 [00:00<?, ?it/s]2025-09-27 06:48:49,406 current_lr 0.000008 at iteration 200
2025-09-27 06:48:49,692 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.194099 | Recon Loss: 0.191003 | Commit Loss: 0.006192 | Perplexity: 2202.599914
2025-09-27 06:49:46,651 current_lr 0.000016 at iteration 400
2025-09-27 06:49:46,946 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.112534 | Recon Loss: 0.095124 | Commit Loss: 0.034821 | Perplexity: 1501.291098
2025-09-27 06:50:43,985 current_lr 0.000024 at iteration 600
2025-09-27 06:50:44,263 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.116169 | Recon Loss: 0.075704 | Commit Loss: 0.080930 | Perplexity: 1519.246395
2025-09-27 06:51:41,352 current_lr 0.000032 at iteration 800
2025-09-27 06:51:41,631 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.110247 | Recon Loss: 0.064761 | Commit Loss: 0.090972 | Perplexity: 1555.301837
2025-09-27 06:52:38,502 current_lr 0.000040 at iteration 1000
2025-09-27 06:52:38,783 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.099613 | Recon Loss: 0.058093 | Commit Loss: 0.083041 | Perplexity: 1549.484070
2025-09-27 06:53:35,702 current_lr 0.000048 at iteration 1200
2025-09-27 06:53:35,988 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.088984 | Recon Loss: 0.053379 | Commit Loss: 0.071209 | Perplexity: 1537.856666
2025-09-27 06:54:32,829 current_lr 0.000056 at iteration 1400
2025-09-27 06:54:33,118 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.079742 | Recon Loss: 0.049771 | Commit Loss: 0.059942 | Perplexity: 1546.795240
Trainning Epoch:   0%|          | 1/330 [07:15<39:47:32, 435.42s/it]2025-09-27 06:55:29,991 current_lr 0.000064 at iteration 1600
2025-09-27 06:55:30,280 Stage: Warm Up | Epoch: 1 | Iter: 1600 | Total Loss: 0.070622 | Recon Loss: 0.045530 | Commit Loss: 0.050184 | Perplexity: 1548.448001
2025-09-27 06:56:27,114 current_lr 0.000072 at iteration 1800
2025-09-27 06:56:27,400 Stage: Warm Up | Epoch: 1 | Iter: 1800 | Total Loss: 0.063044 | Recon Loss: 0.042594 | Commit Loss: 0.040900 | Perplexity: 1531.254992
2025-09-27 06:57:24,132 current_lr 0.000080 at iteration 2000
2025-09-27 06:57:24,401 Stage: Warm Up | Epoch: 1 | Iter: 2000 | Total Loss: 0.056675 | Recon Loss: 0.040188 | Commit Loss: 0.032973 | Perplexity: 1495.923478
2025-09-27 06:58:21,356 current_lr 0.000088 at iteration 2200
2025-09-27 06:58:21,641 Stage: Warm Up | Epoch: 1 | Iter: 2200 | Total Loss: 0.051004 | Recon Loss: 0.037190 | Commit Loss: 0.027629 | Perplexity: 1449.530251
2025-09-27 06:59:18,413 current_lr 0.000096 at iteration 2400
2025-09-27 06:59:18,693 Stage: Warm Up | Epoch: 1 | Iter: 2400 | Total Loss: 0.048387 | Recon Loss: 0.036773 | Commit Loss: 0.023228 | Perplexity: 1404.862506
2025-09-27 07:00:15,714 current_lr 0.000104 at iteration 2600
2025-09-27 07:00:15,995 Stage: Warm Up | Epoch: 1 | Iter: 2600 | Total Loss: 0.046012 | Recon Loss: 0.035840 | Commit Loss: 0.020344 | Perplexity: 1353.324251
2025-09-27 07:01:12,520 current_lr 0.000112 at iteration 2800
2025-09-27 07:01:12,805 Stage: Warm Up | Epoch: 1 | Iter: 2800 | Total Loss: 0.043588 | Recon Loss: 0.034580 | Commit Loss: 0.018016 | Perplexity: 1326.900561
2025-09-27 07:02:09,758 current_lr 0.000120 at iteration 3000
2025-09-27 07:02:10,054 Stage: Warm Up | Epoch: 1 | Iter: 3000 | Total Loss: 0.041171 | Recon Loss: 0.033051 | Commit Loss: 0.016239 | Perplexity: 1308.965540
Trainning Epoch:   1%|          | 2/330 [14:29<39:36:14, 434.68s/it]2025-09-27 07:03:07,216 current_lr 0.000128 at iteration 3200
2025-09-27 07:03:07,511 Stage: Warm Up | Epoch: 2 | Iter: 3200 | Total Loss: 0.039402 | Recon Loss: 0.032193 | Commit Loss: 0.014418 | Perplexity: 1294.548391
2025-09-27 07:04:04,418 current_lr 0.000136 at iteration 3400
2025-09-27 07:04:04,699 Stage: Warm Up | Epoch: 2 | Iter: 3400 | Total Loss: 0.037975 | Recon Loss: 0.031617 | Commit Loss: 0.012717 | Perplexity: 1279.172322
2025-09-27 07:05:01,551 current_lr 0.000144 at iteration 3600
2025-09-27 07:05:01,841 Stage: Warm Up | Epoch: 2 | Iter: 3600 | Total Loss: 0.036814 | Recon Loss: 0.031085 | Commit Loss: 0.011458 | Perplexity: 1278.199384
2025-09-27 07:05:58,836 current_lr 0.000152 at iteration 3800
2025-09-27 07:05:59,124 Stage: Warm Up | Epoch: 2 | Iter: 3800 | Total Loss: 0.034870 | Recon Loss: 0.029601 | Commit Loss: 0.010538 | Perplexity: 1275.360497
2025-09-27 07:06:55,611 current_lr 0.000160 at iteration 4000
2025-09-27 07:06:55,899 Stage: Warm Up | Epoch: 2 | Iter: 4000 | Total Loss: 0.033998 | Recon Loss: 0.029191 | Commit Loss: 0.009615 | Perplexity: 1277.011996
2025-09-27 07:07:53,399 current_lr 0.000168 at iteration 4200
2025-09-27 07:07:53,689 Stage: Warm Up | Epoch: 2 | Iter: 4200 | Total Loss: 0.032391 | Recon Loss: 0.027929 | Commit Loss: 0.008924 | Perplexity: 1285.069652
2025-09-27 07:08:50,749 current_lr 0.000176 at iteration 4400
2025-09-27 07:08:51,042 Stage: Warm Up | Epoch: 2 | Iter: 4400 | Total Loss: 0.031692 | Recon Loss: 0.027492 | Commit Loss: 0.008401 | Perplexity: 1290.105797
Trainning Epoch:   1%|          | 3/330 [21:44<39:29:34, 434.79s/it]2025-09-27 07:09:48,060 current_lr 0.000184 at iteration 4600
2025-09-27 07:09:48,355 Stage: Warm Up | Epoch: 3 | Iter: 4600 | Total Loss: 0.031100 | Recon Loss: 0.027197 | Commit Loss: 0.007808 | Perplexity: 1287.652694
2025-09-27 07:10:45,290 current_lr 0.000192 at iteration 4800
2025-09-27 07:10:45,574 Stage: Warm Up | Epoch: 3 | Iter: 4800 | Total Loss: 0.030190 | Recon Loss: 0.026479 | Commit Loss: 0.007422 | Perplexity: 1278.918094
2025-09-27 07:11:42,593 current_lr 0.000200 at iteration 5000
2025-09-27 07:11:42,891 Stage: Warm Up | Epoch: 3 | Iter: 5000 | Total Loss: 0.029671 | Recon Loss: 0.026138 | Commit Loss: 0.007067 | Perplexity: 1276.545876
2025-09-27 07:12:39,941 Stage: Train 0.5 | Epoch: 3 | Iter: 5200 | Total Loss: 0.029156 | Recon Loss: 0.025705 | Commit Loss: 0.006902 | Perplexity: 1277.208480
2025-09-27 07:13:37,063 Stage: Train 0.5 | Epoch: 3 | Iter: 5400 | Total Loss: 0.028073 | Recon Loss: 0.024741 | Commit Loss: 0.006666 | Perplexity: 1281.113384
2025-09-27 07:14:34,284 Stage: Train 0.5 | Epoch: 3 | Iter: 5600 | Total Loss: 0.027389 | Recon Loss: 0.024167 | Commit Loss: 0.006444 | Perplexity: 1280.271016
2025-09-27 07:15:31,491 Stage: Train 0.5 | Epoch: 3 | Iter: 5800 | Total Loss: 0.026094 | Recon Loss: 0.022974 | Commit Loss: 0.006240 | Perplexity: 1300.690001
2025-09-27 07:16:29,003 Stage: Train 0.5 | Epoch: 3 | Iter: 6000 | Total Loss: 0.025906 | Recon Loss: 0.022894 | Commit Loss: 0.006024 | Perplexity: 1308.760709
Trainning Epoch:   1%|          | 4/330 [28:59<39:22:42, 434.85s/it]2025-09-27 07:17:26,430 Stage: Train 0.5 | Epoch: 4 | Iter: 6200 | Total Loss: 0.024957 | Recon Loss: 0.022029 | Commit Loss: 0.005856 | Perplexity: 1318.463521
2025-09-27 07:18:23,754 Stage: Train 0.5 | Epoch: 4 | Iter: 6400 | Total Loss: 0.024241 | Recon Loss: 0.021394 | Commit Loss: 0.005694 | Perplexity: 1320.007549
2025-09-27 07:19:20,624 Stage: Train 0.5 | Epoch: 4 | Iter: 6600 | Total Loss: 0.023556 | Recon Loss: 0.020658 | Commit Loss: 0.005797 | Perplexity: 1329.741334
2025-09-27 07:20:17,924 Stage: Train 0.5 | Epoch: 4 | Iter: 6800 | Total Loss: 0.023314 | Recon Loss: 0.020411 | Commit Loss: 0.005806 | Perplexity: 1322.139053
2025-09-27 07:21:15,518 Stage: Train 0.5 | Epoch: 4 | Iter: 7000 | Total Loss: 0.023000 | Recon Loss: 0.020128 | Commit Loss: 0.005744 | Perplexity: 1326.724318
2025-09-27 07:22:12,600 Stage: Train 0.5 | Epoch: 4 | Iter: 7200 | Total Loss: 0.022812 | Recon Loss: 0.020002 | Commit Loss: 0.005619 | Perplexity: 1325.243077
2025-09-27 07:23:09,820 Stage: Train 0.5 | Epoch: 4 | Iter: 7400 | Total Loss: 0.022465 | Recon Loss: 0.019697 | Commit Loss: 0.005536 | Perplexity: 1329.154560
Trainning Epoch:   2%|▏         | 5/330 [36:14<39:15:37, 434.88s/it]2025-09-27 07:24:07,304 Stage: Train 0.5 | Epoch: 5 | Iter: 7600 | Total Loss: 0.022296 | Recon Loss: 0.019488 | Commit Loss: 0.005617 | Perplexity: 1328.730405
2025-09-27 07:25:04,174 Stage: Train 0.5 | Epoch: 5 | Iter: 7800 | Total Loss: 0.021436 | Recon Loss: 0.018651 | Commit Loss: 0.005571 | Perplexity: 1337.119486
2025-09-27 07:26:01,323 Stage: Train 0.5 | Epoch: 5 | Iter: 8000 | Total Loss: 0.021623 | Recon Loss: 0.018841 | Commit Loss: 0.005563 | Perplexity: 1345.762585
2025-09-27 07:26:58,531 Stage: Train 0.5 | Epoch: 5 | Iter: 8200 | Total Loss: 0.021147 | Recon Loss: 0.018433 | Commit Loss: 0.005428 | Perplexity: 1345.459906
2025-09-27 07:27:55,807 Stage: Train 0.5 | Epoch: 5 | Iter: 8400 | Total Loss: 0.020484 | Recon Loss: 0.017740 | Commit Loss: 0.005488 | Perplexity: 1359.500862
2025-09-27 07:28:53,135 Stage: Train 0.5 | Epoch: 5 | Iter: 8600 | Total Loss: 0.020846 | Recon Loss: 0.018156 | Commit Loss: 0.005380 | Perplexity: 1356.111880
2025-09-27 07:29:50,400 Stage: Train 0.5 | Epoch: 5 | Iter: 8800 | Total Loss: 0.020302 | Recon Loss: 0.017563 | Commit Loss: 0.005476 | Perplexity: 1357.596666
2025-09-27 07:30:47,425 Stage: Train 0.5 | Epoch: 5 | Iter: 9000 | Total Loss: 0.020167 | Recon Loss: 0.017462 | Commit Loss: 0.005411 | Perplexity: 1360.938446
Trainning Epoch:   2%|▏         | 6/330 [43:28<39:07:33, 434.73s/it]2025-09-27 07:31:44,886 Stage: Train 0.5 | Epoch: 6 | Iter: 9200 | Total Loss: 0.019881 | Recon Loss: 0.017174 | Commit Loss: 0.005414 | Perplexity: 1361.610340
2025-09-27 07:32:42,085 Stage: Train 0.5 | Epoch: 6 | Iter: 9400 | Total Loss: 0.019889 | Recon Loss: 0.017197 | Commit Loss: 0.005384 | Perplexity: 1361.193803
2025-09-27 07:33:39,477 Stage: Train 0.5 | Epoch: 6 | Iter: 9600 | Total Loss: 0.019591 | Recon Loss: 0.016885 | Commit Loss: 0.005412 | Perplexity: 1365.562428
2025-09-27 07:34:36,715 Stage: Train 0.5 | Epoch: 6 | Iter: 9800 | Total Loss: 0.019380 | Recon Loss: 0.016692 | Commit Loss: 0.005376 | Perplexity: 1367.062123
2025-09-27 07:35:33,768 Stage: Train 0.5 | Epoch: 6 | Iter: 10000 | Total Loss: 0.019089 | Recon Loss: 0.016389 | Commit Loss: 0.005400 | Perplexity: 1373.841705
2025-09-27 07:36:30,634 Stage: Train 0.5 | Epoch: 6 | Iter: 10200 | Total Loss: 0.018926 | Recon Loss: 0.016245 | Commit Loss: 0.005361 | Perplexity: 1373.567913
2025-09-27 07:37:27,643 Stage: Train 0.5 | Epoch: 6 | Iter: 10400 | Total Loss: 0.018787 | Recon Loss: 0.016082 | Commit Loss: 0.005410 | Perplexity: 1375.273212
2025-09-27 07:38:24,778 Stage: Train 0.5 | Epoch: 6 | Iter: 10600 | Total Loss: 0.018882 | Recon Loss: 0.016200 | Commit Loss: 0.005362 | Perplexity: 1373.049720
Trainning Epoch:   2%|▏         | 7/330 [50:42<38:59:10, 434.52s/it]2025-09-27 07:39:22,146 Stage: Train 0.5 | Epoch: 7 | Iter: 10800 | Total Loss: 0.018513 | Recon Loss: 0.015853 | Commit Loss: 0.005321 | Perplexity: 1373.201242
2025-09-27 07:40:19,150 Stage: Train 0.5 | Epoch: 7 | Iter: 11000 | Total Loss: 0.018511 | Recon Loss: 0.015827 | Commit Loss: 0.005369 | Perplexity: 1374.136339
2025-09-27 07:41:16,113 Stage: Train 0.5 | Epoch: 7 | Iter: 11200 | Total Loss: 0.018388 | Recon Loss: 0.015708 | Commit Loss: 0.005361 | Perplexity: 1375.452730
2025-09-27 07:42:13,115 Stage: Train 0.5 | Epoch: 7 | Iter: 11400 | Total Loss: 0.018203 | Recon Loss: 0.015527 | Commit Loss: 0.005352 | Perplexity: 1377.012544
2025-09-27 07:43:09,995 Stage: Train 0.5 | Epoch: 7 | Iter: 11600 | Total Loss: 0.018098 | Recon Loss: 0.015431 | Commit Loss: 0.005335 | Perplexity: 1378.050281
2025-09-27 07:44:06,878 Stage: Train 0.5 | Epoch: 7 | Iter: 11800 | Total Loss: 0.018225 | Recon Loss: 0.015573 | Commit Loss: 0.005303 | Perplexity: 1377.337152
2025-09-27 07:45:03,910 Stage: Train 0.5 | Epoch: 7 | Iter: 12000 | Total Loss: 0.017964 | Recon Loss: 0.015278 | Commit Loss: 0.005372 | Perplexity: 1380.825403
Trainning Epoch:   2%|▏         | 8/330 [57:55<38:49:15, 434.02s/it]2025-09-27 07:46:00,970 Stage: Train 0.5 | Epoch: 8 | Iter: 12200 | Total Loss: 0.017578 | Recon Loss: 0.014928 | Commit Loss: 0.005300 | Perplexity: 1376.574959
2025-09-27 07:46:58,053 Stage: Train 0.5 | Epoch: 8 | Iter: 12400 | Total Loss: 0.017749 | Recon Loss: 0.015118 | Commit Loss: 0.005262 | Perplexity: 1381.077470
2025-09-27 07:47:55,270 Stage: Train 0.5 | Epoch: 8 | Iter: 12600 | Total Loss: 0.017592 | Recon Loss: 0.014948 | Commit Loss: 0.005288 | Perplexity: 1381.774802
2025-09-27 07:48:51,994 Stage: Train 0.5 | Epoch: 8 | Iter: 12800 | Total Loss: 0.017367 | Recon Loss: 0.014706 | Commit Loss: 0.005322 | Perplexity: 1382.714700
2025-09-27 07:49:49,012 Stage: Train 0.5 | Epoch: 8 | Iter: 13000 | Total Loss: 0.017539 | Recon Loss: 0.014929 | Commit Loss: 0.005221 | Perplexity: 1375.430931
2025-09-27 07:50:44,495 Stage: Train 0.5 | Epoch: 8 | Iter: 13200 | Total Loss: 0.017315 | Recon Loss: 0.014656 | Commit Loss: 0.005317 | Perplexity: 1384.050372
2025-09-27 07:51:41,593 Stage: Train 0.5 | Epoch: 8 | Iter: 13400 | Total Loss: 0.017081 | Recon Loss: 0.014454 | Commit Loss: 0.005254 | Perplexity: 1383.122044
2025-09-27 07:52:38,385 Stage: Train 0.5 | Epoch: 8 | Iter: 13600 | Total Loss: 0.016934 | Recon Loss: 0.014303 | Commit Loss: 0.005261 | Perplexity: 1387.142104
Trainning Epoch:   3%|▎         | 9/330 [1:05:07<38:37:48, 433.23s/it]2025-09-27 07:53:35,436 Stage: Train 0.5 | Epoch: 9 | Iter: 13800 | Total Loss: 0.017301 | Recon Loss: 0.014661 | Commit Loss: 0.005280 | Perplexity: 1384.214089
2025-09-27 07:54:32,274 Stage: Train 0.5 | Epoch: 9 | Iter: 14000 | Total Loss: 0.016786 | Recon Loss: 0.014177 | Commit Loss: 0.005219 | Perplexity: 1386.424398
2025-09-27 07:55:29,489 Stage: Train 0.5 | Epoch: 9 | Iter: 14200 | Total Loss: 0.016900 | Recon Loss: 0.014276 | Commit Loss: 0.005248 | Perplexity: 1388.654247
2025-09-27 07:56:26,766 Stage: Train 0.5 | Epoch: 9 | Iter: 14400 | Total Loss: 0.016611 | Recon Loss: 0.014027 | Commit Loss: 0.005168 | Perplexity: 1395.681285
2025-09-27 07:57:23,962 Stage: Train 0.5 | Epoch: 9 | Iter: 14600 | Total Loss: 0.016397 | Recon Loss: 0.013842 | Commit Loss: 0.005109 | Perplexity: 1389.485007
2025-09-27 07:58:21,197 Stage: Train 0.5 | Epoch: 9 | Iter: 14800 | Total Loss: 0.016670 | Recon Loss: 0.014050 | Commit Loss: 0.005240 | Perplexity: 1393.889068
2025-09-27 07:59:18,670 Stage: Train 0.5 | Epoch: 9 | Iter: 15000 | Total Loss: 0.016417 | Recon Loss: 0.013816 | Commit Loss: 0.005202 | Perplexity: 1393.346575
Trainning Epoch:   3%|▎         | 10/330 [1:12:21<38:31:49, 433.47s/it]2025-09-27 08:00:15,698 Stage: Train 0.5 | Epoch: 10 | Iter: 15200 | Total Loss: 0.016366 | Recon Loss: 0.013759 | Commit Loss: 0.005214 | Perplexity: 1397.203624
2025-09-27 08:01:12,550 Stage: Train 0.5 | Epoch: 10 | Iter: 15400 | Total Loss: 0.016374 | Recon Loss: 0.013825 | Commit Loss: 0.005097 | Perplexity: 1389.717627
2025-09-27 08:02:09,967 Stage: Train 0.5 | Epoch: 10 | Iter: 15600 | Total Loss: 0.016052 | Recon Loss: 0.013491 | Commit Loss: 0.005122 | Perplexity: 1399.948995
2025-09-27 08:03:07,001 Stage: Train 0.5 | Epoch: 10 | Iter: 15800 | Total Loss: 0.016175 | Recon Loss: 0.013606 | Commit Loss: 0.005137 | Perplexity: 1399.229279
2025-09-27 08:04:04,353 Stage: Train 0.5 | Epoch: 10 | Iter: 16000 | Total Loss: 0.016186 | Recon Loss: 0.013635 | Commit Loss: 0.005102 | Perplexity: 1394.749118
2025-09-27 08:05:01,587 Stage: Train 0.5 | Epoch: 10 | Iter: 16200 | Total Loss: 0.015994 | Recon Loss: 0.013422 | Commit Loss: 0.005144 | Perplexity: 1402.927136
2025-09-27 08:05:58,718 Stage: Train 0.5 | Epoch: 10 | Iter: 16400 | Total Loss: 0.016118 | Recon Loss: 0.013557 | Commit Loss: 0.005122 | Perplexity: 1405.208436
2025-09-27 08:06:55,689 Stage: Train 0.5 | Epoch: 10 | Iter: 16600 | Total Loss: 0.016108 | Recon Loss: 0.013572 | Commit Loss: 0.005072 | Perplexity: 1398.593160
Trainning Epoch:   3%|▎         | 11/330 [1:19:35<38:25:52, 433.71s/it]2025-09-27 08:07:53,044 Stage: Train 0.5 | Epoch: 11 | Iter: 16800 | Total Loss: 0.015956 | Recon Loss: 0.013415 | Commit Loss: 0.005083 | Perplexity: 1405.126754
2025-09-27 08:08:49,977 Stage: Train 0.5 | Epoch: 11 | Iter: 17000 | Total Loss: 0.015797 | Recon Loss: 0.013255 | Commit Loss: 0.005084 | Perplexity: 1404.863748
2025-09-27 08:09:47,214 Stage: Train 0.5 | Epoch: 11 | Iter: 17200 | Total Loss: 0.015643 | Recon Loss: 0.013167 | Commit Loss: 0.004952 | Perplexity: 1399.490670
2025-09-27 08:10:44,282 Stage: Train 0.5 | Epoch: 11 | Iter: 17400 | Total Loss: 0.015473 | Recon Loss: 0.012956 | Commit Loss: 0.005033 | Perplexity: 1406.277568
2025-09-27 08:11:41,559 Stage: Train 0.5 | Epoch: 11 | Iter: 17600 | Total Loss: 0.015596 | Recon Loss: 0.013094 | Commit Loss: 0.005005 | Perplexity: 1409.804319
2025-09-27 08:12:38,469 Stage: Train 0.5 | Epoch: 11 | Iter: 17800 | Total Loss: 0.015687 | Recon Loss: 0.013209 | Commit Loss: 0.004956 | Perplexity: 1405.487443
2025-09-27 08:13:35,475 Stage: Train 0.5 | Epoch: 11 | Iter: 18000 | Total Loss: 0.015237 | Recon Loss: 0.012761 | Commit Loss: 0.004953 | Perplexity: 1409.056061
2025-09-27 08:14:32,774 Stage: Train 0.5 | Epoch: 11 | Iter: 18200 | Total Loss: 0.015438 | Recon Loss: 0.012944 | Commit Loss: 0.004987 | Perplexity: 1407.606953
Trainning Epoch:   4%|▎         | 12/330 [1:26:49<38:18:51, 433.75s/it]2025-09-27 08:15:30,218 Stage: Train 0.5 | Epoch: 12 | Iter: 18400 | Total Loss: 0.015158 | Recon Loss: 0.012711 | Commit Loss: 0.004895 | Perplexity: 1412.432443
2025-09-27 08:16:27,290 Stage: Train 0.5 | Epoch: 12 | Iter: 18600 | Total Loss: 0.015136 | Recon Loss: 0.012722 | Commit Loss: 0.004829 | Perplexity: 1404.102802
2025-09-27 08:17:24,296 Stage: Train 0.5 | Epoch: 12 | Iter: 18800 | Total Loss: 0.015037 | Recon Loss: 0.012587 | Commit Loss: 0.004900 | Perplexity: 1411.593809
2025-09-27 08:18:21,189 Stage: Train 0.5 | Epoch: 12 | Iter: 19000 | Total Loss: 0.015090 | Recon Loss: 0.012654 | Commit Loss: 0.004873 | Perplexity: 1414.030534
2025-09-27 08:19:18,355 Stage: Train 0.5 | Epoch: 12 | Iter: 19200 | Total Loss: 0.015039 | Recon Loss: 0.012630 | Commit Loss: 0.004818 | Perplexity: 1415.680723
2025-09-27 08:20:15,478 Stage: Train 0.5 | Epoch: 12 | Iter: 19400 | Total Loss: 0.014763 | Recon Loss: 0.012390 | Commit Loss: 0.004746 | Perplexity: 1410.558868
2025-09-27 08:21:12,578 Stage: Train 0.5 | Epoch: 12 | Iter: 19600 | Total Loss: 0.014847 | Recon Loss: 0.012425 | Commit Loss: 0.004844 | Perplexity: 1419.220917
Trainning Epoch:   4%|▍         | 13/330 [1:34:03<38:11:32, 433.73s/it]2025-09-27 08:22:09,782 Stage: Train 0.5 | Epoch: 13 | Iter: 19800 | Total Loss: 0.014682 | Recon Loss: 0.012299 | Commit Loss: 0.004765 | Perplexity: 1419.790298
2025-09-27 08:23:06,831 Stage: Train 0.5 | Epoch: 13 | Iter: 20000 | Total Loss: 0.014750 | Recon Loss: 0.012423 | Commit Loss: 0.004654 | Perplexity: 1410.047524
2025-09-27 08:23:06,831 Saving model at iteration 20000
2025-09-27 08:23:07,017 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000
2025-09-27 08:23:07,473 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/model.safetensors
2025-09-27 08:23:07,976 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/optimizer.bin
2025-09-27 08:23:07,976 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/scheduler.bin
2025-09-27 08:23:07,976 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/sampler.bin
2025-09-27 08:23:07,977 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_14_step_20000/random_states_0.pkl
2025-09-27 08:24:05,009 Stage: Train 0.5 | Epoch: 13 | Iter: 20200 | Total Loss: 0.014542 | Recon Loss: 0.012181 | Commit Loss: 0.004722 | Perplexity: 1413.977493
2025-09-27 08:25:01,845 Stage: Train 0.5 | Epoch: 13 | Iter: 20400 | Total Loss: 0.014419 | Recon Loss: 0.012115 | Commit Loss: 0.004608 | Perplexity: 1416.238797
2025-09-27 08:25:58,732 Stage: Train 0.5 | Epoch: 13 | Iter: 20600 | Total Loss: 0.014406 | Recon Loss: 0.012099 | Commit Loss: 0.004613 | Perplexity: 1413.528697
2025-09-27 08:26:55,677 Stage: Train 0.5 | Epoch: 13 | Iter: 20800 | Total Loss: 0.014443 | Recon Loss: 0.012168 | Commit Loss: 0.004552 | Perplexity: 1412.512863
2025-09-27 08:27:52,493 Stage: Train 0.5 | Epoch: 13 | Iter: 21000 | Total Loss: 0.014153 | Recon Loss: 0.011908 | Commit Loss: 0.004490 | Perplexity: 1415.308595
2025-09-27 08:28:49,302 Stage: Train 0.5 | Epoch: 13 | Iter: 21200 | Total Loss: 0.014128 | Recon Loss: 0.011861 | Commit Loss: 0.004534 | Perplexity: 1413.873364
Trainning Epoch:   4%|▍         | 14/330 [1:41:16<38:04:15, 433.72s/it]2025-09-27 08:29:46,487 Stage: Train 0.5 | Epoch: 14 | Iter: 21400 | Total Loss: 0.014037 | Recon Loss: 0.011827 | Commit Loss: 0.004420 | Perplexity: 1403.597465
2025-09-27 08:30:43,438 Stage: Train 0.5 | Epoch: 14 | Iter: 21600 | Total Loss: 0.014070 | Recon Loss: 0.011859 | Commit Loss: 0.004424 | Perplexity: 1408.508879
2025-09-27 08:31:40,433 Stage: Train 0.5 | Epoch: 14 | Iter: 21800 | Total Loss: 0.013899 | Recon Loss: 0.011725 | Commit Loss: 0.004348 | Perplexity: 1406.865081
2025-09-27 08:32:37,640 Stage: Train 0.5 | Epoch: 14 | Iter: 22000 | Total Loss: 0.013746 | Recon Loss: 0.011584 | Commit Loss: 0.004324 | Perplexity: 1407.602957
2025-09-27 08:33:34,709 Stage: Train 0.5 | Epoch: 14 | Iter: 22200 | Total Loss: 0.013642 | Recon Loss: 0.011494 | Commit Loss: 0.004296 | Perplexity: 1403.749182
2025-09-27 08:34:31,764 Stage: Train 0.5 | Epoch: 14 | Iter: 22400 | Total Loss: 0.013675 | Recon Loss: 0.011540 | Commit Loss: 0.004270 | Perplexity: 1404.234201
2025-09-27 08:35:28,603 Stage: Train 0.5 | Epoch: 14 | Iter: 22600 | Total Loss: 0.013725 | Recon Loss: 0.011612 | Commit Loss: 0.004224 | Perplexity: 1401.223135
Trainning Epoch:   5%|▍         | 15/330 [1:48:29<37:55:33, 433.44s/it]2025-09-27 08:36:25,403 Stage: Train 0.5 | Epoch: 15 | Iter: 22800 | Total Loss: 0.013632 | Recon Loss: 0.011550 | Commit Loss: 0.004163 | Perplexity: 1397.571174
2025-09-27 08:37:22,291 Stage: Train 0.5 | Epoch: 15 | Iter: 23000 | Total Loss: 0.013398 | Recon Loss: 0.011347 | Commit Loss: 0.004103 | Perplexity: 1396.544500
2025-09-27 08:38:19,351 Stage: Train 0.5 | Epoch: 15 | Iter: 23200 | Total Loss: 0.013487 | Recon Loss: 0.011440 | Commit Loss: 0.004093 | Perplexity: 1394.098337
2025-09-27 08:39:16,669 Stage: Train 0.5 | Epoch: 15 | Iter: 23400 | Total Loss: 0.013288 | Recon Loss: 0.011223 | Commit Loss: 0.004129 | Perplexity: 1397.223979
2025-09-27 08:40:13,800 Stage: Train 0.5 | Epoch: 15 | Iter: 23600 | Total Loss: 0.013498 | Recon Loss: 0.011468 | Commit Loss: 0.004059 | Perplexity: 1392.795425
2025-09-27 08:41:10,784 Stage: Train 0.5 | Epoch: 15 | Iter: 23800 | Total Loss: 0.013169 | Recon Loss: 0.011155 | Commit Loss: 0.004028 | Perplexity: 1394.867837
2025-09-27 08:42:07,801 Stage: Train 0.5 | Epoch: 15 | Iter: 24000 | Total Loss: 0.013195 | Recon Loss: 0.011195 | Commit Loss: 0.004001 | Perplexity: 1397.391642
2025-09-27 08:43:04,620 Stage: Train 0.5 | Epoch: 15 | Iter: 24200 | Total Loss: 0.013026 | Recon Loss: 0.011066 | Commit Loss: 0.003921 | Perplexity: 1391.737259
Trainning Epoch:   5%|▍         | 16/330 [1:55:42<37:48:05, 433.39s/it]2025-09-27 08:44:01,746 Stage: Train 0.5 | Epoch: 16 | Iter: 24400 | Total Loss: 0.012866 | Recon Loss: 0.010937 | Commit Loss: 0.003857 | Perplexity: 1387.073912
2025-09-27 08:44:59,120 Stage: Train 0.5 | Epoch: 16 | Iter: 24600 | Total Loss: 0.012943 | Recon Loss: 0.011013 | Commit Loss: 0.003860 | Perplexity: 1396.178788
2025-09-27 08:45:56,424 Stage: Train 0.5 | Epoch: 16 | Iter: 24800 | Total Loss: 0.012709 | Recon Loss: 0.010785 | Commit Loss: 0.003848 | Perplexity: 1389.632539
2025-09-27 08:46:53,676 Stage: Train 0.5 | Epoch: 16 | Iter: 25000 | Total Loss: 0.012773 | Recon Loss: 0.010819 | Commit Loss: 0.003907 | Perplexity: 1397.385609
2025-09-27 08:47:50,654 Stage: Train 0.5 | Epoch: 16 | Iter: 25200 | Total Loss: 0.013002 | Recon Loss: 0.011078 | Commit Loss: 0.003848 | Perplexity: 1395.057494
2025-09-27 08:48:47,274 Stage: Train 0.5 | Epoch: 16 | Iter: 25400 | Total Loss: 0.012692 | Recon Loss: 0.010781 | Commit Loss: 0.003820 | Perplexity: 1386.604951
2025-09-27 08:49:44,587 Stage: Train 0.5 | Epoch: 16 | Iter: 25600 | Total Loss: 0.012574 | Recon Loss: 0.010711 | Commit Loss: 0.003726 | Perplexity: 1382.463253
2025-09-27 08:50:41,759 Stage: Train 0.5 | Epoch: 16 | Iter: 25800 | Total Loss: 0.012703 | Recon Loss: 0.010858 | Commit Loss: 0.003691 | Perplexity: 1377.890936
Trainning Epoch:   5%|▌         | 17/330 [2:02:57<37:42:03, 433.62s/it]2025-09-27 08:51:38,873 Stage: Train 0.5 | Epoch: 17 | Iter: 26000 | Total Loss: 0.012446 | Recon Loss: 0.010588 | Commit Loss: 0.003715 | Perplexity: 1384.012555
2025-09-27 08:52:35,911 Stage: Train 0.5 | Epoch: 17 | Iter: 26200 | Total Loss: 0.012413 | Recon Loss: 0.010582 | Commit Loss: 0.003663 | Perplexity: 1374.453231
2025-09-27 08:53:32,816 Stage: Train 0.5 | Epoch: 17 | Iter: 26400 | Total Loss: 0.012414 | Recon Loss: 0.010572 | Commit Loss: 0.003684 | Perplexity: 1383.921074
2025-09-27 08:54:29,326 Stage: Train 0.5 | Epoch: 17 | Iter: 26600 | Total Loss: 0.012417 | Recon Loss: 0.010614 | Commit Loss: 0.003607 | Perplexity: 1375.774617
2025-09-27 08:55:26,116 Stage: Train 0.5 | Epoch: 17 | Iter: 26800 | Total Loss: 0.012300 | Recon Loss: 0.010491 | Commit Loss: 0.003616 | Perplexity: 1374.421247
2025-09-27 08:56:23,109 Stage: Train 0.5 | Epoch: 17 | Iter: 27000 | Total Loss: 0.012366 | Recon Loss: 0.010558 | Commit Loss: 0.003617 | Perplexity: 1375.857969
2025-09-27 08:57:20,260 Stage: Train 0.5 | Epoch: 17 | Iter: 27200 | Total Loss: 0.012197 | Recon Loss: 0.010412 | Commit Loss: 0.003571 | Perplexity: 1374.050800
Trainning Epoch:   5%|▌         | 18/330 [2:10:09<37:32:49, 433.24s/it]2025-09-27 08:58:17,485 Stage: Train 0.5 | Epoch: 18 | Iter: 27400 | Total Loss: 0.012099 | Recon Loss: 0.010323 | Commit Loss: 0.003553 | Perplexity: 1371.491100
2025-09-27 08:59:14,591 Stage: Train 0.5 | Epoch: 18 | Iter: 27600 | Total Loss: 0.012111 | Recon Loss: 0.010361 | Commit Loss: 0.003499 | Perplexity: 1370.350740
2025-09-27 09:00:11,175 Stage: Train 0.5 | Epoch: 18 | Iter: 27800 | Total Loss: 0.011980 | Recon Loss: 0.010252 | Commit Loss: 0.003455 | Perplexity: 1367.310810
2025-09-27 09:01:08,349 Stage: Train 0.5 | Epoch: 18 | Iter: 28000 | Total Loss: 0.011791 | Recon Loss: 0.010071 | Commit Loss: 0.003439 | Perplexity: 1369.187133
2025-09-27 09:02:05,651 Stage: Train 0.5 | Epoch: 18 | Iter: 28200 | Total Loss: 0.011986 | Recon Loss: 0.010277 | Commit Loss: 0.003419 | Perplexity: 1369.406219
2025-09-27 09:03:02,917 Stage: Train 0.5 | Epoch: 18 | Iter: 28400 | Total Loss: 0.011677 | Recon Loss: 0.009976 | Commit Loss: 0.003401 | Perplexity: 1372.562766
2025-09-27 09:04:00,126 Stage: Train 0.5 | Epoch: 18 | Iter: 28600 | Total Loss: 0.011721 | Recon Loss: 0.010039 | Commit Loss: 0.003363 | Perplexity: 1375.132737
2025-09-27 09:04:57,298 Stage: Train 0.5 | Epoch: 18 | Iter: 28800 | Total Loss: 0.011721 | Recon Loss: 0.010048 | Commit Loss: 0.003346 | Perplexity: 1374.322185
Trainning Epoch:   6%|▌         | 19/330 [2:17:23<37:26:44, 433.45s/it]2025-09-27 09:05:54,646 Stage: Train 0.5 | Epoch: 19 | Iter: 29000 | Total Loss: 0.011536 | Recon Loss: 0.009891 | Commit Loss: 0.003291 | Perplexity: 1380.141100
2025-09-27 09:06:51,629 Stage: Train 0.5 | Epoch: 19 | Iter: 29200 | Total Loss: 0.011381 | Recon Loss: 0.009767 | Commit Loss: 0.003229 | Perplexity: 1383.327565
2025-09-27 09:07:48,772 Stage: Train 0.5 | Epoch: 19 | Iter: 29400 | Total Loss: 0.011504 | Recon Loss: 0.009901 | Commit Loss: 0.003207 | Perplexity: 1380.273503
2025-09-27 09:08:45,606 Stage: Train 0.5 | Epoch: 19 | Iter: 29600 | Total Loss: 0.011411 | Recon Loss: 0.009829 | Commit Loss: 0.003163 | Perplexity: 1380.707199
2025-09-27 09:09:41,075 Stage: Train 0.5 | Epoch: 19 | Iter: 29800 | Total Loss: 0.011352 | Recon Loss: 0.009774 | Commit Loss: 0.003155 | Perplexity: 1383.761751
2025-09-27 09:10:38,511 Stage: Train 0.5 | Epoch: 19 | Iter: 30000 | Total Loss: 0.011134 | Recon Loss: 0.009568 | Commit Loss: 0.003132 | Perplexity: 1388.738022
2025-09-27 09:11:35,801 Stage: Train 0.5 | Epoch: 19 | Iter: 30200 | Total Loss: 0.011206 | Recon Loss: 0.009647 | Commit Loss: 0.003119 | Perplexity: 1386.700047
Trainning Epoch:   6%|▌         | 20/330 [2:24:35<37:17:23, 433.04s/it]2025-09-27 09:12:32,648 Stage: Train 0.5 | Epoch: 20 | Iter: 30400 | Total Loss: 0.011144 | Recon Loss: 0.009621 | Commit Loss: 0.003046 | Perplexity: 1381.197391
2025-09-27 09:13:30,009 Stage: Train 0.5 | Epoch: 20 | Iter: 30600 | Total Loss: 0.011216 | Recon Loss: 0.009721 | Commit Loss: 0.002990 | Perplexity: 1388.072205
2025-09-27 09:14:27,201 Stage: Train 0.5 | Epoch: 20 | Iter: 30800 | Total Loss: 0.011008 | Recon Loss: 0.009524 | Commit Loss: 0.002967 | Perplexity: 1387.630718
2025-09-27 09:15:24,417 Stage: Train 0.5 | Epoch: 20 | Iter: 31000 | Total Loss: 0.010913 | Recon Loss: 0.009432 | Commit Loss: 0.002963 | Perplexity: 1389.336070
2025-09-27 09:16:21,426 Stage: Train 0.5 | Epoch: 20 | Iter: 31200 | Total Loss: 0.010926 | Recon Loss: 0.009463 | Commit Loss: 0.002927 | Perplexity: 1390.231625
2025-09-27 09:17:18,655 Stage: Train 0.5 | Epoch: 20 | Iter: 31400 | Total Loss: 0.010798 | Recon Loss: 0.009349 | Commit Loss: 0.002898 | Perplexity: 1390.951712
2025-09-27 09:18:15,472 Stage: Train 0.5 | Epoch: 20 | Iter: 31600 | Total Loss: 0.010835 | Recon Loss: 0.009392 | Commit Loss: 0.002885 | Perplexity: 1391.527216
2025-09-27 09:19:12,515 Stage: Train 0.5 | Epoch: 20 | Iter: 31800 | Total Loss: 0.010688 | Recon Loss: 0.009256 | Commit Loss: 0.002863 | Perplexity: 1394.968186
Trainning Epoch:   6%|▋         | 21/330 [2:31:49<37:11:38, 433.33s/it]2025-09-27 09:20:09,698 Stage: Train 0.5 | Epoch: 21 | Iter: 32000 | Total Loss: 0.010661 | Recon Loss: 0.009249 | Commit Loss: 0.002824 | Perplexity: 1398.171477
2025-09-27 09:21:06,875 Stage: Train 0.5 | Epoch: 21 | Iter: 32200 | Total Loss: 0.010660 | Recon Loss: 0.009254 | Commit Loss: 0.002812 | Perplexity: 1396.539379
2025-09-27 09:22:03,964 Stage: Train 0.5 | Epoch: 21 | Iter: 32400 | Total Loss: 0.010622 | Recon Loss: 0.009227 | Commit Loss: 0.002789 | Perplexity: 1396.192582
2025-09-27 09:23:00,979 Stage: Train 0.5 | Epoch: 21 | Iter: 32600 | Total Loss: 0.010581 | Recon Loss: 0.009186 | Commit Loss: 0.002790 | Perplexity: 1405.260286
2025-09-27 09:23:58,066 Stage: Train 0.5 | Epoch: 21 | Iter: 32800 | Total Loss: 0.010580 | Recon Loss: 0.009198 | Commit Loss: 0.002764 | Perplexity: 1404.546865
2025-09-27 09:24:54,630 Stage: Train 0.5 | Epoch: 21 | Iter: 33000 | Total Loss: 0.010554 | Recon Loss: 0.009199 | Commit Loss: 0.002711 | Perplexity: 1404.311799
2025-09-27 09:25:51,795 Stage: Train 0.5 | Epoch: 21 | Iter: 33200 | Total Loss: 0.010305 | Recon Loss: 0.008964 | Commit Loss: 0.002682 | Perplexity: 1402.275373
2025-09-27 09:26:48,984 Stage: Train 0.5 | Epoch: 21 | Iter: 33400 | Total Loss: 0.010356 | Recon Loss: 0.009001 | Commit Loss: 0.002710 | Perplexity: 1403.574191
Trainning Epoch:   7%|▋         | 22/330 [2:39:02<37:04:24, 433.33s/it]2025-09-27 09:27:46,287 Stage: Train 0.5 | Epoch: 22 | Iter: 33600 | Total Loss: 0.010348 | Recon Loss: 0.009015 | Commit Loss: 0.002667 | Perplexity: 1401.729432
2025-09-27 09:28:43,268 Stage: Train 0.5 | Epoch: 22 | Iter: 33800 | Total Loss: 0.010359 | Recon Loss: 0.009030 | Commit Loss: 0.002659 | Perplexity: 1407.219418
2025-09-27 09:29:40,469 Stage: Train 0.5 | Epoch: 22 | Iter: 34000 | Total Loss: 0.010209 | Recon Loss: 0.008867 | Commit Loss: 0.002685 | Perplexity: 1413.283621
2025-09-27 09:30:37,095 Stage: Train 0.5 | Epoch: 22 | Iter: 34200 | Total Loss: 0.010385 | Recon Loss: 0.009062 | Commit Loss: 0.002646 | Perplexity: 1411.059993
2025-09-27 09:31:34,204 Stage: Train 0.5 | Epoch: 22 | Iter: 34400 | Total Loss: 0.010257 | Recon Loss: 0.008960 | Commit Loss: 0.002593 | Perplexity: 1410.867991
2025-09-27 09:32:30,995 Stage: Train 0.5 | Epoch: 22 | Iter: 34600 | Total Loss: 0.010169 | Recon Loss: 0.008880 | Commit Loss: 0.002578 | Perplexity: 1414.991534
2025-09-27 09:33:28,002 Stage: Train 0.5 | Epoch: 22 | Iter: 34800 | Total Loss: 0.009972 | Recon Loss: 0.008686 | Commit Loss: 0.002571 | Perplexity: 1417.059985
Trainning Epoch:   7%|▋         | 23/330 [2:46:15<36:56:35, 433.21s/it]2025-09-27 09:34:25,204 Stage: Train 0.5 | Epoch: 23 | Iter: 35000 | Total Loss: 0.010280 | Recon Loss: 0.008987 | Commit Loss: 0.002586 | Perplexity: 1424.743414
2025-09-27 09:35:22,460 Stage: Train 0.5 | Epoch: 23 | Iter: 35200 | Total Loss: 0.010155 | Recon Loss: 0.008879 | Commit Loss: 0.002553 | Perplexity: 1427.968011
2025-09-27 09:36:19,323 Stage: Train 0.5 | Epoch: 23 | Iter: 35400 | Total Loss: 0.009924 | Recon Loss: 0.008649 | Commit Loss: 0.002550 | Perplexity: 1430.866433
2025-09-27 09:37:16,588 Stage: Train 0.5 | Epoch: 23 | Iter: 35600 | Total Loss: 0.009895 | Recon Loss: 0.008636 | Commit Loss: 0.002519 | Perplexity: 1433.444355
2025-09-27 09:38:13,546 Stage: Train 0.5 | Epoch: 23 | Iter: 35800 | Total Loss: 0.009873 | Recon Loss: 0.008626 | Commit Loss: 0.002494 | Perplexity: 1429.898883
2025-09-27 09:39:10,541 Stage: Train 0.5 | Epoch: 23 | Iter: 36000 | Total Loss: 0.009780 | Recon Loss: 0.008536 | Commit Loss: 0.002489 | Perplexity: 1433.560585
2025-09-27 09:40:07,402 Stage: Train 0.5 | Epoch: 23 | Iter: 36200 | Total Loss: 0.009788 | Recon Loss: 0.008581 | Commit Loss: 0.002415 | Perplexity: 1432.136940
2025-09-27 09:41:04,517 Stage: Train 0.5 | Epoch: 23 | Iter: 36400 | Total Loss: 0.009853 | Recon Loss: 0.008615 | Commit Loss: 0.002474 | Perplexity: 1447.136726
Trainning Epoch:   7%|▋         | 24/330 [2:53:29<36:49:40, 433.27s/it]2025-09-27 09:42:01,288 Stage: Train 0.5 | Epoch: 24 | Iter: 36600 | Total Loss: 0.009879 | Recon Loss: 0.008662 | Commit Loss: 0.002434 | Perplexity: 1447.780553
2025-09-27 09:42:58,330 Stage: Train 0.5 | Epoch: 24 | Iter: 36800 | Total Loss: 0.009729 | Recon Loss: 0.008526 | Commit Loss: 0.002406 | Perplexity: 1451.198839
2025-09-27 09:43:55,555 Stage: Train 0.5 | Epoch: 24 | Iter: 37000 | Total Loss: 0.009569 | Recon Loss: 0.008374 | Commit Loss: 0.002389 | Perplexity: 1455.535151
2025-09-27 09:44:52,670 Stage: Train 0.5 | Epoch: 24 | Iter: 37200 | Total Loss: 0.009705 | Recon Loss: 0.008514 | Commit Loss: 0.002382 | Perplexity: 1461.913718
2025-09-27 09:45:49,805 Stage: Train 0.5 | Epoch: 24 | Iter: 37400 | Total Loss: 0.009605 | Recon Loss: 0.008418 | Commit Loss: 0.002373 | Perplexity: 1469.460263
2025-09-27 09:46:47,061 Stage: Train 0.5 | Epoch: 24 | Iter: 37600 | Total Loss: 0.009668 | Recon Loss: 0.008499 | Commit Loss: 0.002339 | Perplexity: 1469.857492
2025-09-27 09:47:44,297 Stage: Train 0.5 | Epoch: 24 | Iter: 37800 | Total Loss: 0.009584 | Recon Loss: 0.008420 | Commit Loss: 0.002328 | Perplexity: 1474.035280
Trainning Epoch:   8%|▊         | 25/330 [3:00:42<36:42:55, 433.36s/it]2025-09-27 09:48:41,277 Stage: Train 0.5 | Epoch: 25 | Iter: 38000 | Total Loss: 0.009371 | Recon Loss: 0.008205 | Commit Loss: 0.002333 | Perplexity: 1477.260104
2025-09-27 09:49:38,404 Stage: Train 0.5 | Epoch: 25 | Iter: 38200 | Total Loss: 0.009346 | Recon Loss: 0.008198 | Commit Loss: 0.002297 | Perplexity: 1484.613714
2025-09-27 09:50:35,620 Stage: Train 0.5 | Epoch: 25 | Iter: 38400 | Total Loss: 0.009558 | Recon Loss: 0.008415 | Commit Loss: 0.002288 | Perplexity: 1489.009129
2025-09-27 09:51:32,627 Stage: Train 0.5 | Epoch: 25 | Iter: 38600 | Total Loss: 0.009265 | Recon Loss: 0.008130 | Commit Loss: 0.002271 | Perplexity: 1489.475703
2025-09-27 09:52:29,659 Stage: Train 0.5 | Epoch: 25 | Iter: 38800 | Total Loss: 0.009246 | Recon Loss: 0.008107 | Commit Loss: 0.002279 | Perplexity: 1490.053029
2025-09-27 09:53:26,852 Stage: Train 0.5 | Epoch: 25 | Iter: 39000 | Total Loss: 0.009197 | Recon Loss: 0.008081 | Commit Loss: 0.002233 | Perplexity: 1495.274456
2025-09-27 09:54:23,409 Stage: Train 0.5 | Epoch: 25 | Iter: 39200 | Total Loss: 0.009236 | Recon Loss: 0.008115 | Commit Loss: 0.002242 | Perplexity: 1503.077313
2025-09-27 09:55:20,550 Stage: Train 0.5 | Epoch: 25 | Iter: 39400 | Total Loss: 0.009190 | Recon Loss: 0.008069 | Commit Loss: 0.002242 | Perplexity: 1508.435959
Trainning Epoch:   8%|▊         | 26/330 [3:07:56<36:35:37, 433.35s/it]2025-09-27 09:56:17,709 Stage: Train 0.5 | Epoch: 26 | Iter: 39600 | Total Loss: 0.009231 | Recon Loss: 0.008127 | Commit Loss: 0.002208 | Perplexity: 1509.309078
2025-09-27 09:57:14,999 Stage: Train 0.5 | Epoch: 26 | Iter: 39800 | Total Loss: 0.009166 | Recon Loss: 0.008074 | Commit Loss: 0.002185 | Perplexity: 1514.107736
2025-09-27 09:58:11,897 Stage: Train 0.5 | Epoch: 26 | Iter: 40000 | Total Loss: 0.009164 | Recon Loss: 0.008082 | Commit Loss: 0.002164 | Perplexity: 1512.023457
2025-09-27 09:58:11,897 Saving model at iteration 40000
2025-09-27 09:58:12,492 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000
2025-09-27 09:58:12,921 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/model.safetensors
2025-09-27 09:58:13,406 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/optimizer.bin
2025-09-27 09:58:13,406 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/scheduler.bin
2025-09-27 09:58:13,406 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/sampler.bin
2025-09-27 09:58:13,407 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_27_step_40000/random_states_0.pkl
2025-09-27 09:59:10,460 Stage: Train 0.5 | Epoch: 26 | Iter: 40200 | Total Loss: 0.009135 | Recon Loss: 0.008058 | Commit Loss: 0.002154 | Perplexity: 1520.486492
2025-09-27 10:00:07,149 Stage: Train 0.5 | Epoch: 26 | Iter: 40400 | Total Loss: 0.008982 | Recon Loss: 0.007909 | Commit Loss: 0.002145 | Perplexity: 1527.476733
2025-09-27 10:01:04,243 Stage: Train 0.5 | Epoch: 26 | Iter: 40600 | Total Loss: 0.009040 | Recon Loss: 0.007980 | Commit Loss: 0.002120 | Perplexity: 1533.936030
2025-09-27 10:02:01,407 Stage: Train 0.5 | Epoch: 26 | Iter: 40800 | Total Loss: 0.009012 | Recon Loss: 0.007962 | Commit Loss: 0.002101 | Perplexity: 1542.217944
2025-09-27 10:02:58,440 Stage: Train 0.5 | Epoch: 26 | Iter: 41000 | Total Loss: 0.008824 | Recon Loss: 0.007805 | Commit Loss: 0.002039 | Perplexity: 1541.865668
Trainning Epoch:   8%|▊         | 27/330 [3:15:10<36:30:39, 433.79s/it]2025-09-27 10:03:55,611 Stage: Train 0.5 | Epoch: 27 | Iter: 41200 | Total Loss: 0.008772 | Recon Loss: 0.007761 | Commit Loss: 0.002022 | Perplexity: 1541.430010
2025-09-27 10:04:52,822 Stage: Train 0.5 | Epoch: 27 | Iter: 41400 | Total Loss: 0.008833 | Recon Loss: 0.007820 | Commit Loss: 0.002026 | Perplexity: 1550.047825
2025-09-27 10:05:49,470 Stage: Train 0.5 | Epoch: 27 | Iter: 41600 | Total Loss: 0.008753 | Recon Loss: 0.007748 | Commit Loss: 0.002009 | Perplexity: 1545.606989
2025-09-27 10:06:46,751 Stage: Train 0.5 | Epoch: 27 | Iter: 41800 | Total Loss: 0.008769 | Recon Loss: 0.007775 | Commit Loss: 0.001988 | Perplexity: 1545.448879
2025-09-27 10:07:43,775 Stage: Train 0.5 | Epoch: 27 | Iter: 42000 | Total Loss: 0.008703 | Recon Loss: 0.007718 | Commit Loss: 0.001970 | Perplexity: 1551.268988
2025-09-27 10:08:40,694 Stage: Train 0.5 | Epoch: 27 | Iter: 42200 | Total Loss: 0.008773 | Recon Loss: 0.007779 | Commit Loss: 0.001989 | Perplexity: 1549.675820
2025-09-27 10:09:37,725 Stage: Train 0.5 | Epoch: 27 | Iter: 42400 | Total Loss: 0.008788 | Recon Loss: 0.007799 | Commit Loss: 0.001978 | Perplexity: 1552.724388
Trainning Epoch:   8%|▊         | 28/330 [3:22:24<36:22:30, 433.61s/it]2025-09-27 10:10:34,869 Stage: Train 0.5 | Epoch: 28 | Iter: 42600 | Total Loss: 0.008658 | Recon Loss: 0.007679 | Commit Loss: 0.001958 | Perplexity: 1551.736528
2025-09-27 10:11:31,993 Stage: Train 0.5 | Epoch: 28 | Iter: 42800 | Total Loss: 0.008596 | Recon Loss: 0.007614 | Commit Loss: 0.001963 | Perplexity: 1558.307399
2025-09-27 10:12:28,544 Stage: Train 0.5 | Epoch: 28 | Iter: 43000 | Total Loss: 0.008548 | Recon Loss: 0.007568 | Commit Loss: 0.001959 | Perplexity: 1554.018745
2025-09-27 10:13:25,597 Stage: Train 0.5 | Epoch: 28 | Iter: 43200 | Total Loss: 0.008575 | Recon Loss: 0.007605 | Commit Loss: 0.001940 | Perplexity: 1559.268408
2025-09-27 10:14:22,873 Stage: Train 0.5 | Epoch: 28 | Iter: 43400 | Total Loss: 0.008557 | Recon Loss: 0.007578 | Commit Loss: 0.001958 | Perplexity: 1562.487257
2025-09-27 10:15:20,007 Stage: Train 0.5 | Epoch: 28 | Iter: 43600 | Total Loss: 0.008459 | Recon Loss: 0.007491 | Commit Loss: 0.001936 | Perplexity: 1557.169915
2025-09-27 10:16:16,974 Stage: Train 0.5 | Epoch: 28 | Iter: 43800 | Total Loss: 0.008585 | Recon Loss: 0.007621 | Commit Loss: 0.001928 | Perplexity: 1559.922178
2025-09-27 10:17:14,034 Stage: Train 0.5 | Epoch: 28 | Iter: 44000 | Total Loss: 0.008517 | Recon Loss: 0.007555 | Commit Loss: 0.001924 | Perplexity: 1560.336661
Trainning Epoch:   9%|▉         | 29/330 [3:29:37<36:14:43, 433.50s/it]2025-09-27 10:18:10,926 Stage: Train 0.5 | Epoch: 29 | Iter: 44200 | Total Loss: 0.008432 | Recon Loss: 0.007484 | Commit Loss: 0.001896 | Perplexity: 1560.274695
2025-09-27 10:19:07,828 Stage: Train 0.5 | Epoch: 29 | Iter: 44400 | Total Loss: 0.008462 | Recon Loss: 0.007512 | Commit Loss: 0.001900 | Perplexity: 1562.489992
2025-09-27 10:20:05,116 Stage: Train 0.5 | Epoch: 29 | Iter: 44600 | Total Loss: 0.008454 | Recon Loss: 0.007492 | Commit Loss: 0.001924 | Perplexity: 1564.234933
2025-09-27 10:21:02,431 Stage: Train 0.5 | Epoch: 29 | Iter: 44800 | Total Loss: 0.008403 | Recon Loss: 0.007454 | Commit Loss: 0.001898 | Perplexity: 1558.701278
2025-09-27 10:21:59,766 Stage: Train 0.5 | Epoch: 29 | Iter: 45000 | Total Loss: 0.008386 | Recon Loss: 0.007426 | Commit Loss: 0.001919 | Perplexity: 1564.153523
2025-09-27 10:22:57,021 Stage: Train 0.5 | Epoch: 29 | Iter: 45200 | Total Loss: 0.008437 | Recon Loss: 0.007486 | Commit Loss: 0.001902 | Perplexity: 1563.573409
2025-09-27 10:23:53,580 Stage: Train 0.5 | Epoch: 29 | Iter: 45400 | Total Loss: 0.008350 | Recon Loss: 0.007410 | Commit Loss: 0.001880 | Perplexity: 1563.862925
Trainning Epoch:   9%|▉         | 30/330 [3:36:50<36:07:23, 433.48s/it]2025-09-27 10:24:50,710 Stage: Train 0.5 | Epoch: 30 | Iter: 45600 | Total Loss: 0.008430 | Recon Loss: 0.007480 | Commit Loss: 0.001901 | Perplexity: 1568.139474
2025-09-27 10:25:47,781 Stage: Train 0.5 | Epoch: 30 | Iter: 45800 | Total Loss: 0.008391 | Recon Loss: 0.007462 | Commit Loss: 0.001858 | Perplexity: 1563.432870
2025-09-27 10:26:44,791 Stage: Train 0.5 | Epoch: 30 | Iter: 46000 | Total Loss: 0.008364 | Recon Loss: 0.007428 | Commit Loss: 0.001872 | Perplexity: 1564.807638
2025-09-27 10:27:41,003 Stage: Train 0.5 | Epoch: 30 | Iter: 46200 | Total Loss: 0.008335 | Recon Loss: 0.007396 | Commit Loss: 0.001878 | Perplexity: 1567.405974
2025-09-27 10:28:36,975 Stage: Train 0.5 | Epoch: 30 | Iter: 46400 | Total Loss: 0.008374 | Recon Loss: 0.007431 | Commit Loss: 0.001885 | Perplexity: 1566.106769
2025-09-27 10:29:34,114 Stage: Train 0.5 | Epoch: 30 | Iter: 46600 | Total Loss: 0.008367 | Recon Loss: 0.007429 | Commit Loss: 0.001875 | Perplexity: 1566.299443
2025-09-27 10:30:30,715 Stage: Train 0.5 | Epoch: 30 | Iter: 46800 | Total Loss: 0.008396 | Recon Loss: 0.007465 | Commit Loss: 0.001862 | Perplexity: 1564.408182
2025-09-27 10:31:27,750 Stage: Train 0.5 | Epoch: 30 | Iter: 47000 | Total Loss: 0.008190 | Recon Loss: 0.007269 | Commit Loss: 0.001842 | Perplexity: 1564.297969
Trainning Epoch:   9%|▉         | 31/330 [3:44:01<35:56:47, 432.80s/it]2025-09-27 10:32:25,331 Stage: Train 0.5 | Epoch: 31 | Iter: 47200 | Total Loss: 0.008077 | Recon Loss: 0.007146 | Commit Loss: 0.001863 | Perplexity: 1571.516142
2025-09-27 10:33:22,524 Stage: Train 0.5 | Epoch: 31 | Iter: 47400 | Total Loss: 0.008215 | Recon Loss: 0.007287 | Commit Loss: 0.001856 | Perplexity: 1566.542220
2025-09-27 10:34:19,772 Stage: Train 0.5 | Epoch: 31 | Iter: 47600 | Total Loss: 0.008363 | Recon Loss: 0.007438 | Commit Loss: 0.001850 | Perplexity: 1569.423253
2025-09-27 10:35:17,055 Stage: Train 0.5 | Epoch: 31 | Iter: 47800 | Total Loss: 0.008148 | Recon Loss: 0.007222 | Commit Loss: 0.001850 | Perplexity: 1568.944008
2025-09-27 10:36:13,886 Stage: Train 0.5 | Epoch: 31 | Iter: 48000 | Total Loss: 0.008272 | Recon Loss: 0.007350 | Commit Loss: 0.001844 | Perplexity: 1567.356664
2025-09-27 10:37:11,172 Stage: Train 0.5 | Epoch: 31 | Iter: 48200 | Total Loss: 0.008172 | Recon Loss: 0.007241 | Commit Loss: 0.001862 | Perplexity: 1576.148868
2025-09-27 10:38:08,525 Stage: Train 0.5 | Epoch: 31 | Iter: 48400 | Total Loss: 0.008049 | Recon Loss: 0.007135 | Commit Loss: 0.001828 | Perplexity: 1575.445308
2025-09-27 10:39:05,780 Stage: Train 0.5 | Epoch: 31 | Iter: 48600 | Total Loss: 0.008316 | Recon Loss: 0.007399 | Commit Loss: 0.001834 | Perplexity: 1577.289966
Trainning Epoch:  10%|▉         | 32/330 [3:51:16<35:52:34, 433.40s/it]2025-09-27 10:40:03,232 Stage: Train 0.5 | Epoch: 32 | Iter: 48800 | Total Loss: 0.008193 | Recon Loss: 0.007282 | Commit Loss: 0.001823 | Perplexity: 1574.772181
2025-09-27 10:41:00,573 Stage: Train 0.5 | Epoch: 32 | Iter: 49000 | Total Loss: 0.008004 | Recon Loss: 0.007093 | Commit Loss: 0.001823 | Perplexity: 1574.854840
2025-09-27 10:41:57,536 Stage: Train 0.5 | Epoch: 32 | Iter: 49200 | Total Loss: 0.008227 | Recon Loss: 0.007318 | Commit Loss: 0.001819 | Perplexity: 1574.492832
2025-09-27 10:42:54,822 Stage: Train 0.5 | Epoch: 32 | Iter: 49400 | Total Loss: 0.008048 | Recon Loss: 0.007128 | Commit Loss: 0.001839 | Perplexity: 1576.505966
2025-09-27 10:43:52,276 Stage: Train 0.5 | Epoch: 32 | Iter: 49600 | Total Loss: 0.007979 | Recon Loss: 0.007078 | Commit Loss: 0.001802 | Perplexity: 1577.195956
2025-09-27 10:44:49,594 Stage: Train 0.5 | Epoch: 32 | Iter: 49800 | Total Loss: 0.008052 | Recon Loss: 0.007136 | Commit Loss: 0.001832 | Perplexity: 1577.865412
2025-09-27 10:45:46,941 Stage: Train 0.5 | Epoch: 32 | Iter: 50000 | Total Loss: 0.007957 | Recon Loss: 0.007047 | Commit Loss: 0.001820 | Perplexity: 1573.803684
Trainning Epoch:  10%|█         | 33/330 [3:58:31<35:48:06, 433.96s/it]2025-09-27 10:46:44,440 Stage: Train 0.5 | Epoch: 33 | Iter: 50200 | Total Loss: 0.008047 | Recon Loss: 0.007141 | Commit Loss: 0.001812 | Perplexity: 1579.474818
2025-09-27 10:47:41,235 Stage: Train 0.5 | Epoch: 33 | Iter: 50400 | Total Loss: 0.008030 | Recon Loss: 0.007134 | Commit Loss: 0.001791 | Perplexity: 1579.309913
2025-09-27 10:48:38,503 Stage: Train 0.5 | Epoch: 33 | Iter: 50600 | Total Loss: 0.008001 | Recon Loss: 0.007110 | Commit Loss: 0.001783 | Perplexity: 1575.466202
2025-09-27 10:49:35,587 Stage: Train 0.5 | Epoch: 33 | Iter: 50800 | Total Loss: 0.007867 | Recon Loss: 0.006968 | Commit Loss: 0.001798 | Perplexity: 1581.071495
2025-09-27 10:50:32,915 Stage: Train 0.5 | Epoch: 33 | Iter: 51000 | Total Loss: 0.008023 | Recon Loss: 0.007124 | Commit Loss: 0.001799 | Perplexity: 1584.224752
2025-09-27 10:51:30,127 Stage: Train 0.5 | Epoch: 33 | Iter: 51200 | Total Loss: 0.007953 | Recon Loss: 0.007063 | Commit Loss: 0.001780 | Perplexity: 1580.201620
2025-09-27 10:52:27,249 Stage: Train 0.5 | Epoch: 33 | Iter: 51400 | Total Loss: 0.007826 | Recon Loss: 0.006936 | Commit Loss: 0.001780 | Perplexity: 1585.047953
2025-09-27 10:53:24,381 Stage: Train 0.5 | Epoch: 33 | Iter: 51600 | Total Loss: 0.007946 | Recon Loss: 0.007057 | Commit Loss: 0.001779 | Perplexity: 1591.156977
Trainning Epoch:  10%|█         | 34/330 [4:05:46<35:41:11, 434.02s/it]2025-09-27 10:54:21,517 Stage: Train 0.5 | Epoch: 34 | Iter: 51800 | Total Loss: 0.007918 | Recon Loss: 0.007029 | Commit Loss: 0.001778 | Perplexity: 1590.177160
2025-09-27 10:55:18,956 Stage: Train 0.5 | Epoch: 34 | Iter: 52000 | Total Loss: 0.007811 | Recon Loss: 0.006919 | Commit Loss: 0.001784 | Perplexity: 1593.258258
2025-09-27 10:56:16,158 Stage: Train 0.5 | Epoch: 34 | Iter: 52200 | Total Loss: 0.007841 | Recon Loss: 0.006965 | Commit Loss: 0.001751 | Perplexity: 1591.913652
2025-09-27 10:57:13,433 Stage: Train 0.5 | Epoch: 34 | Iter: 52400 | Total Loss: 0.007765 | Recon Loss: 0.006880 | Commit Loss: 0.001770 | Perplexity: 1600.407178
2025-09-27 10:58:10,354 Stage: Train 0.5 | Epoch: 34 | Iter: 52600 | Total Loss: 0.007831 | Recon Loss: 0.006957 | Commit Loss: 0.001748 | Perplexity: 1595.173860
2025-09-27 10:59:07,550 Stage: Train 0.5 | Epoch: 34 | Iter: 52800 | Total Loss: 0.007733 | Recon Loss: 0.006854 | Commit Loss: 0.001758 | Perplexity: 1600.568365
2025-09-27 11:00:04,423 Stage: Train 0.5 | Epoch: 34 | Iter: 53000 | Total Loss: 0.007784 | Recon Loss: 0.006911 | Commit Loss: 0.001745 | Perplexity: 1603.290090
Trainning Epoch:  11%|█         | 35/330 [4:13:00<35:33:57, 434.02s/it]2025-09-27 11:01:01,736 Stage: Train 0.5 | Epoch: 35 | Iter: 53200 | Total Loss: 0.007857 | Recon Loss: 0.006977 | Commit Loss: 0.001760 | Perplexity: 1605.360914
2025-09-27 11:01:59,043 Stage: Train 0.5 | Epoch: 35 | Iter: 53400 | Total Loss: 0.007732 | Recon Loss: 0.006873 | Commit Loss: 0.001718 | Perplexity: 1604.750375
2025-09-27 11:02:56,155 Stage: Train 0.5 | Epoch: 35 | Iter: 53600 | Total Loss: 0.007767 | Recon Loss: 0.006895 | Commit Loss: 0.001744 | Perplexity: 1606.943177
2025-09-27 11:03:53,345 Stage: Train 0.5 | Epoch: 35 | Iter: 53800 | Total Loss: 0.007734 | Recon Loss: 0.006859 | Commit Loss: 0.001750 | Perplexity: 1613.414647
2025-09-27 11:04:50,651 Stage: Train 0.5 | Epoch: 35 | Iter: 54000 | Total Loss: 0.007657 | Recon Loss: 0.006809 | Commit Loss: 0.001698 | Perplexity: 1609.572033
2025-09-27 11:05:47,748 Stage: Train 0.5 | Epoch: 35 | Iter: 54200 | Total Loss: 0.007640 | Recon Loss: 0.006766 | Commit Loss: 0.001748 | Perplexity: 1617.270107
2025-09-27 11:06:45,087 Stage: Train 0.5 | Epoch: 35 | Iter: 54400 | Total Loss: 0.007683 | Recon Loss: 0.006820 | Commit Loss: 0.001727 | Perplexity: 1620.284721
2025-09-27 11:07:42,189 Stage: Train 0.5 | Epoch: 35 | Iter: 54600 | Total Loss: 0.007763 | Recon Loss: 0.006905 | Commit Loss: 0.001715 | Perplexity: 1624.875568
Trainning Epoch:  11%|█         | 36/330 [4:20:14<35:27:39, 434.22s/it]2025-09-27 11:08:39,362 Stage: Train 0.5 | Epoch: 36 | Iter: 54800 | Total Loss: 0.007631 | Recon Loss: 0.006779 | Commit Loss: 0.001705 | Perplexity: 1622.256243
2025-09-27 11:09:36,550 Stage: Train 0.5 | Epoch: 36 | Iter: 55000 | Total Loss: 0.007663 | Recon Loss: 0.006821 | Commit Loss: 0.001684 | Perplexity: 1620.934221
2025-09-27 11:10:33,840 Stage: Train 0.5 | Epoch: 36 | Iter: 55200 | Total Loss: 0.007630 | Recon Loss: 0.006780 | Commit Loss: 0.001700 | Perplexity: 1626.283467
2025-09-27 11:11:31,093 Stage: Train 0.5 | Epoch: 36 | Iter: 55400 | Total Loss: 0.007666 | Recon Loss: 0.006825 | Commit Loss: 0.001682 | Perplexity: 1628.610553
2025-09-27 11:12:27,914 Stage: Train 0.5 | Epoch: 36 | Iter: 55600 | Total Loss: 0.007588 | Recon Loss: 0.006751 | Commit Loss: 0.001673 | Perplexity: 1628.490883
2025-09-27 11:13:25,048 Stage: Train 0.5 | Epoch: 36 | Iter: 55800 | Total Loss: 0.007584 | Recon Loss: 0.006734 | Commit Loss: 0.001701 | Perplexity: 1636.328338
2025-09-27 11:14:22,246 Stage: Train 0.5 | Epoch: 36 | Iter: 56000 | Total Loss: 0.007715 | Recon Loss: 0.006873 | Commit Loss: 0.001684 | Perplexity: 1636.839780
2025-09-27 11:15:19,437 Stage: Train 0.5 | Epoch: 36 | Iter: 56200 | Total Loss: 0.007563 | Recon Loss: 0.006719 | Commit Loss: 0.001687 | Perplexity: 1639.487314
Trainning Epoch:  11%|█         | 37/330 [4:27:28<35:20:16, 434.19s/it]2025-09-27 11:16:16,859 Stage: Train 0.5 | Epoch: 37 | Iter: 56400 | Total Loss: 0.007480 | Recon Loss: 0.006646 | Commit Loss: 0.001668 | Perplexity: 1640.632997
2025-09-27 11:17:13,855 Stage: Train 0.5 | Epoch: 37 | Iter: 56600 | Total Loss: 0.007463 | Recon Loss: 0.006628 | Commit Loss: 0.001668 | Perplexity: 1645.540567
2025-09-27 11:18:10,662 Stage: Train 0.5 | Epoch: 37 | Iter: 56800 | Total Loss: 0.007546 | Recon Loss: 0.006711 | Commit Loss: 0.001670 | Perplexity: 1646.392644
2025-09-27 11:19:07,975 Stage: Train 0.5 | Epoch: 37 | Iter: 57000 | Total Loss: 0.007590 | Recon Loss: 0.006751 | Commit Loss: 0.001678 | Perplexity: 1653.438992
2025-09-27 11:20:05,019 Stage: Train 0.5 | Epoch: 37 | Iter: 57200 | Total Loss: 0.007457 | Recon Loss: 0.006629 | Commit Loss: 0.001657 | Perplexity: 1653.431081
2025-09-27 11:21:01,943 Stage: Train 0.5 | Epoch: 37 | Iter: 57400 | Total Loss: 0.007517 | Recon Loss: 0.006688 | Commit Loss: 0.001658 | Perplexity: 1661.550681
2025-09-27 11:21:58,750 Stage: Train 0.5 | Epoch: 37 | Iter: 57600 | Total Loss: 0.007498 | Recon Loss: 0.006673 | Commit Loss: 0.001650 | Perplexity: 1663.404681
Trainning Epoch:  12%|█▏        | 38/330 [4:34:42<35:11:36, 433.89s/it]2025-09-27 11:22:55,922 Stage: Train 0.5 | Epoch: 38 | Iter: 57800 | Total Loss: 0.007451 | Recon Loss: 0.006636 | Commit Loss: 0.001631 | Perplexity: 1658.445314
2025-09-27 11:23:52,754 Stage: Train 0.5 | Epoch: 38 | Iter: 58000 | Total Loss: 0.007374 | Recon Loss: 0.006545 | Commit Loss: 0.001659 | Perplexity: 1668.016426
2025-09-27 11:24:49,887 Stage: Train 0.5 | Epoch: 38 | Iter: 58200 | Total Loss: 0.007644 | Recon Loss: 0.006819 | Commit Loss: 0.001649 | Perplexity: 1665.574178
2025-09-27 11:25:46,903 Stage: Train 0.5 | Epoch: 38 | Iter: 58400 | Total Loss: 0.007413 | Recon Loss: 0.006591 | Commit Loss: 0.001643 | Perplexity: 1667.016104
2025-09-27 11:26:44,063 Stage: Train 0.5 | Epoch: 38 | Iter: 58600 | Total Loss: 0.007379 | Recon Loss: 0.006564 | Commit Loss: 0.001629 | Perplexity: 1666.276619
2025-09-27 11:27:41,084 Stage: Train 0.5 | Epoch: 38 | Iter: 58800 | Total Loss: 0.007483 | Recon Loss: 0.006664 | Commit Loss: 0.001638 | Perplexity: 1667.883706
2025-09-27 11:28:38,150 Stage: Train 0.5 | Epoch: 38 | Iter: 59000 | Total Loss: 0.007281 | Recon Loss: 0.006473 | Commit Loss: 0.001616 | Perplexity: 1669.178344
2025-09-27 11:29:34,888 Stage: Train 0.5 | Epoch: 38 | Iter: 59200 | Total Loss: 0.007368 | Recon Loss: 0.006541 | Commit Loss: 0.001653 | Perplexity: 1677.332130
Trainning Epoch:  12%|█▏        | 39/330 [4:41:55<35:03:12, 433.65s/it]2025-09-27 11:30:32,342 Stage: Train 0.5 | Epoch: 39 | Iter: 59400 | Total Loss: 0.007402 | Recon Loss: 0.006573 | Commit Loss: 0.001659 | Perplexity: 1681.508701
2025-09-27 11:31:29,455 Stage: Train 0.5 | Epoch: 39 | Iter: 59600 | Total Loss: 0.007408 | Recon Loss: 0.006586 | Commit Loss: 0.001644 | Perplexity: 1681.819511
2025-09-27 11:32:26,555 Stage: Train 0.5 | Epoch: 39 | Iter: 59800 | Total Loss: 0.007447 | Recon Loss: 0.006628 | Commit Loss: 0.001639 | Perplexity: 1681.432107
2025-09-27 11:33:23,570 Stage: Train 0.5 | Epoch: 39 | Iter: 60000 | Total Loss: 0.007325 | Recon Loss: 0.006508 | Commit Loss: 0.001635 | Perplexity: 1678.807666
2025-09-27 11:33:23,571 Saving model at iteration 60000
2025-09-27 11:33:23,754 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000
2025-09-27 11:33:24,202 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/model.safetensors
2025-09-27 11:33:24,708 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/optimizer.bin
2025-09-27 11:33:24,713 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/scheduler.bin
2025-09-27 11:33:24,713 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/sampler.bin
2025-09-27 11:33:24,714 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_40_step_60000/random_states_0.pkl
2025-09-27 11:34:21,992 Stage: Train 0.5 | Epoch: 39 | Iter: 60200 | Total Loss: 0.007253 | Recon Loss: 0.006449 | Commit Loss: 0.001609 | Perplexity: 1681.976024
2025-09-27 11:35:19,092 Stage: Train 0.5 | Epoch: 39 | Iter: 60400 | Total Loss: 0.007393 | Recon Loss: 0.006577 | Commit Loss: 0.001634 | Perplexity: 1678.590347
2025-09-27 11:36:15,869 Stage: Train 0.5 | Epoch: 39 | Iter: 60600 | Total Loss: 0.007298 | Recon Loss: 0.006482 | Commit Loss: 0.001631 | Perplexity: 1680.148974
Trainning Epoch:  12%|█▏        | 40/330 [4:49:10<34:57:59, 434.07s/it]2025-09-27 11:37:13,238 Stage: Train 0.5 | Epoch: 40 | Iter: 60800 | Total Loss: 0.007216 | Recon Loss: 0.006408 | Commit Loss: 0.001617 | Perplexity: 1682.677720
2025-09-27 11:38:10,392 Stage: Train 0.5 | Epoch: 40 | Iter: 61000 | Total Loss: 0.007317 | Recon Loss: 0.006515 | Commit Loss: 0.001604 | Perplexity: 1682.185317
2025-09-27 11:39:07,680 Stage: Train 0.5 | Epoch: 40 | Iter: 61200 | Total Loss: 0.007348 | Recon Loss: 0.006542 | Commit Loss: 0.001613 | Perplexity: 1683.629562
2025-09-27 11:40:04,775 Stage: Train 0.5 | Epoch: 40 | Iter: 61400 | Total Loss: 0.007294 | Recon Loss: 0.006471 | Commit Loss: 0.001646 | Perplexity: 1690.805200
2025-09-27 11:41:01,958 Stage: Train 0.5 | Epoch: 40 | Iter: 61600 | Total Loss: 0.007230 | Recon Loss: 0.006423 | Commit Loss: 0.001614 | Perplexity: 1684.845817
2025-09-27 11:41:58,781 Stage: Train 0.5 | Epoch: 40 | Iter: 61800 | Total Loss: 0.007310 | Recon Loss: 0.006491 | Commit Loss: 0.001637 | Perplexity: 1691.039360
2025-09-27 11:42:56,136 Stage: Train 0.5 | Epoch: 40 | Iter: 62000 | Total Loss: 0.007322 | Recon Loss: 0.006511 | Commit Loss: 0.001623 | Perplexity: 1688.020659
2025-09-27 11:43:53,234 Stage: Train 0.5 | Epoch: 40 | Iter: 62200 | Total Loss: 0.007242 | Recon Loss: 0.006426 | Commit Loss: 0.001632 | Perplexity: 1689.031580
Trainning Epoch:  12%|█▏        | 41/330 [4:56:24<34:50:56, 434.11s/it]2025-09-27 11:44:50,674 Stage: Train 0.5 | Epoch: 41 | Iter: 62400 | Total Loss: 0.007135 | Recon Loss: 0.006334 | Commit Loss: 0.001603 | Perplexity: 1684.446378
2025-09-27 11:45:47,902 Stage: Train 0.5 | Epoch: 41 | Iter: 62600 | Total Loss: 0.007304 | Recon Loss: 0.006495 | Commit Loss: 0.001616 | Perplexity: 1691.671684
2025-09-27 11:46:43,066 Stage: Train 0.5 | Epoch: 41 | Iter: 62800 | Total Loss: 0.007216 | Recon Loss: 0.006397 | Commit Loss: 0.001636 | Perplexity: 1697.199432
2025-09-27 11:47:39,819 Stage: Train 0.5 | Epoch: 41 | Iter: 63000 | Total Loss: 0.007202 | Recon Loss: 0.006397 | Commit Loss: 0.001611 | Perplexity: 1691.451087
2025-09-27 11:48:36,968 Stage: Train 0.5 | Epoch: 41 | Iter: 63200 | Total Loss: 0.007143 | Recon Loss: 0.006340 | Commit Loss: 0.001607 | Perplexity: 1693.076674
2025-09-27 11:49:34,090 Stage: Train 0.5 | Epoch: 41 | Iter: 63400 | Total Loss: 0.007186 | Recon Loss: 0.006374 | Commit Loss: 0.001623 | Perplexity: 1696.564832
2025-09-27 11:50:31,136 Stage: Train 0.5 | Epoch: 41 | Iter: 63600 | Total Loss: 0.007144 | Recon Loss: 0.006344 | Commit Loss: 0.001600 | Perplexity: 1692.535493
Trainning Epoch:  13%|█▎        | 42/330 [5:03:36<34:40:18, 433.40s/it]2025-09-27 11:51:28,309 Stage: Train 0.5 | Epoch: 42 | Iter: 63800 | Total Loss: 0.007170 | Recon Loss: 0.006358 | Commit Loss: 0.001625 | Perplexity: 1694.977734
2025-09-27 11:52:25,391 Stage: Train 0.5 | Epoch: 42 | Iter: 64000 | Total Loss: 0.007150 | Recon Loss: 0.006352 | Commit Loss: 0.001597 | Perplexity: 1694.523167
2025-09-27 11:53:22,447 Stage: Train 0.5 | Epoch: 42 | Iter: 64200 | Total Loss: 0.007140 | Recon Loss: 0.006329 | Commit Loss: 0.001622 | Perplexity: 1699.660942
2025-09-27 11:54:19,194 Stage: Train 0.5 | Epoch: 42 | Iter: 64400 | Total Loss: 0.007232 | Recon Loss: 0.006429 | Commit Loss: 0.001605 | Perplexity: 1698.903835
2025-09-27 11:55:16,172 Stage: Train 0.5 | Epoch: 42 | Iter: 64600 | Total Loss: 0.007104 | Recon Loss: 0.006300 | Commit Loss: 0.001610 | Perplexity: 1698.876024
2025-09-27 11:56:13,062 Stage: Train 0.5 | Epoch: 42 | Iter: 64800 | Total Loss: 0.007085 | Recon Loss: 0.006273 | Commit Loss: 0.001624 | Perplexity: 1698.779012
2025-09-27 11:57:10,021 Stage: Train 0.5 | Epoch: 42 | Iter: 65000 | Total Loss: 0.007109 | Recon Loss: 0.006299 | Commit Loss: 0.001621 | Perplexity: 1698.032418
2025-09-27 11:58:07,224 Stage: Train 0.5 | Epoch: 42 | Iter: 65200 | Total Loss: 0.007069 | Recon Loss: 0.006262 | Commit Loss: 0.001614 | Perplexity: 1702.732540
Trainning Epoch:  13%|█▎        | 43/330 [5:10:49<34:32:34, 433.29s/it]2025-09-27 11:59:04,439 Stage: Train 0.5 | Epoch: 43 | Iter: 65400 | Total Loss: 0.007058 | Recon Loss: 0.006249 | Commit Loss: 0.001619 | Perplexity: 1701.358764
2025-09-27 12:00:01,164 Stage: Train 0.5 | Epoch: 43 | Iter: 65600 | Total Loss: 0.007154 | Recon Loss: 0.006343 | Commit Loss: 0.001621 | Perplexity: 1703.641381
2025-09-27 12:00:58,273 Stage: Train 0.5 | Epoch: 43 | Iter: 65800 | Total Loss: 0.007043 | Recon Loss: 0.006244 | Commit Loss: 0.001597 | Perplexity: 1697.411294
2025-09-27 12:01:55,449 Stage: Train 0.5 | Epoch: 43 | Iter: 66000 | Total Loss: 0.007108 | Recon Loss: 0.006308 | Commit Loss: 0.001600 | Perplexity: 1701.211793
2025-09-27 12:02:52,347 Stage: Train 0.5 | Epoch: 43 | Iter: 66200 | Total Loss: 0.007116 | Recon Loss: 0.006309 | Commit Loss: 0.001613 | Perplexity: 1706.480516
2025-09-27 12:03:49,610 Stage: Train 0.5 | Epoch: 43 | Iter: 66400 | Total Loss: 0.007185 | Recon Loss: 0.006376 | Commit Loss: 0.001618 | Perplexity: 1702.910262
2025-09-27 12:04:46,630 Stage: Train 0.5 | Epoch: 43 | Iter: 66600 | Total Loss: 0.007054 | Recon Loss: 0.006242 | Commit Loss: 0.001624 | Perplexity: 1703.391488
2025-09-27 12:05:43,485 Stage: Train 0.5 | Epoch: 43 | Iter: 66800 | Total Loss: 0.007117 | Recon Loss: 0.006315 | Commit Loss: 0.001604 | Perplexity: 1703.338679
Trainning Epoch:  13%|█▎        | 44/330 [5:18:02<34:25:08, 433.25s/it]2025-09-27 12:06:40,927 Stage: Train 0.5 | Epoch: 44 | Iter: 67000 | Total Loss: 0.006980 | Recon Loss: 0.006179 | Commit Loss: 0.001603 | Perplexity: 1703.806433
2025-09-27 12:07:38,014 Stage: Train 0.5 | Epoch: 44 | Iter: 67200 | Total Loss: 0.006987 | Recon Loss: 0.006182 | Commit Loss: 0.001609 | Perplexity: 1704.168635
2025-09-27 12:08:35,185 Stage: Train 0.5 | Epoch: 44 | Iter: 67400 | Total Loss: 0.007045 | Recon Loss: 0.006243 | Commit Loss: 0.001604 | Perplexity: 1706.978763
2025-09-27 12:09:32,363 Stage: Train 0.5 | Epoch: 44 | Iter: 67600 | Total Loss: 0.007231 | Recon Loss: 0.006426 | Commit Loss: 0.001610 | Perplexity: 1708.835452
2025-09-27 12:10:29,547 Stage: Train 0.5 | Epoch: 44 | Iter: 67800 | Total Loss: 0.007036 | Recon Loss: 0.006225 | Commit Loss: 0.001621 | Perplexity: 1710.539284
2025-09-27 12:11:26,295 Stage: Train 0.5 | Epoch: 44 | Iter: 68000 | Total Loss: 0.006957 | Recon Loss: 0.006152 | Commit Loss: 0.001610 | Perplexity: 1706.742086
2025-09-27 12:12:23,397 Stage: Train 0.5 | Epoch: 44 | Iter: 68200 | Total Loss: 0.006939 | Recon Loss: 0.006137 | Commit Loss: 0.001604 | Perplexity: 1709.524974
Trainning Epoch:  14%|█▎        | 45/330 [5:25:16<34:18:58, 433.47s/it]2025-09-27 12:13:20,726 Stage: Train 0.5 | Epoch: 45 | Iter: 68400 | Total Loss: 0.007095 | Recon Loss: 0.006289 | Commit Loss: 0.001612 | Perplexity: 1705.253849
2025-09-27 12:14:17,776 Stage: Train 0.5 | Epoch: 45 | Iter: 68600 | Total Loss: 0.007026 | Recon Loss: 0.006222 | Commit Loss: 0.001608 | Perplexity: 1708.511720
2025-09-27 12:15:14,897 Stage: Train 0.5 | Epoch: 45 | Iter: 68800 | Total Loss: 0.006914 | Recon Loss: 0.006107 | Commit Loss: 0.001614 | Perplexity: 1715.071787
2025-09-27 12:16:11,972 Stage: Train 0.5 | Epoch: 45 | Iter: 69000 | Total Loss: 0.006987 | Recon Loss: 0.006184 | Commit Loss: 0.001606 | Perplexity: 1709.481253
2025-09-27 12:17:09,140 Stage: Train 0.5 | Epoch: 45 | Iter: 69200 | Total Loss: 0.006940 | Recon Loss: 0.006145 | Commit Loss: 0.001591 | Perplexity: 1712.758723
2025-09-27 12:18:06,022 Stage: Train 0.5 | Epoch: 45 | Iter: 69400 | Total Loss: 0.006976 | Recon Loss: 0.006172 | Commit Loss: 0.001608 | Perplexity: 1712.214075
2025-09-27 12:19:03,056 Stage: Train 0.5 | Epoch: 45 | Iter: 69600 | Total Loss: 0.006942 | Recon Loss: 0.006131 | Commit Loss: 0.001622 | Perplexity: 1713.978755
2025-09-27 12:20:00,139 Stage: Train 0.5 | Epoch: 45 | Iter: 69800 | Total Loss: 0.006921 | Recon Loss: 0.006124 | Commit Loss: 0.001594 | Perplexity: 1715.862505
Trainning Epoch:  14%|█▍        | 46/330 [5:32:29<34:11:54, 433.50s/it]2025-09-27 12:20:57,412 Stage: Train 0.5 | Epoch: 46 | Iter: 70000 | Total Loss: 0.006925 | Recon Loss: 0.006131 | Commit Loss: 0.001589 | Perplexity: 1708.789072
2025-09-27 12:21:54,449 Stage: Train 0.5 | Epoch: 46 | Iter: 70200 | Total Loss: 0.006936 | Recon Loss: 0.006137 | Commit Loss: 0.001599 | Perplexity: 1715.865991
2025-09-27 12:22:51,598 Stage: Train 0.5 | Epoch: 46 | Iter: 70400 | Total Loss: 0.006930 | Recon Loss: 0.006134 | Commit Loss: 0.001593 | Perplexity: 1719.984609
2025-09-27 12:23:48,390 Stage: Train 0.5 | Epoch: 46 | Iter: 70600 | Total Loss: 0.007036 | Recon Loss: 0.006235 | Commit Loss: 0.001601 | Perplexity: 1714.996179
2025-09-27 12:24:45,380 Stage: Train 0.5 | Epoch: 46 | Iter: 70800 | Total Loss: 0.006907 | Recon Loss: 0.006104 | Commit Loss: 0.001607 | Perplexity: 1718.904733
2025-09-27 12:25:42,388 Stage: Train 0.5 | Epoch: 46 | Iter: 71000 | Total Loss: 0.006869 | Recon Loss: 0.006060 | Commit Loss: 0.001618 | Perplexity: 1720.541693
2025-09-27 12:26:39,495 Stage: Train 0.5 | Epoch: 46 | Iter: 71200 | Total Loss: 0.006921 | Recon Loss: 0.006118 | Commit Loss: 0.001606 | Perplexity: 1718.338497
Trainning Epoch:  14%|█▍        | 47/330 [5:39:43<34:04:44, 433.51s/it]2025-09-27 12:27:36,993 Stage: Train 0.5 | Epoch: 47 | Iter: 71400 | Total Loss: 0.006981 | Recon Loss: 0.006179 | Commit Loss: 0.001604 | Perplexity: 1718.736489
2025-09-27 12:28:34,025 Stage: Train 0.5 | Epoch: 47 | Iter: 71600 | Total Loss: 0.006831 | Recon Loss: 0.006027 | Commit Loss: 0.001607 | Perplexity: 1720.449846
2025-09-27 12:29:30,878 Stage: Train 0.5 | Epoch: 47 | Iter: 71800 | Total Loss: 0.006892 | Recon Loss: 0.006093 | Commit Loss: 0.001597 | Perplexity: 1719.187350
2025-09-27 12:30:28,054 Stage: Train 0.5 | Epoch: 47 | Iter: 72000 | Total Loss: 0.006943 | Recon Loss: 0.006150 | Commit Loss: 0.001587 | Perplexity: 1716.517218
2025-09-27 12:31:25,298 Stage: Train 0.5 | Epoch: 47 | Iter: 72200 | Total Loss: 0.006855 | Recon Loss: 0.006055 | Commit Loss: 0.001601 | Perplexity: 1723.008740
2025-09-27 12:32:22,299 Stage: Train 0.5 | Epoch: 47 | Iter: 72400 | Total Loss: 0.006838 | Recon Loss: 0.006042 | Commit Loss: 0.001591 | Perplexity: 1722.458846
2025-09-27 12:33:19,346 Stage: Train 0.5 | Epoch: 47 | Iter: 72600 | Total Loss: 0.006805 | Recon Loss: 0.005999 | Commit Loss: 0.001611 | Perplexity: 1723.894220
2025-09-27 12:34:16,504 Stage: Train 0.5 | Epoch: 47 | Iter: 72800 | Total Loss: 0.006899 | Recon Loss: 0.006090 | Commit Loss: 0.001619 | Perplexity: 1733.528559
Trainning Epoch:  15%|█▍        | 48/330 [5:46:57<33:57:49, 433.58s/it]2025-09-27 12:35:13,898 Stage: Train 0.5 | Epoch: 48 | Iter: 73000 | Total Loss: 0.006876 | Recon Loss: 0.006072 | Commit Loss: 0.001607 | Perplexity: 1726.417379
2025-09-27 12:36:10,605 Stage: Train 0.5 | Epoch: 48 | Iter: 73200 | Total Loss: 0.006816 | Recon Loss: 0.006025 | Commit Loss: 0.001583 | Perplexity: 1722.032045
2025-09-27 12:37:07,790 Stage: Train 0.5 | Epoch: 48 | Iter: 73400 | Total Loss: 0.006869 | Recon Loss: 0.006069 | Commit Loss: 0.001599 | Perplexity: 1727.547531
2025-09-27 12:38:04,966 Stage: Train 0.5 | Epoch: 48 | Iter: 73600 | Total Loss: 0.006838 | Recon Loss: 0.006031 | Commit Loss: 0.001613 | Perplexity: 1731.814350
2025-09-27 12:39:01,939 Stage: Train 0.5 | Epoch: 48 | Iter: 73800 | Total Loss: 0.006792 | Recon Loss: 0.005993 | Commit Loss: 0.001599 | Perplexity: 1733.545481
2025-09-27 12:39:59,166 Stage: Train 0.5 | Epoch: 48 | Iter: 74000 | Total Loss: 0.006785 | Recon Loss: 0.005989 | Commit Loss: 0.001593 | Perplexity: 1728.987678
2025-09-27 12:40:56,279 Stage: Train 0.5 | Epoch: 48 | Iter: 74200 | Total Loss: 0.006827 | Recon Loss: 0.006020 | Commit Loss: 0.001614 | Perplexity: 1733.684604
2025-09-27 12:41:52,988 Stage: Train 0.5 | Epoch: 48 | Iter: 74400 | Total Loss: 0.006845 | Recon Loss: 0.006037 | Commit Loss: 0.001617 | Perplexity: 1728.761426
Trainning Epoch:  15%|█▍        | 49/330 [5:54:10<33:50:10, 433.49s/it]2025-09-27 12:42:50,313 Stage: Train 0.5 | Epoch: 49 | Iter: 74600 | Total Loss: 0.006803 | Recon Loss: 0.006004 | Commit Loss: 0.001597 | Perplexity: 1728.453842
2025-09-27 12:43:47,415 Stage: Train 0.5 | Epoch: 49 | Iter: 74800 | Total Loss: 0.006774 | Recon Loss: 0.005966 | Commit Loss: 0.001617 | Perplexity: 1730.856279
2025-09-27 12:44:44,669 Stage: Train 0.5 | Epoch: 49 | Iter: 75000 | Total Loss: 0.006771 | Recon Loss: 0.005966 | Commit Loss: 0.001610 | Perplexity: 1733.079756
2025-09-27 12:45:41,892 Stage: Train 0.5 | Epoch: 49 | Iter: 75200 | Total Loss: 0.006821 | Recon Loss: 0.006021 | Commit Loss: 0.001600 | Perplexity: 1729.757250
2025-09-27 12:46:39,055 Stage: Train 0.5 | Epoch: 49 | Iter: 75400 | Total Loss: 0.006804 | Recon Loss: 0.006012 | Commit Loss: 0.001586 | Perplexity: 1730.036874
2025-09-27 12:47:35,967 Stage: Train 0.5 | Epoch: 49 | Iter: 75600 | Total Loss: 0.006754 | Recon Loss: 0.005956 | Commit Loss: 0.001595 | Perplexity: 1731.950657
2025-09-27 12:48:33,170 Stage: Train 0.5 | Epoch: 49 | Iter: 75800 | Total Loss: 0.006871 | Recon Loss: 0.006062 | Commit Loss: 0.001617 | Perplexity: 1735.132186
Trainning Epoch:  15%|█▌        | 50/330 [6:01:24<33:44:05, 433.73s/it]2025-09-27 12:49:30,744 Stage: Train 0.5 | Epoch: 50 | Iter: 76000 | Total Loss: 0.006744 | Recon Loss: 0.005945 | Commit Loss: 0.001599 | Perplexity: 1728.916421
2025-09-27 12:50:27,951 Stage: Train 0.5 | Epoch: 50 | Iter: 76200 | Total Loss: 0.006667 | Recon Loss: 0.005871 | Commit Loss: 0.001592 | Perplexity: 1736.674442
2025-09-27 12:51:25,104 Stage: Train 0.5 | Epoch: 50 | Iter: 76400 | Total Loss: 0.006758 | Recon Loss: 0.005962 | Commit Loss: 0.001594 | Perplexity: 1732.794212
2025-09-27 12:52:22,261 Stage: Train 0.5 | Epoch: 50 | Iter: 76600 | Total Loss: 0.006766 | Recon Loss: 0.005965 | Commit Loss: 0.001602 | Perplexity: 1736.908876
2025-09-27 12:53:19,081 Stage: Train 0.5 | Epoch: 50 | Iter: 76800 | Total Loss: 0.006924 | Recon Loss: 0.006123 | Commit Loss: 0.001603 | Perplexity: 1740.383313
2025-09-27 12:54:16,095 Stage: Train 0.5 | Epoch: 50 | Iter: 77000 | Total Loss: 0.006688 | Recon Loss: 0.005888 | Commit Loss: 0.001600 | Perplexity: 1734.276415
2025-09-27 12:55:13,116 Stage: Train 0.5 | Epoch: 50 | Iter: 77200 | Total Loss: 0.006709 | Recon Loss: 0.005908 | Commit Loss: 0.001602 | Perplexity: 1740.392825
2025-09-27 12:56:10,197 Stage: Train 0.5 | Epoch: 50 | Iter: 77400 | Total Loss: 0.006736 | Recon Loss: 0.005934 | Commit Loss: 0.001605 | Perplexity: 1741.537568
Trainning Epoch:  15%|█▌        | 51/330 [6:08:38<33:36:57, 433.76s/it]2025-09-27 12:57:07,581 Stage: Train 0.5 | Epoch: 51 | Iter: 77600 | Total Loss: 0.006773 | Recon Loss: 0.005979 | Commit Loss: 0.001589 | Perplexity: 1738.624462
2025-09-27 12:58:04,773 Stage: Train 0.5 | Epoch: 51 | Iter: 77800 | Total Loss: 0.006634 | Recon Loss: 0.005832 | Commit Loss: 0.001604 | Perplexity: 1741.151589
2025-09-27 12:59:01,868 Stage: Train 0.5 | Epoch: 51 | Iter: 78000 | Total Loss: 0.006660 | Recon Loss: 0.005864 | Commit Loss: 0.001591 | Perplexity: 1740.660327
2025-09-27 12:59:58,782 Stage: Train 0.5 | Epoch: 51 | Iter: 78200 | Total Loss: 0.006697 | Recon Loss: 0.005898 | Commit Loss: 0.001597 | Perplexity: 1746.030858
2025-09-27 13:00:55,860 Stage: Train 0.5 | Epoch: 51 | Iter: 78400 | Total Loss: 0.006693 | Recon Loss: 0.005893 | Commit Loss: 0.001600 | Perplexity: 1742.949631
2025-09-27 13:01:53,035 Stage: Train 0.5 | Epoch: 51 | Iter: 78600 | Total Loss: 0.006759 | Recon Loss: 0.005957 | Commit Loss: 0.001604 | Perplexity: 1743.651503
2025-09-27 13:02:50,205 Stage: Train 0.5 | Epoch: 51 | Iter: 78800 | Total Loss: 0.006748 | Recon Loss: 0.005941 | Commit Loss: 0.001613 | Perplexity: 1744.811001
Trainning Epoch:  16%|█▌        | 52/330 [6:15:52<33:29:44, 433.76s/it]2025-09-27 13:03:47,301 Stage: Train 0.5 | Epoch: 52 | Iter: 79000 | Total Loss: 0.006633 | Recon Loss: 0.005828 | Commit Loss: 0.001609 | Perplexity: 1745.677005
2025-09-27 13:04:44,519 Stage: Train 0.5 | Epoch: 52 | Iter: 79200 | Total Loss: 0.006721 | Recon Loss: 0.005917 | Commit Loss: 0.001607 | Perplexity: 1747.549803
2025-09-27 13:05:39,938 Stage: Train 0.5 | Epoch: 52 | Iter: 79400 | Total Loss: 0.006674 | Recon Loss: 0.005877 | Commit Loss: 0.001595 | Perplexity: 1745.872247
2025-09-27 13:06:37,193 Stage: Train 0.5 | Epoch: 52 | Iter: 79600 | Total Loss: 0.006599 | Recon Loss: 0.005804 | Commit Loss: 0.001589 | Perplexity: 1747.457305
2025-09-27 13:07:34,317 Stage: Train 0.5 | Epoch: 52 | Iter: 79800 | Total Loss: 0.006667 | Recon Loss: 0.005863 | Commit Loss: 0.001610 | Perplexity: 1751.441375
2025-09-27 13:08:31,489 Stage: Train 0.5 | Epoch: 52 | Iter: 80000 | Total Loss: 0.006782 | Recon Loss: 0.005987 | Commit Loss: 0.001589 | Perplexity: 1746.575759
2025-09-27 13:08:31,489 Saving model at iteration 80000
2025-09-27 13:08:31,846 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000
2025-09-27 13:08:32,305 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/model.safetensors
2025-09-27 13:08:32,784 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/optimizer.bin
2025-09-27 13:08:32,784 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/scheduler.bin
2025-09-27 13:08:32,785 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/sampler.bin
2025-09-27 13:08:32,785 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_53_step_80000/random_states_0.pkl
2025-09-27 13:09:30,098 Stage: Train 0.5 | Epoch: 52 | Iter: 80200 | Total Loss: 0.006579 | Recon Loss: 0.005778 | Commit Loss: 0.001601 | Perplexity: 1755.233951
2025-09-27 13:10:26,872 Stage: Train 0.5 | Epoch: 52 | Iter: 80400 | Total Loss: 0.006681 | Recon Loss: 0.005877 | Commit Loss: 0.001607 | Perplexity: 1752.101976
Trainning Epoch:  16%|█▌        | 53/330 [6:23:06<33:22:31, 433.76s/it]2025-09-27 13:11:23,831 Stage: Train 0.5 | Epoch: 53 | Iter: 80600 | Total Loss: 0.006615 | Recon Loss: 0.005812 | Commit Loss: 0.001607 | Perplexity: 1751.336614
2025-09-27 13:12:21,042 Stage: Train 0.5 | Epoch: 53 | Iter: 80800 | Total Loss: 0.006582 | Recon Loss: 0.005788 | Commit Loss: 0.001588 | Perplexity: 1751.957415
2025-09-27 13:13:18,288 Stage: Train 0.5 | Epoch: 53 | Iter: 81000 | Total Loss: 0.006696 | Recon Loss: 0.005896 | Commit Loss: 0.001600 | Perplexity: 1751.766420
2025-09-27 13:14:15,299 Stage: Train 0.5 | Epoch: 53 | Iter: 81200 | Total Loss: 0.006524 | Recon Loss: 0.005725 | Commit Loss: 0.001597 | Perplexity: 1754.013190
2025-09-27 13:15:12,330 Stage: Train 0.5 | Epoch: 53 | Iter: 81400 | Total Loss: 0.006716 | Recon Loss: 0.005915 | Commit Loss: 0.001601 | Perplexity: 1753.425626
2025-09-27 13:16:09,627 Stage: Train 0.5 | Epoch: 53 | Iter: 81600 | Total Loss: 0.006622 | Recon Loss: 0.005814 | Commit Loss: 0.001616 | Perplexity: 1757.483713
2025-09-27 13:17:06,791 Stage: Train 0.5 | Epoch: 53 | Iter: 81800 | Total Loss: 0.006602 | Recon Loss: 0.005802 | Commit Loss: 0.001601 | Perplexity: 1756.176740
2025-09-27 13:18:03,353 Stage: Train 0.5 | Epoch: 53 | Iter: 82000 | Total Loss: 0.006624 | Recon Loss: 0.005824 | Commit Loss: 0.001600 | Perplexity: 1756.425386
Trainning Epoch:  16%|█▋        | 54/330 [6:30:19<33:14:35, 433.61s/it]2025-09-27 13:19:00,719 Stage: Train 0.5 | Epoch: 54 | Iter: 82200 | Total Loss: 0.006639 | Recon Loss: 0.005840 | Commit Loss: 0.001598 | Perplexity: 1756.352772
2025-09-27 13:19:58,025 Stage: Train 0.5 | Epoch: 54 | Iter: 82400 | Total Loss: 0.006577 | Recon Loss: 0.005777 | Commit Loss: 0.001600 | Perplexity: 1760.243059
2025-09-27 13:20:55,228 Stage: Train 0.5 | Epoch: 54 | Iter: 82600 | Total Loss: 0.006650 | Recon Loss: 0.005847 | Commit Loss: 0.001605 | Perplexity: 1758.892988
2025-09-27 13:21:52,301 Stage: Train 0.5 | Epoch: 54 | Iter: 82800 | Total Loss: 0.006602 | Recon Loss: 0.005801 | Commit Loss: 0.001602 | Perplexity: 1757.939156
2025-09-27 13:22:49,356 Stage: Train 0.5 | Epoch: 54 | Iter: 83000 | Total Loss: 0.006616 | Recon Loss: 0.005815 | Commit Loss: 0.001602 | Perplexity: 1755.185940
2025-09-27 13:23:46,130 Stage: Train 0.5 | Epoch: 54 | Iter: 83200 | Total Loss: 0.006720 | Recon Loss: 0.005919 | Commit Loss: 0.001601 | Perplexity: 1758.630526
2025-09-27 13:24:43,280 Stage: Train 0.5 | Epoch: 54 | Iter: 83400 | Total Loss: 0.006514 | Recon Loss: 0.005718 | Commit Loss: 0.001592 | Perplexity: 1758.571606
Trainning Epoch:  17%|█▋        | 55/330 [6:37:33<33:07:47, 433.70s/it]2025-09-27 13:25:40,612 Stage: Train 0.5 | Epoch: 55 | Iter: 83600 | Total Loss: 0.006524 | Recon Loss: 0.005723 | Commit Loss: 0.001602 | Perplexity: 1755.898592
2025-09-27 13:26:37,669 Stage: Train 0.5 | Epoch: 55 | Iter: 83800 | Total Loss: 0.006543 | Recon Loss: 0.005747 | Commit Loss: 0.001593 | Perplexity: 1760.339485
2025-09-27 13:27:34,908 Stage: Train 0.5 | Epoch: 55 | Iter: 84000 | Total Loss: 0.006634 | Recon Loss: 0.005831 | Commit Loss: 0.001607 | Perplexity: 1762.120029
2025-09-27 13:28:32,175 Stage: Train 0.5 | Epoch: 55 | Iter: 84200 | Total Loss: 0.006528 | Recon Loss: 0.005732 | Commit Loss: 0.001591 | Perplexity: 1757.645577
2025-09-27 13:29:28,906 Stage: Train 0.5 | Epoch: 55 | Iter: 84400 | Total Loss: 0.006682 | Recon Loss: 0.005877 | Commit Loss: 0.001610 | Perplexity: 1762.431397
2025-09-27 13:30:26,049 Stage: Train 0.5 | Epoch: 55 | Iter: 84600 | Total Loss: 0.006603 | Recon Loss: 0.005800 | Commit Loss: 0.001607 | Perplexity: 1761.020839
2025-09-27 13:31:23,206 Stage: Train 0.5 | Epoch: 55 | Iter: 84800 | Total Loss: 0.006623 | Recon Loss: 0.005821 | Commit Loss: 0.001604 | Perplexity: 1760.040226
2025-09-27 13:32:20,503 Stage: Train 0.5 | Epoch: 55 | Iter: 85000 | Total Loss: 0.006456 | Recon Loss: 0.005656 | Commit Loss: 0.001598 | Perplexity: 1756.462444
Trainning Epoch:  17%|█▋        | 56/330 [6:44:47<33:01:08, 433.83s/it]2025-09-27 13:33:17,941 Stage: Train 0.5 | Epoch: 56 | Iter: 85200 | Total Loss: 0.006597 | Recon Loss: 0.005802 | Commit Loss: 0.001590 | Perplexity: 1759.981662
2025-09-27 13:34:15,040 Stage: Train 0.5 | Epoch: 56 | Iter: 85400 | Total Loss: 0.006519 | Recon Loss: 0.005723 | Commit Loss: 0.001592 | Perplexity: 1757.954681
2025-09-27 13:35:11,846 Stage: Train 0.5 | Epoch: 56 | Iter: 85600 | Total Loss: 0.006616 | Recon Loss: 0.005817 | Commit Loss: 0.001597 | Perplexity: 1761.169116
2025-09-27 13:36:09,076 Stage: Train 0.5 | Epoch: 56 | Iter: 85800 | Total Loss: 0.006563 | Recon Loss: 0.005760 | Commit Loss: 0.001606 | Perplexity: 1759.483806
2025-09-27 13:37:06,169 Stage: Train 0.5 | Epoch: 56 | Iter: 86000 | Total Loss: 0.006447 | Recon Loss: 0.005644 | Commit Loss: 0.001605 | Perplexity: 1764.223318
2025-09-27 13:38:03,300 Stage: Train 0.5 | Epoch: 56 | Iter: 86200 | Total Loss: 0.006492 | Recon Loss: 0.005690 | Commit Loss: 0.001603 | Perplexity: 1763.885801
2025-09-27 13:39:00,333 Stage: Train 0.5 | Epoch: 56 | Iter: 86400 | Total Loss: 0.006580 | Recon Loss: 0.005777 | Commit Loss: 0.001606 | Perplexity: 1759.409856
Trainning Epoch:  17%|█▋        | 57/330 [6:52:01<32:53:53, 433.82s/it]2025-09-27 13:39:57,649 Stage: Train 0.5 | Epoch: 57 | Iter: 86600 | Total Loss: 0.006492 | Recon Loss: 0.005689 | Commit Loss: 0.001607 | Perplexity: 1762.217553
2025-09-27 13:40:54,745 Stage: Train 0.5 | Epoch: 57 | Iter: 86800 | Total Loss: 0.006561 | Recon Loss: 0.005764 | Commit Loss: 0.001596 | Perplexity: 1765.844597
2025-09-27 13:41:51,506 Stage: Train 0.5 | Epoch: 57 | Iter: 87000 | Total Loss: 0.006383 | Recon Loss: 0.005589 | Commit Loss: 0.001589 | Perplexity: 1761.585433
2025-09-27 13:42:48,375 Stage: Train 0.5 | Epoch: 57 | Iter: 87200 | Total Loss: 0.006558 | Recon Loss: 0.005754 | Commit Loss: 0.001607 | Perplexity: 1767.281603
2025-09-27 13:43:45,418 Stage: Train 0.5 | Epoch: 57 | Iter: 87400 | Total Loss: 0.006446 | Recon Loss: 0.005644 | Commit Loss: 0.001605 | Perplexity: 1770.473959
2025-09-27 13:44:42,510 Stage: Train 0.5 | Epoch: 57 | Iter: 87600 | Total Loss: 0.006509 | Recon Loss: 0.005708 | Commit Loss: 0.001601 | Perplexity: 1770.205198
2025-09-27 13:45:39,831 Stage: Train 0.5 | Epoch: 57 | Iter: 87800 | Total Loss: 0.006575 | Recon Loss: 0.005774 | Commit Loss: 0.001602 | Perplexity: 1766.803743
2025-09-27 13:46:37,058 Stage: Train 0.5 | Epoch: 57 | Iter: 88000 | Total Loss: 0.006547 | Recon Loss: 0.005742 | Commit Loss: 0.001611 | Perplexity: 1770.269698
Trainning Epoch:  18%|█▊        | 58/330 [6:59:14<32:45:43, 433.61s/it]2025-09-27 13:47:33,806 Stage: Train 0.5 | Epoch: 58 | Iter: 88200 | Total Loss: 0.006410 | Recon Loss: 0.005609 | Commit Loss: 0.001602 | Perplexity: 1771.356555
2025-09-27 13:48:30,935 Stage: Train 0.5 | Epoch: 58 | Iter: 88400 | Total Loss: 0.006515 | Recon Loss: 0.005719 | Commit Loss: 0.001592 | Perplexity: 1769.416920
2025-09-27 13:49:28,056 Stage: Train 0.5 | Epoch: 58 | Iter: 88600 | Total Loss: 0.006543 | Recon Loss: 0.005743 | Commit Loss: 0.001600 | Perplexity: 1771.465657
2025-09-27 13:50:25,059 Stage: Train 0.5 | Epoch: 58 | Iter: 88800 | Total Loss: 0.006462 | Recon Loss: 0.005664 | Commit Loss: 0.001596 | Perplexity: 1772.719084
2025-09-27 13:51:22,205 Stage: Train 0.5 | Epoch: 58 | Iter: 89000 | Total Loss: 0.006500 | Recon Loss: 0.005702 | Commit Loss: 0.001595 | Perplexity: 1768.116655
2025-09-27 13:52:19,339 Stage: Train 0.5 | Epoch: 58 | Iter: 89200 | Total Loss: 0.006467 | Recon Loss: 0.005669 | Commit Loss: 0.001595 | Perplexity: 1772.050858
2025-09-27 13:53:16,072 Stage: Train 0.5 | Epoch: 58 | Iter: 89400 | Total Loss: 0.006432 | Recon Loss: 0.005626 | Commit Loss: 0.001611 | Perplexity: 1774.768412
2025-09-27 13:54:13,150 Stage: Train 0.5 | Epoch: 58 | Iter: 89600 | Total Loss: 0.006424 | Recon Loss: 0.005618 | Commit Loss: 0.001612 | Perplexity: 1775.323436
Trainning Epoch:  18%|█▊        | 59/330 [7:06:27<32:38:13, 433.56s/it]2025-09-27 13:55:10,562 Stage: Train 0.5 | Epoch: 59 | Iter: 89800 | Total Loss: 0.006454 | Recon Loss: 0.005652 | Commit Loss: 0.001603 | Perplexity: 1777.001429
2025-09-27 13:56:07,741 Stage: Train 0.5 | Epoch: 59 | Iter: 90000 | Total Loss: 0.006373 | Recon Loss: 0.005572 | Commit Loss: 0.001602 | Perplexity: 1773.204302
2025-09-27 13:57:04,829 Stage: Train 0.5 | Epoch: 59 | Iter: 90200 | Total Loss: 0.006465 | Recon Loss: 0.005661 | Commit Loss: 0.001609 | Perplexity: 1774.240115
2025-09-27 13:58:01,963 Stage: Train 0.5 | Epoch: 59 | Iter: 90400 | Total Loss: 0.006434 | Recon Loss: 0.005641 | Commit Loss: 0.001586 | Perplexity: 1769.994189
2025-09-27 13:58:59,106 Stage: Train 0.5 | Epoch: 59 | Iter: 90600 | Total Loss: 0.006427 | Recon Loss: 0.005623 | Commit Loss: 0.001608 | Perplexity: 1777.866531
2025-09-27 13:59:56,104 Stage: Train 0.5 | Epoch: 59 | Iter: 90800 | Total Loss: 0.006462 | Recon Loss: 0.005659 | Commit Loss: 0.001606 | Perplexity: 1776.148205
2025-09-27 14:00:53,218 Stage: Train 0.5 | Epoch: 59 | Iter: 91000 | Total Loss: 0.006335 | Recon Loss: 0.005534 | Commit Loss: 0.001600 | Perplexity: 1779.510686
Trainning Epoch:  18%|█▊        | 60/330 [7:13:41<32:31:48, 433.74s/it]2025-09-27 14:01:50,648 Stage: Train 0.5 | Epoch: 60 | Iter: 91200 | Total Loss: 0.006404 | Recon Loss: 0.005594 | Commit Loss: 0.001620 | Perplexity: 1782.210723
2025-09-27 14:02:47,660 Stage: Train 0.5 | Epoch: 60 | Iter: 91400 | Total Loss: 0.006333 | Recon Loss: 0.005531 | Commit Loss: 0.001603 | Perplexity: 1777.525784
2025-09-27 14:03:44,729 Stage: Train 0.5 | Epoch: 60 | Iter: 91600 | Total Loss: 0.006491 | Recon Loss: 0.005685 | Commit Loss: 0.001613 | Perplexity: 1779.880488
2025-09-27 14:04:41,976 Stage: Train 0.5 | Epoch: 60 | Iter: 91800 | Total Loss: 0.006408 | Recon Loss: 0.005609 | Commit Loss: 0.001598 | Perplexity: 1775.948419
2025-09-27 14:05:38,766 Stage: Train 0.5 | Epoch: 60 | Iter: 92000 | Total Loss: 0.006386 | Recon Loss: 0.005583 | Commit Loss: 0.001606 | Perplexity: 1779.435727
2025-09-27 14:06:35,989 Stage: Train 0.5 | Epoch: 60 | Iter: 92200 | Total Loss: 0.006372 | Recon Loss: 0.005572 | Commit Loss: 0.001600 | Perplexity: 1773.723677
2025-09-27 14:07:33,118 Stage: Train 0.5 | Epoch: 60 | Iter: 92400 | Total Loss: 0.006396 | Recon Loss: 0.005593 | Commit Loss: 0.001606 | Perplexity: 1779.924349
2025-09-27 14:08:30,264 Stage: Train 0.5 | Epoch: 60 | Iter: 92600 | Total Loss: 0.006365 | Recon Loss: 0.005566 | Commit Loss: 0.001597 | Perplexity: 1780.801554
Trainning Epoch:  18%|█▊        | 61/330 [7:20:55<32:24:42, 433.76s/it]2025-09-27 14:09:27,710 Stage: Train 0.5 | Epoch: 61 | Iter: 92800 | Total Loss: 0.006475 | Recon Loss: 0.005669 | Commit Loss: 0.001611 | Perplexity: 1779.330338
2025-09-27 14:10:24,924 Stage: Train 0.5 | Epoch: 61 | Iter: 93000 | Total Loss: 0.006344 | Recon Loss: 0.005542 | Commit Loss: 0.001603 | Perplexity: 1781.711971
2025-09-27 14:11:21,422 Stage: Train 0.5 | Epoch: 61 | Iter: 93200 | Total Loss: 0.006383 | Recon Loss: 0.005573 | Commit Loss: 0.001619 | Perplexity: 1785.921447
2025-09-27 14:12:18,647 Stage: Train 0.5 | Epoch: 61 | Iter: 93400 | Total Loss: 0.006398 | Recon Loss: 0.005597 | Commit Loss: 0.001603 | Perplexity: 1775.704186
2025-09-27 14:13:15,832 Stage: Train 0.5 | Epoch: 61 | Iter: 93600 | Total Loss: 0.006332 | Recon Loss: 0.005526 | Commit Loss: 0.001612 | Perplexity: 1781.015088
2025-09-27 14:14:12,692 Stage: Train 0.5 | Epoch: 61 | Iter: 93800 | Total Loss: 0.006351 | Recon Loss: 0.005547 | Commit Loss: 0.001607 | Perplexity: 1780.463207
2025-09-27 14:15:09,703 Stage: Train 0.5 | Epoch: 61 | Iter: 94000 | Total Loss: 0.006405 | Recon Loss: 0.005606 | Commit Loss: 0.001599 | Perplexity: 1779.821968
Trainning Epoch:  19%|█▉        | 62/330 [7:28:09<32:16:55, 433.64s/it]2025-09-27 14:16:06,926 Stage: Train 0.5 | Epoch: 62 | Iter: 94200 | Total Loss: 0.006311 | Recon Loss: 0.005519 | Commit Loss: 0.001582 | Perplexity: 1774.173793
2025-09-27 14:17:03,791 Stage: Train 0.5 | Epoch: 62 | Iter: 94400 | Total Loss: 0.006291 | Recon Loss: 0.005487 | Commit Loss: 0.001607 | Perplexity: 1781.644461
2025-09-27 14:18:01,026 Stage: Train 0.5 | Epoch: 62 | Iter: 94600 | Total Loss: 0.006392 | Recon Loss: 0.005596 | Commit Loss: 0.001593 | Perplexity: 1780.105460
2025-09-27 14:18:58,074 Stage: Train 0.5 | Epoch: 62 | Iter: 94800 | Total Loss: 0.006289 | Recon Loss: 0.005492 | Commit Loss: 0.001595 | Perplexity: 1779.125956
2025-09-27 14:19:55,111 Stage: Train 0.5 | Epoch: 62 | Iter: 95000 | Total Loss: 0.006420 | Recon Loss: 0.005615 | Commit Loss: 0.001608 | Perplexity: 1779.286796
2025-09-27 14:20:52,207 Stage: Train 0.5 | Epoch: 62 | Iter: 95200 | Total Loss: 0.006326 | Recon Loss: 0.005525 | Commit Loss: 0.001602 | Perplexity: 1787.629230
2025-09-27 14:21:49,317 Stage: Train 0.5 | Epoch: 62 | Iter: 95400 | Total Loss: 0.006373 | Recon Loss: 0.005573 | Commit Loss: 0.001600 | Perplexity: 1782.662265
2025-09-27 14:22:46,443 Stage: Train 0.5 | Epoch: 62 | Iter: 95600 | Total Loss: 0.006341 | Recon Loss: 0.005541 | Commit Loss: 0.001600 | Perplexity: 1783.536060
Trainning Epoch:  19%|█▉        | 63/330 [7:35:22<32:09:15, 433.54s/it]2025-09-27 14:23:43,528 Stage: Train 0.5 | Epoch: 63 | Iter: 95800 | Total Loss: 0.006279 | Recon Loss: 0.005473 | Commit Loss: 0.001612 | Perplexity: 1781.000159
2025-09-27 14:24:38,972 Stage: Train 0.5 | Epoch: 63 | Iter: 96000 | Total Loss: 0.006281 | Recon Loss: 0.005482 | Commit Loss: 0.001597 | Perplexity: 1785.281795
2025-09-27 14:25:35,976 Stage: Train 0.5 | Epoch: 63 | Iter: 96200 | Total Loss: 0.006418 | Recon Loss: 0.005613 | Commit Loss: 0.001611 | Perplexity: 1785.187538
2025-09-27 14:26:32,996 Stage: Train 0.5 | Epoch: 63 | Iter: 96400 | Total Loss: 0.006404 | Recon Loss: 0.005605 | Commit Loss: 0.001597 | Perplexity: 1785.075173
2025-09-27 14:27:29,910 Stage: Train 0.5 | Epoch: 63 | Iter: 96600 | Total Loss: 0.006284 | Recon Loss: 0.005477 | Commit Loss: 0.001615 | Perplexity: 1784.388990
2025-09-27 14:28:27,086 Stage: Train 0.5 | Epoch: 63 | Iter: 96800 | Total Loss: 0.006363 | Recon Loss: 0.005553 | Commit Loss: 0.001620 | Perplexity: 1786.436725
2025-09-27 14:29:23,979 Stage: Train 0.5 | Epoch: 63 | Iter: 97000 | Total Loss: 0.006170 | Recon Loss: 0.005369 | Commit Loss: 0.001602 | Perplexity: 1783.575742
2025-09-27 14:30:21,125 Stage: Train 0.5 | Epoch: 63 | Iter: 97200 | Total Loss: 0.006456 | Recon Loss: 0.005656 | Commit Loss: 0.001599 | Perplexity: 1783.265269
Trainning Epoch:  19%|█▉        | 64/330 [7:42:34<31:59:51, 433.05s/it]2025-09-27 14:31:18,256 Stage: Train 0.5 | Epoch: 64 | Iter: 97400 | Total Loss: 0.006219 | Recon Loss: 0.005413 | Commit Loss: 0.001612 | Perplexity: 1786.343398
2025-09-27 14:32:15,333 Stage: Train 0.5 | Epoch: 64 | Iter: 97600 | Total Loss: 0.006304 | Recon Loss: 0.005506 | Commit Loss: 0.001595 | Perplexity: 1788.389244
2025-09-27 14:33:12,254 Stage: Train 0.5 | Epoch: 64 | Iter: 97800 | Total Loss: 0.006340 | Recon Loss: 0.005530 | Commit Loss: 0.001620 | Perplexity: 1794.575526
2025-09-27 14:34:09,483 Stage: Train 0.5 | Epoch: 64 | Iter: 98000 | Total Loss: 0.006295 | Recon Loss: 0.005494 | Commit Loss: 0.001601 | Perplexity: 1787.178615
2025-09-27 14:35:06,323 Stage: Train 0.5 | Epoch: 64 | Iter: 98200 | Total Loss: 0.006318 | Recon Loss: 0.005517 | Commit Loss: 0.001602 | Perplexity: 1786.503556
2025-09-27 14:36:03,481 Stage: Train 0.5 | Epoch: 64 | Iter: 98400 | Total Loss: 0.006286 | Recon Loss: 0.005490 | Commit Loss: 0.001592 | Perplexity: 1786.306138
2025-09-27 14:37:00,509 Stage: Train 0.5 | Epoch: 64 | Iter: 98600 | Total Loss: 0.006235 | Recon Loss: 0.005437 | Commit Loss: 0.001595 | Perplexity: 1792.641543
Trainning Epoch:  20%|█▉        | 65/330 [7:49:47<31:53:02, 433.14s/it]2025-09-27 14:37:57,718 Stage: Train 0.5 | Epoch: 65 | Iter: 98800 | Total Loss: 0.006297 | Recon Loss: 0.005489 | Commit Loss: 0.001615 | Perplexity: 1794.691570
2025-09-27 14:38:54,846 Stage: Train 0.5 | Epoch: 65 | Iter: 99000 | Total Loss: 0.006308 | Recon Loss: 0.005506 | Commit Loss: 0.001603 | Perplexity: 1793.362166
2025-09-27 14:39:52,009 Stage: Train 0.5 | Epoch: 65 | Iter: 99200 | Total Loss: 0.006303 | Recon Loss: 0.005497 | Commit Loss: 0.001611 | Perplexity: 1792.219337
2025-09-27 14:40:49,296 Stage: Train 0.5 | Epoch: 65 | Iter: 99400 | Total Loss: 0.006230 | Recon Loss: 0.005429 | Commit Loss: 0.001602 | Perplexity: 1789.901889
2025-09-27 14:41:46,020 Stage: Train 0.5 | Epoch: 65 | Iter: 99600 | Total Loss: 0.006379 | Recon Loss: 0.005575 | Commit Loss: 0.001609 | Perplexity: 1795.621479
2025-09-27 14:42:43,320 Stage: Train 0.5 | Epoch: 65 | Iter: 99800 | Total Loss: 0.006227 | Recon Loss: 0.005432 | Commit Loss: 0.001591 | Perplexity: 1794.852773
2025-09-27 14:43:40,749 Stage: Train 0.5 | Epoch: 65 | Iter: 100000 | Total Loss: 0.006198 | Recon Loss: 0.005403 | Commit Loss: 0.001589 | Perplexity: 1791.819659
2025-09-27 14:43:40,749 Saving model at iteration 100000
2025-09-27 14:43:40,955 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000
2025-09-27 14:43:41,460 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/model.safetensors
2025-09-27 14:43:41,984 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/optimizer.bin
2025-09-27 14:43:41,984 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/scheduler.bin
2025-09-27 14:43:41,984 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/sampler.bin
2025-09-27 14:43:41,985 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_66_step_100000/random_states_0.pkl
2025-09-27 14:44:39,764 Stage: Train 0.5 | Epoch: 65 | Iter: 100200 | Total Loss: 0.006292 | Recon Loss: 0.005487 | Commit Loss: 0.001609 | Perplexity: 1797.470151
Trainning Epoch:  20%|██        | 66/330 [7:57:03<31:49:46, 434.04s/it]2025-09-27 14:45:37,197 Stage: Train 0.5 | Epoch: 66 | Iter: 100400 | Total Loss: 0.006279 | Recon Loss: 0.005474 | Commit Loss: 0.001610 | Perplexity: 1789.590753
2025-09-27 14:46:34,392 Stage: Train 0.5 | Epoch: 66 | Iter: 100600 | Total Loss: 0.006220 | Recon Loss: 0.005430 | Commit Loss: 0.001581 | Perplexity: 1793.922947
2025-09-27 14:47:31,184 Stage: Train 0.5 | Epoch: 66 | Iter: 100800 | Total Loss: 0.006207 | Recon Loss: 0.005409 | Commit Loss: 0.001597 | Perplexity: 1796.667476
2025-09-27 14:48:28,412 Stage: Train 0.5 | Epoch: 66 | Iter: 101000 | Total Loss: 0.006203 | Recon Loss: 0.005399 | Commit Loss: 0.001609 | Perplexity: 1797.883793
2025-09-27 14:49:25,660 Stage: Train 0.5 | Epoch: 66 | Iter: 101200 | Total Loss: 0.006237 | Recon Loss: 0.005437 | Commit Loss: 0.001601 | Perplexity: 1799.420635
2025-09-27 14:50:22,944 Stage: Train 0.5 | Epoch: 66 | Iter: 101400 | Total Loss: 0.006246 | Recon Loss: 0.005447 | Commit Loss: 0.001598 | Perplexity: 1797.376414
2025-09-27 14:51:20,223 Stage: Train 0.5 | Epoch: 66 | Iter: 101600 | Total Loss: 0.006257 | Recon Loss: 0.005458 | Commit Loss: 0.001597 | Perplexity: 1799.484153
Trainning Epoch:  20%|██        | 67/330 [8:04:18<31:43:08, 434.18s/it]2025-09-27 14:52:17,532 Stage: Train 0.5 | Epoch: 67 | Iter: 101800 | Total Loss: 0.006328 | Recon Loss: 0.005523 | Commit Loss: 0.001610 | Perplexity: 1796.324371
2025-09-27 14:53:14,374 Stage: Train 0.5 | Epoch: 67 | Iter: 102000 | Total Loss: 0.006204 | Recon Loss: 0.005404 | Commit Loss: 0.001599 | Perplexity: 1797.462237
2025-09-27 14:54:11,589 Stage: Train 0.5 | Epoch: 67 | Iter: 102200 | Total Loss: 0.006184 | Recon Loss: 0.005382 | Commit Loss: 0.001604 | Perplexity: 1796.015565
2025-09-27 14:55:08,757 Stage: Train 0.5 | Epoch: 67 | Iter: 102400 | Total Loss: 0.006231 | Recon Loss: 0.005426 | Commit Loss: 0.001610 | Perplexity: 1799.072517
2025-09-27 14:56:05,720 Stage: Train 0.5 | Epoch: 67 | Iter: 102600 | Total Loss: 0.006198 | Recon Loss: 0.005402 | Commit Loss: 0.001593 | Perplexity: 1798.057586
2025-09-27 14:57:03,078 Stage: Train 0.5 | Epoch: 67 | Iter: 102800 | Total Loss: 0.006191 | Recon Loss: 0.005391 | Commit Loss: 0.001600 | Perplexity: 1798.293779
2025-09-27 14:58:00,475 Stage: Train 0.5 | Epoch: 67 | Iter: 103000 | Total Loss: 0.006203 | Recon Loss: 0.005396 | Commit Loss: 0.001614 | Perplexity: 1798.554805
2025-09-27 14:58:57,291 Stage: Train 0.5 | Epoch: 67 | Iter: 103200 | Total Loss: 0.006217 | Recon Loss: 0.005410 | Commit Loss: 0.001614 | Perplexity: 1802.070550
Trainning Epoch:  21%|██        | 68/330 [8:11:32<31:35:37, 434.11s/it]2025-09-27 14:59:54,552 Stage: Train 0.5 | Epoch: 68 | Iter: 103400 | Total Loss: 0.006223 | Recon Loss: 0.005423 | Commit Loss: 0.001600 | Perplexity: 1799.530122
2025-09-27 15:00:51,862 Stage: Train 0.5 | Epoch: 68 | Iter: 103600 | Total Loss: 0.006189 | Recon Loss: 0.005383 | Commit Loss: 0.001612 | Perplexity: 1798.875839
2025-09-27 15:01:49,149 Stage: Train 0.5 | Epoch: 68 | Iter: 103800 | Total Loss: 0.006323 | Recon Loss: 0.005518 | Commit Loss: 0.001611 | Perplexity: 1797.913600
2025-09-27 15:02:46,329 Stage: Train 0.5 | Epoch: 68 | Iter: 104000 | Total Loss: 0.006179 | Recon Loss: 0.005378 | Commit Loss: 0.001602 | Perplexity: 1795.365809
2025-09-27 15:03:43,551 Stage: Train 0.5 | Epoch: 68 | Iter: 104200 | Total Loss: 0.006195 | Recon Loss: 0.005392 | Commit Loss: 0.001608 | Perplexity: 1799.793664
2025-09-27 15:04:41,041 Stage: Train 0.5 | Epoch: 68 | Iter: 104400 | Total Loss: 0.006147 | Recon Loss: 0.005357 | Commit Loss: 0.001581 | Perplexity: 1796.912997
2025-09-27 15:05:37,804 Stage: Train 0.5 | Epoch: 68 | Iter: 104600 | Total Loss: 0.006253 | Recon Loss: 0.005446 | Commit Loss: 0.001614 | Perplexity: 1805.054074
2025-09-27 15:06:35,004 Stage: Train 0.5 | Epoch: 68 | Iter: 104800 | Total Loss: 0.006181 | Recon Loss: 0.005377 | Commit Loss: 0.001608 | Perplexity: 1803.166158
Trainning Epoch:  21%|██        | 69/330 [8:18:46<31:28:52, 434.23s/it]2025-09-27 15:07:32,474 Stage: Train 0.5 | Epoch: 69 | Iter: 105000 | Total Loss: 0.006216 | Recon Loss: 0.005414 | Commit Loss: 0.001604 | Perplexity: 1802.508147
2025-09-27 15:08:29,681 Stage: Train 0.5 | Epoch: 69 | Iter: 105200 | Total Loss: 0.006119 | Recon Loss: 0.005320 | Commit Loss: 0.001599 | Perplexity: 1802.614265
2025-09-27 15:09:27,048 Stage: Train 0.5 | Epoch: 69 | Iter: 105400 | Total Loss: 0.006193 | Recon Loss: 0.005389 | Commit Loss: 0.001608 | Perplexity: 1802.853106
2025-09-27 15:10:24,339 Stage: Train 0.5 | Epoch: 69 | Iter: 105600 | Total Loss: 0.006170 | Recon Loss: 0.005369 | Commit Loss: 0.001601 | Perplexity: 1800.655135
2025-09-27 15:11:21,275 Stage: Train 0.5 | Epoch: 69 | Iter: 105800 | Total Loss: 0.006168 | Recon Loss: 0.005362 | Commit Loss: 0.001612 | Perplexity: 1804.306415
2025-09-27 15:12:18,546 Stage: Train 0.5 | Epoch: 69 | Iter: 106000 | Total Loss: 0.006133 | Recon Loss: 0.005323 | Commit Loss: 0.001621 | Perplexity: 1807.694564
2025-09-27 15:13:15,697 Stage: Train 0.5 | Epoch: 69 | Iter: 106200 | Total Loss: 0.006281 | Recon Loss: 0.005479 | Commit Loss: 0.001604 | Perplexity: 1801.731595
Trainning Epoch:  21%|██        | 70/330 [8:26:01<31:22:11, 434.35s/it]2025-09-27 15:14:12,951 Stage: Train 0.5 | Epoch: 70 | Iter: 106400 | Total Loss: 0.006180 | Recon Loss: 0.005374 | Commit Loss: 0.001611 | Perplexity: 1801.666561
2025-09-27 15:15:09,822 Stage: Train 0.5 | Epoch: 70 | Iter: 106600 | Total Loss: 0.006138 | Recon Loss: 0.005336 | Commit Loss: 0.001604 | Perplexity: 1803.302716
2025-09-27 15:16:06,816 Stage: Train 0.5 | Epoch: 70 | Iter: 106800 | Total Loss: 0.006179 | Recon Loss: 0.005380 | Commit Loss: 0.001600 | Perplexity: 1806.849679
2025-09-27 15:17:03,478 Stage: Train 0.5 | Epoch: 70 | Iter: 107000 | Total Loss: 0.006154 | Recon Loss: 0.005346 | Commit Loss: 0.001614 | Perplexity: 1807.472473
2025-09-27 15:18:00,827 Stage: Train 0.5 | Epoch: 70 | Iter: 107200 | Total Loss: 0.006134 | Recon Loss: 0.005328 | Commit Loss: 0.001612 | Perplexity: 1806.709308
2025-09-27 15:18:57,990 Stage: Train 0.5 | Epoch: 70 | Iter: 107400 | Total Loss: 0.006148 | Recon Loss: 0.005349 | Commit Loss: 0.001597 | Perplexity: 1803.144673
2025-09-27 15:19:55,156 Stage: Train 0.5 | Epoch: 70 | Iter: 107600 | Total Loss: 0.006239 | Recon Loss: 0.005434 | Commit Loss: 0.001610 | Perplexity: 1801.980328
2025-09-27 15:20:52,332 Stage: Train 0.5 | Epoch: 70 | Iter: 107800 | Total Loss: 0.006144 | Recon Loss: 0.005331 | Commit Loss: 0.001626 | Perplexity: 1811.624977
Trainning Epoch:  22%|██▏       | 71/330 [8:33:15<31:13:58, 434.12s/it]2025-09-27 15:21:49,594 Stage: Train 0.5 | Epoch: 71 | Iter: 108000 | Total Loss: 0.006106 | Recon Loss: 0.005309 | Commit Loss: 0.001594 | Perplexity: 1801.118373
2025-09-27 15:22:46,478 Stage: Train 0.5 | Epoch: 71 | Iter: 108200 | Total Loss: 0.006125 | Recon Loss: 0.005326 | Commit Loss: 0.001598 | Perplexity: 1806.692419
2025-09-27 15:23:43,699 Stage: Train 0.5 | Epoch: 71 | Iter: 108400 | Total Loss: 0.006112 | Recon Loss: 0.005307 | Commit Loss: 0.001609 | Perplexity: 1806.844932
2025-09-27 15:24:40,913 Stage: Train 0.5 | Epoch: 71 | Iter: 108600 | Total Loss: 0.006237 | Recon Loss: 0.005429 | Commit Loss: 0.001615 | Perplexity: 1811.770911
2025-09-27 15:25:38,302 Stage: Train 0.5 | Epoch: 71 | Iter: 108800 | Total Loss: 0.006071 | Recon Loss: 0.005271 | Commit Loss: 0.001599 | Perplexity: 1805.651086
2025-09-27 15:26:35,611 Stage: Train 0.5 | Epoch: 71 | Iter: 109000 | Total Loss: 0.006117 | Recon Loss: 0.005314 | Commit Loss: 0.001605 | Perplexity: 1803.568485
2025-09-27 15:27:32,762 Stage: Train 0.5 | Epoch: 71 | Iter: 109200 | Total Loss: 0.006234 | Recon Loss: 0.005431 | Commit Loss: 0.001605 | Perplexity: 1808.135589
Trainning Epoch:  22%|██▏       | 72/330 [8:40:29<31:07:05, 434.21s/it]2025-09-27 15:28:30,133 Stage: Train 0.5 | Epoch: 72 | Iter: 109400 | Total Loss: 0.006050 | Recon Loss: 0.005239 | Commit Loss: 0.001622 | Perplexity: 1807.342869
2025-09-27 15:29:26,841 Stage: Train 0.5 | Epoch: 72 | Iter: 109600 | Total Loss: 0.006091 | Recon Loss: 0.005283 | Commit Loss: 0.001616 | Perplexity: 1805.398985
2025-09-27 15:30:24,015 Stage: Train 0.5 | Epoch: 72 | Iter: 109800 | Total Loss: 0.006135 | Recon Loss: 0.005330 | Commit Loss: 0.001610 | Perplexity: 1805.817180
2025-09-27 15:31:20,937 Stage: Train 0.5 | Epoch: 72 | Iter: 110000 | Total Loss: 0.006157 | Recon Loss: 0.005353 | Commit Loss: 0.001608 | Perplexity: 1804.626324
2025-09-27 15:32:18,149 Stage: Train 0.5 | Epoch: 72 | Iter: 110200 | Total Loss: 0.006096 | Recon Loss: 0.005296 | Commit Loss: 0.001600 | Perplexity: 1804.732766
2025-09-27 15:33:15,403 Stage: Train 0.5 | Epoch: 72 | Iter: 110400 | Total Loss: 0.006076 | Recon Loss: 0.005275 | Commit Loss: 0.001602 | Perplexity: 1804.985444
2025-09-27 15:34:12,390 Stage: Train 0.5 | Epoch: 72 | Iter: 110600 | Total Loss: 0.006016 | Recon Loss: 0.005215 | Commit Loss: 0.001601 | Perplexity: 1805.492725
2025-09-27 15:35:09,225 Stage: Train 0.5 | Epoch: 72 | Iter: 110800 | Total Loss: 0.006092 | Recon Loss: 0.005273 | Commit Loss: 0.001639 | Perplexity: 1814.533612
Trainning Epoch:  22%|██▏       | 73/330 [8:47:42<30:58:44, 433.95s/it]2025-09-27 15:36:06,574 Stage: Train 0.5 | Epoch: 73 | Iter: 111000 | Total Loss: 0.006184 | Recon Loss: 0.005384 | Commit Loss: 0.001602 | Perplexity: 1804.492041
2025-09-27 15:37:03,828 Stage: Train 0.5 | Epoch: 73 | Iter: 111200 | Total Loss: 0.006070 | Recon Loss: 0.005272 | Commit Loss: 0.001595 | Perplexity: 1805.569785
2025-09-27 15:38:01,036 Stage: Train 0.5 | Epoch: 73 | Iter: 111400 | Total Loss: 0.006064 | Recon Loss: 0.005258 | Commit Loss: 0.001612 | Perplexity: 1811.196821
2025-09-27 15:38:58,167 Stage: Train 0.5 | Epoch: 73 | Iter: 111600 | Total Loss: 0.006112 | Recon Loss: 0.005309 | Commit Loss: 0.001605 | Perplexity: 1806.043146
2025-09-27 15:39:55,442 Stage: Train 0.5 | Epoch: 73 | Iter: 111800 | Total Loss: 0.006125 | Recon Loss: 0.005312 | Commit Loss: 0.001626 | Perplexity: 1812.299271
2025-09-27 15:40:52,184 Stage: Train 0.5 | Epoch: 73 | Iter: 112000 | Total Loss: 0.006127 | Recon Loss: 0.005322 | Commit Loss: 0.001610 | Perplexity: 1805.633956
2025-09-27 15:41:49,368 Stage: Train 0.5 | Epoch: 73 | Iter: 112200 | Total Loss: 0.006145 | Recon Loss: 0.005334 | Commit Loss: 0.001622 | Perplexity: 1809.324978
2025-09-27 15:42:44,975 Stage: Train 0.5 | Epoch: 73 | Iter: 112400 | Total Loss: 0.006028 | Recon Loss: 0.005223 | Commit Loss: 0.001610 | Perplexity: 1805.203531
Trainning Epoch:  22%|██▏       | 74/330 [8:54:55<30:49:46, 433.54s/it]2025-09-27 15:43:42,421 Stage: Train 0.5 | Epoch: 74 | Iter: 112600 | Total Loss: 0.006043 | Recon Loss: 0.005243 | Commit Loss: 0.001602 | Perplexity: 1804.966389
2025-09-27 15:44:39,749 Stage: Train 0.5 | Epoch: 74 | Iter: 112800 | Total Loss: 0.006021 | Recon Loss: 0.005220 | Commit Loss: 0.001603 | Perplexity: 1807.805532
2025-09-27 15:45:36,906 Stage: Train 0.5 | Epoch: 74 | Iter: 113000 | Total Loss: 0.006065 | Recon Loss: 0.005263 | Commit Loss: 0.001603 | Perplexity: 1806.073163
2025-09-27 15:46:34,091 Stage: Train 0.5 | Epoch: 74 | Iter: 113200 | Total Loss: 0.006090 | Recon Loss: 0.005281 | Commit Loss: 0.001619 | Perplexity: 1807.061578
2025-09-27 15:47:30,723 Stage: Train 0.5 | Epoch: 74 | Iter: 113400 | Total Loss: 0.006072 | Recon Loss: 0.005257 | Commit Loss: 0.001631 | Perplexity: 1812.820367
2025-09-27 15:48:27,916 Stage: Train 0.5 | Epoch: 74 | Iter: 113600 | Total Loss: 0.006070 | Recon Loss: 0.005269 | Commit Loss: 0.001602 | Perplexity: 1806.186351
2025-09-27 15:49:24,987 Stage: Train 0.5 | Epoch: 74 | Iter: 113800 | Total Loss: 0.006033 | Recon Loss: 0.005225 | Commit Loss: 0.001615 | Perplexity: 1811.548190
Trainning Epoch:  23%|██▎       | 75/330 [9:02:09<30:43:06, 433.67s/it]2025-09-27 15:50:22,290 Stage: Train 0.5 | Epoch: 75 | Iter: 114000 | Total Loss: 0.006025 | Recon Loss: 0.005222 | Commit Loss: 0.001607 | Perplexity: 1805.268687
2025-09-27 15:51:19,410 Stage: Train 0.5 | Epoch: 75 | Iter: 114200 | Total Loss: 0.006015 | Recon Loss: 0.005214 | Commit Loss: 0.001601 | Perplexity: 1808.398450
2025-09-27 15:52:16,432 Stage: Train 0.5 | Epoch: 75 | Iter: 114400 | Total Loss: 0.006025 | Recon Loss: 0.005220 | Commit Loss: 0.001611 | Perplexity: 1810.005756
2025-09-27 15:53:13,178 Stage: Train 0.5 | Epoch: 75 | Iter: 114600 | Total Loss: 0.006124 | Recon Loss: 0.005312 | Commit Loss: 0.001624 | Perplexity: 1810.700829
2025-09-27 15:54:10,449 Stage: Train 0.5 | Epoch: 75 | Iter: 114800 | Total Loss: 0.006058 | Recon Loss: 0.005253 | Commit Loss: 0.001609 | Perplexity: 1810.697790
2025-09-27 15:55:07,565 Stage: Train 0.5 | Epoch: 75 | Iter: 115000 | Total Loss: 0.006101 | Recon Loss: 0.005295 | Commit Loss: 0.001612 | Perplexity: 1806.374039
2025-09-27 15:56:04,963 Stage: Train 0.5 | Epoch: 75 | Iter: 115200 | Total Loss: 0.006002 | Recon Loss: 0.005194 | Commit Loss: 0.001615 | Perplexity: 1807.282000
2025-09-27 15:57:01,903 Stage: Train 0.5 | Epoch: 75 | Iter: 115400 | Total Loss: 0.006152 | Recon Loss: 0.005345 | Commit Loss: 0.001613 | Perplexity: 1804.294324
Trainning Epoch:  23%|██▎       | 76/330 [9:09:23<30:36:02, 433.71s/it]2025-09-27 15:57:59,321 Stage: Train 0.5 | Epoch: 76 | Iter: 115600 | Total Loss: 0.005995 | Recon Loss: 0.005190 | Commit Loss: 0.001610 | Perplexity: 1809.882628
2025-09-27 15:58:56,016 Stage: Train 0.5 | Epoch: 76 | Iter: 115800 | Total Loss: 0.006095 | Recon Loss: 0.005291 | Commit Loss: 0.001607 | Perplexity: 1810.030504
2025-09-27 15:59:53,161 Stage: Train 0.5 | Epoch: 76 | Iter: 116000 | Total Loss: 0.006051 | Recon Loss: 0.005250 | Commit Loss: 0.001603 | Perplexity: 1805.597745
2025-09-27 16:00:50,364 Stage: Train 0.5 | Epoch: 76 | Iter: 116200 | Total Loss: 0.006016 | Recon Loss: 0.005205 | Commit Loss: 0.001623 | Perplexity: 1809.740901
2025-09-27 16:01:47,541 Stage: Train 0.5 | Epoch: 76 | Iter: 116400 | Total Loss: 0.006061 | Recon Loss: 0.005249 | Commit Loss: 0.001626 | Perplexity: 1811.627050
2025-09-27 16:02:44,744 Stage: Train 0.5 | Epoch: 76 | Iter: 116600 | Total Loss: 0.006040 | Recon Loss: 0.005232 | Commit Loss: 0.001615 | Perplexity: 1808.572803
2025-09-27 16:03:41,553 Stage: Train 0.5 | Epoch: 76 | Iter: 116800 | Total Loss: 0.006022 | Recon Loss: 0.005205 | Commit Loss: 0.001634 | Perplexity: 1811.172380
Trainning Epoch:  23%|██▎       | 77/330 [9:16:36<30:28:35, 433.66s/it]2025-09-27 16:04:38,377 Stage: Train 0.5 | Epoch: 77 | Iter: 117000 | Total Loss: 0.006031 | Recon Loss: 0.005222 | Commit Loss: 0.001617 | Perplexity: 1810.432047
2025-09-27 16:05:35,484 Stage: Train 0.5 | Epoch: 77 | Iter: 117200 | Total Loss: 0.006001 | Recon Loss: 0.005199 | Commit Loss: 0.001604 | Perplexity: 1809.820582
2025-09-27 16:06:32,662 Stage: Train 0.5 | Epoch: 77 | Iter: 117400 | Total Loss: 0.005982 | Recon Loss: 0.005169 | Commit Loss: 0.001625 | Perplexity: 1812.226595
2025-09-27 16:07:30,075 Stage: Train 0.5 | Epoch: 77 | Iter: 117600 | Total Loss: 0.006002 | Recon Loss: 0.005193 | Commit Loss: 0.001616 | Perplexity: 1812.987391
2025-09-27 16:08:27,039 Stage: Train 0.5 | Epoch: 77 | Iter: 117800 | Total Loss: 0.006030 | Recon Loss: 0.005221 | Commit Loss: 0.001619 | Perplexity: 1809.387565
2025-09-27 16:09:24,223 Stage: Train 0.5 | Epoch: 77 | Iter: 118000 | Total Loss: 0.006058 | Recon Loss: 0.005242 | Commit Loss: 0.001632 | Perplexity: 1811.955476
2025-09-27 16:10:21,442 Stage: Train 0.5 | Epoch: 77 | Iter: 118200 | Total Loss: 0.005993 | Recon Loss: 0.005175 | Commit Loss: 0.001636 | Perplexity: 1811.280943
2025-09-27 16:11:18,313 Stage: Train 0.5 | Epoch: 77 | Iter: 118400 | Total Loss: 0.006122 | Recon Loss: 0.005309 | Commit Loss: 0.001625 | Perplexity: 1809.740019
Trainning Epoch:  24%|██▎       | 78/330 [9:23:50<30:21:24, 433.67s/it]2025-09-27 16:12:15,680 Stage: Train 0.5 | Epoch: 78 | Iter: 118600 | Total Loss: 0.005941 | Recon Loss: 0.005128 | Commit Loss: 0.001625 | Perplexity: 1808.574225
2025-09-27 16:13:12,816 Stage: Train 0.5 | Epoch: 78 | Iter: 118800 | Total Loss: 0.005937 | Recon Loss: 0.005131 | Commit Loss: 0.001613 | Perplexity: 1806.889095
2025-09-27 16:14:10,030 Stage: Train 0.5 | Epoch: 78 | Iter: 119000 | Total Loss: 0.006017 | Recon Loss: 0.005203 | Commit Loss: 0.001628 | Perplexity: 1812.294286
2025-09-27 16:15:07,028 Stage: Train 0.5 | Epoch: 78 | Iter: 119200 | Total Loss: 0.006034 | Recon Loss: 0.005220 | Commit Loss: 0.001629 | Perplexity: 1808.503217
2025-09-27 16:16:04,112 Stage: Train 0.5 | Epoch: 78 | Iter: 119400 | Total Loss: 0.005991 | Recon Loss: 0.005179 | Commit Loss: 0.001624 | Perplexity: 1812.571580
2025-09-27 16:17:00,955 Stage: Train 0.5 | Epoch: 78 | Iter: 119600 | Total Loss: 0.006030 | Recon Loss: 0.005218 | Commit Loss: 0.001625 | Perplexity: 1808.890804
2025-09-27 16:17:57,943 Stage: Train 0.5 | Epoch: 78 | Iter: 119800 | Total Loss: 0.006080 | Recon Loss: 0.005268 | Commit Loss: 0.001623 | Perplexity: 1809.058729
2025-09-27 16:18:55,007 Stage: Train 0.5 | Epoch: 78 | Iter: 120000 | Total Loss: 0.006113 | Recon Loss: 0.005305 | Commit Loss: 0.001617 | Perplexity: 1805.388718
2025-09-27 16:18:55,007 Saving model at iteration 120000
2025-09-27 16:18:55,216 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000
2025-09-27 16:18:55,754 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/model.safetensors
2025-09-27 16:18:56,320 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/optimizer.bin
2025-09-27 16:18:56,320 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/scheduler.bin
2025-09-27 16:18:56,320 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/sampler.bin
2025-09-27 16:18:56,321 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_79_step_120000/random_states_0.pkl
Trainning Epoch:  24%|██▍       | 79/330 [9:31:05<30:16:08, 434.14s/it]2025-09-27 16:19:54,077 Stage: Train 0.5 | Epoch: 79 | Iter: 120200 | Total Loss: 0.006102 | Recon Loss: 0.005293 | Commit Loss: 0.001618 | Perplexity: 1811.021883
2025-09-27 16:20:51,292 Stage: Train 0.5 | Epoch: 79 | Iter: 120400 | Total Loss: 0.005989 | Recon Loss: 0.005184 | Commit Loss: 0.001610 | Perplexity: 1810.190775
2025-09-27 16:21:48,408 Stage: Train 0.5 | Epoch: 79 | Iter: 120600 | Total Loss: 0.005987 | Recon Loss: 0.005178 | Commit Loss: 0.001618 | Perplexity: 1806.492385
2025-09-27 16:22:45,027 Stage: Train 0.5 | Epoch: 79 | Iter: 120800 | Total Loss: 0.005955 | Recon Loss: 0.005144 | Commit Loss: 0.001623 | Perplexity: 1804.861689
2025-09-27 16:23:42,165 Stage: Train 0.5 | Epoch: 79 | Iter: 121000 | Total Loss: 0.006010 | Recon Loss: 0.005200 | Commit Loss: 0.001620 | Perplexity: 1812.593437
2025-09-27 16:24:39,335 Stage: Train 0.5 | Epoch: 79 | Iter: 121200 | Total Loss: 0.005915 | Recon Loss: 0.005103 | Commit Loss: 0.001624 | Perplexity: 1811.284606
2025-09-27 16:25:36,455 Stage: Train 0.5 | Epoch: 79 | Iter: 121400 | Total Loss: 0.006021 | Recon Loss: 0.005210 | Commit Loss: 0.001623 | Perplexity: 1808.837813
Trainning Epoch:  24%|██▍       | 80/330 [9:38:19<30:08:22, 434.01s/it]2025-09-27 16:26:33,739 Stage: Train 0.5 | Epoch: 80 | Iter: 121600 | Total Loss: 0.005988 | Recon Loss: 0.005176 | Commit Loss: 0.001624 | Perplexity: 1806.175810
2025-09-27 16:27:30,941 Stage: Train 0.5 | Epoch: 80 | Iter: 121800 | Total Loss: 0.006049 | Recon Loss: 0.005241 | Commit Loss: 0.001616 | Perplexity: 1805.866766
2025-09-27 16:28:27,732 Stage: Train 0.5 | Epoch: 80 | Iter: 122000 | Total Loss: 0.005915 | Recon Loss: 0.005103 | Commit Loss: 0.001624 | Perplexity: 1811.146608
2025-09-27 16:29:24,880 Stage: Train 0.5 | Epoch: 80 | Iter: 122200 | Total Loss: 0.005960 | Recon Loss: 0.005149 | Commit Loss: 0.001622 | Perplexity: 1804.101233
2025-09-27 16:30:22,242 Stage: Train 0.5 | Epoch: 80 | Iter: 122400 | Total Loss: 0.006011 | Recon Loss: 0.005201 | Commit Loss: 0.001622 | Perplexity: 1808.624023
2025-09-27 16:31:19,516 Stage: Train 0.5 | Epoch: 80 | Iter: 122600 | Total Loss: 0.006053 | Recon Loss: 0.005237 | Commit Loss: 0.001633 | Perplexity: 1809.028776
2025-09-27 16:32:16,803 Stage: Train 0.5 | Epoch: 80 | Iter: 122800 | Total Loss: 0.005980 | Recon Loss: 0.005166 | Commit Loss: 0.001628 | Perplexity: 1809.358467
2025-09-27 16:33:13,856 Stage: Train 0.5 | Epoch: 80 | Iter: 123000 | Total Loss: 0.006032 | Recon Loss: 0.005217 | Commit Loss: 0.001631 | Perplexity: 1808.163657
Trainning Epoch:  25%|██▍       | 81/330 [9:45:33<30:01:33, 434.11s/it]2025-09-27 16:34:11,080 Stage: Train 0.5 | Epoch: 81 | Iter: 123200 | Total Loss: 0.005845 | Recon Loss: 0.005037 | Commit Loss: 0.001618 | Perplexity: 1805.781227
2025-09-27 16:35:07,831 Stage: Train 0.5 | Epoch: 81 | Iter: 123400 | Total Loss: 0.005956 | Recon Loss: 0.005147 | Commit Loss: 0.001618 | Perplexity: 1809.489138
2025-09-27 16:36:05,065 Stage: Train 0.5 | Epoch: 81 | Iter: 123600 | Total Loss: 0.006051 | Recon Loss: 0.005240 | Commit Loss: 0.001622 | Perplexity: 1810.085255
2025-09-27 16:37:02,177 Stage: Train 0.5 | Epoch: 81 | Iter: 123800 | Total Loss: 0.005882 | Recon Loss: 0.005063 | Commit Loss: 0.001638 | Perplexity: 1810.664943
2025-09-27 16:37:59,339 Stage: Train 0.5 | Epoch: 81 | Iter: 124000 | Total Loss: 0.005974 | Recon Loss: 0.005157 | Commit Loss: 0.001634 | Perplexity: 1815.075468
2025-09-27 16:38:56,466 Stage: Train 0.5 | Epoch: 81 | Iter: 124200 | Total Loss: 0.005966 | Recon Loss: 0.005151 | Commit Loss: 0.001630 | Perplexity: 1810.236554
2025-09-27 16:39:53,584 Stage: Train 0.5 | Epoch: 81 | Iter: 124400 | Total Loss: 0.005931 | Recon Loss: 0.005116 | Commit Loss: 0.001629 | Perplexity: 1809.622327
Trainning Epoch:  25%|██▍       | 82/330 [9:52:47<29:53:21, 433.88s/it]2025-09-27 16:40:50,564 Stage: Train 0.5 | Epoch: 82 | Iter: 124600 | Total Loss: 0.005972 | Recon Loss: 0.005152 | Commit Loss: 0.001639 | Perplexity: 1805.237916
2025-09-27 16:41:47,761 Stage: Train 0.5 | Epoch: 82 | Iter: 124800 | Total Loss: 0.005982 | Recon Loss: 0.005167 | Commit Loss: 0.001629 | Perplexity: 1809.951636
2025-09-27 16:42:45,029 Stage: Train 0.5 | Epoch: 82 | Iter: 125000 | Total Loss: 0.005923 | Recon Loss: 0.005111 | Commit Loss: 0.001625 | Perplexity: 1808.216710
2025-09-27 16:43:42,212 Stage: Train 0.5 | Epoch: 82 | Iter: 125200 | Total Loss: 0.005898 | Recon Loss: 0.005081 | Commit Loss: 0.001634 | Perplexity: 1810.864042
2025-09-27 16:44:39,262 Stage: Train 0.5 | Epoch: 82 | Iter: 125400 | Total Loss: 0.005963 | Recon Loss: 0.005146 | Commit Loss: 0.001633 | Perplexity: 1804.440780
2025-09-27 16:45:36,384 Stage: Train 0.5 | Epoch: 82 | Iter: 125600 | Total Loss: 0.005947 | Recon Loss: 0.005137 | Commit Loss: 0.001619 | Perplexity: 1805.087223
2025-09-27 16:46:33,247 Stage: Train 0.5 | Epoch: 82 | Iter: 125800 | Total Loss: 0.005941 | Recon Loss: 0.005122 | Commit Loss: 0.001638 | Perplexity: 1813.899510
2025-09-27 16:47:30,511 Stage: Train 0.5 | Epoch: 82 | Iter: 126000 | Total Loss: 0.005977 | Recon Loss: 0.005158 | Commit Loss: 0.001638 | Perplexity: 1810.992761
Trainning Epoch:  25%|██▌       | 83/330 [10:00:01<29:46:26, 433.96s/it]2025-09-27 16:48:27,784 Stage: Train 0.5 | Epoch: 83 | Iter: 126200 | Total Loss: 0.005891 | Recon Loss: 0.005082 | Commit Loss: 0.001617 | Perplexity: 1808.893460
2025-09-27 16:49:24,823 Stage: Train 0.5 | Epoch: 83 | Iter: 126400 | Total Loss: 0.005964 | Recon Loss: 0.005148 | Commit Loss: 0.001631 | Perplexity: 1807.095861
2025-09-27 16:50:22,061 Stage: Train 0.5 | Epoch: 83 | Iter: 126600 | Total Loss: 0.005935 | Recon Loss: 0.005122 | Commit Loss: 0.001626 | Perplexity: 1806.465656
2025-09-27 16:51:19,210 Stage: Train 0.5 | Epoch: 83 | Iter: 126800 | Total Loss: 0.005866 | Recon Loss: 0.005047 | Commit Loss: 0.001636 | Perplexity: 1808.500917
2025-09-27 16:52:16,291 Stage: Train 0.5 | Epoch: 83 | Iter: 127000 | Total Loss: 0.006012 | Recon Loss: 0.005194 | Commit Loss: 0.001636 | Perplexity: 1814.694274
2025-09-27 16:53:12,993 Stage: Train 0.5 | Epoch: 83 | Iter: 127200 | Total Loss: 0.005874 | Recon Loss: 0.005054 | Commit Loss: 0.001640 | Perplexity: 1811.211215
2025-09-27 16:54:10,258 Stage: Train 0.5 | Epoch: 83 | Iter: 127400 | Total Loss: 0.005959 | Recon Loss: 0.005137 | Commit Loss: 0.001644 | Perplexity: 1814.528483
Trainning Epoch:  25%|██▌       | 84/330 [10:07:14<29:38:40, 433.82s/it]2025-09-27 16:55:07,332 Stage: Train 0.5 | Epoch: 84 | Iter: 127600 | Total Loss: 0.005893 | Recon Loss: 0.005078 | Commit Loss: 0.001630 | Perplexity: 1804.628150
2025-09-27 16:56:04,443 Stage: Train 0.5 | Epoch: 84 | Iter: 127800 | Total Loss: 0.005926 | Recon Loss: 0.005108 | Commit Loss: 0.001636 | Perplexity: 1808.762228
2025-09-27 16:57:01,720 Stage: Train 0.5 | Epoch: 84 | Iter: 128000 | Total Loss: 0.005863 | Recon Loss: 0.005044 | Commit Loss: 0.001638 | Perplexity: 1813.235819
2025-09-27 16:57:59,038 Stage: Train 0.5 | Epoch: 84 | Iter: 128200 | Total Loss: 0.005925 | Recon Loss: 0.005112 | Commit Loss: 0.001626 | Perplexity: 1807.358746
2025-09-27 16:58:55,869 Stage: Train 0.5 | Epoch: 84 | Iter: 128400 | Total Loss: 0.005953 | Recon Loss: 0.005133 | Commit Loss: 0.001639 | Perplexity: 1807.742808
2025-09-27 16:59:53,211 Stage: Train 0.5 | Epoch: 84 | Iter: 128600 | Total Loss: 0.005875 | Recon Loss: 0.005060 | Commit Loss: 0.001631 | Perplexity: 1809.528468
2025-09-27 17:00:50,529 Stage: Train 0.5 | Epoch: 84 | Iter: 128800 | Total Loss: 0.005907 | Recon Loss: 0.005086 | Commit Loss: 0.001641 | Perplexity: 1810.076462
2025-09-27 17:01:46,092 Stage: Train 0.5 | Epoch: 84 | Iter: 129000 | Total Loss: 0.005938 | Recon Loss: 0.005114 | Commit Loss: 0.001647 | Perplexity: 1811.693360
Trainning Epoch:  26%|██▌       | 85/330 [10:14:27<29:30:25, 433.57s/it]2025-09-27 17:02:43,445 Stage: Train 0.5 | Epoch: 85 | Iter: 129200 | Total Loss: 0.005867 | Recon Loss: 0.005054 | Commit Loss: 0.001627 | Perplexity: 1805.753054
2025-09-27 17:03:40,751 Stage: Train 0.5 | Epoch: 85 | Iter: 129400 | Total Loss: 0.005911 | Recon Loss: 0.005094 | Commit Loss: 0.001634 | Perplexity: 1810.761670
2025-09-27 17:04:37,457 Stage: Train 0.5 | Epoch: 85 | Iter: 129600 | Total Loss: 0.005909 | Recon Loss: 0.005087 | Commit Loss: 0.001643 | Perplexity: 1809.395692
2025-09-27 17:05:34,677 Stage: Train 0.5 | Epoch: 85 | Iter: 129800 | Total Loss: 0.005969 | Recon Loss: 0.005147 | Commit Loss: 0.001644 | Perplexity: 1813.231006
2025-09-27 17:06:31,962 Stage: Train 0.5 | Epoch: 85 | Iter: 130000 | Total Loss: 0.005854 | Recon Loss: 0.005037 | Commit Loss: 0.001633 | Perplexity: 1808.027311
2025-09-27 17:07:29,247 Stage: Train 0.5 | Epoch: 85 | Iter: 130200 | Total Loss: 0.005874 | Recon Loss: 0.005056 | Commit Loss: 0.001636 | Perplexity: 1808.568239
2025-09-27 17:08:26,330 Stage: Train 0.5 | Epoch: 85 | Iter: 130400 | Total Loss: 0.005933 | Recon Loss: 0.005108 | Commit Loss: 0.001650 | Perplexity: 1812.938221
2025-09-27 17:09:23,520 Stage: Train 0.5 | Epoch: 85 | Iter: 130600 | Total Loss: 0.005824 | Recon Loss: 0.005003 | Commit Loss: 0.001642 | Perplexity: 1810.983406
Trainning Epoch:  26%|██▌       | 86/330 [10:21:41<29:24:02, 433.78s/it]2025-09-27 17:10:20,478 Stage: Train 0.5 | Epoch: 86 | Iter: 130800 | Total Loss: 0.005855 | Recon Loss: 0.005034 | Commit Loss: 0.001642 | Perplexity: 1811.169665
2025-09-27 17:11:17,814 Stage: Train 0.5 | Epoch: 86 | Iter: 131000 | Total Loss: 0.005914 | Recon Loss: 0.005093 | Commit Loss: 0.001641 | Perplexity: 1808.658222
2025-09-27 17:12:15,078 Stage: Train 0.5 | Epoch: 86 | Iter: 131200 | Total Loss: 0.005894 | Recon Loss: 0.005077 | Commit Loss: 0.001633 | Perplexity: 1808.108338
2025-09-27 17:13:12,322 Stage: Train 0.5 | Epoch: 86 | Iter: 131400 | Total Loss: 0.005995 | Recon Loss: 0.005179 | Commit Loss: 0.001633 | Perplexity: 1806.532768
2025-09-27 17:14:09,549 Stage: Train 0.5 | Epoch: 86 | Iter: 131600 | Total Loss: 0.005835 | Recon Loss: 0.005018 | Commit Loss: 0.001635 | Perplexity: 1808.196517
2025-09-27 17:15:06,877 Stage: Train 0.5 | Epoch: 86 | Iter: 131800 | Total Loss: 0.005940 | Recon Loss: 0.005120 | Commit Loss: 0.001639 | Perplexity: 1807.005670
2025-09-27 17:16:04,141 Stage: Train 0.5 | Epoch: 86 | Iter: 132000 | Total Loss: 0.005868 | Recon Loss: 0.005046 | Commit Loss: 0.001643 | Perplexity: 1809.783501
Trainning Epoch:  26%|██▋       | 87/330 [10:28:56<29:17:32, 433.96s/it]2025-09-27 17:17:01,196 Stage: Train 0.5 | Epoch: 87 | Iter: 132200 | Total Loss: 0.005913 | Recon Loss: 0.005086 | Commit Loss: 0.001655 | Perplexity: 1811.712997
2025-09-27 17:17:58,599 Stage: Train 0.5 | Epoch: 87 | Iter: 132400 | Total Loss: 0.005907 | Recon Loss: 0.005085 | Commit Loss: 0.001645 | Perplexity: 1811.012422
2025-09-27 17:18:55,796 Stage: Train 0.5 | Epoch: 87 | Iter: 132600 | Total Loss: 0.005911 | Recon Loss: 0.005091 | Commit Loss: 0.001640 | Perplexity: 1807.375513
2025-09-27 17:19:53,040 Stage: Train 0.5 | Epoch: 87 | Iter: 132800 | Total Loss: 0.005843 | Recon Loss: 0.005020 | Commit Loss: 0.001646 | Perplexity: 1811.246584
2025-09-27 17:20:50,357 Stage: Train 0.5 | Epoch: 87 | Iter: 133000 | Total Loss: 0.005906 | Recon Loss: 0.005085 | Commit Loss: 0.001641 | Perplexity: 1807.201521
2025-09-27 17:21:47,497 Stage: Train 0.5 | Epoch: 87 | Iter: 133200 | Total Loss: 0.005811 | Recon Loss: 0.004991 | Commit Loss: 0.001640 | Perplexity: 1810.650771
2025-09-27 17:22:44,532 Stage: Train 0.5 | Epoch: 87 | Iter: 133400 | Total Loss: 0.005898 | Recon Loss: 0.005076 | Commit Loss: 0.001643 | Perplexity: 1810.279924
2025-09-27 17:23:41,810 Stage: Train 0.5 | Epoch: 87 | Iter: 133600 | Total Loss: 0.005826 | Recon Loss: 0.005014 | Commit Loss: 0.001625 | Perplexity: 1802.666223
Trainning Epoch:  27%|██▋       | 88/330 [10:36:11<29:11:21, 434.22s/it]2025-09-27 17:24:39,040 Stage: Train 0.5 | Epoch: 88 | Iter: 133800 | Total Loss: 0.005925 | Recon Loss: 0.005100 | Commit Loss: 0.001650 | Perplexity: 1808.834718
2025-09-27 17:25:36,149 Stage: Train 0.5 | Epoch: 88 | Iter: 134000 | Total Loss: 0.005809 | Recon Loss: 0.004985 | Commit Loss: 0.001647 | Perplexity: 1813.872457
2025-09-27 17:26:33,274 Stage: Train 0.5 | Epoch: 88 | Iter: 134200 | Total Loss: 0.005880 | Recon Loss: 0.005063 | Commit Loss: 0.001635 | Perplexity: 1806.966235
2025-09-27 17:27:30,493 Stage: Train 0.5 | Epoch: 88 | Iter: 134400 | Total Loss: 0.005835 | Recon Loss: 0.005012 | Commit Loss: 0.001646 | Perplexity: 1809.785456
2025-09-27 17:28:27,135 Stage: Train 0.5 | Epoch: 88 | Iter: 134600 | Total Loss: 0.005901 | Recon Loss: 0.005080 | Commit Loss: 0.001642 | Perplexity: 1805.027568
2025-09-27 17:29:24,238 Stage: Train 0.5 | Epoch: 88 | Iter: 134800 | Total Loss: 0.005828 | Recon Loss: 0.005000 | Commit Loss: 0.001656 | Perplexity: 1811.579597
2025-09-27 17:30:21,518 Stage: Train 0.5 | Epoch: 88 | Iter: 135000 | Total Loss: 0.005990 | Recon Loss: 0.005165 | Commit Loss: 0.001649 | Perplexity: 1808.325356
Trainning Epoch:  27%|██▋       | 89/330 [10:43:24<29:03:35, 434.09s/it]2025-09-27 17:31:18,986 Stage: Train 0.5 | Epoch: 89 | Iter: 135200 | Total Loss: 0.005823 | Recon Loss: 0.004999 | Commit Loss: 0.001648 | Perplexity: 1806.312883
2025-09-27 17:32:16,237 Stage: Train 0.5 | Epoch: 89 | Iter: 135400 | Total Loss: 0.005814 | Recon Loss: 0.004990 | Commit Loss: 0.001646 | Perplexity: 1810.159971
2025-09-27 17:33:13,650 Stage: Train 0.5 | Epoch: 89 | Iter: 135600 | Total Loss: 0.005864 | Recon Loss: 0.005041 | Commit Loss: 0.001646 | Perplexity: 1811.089332
2025-09-27 17:34:10,991 Stage: Train 0.5 | Epoch: 89 | Iter: 135800 | Total Loss: 0.005879 | Recon Loss: 0.005058 | Commit Loss: 0.001641 | Perplexity: 1810.749379
2025-09-27 17:35:07,661 Stage: Train 0.5 | Epoch: 89 | Iter: 136000 | Total Loss: 0.005876 | Recon Loss: 0.005053 | Commit Loss: 0.001646 | Perplexity: 1809.597418
2025-09-27 17:36:04,957 Stage: Train 0.5 | Epoch: 89 | Iter: 136200 | Total Loss: 0.005909 | Recon Loss: 0.005083 | Commit Loss: 0.001651 | Perplexity: 1808.474100
2025-09-27 17:37:02,109 Stage: Train 0.5 | Epoch: 89 | Iter: 136400 | Total Loss: 0.005828 | Recon Loss: 0.005005 | Commit Loss: 0.001647 | Perplexity: 1807.411070
2025-09-27 17:37:59,348 Stage: Train 0.5 | Epoch: 89 | Iter: 136600 | Total Loss: 0.005809 | Recon Loss: 0.004986 | Commit Loss: 0.001646 | Perplexity: 1809.902682
Trainning Epoch:  27%|██▋       | 90/330 [10:50:39<28:56:53, 434.22s/it]2025-09-27 17:38:56,659 Stage: Train 0.5 | Epoch: 90 | Iter: 136800 | Total Loss: 0.005855 | Recon Loss: 0.005033 | Commit Loss: 0.001643 | Perplexity: 1809.579590
2025-09-27 17:39:53,965 Stage: Train 0.5 | Epoch: 90 | Iter: 137000 | Total Loss: 0.005843 | Recon Loss: 0.005018 | Commit Loss: 0.001650 | Perplexity: 1807.548812
2025-09-27 17:40:50,784 Stage: Train 0.5 | Epoch: 90 | Iter: 137200 | Total Loss: 0.005847 | Recon Loss: 0.005020 | Commit Loss: 0.001653 | Perplexity: 1809.086655
2025-09-27 17:41:47,981 Stage: Train 0.5 | Epoch: 90 | Iter: 137400 | Total Loss: 0.005744 | Recon Loss: 0.004921 | Commit Loss: 0.001646 | Perplexity: 1807.020366
2025-09-27 17:42:45,064 Stage: Train 0.5 | Epoch: 90 | Iter: 137600 | Total Loss: 0.005900 | Recon Loss: 0.005073 | Commit Loss: 0.001655 | Perplexity: 1810.620021
2025-09-27 17:43:42,365 Stage: Train 0.5 | Epoch: 90 | Iter: 137800 | Total Loss: 0.005809 | Recon Loss: 0.004981 | Commit Loss: 0.001656 | Perplexity: 1809.732289
2025-09-27 17:44:39,602 Stage: Train 0.5 | Epoch: 90 | Iter: 138000 | Total Loss: 0.005815 | Recon Loss: 0.004986 | Commit Loss: 0.001657 | Perplexity: 1809.491660
2025-09-27 17:45:36,627 Stage: Train 0.5 | Epoch: 90 | Iter: 138200 | Total Loss: 0.005880 | Recon Loss: 0.005052 | Commit Loss: 0.001656 | Perplexity: 1807.040881
Trainning Epoch:  28%|██▊       | 91/330 [10:57:53<28:49:33, 434.20s/it]2025-09-27 17:46:33,703 Stage: Train 0.5 | Epoch: 91 | Iter: 138400 | Total Loss: 0.005770 | Recon Loss: 0.004950 | Commit Loss: 0.001639 | Perplexity: 1804.574223
2025-09-27 17:47:31,053 Stage: Train 0.5 | Epoch: 91 | Iter: 138600 | Total Loss: 0.005756 | Recon Loss: 0.004928 | Commit Loss: 0.001655 | Perplexity: 1808.588092
2025-09-27 17:48:28,367 Stage: Train 0.5 | Epoch: 91 | Iter: 138800 | Total Loss: 0.005912 | Recon Loss: 0.005078 | Commit Loss: 0.001668 | Perplexity: 1807.003861
2025-09-27 17:49:25,673 Stage: Train 0.5 | Epoch: 91 | Iter: 139000 | Total Loss: 0.005782 | Recon Loss: 0.004953 | Commit Loss: 0.001658 | Perplexity: 1809.148934
2025-09-27 17:50:22,983 Stage: Train 0.5 | Epoch: 91 | Iter: 139200 | Total Loss: 0.005946 | Recon Loss: 0.005117 | Commit Loss: 0.001659 | Perplexity: 1812.055016
2025-09-27 17:51:20,260 Stage: Train 0.5 | Epoch: 91 | Iter: 139400 | Total Loss: 0.005761 | Recon Loss: 0.004936 | Commit Loss: 0.001648 | Perplexity: 1807.975331
2025-09-27 17:52:17,138 Stage: Train 0.5 | Epoch: 91 | Iter: 139600 | Total Loss: 0.005790 | Recon Loss: 0.004960 | Commit Loss: 0.001659 | Perplexity: 1806.967612
Trainning Epoch:  28%|██▊       | 92/330 [11:05:08<28:42:41, 434.29s/it]2025-09-27 17:53:14,513 Stage: Train 0.5 | Epoch: 92 | Iter: 139800 | Total Loss: 0.005838 | Recon Loss: 0.005008 | Commit Loss: 0.001661 | Perplexity: 1809.923920
2025-09-27 17:54:11,457 Stage: Train 0.5 | Epoch: 92 | Iter: 140000 | Total Loss: 0.005845 | Recon Loss: 0.005016 | Commit Loss: 0.001657 | Perplexity: 1813.515485
2025-09-27 17:54:11,458 Saving model at iteration 140000
2025-09-27 17:54:11,709 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000
2025-09-27 17:54:12,158 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/model.safetensors
2025-09-27 17:54:12,632 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/optimizer.bin
2025-09-27 17:54:12,632 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/scheduler.bin
2025-09-27 17:54:12,632 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/sampler.bin
2025-09-27 17:54:12,633 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_93_step_140000/random_states_0.pkl
2025-09-27 17:55:10,070 Stage: Train 0.5 | Epoch: 92 | Iter: 140200 | Total Loss: 0.005700 | Recon Loss: 0.004875 | Commit Loss: 0.001650 | Perplexity: 1806.697764
2025-09-27 17:56:07,248 Stage: Train 0.5 | Epoch: 92 | Iter: 140400 | Total Loss: 0.005834 | Recon Loss: 0.005004 | Commit Loss: 0.001660 | Perplexity: 1807.636179
2025-09-27 17:57:04,267 Stage: Train 0.5 | Epoch: 92 | Iter: 140600 | Total Loss: 0.005780 | Recon Loss: 0.004950 | Commit Loss: 0.001660 | Perplexity: 1806.464960
2025-09-27 17:58:01,689 Stage: Train 0.5 | Epoch: 92 | Iter: 140800 | Total Loss: 0.005943 | Recon Loss: 0.005105 | Commit Loss: 0.001677 | Perplexity: 1808.339716
2025-09-27 17:58:58,540 Stage: Train 0.5 | Epoch: 92 | Iter: 141000 | Total Loss: 0.005737 | Recon Loss: 0.004913 | Commit Loss: 0.001649 | Perplexity: 1808.175562
2025-09-27 17:59:55,762 Stage: Train 0.5 | Epoch: 92 | Iter: 141200 | Total Loss: 0.005772 | Recon Loss: 0.004951 | Commit Loss: 0.001642 | Perplexity: 1803.113743
Trainning Epoch:  28%|██▊       | 93/330 [11:12:23<28:36:53, 434.66s/it]2025-09-27 18:00:53,155 Stage: Train 0.5 | Epoch: 93 | Iter: 141400 | Total Loss: 0.005849 | Recon Loss: 0.005020 | Commit Loss: 0.001658 | Perplexity: 1808.109075
2025-09-27 18:01:50,325 Stage: Train 0.5 | Epoch: 93 | Iter: 141600 | Total Loss: 0.005778 | Recon Loss: 0.004951 | Commit Loss: 0.001654 | Perplexity: 1809.007474
2025-09-27 18:02:47,549 Stage: Train 0.5 | Epoch: 93 | Iter: 141800 | Total Loss: 0.005843 | Recon Loss: 0.005012 | Commit Loss: 0.001661 | Perplexity: 1807.609347
2025-09-27 18:03:44,836 Stage: Train 0.5 | Epoch: 93 | Iter: 142000 | Total Loss: 0.005785 | Recon Loss: 0.004961 | Commit Loss: 0.001649 | Perplexity: 1804.462382
2025-09-27 18:04:41,563 Stage: Train 0.5 | Epoch: 93 | Iter: 142200 | Total Loss: 0.005809 | Recon Loss: 0.004987 | Commit Loss: 0.001645 | Perplexity: 1807.719346
2025-09-27 18:05:38,753 Stage: Train 0.5 | Epoch: 93 | Iter: 142400 | Total Loss: 0.005794 | Recon Loss: 0.004967 | Commit Loss: 0.001654 | Perplexity: 1806.878801
2025-09-27 18:06:36,071 Stage: Train 0.5 | Epoch: 93 | Iter: 142600 | Total Loss: 0.005810 | Recon Loss: 0.004974 | Commit Loss: 0.001670 | Perplexity: 1810.142877
Trainning Epoch:  28%|██▊       | 94/330 [11:19:38<28:29:23, 434.59s/it]2025-09-27 18:07:33,585 Stage: Train 0.5 | Epoch: 94 | Iter: 142800 | Total Loss: 0.005921 | Recon Loss: 0.005093 | Commit Loss: 0.001656 | Perplexity: 1809.356807
2025-09-27 18:08:30,903 Stage: Train 0.5 | Epoch: 94 | Iter: 143000 | Total Loss: 0.005722 | Recon Loss: 0.004892 | Commit Loss: 0.001660 | Perplexity: 1810.606136
2025-09-27 18:09:28,169 Stage: Train 0.5 | Epoch: 94 | Iter: 143200 | Total Loss: 0.005763 | Recon Loss: 0.004937 | Commit Loss: 0.001652 | Perplexity: 1808.611391
2025-09-27 18:10:25,183 Stage: Train 0.5 | Epoch: 94 | Iter: 143400 | Total Loss: 0.005910 | Recon Loss: 0.005079 | Commit Loss: 0.001661 | Perplexity: 1811.862045
2025-09-27 18:11:22,504 Stage: Train 0.5 | Epoch: 94 | Iter: 143600 | Total Loss: 0.005757 | Recon Loss: 0.004933 | Commit Loss: 0.001650 | Perplexity: 1802.797222
2025-09-27 18:12:19,954 Stage: Train 0.5 | Epoch: 94 | Iter: 143800 | Total Loss: 0.005786 | Recon Loss: 0.004959 | Commit Loss: 0.001654 | Perplexity: 1803.606315
2025-09-27 18:13:17,271 Stage: Train 0.5 | Epoch: 94 | Iter: 144000 | Total Loss: 0.005844 | Recon Loss: 0.005009 | Commit Loss: 0.001669 | Perplexity: 1811.249512
2025-09-27 18:14:14,537 Stage: Train 0.5 | Epoch: 94 | Iter: 144200 | Total Loss: 0.005766 | Recon Loss: 0.004933 | Commit Loss: 0.001665 | Perplexity: 1809.105889
Trainning Epoch:  29%|██▉       | 95/330 [11:26:53<28:22:53, 434.78s/it]2025-09-27 18:15:11,947 Stage: Train 0.5 | Epoch: 95 | Iter: 144400 | Total Loss: 0.005804 | Recon Loss: 0.004980 | Commit Loss: 0.001650 | Perplexity: 1804.401855
2025-09-27 18:16:08,777 Stage: Train 0.5 | Epoch: 95 | Iter: 144600 | Total Loss: 0.005731 | Recon Loss: 0.004902 | Commit Loss: 0.001657 | Perplexity: 1805.562100
2025-09-27 18:17:06,063 Stage: Train 0.5 | Epoch: 95 | Iter: 144800 | Total Loss: 0.005815 | Recon Loss: 0.004983 | Commit Loss: 0.001663 | Perplexity: 1812.996541
2025-09-27 18:18:03,365 Stage: Train 0.5 | Epoch: 95 | Iter: 145000 | Total Loss: 0.005781 | Recon Loss: 0.004945 | Commit Loss: 0.001670 | Perplexity: 1805.588033
2025-09-27 18:19:00,497 Stage: Train 0.5 | Epoch: 95 | Iter: 145200 | Total Loss: 0.005725 | Recon Loss: 0.004892 | Commit Loss: 0.001666 | Perplexity: 1809.322585
2025-09-27 18:19:57,687 Stage: Train 0.5 | Epoch: 95 | Iter: 145400 | Total Loss: 0.005780 | Recon Loss: 0.004941 | Commit Loss: 0.001676 | Perplexity: 1809.819979
2025-09-27 18:20:53,860 Stage: Train 0.5 | Epoch: 95 | Iter: 145600 | Total Loss: 0.005755 | Recon Loss: 0.004926 | Commit Loss: 0.001659 | Perplexity: 1808.268053
2025-09-27 18:21:50,958 Stage: Train 0.5 | Epoch: 95 | Iter: 145800 | Total Loss: 0.005785 | Recon Loss: 0.004956 | Commit Loss: 0.001658 | Perplexity: 1807.976647
Trainning Epoch:  29%|██▉       | 96/330 [11:34:06<28:13:43, 434.29s/it]2025-09-27 18:22:47,700 Stage: Train 0.5 | Epoch: 96 | Iter: 146000 | Total Loss: 0.005748 | Recon Loss: 0.004914 | Commit Loss: 0.001667 | Perplexity: 1811.803826
2025-09-27 18:23:44,958 Stage: Train 0.5 | Epoch: 96 | Iter: 146200 | Total Loss: 0.005758 | Recon Loss: 0.004921 | Commit Loss: 0.001672 | Perplexity: 1811.897812
2025-09-27 18:24:42,142 Stage: Train 0.5 | Epoch: 96 | Iter: 146400 | Total Loss: 0.005746 | Recon Loss: 0.004917 | Commit Loss: 0.001657 | Perplexity: 1804.441320
2025-09-27 18:25:39,290 Stage: Train 0.5 | Epoch: 96 | Iter: 146600 | Total Loss: 0.005703 | Recon Loss: 0.004870 | Commit Loss: 0.001666 | Perplexity: 1810.643441
2025-09-27 18:26:36,420 Stage: Train 0.5 | Epoch: 96 | Iter: 146800 | Total Loss: 0.005693 | Recon Loss: 0.004862 | Commit Loss: 0.001661 | Perplexity: 1806.780897
2025-09-27 18:27:33,727 Stage: Train 0.5 | Epoch: 96 | Iter: 147000 | Total Loss: 0.005805 | Recon Loss: 0.004972 | Commit Loss: 0.001665 | Perplexity: 1806.371880
2025-09-27 18:28:30,537 Stage: Train 0.5 | Epoch: 96 | Iter: 147200 | Total Loss: 0.005817 | Recon Loss: 0.004978 | Commit Loss: 0.001678 | Perplexity: 1808.279720
Trainning Epoch:  29%|██▉       | 97/330 [11:41:20<28:05:55, 434.14s/it]2025-09-27 18:29:28,105 Stage: Train 0.5 | Epoch: 97 | Iter: 147400 | Total Loss: 0.005788 | Recon Loss: 0.004953 | Commit Loss: 0.001671 | Perplexity: 1806.157719
2025-09-27 18:30:25,353 Stage: Train 0.5 | Epoch: 97 | Iter: 147600 | Total Loss: 0.005754 | Recon Loss: 0.004919 | Commit Loss: 0.001669 | Perplexity: 1809.518560
2025-09-27 18:31:22,502 Stage: Train 0.5 | Epoch: 97 | Iter: 147800 | Total Loss: 0.005743 | Recon Loss: 0.004912 | Commit Loss: 0.001662 | Perplexity: 1807.703967
2025-09-27 18:32:19,693 Stage: Train 0.5 | Epoch: 97 | Iter: 148000 | Total Loss: 0.005737 | Recon Loss: 0.004906 | Commit Loss: 0.001661 | Perplexity: 1806.507465
2025-09-27 18:33:16,997 Stage: Train 0.5 | Epoch: 97 | Iter: 148200 | Total Loss: 0.005770 | Recon Loss: 0.004936 | Commit Loss: 0.001668 | Perplexity: 1807.804871
2025-09-27 18:34:13,631 Stage: Train 0.5 | Epoch: 97 | Iter: 148400 | Total Loss: 0.005764 | Recon Loss: 0.004929 | Commit Loss: 0.001669 | Perplexity: 1810.136855
2025-09-27 18:35:10,793 Stage: Train 0.5 | Epoch: 97 | Iter: 148600 | Total Loss: 0.005697 | Recon Loss: 0.004858 | Commit Loss: 0.001678 | Perplexity: 1811.116107
2025-09-27 18:36:08,080 Stage: Train 0.5 | Epoch: 97 | Iter: 148800 | Total Loss: 0.005795 | Recon Loss: 0.004960 | Commit Loss: 0.001671 | Perplexity: 1808.647401
Trainning Epoch:  30%|██▉       | 98/330 [11:48:34<27:58:58, 434.22s/it]2025-09-27 18:37:05,729 Stage: Train 0.5 | Epoch: 98 | Iter: 149000 | Total Loss: 0.005713 | Recon Loss: 0.004882 | Commit Loss: 0.001663 | Perplexity: 1805.505723
2025-09-27 18:38:02,936 Stage: Train 0.5 | Epoch: 98 | Iter: 149200 | Total Loss: 0.005717 | Recon Loss: 0.004887 | Commit Loss: 0.001659 | Perplexity: 1806.549976
2025-09-27 18:39:00,094 Stage: Train 0.5 | Epoch: 98 | Iter: 149400 | Total Loss: 0.005798 | Recon Loss: 0.004960 | Commit Loss: 0.001675 | Perplexity: 1810.118085
2025-09-27 18:39:57,363 Stage: Train 0.5 | Epoch: 98 | Iter: 149600 | Total Loss: 0.005772 | Recon Loss: 0.004928 | Commit Loss: 0.001688 | Perplexity: 1813.036477
2025-09-27 18:40:54,271 Stage: Train 0.5 | Epoch: 98 | Iter: 149800 | Total Loss: 0.005725 | Recon Loss: 0.004887 | Commit Loss: 0.001677 | Perplexity: 1811.451046
2025-09-27 18:41:51,661 Stage: Train 0.5 | Epoch: 98 | Iter: 150000 | Total Loss: 0.005797 | Recon Loss: 0.004963 | Commit Loss: 0.001669 | Perplexity: 1806.106694
2025-09-27 18:42:48,789 Stage: Train 0.5 | Epoch: 98 | Iter: 150200 | Total Loss: 0.005622 | Recon Loss: 0.004794 | Commit Loss: 0.001656 | Perplexity: 1808.903746
Trainning Epoch:  30%|███       | 99/330 [11:55:49<27:52:10, 434.33s/it]2025-09-27 18:43:46,150 Stage: Train 0.5 | Epoch: 99 | Iter: 150400 | Total Loss: 0.005724 | Recon Loss: 0.004892 | Commit Loss: 0.001663 | Perplexity: 1809.303461
2025-09-27 18:44:43,476 Stage: Train 0.5 | Epoch: 99 | Iter: 150600 | Total Loss: 0.005691 | Recon Loss: 0.004862 | Commit Loss: 0.001658 | Perplexity: 1806.069994
2025-09-27 18:45:40,745 Stage: Train 0.5 | Epoch: 99 | Iter: 150800 | Total Loss: 0.005730 | Recon Loss: 0.004889 | Commit Loss: 0.001682 | Perplexity: 1809.469869
2025-09-27 18:46:37,429 Stage: Train 0.5 | Epoch: 99 | Iter: 151000 | Total Loss: 0.005773 | Recon Loss: 0.004935 | Commit Loss: 0.001676 | Perplexity: 1810.818538
2025-09-27 18:47:34,580 Stage: Train 0.5 | Epoch: 99 | Iter: 151200 | Total Loss: 0.005667 | Recon Loss: 0.004831 | Commit Loss: 0.001674 | Perplexity: 1805.510841
2025-09-27 18:48:31,742 Stage: Train 0.5 | Epoch: 99 | Iter: 151400 | Total Loss: 0.005705 | Recon Loss: 0.004866 | Commit Loss: 0.001679 | Perplexity: 1808.140363
2025-09-27 18:49:29,035 Stage: Train 0.5 | Epoch: 99 | Iter: 151600 | Total Loss: 0.005764 | Recon Loss: 0.004926 | Commit Loss: 0.001675 | Perplexity: 1807.892806
2025-09-27 18:50:25,803 Stage: Train 0.5 | Epoch: 99 | Iter: 151800 | Total Loss: 0.005714 | Recon Loss: 0.004875 | Commit Loss: 0.001679 | Perplexity: 1811.719713
Trainning Epoch:  30%|███       | 100/330 [12:03:03<27:44:24, 434.19s/it]2025-09-27 18:51:23,249 Stage: Train 0.5 | Epoch: 100 | Iter: 152000 | Total Loss: 0.005661 | Recon Loss: 0.004830 | Commit Loss: 0.001662 | Perplexity: 1807.567000
2025-09-27 18:52:20,097 Stage: Train 0.5 | Epoch: 100 | Iter: 152200 | Total Loss: 0.005701 | Recon Loss: 0.004866 | Commit Loss: 0.001671 | Perplexity: 1808.286788
2025-09-27 18:53:17,271 Stage: Train 0.5 | Epoch: 100 | Iter: 152400 | Total Loss: 0.005636 | Recon Loss: 0.004799 | Commit Loss: 0.001675 | Perplexity: 1807.295002
2025-09-27 18:54:14,382 Stage: Train 0.5 | Epoch: 100 | Iter: 152600 | Total Loss: 0.005770 | Recon Loss: 0.004933 | Commit Loss: 0.001674 | Perplexity: 1809.173920
2025-09-27 18:55:11,305 Stage: Train 0.5 | Epoch: 100 | Iter: 152800 | Total Loss: 0.005674 | Recon Loss: 0.004832 | Commit Loss: 0.001685 | Perplexity: 1810.153894
2025-09-27 18:56:08,484 Stage: Train 0.5 | Epoch: 100 | Iter: 153000 | Total Loss: 0.005727 | Recon Loss: 0.004885 | Commit Loss: 0.001685 | Perplexity: 1810.838528
2025-09-27 18:57:05,656 Stage: Train 0.5 | Epoch: 100 | Iter: 153200 | Total Loss: 0.005687 | Recon Loss: 0.004850 | Commit Loss: 0.001673 | Perplexity: 1802.648483
2025-09-27 18:58:02,614 Stage: Train 0.5 | Epoch: 100 | Iter: 153400 | Total Loss: 0.005692 | Recon Loss: 0.004849 | Commit Loss: 0.001685 | Perplexity: 1811.139631
Trainning Epoch:  31%|███       | 101/330 [12:10:16<27:36:31, 434.02s/it]2025-09-27 18:58:59,870 Stage: Train 0.5 | Epoch: 101 | Iter: 153600 | Total Loss: 0.005712 | Recon Loss: 0.004874 | Commit Loss: 0.001676 | Perplexity: 1806.162690
2025-09-27 18:59:57,234 Stage: Train 0.5 | Epoch: 101 | Iter: 153800 | Total Loss: 0.005719 | Recon Loss: 0.004876 | Commit Loss: 0.001686 | Perplexity: 1813.395616
2025-09-27 19:00:54,582 Stage: Train 0.5 | Epoch: 101 | Iter: 154000 | Total Loss: 0.005717 | Recon Loss: 0.004878 | Commit Loss: 0.001677 | Perplexity: 1809.236716
2025-09-27 19:01:51,815 Stage: Train 0.5 | Epoch: 101 | Iter: 154200 | Total Loss: 0.005676 | Recon Loss: 0.004833 | Commit Loss: 0.001686 | Perplexity: 1811.764233
2025-09-27 19:02:49,060 Stage: Train 0.5 | Epoch: 101 | Iter: 154400 | Total Loss: 0.005683 | Recon Loss: 0.004845 | Commit Loss: 0.001675 | Perplexity: 1808.798621
2025-09-27 19:03:46,320 Stage: Train 0.5 | Epoch: 101 | Iter: 154600 | Total Loss: 0.005721 | Recon Loss: 0.004884 | Commit Loss: 0.001673 | Perplexity: 1807.310687
2025-09-27 19:04:43,097 Stage: Train 0.5 | Epoch: 101 | Iter: 154800 | Total Loss: 0.005718 | Recon Loss: 0.004882 | Commit Loss: 0.001672 | Perplexity: 1806.148108
Trainning Epoch:  31%|███       | 102/330 [12:17:31<27:29:47, 434.16s/it]2025-09-27 19:05:40,327 Stage: Train 0.5 | Epoch: 102 | Iter: 155000 | Total Loss: 0.005670 | Recon Loss: 0.004830 | Commit Loss: 0.001679 | Perplexity: 1807.754383
2025-09-27 19:06:37,648 Stage: Train 0.5 | Epoch: 102 | Iter: 155200 | Total Loss: 0.005697 | Recon Loss: 0.004864 | Commit Loss: 0.001668 | Perplexity: 1807.482256
2025-09-27 19:07:34,737 Stage: Train 0.5 | Epoch: 102 | Iter: 155400 | Total Loss: 0.005703 | Recon Loss: 0.004863 | Commit Loss: 0.001680 | Perplexity: 1808.429252
2025-09-27 19:08:31,985 Stage: Train 0.5 | Epoch: 102 | Iter: 155600 | Total Loss: 0.005746 | Recon Loss: 0.004905 | Commit Loss: 0.001682 | Perplexity: 1807.056531
2025-09-27 19:09:29,198 Stage: Train 0.5 | Epoch: 102 | Iter: 155800 | Total Loss: 0.005626 | Recon Loss: 0.004788 | Commit Loss: 0.001675 | Perplexity: 1807.764943
2025-09-27 19:10:26,087 Stage: Train 0.5 | Epoch: 102 | Iter: 156000 | Total Loss: 0.005745 | Recon Loss: 0.004899 | Commit Loss: 0.001693 | Perplexity: 1812.641456
2025-09-27 19:11:23,175 Stage: Train 0.5 | Epoch: 102 | Iter: 156200 | Total Loss: 0.005647 | Recon Loss: 0.004801 | Commit Loss: 0.001692 | Perplexity: 1812.337002
2025-09-27 19:12:20,408 Stage: Train 0.5 | Epoch: 102 | Iter: 156400 | Total Loss: 0.005721 | Recon Loss: 0.004882 | Commit Loss: 0.001679 | Perplexity: 1807.997325
Trainning Epoch:  31%|███       | 103/330 [12:24:45<27:22:39, 434.18s/it]2025-09-27 19:13:17,915 Stage: Train 0.5 | Epoch: 103 | Iter: 156600 | Total Loss: 0.005622 | Recon Loss: 0.004782 | Commit Loss: 0.001679 | Perplexity: 1806.709871
2025-09-27 19:14:15,085 Stage: Train 0.5 | Epoch: 103 | Iter: 156800 | Total Loss: 0.005698 | Recon Loss: 0.004859 | Commit Loss: 0.001678 | Perplexity: 1807.413570
2025-09-27 19:15:12,333 Stage: Train 0.5 | Epoch: 103 | Iter: 157000 | Total Loss: 0.005661 | Recon Loss: 0.004818 | Commit Loss: 0.001684 | Perplexity: 1811.399542
2025-09-27 19:16:09,112 Stage: Train 0.5 | Epoch: 103 | Iter: 157200 | Total Loss: 0.005707 | Recon Loss: 0.004865 | Commit Loss: 0.001684 | Perplexity: 1809.928610
2025-09-27 19:17:06,431 Stage: Train 0.5 | Epoch: 103 | Iter: 157400 | Total Loss: 0.005692 | Recon Loss: 0.004847 | Commit Loss: 0.001690 | Perplexity: 1812.154109
2025-09-27 19:18:03,725 Stage: Train 0.5 | Epoch: 103 | Iter: 157600 | Total Loss: 0.005653 | Recon Loss: 0.004807 | Commit Loss: 0.001694 | Perplexity: 1811.963290
2025-09-27 19:19:00,952 Stage: Train 0.5 | Epoch: 103 | Iter: 157800 | Total Loss: 0.005616 | Recon Loss: 0.004776 | Commit Loss: 0.001679 | Perplexity: 1806.753708
Trainning Epoch:  32%|███▏      | 104/330 [12:32:00<27:15:53, 434.31s/it]2025-09-27 19:19:58,390 Stage: Train 0.5 | Epoch: 104 | Iter: 158000 | Total Loss: 0.005681 | Recon Loss: 0.004837 | Commit Loss: 0.001687 | Perplexity: 1809.317092
2025-09-27 19:20:55,551 Stage: Train 0.5 | Epoch: 104 | Iter: 158200 | Total Loss: 0.005673 | Recon Loss: 0.004833 | Commit Loss: 0.001680 | Perplexity: 1809.688842
2025-09-27 19:21:52,864 Stage: Train 0.5 | Epoch: 104 | Iter: 158400 | Total Loss: 0.005588 | Recon Loss: 0.004751 | Commit Loss: 0.001673 | Perplexity: 1805.470352
2025-09-27 19:22:49,749 Stage: Train 0.5 | Epoch: 104 | Iter: 158600 | Total Loss: 0.005682 | Recon Loss: 0.004835 | Commit Loss: 0.001693 | Perplexity: 1810.763071
2025-09-27 19:23:46,998 Stage: Train 0.5 | Epoch: 104 | Iter: 158800 | Total Loss: 0.005736 | Recon Loss: 0.004895 | Commit Loss: 0.001682 | Perplexity: 1807.170779
2025-09-27 19:24:44,294 Stage: Train 0.5 | Epoch: 104 | Iter: 159000 | Total Loss: 0.005669 | Recon Loss: 0.004826 | Commit Loss: 0.001685 | Perplexity: 1807.666240
2025-09-27 19:25:41,345 Stage: Train 0.5 | Epoch: 104 | Iter: 159200 | Total Loss: 0.005665 | Recon Loss: 0.004818 | Commit Loss: 0.001693 | Perplexity: 1809.854445
2025-09-27 19:26:38,552 Stage: Train 0.5 | Epoch: 104 | Iter: 159400 | Total Loss: 0.005665 | Recon Loss: 0.004819 | Commit Loss: 0.001692 | Perplexity: 1807.510340
Trainning Epoch:  32%|███▏      | 105/330 [12:39:14<27:08:39, 434.31s/it]2025-09-27 19:27:35,885 Stage: Train 0.5 | Epoch: 105 | Iter: 159600 | Total Loss: 0.005664 | Recon Loss: 0.004822 | Commit Loss: 0.001685 | Perplexity: 1806.267175
2025-09-27 19:28:32,883 Stage: Train 0.5 | Epoch: 105 | Iter: 159800 | Total Loss: 0.005597 | Recon Loss: 0.004756 | Commit Loss: 0.001681 | Perplexity: 1806.828528
2025-09-27 19:29:30,207 Stage: Train 0.5 | Epoch: 105 | Iter: 160000 | Total Loss: 0.005718 | Recon Loss: 0.004875 | Commit Loss: 0.001684 | Perplexity: 1805.419150
2025-09-27 19:29:30,207 Saving model at iteration 160000
2025-09-27 19:29:30,398 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000
2025-09-27 19:29:30,905 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/model.safetensors
2025-09-27 19:29:31,389 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/optimizer.bin
2025-09-27 19:29:31,390 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/scheduler.bin
2025-09-27 19:29:31,390 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/sampler.bin
2025-09-27 19:29:31,391 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_106_step_160000/random_states_0.pkl
2025-09-27 19:30:28,618 Stage: Train 0.5 | Epoch: 105 | Iter: 160200 | Total Loss: 0.005719 | Recon Loss: 0.004878 | Commit Loss: 0.001683 | Perplexity: 1806.633752
2025-09-27 19:31:25,896 Stage: Train 0.5 | Epoch: 105 | Iter: 160400 | Total Loss: 0.005654 | Recon Loss: 0.004809 | Commit Loss: 0.001690 | Perplexity: 1810.407820
2025-09-27 19:32:23,241 Stage: Train 0.5 | Epoch: 105 | Iter: 160600 | Total Loss: 0.005662 | Recon Loss: 0.004817 | Commit Loss: 0.001691 | Perplexity: 1807.715568
2025-09-27 19:33:20,529 Stage: Train 0.5 | Epoch: 105 | Iter: 160800 | Total Loss: 0.005627 | Recon Loss: 0.004789 | Commit Loss: 0.001676 | Perplexity: 1808.543908
2025-09-27 19:34:17,410 Stage: Train 0.5 | Epoch: 105 | Iter: 161000 | Total Loss: 0.005707 | Recon Loss: 0.004855 | Commit Loss: 0.001704 | Perplexity: 1808.276111
Trainning Epoch:  32%|███▏      | 106/330 [12:46:30<27:03:02, 434.74s/it]2025-09-27 19:35:14,796 Stage: Train 0.5 | Epoch: 106 | Iter: 161200 | Total Loss: 0.005638 | Recon Loss: 0.004792 | Commit Loss: 0.001692 | Perplexity: 1809.419606
2025-09-27 19:36:12,053 Stage: Train 0.5 | Epoch: 106 | Iter: 161400 | Total Loss: 0.005638 | Recon Loss: 0.004793 | Commit Loss: 0.001691 | Perplexity: 1809.158558
2025-09-27 19:37:09,092 Stage: Train 0.5 | Epoch: 106 | Iter: 161600 | Total Loss: 0.005721 | Recon Loss: 0.004878 | Commit Loss: 0.001687 | Perplexity: 1807.973234
2025-09-27 19:38:06,442 Stage: Train 0.5 | Epoch: 106 | Iter: 161800 | Total Loss: 0.005614 | Recon Loss: 0.004768 | Commit Loss: 0.001692 | Perplexity: 1811.480814
2025-09-27 19:39:02,433 Stage: Train 0.5 | Epoch: 106 | Iter: 162000 | Total Loss: 0.005622 | Recon Loss: 0.004778 | Commit Loss: 0.001688 | Perplexity: 1809.041713
2025-09-27 19:39:59,281 Stage: Train 0.5 | Epoch: 106 | Iter: 162200 | Total Loss: 0.005607 | Recon Loss: 0.004762 | Commit Loss: 0.001689 | Perplexity: 1808.727593
2025-09-27 19:40:56,450 Stage: Train 0.5 | Epoch: 106 | Iter: 162400 | Total Loss: 0.005698 | Recon Loss: 0.004851 | Commit Loss: 0.001693 | Perplexity: 1805.938904
Trainning Epoch:  32%|███▏      | 107/330 [12:53:43<26:53:57, 434.25s/it]2025-09-27 19:41:53,807 Stage: Train 0.5 | Epoch: 107 | Iter: 162600 | Total Loss: 0.005659 | Recon Loss: 0.004817 | Commit Loss: 0.001685 | Perplexity: 1801.530305
2025-09-27 19:42:50,890 Stage: Train 0.5 | Epoch: 107 | Iter: 162800 | Total Loss: 0.005623 | Recon Loss: 0.004784 | Commit Loss: 0.001679 | Perplexity: 1805.850863
2025-09-27 19:43:48,156 Stage: Train 0.5 | Epoch: 107 | Iter: 163000 | Total Loss: 0.005615 | Recon Loss: 0.004772 | Commit Loss: 0.001687 | Perplexity: 1808.223969
2025-09-27 19:44:45,480 Stage: Train 0.5 | Epoch: 107 | Iter: 163200 | Total Loss: 0.005669 | Recon Loss: 0.004822 | Commit Loss: 0.001694 | Perplexity: 1811.430183
2025-09-27 19:45:42,792 Stage: Train 0.5 | Epoch: 107 | Iter: 163400 | Total Loss: 0.005608 | Recon Loss: 0.004760 | Commit Loss: 0.001696 | Perplexity: 1807.348846
2025-09-27 19:46:39,619 Stage: Train 0.5 | Epoch: 107 | Iter: 163600 | Total Loss: 0.005615 | Recon Loss: 0.004764 | Commit Loss: 0.001701 | Perplexity: 1808.529340
2025-09-27 19:47:36,836 Stage: Train 0.5 | Epoch: 107 | Iter: 163800 | Total Loss: 0.005693 | Recon Loss: 0.004841 | Commit Loss: 0.001704 | Perplexity: 1813.898819
2025-09-27 19:48:34,115 Stage: Train 0.5 | Epoch: 107 | Iter: 164000 | Total Loss: 0.005632 | Recon Loss: 0.004781 | Commit Loss: 0.001702 | Perplexity: 1808.119819
Trainning Epoch:  33%|███▎      | 108/330 [13:00:57<26:47:01, 434.33s/it]2025-09-27 19:49:31,573 Stage: Train 0.5 | Epoch: 108 | Iter: 164200 | Total Loss: 0.005625 | Recon Loss: 0.004787 | Commit Loss: 0.001676 | Perplexity: 1806.310210
2025-09-27 19:50:28,913 Stage: Train 0.5 | Epoch: 108 | Iter: 164400 | Total Loss: 0.005645 | Recon Loss: 0.004794 | Commit Loss: 0.001703 | Perplexity: 1808.993496
2025-09-27 19:51:26,133 Stage: Train 0.5 | Epoch: 108 | Iter: 164600 | Total Loss: 0.005624 | Recon Loss: 0.004777 | Commit Loss: 0.001695 | Perplexity: 1807.278457
2025-09-27 19:52:23,094 Stage: Train 0.5 | Epoch: 108 | Iter: 164800 | Total Loss: 0.005612 | Recon Loss: 0.004768 | Commit Loss: 0.001689 | Perplexity: 1808.016393
2025-09-27 19:53:20,463 Stage: Train 0.5 | Epoch: 108 | Iter: 165000 | Total Loss: 0.005672 | Recon Loss: 0.004825 | Commit Loss: 0.001694 | Perplexity: 1807.339431
2025-09-27 19:54:17,521 Stage: Train 0.5 | Epoch: 108 | Iter: 165200 | Total Loss: 0.005654 | Recon Loss: 0.004806 | Commit Loss: 0.001695 | Perplexity: 1810.475610
2025-09-27 19:55:14,787 Stage: Train 0.5 | Epoch: 108 | Iter: 165400 | Total Loss: 0.005652 | Recon Loss: 0.004806 | Commit Loss: 0.001692 | Perplexity: 1806.049031
Trainning Epoch:  33%|███▎      | 109/330 [13:08:12<26:40:16, 434.46s/it]2025-09-27 19:56:12,358 Stage: Train 0.5 | Epoch: 109 | Iter: 165600 | Total Loss: 0.005686 | Recon Loss: 0.004838 | Commit Loss: 0.001695 | Perplexity: 1805.162708
2025-09-27 19:57:09,603 Stage: Train 0.5 | Epoch: 109 | Iter: 165800 | Total Loss: 0.005563 | Recon Loss: 0.004721 | Commit Loss: 0.001684 | Perplexity: 1805.014189
2025-09-27 19:58:06,608 Stage: Train 0.5 | Epoch: 109 | Iter: 166000 | Total Loss: 0.005641 | Recon Loss: 0.004795 | Commit Loss: 0.001691 | Perplexity: 1806.853190
2025-09-27 19:59:04,102 Stage: Train 0.5 | Epoch: 109 | Iter: 166200 | Total Loss: 0.005639 | Recon Loss: 0.004788 | Commit Loss: 0.001702 | Perplexity: 1813.182859
2025-09-27 20:00:01,201 Stage: Train 0.5 | Epoch: 109 | Iter: 166400 | Total Loss: 0.005573 | Recon Loss: 0.004722 | Commit Loss: 0.001703 | Perplexity: 1809.467078
2025-09-27 20:00:58,318 Stage: Train 0.5 | Epoch: 109 | Iter: 166600 | Total Loss: 0.005634 | Recon Loss: 0.004781 | Commit Loss: 0.001707 | Perplexity: 1810.703267
2025-09-27 20:01:55,544 Stage: Train 0.5 | Epoch: 109 | Iter: 166800 | Total Loss: 0.005585 | Recon Loss: 0.004740 | Commit Loss: 0.001691 | Perplexity: 1808.433473
2025-09-27 20:02:52,700 Stage: Train 0.5 | Epoch: 109 | Iter: 167000 | Total Loss: 0.005603 | Recon Loss: 0.004750 | Commit Loss: 0.001706 | Perplexity: 1809.894379
Trainning Epoch:  33%|███▎      | 110/330 [13:15:27<26:33:09, 434.50s/it]2025-09-27 20:03:49,252 Stage: Train 0.5 | Epoch: 110 | Iter: 167200 | Total Loss: 0.005640 | Recon Loss: 0.004791 | Commit Loss: 0.001698 | Perplexity: 1808.193839
2025-09-27 20:04:46,497 Stage: Train 0.5 | Epoch: 110 | Iter: 167400 | Total Loss: 0.005514 | Recon Loss: 0.004674 | Commit Loss: 0.001681 | Perplexity: 1804.629203
2025-09-27 20:05:43,783 Stage: Train 0.5 | Epoch: 110 | Iter: 167600 | Total Loss: 0.005672 | Recon Loss: 0.004821 | Commit Loss: 0.001702 | Perplexity: 1809.828456
2025-09-27 20:06:40,970 Stage: Train 0.5 | Epoch: 110 | Iter: 167800 | Total Loss: 0.005614 | Recon Loss: 0.004757 | Commit Loss: 0.001714 | Perplexity: 1812.627171
2025-09-27 20:07:38,161 Stage: Train 0.5 | Epoch: 110 | Iter: 168000 | Total Loss: 0.005573 | Recon Loss: 0.004724 | Commit Loss: 0.001697 | Perplexity: 1807.649323
2025-09-27 20:08:35,325 Stage: Train 0.5 | Epoch: 110 | Iter: 168200 | Total Loss: 0.005576 | Recon Loss: 0.004723 | Commit Loss: 0.001705 | Perplexity: 1811.928948
2025-09-27 20:09:32,470 Stage: Train 0.5 | Epoch: 110 | Iter: 168400 | Total Loss: 0.005576 | Recon Loss: 0.004720 | Commit Loss: 0.001712 | Perplexity: 1813.309186
2025-09-27 20:10:29,409 Stage: Train 0.5 | Epoch: 110 | Iter: 168600 | Total Loss: 0.005687 | Recon Loss: 0.004835 | Commit Loss: 0.001704 | Perplexity: 1809.735238
Trainning Epoch:  34%|███▎      | 111/330 [13:22:40<26:24:58, 434.24s/it]2025-09-27 20:11:27,275 Stage: Train 0.5 | Epoch: 111 | Iter: 168800 | Total Loss: 0.005545 | Recon Loss: 0.004700 | Commit Loss: 0.001690 | Perplexity: 1804.298973
2025-09-27 20:12:24,432 Stage: Train 0.5 | Epoch: 111 | Iter: 169000 | Total Loss: 0.005684 | Recon Loss: 0.004836 | Commit Loss: 0.001696 | Perplexity: 1806.584291
2025-09-27 20:13:21,575 Stage: Train 0.5 | Epoch: 111 | Iter: 169200 | Total Loss: 0.005604 | Recon Loss: 0.004751 | Commit Loss: 0.001705 | Perplexity: 1808.973715
2025-09-27 20:14:18,731 Stage: Train 0.5 | Epoch: 111 | Iter: 169400 | Total Loss: 0.005600 | Recon Loss: 0.004744 | Commit Loss: 0.001711 | Perplexity: 1807.314661
2025-09-27 20:15:15,849 Stage: Train 0.5 | Epoch: 111 | Iter: 169600 | Total Loss: 0.005600 | Recon Loss: 0.004752 | Commit Loss: 0.001696 | Perplexity: 1809.048538
2025-09-27 20:16:12,731 Stage: Train 0.5 | Epoch: 111 | Iter: 169800 | Total Loss: 0.005647 | Recon Loss: 0.004800 | Commit Loss: 0.001695 | Perplexity: 1806.951193
2025-09-27 20:17:09,946 Stage: Train 0.5 | Epoch: 111 | Iter: 170000 | Total Loss: 0.005574 | Recon Loss: 0.004724 | Commit Loss: 0.001701 | Perplexity: 1807.987249
Trainning Epoch:  34%|███▍      | 112/330 [13:29:55<26:18:10, 434.36s/it]2025-09-27 20:18:07,376 Stage: Train 0.5 | Epoch: 112 | Iter: 170200 | Total Loss: 0.005583 | Recon Loss: 0.004733 | Commit Loss: 0.001701 | Perplexity: 1809.716569
2025-09-27 20:19:04,653 Stage: Train 0.5 | Epoch: 112 | Iter: 170400 | Total Loss: 0.005509 | Recon Loss: 0.004659 | Commit Loss: 0.001700 | Perplexity: 1809.610771
2025-09-27 20:20:02,022 Stage: Train 0.5 | Epoch: 112 | Iter: 170600 | Total Loss: 0.005540 | Recon Loss: 0.004683 | Commit Loss: 0.001713 | Perplexity: 1807.724971
2025-09-27 20:20:59,314 Stage: Train 0.5 | Epoch: 112 | Iter: 170800 | Total Loss: 0.005549 | Recon Loss: 0.004699 | Commit Loss: 0.001701 | Perplexity: 1809.242919
2025-09-27 20:21:56,189 Stage: Train 0.5 | Epoch: 112 | Iter: 171000 | Total Loss: 0.005571 | Recon Loss: 0.004718 | Commit Loss: 0.001705 | Perplexity: 1811.423839
2025-09-27 20:22:53,481 Stage: Train 0.5 | Epoch: 112 | Iter: 171200 | Total Loss: 0.005611 | Recon Loss: 0.004762 | Commit Loss: 0.001699 | Perplexity: 1806.115698
2025-09-27 20:23:50,733 Stage: Train 0.5 | Epoch: 112 | Iter: 171400 | Total Loss: 0.005559 | Recon Loss: 0.004706 | Commit Loss: 0.001707 | Perplexity: 1807.875831
2025-09-27 20:24:47,995 Stage: Train 0.5 | Epoch: 112 | Iter: 171600 | Total Loss: 0.005568 | Recon Loss: 0.004710 | Commit Loss: 0.001717 | Perplexity: 1813.723188
Trainning Epoch:  34%|███▍      | 113/330 [13:37:10<26:11:23, 434.49s/it]2025-09-27 20:25:45,378 Stage: Train 0.5 | Epoch: 113 | Iter: 171800 | Total Loss: 0.005570 | Recon Loss: 0.004723 | Commit Loss: 0.001694 | Perplexity: 1802.586183
2025-09-27 20:26:42,608 Stage: Train 0.5 | Epoch: 113 | Iter: 172000 | Total Loss: 0.005562 | Recon Loss: 0.004711 | Commit Loss: 0.001703 | Perplexity: 1808.460977
2025-09-27 20:27:39,783 Stage: Train 0.5 | Epoch: 113 | Iter: 172200 | Total Loss: 0.005570 | Recon Loss: 0.004714 | Commit Loss: 0.001711 | Perplexity: 1813.899337
2025-09-27 20:28:36,646 Stage: Train 0.5 | Epoch: 113 | Iter: 172400 | Total Loss: 0.005519 | Recon Loss: 0.004669 | Commit Loss: 0.001700 | Perplexity: 1805.905739
2025-09-27 20:29:33,913 Stage: Train 0.5 | Epoch: 113 | Iter: 172600 | Total Loss: 0.005602 | Recon Loss: 0.004745 | Commit Loss: 0.001714 | Perplexity: 1807.079767
2025-09-27 20:30:31,104 Stage: Train 0.5 | Epoch: 113 | Iter: 172800 | Total Loss: 0.005584 | Recon Loss: 0.004729 | Commit Loss: 0.001710 | Perplexity: 1811.575820
2025-09-27 20:31:28,288 Stage: Train 0.5 | Epoch: 113 | Iter: 173000 | Total Loss: 0.005627 | Recon Loss: 0.004773 | Commit Loss: 0.001707 | Perplexity: 1810.374770
Trainning Epoch:  35%|███▍      | 114/330 [13:44:24<26:03:48, 434.39s/it]2025-09-27 20:32:25,474 Stage: Train 0.5 | Epoch: 114 | Iter: 173200 | Total Loss: 0.005605 | Recon Loss: 0.004752 | Commit Loss: 0.001707 | Perplexity: 1806.551317
2025-09-27 20:33:22,688 Stage: Train 0.5 | Epoch: 114 | Iter: 173400 | Total Loss: 0.005510 | Recon Loss: 0.004658 | Commit Loss: 0.001703 | Perplexity: 1810.574238
2025-09-27 20:34:19,394 Stage: Train 0.5 | Epoch: 114 | Iter: 173600 | Total Loss: 0.005578 | Recon Loss: 0.004724 | Commit Loss: 0.001709 | Perplexity: 1809.570930
2025-09-27 20:35:16,481 Stage: Train 0.5 | Epoch: 114 | Iter: 173800 | Total Loss: 0.005563 | Recon Loss: 0.004714 | Commit Loss: 0.001696 | Perplexity: 1807.592117
2025-09-27 20:36:13,780 Stage: Train 0.5 | Epoch: 114 | Iter: 174000 | Total Loss: 0.005518 | Recon Loss: 0.004668 | Commit Loss: 0.001700 | Perplexity: 1809.888313
2025-09-27 20:37:10,955 Stage: Train 0.5 | Epoch: 114 | Iter: 174200 | Total Loss: 0.005561 | Recon Loss: 0.004704 | Commit Loss: 0.001715 | Perplexity: 1806.649770
2025-09-27 20:38:08,117 Stage: Train 0.5 | Epoch: 114 | Iter: 174400 | Total Loss: 0.005590 | Recon Loss: 0.004737 | Commit Loss: 0.001707 | Perplexity: 1805.083953
2025-09-27 20:39:05,273 Stage: Train 0.5 | Epoch: 114 | Iter: 174600 | Total Loss: 0.005660 | Recon Loss: 0.004802 | Commit Loss: 0.001716 | Perplexity: 1806.845285
Trainning Epoch:  35%|███▍      | 115/330 [13:51:38<25:56:06, 434.26s/it]2025-09-27 20:40:02,290 Stage: Train 0.5 | Epoch: 115 | Iter: 174800 | Total Loss: 0.005544 | Recon Loss: 0.004692 | Commit Loss: 0.001705 | Perplexity: 1805.737477
2025-09-27 20:40:59,340 Stage: Train 0.5 | Epoch: 115 | Iter: 175000 | Total Loss: 0.005595 | Recon Loss: 0.004744 | Commit Loss: 0.001703 | Perplexity: 1805.059812
2025-09-27 20:41:56,563 Stage: Train 0.5 | Epoch: 115 | Iter: 175200 | Total Loss: 0.005539 | Recon Loss: 0.004685 | Commit Loss: 0.001708 | Perplexity: 1807.440884
2025-09-27 20:42:53,564 Stage: Train 0.5 | Epoch: 115 | Iter: 175400 | Total Loss: 0.005614 | Recon Loss: 0.004757 | Commit Loss: 0.001713 | Perplexity: 1808.241839
2025-09-27 20:43:50,775 Stage: Train 0.5 | Epoch: 115 | Iter: 175600 | Total Loss: 0.005538 | Recon Loss: 0.004689 | Commit Loss: 0.001699 | Perplexity: 1810.017571
2025-09-27 20:44:47,833 Stage: Train 0.5 | Epoch: 115 | Iter: 175800 | Total Loss: 0.005576 | Recon Loss: 0.004721 | Commit Loss: 0.001709 | Perplexity: 1809.970720
2025-09-27 20:45:44,659 Stage: Train 0.5 | Epoch: 115 | Iter: 176000 | Total Loss: 0.005572 | Recon Loss: 0.004720 | Commit Loss: 0.001705 | Perplexity: 1807.938116
2025-09-27 20:46:41,878 Stage: Train 0.5 | Epoch: 115 | Iter: 176200 | Total Loss: 0.005566 | Recon Loss: 0.004711 | Commit Loss: 0.001709 | Perplexity: 1806.161675
Trainning Epoch:  35%|███▌      | 116/330 [13:58:51<25:48:00, 434.02s/it]2025-09-27 20:47:39,141 Stage: Train 0.5 | Epoch: 116 | Iter: 176400 | Total Loss: 0.005546 | Recon Loss: 0.004697 | Commit Loss: 0.001698 | Perplexity: 1806.282153
2025-09-27 20:48:36,443 Stage: Train 0.5 | Epoch: 116 | Iter: 176600 | Total Loss: 0.005566 | Recon Loss: 0.004711 | Commit Loss: 0.001710 | Perplexity: 1809.519853
2025-09-27 20:49:33,466 Stage: Train 0.5 | Epoch: 116 | Iter: 176800 | Total Loss: 0.005490 | Recon Loss: 0.004635 | Commit Loss: 0.001710 | Perplexity: 1806.327855
2025-09-27 20:50:30,614 Stage: Train 0.5 | Epoch: 116 | Iter: 177000 | Total Loss: 0.005545 | Recon Loss: 0.004688 | Commit Loss: 0.001713 | Perplexity: 1812.076245
2025-09-27 20:51:27,644 Stage: Train 0.5 | Epoch: 116 | Iter: 177200 | Total Loss: 0.005512 | Recon Loss: 0.004655 | Commit Loss: 0.001714 | Perplexity: 1808.064263
2025-09-27 20:52:24,352 Stage: Train 0.5 | Epoch: 116 | Iter: 177400 | Total Loss: 0.005540 | Recon Loss: 0.004681 | Commit Loss: 0.001717 | Perplexity: 1808.284327
2025-09-27 20:53:21,584 Stage: Train 0.5 | Epoch: 116 | Iter: 177600 | Total Loss: 0.005621 | Recon Loss: 0.004763 | Commit Loss: 0.001715 | Perplexity: 1808.712573
Trainning Epoch:  35%|███▌      | 117/330 [14:06:05<25:40:41, 434.00s/it]2025-09-27 20:54:19,187 Stage: Train 0.5 | Epoch: 117 | Iter: 177800 | Total Loss: 0.005518 | Recon Loss: 0.004664 | Commit Loss: 0.001708 | Perplexity: 1801.450952
2025-09-27 20:55:16,539 Stage: Train 0.5 | Epoch: 117 | Iter: 178000 | Total Loss: 0.005514 | Recon Loss: 0.004660 | Commit Loss: 0.001708 | Perplexity: 1803.183317
2025-09-27 20:56:13,960 Stage: Train 0.5 | Epoch: 117 | Iter: 178200 | Total Loss: 0.005545 | Recon Loss: 0.004687 | Commit Loss: 0.001717 | Perplexity: 1806.159604
2025-09-27 20:57:11,154 Stage: Train 0.5 | Epoch: 117 | Iter: 178400 | Total Loss: 0.005579 | Recon Loss: 0.004723 | Commit Loss: 0.001713 | Perplexity: 1809.111188
2025-09-27 20:58:06,963 Stage: Train 0.5 | Epoch: 117 | Iter: 178600 | Total Loss: 0.005584 | Recon Loss: 0.004725 | Commit Loss: 0.001717 | Perplexity: 1811.595158
2025-09-27 20:59:04,061 Stage: Train 0.5 | Epoch: 117 | Iter: 178800 | Total Loss: 0.005560 | Recon Loss: 0.004704 | Commit Loss: 0.001712 | Perplexity: 1809.079773
2025-09-27 21:00:01,141 Stage: Train 0.5 | Epoch: 117 | Iter: 179000 | Total Loss: 0.005486 | Recon Loss: 0.004628 | Commit Loss: 0.001716 | Perplexity: 1811.169072
2025-09-27 21:00:58,414 Stage: Train 0.5 | Epoch: 117 | Iter: 179200 | Total Loss: 0.005515 | Recon Loss: 0.004660 | Commit Loss: 0.001710 | Perplexity: 1810.164432
Trainning Epoch:  36%|███▌      | 118/330 [14:13:19<25:32:49, 433.82s/it]2025-09-27 21:01:55,734 Stage: Train 0.5 | Epoch: 118 | Iter: 179400 | Total Loss: 0.005643 | Recon Loss: 0.004784 | Commit Loss: 0.001718 | Perplexity: 1810.402687
2025-09-27 21:02:52,941 Stage: Train 0.5 | Epoch: 118 | Iter: 179600 | Total Loss: 0.005489 | Recon Loss: 0.004631 | Commit Loss: 0.001714 | Perplexity: 1815.283254
2025-09-27 21:03:49,943 Stage: Train 0.5 | Epoch: 118 | Iter: 179800 | Total Loss: 0.005562 | Recon Loss: 0.004705 | Commit Loss: 0.001713 | Perplexity: 1805.767536
2025-09-27 21:04:47,239 Stage: Train 0.5 | Epoch: 118 | Iter: 180000 | Total Loss: 0.005471 | Recon Loss: 0.004615 | Commit Loss: 0.001712 | Perplexity: 1812.979282
2025-09-27 21:04:47,239 Saving model at iteration 180000
2025-09-27 21:04:47,427 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000
2025-09-27 21:04:47,882 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/model.safetensors
2025-09-27 21:04:48,375 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/optimizer.bin
2025-09-27 21:04:48,375 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/scheduler.bin
2025-09-27 21:04:48,375 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/sampler.bin
2025-09-27 21:04:48,376 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_119_step_180000/random_states_0.pkl
2025-09-27 21:05:45,799 Stage: Train 0.5 | Epoch: 118 | Iter: 180200 | Total Loss: 0.005508 | Recon Loss: 0.004649 | Commit Loss: 0.001717 | Perplexity: 1809.952686
2025-09-27 21:06:43,295 Stage: Train 0.5 | Epoch: 118 | Iter: 180400 | Total Loss: 0.005581 | Recon Loss: 0.004723 | Commit Loss: 0.001714 | Perplexity: 1809.442026
2025-09-27 21:07:40,554 Stage: Train 0.5 | Epoch: 118 | Iter: 180600 | Total Loss: 0.005477 | Recon Loss: 0.004618 | Commit Loss: 0.001718 | Perplexity: 1811.113038
Trainning Epoch:  36%|███▌      | 119/330 [14:20:35<25:28:11, 434.55s/it]2025-09-27 21:08:38,048 Stage: Train 0.5 | Epoch: 119 | Iter: 180800 | Total Loss: 0.005516 | Recon Loss: 0.004662 | Commit Loss: 0.001707 | Perplexity: 1807.878787
2025-09-27 21:09:35,266 Stage: Train 0.5 | Epoch: 119 | Iter: 181000 | Total Loss: 0.005569 | Recon Loss: 0.004713 | Commit Loss: 0.001712 | Perplexity: 1811.353371
2025-09-27 21:10:31,916 Stage: Train 0.5 | Epoch: 119 | Iter: 181200 | Total Loss: 0.005526 | Recon Loss: 0.004662 | Commit Loss: 0.001728 | Perplexity: 1813.943221
2025-09-27 21:11:29,236 Stage: Train 0.5 | Epoch: 119 | Iter: 181400 | Total Loss: 0.005516 | Recon Loss: 0.004663 | Commit Loss: 0.001706 | Perplexity: 1808.331255
2025-09-27 21:12:26,390 Stage: Train 0.5 | Epoch: 119 | Iter: 181600 | Total Loss: 0.005526 | Recon Loss: 0.004671 | Commit Loss: 0.001710 | Perplexity: 1807.841365
2025-09-27 21:13:23,577 Stage: Train 0.5 | Epoch: 119 | Iter: 181800 | Total Loss: 0.005493 | Recon Loss: 0.004641 | Commit Loss: 0.001704 | Perplexity: 1806.753194
2025-09-27 21:14:20,569 Stage: Train 0.5 | Epoch: 119 | Iter: 182000 | Total Loss: 0.005472 | Recon Loss: 0.004613 | Commit Loss: 0.001719 | Perplexity: 1813.730337
2025-09-27 21:15:17,815 Stage: Train 0.5 | Epoch: 119 | Iter: 182200 | Total Loss: 0.005558 | Recon Loss: 0.004704 | Commit Loss: 0.001709 | Perplexity: 1805.925091
Trainning Epoch:  36%|███▋      | 120/330 [14:27:49<25:20:09, 434.33s/it]2025-09-27 21:16:14,952 Stage: Train 0.5 | Epoch: 120 | Iter: 182400 | Total Loss: 0.005587 | Recon Loss: 0.004727 | Commit Loss: 0.001720 | Perplexity: 1810.034473
2025-09-27 21:17:12,160 Stage: Train 0.5 | Epoch: 120 | Iter: 182600 | Total Loss: 0.005440 | Recon Loss: 0.004585 | Commit Loss: 0.001711 | Perplexity: 1810.747092
2025-09-27 21:18:09,279 Stage: Train 0.5 | Epoch: 120 | Iter: 182800 | Total Loss: 0.005491 | Recon Loss: 0.004633 | Commit Loss: 0.001716 | Perplexity: 1805.743404
2025-09-27 21:19:06,453 Stage: Train 0.5 | Epoch: 120 | Iter: 183000 | Total Loss: 0.005605 | Recon Loss: 0.004751 | Commit Loss: 0.001708 | Perplexity: 1808.328400
2025-09-27 21:20:03,603 Stage: Train 0.5 | Epoch: 120 | Iter: 183200 | Total Loss: 0.005460 | Recon Loss: 0.004605 | Commit Loss: 0.001709 | Perplexity: 1806.372881
2025-09-27 21:21:00,836 Stage: Train 0.5 | Epoch: 120 | Iter: 183400 | Total Loss: 0.005518 | Recon Loss: 0.004664 | Commit Loss: 0.001708 | Perplexity: 1807.139675
2025-09-27 21:21:57,543 Stage: Train 0.5 | Epoch: 120 | Iter: 183600 | Total Loss: 0.005477 | Recon Loss: 0.004620 | Commit Loss: 0.001714 | Perplexity: 1808.604120
Trainning Epoch:  37%|███▋      | 121/330 [14:35:03<25:12:43, 434.27s/it]2025-09-27 21:22:55,060 Stage: Train 0.5 | Epoch: 121 | Iter: 183800 | Total Loss: 0.005485 | Recon Loss: 0.004630 | Commit Loss: 0.001710 | Perplexity: 1808.594238
2025-09-27 21:23:52,133 Stage: Train 0.5 | Epoch: 121 | Iter: 184000 | Total Loss: 0.005552 | Recon Loss: 0.004698 | Commit Loss: 0.001708 | Perplexity: 1808.165483
2025-09-27 21:24:49,495 Stage: Train 0.5 | Epoch: 121 | Iter: 184200 | Total Loss: 0.005510 | Recon Loss: 0.004656 | Commit Loss: 0.001708 | Perplexity: 1807.230081
2025-09-27 21:25:46,638 Stage: Train 0.5 | Epoch: 121 | Iter: 184400 | Total Loss: 0.005514 | Recon Loss: 0.004653 | Commit Loss: 0.001721 | Perplexity: 1812.811494
2025-09-27 21:26:43,682 Stage: Train 0.5 | Epoch: 121 | Iter: 184600 | Total Loss: 0.005481 | Recon Loss: 0.004616 | Commit Loss: 0.001730 | Perplexity: 1811.850957
2025-09-27 21:27:40,536 Stage: Train 0.5 | Epoch: 121 | Iter: 184800 | Total Loss: 0.005525 | Recon Loss: 0.004668 | Commit Loss: 0.001714 | Perplexity: 1808.512686
2025-09-27 21:28:37,681 Stage: Train 0.5 | Epoch: 121 | Iter: 185000 | Total Loss: 0.005531 | Recon Loss: 0.004678 | Commit Loss: 0.001706 | Perplexity: 1809.880571
2025-09-27 21:29:35,017 Stage: Train 0.5 | Epoch: 121 | Iter: 185200 | Total Loss: 0.005533 | Recon Loss: 0.004674 | Commit Loss: 0.001719 | Perplexity: 1805.081643
Trainning Epoch:  37%|███▋      | 122/330 [14:42:17<25:05:21, 434.24s/it]2025-09-27 21:30:32,416 Stage: Train 0.5 | Epoch: 122 | Iter: 185400 | Total Loss: 0.005456 | Recon Loss: 0.004603 | Commit Loss: 0.001705 | Perplexity: 1805.780286
2025-09-27 21:31:29,610 Stage: Train 0.5 | Epoch: 122 | Iter: 185600 | Total Loss: 0.005494 | Recon Loss: 0.004638 | Commit Loss: 0.001712 | Perplexity: 1811.177618
2025-09-27 21:32:26,722 Stage: Train 0.5 | Epoch: 122 | Iter: 185800 | Total Loss: 0.005482 | Recon Loss: 0.004628 | Commit Loss: 0.001710 | Perplexity: 1808.290283
2025-09-27 21:33:24,095 Stage: Train 0.5 | Epoch: 122 | Iter: 186000 | Total Loss: 0.005493 | Recon Loss: 0.004633 | Commit Loss: 0.001720 | Perplexity: 1808.360812
2025-09-27 21:34:21,128 Stage: Train 0.5 | Epoch: 122 | Iter: 186200 | Total Loss: 0.005527 | Recon Loss: 0.004669 | Commit Loss: 0.001716 | Perplexity: 1809.997325
2025-09-27 21:35:18,131 Stage: Train 0.5 | Epoch: 122 | Iter: 186400 | Total Loss: 0.005482 | Recon Loss: 0.004618 | Commit Loss: 0.001727 | Perplexity: 1811.211636
2025-09-27 21:36:15,239 Stage: Train 0.5 | Epoch: 122 | Iter: 186600 | Total Loss: 0.005485 | Recon Loss: 0.004627 | Commit Loss: 0.001716 | Perplexity: 1806.621204
2025-09-27 21:37:12,408 Stage: Train 0.5 | Epoch: 122 | Iter: 186800 | Total Loss: 0.005528 | Recon Loss: 0.004664 | Commit Loss: 0.001727 | Perplexity: 1810.742924
Trainning Epoch:  37%|███▋      | 123/330 [14:49:31<24:58:08, 434.24s/it]2025-09-27 21:38:09,853 Stage: Train 0.5 | Epoch: 123 | Iter: 187000 | Total Loss: 0.005508 | Recon Loss: 0.004650 | Commit Loss: 0.001714 | Perplexity: 1808.391477
2025-09-27 21:39:07,094 Stage: Train 0.5 | Epoch: 123 | Iter: 187200 | Total Loss: 0.005506 | Recon Loss: 0.004646 | Commit Loss: 0.001720 | Perplexity: 1808.051049
2025-09-27 21:40:03,837 Stage: Train 0.5 | Epoch: 123 | Iter: 187400 | Total Loss: 0.005466 | Recon Loss: 0.004604 | Commit Loss: 0.001724 | Perplexity: 1809.453848
2025-09-27 21:41:00,983 Stage: Train 0.5 | Epoch: 123 | Iter: 187600 | Total Loss: 0.005506 | Recon Loss: 0.004650 | Commit Loss: 0.001712 | Perplexity: 1806.008462
2025-09-27 21:41:58,349 Stage: Train 0.5 | Epoch: 123 | Iter: 187800 | Total Loss: 0.005495 | Recon Loss: 0.004636 | Commit Loss: 0.001718 | Perplexity: 1809.672670
2025-09-27 21:42:55,488 Stage: Train 0.5 | Epoch: 123 | Iter: 188000 | Total Loss: 0.005486 | Recon Loss: 0.004625 | Commit Loss: 0.001722 | Perplexity: 1810.290001
2025-09-27 21:43:52,483 Stage: Train 0.5 | Epoch: 123 | Iter: 188200 | Total Loss: 0.005485 | Recon Loss: 0.004622 | Commit Loss: 0.001728 | Perplexity: 1816.790195
Trainning Epoch:  38%|███▊      | 124/330 [14:56:45<24:50:42, 434.19s/it]2025-09-27 21:44:49,772 Stage: Train 0.5 | Epoch: 124 | Iter: 188400 | Total Loss: 0.005486 | Recon Loss: 0.004627 | Commit Loss: 0.001719 | Perplexity: 1808.680416
2025-09-27 21:45:46,605 Stage: Train 0.5 | Epoch: 124 | Iter: 188600 | Total Loss: 0.005467 | Recon Loss: 0.004610 | Commit Loss: 0.001713 | Perplexity: 1813.082486
2025-09-27 21:46:43,740 Stage: Train 0.5 | Epoch: 124 | Iter: 188800 | Total Loss: 0.005464 | Recon Loss: 0.004602 | Commit Loss: 0.001724 | Perplexity: 1813.303897
2025-09-27 21:47:41,029 Stage: Train 0.5 | Epoch: 124 | Iter: 189000 | Total Loss: 0.005542 | Recon Loss: 0.004685 | Commit Loss: 0.001714 | Perplexity: 1803.526635
2025-09-27 21:48:38,336 Stage: Train 0.5 | Epoch: 124 | Iter: 189200 | Total Loss: 0.005433 | Recon Loss: 0.004573 | Commit Loss: 0.001718 | Perplexity: 1810.885316
2025-09-27 21:49:35,434 Stage: Train 0.5 | Epoch: 124 | Iter: 189400 | Total Loss: 0.005505 | Recon Loss: 0.004646 | Commit Loss: 0.001719 | Perplexity: 1811.993907
2025-09-27 21:50:32,846 Stage: Train 0.5 | Epoch: 124 | Iter: 189600 | Total Loss: 0.005437 | Recon Loss: 0.004576 | Commit Loss: 0.001722 | Perplexity: 1810.386550
2025-09-27 21:51:30,307 Stage: Train 0.5 | Epoch: 124 | Iter: 189800 | Total Loss: 0.005518 | Recon Loss: 0.004655 | Commit Loss: 0.001727 | Perplexity: 1807.939283
Trainning Epoch:  38%|███▊      | 125/330 [15:04:00<24:43:37, 434.23s/it]2025-09-27 21:52:27,278 Stage: Train 0.5 | Epoch: 125 | Iter: 190000 | Total Loss: 0.005444 | Recon Loss: 0.004587 | Commit Loss: 0.001714 | Perplexity: 1806.982058
2025-09-27 21:53:24,509 Stage: Train 0.5 | Epoch: 125 | Iter: 190200 | Total Loss: 0.005446 | Recon Loss: 0.004593 | Commit Loss: 0.001705 | Perplexity: 1813.627697
2025-09-27 21:54:21,725 Stage: Train 0.5 | Epoch: 125 | Iter: 190400 | Total Loss: 0.005462 | Recon Loss: 0.004604 | Commit Loss: 0.001716 | Perplexity: 1812.897911
2025-09-27 21:55:18,922 Stage: Train 0.5 | Epoch: 125 | Iter: 190600 | Total Loss: 0.005508 | Recon Loss: 0.004645 | Commit Loss: 0.001726 | Perplexity: 1807.631807
2025-09-27 21:56:16,057 Stage: Train 0.5 | Epoch: 125 | Iter: 190800 | Total Loss: 0.005476 | Recon Loss: 0.004615 | Commit Loss: 0.001723 | Perplexity: 1811.799838
2025-09-27 21:57:13,142 Stage: Train 0.5 | Epoch: 125 | Iter: 191000 | Total Loss: 0.005515 | Recon Loss: 0.004652 | Commit Loss: 0.001725 | Perplexity: 1808.163293
2025-09-27 21:58:10,262 Stage: Train 0.5 | Epoch: 125 | Iter: 191200 | Total Loss: 0.005472 | Recon Loss: 0.004613 | Commit Loss: 0.001718 | Perplexity: 1812.734878
Trainning Epoch:  38%|███▊      | 126/330 [15:11:14<24:36:30, 434.27s/it]2025-09-27 21:59:07,648 Stage: Train 0.5 | Epoch: 126 | Iter: 191400 | Total Loss: 0.005441 | Recon Loss: 0.004581 | Commit Loss: 0.001721 | Perplexity: 1808.391112
2025-09-27 22:00:04,860 Stage: Train 0.5 | Epoch: 126 | Iter: 191600 | Total Loss: 0.005446 | Recon Loss: 0.004588 | Commit Loss: 0.001716 | Perplexity: 1812.534768
2025-09-27 22:01:02,088 Stage: Train 0.5 | Epoch: 126 | Iter: 191800 | Total Loss: 0.005411 | Recon Loss: 0.004556 | Commit Loss: 0.001711 | Perplexity: 1810.381259
2025-09-27 22:01:59,350 Stage: Train 0.5 | Epoch: 126 | Iter: 192000 | Total Loss: 0.005546 | Recon Loss: 0.004686 | Commit Loss: 0.001720 | Perplexity: 1811.008270
2025-09-27 22:02:56,681 Stage: Train 0.5 | Epoch: 126 | Iter: 192200 | Total Loss: 0.005439 | Recon Loss: 0.004582 | Commit Loss: 0.001715 | Perplexity: 1810.365959
2025-09-27 22:03:53,654 Stage: Train 0.5 | Epoch: 126 | Iter: 192400 | Total Loss: 0.005425 | Recon Loss: 0.004572 | Commit Loss: 0.001706 | Perplexity: 1809.065549
2025-09-27 22:04:50,972 Stage: Train 0.5 | Epoch: 126 | Iter: 192600 | Total Loss: 0.005509 | Recon Loss: 0.004647 | Commit Loss: 0.001724 | Perplexity: 1812.354907
2025-09-27 22:05:48,581 Stage: Train 0.5 | Epoch: 126 | Iter: 192800 | Total Loss: 0.005496 | Recon Loss: 0.004634 | Commit Loss: 0.001724 | Perplexity: 1815.544240
Trainning Epoch:  38%|███▊      | 127/330 [15:18:29<24:30:18, 434.57s/it]2025-09-27 22:06:46,183 Stage: Train 0.5 | Epoch: 127 | Iter: 193000 | Total Loss: 0.005496 | Recon Loss: 0.004629 | Commit Loss: 0.001735 | Perplexity: 1812.369716
2025-09-27 22:07:43,454 Stage: Train 0.5 | Epoch: 127 | Iter: 193200 | Total Loss: 0.005505 | Recon Loss: 0.004644 | Commit Loss: 0.001721 | Perplexity: 1814.655312
2025-09-27 22:08:40,848 Stage: Train 0.5 | Epoch: 127 | Iter: 193400 | Total Loss: 0.005493 | Recon Loss: 0.004635 | Commit Loss: 0.001717 | Perplexity: 1814.770292
2025-09-27 22:09:37,853 Stage: Train 0.5 | Epoch: 127 | Iter: 193600 | Total Loss: 0.005386 | Recon Loss: 0.004528 | Commit Loss: 0.001717 | Perplexity: 1811.736849
2025-09-27 22:10:35,296 Stage: Train 0.5 | Epoch: 127 | Iter: 193800 | Total Loss: 0.005433 | Recon Loss: 0.004575 | Commit Loss: 0.001717 | Perplexity: 1812.474500
2025-09-27 22:11:32,745 Stage: Train 0.5 | Epoch: 127 | Iter: 194000 | Total Loss: 0.005486 | Recon Loss: 0.004626 | Commit Loss: 0.001719 | Perplexity: 1813.168430
2025-09-27 22:12:30,072 Stage: Train 0.5 | Epoch: 127 | Iter: 194200 | Total Loss: 0.005457 | Recon Loss: 0.004591 | Commit Loss: 0.001731 | Perplexity: 1815.586039
2025-09-27 22:13:27,260 Stage: Train 0.5 | Epoch: 127 | Iter: 194400 | Total Loss: 0.005408 | Recon Loss: 0.004550 | Commit Loss: 0.001716 | Perplexity: 1813.489846
Trainning Epoch:  39%|███▉      | 128/330 [15:25:45<24:23:48, 434.80s/it]2025-09-27 22:14:24,653 Stage: Train 0.5 | Epoch: 128 | Iter: 194600 | Total Loss: 0.005385 | Recon Loss: 0.004523 | Commit Loss: 0.001724 | Perplexity: 1816.937219
2025-09-27 22:15:21,683 Stage: Train 0.5 | Epoch: 128 | Iter: 194800 | Total Loss: 0.005465 | Recon Loss: 0.004605 | Commit Loss: 0.001720 | Perplexity: 1812.034681
2025-09-27 22:16:18,442 Stage: Train 0.5 | Epoch: 128 | Iter: 195000 | Total Loss: 0.005434 | Recon Loss: 0.004575 | Commit Loss: 0.001720 | Perplexity: 1811.012042
2025-09-27 22:16:50,472 Stage: Train 0.5 | Epoch: 128 | Iter: 195200 | Total Loss: 0.005468 | Recon Loss: 0.004601 | Commit Loss: 0.001734 | Perplexity: 1815.355369
2025-09-27 22:17:17,155 Stage: Train 0.5 | Epoch: 128 | Iter: 195400 | Total Loss: 0.005428 | Recon Loss: 0.004575 | Commit Loss: 0.001707 | Perplexity: 1810.114392
2025-09-27 22:17:43,855 Stage: Train 0.5 | Epoch: 128 | Iter: 195600 | Total Loss: 0.005437 | Recon Loss: 0.004580 | Commit Loss: 0.001712 | Perplexity: 1804.598041
2025-09-27 22:18:10,578 Stage: Train 0.5 | Epoch: 128 | Iter: 195800 | Total Loss: 0.005478 | Recon Loss: 0.004612 | Commit Loss: 0.001733 | Perplexity: 1817.053041
Trainning Epoch:  39%|███▉      | 129/330 [15:30:39<21:55:31, 392.69s/it]2025-09-27 22:18:37,472 Stage: Train 0.5 | Epoch: 129 | Iter: 196000 | Total Loss: 0.005497 | Recon Loss: 0.004637 | Commit Loss: 0.001721 | Perplexity: 1810.017639
2025-09-27 22:19:04,266 Stage: Train 0.5 | Epoch: 129 | Iter: 196200 | Total Loss: 0.005425 | Recon Loss: 0.004562 | Commit Loss: 0.001726 | Perplexity: 1815.396542
2025-09-27 22:19:30,857 Stage: Train 0.5 | Epoch: 129 | Iter: 196400 | Total Loss: 0.005452 | Recon Loss: 0.004592 | Commit Loss: 0.001720 | Perplexity: 1812.127776
2025-09-27 22:19:57,557 Stage: Train 0.5 | Epoch: 129 | Iter: 196600 | Total Loss: 0.005409 | Recon Loss: 0.004551 | Commit Loss: 0.001717 | Perplexity: 1814.560253
2025-09-27 22:20:24,284 Stage: Train 0.5 | Epoch: 129 | Iter: 196800 | Total Loss: 0.005494 | Recon Loss: 0.004627 | Commit Loss: 0.001733 | Perplexity: 1812.099846
2025-09-27 22:20:50,946 Stage: Train 0.5 | Epoch: 129 | Iter: 197000 | Total Loss: 0.005407 | Recon Loss: 0.004552 | Commit Loss: 0.001709 | Perplexity: 1811.175824
2025-09-27 22:21:17,682 Stage: Train 0.5 | Epoch: 129 | Iter: 197200 | Total Loss: 0.005363 | Recon Loss: 0.004501 | Commit Loss: 0.001724 | Perplexity: 1817.785058
2025-09-27 22:21:44,426 Stage: Train 0.5 | Epoch: 129 | Iter: 197400 | Total Loss: 0.005493 | Recon Loss: 0.004625 | Commit Loss: 0.001736 | Perplexity: 1814.615817
Trainning Epoch:  39%|███▉      | 130/330 [15:34:02<18:39:16, 335.78s/it]2025-09-27 22:22:11,428 Stage: Train 0.5 | Epoch: 130 | Iter: 197600 | Total Loss: 0.005412 | Recon Loss: 0.004551 | Commit Loss: 0.001722 | Perplexity: 1813.776223
2025-09-27 22:22:38,113 Stage: Train 0.5 | Epoch: 130 | Iter: 197800 | Total Loss: 0.005389 | Recon Loss: 0.004528 | Commit Loss: 0.001723 | Perplexity: 1813.799873
2025-09-27 22:23:04,641 Stage: Train 0.5 | Epoch: 130 | Iter: 198000 | Total Loss: 0.005436 | Recon Loss: 0.004574 | Commit Loss: 0.001722 | Perplexity: 1816.850538
2025-09-27 22:23:31,305 Stage: Train 0.5 | Epoch: 130 | Iter: 198200 | Total Loss: 0.005467 | Recon Loss: 0.004605 | Commit Loss: 0.001725 | Perplexity: 1813.150499
2025-09-27 22:23:58,073 Stage: Train 0.5 | Epoch: 130 | Iter: 198400 | Total Loss: 0.005437 | Recon Loss: 0.004577 | Commit Loss: 0.001721 | Perplexity: 1808.077131
2025-09-27 22:24:24,836 Stage: Train 0.5 | Epoch: 130 | Iter: 198600 | Total Loss: 0.005418 | Recon Loss: 0.004557 | Commit Loss: 0.001724 | Perplexity: 1813.558746
2025-09-27 22:24:51,561 Stage: Train 0.5 | Epoch: 130 | Iter: 198800 | Total Loss: 0.005418 | Recon Loss: 0.004554 | Commit Loss: 0.001728 | Perplexity: 1816.713792
Trainning Epoch:  40%|███▉      | 131/330 [15:37:25<16:21:33, 295.95s/it]2025-09-27 22:25:18,452 Stage: Train 0.5 | Epoch: 131 | Iter: 199000 | Total Loss: 0.005449 | Recon Loss: 0.004588 | Commit Loss: 0.001722 | Perplexity: 1812.427008
2025-09-27 22:25:45,283 Stage: Train 0.5 | Epoch: 131 | Iter: 199200 | Total Loss: 0.005429 | Recon Loss: 0.004568 | Commit Loss: 0.001720 | Perplexity: 1814.243621
2025-09-27 22:26:11,999 Stage: Train 0.5 | Epoch: 131 | Iter: 199400 | Total Loss: 0.005391 | Recon Loss: 0.004530 | Commit Loss: 0.001721 | Perplexity: 1816.063091
2025-09-27 22:26:38,678 Stage: Train 0.5 | Epoch: 131 | Iter: 199600 | Total Loss: 0.005419 | Recon Loss: 0.004559 | Commit Loss: 0.001721 | Perplexity: 1815.621484
2025-09-27 22:27:05,424 Stage: Train 0.5 | Epoch: 131 | Iter: 199800 | Total Loss: 0.005456 | Recon Loss: 0.004594 | Commit Loss: 0.001723 | Perplexity: 1812.890632
2025-09-27 22:27:32,046 Stage: Train 0.5 | Epoch: 131 | Iter: 200000 | Total Loss: 0.005479 | Recon Loss: 0.004612 | Commit Loss: 0.001733 | Perplexity: 1815.611483
2025-09-27 22:27:32,047 Saving model at iteration 200000
2025-09-27 22:27:32,511 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000
2025-09-27 22:27:32,989 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/model.safetensors
2025-09-27 22:27:33,523 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/optimizer.bin
2025-09-27 22:27:33,523 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/scheduler.bin
2025-09-27 22:27:33,523 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/sampler.bin
2025-09-27 22:27:33,524 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_132_step_200000/random_states_0.pkl
2025-09-27 22:28:00,268 Stage: Train 0.5 | Epoch: 131 | Iter: 200200 | Total Loss: 0.005378 | Recon Loss: 0.004522 | Commit Loss: 0.001713 | Perplexity: 1808.483510
2025-09-27 22:28:26,896 Stage: Train 0.5 | Epoch: 131 | Iter: 200400 | Total Loss: 0.005441 | Recon Loss: 0.004581 | Commit Loss: 0.001720 | Perplexity: 1813.496187
Trainning Epoch:  40%|████      | 132/330 [15:40:50<14:46:10, 268.54s/it]2025-09-27 22:28:53,832 Stage: Train 0.5 | Epoch: 132 | Iter: 200600 | Total Loss: 0.005404 | Recon Loss: 0.004548 | Commit Loss: 0.001712 | Perplexity: 1809.421294
2025-09-27 22:29:20,517 Stage: Train 0.5 | Epoch: 132 | Iter: 200800 | Total Loss: 0.005365 | Recon Loss: 0.004509 | Commit Loss: 0.001712 | Perplexity: 1812.160305
2025-09-27 22:29:47,255 Stage: Train 0.5 | Epoch: 132 | Iter: 201000 | Total Loss: 0.005512 | Recon Loss: 0.004645 | Commit Loss: 0.001732 | Perplexity: 1815.176846
2025-09-27 22:30:13,979 Stage: Train 0.5 | Epoch: 132 | Iter: 201200 | Total Loss: 0.005406 | Recon Loss: 0.004542 | Commit Loss: 0.001728 | Perplexity: 1815.150783
2025-09-27 22:30:40,780 Stage: Train 0.5 | Epoch: 132 | Iter: 201400 | Total Loss: 0.005386 | Recon Loss: 0.004526 | Commit Loss: 0.001719 | Perplexity: 1812.156325
2025-09-27 22:31:07,508 Stage: Train 0.5 | Epoch: 132 | Iter: 201600 | Total Loss: 0.005445 | Recon Loss: 0.004584 | Commit Loss: 0.001721 | Perplexity: 1812.936833
2025-09-27 22:31:34,181 Stage: Train 0.5 | Epoch: 132 | Iter: 201800 | Total Loss: 0.005374 | Recon Loss: 0.004514 | Commit Loss: 0.001720 | Perplexity: 1812.066894
2025-09-27 22:32:00,884 Stage: Train 0.5 | Epoch: 132 | Iter: 202000 | Total Loss: 0.005490 | Recon Loss: 0.004623 | Commit Loss: 0.001734 | Perplexity: 1815.242875
Trainning Epoch:  40%|████      | 133/330 [15:44:13<13:37:17, 248.92s/it]2025-09-27 22:32:27,674 Stage: Train 0.5 | Epoch: 133 | Iter: 202200 | Total Loss: 0.005353 | Recon Loss: 0.004493 | Commit Loss: 0.001720 | Perplexity: 1813.796980
2025-09-27 22:32:54,302 Stage: Train 0.5 | Epoch: 133 | Iter: 202400 | Total Loss: 0.005391 | Recon Loss: 0.004534 | Commit Loss: 0.001715 | Perplexity: 1813.450615
2025-09-27 22:33:21,118 Stage: Train 0.5 | Epoch: 133 | Iter: 202600 | Total Loss: 0.005372 | Recon Loss: 0.004512 | Commit Loss: 0.001720 | Perplexity: 1812.505446
2025-09-27 22:33:47,796 Stage: Train 0.5 | Epoch: 133 | Iter: 202800 | Total Loss: 0.005488 | Recon Loss: 0.004620 | Commit Loss: 0.001735 | Perplexity: 1816.399985
2025-09-27 22:34:14,602 Stage: Train 0.5 | Epoch: 133 | Iter: 203000 | Total Loss: 0.005405 | Recon Loss: 0.004543 | Commit Loss: 0.001725 | Perplexity: 1813.417842
2025-09-27 22:34:41,318 Stage: Train 0.5 | Epoch: 133 | Iter: 203200 | Total Loss: 0.005400 | Recon Loss: 0.004542 | Commit Loss: 0.001716 | Perplexity: 1811.642660
2025-09-27 22:35:07,998 Stage: Train 0.5 | Epoch: 133 | Iter: 203400 | Total Loss: 0.005473 | Recon Loss: 0.004607 | Commit Loss: 0.001732 | Perplexity: 1811.668555
Trainning Epoch:  41%|████      | 134/330 [15:47:36<12:48:10, 235.16s/it]2025-09-27 22:35:34,964 Stage: Train 0.5 | Epoch: 134 | Iter: 203600 | Total Loss: 0.005399 | Recon Loss: 0.004542 | Commit Loss: 0.001715 | Perplexity: 1809.870095
2025-09-27 22:36:01,625 Stage: Train 0.5 | Epoch: 134 | Iter: 203800 | Total Loss: 0.005352 | Recon Loss: 0.004490 | Commit Loss: 0.001724 | Perplexity: 1816.851017
2025-09-27 22:36:28,389 Stage: Train 0.5 | Epoch: 134 | Iter: 204000 | Total Loss: 0.005379 | Recon Loss: 0.004518 | Commit Loss: 0.001722 | Perplexity: 1810.775795
2025-09-27 22:36:55,048 Stage: Train 0.5 | Epoch: 134 | Iter: 204200 | Total Loss: 0.005349 | Recon Loss: 0.004487 | Commit Loss: 0.001724 | Perplexity: 1813.754059
2025-09-27 22:37:21,784 Stage: Train 0.5 | Epoch: 134 | Iter: 204400 | Total Loss: 0.005450 | Recon Loss: 0.004580 | Commit Loss: 0.001740 | Perplexity: 1815.221993
2025-09-27 22:37:48,458 Stage: Train 0.5 | Epoch: 134 | Iter: 204600 | Total Loss: 0.005346 | Recon Loss: 0.004488 | Commit Loss: 0.001716 | Perplexity: 1810.636835
2025-09-27 22:38:15,156 Stage: Train 0.5 | Epoch: 134 | Iter: 204800 | Total Loss: 0.005400 | Recon Loss: 0.004539 | Commit Loss: 0.001722 | Perplexity: 1815.313813
2025-09-27 22:38:41,893 Stage: Train 0.5 | Epoch: 134 | Iter: 205000 | Total Loss: 0.005409 | Recon Loss: 0.004548 | Commit Loss: 0.001724 | Perplexity: 1813.902016
Trainning Epoch:  41%|████      | 135/330 [15:50:59<12:12:55, 225.52s/it]2025-09-27 22:39:08,804 Stage: Train 0.5 | Epoch: 135 | Iter: 205200 | Total Loss: 0.005380 | Recon Loss: 0.004517 | Commit Loss: 0.001726 | Perplexity: 1813.341313
2025-09-27 22:39:35,504 Stage: Train 0.5 | Epoch: 135 | Iter: 205400 | Total Loss: 0.005374 | Recon Loss: 0.004517 | Commit Loss: 0.001715 | Perplexity: 1809.688625
2025-09-27 22:40:02,172 Stage: Train 0.5 | Epoch: 135 | Iter: 205600 | Total Loss: 0.005355 | Recon Loss: 0.004492 | Commit Loss: 0.001726 | Perplexity: 1816.097122
2025-09-27 22:40:28,854 Stage: Train 0.5 | Epoch: 135 | Iter: 205800 | Total Loss: 0.005400 | Recon Loss: 0.004538 | Commit Loss: 0.001723 | Perplexity: 1815.292954
2025-09-27 22:40:55,703 Stage: Train 0.5 | Epoch: 135 | Iter: 206000 | Total Loss: 0.005419 | Recon Loss: 0.004559 | Commit Loss: 0.001720 | Perplexity: 1812.602189
2025-09-27 22:41:22,468 Stage: Train 0.5 | Epoch: 135 | Iter: 206200 | Total Loss: 0.005346 | Recon Loss: 0.004481 | Commit Loss: 0.001729 | Perplexity: 1816.116138
2025-09-27 22:41:49,155 Stage: Train 0.5 | Epoch: 135 | Iter: 206400 | Total Loss: 0.005379 | Recon Loss: 0.004515 | Commit Loss: 0.001729 | Perplexity: 1816.434139
Trainning Epoch:  41%|████      | 136/330 [15:54:22<11:47:33, 218.83s/it]2025-09-27 22:42:16,149 Stage: Train 0.5 | Epoch: 136 | Iter: 206600 | Total Loss: 0.005426 | Recon Loss: 0.004559 | Commit Loss: 0.001733 | Perplexity: 1818.920014
2025-09-27 22:42:42,824 Stage: Train 0.5 | Epoch: 136 | Iter: 206800 | Total Loss: 0.005361 | Recon Loss: 0.004501 | Commit Loss: 0.001720 | Perplexity: 1816.175659
2025-09-27 22:43:09,590 Stage: Train 0.5 | Epoch: 136 | Iter: 207000 | Total Loss: 0.005326 | Recon Loss: 0.004467 | Commit Loss: 0.001718 | Perplexity: 1815.426765
2025-09-27 22:43:36,356 Stage: Train 0.5 | Epoch: 136 | Iter: 207200 | Total Loss: 0.005424 | Recon Loss: 0.004564 | Commit Loss: 0.001721 | Perplexity: 1813.422719
2025-09-27 22:44:03,086 Stage: Train 0.5 | Epoch: 136 | Iter: 207400 | Total Loss: 0.005374 | Recon Loss: 0.004515 | Commit Loss: 0.001719 | Perplexity: 1810.605576
2025-09-27 22:44:29,941 Stage: Train 0.5 | Epoch: 136 | Iter: 207600 | Total Loss: 0.005352 | Recon Loss: 0.004493 | Commit Loss: 0.001717 | Perplexity: 1815.821957
2025-09-27 22:44:56,640 Stage: Train 0.5 | Epoch: 136 | Iter: 207800 | Total Loss: 0.005390 | Recon Loss: 0.004530 | Commit Loss: 0.001721 | Perplexity: 1813.210998
2025-09-27 22:45:23,182 Stage: Train 0.5 | Epoch: 136 | Iter: 208000 | Total Loss: 0.005410 | Recon Loss: 0.004541 | Commit Loss: 0.001739 | Perplexity: 1817.574638
Trainning Epoch:  42%|████▏     | 137/330 [15:57:45<11:28:41, 214.10s/it]2025-09-27 22:45:50,074 Stage: Train 0.5 | Epoch: 137 | Iter: 208200 | Total Loss: 0.005414 | Recon Loss: 0.004550 | Commit Loss: 0.001729 | Perplexity: 1816.449745
2025-09-27 22:46:16,749 Stage: Train 0.5 | Epoch: 137 | Iter: 208400 | Total Loss: 0.005360 | Recon Loss: 0.004503 | Commit Loss: 0.001715 | Perplexity: 1814.580802
2025-09-27 22:46:43,533 Stage: Train 0.5 | Epoch: 137 | Iter: 208600 | Total Loss: 0.005396 | Recon Loss: 0.004530 | Commit Loss: 0.001734 | Perplexity: 1820.007153
2025-09-27 22:47:10,340 Stage: Train 0.5 | Epoch: 137 | Iter: 208800 | Total Loss: 0.005377 | Recon Loss: 0.004517 | Commit Loss: 0.001720 | Perplexity: 1812.670713
2025-09-27 22:47:36,998 Stage: Train 0.5 | Epoch: 137 | Iter: 209000 | Total Loss: 0.005391 | Recon Loss: 0.004534 | Commit Loss: 0.001715 | Perplexity: 1810.896422
2025-09-27 22:48:03,610 Stage: Train 0.5 | Epoch: 137 | Iter: 209200 | Total Loss: 0.005357 | Recon Loss: 0.004493 | Commit Loss: 0.001727 | Perplexity: 1817.911572
2025-09-27 22:48:30,371 Stage: Train 0.5 | Epoch: 137 | Iter: 209400 | Total Loss: 0.005372 | Recon Loss: 0.004508 | Commit Loss: 0.001729 | Perplexity: 1812.052574
2025-09-27 22:48:57,066 Stage: Train 0.5 | Epoch: 137 | Iter: 209600 | Total Loss: 0.005422 | Recon Loss: 0.004557 | Commit Loss: 0.001731 | Perplexity: 1815.753418
Trainning Epoch:  42%|████▏     | 138/330 [16:01:08<11:14:35, 210.81s/it]2025-09-27 22:49:24,036 Stage: Train 0.5 | Epoch: 138 | Iter: 209800 | Total Loss: 0.005350 | Recon Loss: 0.004491 | Commit Loss: 0.001720 | Perplexity: 1813.196708
2025-09-27 22:49:50,694 Stage: Train 0.5 | Epoch: 138 | Iter: 210000 | Total Loss: 0.005369 | Recon Loss: 0.004512 | Commit Loss: 0.001713 | Perplexity: 1814.652242
2025-09-27 22:50:17,483 Stage: Train 0.5 | Epoch: 138 | Iter: 210200 | Total Loss: 0.005334 | Recon Loss: 0.004466 | Commit Loss: 0.001736 | Perplexity: 1818.935438
2025-09-27 22:50:44,125 Stage: Train 0.5 | Epoch: 138 | Iter: 210400 | Total Loss: 0.005349 | Recon Loss: 0.004485 | Commit Loss: 0.001726 | Perplexity: 1816.269755
2025-09-27 22:51:10,769 Stage: Train 0.5 | Epoch: 138 | Iter: 210600 | Total Loss: 0.005373 | Recon Loss: 0.004511 | Commit Loss: 0.001724 | Perplexity: 1815.117067
2025-09-27 22:51:37,544 Stage: Train 0.5 | Epoch: 138 | Iter: 210800 | Total Loss: 0.005359 | Recon Loss: 0.004505 | Commit Loss: 0.001709 | Perplexity: 1811.719284
2025-09-27 22:52:04,310 Stage: Train 0.5 | Epoch: 138 | Iter: 211000 | Total Loss: 0.005360 | Recon Loss: 0.004496 | Commit Loss: 0.001728 | Perplexity: 1812.643926
Trainning Epoch:  42%|████▏     | 139/330 [16:04:31<11:03:49, 208.53s/it]2025-09-27 22:52:31,313 Stage: Train 0.5 | Epoch: 139 | Iter: 211200 | Total Loss: 0.005344 | Recon Loss: 0.004480 | Commit Loss: 0.001727 | Perplexity: 1815.147027
2025-09-27 22:52:58,039 Stage: Train 0.5 | Epoch: 139 | Iter: 211400 | Total Loss: 0.005332 | Recon Loss: 0.004473 | Commit Loss: 0.001717 | Perplexity: 1815.415607
2025-09-27 22:53:24,808 Stage: Train 0.5 | Epoch: 139 | Iter: 211600 | Total Loss: 0.005273 | Recon Loss: 0.004415 | Commit Loss: 0.001716 | Perplexity: 1811.414727
2025-09-27 22:53:51,487 Stage: Train 0.5 | Epoch: 139 | Iter: 211800 | Total Loss: 0.005433 | Recon Loss: 0.004570 | Commit Loss: 0.001727 | Perplexity: 1818.815323
2025-09-27 22:54:18,096 Stage: Train 0.5 | Epoch: 139 | Iter: 212000 | Total Loss: 0.005290 | Recon Loss: 0.004431 | Commit Loss: 0.001718 | Perplexity: 1819.371503
2025-09-27 22:54:44,537 Stage: Train 0.5 | Epoch: 139 | Iter: 212200 | Total Loss: 0.005381 | Recon Loss: 0.004507 | Commit Loss: 0.001747 | Perplexity: 1822.948124
2025-09-27 22:55:11,150 Stage: Train 0.5 | Epoch: 139 | Iter: 212400 | Total Loss: 0.005373 | Recon Loss: 0.004515 | Commit Loss: 0.001715 | Perplexity: 1814.260051
2025-09-27 22:55:37,951 Stage: Train 0.5 | Epoch: 139 | Iter: 212600 | Total Loss: 0.005314 | Recon Loss: 0.004447 | Commit Loss: 0.001733 | Perplexity: 1817.653958
Trainning Epoch:  42%|████▏     | 140/330 [16:07:54<10:54:54, 206.81s/it]2025-09-27 22:56:04,937 Stage: Train 0.5 | Epoch: 140 | Iter: 212800 | Total Loss: 0.005287 | Recon Loss: 0.004424 | Commit Loss: 0.001726 | Perplexity: 1818.636476
2025-09-27 22:56:31,567 Stage: Train 0.5 | Epoch: 140 | Iter: 213000 | Total Loss: 0.005371 | Recon Loss: 0.004516 | Commit Loss: 0.001710 | Perplexity: 1812.911111
2025-09-27 22:56:58,409 Stage: Train 0.5 | Epoch: 140 | Iter: 213200 | Total Loss: 0.005369 | Recon Loss: 0.004506 | Commit Loss: 0.001725 | Perplexity: 1817.855283
2025-09-27 22:57:25,202 Stage: Train 0.5 | Epoch: 140 | Iter: 213400 | Total Loss: 0.005380 | Recon Loss: 0.004514 | Commit Loss: 0.001731 | Perplexity: 1823.335001
2025-09-27 22:57:51,926 Stage: Train 0.5 | Epoch: 140 | Iter: 213600 | Total Loss: 0.005264 | Recon Loss: 0.004410 | Commit Loss: 0.001706 | Perplexity: 1811.320173
2025-09-27 22:58:18,693 Stage: Train 0.5 | Epoch: 140 | Iter: 213800 | Total Loss: 0.005317 | Recon Loss: 0.004455 | Commit Loss: 0.001723 | Perplexity: 1816.967919
2025-09-27 22:58:45,498 Stage: Train 0.5 | Epoch: 140 | Iter: 214000 | Total Loss: 0.005408 | Recon Loss: 0.004549 | Commit Loss: 0.001719 | Perplexity: 1816.289784
Trainning Epoch:  43%|████▎     | 141/330 [16:11:18<10:48:16, 205.80s/it]2025-09-27 22:59:12,477 Stage: Train 0.5 | Epoch: 141 | Iter: 214200 | Total Loss: 0.005340 | Recon Loss: 0.004480 | Commit Loss: 0.001719 | Perplexity: 1821.093094
2025-09-27 22:59:39,284 Stage: Train 0.5 | Epoch: 141 | Iter: 214400 | Total Loss: 0.005303 | Recon Loss: 0.004452 | Commit Loss: 0.001701 | Perplexity: 1813.388241
2025-09-27 23:00:05,946 Stage: Train 0.5 | Epoch: 141 | Iter: 214600 | Total Loss: 0.005344 | Recon Loss: 0.004489 | Commit Loss: 0.001712 | Perplexity: 1815.842106
2025-09-27 23:00:32,616 Stage: Train 0.5 | Epoch: 141 | Iter: 214800 | Total Loss: 0.005331 | Recon Loss: 0.004469 | Commit Loss: 0.001724 | Perplexity: 1819.938819
2025-09-27 23:00:59,317 Stage: Train 0.5 | Epoch: 141 | Iter: 215000 | Total Loss: 0.005311 | Recon Loss: 0.004449 | Commit Loss: 0.001723 | Perplexity: 1817.110026
2025-09-27 23:01:26,094 Stage: Train 0.5 | Epoch: 141 | Iter: 215200 | Total Loss: 0.005343 | Recon Loss: 0.004483 | Commit Loss: 0.001722 | Perplexity: 1821.935815
2025-09-27 23:01:52,794 Stage: Train 0.5 | Epoch: 141 | Iter: 215400 | Total Loss: 0.005357 | Recon Loss: 0.004491 | Commit Loss: 0.001732 | Perplexity: 1824.571210
2025-09-27 23:02:19,520 Stage: Train 0.5 | Epoch: 141 | Iter: 215600 | Total Loss: 0.005265 | Recon Loss: 0.004410 | Commit Loss: 0.001712 | Perplexity: 1816.741559
Trainning Epoch:  43%|████▎     | 142/330 [16:14:41<10:42:20, 205.00s/it]2025-09-27 23:02:46,411 Stage: Train 0.5 | Epoch: 142 | Iter: 215800 | Total Loss: 0.005369 | Recon Loss: 0.004515 | Commit Loss: 0.001708 | Perplexity: 1816.340296
2025-09-27 23:03:13,163 Stage: Train 0.5 | Epoch: 142 | Iter: 216000 | Total Loss: 0.005334 | Recon Loss: 0.004471 | Commit Loss: 0.001727 | Perplexity: 1818.971315
2025-09-27 23:03:39,922 Stage: Train 0.5 | Epoch: 142 | Iter: 216200 | Total Loss: 0.005298 | Recon Loss: 0.004442 | Commit Loss: 0.001711 | Perplexity: 1819.893627
2025-09-27 23:04:06,654 Stage: Train 0.5 | Epoch: 142 | Iter: 216400 | Total Loss: 0.005314 | Recon Loss: 0.004456 | Commit Loss: 0.001715 | Perplexity: 1817.556645
2025-09-27 23:04:33,250 Stage: Train 0.5 | Epoch: 142 | Iter: 216600 | Total Loss: 0.005321 | Recon Loss: 0.004461 | Commit Loss: 0.001720 | Perplexity: 1823.114991
2025-09-27 23:04:59,864 Stage: Train 0.5 | Epoch: 142 | Iter: 216800 | Total Loss: 0.005322 | Recon Loss: 0.004462 | Commit Loss: 0.001722 | Perplexity: 1823.513600
2025-09-27 23:05:26,582 Stage: Train 0.5 | Epoch: 142 | Iter: 217000 | Total Loss: 0.005298 | Recon Loss: 0.004441 | Commit Loss: 0.001714 | Perplexity: 1819.042501
2025-09-27 23:05:53,268 Stage: Train 0.5 | Epoch: 142 | Iter: 217200 | Total Loss: 0.005298 | Recon Loss: 0.004436 | Commit Loss: 0.001725 | Perplexity: 1821.640934
Trainning Epoch:  43%|████▎     | 143/330 [16:18:04<10:37:00, 204.39s/it]2025-09-27 23:06:20,237 Stage: Train 0.5 | Epoch: 143 | Iter: 217400 | Total Loss: 0.005317 | Recon Loss: 0.004457 | Commit Loss: 0.001720 | Perplexity: 1821.865942
2025-09-27 23:06:46,986 Stage: Train 0.5 | Epoch: 143 | Iter: 217600 | Total Loss: 0.005324 | Recon Loss: 0.004465 | Commit Loss: 0.001718 | Perplexity: 1823.478727
2025-09-27 23:07:13,742 Stage: Train 0.5 | Epoch: 143 | Iter: 217800 | Total Loss: 0.005271 | Recon Loss: 0.004414 | Commit Loss: 0.001713 | Perplexity: 1818.155956
2025-09-27 23:07:40,565 Stage: Train 0.5 | Epoch: 143 | Iter: 218000 | Total Loss: 0.005273 | Recon Loss: 0.004419 | Commit Loss: 0.001708 | Perplexity: 1821.868497
2025-09-27 23:08:07,303 Stage: Train 0.5 | Epoch: 143 | Iter: 218200 | Total Loss: 0.005330 | Recon Loss: 0.004472 | Commit Loss: 0.001715 | Perplexity: 1819.823569
2025-09-27 23:08:34,058 Stage: Train 0.5 | Epoch: 143 | Iter: 218400 | Total Loss: 0.005305 | Recon Loss: 0.004446 | Commit Loss: 0.001717 | Perplexity: 1822.445384
2025-09-27 23:09:00,823 Stage: Train 0.5 | Epoch: 143 | Iter: 218600 | Total Loss: 0.005288 | Recon Loss: 0.004429 | Commit Loss: 0.001719 | Perplexity: 1822.068380
Trainning Epoch:  44%|████▎     | 144/330 [16:21:27<10:32:46, 204.12s/it]2025-09-27 23:09:27,841 Stage: Train 0.5 | Epoch: 144 | Iter: 218800 | Total Loss: 0.005322 | Recon Loss: 0.004461 | Commit Loss: 0.001723 | Perplexity: 1824.506206
2025-09-27 23:09:54,556 Stage: Train 0.5 | Epoch: 144 | Iter: 219000 | Total Loss: 0.005278 | Recon Loss: 0.004422 | Commit Loss: 0.001712 | Perplexity: 1820.841406
2025-09-27 23:10:21,160 Stage: Train 0.5 | Epoch: 144 | Iter: 219200 | Total Loss: 0.005305 | Recon Loss: 0.004451 | Commit Loss: 0.001708 | Perplexity: 1818.876777
2025-09-27 23:10:47,781 Stage: Train 0.5 | Epoch: 144 | Iter: 219400 | Total Loss: 0.005286 | Recon Loss: 0.004423 | Commit Loss: 0.001725 | Perplexity: 1824.234457
2025-09-27 23:11:14,496 Stage: Train 0.5 | Epoch: 144 | Iter: 219600 | Total Loss: 0.005334 | Recon Loss: 0.004479 | Commit Loss: 0.001711 | Perplexity: 1822.599861
2025-09-27 23:11:41,079 Stage: Train 0.5 | Epoch: 144 | Iter: 219800 | Total Loss: 0.005308 | Recon Loss: 0.004444 | Commit Loss: 0.001730 | Perplexity: 1826.312982
2025-09-27 23:12:07,909 Stage: Train 0.5 | Epoch: 144 | Iter: 220000 | Total Loss: 0.005324 | Recon Loss: 0.004464 | Commit Loss: 0.001718 | Perplexity: 1820.963962
2025-09-27 23:12:07,909 Saving model at iteration 220000
2025-09-27 23:12:08,105 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000
2025-09-27 23:12:08,557 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/model.safetensors
2025-09-27 23:12:09,041 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/optimizer.bin
2025-09-27 23:12:09,041 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/scheduler.bin
2025-09-27 23:12:09,042 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/sampler.bin
2025-09-27 23:12:09,042 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_145_step_220000/random_states_0.pkl
2025-09-27 23:12:35,766 Stage: Train 0.5 | Epoch: 144 | Iter: 220200 | Total Loss: 0.005301 | Recon Loss: 0.004443 | Commit Loss: 0.001715 | Perplexity: 1823.476473
Trainning Epoch:  44%|████▍     | 145/330 [16:24:51<10:29:19, 204.10s/it]2025-09-27 23:13:02,644 Stage: Train 0.5 | Epoch: 145 | Iter: 220400 | Total Loss: 0.005289 | Recon Loss: 0.004428 | Commit Loss: 0.001721 | Perplexity: 1824.284424
2025-09-27 23:13:29,383 Stage: Train 0.5 | Epoch: 145 | Iter: 220600 | Total Loss: 0.005312 | Recon Loss: 0.004454 | Commit Loss: 0.001715 | Perplexity: 1824.030349
2025-09-27 23:13:56,055 Stage: Train 0.5 | Epoch: 145 | Iter: 220800 | Total Loss: 0.005365 | Recon Loss: 0.004511 | Commit Loss: 0.001709 | Perplexity: 1820.652194
2025-09-27 23:14:22,773 Stage: Train 0.5 | Epoch: 145 | Iter: 221000 | Total Loss: 0.005292 | Recon Loss: 0.004432 | Commit Loss: 0.001720 | Perplexity: 1824.504013
2025-09-27 23:14:49,393 Stage: Train 0.5 | Epoch: 145 | Iter: 221200 | Total Loss: 0.005240 | Recon Loss: 0.004383 | Commit Loss: 0.001714 | Perplexity: 1824.376030
2025-09-27 23:15:16,092 Stage: Train 0.5 | Epoch: 145 | Iter: 221400 | Total Loss: 0.005271 | Recon Loss: 0.004415 | Commit Loss: 0.001712 | Perplexity: 1818.163387
2025-09-27 23:15:42,841 Stage: Train 0.5 | Epoch: 145 | Iter: 221600 | Total Loss: 0.005237 | Recon Loss: 0.004380 | Commit Loss: 0.001714 | Perplexity: 1824.081070
Trainning Epoch:  44%|████▍     | 146/330 [16:28:14<10:24:50, 203.76s/it]2025-09-27 23:16:09,708 Stage: Train 0.5 | Epoch: 146 | Iter: 221800 | Total Loss: 0.005233 | Recon Loss: 0.004373 | Commit Loss: 0.001719 | Perplexity: 1825.509669
2025-09-27 23:16:36,374 Stage: Train 0.5 | Epoch: 146 | Iter: 222000 | Total Loss: 0.005335 | Recon Loss: 0.004482 | Commit Loss: 0.001707 | Perplexity: 1824.768226
2025-09-27 23:17:03,049 Stage: Train 0.5 | Epoch: 146 | Iter: 222200 | Total Loss: 0.005282 | Recon Loss: 0.004426 | Commit Loss: 0.001713 | Perplexity: 1824.511999
2025-09-27 23:17:29,712 Stage: Train 0.5 | Epoch: 146 | Iter: 222400 | Total Loss: 0.005232 | Recon Loss: 0.004372 | Commit Loss: 0.001721 | Perplexity: 1823.659783
2025-09-27 23:17:56,364 Stage: Train 0.5 | Epoch: 146 | Iter: 222600 | Total Loss: 0.005322 | Recon Loss: 0.004456 | Commit Loss: 0.001731 | Perplexity: 1828.651636
2025-09-27 23:18:23,074 Stage: Train 0.5 | Epoch: 146 | Iter: 222800 | Total Loss: 0.005323 | Recon Loss: 0.004466 | Commit Loss: 0.001713 | Perplexity: 1821.925538
2025-09-27 23:18:49,800 Stage: Train 0.5 | Epoch: 146 | Iter: 223000 | Total Loss: 0.005250 | Recon Loss: 0.004394 | Commit Loss: 0.001711 | Perplexity: 1822.445547
2025-09-27 23:19:16,572 Stage: Train 0.5 | Epoch: 146 | Iter: 223200 | Total Loss: 0.005276 | Recon Loss: 0.004421 | Commit Loss: 0.001710 | Perplexity: 1820.475764
Trainning Epoch:  45%|████▍     | 147/330 [16:31:37<10:20:41, 203.51s/it]2025-09-27 23:19:43,493 Stage: Train 0.5 | Epoch: 147 | Iter: 223400 | Total Loss: 0.005273 | Recon Loss: 0.004413 | Commit Loss: 0.001719 | Perplexity: 1823.866607
2025-09-27 23:20:10,167 Stage: Train 0.5 | Epoch: 147 | Iter: 223600 | Total Loss: 0.005317 | Recon Loss: 0.004464 | Commit Loss: 0.001705 | Perplexity: 1821.293685
2025-09-27 23:20:36,802 Stage: Train 0.5 | Epoch: 147 | Iter: 223800 | Total Loss: 0.005324 | Recon Loss: 0.004467 | Commit Loss: 0.001714 | Perplexity: 1826.199125
2025-09-27 23:21:03,473 Stage: Train 0.5 | Epoch: 147 | Iter: 224000 | Total Loss: 0.005239 | Recon Loss: 0.004375 | Commit Loss: 0.001728 | Perplexity: 1825.074820
2025-09-27 23:21:29,992 Stage: Train 0.5 | Epoch: 147 | Iter: 224200 | Total Loss: 0.005274 | Recon Loss: 0.004417 | Commit Loss: 0.001714 | Perplexity: 1825.008228
2025-09-27 23:21:56,691 Stage: Train 0.5 | Epoch: 147 | Iter: 224400 | Total Loss: 0.005282 | Recon Loss: 0.004423 | Commit Loss: 0.001719 | Perplexity: 1826.203138
2025-09-27 23:22:23,320 Stage: Train 0.5 | Epoch: 147 | Iter: 224600 | Total Loss: 0.005255 | Recon Loss: 0.004400 | Commit Loss: 0.001710 | Perplexity: 1818.083444
2025-09-27 23:22:50,002 Stage: Train 0.5 | Epoch: 147 | Iter: 224800 | Total Loss: 0.005293 | Recon Loss: 0.004437 | Commit Loss: 0.001712 | Perplexity: 1820.234555
Trainning Epoch:  45%|████▍     | 148/330 [16:35:00<10:16:30, 203.24s/it]2025-09-27 23:23:16,865 Stage: Train 0.5 | Epoch: 148 | Iter: 225000 | Total Loss: 0.005289 | Recon Loss: 0.004432 | Commit Loss: 0.001713 | Perplexity: 1821.694989
2025-09-27 23:23:43,685 Stage: Train 0.5 | Epoch: 148 | Iter: 225200 | Total Loss: 0.005256 | Recon Loss: 0.004402 | Commit Loss: 0.001707 | Perplexity: 1822.652635
2025-09-27 23:24:10,345 Stage: Train 0.5 | Epoch: 148 | Iter: 225400 | Total Loss: 0.005276 | Recon Loss: 0.004418 | Commit Loss: 0.001716 | Perplexity: 1829.481130
2025-09-27 23:24:37,101 Stage: Train 0.5 | Epoch: 148 | Iter: 225600 | Total Loss: 0.005288 | Recon Loss: 0.004424 | Commit Loss: 0.001728 | Perplexity: 1826.797149
2025-09-27 23:25:03,829 Stage: Train 0.5 | Epoch: 148 | Iter: 225800 | Total Loss: 0.005199 | Recon Loss: 0.004340 | Commit Loss: 0.001716 | Perplexity: 1825.474680
2025-09-27 23:25:30,528 Stage: Train 0.5 | Epoch: 148 | Iter: 226000 | Total Loss: 0.005271 | Recon Loss: 0.004417 | Commit Loss: 0.001707 | Perplexity: 1825.372880
2025-09-27 23:25:57,271 Stage: Train 0.5 | Epoch: 148 | Iter: 226200 | Total Loss: 0.005224 | Recon Loss: 0.004369 | Commit Loss: 0.001711 | Perplexity: 1827.687791
Trainning Epoch:  45%|████▌     | 149/330 [16:38:23<10:13:01, 203.21s/it]2025-09-27 23:26:24,132 Stage: Train 0.5 | Epoch: 149 | Iter: 226400 | Total Loss: 0.005291 | Recon Loss: 0.004433 | Commit Loss: 0.001717 | Perplexity: 1823.767738
2025-09-27 23:26:50,846 Stage: Train 0.5 | Epoch: 149 | Iter: 226600 | Total Loss: 0.005189 | Recon Loss: 0.004334 | Commit Loss: 0.001711 | Perplexity: 1828.645075
2025-09-27 23:27:17,620 Stage: Train 0.5 | Epoch: 149 | Iter: 226800 | Total Loss: 0.005250 | Recon Loss: 0.004395 | Commit Loss: 0.001710 | Perplexity: 1821.250179
2025-09-27 23:27:44,240 Stage: Train 0.5 | Epoch: 149 | Iter: 227000 | Total Loss: 0.005251 | Recon Loss: 0.004399 | Commit Loss: 0.001704 | Perplexity: 1821.364703
2025-09-27 23:28:11,040 Stage: Train 0.5 | Epoch: 149 | Iter: 227200 | Total Loss: 0.005253 | Recon Loss: 0.004391 | Commit Loss: 0.001723 | Perplexity: 1826.017076
2025-09-27 23:28:37,761 Stage: Train 0.5 | Epoch: 149 | Iter: 227400 | Total Loss: 0.005358 | Recon Loss: 0.004496 | Commit Loss: 0.001723 | Perplexity: 1826.263163
2025-09-27 23:29:04,392 Stage: Train 0.5 | Epoch: 149 | Iter: 227600 | Total Loss: 0.005223 | Recon Loss: 0.004371 | Commit Loss: 0.001704 | Perplexity: 1822.039133
2025-09-27 23:29:31,129 Stage: Train 0.5 | Epoch: 149 | Iter: 227800 | Total Loss: 0.005218 | Recon Loss: 0.004359 | Commit Loss: 0.001718 | Perplexity: 1827.073397
Trainning Epoch:  45%|████▌     | 150/330 [16:41:46<10:09:30, 203.17s/it]2025-09-27 23:29:58,119 Stage: Train 0.5 | Epoch: 150 | Iter: 228000 | Total Loss: 0.005236 | Recon Loss: 0.004383 | Commit Loss: 0.001708 | Perplexity: 1823.812171
2025-09-27 23:30:24,757 Stage: Train 0.5 | Epoch: 150 | Iter: 228200 | Total Loss: 0.005229 | Recon Loss: 0.004374 | Commit Loss: 0.001710 | Perplexity: 1826.213851
2025-09-27 23:30:51,472 Stage: Train 0.5 | Epoch: 150 | Iter: 228400 | Total Loss: 0.005289 | Recon Loss: 0.004432 | Commit Loss: 0.001713 | Perplexity: 1827.098108
2025-09-27 23:31:18,056 Stage: Train 0.5 | Epoch: 150 | Iter: 228600 | Total Loss: 0.005241 | Recon Loss: 0.004384 | Commit Loss: 0.001714 | Perplexity: 1825.371712
2025-09-27 23:31:44,763 Stage: Train 0.5 | Epoch: 150 | Iter: 228800 | Total Loss: 0.005249 | Recon Loss: 0.004391 | Commit Loss: 0.001717 | Perplexity: 1824.962275
2025-09-27 23:32:11,451 Stage: Train 0.5 | Epoch: 150 | Iter: 229000 | Total Loss: 0.005289 | Recon Loss: 0.004430 | Commit Loss: 0.001718 | Perplexity: 1826.815909
2025-09-27 23:32:38,150 Stage: Train 0.5 | Epoch: 150 | Iter: 229200 | Total Loss: 0.005251 | Recon Loss: 0.004396 | Commit Loss: 0.001710 | Perplexity: 1823.217320
Trainning Epoch:  46%|████▌     | 151/330 [16:45:09<10:05:53, 203.09s/it]2025-09-27 23:33:05,122 Stage: Train 0.5 | Epoch: 151 | Iter: 229400 | Total Loss: 0.005242 | Recon Loss: 0.004381 | Commit Loss: 0.001723 | Perplexity: 1829.868223
2025-09-27 23:33:31,792 Stage: Train 0.5 | Epoch: 151 | Iter: 229600 | Total Loss: 0.005203 | Recon Loss: 0.004348 | Commit Loss: 0.001709 | Perplexity: 1827.790032
2025-09-27 23:33:58,425 Stage: Train 0.5 | Epoch: 151 | Iter: 229800 | Total Loss: 0.005225 | Recon Loss: 0.004368 | Commit Loss: 0.001715 | Perplexity: 1828.143999
2025-09-27 23:34:25,274 Stage: Train 0.5 | Epoch: 151 | Iter: 230000 | Total Loss: 0.005248 | Recon Loss: 0.004386 | Commit Loss: 0.001723 | Perplexity: 1826.363605
2025-09-27 23:34:51,817 Stage: Train 0.5 | Epoch: 151 | Iter: 230200 | Total Loss: 0.005305 | Recon Loss: 0.004446 | Commit Loss: 0.001717 | Perplexity: 1828.106443
2025-09-27 23:35:18,409 Stage: Train 0.5 | Epoch: 151 | Iter: 230400 | Total Loss: 0.005234 | Recon Loss: 0.004381 | Commit Loss: 0.001706 | Perplexity: 1827.155621
2025-09-27 23:35:45,112 Stage: Train 0.5 | Epoch: 151 | Iter: 230600 | Total Loss: 0.005190 | Recon Loss: 0.004334 | Commit Loss: 0.001712 | Perplexity: 1826.485584
2025-09-27 23:36:11,824 Stage: Train 0.5 | Epoch: 151 | Iter: 230800 | Total Loss: 0.005282 | Recon Loss: 0.004422 | Commit Loss: 0.001720 | Perplexity: 1824.369198
Trainning Epoch:  46%|████▌     | 152/330 [16:48:32<10:02:19, 203.03s/it]2025-09-27 23:36:38,819 Stage: Train 0.5 | Epoch: 152 | Iter: 231000 | Total Loss: 0.005288 | Recon Loss: 0.004429 | Commit Loss: 0.001717 | Perplexity: 1823.075197
2025-09-27 23:37:05,517 Stage: Train 0.5 | Epoch: 152 | Iter: 231200 | Total Loss: 0.005135 | Recon Loss: 0.004278 | Commit Loss: 0.001714 | Perplexity: 1826.743500
2025-09-27 23:37:32,206 Stage: Train 0.5 | Epoch: 152 | Iter: 231400 | Total Loss: 0.005175 | Recon Loss: 0.004321 | Commit Loss: 0.001708 | Perplexity: 1826.882819
2025-09-27 23:37:58,880 Stage: Train 0.5 | Epoch: 152 | Iter: 231600 | Total Loss: 0.005221 | Recon Loss: 0.004360 | Commit Loss: 0.001720 | Perplexity: 1824.479850
2025-09-27 23:38:25,577 Stage: Train 0.5 | Epoch: 152 | Iter: 231800 | Total Loss: 0.005268 | Recon Loss: 0.004410 | Commit Loss: 0.001715 | Perplexity: 1823.921735
2025-09-27 23:38:52,345 Stage: Train 0.5 | Epoch: 152 | Iter: 232000 | Total Loss: 0.005284 | Recon Loss: 0.004431 | Commit Loss: 0.001707 | Perplexity: 1824.954675
2025-09-27 23:39:19,090 Stage: Train 0.5 | Epoch: 152 | Iter: 232200 | Total Loss: 0.005197 | Recon Loss: 0.004337 | Commit Loss: 0.001720 | Perplexity: 1827.676897
2025-09-27 23:39:45,831 Stage: Train 0.5 | Epoch: 152 | Iter: 232400 | Total Loss: 0.005275 | Recon Loss: 0.004416 | Commit Loss: 0.001719 | Perplexity: 1827.238725
Trainning Epoch:  46%|████▋     | 153/330 [16:51:55<9:59:04, 203.08s/it] 2025-09-27 23:40:12,709 Stage: Train 0.5 | Epoch: 153 | Iter: 232600 | Total Loss: 0.005203 | Recon Loss: 0.004343 | Commit Loss: 0.001720 | Perplexity: 1827.840671
2025-09-27 23:40:39,340 Stage: Train 0.5 | Epoch: 153 | Iter: 232800 | Total Loss: 0.005209 | Recon Loss: 0.004344 | Commit Loss: 0.001730 | Perplexity: 1831.258900
2025-09-27 23:41:06,174 Stage: Train 0.5 | Epoch: 153 | Iter: 233000 | Total Loss: 0.005264 | Recon Loss: 0.004406 | Commit Loss: 0.001716 | Perplexity: 1827.225717
2025-09-27 23:41:32,890 Stage: Train 0.5 | Epoch: 153 | Iter: 233200 | Total Loss: 0.005229 | Recon Loss: 0.004373 | Commit Loss: 0.001712 | Perplexity: 1823.885937
2025-09-27 23:41:59,518 Stage: Train 0.5 | Epoch: 153 | Iter: 233400 | Total Loss: 0.005263 | Recon Loss: 0.004405 | Commit Loss: 0.001716 | Perplexity: 1829.476935
2025-09-27 23:42:26,342 Stage: Train 0.5 | Epoch: 153 | Iter: 233600 | Total Loss: 0.005242 | Recon Loss: 0.004387 | Commit Loss: 0.001711 | Perplexity: 1825.540402
2025-09-27 23:42:53,007 Stage: Train 0.5 | Epoch: 153 | Iter: 233800 | Total Loss: 0.005182 | Recon Loss: 0.004323 | Commit Loss: 0.001716 | Perplexity: 1829.662923
Trainning Epoch:  47%|████▋     | 154/330 [16:55:18<9:55:40, 203.07s/it]2025-09-27 23:43:19,939 Stage: Train 0.5 | Epoch: 154 | Iter: 234000 | Total Loss: 0.005249 | Recon Loss: 0.004388 | Commit Loss: 0.001722 | Perplexity: 1830.773699
2025-09-27 23:43:46,699 Stage: Train 0.5 | Epoch: 154 | Iter: 234200 | Total Loss: 0.005185 | Recon Loss: 0.004329 | Commit Loss: 0.001711 | Perplexity: 1828.555571
2025-09-27 23:44:13,428 Stage: Train 0.5 | Epoch: 154 | Iter: 234400 | Total Loss: 0.005309 | Recon Loss: 0.004448 | Commit Loss: 0.001722 | Perplexity: 1827.964951
2025-09-27 23:44:40,094 Stage: Train 0.5 | Epoch: 154 | Iter: 234600 | Total Loss: 0.005244 | Recon Loss: 0.004380 | Commit Loss: 0.001727 | Perplexity: 1831.131183
2025-09-27 23:45:06,821 Stage: Train 0.5 | Epoch: 154 | Iter: 234800 | Total Loss: 0.005216 | Recon Loss: 0.004363 | Commit Loss: 0.001707 | Perplexity: 1824.088313
2025-09-27 23:45:33,524 Stage: Train 0.5 | Epoch: 154 | Iter: 235000 | Total Loss: 0.005187 | Recon Loss: 0.004332 | Commit Loss: 0.001711 | Perplexity: 1824.926042
2025-09-27 23:46:00,136 Stage: Train 0.5 | Epoch: 154 | Iter: 235200 | Total Loss: 0.005235 | Recon Loss: 0.004380 | Commit Loss: 0.001709 | Perplexity: 1823.047114
2025-09-27 23:46:26,907 Stage: Train 0.5 | Epoch: 154 | Iter: 235400 | Total Loss: 0.005189 | Recon Loss: 0.004332 | Commit Loss: 0.001714 | Perplexity: 1829.623028
Trainning Epoch:  47%|████▋     | 155/330 [16:58:41<9:52:18, 203.08s/it]2025-09-27 23:46:53,923 Stage: Train 0.5 | Epoch: 155 | Iter: 235600 | Total Loss: 0.005293 | Recon Loss: 0.004430 | Commit Loss: 0.001725 | Perplexity: 1827.348504
2025-09-27 23:47:20,525 Stage: Train 0.5 | Epoch: 155 | Iter: 235800 | Total Loss: 0.005161 | Recon Loss: 0.004305 | Commit Loss: 0.001710 | Perplexity: 1828.593003
2025-09-27 23:47:47,207 Stage: Train 0.5 | Epoch: 155 | Iter: 236000 | Total Loss: 0.005192 | Recon Loss: 0.004333 | Commit Loss: 0.001718 | Perplexity: 1826.493373
2025-09-27 23:48:14,032 Stage: Train 0.5 | Epoch: 155 | Iter: 236200 | Total Loss: 0.005267 | Recon Loss: 0.004408 | Commit Loss: 0.001718 | Perplexity: 1828.456039
2025-09-27 23:48:40,857 Stage: Train 0.5 | Epoch: 155 | Iter: 236400 | Total Loss: 0.005218 | Recon Loss: 0.004362 | Commit Loss: 0.001713 | Perplexity: 1826.539817
2025-09-27 23:49:07,608 Stage: Train 0.5 | Epoch: 155 | Iter: 236600 | Total Loss: 0.005213 | Recon Loss: 0.004356 | Commit Loss: 0.001715 | Perplexity: 1827.528154
2025-09-27 23:49:34,265 Stage: Train 0.5 | Epoch: 155 | Iter: 236800 | Total Loss: 0.005240 | Recon Loss: 0.004384 | Commit Loss: 0.001713 | Perplexity: 1828.847916
Trainning Epoch:  47%|████▋     | 156/330 [17:02:04<9:49:01, 203.11s/it]2025-09-27 23:50:01,121 Stage: Train 0.5 | Epoch: 156 | Iter: 237000 | Total Loss: 0.005159 | Recon Loss: 0.004306 | Commit Loss: 0.001707 | Perplexity: 1825.834108
2025-09-27 23:50:27,892 Stage: Train 0.5 | Epoch: 156 | Iter: 237200 | Total Loss: 0.005296 | Recon Loss: 0.004437 | Commit Loss: 0.001719 | Perplexity: 1826.889449
2025-09-27 23:50:54,557 Stage: Train 0.5 | Epoch: 156 | Iter: 237400 | Total Loss: 0.005167 | Recon Loss: 0.004314 | Commit Loss: 0.001706 | Perplexity: 1829.199669
2025-09-27 23:51:21,152 Stage: Train 0.5 | Epoch: 156 | Iter: 237600 | Total Loss: 0.005178 | Recon Loss: 0.004322 | Commit Loss: 0.001712 | Perplexity: 1824.561738
2025-09-27 23:51:47,819 Stage: Train 0.5 | Epoch: 156 | Iter: 237800 | Total Loss: 0.005189 | Recon Loss: 0.004333 | Commit Loss: 0.001713 | Perplexity: 1828.494021
2025-09-27 23:52:14,592 Stage: Train 0.5 | Epoch: 156 | Iter: 238000 | Total Loss: 0.005201 | Recon Loss: 0.004341 | Commit Loss: 0.001720 | Perplexity: 1827.903013
2025-09-27 23:52:41,239 Stage: Train 0.5 | Epoch: 156 | Iter: 238200 | Total Loss: 0.005196 | Recon Loss: 0.004339 | Commit Loss: 0.001715 | Perplexity: 1829.129073
2025-09-27 23:53:07,950 Stage: Train 0.5 | Epoch: 156 | Iter: 238400 | Total Loss: 0.005303 | Recon Loss: 0.004441 | Commit Loss: 0.001723 | Perplexity: 1828.241818
Trainning Epoch:  48%|████▊     | 157/330 [17:05:27<9:45:25, 203.04s/it]2025-09-27 23:53:34,773 Stage: Train 0.5 | Epoch: 157 | Iter: 238600 | Total Loss: 0.005197 | Recon Loss: 0.004342 | Commit Loss: 0.001710 | Perplexity: 1824.122881
2025-09-27 23:54:01,365 Stage: Train 0.5 | Epoch: 157 | Iter: 238800 | Total Loss: 0.005229 | Recon Loss: 0.004374 | Commit Loss: 0.001710 | Perplexity: 1825.266208
2025-09-27 23:54:28,156 Stage: Train 0.5 | Epoch: 157 | Iter: 239000 | Total Loss: 0.005187 | Recon Loss: 0.004332 | Commit Loss: 0.001709 | Perplexity: 1828.469883
2025-09-27 23:54:54,976 Stage: Train 0.5 | Epoch: 157 | Iter: 239200 | Total Loss: 0.005199 | Recon Loss: 0.004340 | Commit Loss: 0.001718 | Perplexity: 1830.369514
2025-09-27 23:55:21,668 Stage: Train 0.5 | Epoch: 157 | Iter: 239400 | Total Loss: 0.005256 | Recon Loss: 0.004400 | Commit Loss: 0.001710 | Perplexity: 1829.648499
2025-09-27 23:55:48,356 Stage: Train 0.5 | Epoch: 157 | Iter: 239600 | Total Loss: 0.005165 | Recon Loss: 0.004310 | Commit Loss: 0.001710 | Perplexity: 1826.609289
2025-09-27 23:56:15,032 Stage: Train 0.5 | Epoch: 157 | Iter: 239800 | Total Loss: 0.005256 | Recon Loss: 0.004400 | Commit Loss: 0.001713 | Perplexity: 1831.184343
2025-09-27 23:56:41,831 Stage: Train 0.5 | Epoch: 157 | Iter: 240000 | Total Loss: 0.005219 | Recon Loss: 0.004362 | Commit Loss: 0.001714 | Perplexity: 1830.607731
2025-09-27 23:56:41,831 Saving model at iteration 240000
2025-09-27 23:56:42,042 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000
2025-09-27 23:56:42,546 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/model.safetensors
2025-09-27 23:56:43,138 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/optimizer.bin
2025-09-27 23:56:43,139 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/scheduler.bin
2025-09-27 23:56:43,139 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/sampler.bin
2025-09-27 23:56:43,140 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_158_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 158/330 [17:08:52<9:43:34, 203.57s/it]2025-09-27 23:57:10,453 Stage: Train 0.5 | Epoch: 158 | Iter: 240200 | Total Loss: 0.005123 | Recon Loss: 0.004273 | Commit Loss: 0.001699 | Perplexity: 1826.549495
2025-09-27 23:57:37,273 Stage: Train 0.5 | Epoch: 158 | Iter: 240400 | Total Loss: 0.005143 | Recon Loss: 0.004286 | Commit Loss: 0.001714 | Perplexity: 1830.455172
2025-09-27 23:58:03,988 Stage: Train 0.5 | Epoch: 158 | Iter: 240600 | Total Loss: 0.005275 | Recon Loss: 0.004409 | Commit Loss: 0.001733 | Perplexity: 1834.685375
2025-09-27 23:58:30,716 Stage: Train 0.5 | Epoch: 158 | Iter: 240800 | Total Loss: 0.005179 | Recon Loss: 0.004327 | Commit Loss: 0.001705 | Perplexity: 1827.090803
2025-09-27 23:58:57,378 Stage: Train 0.5 | Epoch: 158 | Iter: 241000 | Total Loss: 0.005129 | Recon Loss: 0.004275 | Commit Loss: 0.001708 | Perplexity: 1833.583437
2025-09-27 23:59:24,108 Stage: Train 0.5 | Epoch: 158 | Iter: 241200 | Total Loss: 0.005198 | Recon Loss: 0.004342 | Commit Loss: 0.001712 | Perplexity: 1830.111282
2025-09-27 23:59:50,852 Stage: Train 0.5 | Epoch: 158 | Iter: 241400 | Total Loss: 0.005174 | Recon Loss: 0.004324 | Commit Loss: 0.001700 | Perplexity: 1825.564361
Trainning Epoch:  48%|████▊     | 159/330 [17:12:15<9:39:51, 203.46s/it]2025-09-28 00:00:17,730 Stage: Train 0.5 | Epoch: 159 | Iter: 241600 | Total Loss: 0.005234 | Recon Loss: 0.004372 | Commit Loss: 0.001724 | Perplexity: 1834.000680
2025-09-28 00:00:44,461 Stage: Train 0.5 | Epoch: 159 | Iter: 241800 | Total Loss: 0.005145 | Recon Loss: 0.004295 | Commit Loss: 0.001700 | Perplexity: 1830.187897
2025-09-28 00:01:11,071 Stage: Train 0.5 | Epoch: 159 | Iter: 242000 | Total Loss: 0.005184 | Recon Loss: 0.004329 | Commit Loss: 0.001709 | Perplexity: 1827.330215
2025-09-28 00:01:37,704 Stage: Train 0.5 | Epoch: 159 | Iter: 242200 | Total Loss: 0.005192 | Recon Loss: 0.004336 | Commit Loss: 0.001712 | Perplexity: 1830.517096
2025-09-28 00:02:04,430 Stage: Train 0.5 | Epoch: 159 | Iter: 242400 | Total Loss: 0.005192 | Recon Loss: 0.004332 | Commit Loss: 0.001719 | Perplexity: 1829.212577
2025-09-28 00:02:31,211 Stage: Train 0.5 | Epoch: 159 | Iter: 242600 | Total Loss: 0.005177 | Recon Loss: 0.004325 | Commit Loss: 0.001705 | Perplexity: 1829.108503
2025-09-28 00:02:57,952 Stage: Train 0.5 | Epoch: 159 | Iter: 242800 | Total Loss: 0.005195 | Recon Loss: 0.004343 | Commit Loss: 0.001704 | Perplexity: 1826.451177
2025-09-28 00:03:24,769 Stage: Train 0.5 | Epoch: 159 | Iter: 243000 | Total Loss: 0.005176 | Recon Loss: 0.004319 | Commit Loss: 0.001715 | Perplexity: 1829.383656
Trainning Epoch:  48%|████▊     | 160/330 [17:15:38<9:36:10, 203.36s/it]2025-09-28 00:03:51,704 Stage: Train 0.5 | Epoch: 160 | Iter: 243200 | Total Loss: 0.005155 | Recon Loss: 0.004305 | Commit Loss: 0.001700 | Perplexity: 1826.323186
2025-09-28 00:04:18,369 Stage: Train 0.5 | Epoch: 160 | Iter: 243400 | Total Loss: 0.005140 | Recon Loss: 0.004283 | Commit Loss: 0.001715 | Perplexity: 1832.618360
2025-09-28 00:04:45,012 Stage: Train 0.5 | Epoch: 160 | Iter: 243600 | Total Loss: 0.005232 | Recon Loss: 0.004376 | Commit Loss: 0.001711 | Perplexity: 1827.650420
2025-09-28 00:05:11,742 Stage: Train 0.5 | Epoch: 160 | Iter: 243800 | Total Loss: 0.005156 | Recon Loss: 0.004297 | Commit Loss: 0.001718 | Perplexity: 1833.611345
2025-09-28 00:05:38,510 Stage: Train 0.5 | Epoch: 160 | Iter: 244000 | Total Loss: 0.005204 | Recon Loss: 0.004349 | Commit Loss: 0.001710 | Perplexity: 1828.108540
2025-09-28 00:06:05,187 Stage: Train 0.5 | Epoch: 160 | Iter: 244200 | Total Loss: 0.005175 | Recon Loss: 0.004322 | Commit Loss: 0.001706 | Perplexity: 1829.183804
2025-09-28 00:06:31,971 Stage: Train 0.5 | Epoch: 160 | Iter: 244400 | Total Loss: 0.005146 | Recon Loss: 0.004294 | Commit Loss: 0.001705 | Perplexity: 1828.297310
Trainning Epoch:  49%|████▉     | 161/330 [17:19:01<9:32:35, 203.29s/it]2025-09-28 00:06:58,992 Stage: Train 0.5 | Epoch: 161 | Iter: 244600 | Total Loss: 0.005187 | Recon Loss: 0.004336 | Commit Loss: 0.001703 | Perplexity: 1825.088513
2025-09-28 00:07:25,784 Stage: Train 0.5 | Epoch: 161 | Iter: 244800 | Total Loss: 0.005227 | Recon Loss: 0.004374 | Commit Loss: 0.001705 | Perplexity: 1826.851752
2025-09-28 00:07:52,558 Stage: Train 0.5 | Epoch: 161 | Iter: 245000 | Total Loss: 0.005158 | Recon Loss: 0.004306 | Commit Loss: 0.001706 | Perplexity: 1829.297411
2025-09-28 00:08:19,225 Stage: Train 0.5 | Epoch: 161 | Iter: 245200 | Total Loss: 0.005195 | Recon Loss: 0.004339 | Commit Loss: 0.001712 | Perplexity: 1833.156039
2025-09-28 00:08:45,969 Stage: Train 0.5 | Epoch: 161 | Iter: 245400 | Total Loss: 0.005129 | Recon Loss: 0.004270 | Commit Loss: 0.001717 | Perplexity: 1831.278373
2025-09-28 00:09:12,728 Stage: Train 0.5 | Epoch: 161 | Iter: 245600 | Total Loss: 0.005132 | Recon Loss: 0.004281 | Commit Loss: 0.001702 | Perplexity: 1826.322739
2025-09-28 00:09:39,519 Stage: Train 0.5 | Epoch: 161 | Iter: 245800 | Total Loss: 0.005186 | Recon Loss: 0.004335 | Commit Loss: 0.001702 | Perplexity: 1828.768195
2025-09-28 00:10:06,335 Stage: Train 0.5 | Epoch: 161 | Iter: 246000 | Total Loss: 0.005158 | Recon Loss: 0.004304 | Commit Loss: 0.001707 | Perplexity: 1828.348156
Trainning Epoch:  49%|████▉     | 162/330 [17:22:25<9:29:22, 203.35s/it]2025-09-28 00:10:33,304 Stage: Train 0.5 | Epoch: 162 | Iter: 246200 | Total Loss: 0.005191 | Recon Loss: 0.004337 | Commit Loss: 0.001707 | Perplexity: 1832.363709
2025-09-28 00:11:00,110 Stage: Train 0.5 | Epoch: 162 | Iter: 246400 | Total Loss: 0.005124 | Recon Loss: 0.004267 | Commit Loss: 0.001714 | Perplexity: 1832.391105
2025-09-28 00:11:26,941 Stage: Train 0.5 | Epoch: 162 | Iter: 246600 | Total Loss: 0.005145 | Recon Loss: 0.004293 | Commit Loss: 0.001703 | Perplexity: 1826.852433
2025-09-28 00:11:53,707 Stage: Train 0.5 | Epoch: 162 | Iter: 246800 | Total Loss: 0.005157 | Recon Loss: 0.004305 | Commit Loss: 0.001704 | Perplexity: 1827.796261
2025-09-28 00:12:20,572 Stage: Train 0.5 | Epoch: 162 | Iter: 247000 | Total Loss: 0.005167 | Recon Loss: 0.004316 | Commit Loss: 0.001701 | Perplexity: 1826.854617
2025-09-28 00:12:47,315 Stage: Train 0.5 | Epoch: 162 | Iter: 247200 | Total Loss: 0.005194 | Recon Loss: 0.004335 | Commit Loss: 0.001718 | Perplexity: 1835.960800
2025-09-28 00:13:14,096 Stage: Train 0.5 | Epoch: 162 | Iter: 247400 | Total Loss: 0.005234 | Recon Loss: 0.004374 | Commit Loss: 0.001720 | Perplexity: 1830.915775
Trainning Epoch:  49%|████▉     | 163/330 [17:25:49<9:26:19, 203.47s/it]2025-09-28 00:13:41,129 Stage: Train 0.5 | Epoch: 163 | Iter: 247600 | Total Loss: 0.005144 | Recon Loss: 0.004296 | Commit Loss: 0.001696 | Perplexity: 1825.436461
2025-09-28 00:14:07,821 Stage: Train 0.5 | Epoch: 163 | Iter: 247800 | Total Loss: 0.005114 | Recon Loss: 0.004258 | Commit Loss: 0.001712 | Perplexity: 1836.485394
2025-09-28 00:14:34,590 Stage: Train 0.5 | Epoch: 163 | Iter: 248000 | Total Loss: 0.005147 | Recon Loss: 0.004293 | Commit Loss: 0.001708 | Perplexity: 1830.229866
2025-09-28 00:15:01,392 Stage: Train 0.5 | Epoch: 163 | Iter: 248200 | Total Loss: 0.005158 | Recon Loss: 0.004310 | Commit Loss: 0.001697 | Perplexity: 1826.627290
2025-09-28 00:15:28,193 Stage: Train 0.5 | Epoch: 163 | Iter: 248400 | Total Loss: 0.005227 | Recon Loss: 0.004374 | Commit Loss: 0.001705 | Perplexity: 1825.625992
2025-09-28 00:15:54,984 Stage: Train 0.5 | Epoch: 163 | Iter: 248600 | Total Loss: 0.005086 | Recon Loss: 0.004234 | Commit Loss: 0.001705 | Perplexity: 1828.751630
2025-09-28 00:16:21,741 Stage: Train 0.5 | Epoch: 163 | Iter: 248800 | Total Loss: 0.005174 | Recon Loss: 0.004318 | Commit Loss: 0.001712 | Perplexity: 1835.612580
2025-09-28 00:16:48,489 Stage: Train 0.5 | Epoch: 163 | Iter: 249000 | Total Loss: 0.005183 | Recon Loss: 0.004326 | Commit Loss: 0.001714 | Perplexity: 1829.411589
Trainning Epoch:  50%|████▉     | 164/330 [17:29:13<9:23:12, 203.57s/it]2025-09-28 00:17:15,854 Stage: Train 0.5 | Epoch: 164 | Iter: 249200 | Total Loss: 0.005190 | Recon Loss: 0.004338 | Commit Loss: 0.001704 | Perplexity: 1831.575330
2025-09-28 00:17:42,819 Stage: Train 0.5 | Epoch: 164 | Iter: 249400 | Total Loss: 0.005108 | Recon Loss: 0.004258 | Commit Loss: 0.001700 | Perplexity: 1827.601024
2025-09-28 00:18:09,860 Stage: Train 0.5 | Epoch: 164 | Iter: 249600 | Total Loss: 0.005127 | Recon Loss: 0.004277 | Commit Loss: 0.001701 | Perplexity: 1825.780265
2025-09-28 00:18:36,873 Stage: Train 0.5 | Epoch: 164 | Iter: 249800 | Total Loss: 0.005218 | Recon Loss: 0.004364 | Commit Loss: 0.001708 | Perplexity: 1834.057973
2025-09-28 00:19:03,670 Stage: Train 0.5 | Epoch: 164 | Iter: 250000 | Total Loss: 0.005140 | Recon Loss: 0.004289 | Commit Loss: 0.001703 | Perplexity: 1833.056721
2025-09-28 00:19:30,486 Stage: Train 0.5 | Epoch: 164 | Iter: 250200 | Total Loss: 0.005163 | Recon Loss: 0.004311 | Commit Loss: 0.001704 | Perplexity: 1831.491241
2025-09-28 00:19:57,320 Stage: Train 0.5 | Epoch: 164 | Iter: 250400 | Total Loss: 0.005153 | Recon Loss: 0.004301 | Commit Loss: 0.001704 | Perplexity: 1826.604210
2025-09-28 00:20:24,002 Stage: Train 0.5 | Epoch: 164 | Iter: 250600 | Total Loss: 0.005167 | Recon Loss: 0.004312 | Commit Loss: 0.001709 | Perplexity: 1831.022407
Trainning Epoch:  50%|█████     | 165/330 [17:32:37<9:20:29, 203.81s/it]2025-09-28 00:20:51,004 Stage: Train 0.5 | Epoch: 165 | Iter: 250800 | Total Loss: 0.005125 | Recon Loss: 0.004271 | Commit Loss: 0.001707 | Perplexity: 1827.898281
2025-09-28 00:21:17,615 Stage: Train 0.5 | Epoch: 165 | Iter: 251000 | Total Loss: 0.005141 | Recon Loss: 0.004292 | Commit Loss: 0.001700 | Perplexity: 1830.455712
2025-09-28 00:21:44,361 Stage: Train 0.5 | Epoch: 165 | Iter: 251200 | Total Loss: 0.005169 | Recon Loss: 0.004312 | Commit Loss: 0.001714 | Perplexity: 1832.896599
2025-09-28 00:22:11,101 Stage: Train 0.5 | Epoch: 165 | Iter: 251400 | Total Loss: 0.005130 | Recon Loss: 0.004278 | Commit Loss: 0.001705 | Perplexity: 1829.695560
2025-09-28 00:22:37,648 Stage: Train 0.5 | Epoch: 165 | Iter: 251600 | Total Loss: 0.005108 | Recon Loss: 0.004254 | Commit Loss: 0.001706 | Perplexity: 1828.909301
2025-09-28 00:23:04,291 Stage: Train 0.5 | Epoch: 165 | Iter: 251800 | Total Loss: 0.005170 | Recon Loss: 0.004314 | Commit Loss: 0.001712 | Perplexity: 1830.056066
2025-09-28 00:23:30,921 Stage: Train 0.5 | Epoch: 165 | Iter: 252000 | Total Loss: 0.005124 | Recon Loss: 0.004270 | Commit Loss: 0.001709 | Perplexity: 1826.404407
Trainning Epoch:  50%|█████     | 166/330 [17:36:00<9:16:12, 203.49s/it]2025-09-28 00:23:57,801 Stage: Train 0.5 | Epoch: 166 | Iter: 252200 | Total Loss: 0.005175 | Recon Loss: 0.004315 | Commit Loss: 0.001720 | Perplexity: 1826.466475
2025-09-28 00:24:24,497 Stage: Train 0.5 | Epoch: 166 | Iter: 252400 | Total Loss: 0.005095 | Recon Loss: 0.004239 | Commit Loss: 0.001713 | Perplexity: 1831.555615
2025-09-28 00:24:51,119 Stage: Train 0.5 | Epoch: 166 | Iter: 252600 | Total Loss: 0.005160 | Recon Loss: 0.004305 | Commit Loss: 0.001708 | Perplexity: 1830.762786
2025-09-28 00:25:17,663 Stage: Train 0.5 | Epoch: 166 | Iter: 252800 | Total Loss: 0.005068 | Recon Loss: 0.004215 | Commit Loss: 0.001706 | Perplexity: 1828.610914
2025-09-28 00:25:44,296 Stage: Train 0.5 | Epoch: 166 | Iter: 253000 | Total Loss: 0.005197 | Recon Loss: 0.004335 | Commit Loss: 0.001724 | Perplexity: 1832.892296
2025-09-28 00:26:11,077 Stage: Train 0.5 | Epoch: 166 | Iter: 253200 | Total Loss: 0.005148 | Recon Loss: 0.004294 | Commit Loss: 0.001709 | Perplexity: 1829.532306
2025-09-28 00:26:37,635 Stage: Train 0.5 | Epoch: 166 | Iter: 253400 | Total Loss: 0.005168 | Recon Loss: 0.004312 | Commit Loss: 0.001712 | Perplexity: 1830.677653
2025-09-28 00:27:04,229 Stage: Train 0.5 | Epoch: 166 | Iter: 253600 | Total Loss: 0.005152 | Recon Loss: 0.004296 | Commit Loss: 0.001712 | Perplexity: 1829.863591
Trainning Epoch:  51%|█████     | 167/330 [17:39:22<9:12:10, 203.25s/it]2025-09-28 00:27:31,290 Stage: Train 0.5 | Epoch: 167 | Iter: 253800 | Total Loss: 0.005066 | Recon Loss: 0.004216 | Commit Loss: 0.001700 | Perplexity: 1826.716715
2025-09-28 00:27:57,974 Stage: Train 0.5 | Epoch: 167 | Iter: 254000 | Total Loss: 0.005167 | Recon Loss: 0.004315 | Commit Loss: 0.001705 | Perplexity: 1826.966357
2025-09-28 00:28:24,770 Stage: Train 0.5 | Epoch: 167 | Iter: 254200 | Total Loss: 0.005066 | Recon Loss: 0.004214 | Commit Loss: 0.001704 | Perplexity: 1832.064772
2025-09-28 00:28:51,479 Stage: Train 0.5 | Epoch: 167 | Iter: 254400 | Total Loss: 0.005133 | Recon Loss: 0.004277 | Commit Loss: 0.001712 | Perplexity: 1831.606261
2025-09-28 00:29:18,143 Stage: Train 0.5 | Epoch: 167 | Iter: 254600 | Total Loss: 0.005145 | Recon Loss: 0.004287 | Commit Loss: 0.001715 | Perplexity: 1828.362087
2025-09-28 00:29:44,843 Stage: Train 0.5 | Epoch: 167 | Iter: 254800 | Total Loss: 0.005208 | Recon Loss: 0.004346 | Commit Loss: 0.001724 | Perplexity: 1831.878238
2025-09-28 00:30:11,515 Stage: Train 0.5 | Epoch: 167 | Iter: 255000 | Total Loss: 0.005091 | Recon Loss: 0.004237 | Commit Loss: 0.001708 | Perplexity: 1828.719866
Trainning Epoch:  51%|█████     | 168/330 [17:42:45<9:08:32, 203.17s/it]2025-09-28 00:30:38,506 Stage: Train 0.5 | Epoch: 168 | Iter: 255200 | Total Loss: 0.005090 | Recon Loss: 0.004237 | Commit Loss: 0.001707 | Perplexity: 1830.391572
2025-09-28 00:31:05,283 Stage: Train 0.5 | Epoch: 168 | Iter: 255400 | Total Loss: 0.005121 | Recon Loss: 0.004265 | Commit Loss: 0.001713 | Perplexity: 1831.814195
2025-09-28 00:31:32,108 Stage: Train 0.5 | Epoch: 168 | Iter: 255600 | Total Loss: 0.005154 | Recon Loss: 0.004298 | Commit Loss: 0.001712 | Perplexity: 1828.345726
2025-09-28 00:31:58,813 Stage: Train 0.5 | Epoch: 168 | Iter: 255800 | Total Loss: 0.005091 | Recon Loss: 0.004237 | Commit Loss: 0.001708 | Perplexity: 1827.599236
2025-09-28 00:32:25,462 Stage: Train 0.5 | Epoch: 168 | Iter: 256000 | Total Loss: 0.005118 | Recon Loss: 0.004264 | Commit Loss: 0.001708 | Perplexity: 1827.093949
2025-09-28 00:32:52,191 Stage: Train 0.5 | Epoch: 168 | Iter: 256200 | Total Loss: 0.005098 | Recon Loss: 0.004242 | Commit Loss: 0.001712 | Perplexity: 1833.901605
2025-09-28 00:33:19,003 Stage: Train 0.5 | Epoch: 168 | Iter: 256400 | Total Loss: 0.005180 | Recon Loss: 0.004322 | Commit Loss: 0.001717 | Perplexity: 1831.379981
2025-09-28 00:33:45,607 Stage: Train 0.5 | Epoch: 168 | Iter: 256600 | Total Loss: 0.005051 | Recon Loss: 0.004194 | Commit Loss: 0.001714 | Perplexity: 1828.409125
Trainning Epoch:  51%|█████     | 169/330 [17:46:09<9:05:17, 203.21s/it]2025-09-28 00:34:12,573 Stage: Train 0.5 | Epoch: 169 | Iter: 256800 | Total Loss: 0.005149 | Recon Loss: 0.004294 | Commit Loss: 0.001709 | Perplexity: 1831.537432
2025-09-28 00:34:39,235 Stage: Train 0.5 | Epoch: 169 | Iter: 257000 | Total Loss: 0.005174 | Recon Loss: 0.004318 | Commit Loss: 0.001711 | Perplexity: 1830.809011
2025-09-28 00:35:05,830 Stage: Train 0.5 | Epoch: 169 | Iter: 257200 | Total Loss: 0.005172 | Recon Loss: 0.004319 | Commit Loss: 0.001705 | Perplexity: 1828.543033
2025-09-28 00:35:32,361 Stage: Train 0.5 | Epoch: 169 | Iter: 257400 | Total Loss: 0.005117 | Recon Loss: 0.004260 | Commit Loss: 0.001714 | Perplexity: 1830.078627
2025-09-28 00:35:58,922 Stage: Train 0.5 | Epoch: 169 | Iter: 257600 | Total Loss: 0.005148 | Recon Loss: 0.004295 | Commit Loss: 0.001706 | Perplexity: 1828.768011
2025-09-28 00:36:25,616 Stage: Train 0.5 | Epoch: 169 | Iter: 257800 | Total Loss: 0.005102 | Recon Loss: 0.004250 | Commit Loss: 0.001703 | Perplexity: 1828.192737
2025-09-28 00:36:52,307 Stage: Train 0.5 | Epoch: 169 | Iter: 258000 | Total Loss: 0.005128 | Recon Loss: 0.004273 | Commit Loss: 0.001710 | Perplexity: 1830.447296
2025-09-28 00:37:18,950 Stage: Train 0.5 | Epoch: 169 | Iter: 258200 | Total Loss: 0.005205 | Recon Loss: 0.004344 | Commit Loss: 0.001721 | Perplexity: 1832.091688
Trainning Epoch:  52%|█████▏    | 170/330 [17:49:31<9:01:26, 203.04s/it]2025-09-28 00:37:46,202 Stage: Train 0.5 | Epoch: 170 | Iter: 258400 | Total Loss: 0.005151 | Recon Loss: 0.004301 | Commit Loss: 0.001700 | Perplexity: 1825.256683
2025-09-28 00:38:12,876 Stage: Train 0.5 | Epoch: 170 | Iter: 258600 | Total Loss: 0.005031 | Recon Loss: 0.004176 | Commit Loss: 0.001710 | Perplexity: 1832.405637
2025-09-28 00:38:39,527 Stage: Train 0.5 | Epoch: 170 | Iter: 258800 | Total Loss: 0.005152 | Recon Loss: 0.004296 | Commit Loss: 0.001712 | Perplexity: 1829.062047
2025-09-28 00:39:06,195 Stage: Train 0.5 | Epoch: 170 | Iter: 259000 | Total Loss: 0.005102 | Recon Loss: 0.004245 | Commit Loss: 0.001713 | Perplexity: 1832.021390
2025-09-28 00:39:32,905 Stage: Train 0.5 | Epoch: 170 | Iter: 259200 | Total Loss: 0.005098 | Recon Loss: 0.004247 | Commit Loss: 0.001703 | Perplexity: 1828.042075
2025-09-28 00:39:59,557 Stage: Train 0.5 | Epoch: 170 | Iter: 259400 | Total Loss: 0.005134 | Recon Loss: 0.004282 | Commit Loss: 0.001703 | Perplexity: 1831.473600
2025-09-28 00:40:26,250 Stage: Train 0.5 | Epoch: 170 | Iter: 259600 | Total Loss: 0.005132 | Recon Loss: 0.004274 | Commit Loss: 0.001718 | Perplexity: 1830.856253
Trainning Epoch:  52%|█████▏    | 171/330 [17:52:54<8:57:51, 202.97s/it]2025-09-28 00:40:52,983 Stage: Train 0.5 | Epoch: 171 | Iter: 259800 | Total Loss: 0.005077 | Recon Loss: 0.004223 | Commit Loss: 0.001707 | Perplexity: 1828.211367
2025-09-28 00:41:19,666 Stage: Train 0.5 | Epoch: 171 | Iter: 260000 | Total Loss: 0.005162 | Recon Loss: 0.004305 | Commit Loss: 0.001713 | Perplexity: 1830.487928
2025-09-28 00:41:19,666 Saving model at iteration 260000
2025-09-28 00:41:19,965 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000
2025-09-28 00:41:20,479 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/model.safetensors
2025-09-28 00:41:21,015 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/optimizer.bin
2025-09-28 00:41:21,016 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/scheduler.bin
2025-09-28 00:41:21,016 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/sampler.bin
2025-09-28 00:41:21,017 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_172_step_260000/random_states_0.pkl
2025-09-28 00:41:47,769 Stage: Train 0.5 | Epoch: 171 | Iter: 260200 | Total Loss: 0.005052 | Recon Loss: 0.004199 | Commit Loss: 0.001706 | Perplexity: 1830.568766
2025-09-28 00:42:14,492 Stage: Train 0.5 | Epoch: 171 | Iter: 260400 | Total Loss: 0.005120 | Recon Loss: 0.004265 | Commit Loss: 0.001709 | Perplexity: 1831.501722
2025-09-28 00:42:41,119 Stage: Train 0.5 | Epoch: 171 | Iter: 260600 | Total Loss: 0.005105 | Recon Loss: 0.004249 | Commit Loss: 0.001712 | Perplexity: 1833.413669
2025-09-28 00:43:07,803 Stage: Train 0.5 | Epoch: 171 | Iter: 260800 | Total Loss: 0.005105 | Recon Loss: 0.004249 | Commit Loss: 0.001712 | Perplexity: 1829.533941
2025-09-28 00:43:34,479 Stage: Train 0.5 | Epoch: 171 | Iter: 261000 | Total Loss: 0.005142 | Recon Loss: 0.004285 | Commit Loss: 0.001714 | Perplexity: 1830.232670
2025-09-28 00:44:01,290 Stage: Train 0.5 | Epoch: 171 | Iter: 261200 | Total Loss: 0.005100 | Recon Loss: 0.004242 | Commit Loss: 0.001715 | Perplexity: 1832.518132
Trainning Epoch:  52%|█████▏    | 172/330 [17:56:19<8:55:40, 203.42s/it]2025-09-28 00:44:28,310 Stage: Train 0.5 | Epoch: 172 | Iter: 261400 | Total Loss: 0.005151 | Recon Loss: 0.004296 | Commit Loss: 0.001711 | Perplexity: 1828.822351
2025-09-28 00:44:54,980 Stage: Train 0.5 | Epoch: 172 | Iter: 261600 | Total Loss: 0.005070 | Recon Loss: 0.004215 | Commit Loss: 0.001710 | Perplexity: 1829.679365
2025-09-28 00:45:21,754 Stage: Train 0.5 | Epoch: 172 | Iter: 261800 | Total Loss: 0.005047 | Recon Loss: 0.004199 | Commit Loss: 0.001696 | Perplexity: 1826.958681
2025-09-28 00:45:48,470 Stage: Train 0.5 | Epoch: 172 | Iter: 262000 | Total Loss: 0.005147 | Recon Loss: 0.004289 | Commit Loss: 0.001717 | Perplexity: 1835.467899
2025-09-28 00:46:15,148 Stage: Train 0.5 | Epoch: 172 | Iter: 262200 | Total Loss: 0.005102 | Recon Loss: 0.004246 | Commit Loss: 0.001713 | Perplexity: 1827.590767
2025-09-28 00:46:41,881 Stage: Train 0.5 | Epoch: 172 | Iter: 262400 | Total Loss: 0.005102 | Recon Loss: 0.004246 | Commit Loss: 0.001712 | Perplexity: 1829.853774
2025-09-28 00:47:08,550 Stage: Train 0.5 | Epoch: 172 | Iter: 262600 | Total Loss: 0.005129 | Recon Loss: 0.004272 | Commit Loss: 0.001713 | Perplexity: 1832.787417
Trainning Epoch:  52%|█████▏    | 173/330 [17:59:42<8:52:20, 203.44s/it]2025-09-28 00:47:35,801 Stage: Train 0.5 | Epoch: 173 | Iter: 262800 | Total Loss: 0.005128 | Recon Loss: 0.004274 | Commit Loss: 0.001709 | Perplexity: 1830.929240
2025-09-28 00:48:02,668 Stage: Train 0.5 | Epoch: 173 | Iter: 263000 | Total Loss: 0.005036 | Recon Loss: 0.004184 | Commit Loss: 0.001704 | Perplexity: 1830.994681
2025-09-28 00:48:29,462 Stage: Train 0.5 | Epoch: 173 | Iter: 263200 | Total Loss: 0.005147 | Recon Loss: 0.004298 | Commit Loss: 0.001698 | Perplexity: 1827.228873
2025-09-28 00:48:56,091 Stage: Train 0.5 | Epoch: 173 | Iter: 263400 | Total Loss: 0.005083 | Recon Loss: 0.004225 | Commit Loss: 0.001716 | Perplexity: 1832.488647
2025-09-28 00:49:22,804 Stage: Train 0.5 | Epoch: 173 | Iter: 263600 | Total Loss: 0.005092 | Recon Loss: 0.004236 | Commit Loss: 0.001713 | Perplexity: 1830.440439
2025-09-28 00:49:49,561 Stage: Train 0.5 | Epoch: 173 | Iter: 263800 | Total Loss: 0.005062 | Recon Loss: 0.004209 | Commit Loss: 0.001707 | Perplexity: 1829.374726
2025-09-28 00:50:16,362 Stage: Train 0.5 | Epoch: 173 | Iter: 264000 | Total Loss: 0.005150 | Recon Loss: 0.004291 | Commit Loss: 0.001718 | Perplexity: 1834.494100
2025-09-28 00:50:42,971 Stage: Train 0.5 | Epoch: 173 | Iter: 264200 | Total Loss: 0.005097 | Recon Loss: 0.004238 | Commit Loss: 0.001718 | Perplexity: 1831.890031
Trainning Epoch:  53%|█████▎    | 174/330 [18:03:05<8:48:42, 203.35s/it]2025-09-28 00:51:09,835 Stage: Train 0.5 | Epoch: 174 | Iter: 264400 | Total Loss: 0.005122 | Recon Loss: 0.004261 | Commit Loss: 0.001723 | Perplexity: 1834.034374
2025-09-28 00:51:36,520 Stage: Train 0.5 | Epoch: 174 | Iter: 264600 | Total Loss: 0.005101 | Recon Loss: 0.004241 | Commit Loss: 0.001719 | Perplexity: 1830.534416
2025-09-28 00:52:03,179 Stage: Train 0.5 | Epoch: 174 | Iter: 264800 | Total Loss: 0.005092 | Recon Loss: 0.004236 | Commit Loss: 0.001713 | Perplexity: 1825.553983
2025-09-28 00:52:29,844 Stage: Train 0.5 | Epoch: 174 | Iter: 265000 | Total Loss: 0.005106 | Recon Loss: 0.004254 | Commit Loss: 0.001704 | Perplexity: 1831.634164
2025-09-28 00:52:56,582 Stage: Train 0.5 | Epoch: 174 | Iter: 265200 | Total Loss: 0.005048 | Recon Loss: 0.004193 | Commit Loss: 0.001711 | Perplexity: 1834.200692
2025-09-28 00:53:23,409 Stage: Train 0.5 | Epoch: 174 | Iter: 265400 | Total Loss: 0.005115 | Recon Loss: 0.004255 | Commit Loss: 0.001719 | Perplexity: 1833.937144
2025-09-28 00:53:50,097 Stage: Train 0.5 | Epoch: 174 | Iter: 265600 | Total Loss: 0.005040 | Recon Loss: 0.004188 | Commit Loss: 0.001704 | Perplexity: 1829.279129
2025-09-28 00:54:16,836 Stage: Train 0.5 | Epoch: 174 | Iter: 265800 | Total Loss: 0.005137 | Recon Loss: 0.004282 | Commit Loss: 0.001710 | Perplexity: 1829.476535
Trainning Epoch:  53%|█████▎    | 175/330 [18:06:28<8:45:13, 203.31s/it]2025-09-28 00:54:43,849 Stage: Train 0.5 | Epoch: 175 | Iter: 266000 | Total Loss: 0.005033 | Recon Loss: 0.004178 | Commit Loss: 0.001709 | Perplexity: 1830.092965
2025-09-28 00:55:10,545 Stage: Train 0.5 | Epoch: 175 | Iter: 266200 | Total Loss: 0.005082 | Recon Loss: 0.004224 | Commit Loss: 0.001714 | Perplexity: 1835.424144
2025-09-28 00:55:37,212 Stage: Train 0.5 | Epoch: 175 | Iter: 266400 | Total Loss: 0.005069 | Recon Loss: 0.004212 | Commit Loss: 0.001714 | Perplexity: 1833.290538
2025-09-28 00:56:03,926 Stage: Train 0.5 | Epoch: 175 | Iter: 266600 | Total Loss: 0.005114 | Recon Loss: 0.004259 | Commit Loss: 0.001710 | Perplexity: 1830.932200
2025-09-28 00:56:30,690 Stage: Train 0.5 | Epoch: 175 | Iter: 266800 | Total Loss: 0.005074 | Recon Loss: 0.004222 | Commit Loss: 0.001703 | Perplexity: 1828.196829
2025-09-28 00:56:57,331 Stage: Train 0.5 | Epoch: 175 | Iter: 267000 | Total Loss: 0.005016 | Recon Loss: 0.004160 | Commit Loss: 0.001712 | Perplexity: 1828.771094
2025-09-28 00:57:23,972 Stage: Train 0.5 | Epoch: 175 | Iter: 267200 | Total Loss: 0.005196 | Recon Loss: 0.004328 | Commit Loss: 0.001735 | Perplexity: 1838.308904
Trainning Epoch:  53%|█████▎    | 176/330 [18:09:51<8:41:31, 203.19s/it]2025-09-28 00:57:50,711 Stage: Train 0.5 | Epoch: 176 | Iter: 267400 | Total Loss: 0.005046 | Recon Loss: 0.004195 | Commit Loss: 0.001703 | Perplexity: 1824.371123
2025-09-28 00:58:17,461 Stage: Train 0.5 | Epoch: 176 | Iter: 267600 | Total Loss: 0.005086 | Recon Loss: 0.004234 | Commit Loss: 0.001704 | Perplexity: 1825.966617
2025-09-28 00:58:44,162 Stage: Train 0.5 | Epoch: 176 | Iter: 267800 | Total Loss: 0.005044 | Recon Loss: 0.004186 | Commit Loss: 0.001716 | Perplexity: 1832.815755
2025-09-28 00:59:10,910 Stage: Train 0.5 | Epoch: 176 | Iter: 268000 | Total Loss: 0.005124 | Recon Loss: 0.004266 | Commit Loss: 0.001716 | Perplexity: 1832.594527
2025-09-28 00:59:37,473 Stage: Train 0.5 | Epoch: 176 | Iter: 268200 | Total Loss: 0.005045 | Recon Loss: 0.004188 | Commit Loss: 0.001713 | Perplexity: 1833.656805
2025-09-28 01:00:04,112 Stage: Train 0.5 | Epoch: 176 | Iter: 268400 | Total Loss: 0.005114 | Recon Loss: 0.004255 | Commit Loss: 0.001719 | Perplexity: 1832.339988
2025-09-28 01:00:30,840 Stage: Train 0.5 | Epoch: 176 | Iter: 268600 | Total Loss: 0.005134 | Recon Loss: 0.004281 | Commit Loss: 0.001705 | Perplexity: 1830.384195
2025-09-28 01:00:57,587 Stage: Train 0.5 | Epoch: 176 | Iter: 268800 | Total Loss: 0.005021 | Recon Loss: 0.004166 | Commit Loss: 0.001710 | Perplexity: 1830.144853
Trainning Epoch:  54%|█████▎    | 177/330 [18:13:14<8:37:56, 203.11s/it]2025-09-28 01:01:24,562 Stage: Train 0.5 | Epoch: 177 | Iter: 269000 | Total Loss: 0.005052 | Recon Loss: 0.004202 | Commit Loss: 0.001699 | Perplexity: 1830.377729
2025-09-28 01:01:51,201 Stage: Train 0.5 | Epoch: 177 | Iter: 269200 | Total Loss: 0.005091 | Recon Loss: 0.004237 | Commit Loss: 0.001708 | Perplexity: 1832.763331
2025-09-28 01:02:17,932 Stage: Train 0.5 | Epoch: 177 | Iter: 269400 | Total Loss: 0.005084 | Recon Loss: 0.004232 | Commit Loss: 0.001705 | Perplexity: 1832.960833
2025-09-28 01:02:44,692 Stage: Train 0.5 | Epoch: 177 | Iter: 269600 | Total Loss: 0.005089 | Recon Loss: 0.004229 | Commit Loss: 0.001719 | Perplexity: 1831.427264
2025-09-28 01:03:10,997 Stage: Train 0.5 | Epoch: 177 | Iter: 269800 | Total Loss: 0.005024 | Recon Loss: 0.004166 | Commit Loss: 0.001715 | Perplexity: 1832.654151
2025-09-28 01:03:37,627 Stage: Train 0.5 | Epoch: 177 | Iter: 270000 | Total Loss: 0.005115 | Recon Loss: 0.004257 | Commit Loss: 0.001715 | Perplexity: 1830.039039
2025-09-28 01:04:04,492 Stage: Train 0.5 | Epoch: 177 | Iter: 270200 | Total Loss: 0.005049 | Recon Loss: 0.004191 | Commit Loss: 0.001716 | Perplexity: 1831.306699
Trainning Epoch:  54%|█████▍    | 178/330 [18:16:37<8:34:19, 203.03s/it]2025-09-28 01:04:31,542 Stage: Train 0.5 | Epoch: 178 | Iter: 270400 | Total Loss: 0.005039 | Recon Loss: 0.004180 | Commit Loss: 0.001717 | Perplexity: 1830.284702
2025-09-28 01:04:58,236 Stage: Train 0.5 | Epoch: 178 | Iter: 270600 | Total Loss: 0.005073 | Recon Loss: 0.004219 | Commit Loss: 0.001707 | Perplexity: 1831.411567
2025-09-28 01:05:25,028 Stage: Train 0.5 | Epoch: 178 | Iter: 270800 | Total Loss: 0.005093 | Recon Loss: 0.004235 | Commit Loss: 0.001717 | Perplexity: 1830.880942
2025-09-28 01:05:51,576 Stage: Train 0.5 | Epoch: 178 | Iter: 271000 | Total Loss: 0.005041 | Recon Loss: 0.004185 | Commit Loss: 0.001712 | Perplexity: 1831.945461
2025-09-28 01:06:18,249 Stage: Train 0.5 | Epoch: 178 | Iter: 271200 | Total Loss: 0.005061 | Recon Loss: 0.004203 | Commit Loss: 0.001715 | Perplexity: 1833.837060
2025-09-28 01:06:44,961 Stage: Train 0.5 | Epoch: 178 | Iter: 271400 | Total Loss: 0.005069 | Recon Loss: 0.004216 | Commit Loss: 0.001706 | Perplexity: 1832.042195
2025-09-28 01:07:11,602 Stage: Train 0.5 | Epoch: 178 | Iter: 271600 | Total Loss: 0.005136 | Recon Loss: 0.004277 | Commit Loss: 0.001718 | Perplexity: 1833.616180
2025-09-28 01:07:38,249 Stage: Train 0.5 | Epoch: 178 | Iter: 271800 | Total Loss: 0.005023 | Recon Loss: 0.004168 | Commit Loss: 0.001709 | Perplexity: 1830.156540
Trainning Epoch:  54%|█████▍    | 179/330 [18:20:00<8:30:49, 202.98s/it]2025-09-28 01:08:05,181 Stage: Train 0.5 | Epoch: 179 | Iter: 272000 | Total Loss: 0.005045 | Recon Loss: 0.004187 | Commit Loss: 0.001716 | Perplexity: 1826.835294
2025-09-28 01:08:31,972 Stage: Train 0.5 | Epoch: 179 | Iter: 272200 | Total Loss: 0.005082 | Recon Loss: 0.004230 | Commit Loss: 0.001703 | Perplexity: 1828.215052
2025-09-28 01:08:58,620 Stage: Train 0.5 | Epoch: 179 | Iter: 272400 | Total Loss: 0.005010 | Recon Loss: 0.004161 | Commit Loss: 0.001700 | Perplexity: 1824.731013
2025-09-28 01:09:25,357 Stage: Train 0.5 | Epoch: 179 | Iter: 272600 | Total Loss: 0.005025 | Recon Loss: 0.004169 | Commit Loss: 0.001712 | Perplexity: 1836.470490
2025-09-28 01:09:52,018 Stage: Train 0.5 | Epoch: 179 | Iter: 272800 | Total Loss: 0.005057 | Recon Loss: 0.004197 | Commit Loss: 0.001720 | Perplexity: 1836.947418
2025-09-28 01:10:18,650 Stage: Train 0.5 | Epoch: 179 | Iter: 273000 | Total Loss: 0.005070 | Recon Loss: 0.004214 | Commit Loss: 0.001712 | Perplexity: 1837.333099
2025-09-28 01:10:45,292 Stage: Train 0.5 | Epoch: 179 | Iter: 273200 | Total Loss: 0.005059 | Recon Loss: 0.004198 | Commit Loss: 0.001723 | Perplexity: 1834.366622
2025-09-28 01:11:12,024 Stage: Train 0.5 | Epoch: 179 | Iter: 273400 | Total Loss: 0.005172 | Recon Loss: 0.004315 | Commit Loss: 0.001716 | Perplexity: 1828.289235
Trainning Epoch:  55%|█████▍    | 180/330 [18:23:23<8:27:27, 202.98s/it]2025-09-28 01:11:38,936 Stage: Train 0.5 | Epoch: 180 | Iter: 273600 | Total Loss: 0.005044 | Recon Loss: 0.004192 | Commit Loss: 0.001704 | Perplexity: 1832.517217
2025-09-28 01:12:05,716 Stage: Train 0.5 | Epoch: 180 | Iter: 273800 | Total Loss: 0.005077 | Recon Loss: 0.004223 | Commit Loss: 0.001710 | Perplexity: 1829.749231
2025-09-28 01:12:32,420 Stage: Train 0.5 | Epoch: 180 | Iter: 274000 | Total Loss: 0.004992 | Recon Loss: 0.004136 | Commit Loss: 0.001714 | Perplexity: 1832.892012
2025-09-28 01:12:59,079 Stage: Train 0.5 | Epoch: 180 | Iter: 274200 | Total Loss: 0.005070 | Recon Loss: 0.004212 | Commit Loss: 0.001715 | Perplexity: 1832.610580
2025-09-28 01:13:25,681 Stage: Train 0.5 | Epoch: 180 | Iter: 274400 | Total Loss: 0.005060 | Recon Loss: 0.004207 | Commit Loss: 0.001705 | Perplexity: 1827.357787
2025-09-28 01:13:52,430 Stage: Train 0.5 | Epoch: 180 | Iter: 274600 | Total Loss: 0.005076 | Recon Loss: 0.004221 | Commit Loss: 0.001712 | Perplexity: 1832.356055
2025-09-28 01:14:19,124 Stage: Train 0.5 | Epoch: 180 | Iter: 274800 | Total Loss: 0.005051 | Recon Loss: 0.004196 | Commit Loss: 0.001712 | Perplexity: 1831.693251
Trainning Epoch:  55%|█████▍    | 181/330 [18:26:46<8:24:04, 202.99s/it]2025-09-28 01:14:46,147 Stage: Train 0.5 | Epoch: 181 | Iter: 275000 | Total Loss: 0.005072 | Recon Loss: 0.004219 | Commit Loss: 0.001707 | Perplexity: 1830.480883
2025-09-28 01:15:12,850 Stage: Train 0.5 | Epoch: 181 | Iter: 275200 | Total Loss: 0.005005 | Recon Loss: 0.004151 | Commit Loss: 0.001709 | Perplexity: 1833.267250
2025-09-28 01:15:39,544 Stage: Train 0.5 | Epoch: 181 | Iter: 275400 | Total Loss: 0.005048 | Recon Loss: 0.004197 | Commit Loss: 0.001702 | Perplexity: 1830.381334
2025-09-28 01:16:06,223 Stage: Train 0.5 | Epoch: 181 | Iter: 275600 | Total Loss: 0.005090 | Recon Loss: 0.004234 | Commit Loss: 0.001711 | Perplexity: 1829.799244
2025-09-28 01:16:32,898 Stage: Train 0.5 | Epoch: 181 | Iter: 275800 | Total Loss: 0.005096 | Recon Loss: 0.004235 | Commit Loss: 0.001723 | Perplexity: 1837.053210
2025-09-28 01:16:59,370 Stage: Train 0.5 | Epoch: 181 | Iter: 276000 | Total Loss: 0.005040 | Recon Loss: 0.004189 | Commit Loss: 0.001701 | Perplexity: 1828.758892
2025-09-28 01:17:26,117 Stage: Train 0.5 | Epoch: 181 | Iter: 276200 | Total Loss: 0.005041 | Recon Loss: 0.004187 | Commit Loss: 0.001708 | Perplexity: 1828.465898
2025-09-28 01:17:52,841 Stage: Train 0.5 | Epoch: 181 | Iter: 276400 | Total Loss: 0.005041 | Recon Loss: 0.004179 | Commit Loss: 0.001724 | Perplexity: 1833.125998
Trainning Epoch:  55%|█████▌    | 182/330 [18:30:09<8:20:36, 202.95s/it]2025-09-28 01:18:19,723 Stage: Train 0.5 | Epoch: 182 | Iter: 276600 | Total Loss: 0.005024 | Recon Loss: 0.004169 | Commit Loss: 0.001711 | Perplexity: 1828.351385
2025-09-28 01:18:46,524 Stage: Train 0.5 | Epoch: 182 | Iter: 276800 | Total Loss: 0.005033 | Recon Loss: 0.004180 | Commit Loss: 0.001705 | Perplexity: 1832.054489
2025-09-28 01:19:13,182 Stage: Train 0.5 | Epoch: 182 | Iter: 277000 | Total Loss: 0.005027 | Recon Loss: 0.004172 | Commit Loss: 0.001709 | Perplexity: 1834.203778
2025-09-28 01:19:39,980 Stage: Train 0.5 | Epoch: 182 | Iter: 277200 | Total Loss: 0.005063 | Recon Loss: 0.004204 | Commit Loss: 0.001716 | Perplexity: 1836.327120
2025-09-28 01:20:06,815 Stage: Train 0.5 | Epoch: 182 | Iter: 277400 | Total Loss: 0.005067 | Recon Loss: 0.004207 | Commit Loss: 0.001718 | Perplexity: 1832.593497
2025-09-28 01:20:33,705 Stage: Train 0.5 | Epoch: 182 | Iter: 277600 | Total Loss: 0.005107 | Recon Loss: 0.004249 | Commit Loss: 0.001717 | Perplexity: 1834.751370
2025-09-28 01:21:00,436 Stage: Train 0.5 | Epoch: 182 | Iter: 277800 | Total Loss: 0.005091 | Recon Loss: 0.004233 | Commit Loss: 0.001717 | Perplexity: 1834.663762
Trainning Epoch:  55%|█████▌    | 183/330 [18:33:32<8:17:40, 203.13s/it]2025-09-28 01:21:27,484 Stage: Train 0.5 | Epoch: 183 | Iter: 278000 | Total Loss: 0.005042 | Recon Loss: 0.004186 | Commit Loss: 0.001711 | Perplexity: 1831.792399
2025-09-28 01:21:54,279 Stage: Train 0.5 | Epoch: 183 | Iter: 278200 | Total Loss: 0.005012 | Recon Loss: 0.004161 | Commit Loss: 0.001701 | Perplexity: 1828.924055
2025-09-28 01:22:20,910 Stage: Train 0.5 | Epoch: 183 | Iter: 278400 | Total Loss: 0.005001 | Recon Loss: 0.004145 | Commit Loss: 0.001712 | Perplexity: 1832.682750
2025-09-28 01:22:47,673 Stage: Train 0.5 | Epoch: 183 | Iter: 278600 | Total Loss: 0.005087 | Recon Loss: 0.004232 | Commit Loss: 0.001710 | Perplexity: 1831.654345
2025-09-28 01:23:14,367 Stage: Train 0.5 | Epoch: 183 | Iter: 278800 | Total Loss: 0.005097 | Recon Loss: 0.004242 | Commit Loss: 0.001711 | Perplexity: 1831.661086
2025-09-28 01:23:41,121 Stage: Train 0.5 | Epoch: 183 | Iter: 279000 | Total Loss: 0.005001 | Recon Loss: 0.004148 | Commit Loss: 0.001705 | Perplexity: 1830.355770
2025-09-28 01:24:07,908 Stage: Train 0.5 | Epoch: 183 | Iter: 279200 | Total Loss: 0.005056 | Recon Loss: 0.004205 | Commit Loss: 0.001703 | Perplexity: 1829.852181
2025-09-28 01:24:34,637 Stage: Train 0.5 | Epoch: 183 | Iter: 279400 | Total Loss: 0.005088 | Recon Loss: 0.004224 | Commit Loss: 0.001727 | Perplexity: 1833.752515
Trainning Epoch:  56%|█████▌    | 184/330 [18:36:56<8:14:27, 203.21s/it]2025-09-28 01:25:01,695 Stage: Train 0.5 | Epoch: 184 | Iter: 279600 | Total Loss: 0.005020 | Recon Loss: 0.004163 | Commit Loss: 0.001713 | Perplexity: 1832.766686
2025-09-28 01:25:28,368 Stage: Train 0.5 | Epoch: 184 | Iter: 279800 | Total Loss: 0.005045 | Recon Loss: 0.004188 | Commit Loss: 0.001713 | Perplexity: 1831.836827
2025-09-28 01:25:55,140 Stage: Train 0.5 | Epoch: 184 | Iter: 280000 | Total Loss: 0.005037 | Recon Loss: 0.004183 | Commit Loss: 0.001708 | Perplexity: 1831.848171
2025-09-28 01:25:55,141 Saving model at iteration 280000
2025-09-28 01:25:55,791 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000
2025-09-28 01:25:56,329 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/model.safetensors
2025-09-28 01:25:56,870 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/optimizer.bin
2025-09-28 01:25:56,870 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/scheduler.bin
2025-09-28 01:25:56,870 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/sampler.bin
2025-09-28 01:25:56,871 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_185_step_280000/random_states_0.pkl
2025-09-28 01:26:23,628 Stage: Train 0.5 | Epoch: 184 | Iter: 280200 | Total Loss: 0.005059 | Recon Loss: 0.004201 | Commit Loss: 0.001716 | Perplexity: 1835.543969
2025-09-28 01:26:50,364 Stage: Train 0.5 | Epoch: 184 | Iter: 280400 | Total Loss: 0.005076 | Recon Loss: 0.004219 | Commit Loss: 0.001714 | Perplexity: 1829.840530
2025-09-28 01:27:16,905 Stage: Train 0.5 | Epoch: 184 | Iter: 280600 | Total Loss: 0.005039 | Recon Loss: 0.004182 | Commit Loss: 0.001714 | Perplexity: 1837.812885
2025-09-28 01:27:43,508 Stage: Train 0.5 | Epoch: 184 | Iter: 280800 | Total Loss: 0.005017 | Recon Loss: 0.004161 | Commit Loss: 0.001711 | Perplexity: 1830.959420
2025-09-28 01:28:10,086 Stage: Train 0.5 | Epoch: 184 | Iter: 281000 | Total Loss: 0.004986 | Recon Loss: 0.004132 | Commit Loss: 0.001706 | Perplexity: 1832.705902
Trainning Epoch:  56%|█████▌    | 185/330 [18:40:20<8:12:04, 203.62s/it]2025-09-28 01:28:37,226 Stage: Train 0.5 | Epoch: 185 | Iter: 281200 | Total Loss: 0.005059 | Recon Loss: 0.004202 | Commit Loss: 0.001713 | Perplexity: 1832.339556
2025-09-28 01:29:03,956 Stage: Train 0.5 | Epoch: 185 | Iter: 281400 | Total Loss: 0.005041 | Recon Loss: 0.004187 | Commit Loss: 0.001709 | Perplexity: 1831.693768
2025-09-28 01:29:30,692 Stage: Train 0.5 | Epoch: 185 | Iter: 281600 | Total Loss: 0.005022 | Recon Loss: 0.004168 | Commit Loss: 0.001709 | Perplexity: 1834.324680
2025-09-28 01:29:57,305 Stage: Train 0.5 | Epoch: 185 | Iter: 281800 | Total Loss: 0.005065 | Recon Loss: 0.004209 | Commit Loss: 0.001713 | Perplexity: 1834.683754
2025-09-28 01:30:23,972 Stage: Train 0.5 | Epoch: 185 | Iter: 282000 | Total Loss: 0.004973 | Recon Loss: 0.004119 | Commit Loss: 0.001707 | Perplexity: 1832.168860
2025-09-28 01:30:50,544 Stage: Train 0.5 | Epoch: 185 | Iter: 282200 | Total Loss: 0.005003 | Recon Loss: 0.004144 | Commit Loss: 0.001717 | Perplexity: 1832.895665
2025-09-28 01:31:17,263 Stage: Train 0.5 | Epoch: 185 | Iter: 282400 | Total Loss: 0.005108 | Recon Loss: 0.004253 | Commit Loss: 0.001709 | Perplexity: 1833.926771
Trainning Epoch:  56%|█████▋    | 186/330 [18:43:43<8:08:19, 203.47s/it]2025-09-28 01:31:44,372 Stage: Train 0.5 | Epoch: 186 | Iter: 282600 | Total Loss: 0.005034 | Recon Loss: 0.004178 | Commit Loss: 0.001711 | Perplexity: 1831.224694
2025-09-28 01:32:11,226 Stage: Train 0.5 | Epoch: 186 | Iter: 282800 | Total Loss: 0.004989 | Recon Loss: 0.004137 | Commit Loss: 0.001705 | Perplexity: 1831.110424
2025-09-28 01:32:38,031 Stage: Train 0.5 | Epoch: 186 | Iter: 283000 | Total Loss: 0.005020 | Recon Loss: 0.004163 | Commit Loss: 0.001715 | Perplexity: 1834.770342
2025-09-28 01:33:04,696 Stage: Train 0.5 | Epoch: 186 | Iter: 283200 | Total Loss: 0.004980 | Recon Loss: 0.004124 | Commit Loss: 0.001712 | Perplexity: 1834.002194
2025-09-28 01:33:31,296 Stage: Train 0.5 | Epoch: 186 | Iter: 283400 | Total Loss: 0.005057 | Recon Loss: 0.004200 | Commit Loss: 0.001715 | Perplexity: 1834.719789
2025-09-28 01:33:57,916 Stage: Train 0.5 | Epoch: 186 | Iter: 283600 | Total Loss: 0.005044 | Recon Loss: 0.004185 | Commit Loss: 0.001717 | Perplexity: 1836.133433
2025-09-28 01:34:24,543 Stage: Train 0.5 | Epoch: 186 | Iter: 283800 | Total Loss: 0.004983 | Recon Loss: 0.004126 | Commit Loss: 0.001713 | Perplexity: 1836.444930
2025-09-28 01:34:51,277 Stage: Train 0.5 | Epoch: 186 | Iter: 284000 | Total Loss: 0.005087 | Recon Loss: 0.004228 | Commit Loss: 0.001718 | Perplexity: 1833.556671
Trainning Epoch:  57%|█████▋    | 187/330 [18:47:07<8:04:43, 203.38s/it]2025-09-28 01:35:18,274 Stage: Train 0.5 | Epoch: 187 | Iter: 284200 | Total Loss: 0.004967 | Recon Loss: 0.004112 | Commit Loss: 0.001709 | Perplexity: 1830.658688
2025-09-28 01:35:44,975 Stage: Train 0.5 | Epoch: 187 | Iter: 284400 | Total Loss: 0.005021 | Recon Loss: 0.004162 | Commit Loss: 0.001719 | Perplexity: 1836.285409
2025-09-28 01:36:11,643 Stage: Train 0.5 | Epoch: 187 | Iter: 284600 | Total Loss: 0.005007 | Recon Loss: 0.004155 | Commit Loss: 0.001705 | Perplexity: 1834.387596
2025-09-28 01:36:38,201 Stage: Train 0.5 | Epoch: 187 | Iter: 284800 | Total Loss: 0.005078 | Recon Loss: 0.004222 | Commit Loss: 0.001712 | Perplexity: 1832.375946
2025-09-28 01:37:05,061 Stage: Train 0.5 | Epoch: 187 | Iter: 285000 | Total Loss: 0.005033 | Recon Loss: 0.004174 | Commit Loss: 0.001717 | Perplexity: 1837.058063
2025-09-28 01:37:31,728 Stage: Train 0.5 | Epoch: 187 | Iter: 285200 | Total Loss: 0.005033 | Recon Loss: 0.004177 | Commit Loss: 0.001711 | Perplexity: 1832.537540
2025-09-28 01:37:58,273 Stage: Train 0.5 | Epoch: 187 | Iter: 285400 | Total Loss: 0.004967 | Recon Loss: 0.004111 | Commit Loss: 0.001712 | Perplexity: 1835.872052
Trainning Epoch:  57%|█████▋    | 188/330 [18:50:29<8:00:58, 203.23s/it]2025-09-28 01:38:25,280 Stage: Train 0.5 | Epoch: 188 | Iter: 285600 | Total Loss: 0.005043 | Recon Loss: 0.004185 | Commit Loss: 0.001715 | Perplexity: 1836.514792
2025-09-28 01:38:51,844 Stage: Train 0.5 | Epoch: 188 | Iter: 285800 | Total Loss: 0.004947 | Recon Loss: 0.004096 | Commit Loss: 0.001701 | Perplexity: 1833.750504
2025-09-28 01:39:18,560 Stage: Train 0.5 | Epoch: 188 | Iter: 286000 | Total Loss: 0.005034 | Recon Loss: 0.004178 | Commit Loss: 0.001710 | Perplexity: 1832.167249
2025-09-28 01:39:45,220 Stage: Train 0.5 | Epoch: 188 | Iter: 286200 | Total Loss: 0.004972 | Recon Loss: 0.004112 | Commit Loss: 0.001720 | Perplexity: 1836.733922
2025-09-28 01:40:11,890 Stage: Train 0.5 | Epoch: 188 | Iter: 286400 | Total Loss: 0.004996 | Recon Loss: 0.004143 | Commit Loss: 0.001707 | Perplexity: 1830.720044
2025-09-28 01:40:38,593 Stage: Train 0.5 | Epoch: 188 | Iter: 286600 | Total Loss: 0.005010 | Recon Loss: 0.004152 | Commit Loss: 0.001716 | Perplexity: 1836.431107
2025-09-28 01:41:05,198 Stage: Train 0.5 | Epoch: 188 | Iter: 286800 | Total Loss: 0.005061 | Recon Loss: 0.004199 | Commit Loss: 0.001725 | Perplexity: 1835.245994
2025-09-28 01:41:31,870 Stage: Train 0.5 | Epoch: 188 | Iter: 287000 | Total Loss: 0.005044 | Recon Loss: 0.004185 | Commit Loss: 0.001717 | Perplexity: 1835.642588
Trainning Epoch:  57%|█████▋    | 189/330 [18:53:52<7:57:15, 203.09s/it]2025-09-28 01:41:58,764 Stage: Train 0.5 | Epoch: 189 | Iter: 287200 | Total Loss: 0.004984 | Recon Loss: 0.004130 | Commit Loss: 0.001707 | Perplexity: 1833.157334
2025-09-28 01:42:25,387 Stage: Train 0.5 | Epoch: 189 | Iter: 287400 | Total Loss: 0.005056 | Recon Loss: 0.004194 | Commit Loss: 0.001723 | Perplexity: 1839.373436
2025-09-28 01:42:51,785 Stage: Train 0.5 | Epoch: 189 | Iter: 287600 | Total Loss: 0.004991 | Recon Loss: 0.004136 | Commit Loss: 0.001710 | Perplexity: 1833.787357
2025-09-28 01:43:18,408 Stage: Train 0.5 | Epoch: 189 | Iter: 287800 | Total Loss: 0.004995 | Recon Loss: 0.004136 | Commit Loss: 0.001718 | Perplexity: 1837.273991
2025-09-28 01:43:45,052 Stage: Train 0.5 | Epoch: 189 | Iter: 288000 | Total Loss: 0.005014 | Recon Loss: 0.004159 | Commit Loss: 0.001710 | Perplexity: 1831.931330
2025-09-28 01:44:11,587 Stage: Train 0.5 | Epoch: 189 | Iter: 288200 | Total Loss: 0.005001 | Recon Loss: 0.004142 | Commit Loss: 0.001717 | Perplexity: 1832.664686
2025-09-28 01:44:38,336 Stage: Train 0.5 | Epoch: 189 | Iter: 288400 | Total Loss: 0.005042 | Recon Loss: 0.004186 | Commit Loss: 0.001711 | Perplexity: 1830.469125
2025-09-28 01:45:05,005 Stage: Train 0.5 | Epoch: 189 | Iter: 288600 | Total Loss: 0.004967 | Recon Loss: 0.004113 | Commit Loss: 0.001710 | Perplexity: 1835.744879
Trainning Epoch:  58%|█████▊    | 190/330 [18:57:15<7:53:21, 202.86s/it]2025-09-28 01:45:32,013 Stage: Train 0.5 | Epoch: 190 | Iter: 288800 | Total Loss: 0.004989 | Recon Loss: 0.004136 | Commit Loss: 0.001706 | Perplexity: 1833.014360
2025-09-28 01:45:58,735 Stage: Train 0.5 | Epoch: 190 | Iter: 289000 | Total Loss: 0.005101 | Recon Loss: 0.004236 | Commit Loss: 0.001730 | Perplexity: 1838.348455
2025-09-28 01:46:25,404 Stage: Train 0.5 | Epoch: 190 | Iter: 289200 | Total Loss: 0.005005 | Recon Loss: 0.004150 | Commit Loss: 0.001709 | Perplexity: 1831.505744
2025-09-28 01:46:52,118 Stage: Train 0.5 | Epoch: 190 | Iter: 289400 | Total Loss: 0.004978 | Recon Loss: 0.004115 | Commit Loss: 0.001724 | Perplexity: 1836.498276
2025-09-28 01:47:18,813 Stage: Train 0.5 | Epoch: 190 | Iter: 289600 | Total Loss: 0.004964 | Recon Loss: 0.004107 | Commit Loss: 0.001714 | Perplexity: 1830.560565
2025-09-28 01:47:45,587 Stage: Train 0.5 | Epoch: 190 | Iter: 289800 | Total Loss: 0.005004 | Recon Loss: 0.004150 | Commit Loss: 0.001708 | Perplexity: 1830.958715
2025-09-28 01:48:12,220 Stage: Train 0.5 | Epoch: 190 | Iter: 290000 | Total Loss: 0.005021 | Recon Loss: 0.004161 | Commit Loss: 0.001719 | Perplexity: 1831.317145
Trainning Epoch:  58%|█████▊    | 191/330 [19:00:38<7:50:07, 202.93s/it]2025-09-28 01:48:39,142 Stage: Train 0.5 | Epoch: 191 | Iter: 290200 | Total Loss: 0.005041 | Recon Loss: 0.004187 | Commit Loss: 0.001709 | Perplexity: 1832.729434
2025-09-28 01:49:05,772 Stage: Train 0.5 | Epoch: 191 | Iter: 290400 | Total Loss: 0.005034 | Recon Loss: 0.004177 | Commit Loss: 0.001714 | Perplexity: 1834.989128
2025-09-28 01:49:32,348 Stage: Train 0.5 | Epoch: 191 | Iter: 290600 | Total Loss: 0.004971 | Recon Loss: 0.004113 | Commit Loss: 0.001716 | Perplexity: 1836.559081
2025-09-28 01:49:58,977 Stage: Train 0.5 | Epoch: 191 | Iter: 290800 | Total Loss: 0.004991 | Recon Loss: 0.004137 | Commit Loss: 0.001708 | Perplexity: 1833.821152
2025-09-28 01:50:25,599 Stage: Train 0.5 | Epoch: 191 | Iter: 291000 | Total Loss: 0.004971 | Recon Loss: 0.004115 | Commit Loss: 0.001711 | Perplexity: 1835.591716
2025-09-28 01:50:52,424 Stage: Train 0.5 | Epoch: 191 | Iter: 291200 | Total Loss: 0.004999 | Recon Loss: 0.004140 | Commit Loss: 0.001717 | Perplexity: 1836.503684
2025-09-28 01:51:19,155 Stage: Train 0.5 | Epoch: 191 | Iter: 291400 | Total Loss: 0.004975 | Recon Loss: 0.004117 | Commit Loss: 0.001715 | Perplexity: 1833.723776
2025-09-28 01:51:45,908 Stage: Train 0.5 | Epoch: 191 | Iter: 291600 | Total Loss: 0.004967 | Recon Loss: 0.004109 | Commit Loss: 0.001717 | Perplexity: 1831.538200
Trainning Epoch:  58%|█████▊    | 192/330 [19:04:01<7:46:43, 202.92s/it]2025-09-28 01:52:12,857 Stage: Train 0.5 | Epoch: 192 | Iter: 291800 | Total Loss: 0.005066 | Recon Loss: 0.004204 | Commit Loss: 0.001725 | Perplexity: 1834.755475
2025-09-28 01:52:39,632 Stage: Train 0.5 | Epoch: 192 | Iter: 292000 | Total Loss: 0.004981 | Recon Loss: 0.004127 | Commit Loss: 0.001708 | Perplexity: 1832.731179
2025-09-28 01:53:06,176 Stage: Train 0.5 | Epoch: 192 | Iter: 292200 | Total Loss: 0.004943 | Recon Loss: 0.004090 | Commit Loss: 0.001706 | Perplexity: 1830.471224
2025-09-28 01:53:32,823 Stage: Train 0.5 | Epoch: 192 | Iter: 292400 | Total Loss: 0.005020 | Recon Loss: 0.004160 | Commit Loss: 0.001720 | Perplexity: 1834.704178
2025-09-28 01:53:59,512 Stage: Train 0.5 | Epoch: 192 | Iter: 292600 | Total Loss: 0.005018 | Recon Loss: 0.004163 | Commit Loss: 0.001710 | Perplexity: 1830.677740
2025-09-28 01:54:26,131 Stage: Train 0.5 | Epoch: 192 | Iter: 292800 | Total Loss: 0.005064 | Recon Loss: 0.004206 | Commit Loss: 0.001716 | Perplexity: 1832.915737
2025-09-28 01:54:52,853 Stage: Train 0.5 | Epoch: 192 | Iter: 293000 | Total Loss: 0.005017 | Recon Loss: 0.004159 | Commit Loss: 0.001717 | Perplexity: 1835.507167
Trainning Epoch:  58%|█████▊    | 193/330 [19:07:23<7:43:15, 202.88s/it]2025-09-28 01:55:19,824 Stage: Train 0.5 | Epoch: 193 | Iter: 293200 | Total Loss: 0.004982 | Recon Loss: 0.004131 | Commit Loss: 0.001703 | Perplexity: 1831.749787
2025-09-28 01:55:46,549 Stage: Train 0.5 | Epoch: 193 | Iter: 293400 | Total Loss: 0.004960 | Recon Loss: 0.004105 | Commit Loss: 0.001710 | Perplexity: 1832.632758
2025-09-28 01:56:13,184 Stage: Train 0.5 | Epoch: 193 | Iter: 293600 | Total Loss: 0.004956 | Recon Loss: 0.004100 | Commit Loss: 0.001711 | Perplexity: 1837.281125
2025-09-28 01:56:39,800 Stage: Train 0.5 | Epoch: 193 | Iter: 293800 | Total Loss: 0.005055 | Recon Loss: 0.004196 | Commit Loss: 0.001717 | Perplexity: 1834.913950
2025-09-28 01:57:06,629 Stage: Train 0.5 | Epoch: 193 | Iter: 294000 | Total Loss: 0.004989 | Recon Loss: 0.004134 | Commit Loss: 0.001710 | Perplexity: 1833.837999
2025-09-28 01:57:33,322 Stage: Train 0.5 | Epoch: 193 | Iter: 294200 | Total Loss: 0.004979 | Recon Loss: 0.004119 | Commit Loss: 0.001721 | Perplexity: 1836.418564
2025-09-28 01:58:00,081 Stage: Train 0.5 | Epoch: 193 | Iter: 294400 | Total Loss: 0.005042 | Recon Loss: 0.004180 | Commit Loss: 0.001724 | Perplexity: 1836.372453
2025-09-28 01:58:26,727 Stage: Train 0.5 | Epoch: 193 | Iter: 294600 | Total Loss: 0.004991 | Recon Loss: 0.004131 | Commit Loss: 0.001719 | Perplexity: 1835.626382
Trainning Epoch:  59%|█████▉    | 194/330 [19:10:46<7:39:57, 202.92s/it]2025-09-28 01:58:53,614 Stage: Train 0.5 | Epoch: 194 | Iter: 294800 | Total Loss: 0.004891 | Recon Loss: 0.004038 | Commit Loss: 0.001704 | Perplexity: 1831.466351
2025-09-28 01:59:20,155 Stage: Train 0.5 | Epoch: 194 | Iter: 295000 | Total Loss: 0.004988 | Recon Loss: 0.004127 | Commit Loss: 0.001722 | Perplexity: 1837.835289
2025-09-28 01:59:46,618 Stage: Train 0.5 | Epoch: 194 | Iter: 295200 | Total Loss: 0.004999 | Recon Loss: 0.004144 | Commit Loss: 0.001711 | Perplexity: 1830.809386
2025-09-28 02:00:13,303 Stage: Train 0.5 | Epoch: 194 | Iter: 295400 | Total Loss: 0.004979 | Recon Loss: 0.004121 | Commit Loss: 0.001716 | Perplexity: 1833.948732
2025-09-28 02:00:39,892 Stage: Train 0.5 | Epoch: 194 | Iter: 295600 | Total Loss: 0.004963 | Recon Loss: 0.004102 | Commit Loss: 0.001721 | Perplexity: 1833.655041
2025-09-28 02:01:06,567 Stage: Train 0.5 | Epoch: 194 | Iter: 295800 | Total Loss: 0.004944 | Recon Loss: 0.004089 | Commit Loss: 0.001710 | Perplexity: 1835.076547
2025-09-28 02:01:33,104 Stage: Train 0.5 | Epoch: 194 | Iter: 296000 | Total Loss: 0.005016 | Recon Loss: 0.004160 | Commit Loss: 0.001711 | Perplexity: 1834.336349
2025-09-28 02:01:59,777 Stage: Train 0.5 | Epoch: 194 | Iter: 296200 | Total Loss: 0.004984 | Recon Loss: 0.004121 | Commit Loss: 0.001725 | Perplexity: 1839.534542
Trainning Epoch:  59%|█████▉    | 195/330 [19:14:09<7:36:10, 202.75s/it]2025-09-28 02:02:26,534 Stage: Train 0.5 | Epoch: 195 | Iter: 296400 | Total Loss: 0.005006 | Recon Loss: 0.004148 | Commit Loss: 0.001715 | Perplexity: 1836.362532
2025-09-28 02:02:53,085 Stage: Train 0.5 | Epoch: 195 | Iter: 296600 | Total Loss: 0.004974 | Recon Loss: 0.004116 | Commit Loss: 0.001716 | Perplexity: 1835.779263
2025-09-28 02:03:19,683 Stage: Train 0.5 | Epoch: 195 | Iter: 296800 | Total Loss: 0.004959 | Recon Loss: 0.004101 | Commit Loss: 0.001716 | Perplexity: 1832.292041
2025-09-28 02:03:46,376 Stage: Train 0.5 | Epoch: 195 | Iter: 297000 | Total Loss: 0.004964 | Recon Loss: 0.004106 | Commit Loss: 0.001716 | Perplexity: 1832.965289
2025-09-28 02:04:12,992 Stage: Train 0.5 | Epoch: 195 | Iter: 297200 | Total Loss: 0.005006 | Recon Loss: 0.004151 | Commit Loss: 0.001709 | Perplexity: 1833.861062
2025-09-28 02:04:39,644 Stage: Train 0.5 | Epoch: 195 | Iter: 297400 | Total Loss: 0.004927 | Recon Loss: 0.004070 | Commit Loss: 0.001714 | Perplexity: 1835.669146
2025-09-28 02:05:06,236 Stage: Train 0.5 | Epoch: 195 | Iter: 297600 | Total Loss: 0.005014 | Recon Loss: 0.004151 | Commit Loss: 0.001725 | Perplexity: 1840.417622
Trainning Epoch:  59%|█████▉    | 196/330 [19:17:31<7:32:30, 202.62s/it]2025-09-28 02:05:33,227 Stage: Train 0.5 | Epoch: 196 | Iter: 297800 | Total Loss: 0.005019 | Recon Loss: 0.004167 | Commit Loss: 0.001704 | Perplexity: 1832.788180
2025-09-28 02:05:59,964 Stage: Train 0.5 | Epoch: 196 | Iter: 298000 | Total Loss: 0.004922 | Recon Loss: 0.004068 | Commit Loss: 0.001707 | Perplexity: 1834.356440
2025-09-28 02:06:26,569 Stage: Train 0.5 | Epoch: 196 | Iter: 298200 | Total Loss: 0.004991 | Recon Loss: 0.004133 | Commit Loss: 0.001717 | Perplexity: 1835.001246
2025-09-28 02:06:53,265 Stage: Train 0.5 | Epoch: 196 | Iter: 298400 | Total Loss: 0.004962 | Recon Loss: 0.004104 | Commit Loss: 0.001716 | Perplexity: 1836.747476
2025-09-28 02:07:19,881 Stage: Train 0.5 | Epoch: 196 | Iter: 298600 | Total Loss: 0.005052 | Recon Loss: 0.004196 | Commit Loss: 0.001714 | Perplexity: 1834.797445
2025-09-28 02:07:46,631 Stage: Train 0.5 | Epoch: 196 | Iter: 298800 | Total Loss: 0.004970 | Recon Loss: 0.004111 | Commit Loss: 0.001718 | Perplexity: 1836.996999
2025-09-28 02:08:13,259 Stage: Train 0.5 | Epoch: 196 | Iter: 299000 | Total Loss: 0.004963 | Recon Loss: 0.004104 | Commit Loss: 0.001718 | Perplexity: 1838.248016
2025-09-28 02:08:39,895 Stage: Train 0.5 | Epoch: 196 | Iter: 299200 | Total Loss: 0.004950 | Recon Loss: 0.004087 | Commit Loss: 0.001725 | Perplexity: 1833.200652
Trainning Epoch:  60%|█████▉    | 197/330 [19:20:54<7:29:16, 202.68s/it]2025-09-28 02:09:06,887 Stage: Train 0.5 | Epoch: 197 | Iter: 299400 | Total Loss: 0.004941 | Recon Loss: 0.004082 | Commit Loss: 0.001717 | Perplexity: 1838.960069
2025-09-28 02:09:33,519 Stage: Train 0.5 | Epoch: 197 | Iter: 299600 | Total Loss: 0.005026 | Recon Loss: 0.004170 | Commit Loss: 0.001712 | Perplexity: 1836.159902
2025-09-28 02:10:00,187 Stage: Train 0.5 | Epoch: 197 | Iter: 299800 | Total Loss: 0.004959 | Recon Loss: 0.004100 | Commit Loss: 0.001719 | Perplexity: 1836.437294
2025-09-28 02:10:26,960 Stage: Train 0.5 | Epoch: 197 | Iter: 300000 | Total Loss: 0.004960 | Recon Loss: 0.004099 | Commit Loss: 0.001722 | Perplexity: 1836.959320
2025-09-28 02:10:26,960 Saving model at iteration 300000
2025-09-28 02:10:27,195 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000
2025-09-28 02:10:27,743 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/model.safetensors
2025-09-28 02:10:28,282 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/optimizer.bin
2025-09-28 02:10:28,282 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/scheduler.bin
2025-09-28 02:10:28,282 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/sampler.bin
2025-09-28 02:10:28,283 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_198_step_300000/random_states_0.pkl
2025-09-28 02:10:55,449 Stage: Train 0.5 | Epoch: 197 | Iter: 300200 | Total Loss: 0.004994 | Recon Loss: 0.004136 | Commit Loss: 0.001716 | Perplexity: 1836.159130
2025-09-28 02:11:22,075 Stage: Train 0.5 | Epoch: 197 | Iter: 300400 | Total Loss: 0.004959 | Recon Loss: 0.004099 | Commit Loss: 0.001719 | Perplexity: 1842.196754
2025-09-28 02:11:48,685 Stage: Train 0.5 | Epoch: 197 | Iter: 300600 | Total Loss: 0.005026 | Recon Loss: 0.004165 | Commit Loss: 0.001723 | Perplexity: 1837.113347
Trainning Epoch:  60%|██████    | 198/330 [19:24:19<7:27:12, 203.28s/it]2025-09-28 02:12:15,625 Stage: Train 0.5 | Epoch: 198 | Iter: 300800 | Total Loss: 0.004941 | Recon Loss: 0.004083 | Commit Loss: 0.001717 | Perplexity: 1834.673313
2025-09-28 02:12:42,177 Stage: Train 0.5 | Epoch: 198 | Iter: 301000 | Total Loss: 0.004988 | Recon Loss: 0.004138 | Commit Loss: 0.001701 | Perplexity: 1833.192656
2025-09-28 02:13:08,786 Stage: Train 0.5 | Epoch: 198 | Iter: 301200 | Total Loss: 0.004964 | Recon Loss: 0.004103 | Commit Loss: 0.001722 | Perplexity: 1838.505265
2025-09-28 02:13:35,469 Stage: Train 0.5 | Epoch: 198 | Iter: 301400 | Total Loss: 0.004969 | Recon Loss: 0.004111 | Commit Loss: 0.001716 | Perplexity: 1836.015134
2025-09-28 02:14:02,068 Stage: Train 0.5 | Epoch: 198 | Iter: 301600 | Total Loss: 0.004984 | Recon Loss: 0.004132 | Commit Loss: 0.001704 | Perplexity: 1835.430933
2025-09-28 02:14:28,772 Stage: Train 0.5 | Epoch: 198 | Iter: 301800 | Total Loss: 0.004973 | Recon Loss: 0.004114 | Commit Loss: 0.001717 | Perplexity: 1837.300177
2025-09-28 02:14:55,421 Stage: Train 0.5 | Epoch: 198 | Iter: 302000 | Total Loss: 0.004992 | Recon Loss: 0.004134 | Commit Loss: 0.001716 | Perplexity: 1839.191063
2025-09-28 02:15:22,130 Stage: Train 0.5 | Epoch: 198 | Iter: 302200 | Total Loss: 0.004972 | Recon Loss: 0.004111 | Commit Loss: 0.001722 | Perplexity: 1839.567986
Trainning Epoch:  60%|██████    | 199/330 [19:27:41<7:23:22, 203.07s/it]2025-09-28 02:15:49,065 Stage: Train 0.5 | Epoch: 199 | Iter: 302400 | Total Loss: 0.004973 | Recon Loss: 0.004117 | Commit Loss: 0.001713 | Perplexity: 1833.114357
2025-09-28 02:16:15,639 Stage: Train 0.5 | Epoch: 199 | Iter: 302600 | Total Loss: 0.004972 | Recon Loss: 0.004113 | Commit Loss: 0.001718 | Perplexity: 1834.476077
2025-09-28 02:16:42,375 Stage: Train 0.5 | Epoch: 199 | Iter: 302800 | Total Loss: 0.004968 | Recon Loss: 0.004110 | Commit Loss: 0.001716 | Perplexity: 1838.013033
2025-09-28 02:17:09,096 Stage: Train 0.5 | Epoch: 199 | Iter: 303000 | Total Loss: 0.004951 | Recon Loss: 0.004093 | Commit Loss: 0.001715 | Perplexity: 1837.381273
2025-09-28 02:17:35,748 Stage: Train 0.5 | Epoch: 199 | Iter: 303200 | Total Loss: 0.004984 | Recon Loss: 0.004122 | Commit Loss: 0.001724 | Perplexity: 1841.916891
2025-09-28 02:18:02,407 Stage: Train 0.5 | Epoch: 199 | Iter: 303400 | Total Loss: 0.004976 | Recon Loss: 0.004121 | Commit Loss: 0.001709 | Perplexity: 1831.989512
2025-09-28 02:18:29,059 Stage: Train 0.5 | Epoch: 199 | Iter: 303600 | Total Loss: 0.004934 | Recon Loss: 0.004076 | Commit Loss: 0.001714 | Perplexity: 1840.597191
2025-09-28 02:18:55,787 Stage: Train 0.5 | Epoch: 199 | Iter: 303800 | Total Loss: 0.004964 | Recon Loss: 0.004102 | Commit Loss: 0.001722 | Perplexity: 1837.628275
Trainning Epoch:  61%|██████    | 200/330 [19:31:04<7:19:52, 203.02s/it]2025-09-28 02:19:22,837 Stage: Train 0.5 | Epoch: 200 | Iter: 304000 | Total Loss: 0.004971 | Recon Loss: 0.004113 | Commit Loss: 0.001716 | Perplexity: 1839.079988
2025-09-28 02:19:49,603 Stage: Train 0.5 | Epoch: 200 | Iter: 304200 | Total Loss: 0.004936 | Recon Loss: 0.004076 | Commit Loss: 0.001720 | Perplexity: 1835.267895
2025-09-28 02:20:16,420 Stage: Train 0.5 | Epoch: 200 | Iter: 304400 | Total Loss: 0.004965 | Recon Loss: 0.004109 | Commit Loss: 0.001713 | Perplexity: 1835.645950
2025-09-28 02:20:43,149 Stage: Train 0.5 | Epoch: 200 | Iter: 304600 | Total Loss: 0.004989 | Recon Loss: 0.004131 | Commit Loss: 0.001716 | Perplexity: 1837.239805
2025-09-28 02:21:09,732 Stage: Train 0.5 | Epoch: 200 | Iter: 304800 | Total Loss: 0.004927 | Recon Loss: 0.004070 | Commit Loss: 0.001715 | Perplexity: 1838.753298
2025-09-28 02:21:36,367 Stage: Train 0.5 | Epoch: 200 | Iter: 305000 | Total Loss: 0.004992 | Recon Loss: 0.004136 | Commit Loss: 0.001712 | Perplexity: 1833.286926
2025-09-28 02:22:02,973 Stage: Train 0.5 | Epoch: 200 | Iter: 305200 | Total Loss: 0.004956 | Recon Loss: 0.004100 | Commit Loss: 0.001712 | Perplexity: 1837.950261
Trainning Epoch:  61%|██████    | 201/330 [19:34:27<7:16:33, 203.05s/it]2025-09-28 02:22:30,068 Stage: Train 0.5 | Epoch: 201 | Iter: 305400 | Total Loss: 0.004927 | Recon Loss: 0.004069 | Commit Loss: 0.001716 | Perplexity: 1836.335182
2025-09-28 02:22:56,781 Stage: Train 0.5 | Epoch: 201 | Iter: 305600 | Total Loss: 0.004932 | Recon Loss: 0.004075 | Commit Loss: 0.001713 | Perplexity: 1838.759467
2025-09-28 02:23:23,434 Stage: Train 0.5 | Epoch: 201 | Iter: 305800 | Total Loss: 0.004973 | Recon Loss: 0.004119 | Commit Loss: 0.001708 | Perplexity: 1835.162711
2025-09-28 02:23:50,186 Stage: Train 0.5 | Epoch: 201 | Iter: 306000 | Total Loss: 0.004906 | Recon Loss: 0.004050 | Commit Loss: 0.001713 | Perplexity: 1835.077070
2025-09-28 02:24:16,877 Stage: Train 0.5 | Epoch: 201 | Iter: 306200 | Total Loss: 0.004969 | Recon Loss: 0.004111 | Commit Loss: 0.001717 | Perplexity: 1839.084259
2025-09-28 02:24:43,581 Stage: Train 0.5 | Epoch: 201 | Iter: 306400 | Total Loss: 0.004951 | Recon Loss: 0.004088 | Commit Loss: 0.001726 | Perplexity: 1842.682220
2025-09-28 02:25:10,310 Stage: Train 0.5 | Epoch: 201 | Iter: 306600 | Total Loss: 0.004930 | Recon Loss: 0.004074 | Commit Loss: 0.001712 | Perplexity: 1836.523259
2025-09-28 02:25:36,839 Stage: Train 0.5 | Epoch: 201 | Iter: 306800 | Total Loss: 0.004901 | Recon Loss: 0.004040 | Commit Loss: 0.001722 | Perplexity: 1835.160917
Trainning Epoch:  61%|██████    | 202/330 [19:37:50<7:13:08, 203.03s/it]2025-09-28 02:26:03,658 Stage: Train 0.5 | Epoch: 202 | Iter: 307000 | Total Loss: 0.004913 | Recon Loss: 0.004060 | Commit Loss: 0.001706 | Perplexity: 1835.127653
2025-09-28 02:26:30,412 Stage: Train 0.5 | Epoch: 202 | Iter: 307200 | Total Loss: 0.004932 | Recon Loss: 0.004072 | Commit Loss: 0.001721 | Perplexity: 1838.474381
2025-09-28 02:26:57,105 Stage: Train 0.5 | Epoch: 202 | Iter: 307400 | Total Loss: 0.004960 | Recon Loss: 0.004100 | Commit Loss: 0.001719 | Perplexity: 1842.189944
2025-09-28 02:27:23,814 Stage: Train 0.5 | Epoch: 202 | Iter: 307600 | Total Loss: 0.004945 | Recon Loss: 0.004092 | Commit Loss: 0.001707 | Perplexity: 1832.632230
2025-09-28 02:27:50,515 Stage: Train 0.5 | Epoch: 202 | Iter: 307800 | Total Loss: 0.004899 | Recon Loss: 0.004045 | Commit Loss: 0.001708 | Perplexity: 1836.446266
2025-09-28 02:28:17,250 Stage: Train 0.5 | Epoch: 202 | Iter: 308000 | Total Loss: 0.004983 | Recon Loss: 0.004125 | Commit Loss: 0.001716 | Perplexity: 1838.296848
2025-09-28 02:28:43,867 Stage: Train 0.5 | Epoch: 202 | Iter: 308200 | Total Loss: 0.004936 | Recon Loss: 0.004075 | Commit Loss: 0.001721 | Perplexity: 1841.071586
Trainning Epoch:  62%|██████▏   | 203/330 [19:41:13<7:09:39, 202.99s/it]2025-09-28 02:29:10,799 Stage: Train 0.5 | Epoch: 203 | Iter: 308400 | Total Loss: 0.004948 | Recon Loss: 0.004093 | Commit Loss: 0.001711 | Perplexity: 1839.234237
2025-09-28 02:29:37,469 Stage: Train 0.5 | Epoch: 203 | Iter: 308600 | Total Loss: 0.004948 | Recon Loss: 0.004093 | Commit Loss: 0.001711 | Perplexity: 1838.787662
2025-09-28 02:30:04,127 Stage: Train 0.5 | Epoch: 203 | Iter: 308800 | Total Loss: 0.004926 | Recon Loss: 0.004070 | Commit Loss: 0.001712 | Perplexity: 1838.191362
2025-09-28 02:30:30,735 Stage: Train 0.5 | Epoch: 203 | Iter: 309000 | Total Loss: 0.004957 | Recon Loss: 0.004106 | Commit Loss: 0.001702 | Perplexity: 1836.688325
2025-09-28 02:30:57,520 Stage: Train 0.5 | Epoch: 203 | Iter: 309200 | Total Loss: 0.004892 | Recon Loss: 0.004037 | Commit Loss: 0.001711 | Perplexity: 1836.591880
2025-09-28 02:31:24,320 Stage: Train 0.5 | Epoch: 203 | Iter: 309400 | Total Loss: 0.004976 | Recon Loss: 0.004118 | Commit Loss: 0.001715 | Perplexity: 1834.803572
2025-09-28 02:31:50,905 Stage: Train 0.5 | Epoch: 203 | Iter: 309600 | Total Loss: 0.004924 | Recon Loss: 0.004064 | Commit Loss: 0.001719 | Perplexity: 1840.485381
2025-09-28 02:32:17,573 Stage: Train 0.5 | Epoch: 203 | Iter: 309800 | Total Loss: 0.004967 | Recon Loss: 0.004112 | Commit Loss: 0.001708 | Perplexity: 1836.571005
Trainning Epoch:  62%|██████▏   | 204/330 [19:44:36<7:06:15, 202.98s/it]2025-09-28 02:32:44,671 Stage: Train 0.5 | Epoch: 204 | Iter: 310000 | Total Loss: 0.004981 | Recon Loss: 0.004121 | Commit Loss: 0.001719 | Perplexity: 1844.650369
2025-09-28 02:33:11,324 Stage: Train 0.5 | Epoch: 204 | Iter: 310200 | Total Loss: 0.004897 | Recon Loss: 0.004045 | Commit Loss: 0.001703 | Perplexity: 1835.886689
2025-09-28 02:33:38,070 Stage: Train 0.5 | Epoch: 204 | Iter: 310400 | Total Loss: 0.004929 | Recon Loss: 0.004069 | Commit Loss: 0.001719 | Perplexity: 1841.516812
2025-09-28 02:34:04,644 Stage: Train 0.5 | Epoch: 204 | Iter: 310600 | Total Loss: 0.004926 | Recon Loss: 0.004063 | Commit Loss: 0.001725 | Perplexity: 1840.028873
2025-09-28 02:34:31,436 Stage: Train 0.5 | Epoch: 204 | Iter: 310800 | Total Loss: 0.004894 | Recon Loss: 0.004038 | Commit Loss: 0.001713 | Perplexity: 1837.929202
2025-09-28 02:34:58,074 Stage: Train 0.5 | Epoch: 204 | Iter: 311000 | Total Loss: 0.004958 | Recon Loss: 0.004100 | Commit Loss: 0.001716 | Perplexity: 1837.434573
2025-09-28 02:35:24,719 Stage: Train 0.5 | Epoch: 204 | Iter: 311200 | Total Loss: 0.004961 | Recon Loss: 0.004106 | Commit Loss: 0.001710 | Perplexity: 1840.208578
Trainning Epoch:  62%|██████▏   | 205/330 [19:47:59<7:02:53, 202.99s/it]2025-09-28 02:35:51,708 Stage: Train 0.5 | Epoch: 205 | Iter: 311400 | Total Loss: 0.004954 | Recon Loss: 0.004093 | Commit Loss: 0.001721 | Perplexity: 1836.124802
2025-09-28 02:36:18,396 Stage: Train 0.5 | Epoch: 205 | Iter: 311600 | Total Loss: 0.004910 | Recon Loss: 0.004057 | Commit Loss: 0.001707 | Perplexity: 1839.199514
2025-09-28 02:36:45,024 Stage: Train 0.5 | Epoch: 205 | Iter: 311800 | Total Loss: 0.004877 | Recon Loss: 0.004021 | Commit Loss: 0.001711 | Perplexity: 1839.908360
2025-09-28 02:37:11,656 Stage: Train 0.5 | Epoch: 205 | Iter: 312000 | Total Loss: 0.004943 | Recon Loss: 0.004083 | Commit Loss: 0.001719 | Perplexity: 1840.058242
2025-09-28 02:37:38,292 Stage: Train 0.5 | Epoch: 205 | Iter: 312200 | Total Loss: 0.004957 | Recon Loss: 0.004098 | Commit Loss: 0.001718 | Perplexity: 1839.219750
2025-09-28 02:38:04,871 Stage: Train 0.5 | Epoch: 205 | Iter: 312400 | Total Loss: 0.004956 | Recon Loss: 0.004096 | Commit Loss: 0.001721 | Perplexity: 1840.114308
2025-09-28 02:38:31,554 Stage: Train 0.5 | Epoch: 205 | Iter: 312600 | Total Loss: 0.004872 | Recon Loss: 0.004015 | Commit Loss: 0.001714 | Perplexity: 1838.798625
2025-09-28 02:38:58,210 Stage: Train 0.5 | Epoch: 205 | Iter: 312800 | Total Loss: 0.004934 | Recon Loss: 0.004076 | Commit Loss: 0.001716 | Perplexity: 1839.542423
Trainning Epoch:  62%|██████▏   | 206/330 [19:51:22<6:59:19, 202.90s/it]2025-09-28 02:39:25,174 Stage: Train 0.5 | Epoch: 206 | Iter: 313000 | Total Loss: 0.004903 | Recon Loss: 0.004046 | Commit Loss: 0.001715 | Perplexity: 1836.328177
2025-09-28 02:39:51,852 Stage: Train 0.5 | Epoch: 206 | Iter: 313200 | Total Loss: 0.004884 | Recon Loss: 0.004030 | Commit Loss: 0.001709 | Perplexity: 1835.918204
2025-09-28 02:40:18,516 Stage: Train 0.5 | Epoch: 206 | Iter: 313400 | Total Loss: 0.004960 | Recon Loss: 0.004100 | Commit Loss: 0.001721 | Perplexity: 1842.704417
2025-09-28 02:40:45,130 Stage: Train 0.5 | Epoch: 206 | Iter: 313600 | Total Loss: 0.004890 | Recon Loss: 0.004036 | Commit Loss: 0.001709 | Perplexity: 1839.098179
2025-09-28 02:41:11,861 Stage: Train 0.5 | Epoch: 206 | Iter: 313800 | Total Loss: 0.004939 | Recon Loss: 0.004079 | Commit Loss: 0.001722 | Perplexity: 1843.734536
2025-09-28 02:41:38,582 Stage: Train 0.5 | Epoch: 206 | Iter: 314000 | Total Loss: 0.004918 | Recon Loss: 0.004059 | Commit Loss: 0.001717 | Perplexity: 1838.115638
2025-09-28 02:42:05,311 Stage: Train 0.5 | Epoch: 206 | Iter: 314200 | Total Loss: 0.004913 | Recon Loss: 0.004058 | Commit Loss: 0.001711 | Perplexity: 1837.168590
2025-09-28 02:42:31,800 Stage: Train 0.5 | Epoch: 206 | Iter: 314400 | Total Loss: 0.004923 | Recon Loss: 0.004066 | Commit Loss: 0.001713 | Perplexity: 1837.718565
Trainning Epoch:  63%|██████▎   | 207/330 [19:54:44<6:55:50, 202.85s/it]2025-09-28 02:42:58,815 Stage: Train 0.5 | Epoch: 207 | Iter: 314600 | Total Loss: 0.004939 | Recon Loss: 0.004081 | Commit Loss: 0.001716 | Perplexity: 1840.793061
2025-09-28 02:43:25,380 Stage: Train 0.5 | Epoch: 207 | Iter: 314800 | Total Loss: 0.004957 | Recon Loss: 0.004097 | Commit Loss: 0.001720 | Perplexity: 1841.762544
2025-09-28 02:43:51,957 Stage: Train 0.5 | Epoch: 207 | Iter: 315000 | Total Loss: 0.004880 | Recon Loss: 0.004024 | Commit Loss: 0.001711 | Perplexity: 1838.197175
2025-09-28 02:44:18,629 Stage: Train 0.5 | Epoch: 207 | Iter: 315200 | Total Loss: 0.004979 | Recon Loss: 0.004123 | Commit Loss: 0.001711 | Perplexity: 1838.326810
2025-09-28 02:44:45,171 Stage: Train 0.5 | Epoch: 207 | Iter: 315400 | Total Loss: 0.004911 | Recon Loss: 0.004049 | Commit Loss: 0.001725 | Perplexity: 1840.313624
2025-09-28 02:45:11,773 Stage: Train 0.5 | Epoch: 207 | Iter: 315600 | Total Loss: 0.004915 | Recon Loss: 0.004053 | Commit Loss: 0.001722 | Perplexity: 1839.113305
2025-09-28 02:45:38,462 Stage: Train 0.5 | Epoch: 207 | Iter: 315800 | Total Loss: 0.004990 | Recon Loss: 0.004136 | Commit Loss: 0.001708 | Perplexity: 1836.121599
Trainning Epoch:  63%|██████▎   | 208/330 [19:58:07<6:52:17, 202.77s/it]2025-09-28 02:46:05,428 Stage: Train 0.5 | Epoch: 208 | Iter: 316000 | Total Loss: 0.004901 | Recon Loss: 0.004046 | Commit Loss: 0.001710 | Perplexity: 1837.480895
2025-09-28 02:46:32,039 Stage: Train 0.5 | Epoch: 208 | Iter: 316200 | Total Loss: 0.004907 | Recon Loss: 0.004054 | Commit Loss: 0.001707 | Perplexity: 1838.545002
2025-09-28 02:46:58,718 Stage: Train 0.5 | Epoch: 208 | Iter: 316400 | Total Loss: 0.004928 | Recon Loss: 0.004074 | Commit Loss: 0.001706 | Perplexity: 1837.960955
2025-09-28 02:47:25,404 Stage: Train 0.5 | Epoch: 208 | Iter: 316600 | Total Loss: 0.004901 | Recon Loss: 0.004043 | Commit Loss: 0.001715 | Perplexity: 1836.220274
2025-09-28 02:47:51,972 Stage: Train 0.5 | Epoch: 208 | Iter: 316800 | Total Loss: 0.004953 | Recon Loss: 0.004098 | Commit Loss: 0.001710 | Perplexity: 1833.610511
2025-09-28 02:48:18,595 Stage: Train 0.5 | Epoch: 208 | Iter: 317000 | Total Loss: 0.004898 | Recon Loss: 0.004043 | Commit Loss: 0.001710 | Perplexity: 1841.371550
2025-09-28 02:48:45,256 Stage: Train 0.5 | Epoch: 208 | Iter: 317200 | Total Loss: 0.004987 | Recon Loss: 0.004125 | Commit Loss: 0.001725 | Perplexity: 1841.000940
2025-09-28 02:49:11,968 Stage: Train 0.5 | Epoch: 208 | Iter: 317400 | Total Loss: 0.004873 | Recon Loss: 0.004016 | Commit Loss: 0.001714 | Perplexity: 1840.709658
Trainning Epoch:  63%|██████▎   | 209/330 [20:01:30<6:48:52, 202.75s/it]2025-09-28 02:49:38,960 Stage: Train 0.5 | Epoch: 209 | Iter: 317600 | Total Loss: 0.004933 | Recon Loss: 0.004080 | Commit Loss: 0.001706 | Perplexity: 1836.052565
2025-09-28 02:50:05,590 Stage: Train 0.5 | Epoch: 209 | Iter: 317800 | Total Loss: 0.004835 | Recon Loss: 0.003988 | Commit Loss: 0.001694 | Perplexity: 1831.561476
2025-09-28 02:50:32,128 Stage: Train 0.5 | Epoch: 209 | Iter: 318000 | Total Loss: 0.004907 | Recon Loss: 0.004047 | Commit Loss: 0.001720 | Perplexity: 1842.373381
2025-09-28 02:50:58,715 Stage: Train 0.5 | Epoch: 209 | Iter: 318200 | Total Loss: 0.004891 | Recon Loss: 0.004033 | Commit Loss: 0.001714 | Perplexity: 1845.668848
2025-09-28 02:51:25,239 Stage: Train 0.5 | Epoch: 209 | Iter: 318400 | Total Loss: 0.004911 | Recon Loss: 0.004054 | Commit Loss: 0.001714 | Perplexity: 1838.810463
2025-09-28 02:51:51,999 Stage: Train 0.5 | Epoch: 209 | Iter: 318600 | Total Loss: 0.004924 | Recon Loss: 0.004063 | Commit Loss: 0.001721 | Perplexity: 1838.597592
2025-09-28 02:52:18,612 Stage: Train 0.5 | Epoch: 209 | Iter: 318800 | Total Loss: 0.004951 | Recon Loss: 0.004087 | Commit Loss: 0.001727 | Perplexity: 1841.782089
Trainning Epoch:  64%|██████▎   | 210/330 [20:04:52<6:45:26, 202.72s/it]2025-09-28 02:52:45,754 Stage: Train 0.5 | Epoch: 210 | Iter: 319000 | Total Loss: 0.004923 | Recon Loss: 0.004063 | Commit Loss: 0.001720 | Perplexity: 1841.633533
2025-09-28 02:53:12,394 Stage: Train 0.5 | Epoch: 210 | Iter: 319200 | Total Loss: 0.004852 | Recon Loss: 0.003997 | Commit Loss: 0.001711 | Perplexity: 1839.371984
2025-09-28 02:53:39,114 Stage: Train 0.5 | Epoch: 210 | Iter: 319400 | Total Loss: 0.004869 | Recon Loss: 0.004014 | Commit Loss: 0.001711 | Perplexity: 1839.817272
2025-09-28 02:54:05,746 Stage: Train 0.5 | Epoch: 210 | Iter: 319600 | Total Loss: 0.004913 | Recon Loss: 0.004061 | Commit Loss: 0.001704 | Perplexity: 1838.249261
2025-09-28 02:54:32,398 Stage: Train 0.5 | Epoch: 210 | Iter: 319800 | Total Loss: 0.004927 | Recon Loss: 0.004071 | Commit Loss: 0.001712 | Perplexity: 1838.756135
2025-09-28 02:54:59,058 Stage: Train 0.5 | Epoch: 210 | Iter: 320000 | Total Loss: 0.004927 | Recon Loss: 0.004067 | Commit Loss: 0.001720 | Perplexity: 1837.588240
2025-09-28 02:54:59,058 Saving model at iteration 320000
2025-09-28 02:54:59,280 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000
2025-09-28 02:54:59,822 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/model.safetensors
2025-09-28 02:55:00,358 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/optimizer.bin
2025-09-28 02:55:00,358 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/scheduler.bin
2025-09-28 02:55:00,358 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/sampler.bin
2025-09-28 02:55:00,359 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_211_step_320000/random_states_0.pkl
2025-09-28 02:55:27,397 Stage: Train 0.5 | Epoch: 210 | Iter: 320200 | Total Loss: 0.004910 | Recon Loss: 0.004054 | Commit Loss: 0.001713 | Perplexity: 1841.340446
2025-09-28 02:55:54,086 Stage: Train 0.5 | Epoch: 210 | Iter: 320400 | Total Loss: 0.004912 | Recon Loss: 0.004055 | Commit Loss: 0.001714 | Perplexity: 1841.149490
Trainning Epoch:  64%|██████▍   | 211/330 [20:08:17<6:43:10, 203.28s/it]2025-09-28 02:56:21,042 Stage: Train 0.5 | Epoch: 211 | Iter: 320600 | Total Loss: 0.004942 | Recon Loss: 0.004086 | Commit Loss: 0.001712 | Perplexity: 1833.782103
2025-09-28 02:56:47,572 Stage: Train 0.5 | Epoch: 211 | Iter: 320800 | Total Loss: 0.004852 | Recon Loss: 0.003996 | Commit Loss: 0.001712 | Perplexity: 1839.595869
2025-09-28 02:57:14,338 Stage: Train 0.5 | Epoch: 211 | Iter: 321000 | Total Loss: 0.004945 | Recon Loss: 0.004092 | Commit Loss: 0.001706 | Perplexity: 1839.298958
2025-09-28 02:57:41,168 Stage: Train 0.5 | Epoch: 211 | Iter: 321200 | Total Loss: 0.004915 | Recon Loss: 0.004056 | Commit Loss: 0.001718 | Perplexity: 1836.957493
2025-09-28 02:58:07,864 Stage: Train 0.5 | Epoch: 211 | Iter: 321400 | Total Loss: 0.004904 | Recon Loss: 0.004048 | Commit Loss: 0.001711 | Perplexity: 1839.372842
2025-09-28 02:58:34,561 Stage: Train 0.5 | Epoch: 211 | Iter: 321600 | Total Loss: 0.004884 | Recon Loss: 0.004027 | Commit Loss: 0.001715 | Perplexity: 1840.447432
2025-09-28 02:59:01,168 Stage: Train 0.5 | Epoch: 211 | Iter: 321800 | Total Loss: 0.004953 | Recon Loss: 0.004096 | Commit Loss: 0.001713 | Perplexity: 1838.210187
2025-09-28 02:59:27,971 Stage: Train 0.5 | Epoch: 211 | Iter: 322000 | Total Loss: 0.004871 | Recon Loss: 0.004012 | Commit Loss: 0.001718 | Perplexity: 1839.699391
Trainning Epoch:  64%|██████▍   | 212/330 [20:11:40<6:39:38, 203.21s/it]2025-09-28 02:59:54,984 Stage: Train 0.5 | Epoch: 212 | Iter: 322200 | Total Loss: 0.004898 | Recon Loss: 0.004040 | Commit Loss: 0.001717 | Perplexity: 1839.219474
2025-09-28 03:00:21,545 Stage: Train 0.5 | Epoch: 212 | Iter: 322400 | Total Loss: 0.004964 | Recon Loss: 0.004112 | Commit Loss: 0.001704 | Perplexity: 1836.969684
2025-09-28 03:00:48,021 Stage: Train 0.5 | Epoch: 212 | Iter: 322600 | Total Loss: 0.004812 | Recon Loss: 0.003955 | Commit Loss: 0.001714 | Perplexity: 1841.188874
2025-09-28 03:01:14,742 Stage: Train 0.5 | Epoch: 212 | Iter: 322800 | Total Loss: 0.004927 | Recon Loss: 0.004068 | Commit Loss: 0.001718 | Perplexity: 1836.029833
2025-09-28 03:01:41,328 Stage: Train 0.5 | Epoch: 212 | Iter: 323000 | Total Loss: 0.004882 | Recon Loss: 0.004027 | Commit Loss: 0.001712 | Perplexity: 1837.539962
2025-09-28 03:02:07,923 Stage: Train 0.5 | Epoch: 212 | Iter: 323200 | Total Loss: 0.004904 | Recon Loss: 0.004046 | Commit Loss: 0.001717 | Perplexity: 1838.332224
2025-09-28 03:02:34,594 Stage: Train 0.5 | Epoch: 212 | Iter: 323400 | Total Loss: 0.004922 | Recon Loss: 0.004067 | Commit Loss: 0.001710 | Perplexity: 1840.765398
Trainning Epoch:  65%|██████▍   | 213/330 [20:15:02<6:35:51, 203.00s/it]2025-09-28 03:03:01,604 Stage: Train 0.5 | Epoch: 213 | Iter: 323600 | Total Loss: 0.004869 | Recon Loss: 0.004015 | Commit Loss: 0.001709 | Perplexity: 1839.096522
2025-09-28 03:03:28,285 Stage: Train 0.5 | Epoch: 213 | Iter: 323800 | Total Loss: 0.004880 | Recon Loss: 0.004024 | Commit Loss: 0.001712 | Perplexity: 1835.657040
2025-09-28 03:03:54,935 Stage: Train 0.5 | Epoch: 213 | Iter: 324000 | Total Loss: 0.004914 | Recon Loss: 0.004057 | Commit Loss: 0.001714 | Perplexity: 1840.214011
2025-09-28 03:04:21,616 Stage: Train 0.5 | Epoch: 213 | Iter: 324200 | Total Loss: 0.004901 | Recon Loss: 0.004044 | Commit Loss: 0.001714 | Perplexity: 1840.603633
2025-09-28 03:04:48,249 Stage: Train 0.5 | Epoch: 213 | Iter: 324400 | Total Loss: 0.004990 | Recon Loss: 0.004135 | Commit Loss: 0.001711 | Perplexity: 1838.949484
2025-09-28 03:05:14,975 Stage: Train 0.5 | Epoch: 213 | Iter: 324600 | Total Loss: 0.004940 | Recon Loss: 0.004083 | Commit Loss: 0.001714 | Perplexity: 1844.626483
2025-09-28 03:05:41,719 Stage: Train 0.5 | Epoch: 213 | Iter: 324800 | Total Loss: 0.004877 | Recon Loss: 0.004022 | Commit Loss: 0.001710 | Perplexity: 1837.887745
2025-09-28 03:06:08,397 Stage: Train 0.5 | Epoch: 213 | Iter: 325000 | Total Loss: 0.004840 | Recon Loss: 0.003983 | Commit Loss: 0.001715 | Perplexity: 1837.004357
Trainning Epoch:  65%|██████▍   | 214/330 [20:18:25<6:32:24, 202.97s/it]2025-09-28 03:06:35,394 Stage: Train 0.5 | Epoch: 214 | Iter: 325200 | Total Loss: 0.004948 | Recon Loss: 0.004091 | Commit Loss: 0.001715 | Perplexity: 1841.285121
2025-09-28 03:07:02,088 Stage: Train 0.5 | Epoch: 214 | Iter: 325400 | Total Loss: 0.004951 | Recon Loss: 0.004091 | Commit Loss: 0.001719 | Perplexity: 1842.985143
2025-09-28 03:07:28,815 Stage: Train 0.5 | Epoch: 214 | Iter: 325600 | Total Loss: 0.004889 | Recon Loss: 0.004032 | Commit Loss: 0.001713 | Perplexity: 1837.606177
2025-09-28 03:07:55,537 Stage: Train 0.5 | Epoch: 214 | Iter: 325800 | Total Loss: 0.004856 | Recon Loss: 0.003996 | Commit Loss: 0.001720 | Perplexity: 1844.013574
2025-09-28 03:08:22,239 Stage: Train 0.5 | Epoch: 214 | Iter: 326000 | Total Loss: 0.004856 | Recon Loss: 0.003999 | Commit Loss: 0.001714 | Perplexity: 1841.171154
2025-09-28 03:08:48,931 Stage: Train 0.5 | Epoch: 214 | Iter: 326200 | Total Loss: 0.004880 | Recon Loss: 0.004023 | Commit Loss: 0.001712 | Perplexity: 1832.967097
2025-09-28 03:09:15,627 Stage: Train 0.5 | Epoch: 214 | Iter: 326400 | Total Loss: 0.004903 | Recon Loss: 0.004045 | Commit Loss: 0.001716 | Perplexity: 1844.427877
Trainning Epoch:  65%|██████▌   | 215/330 [20:21:49<6:29:07, 203.02s/it]2025-09-28 03:09:42,576 Stage: Train 0.5 | Epoch: 215 | Iter: 326600 | Total Loss: 0.004828 | Recon Loss: 0.003970 | Commit Loss: 0.001715 | Perplexity: 1838.723054
2025-09-28 03:10:09,195 Stage: Train 0.5 | Epoch: 215 | Iter: 326800 | Total Loss: 0.004879 | Recon Loss: 0.004017 | Commit Loss: 0.001722 | Perplexity: 1838.279780
2025-09-28 03:10:35,883 Stage: Train 0.5 | Epoch: 215 | Iter: 327000 | Total Loss: 0.004868 | Recon Loss: 0.004010 | Commit Loss: 0.001715 | Perplexity: 1841.160259
2025-09-28 03:11:02,662 Stage: Train 0.5 | Epoch: 215 | Iter: 327200 | Total Loss: 0.004881 | Recon Loss: 0.004025 | Commit Loss: 0.001712 | Perplexity: 1837.408585
2025-09-28 03:11:29,370 Stage: Train 0.5 | Epoch: 215 | Iter: 327400 | Total Loss: 0.004856 | Recon Loss: 0.004000 | Commit Loss: 0.001713 | Perplexity: 1840.813160
2025-09-28 03:11:55,956 Stage: Train 0.5 | Epoch: 215 | Iter: 327600 | Total Loss: 0.004862 | Recon Loss: 0.004008 | Commit Loss: 0.001709 | Perplexity: 1836.078167
2025-09-28 03:12:22,719 Stage: Train 0.5 | Epoch: 215 | Iter: 327800 | Total Loss: 0.004876 | Recon Loss: 0.004018 | Commit Loss: 0.001716 | Perplexity: 1844.827252
2025-09-28 03:12:49,293 Stage: Train 0.5 | Epoch: 215 | Iter: 328000 | Total Loss: 0.004891 | Recon Loss: 0.004030 | Commit Loss: 0.001721 | Perplexity: 1842.695737
Trainning Epoch:  65%|██████▌   | 216/330 [20:25:11<6:25:38, 202.97s/it]2025-09-28 03:13:16,260 Stage: Train 0.5 | Epoch: 216 | Iter: 328200 | Total Loss: 0.004926 | Recon Loss: 0.004070 | Commit Loss: 0.001713 | Perplexity: 1842.375822
2025-09-28 03:13:42,929 Stage: Train 0.5 | Epoch: 216 | Iter: 328400 | Total Loss: 0.004955 | Recon Loss: 0.004095 | Commit Loss: 0.001719 | Perplexity: 1845.152965
2025-09-28 03:14:09,575 Stage: Train 0.5 | Epoch: 216 | Iter: 328600 | Total Loss: 0.004831 | Recon Loss: 0.003976 | Commit Loss: 0.001710 | Perplexity: 1841.378294
2025-09-28 03:14:36,278 Stage: Train 0.5 | Epoch: 216 | Iter: 328800 | Total Loss: 0.004878 | Recon Loss: 0.004023 | Commit Loss: 0.001711 | Perplexity: 1841.020757
2025-09-28 03:15:02,881 Stage: Train 0.5 | Epoch: 216 | Iter: 329000 | Total Loss: 0.004932 | Recon Loss: 0.004070 | Commit Loss: 0.001723 | Perplexity: 1844.031893
2025-09-28 03:15:29,424 Stage: Train 0.5 | Epoch: 216 | Iter: 329200 | Total Loss: 0.004866 | Recon Loss: 0.004008 | Commit Loss: 0.001716 | Perplexity: 1838.311088
2025-09-28 03:15:56,236 Stage: Train 0.5 | Epoch: 216 | Iter: 329400 | Total Loss: 0.004859 | Recon Loss: 0.004003 | Commit Loss: 0.001712 | Perplexity: 1840.557883
2025-09-28 03:16:22,953 Stage: Train 0.5 | Epoch: 216 | Iter: 329600 | Total Loss: 0.004838 | Recon Loss: 0.003982 | Commit Loss: 0.001712 | Perplexity: 1837.562736
Trainning Epoch:  66%|██████▌   | 217/330 [20:28:34<6:22:12, 202.94s/it]2025-09-28 03:16:49,916 Stage: Train 0.5 | Epoch: 217 | Iter: 329800 | Total Loss: 0.004899 | Recon Loss: 0.004038 | Commit Loss: 0.001722 | Perplexity: 1842.337873
2025-09-28 03:17:16,403 Stage: Train 0.5 | Epoch: 217 | Iter: 330000 | Total Loss: 0.004882 | Recon Loss: 0.004025 | Commit Loss: 0.001714 | Perplexity: 1838.115120
2025-09-28 03:17:43,093 Stage: Train 0.5 | Epoch: 217 | Iter: 330200 | Total Loss: 0.004866 | Recon Loss: 0.004012 | Commit Loss: 0.001708 | Perplexity: 1840.243938
2025-09-28 03:18:09,761 Stage: Train 0.5 | Epoch: 217 | Iter: 330400 | Total Loss: 0.004850 | Recon Loss: 0.003993 | Commit Loss: 0.001714 | Perplexity: 1844.119381
2025-09-28 03:18:36,417 Stage: Train 0.5 | Epoch: 217 | Iter: 330600 | Total Loss: 0.004883 | Recon Loss: 0.004023 | Commit Loss: 0.001720 | Perplexity: 1843.810543
2025-09-28 03:19:03,033 Stage: Train 0.5 | Epoch: 217 | Iter: 330800 | Total Loss: 0.004892 | Recon Loss: 0.004031 | Commit Loss: 0.001722 | Perplexity: 1845.414586
2025-09-28 03:19:29,699 Stage: Train 0.5 | Epoch: 217 | Iter: 331000 | Total Loss: 0.004872 | Recon Loss: 0.004018 | Commit Loss: 0.001707 | Perplexity: 1835.224005
Trainning Epoch:  66%|██████▌   | 218/330 [20:31:57<6:18:36, 202.83s/it]2025-09-28 03:19:56,590 Stage: Train 0.5 | Epoch: 218 | Iter: 331200 | Total Loss: 0.004846 | Recon Loss: 0.003989 | Commit Loss: 0.001715 | Perplexity: 1841.471573
2025-09-28 03:20:23,389 Stage: Train 0.5 | Epoch: 218 | Iter: 331400 | Total Loss: 0.004886 | Recon Loss: 0.004032 | Commit Loss: 0.001708 | Perplexity: 1842.456838
2025-09-28 03:20:50,050 Stage: Train 0.5 | Epoch: 218 | Iter: 331600 | Total Loss: 0.004911 | Recon Loss: 0.004058 | Commit Loss: 0.001707 | Perplexity: 1840.372054
2025-09-28 03:21:16,699 Stage: Train 0.5 | Epoch: 218 | Iter: 331800 | Total Loss: 0.004832 | Recon Loss: 0.003980 | Commit Loss: 0.001705 | Perplexity: 1838.962089
2025-09-28 03:21:43,171 Stage: Train 0.5 | Epoch: 218 | Iter: 332000 | Total Loss: 0.004923 | Recon Loss: 0.004067 | Commit Loss: 0.001713 | Perplexity: 1843.282668
2025-09-28 03:22:09,774 Stage: Train 0.5 | Epoch: 218 | Iter: 332200 | Total Loss: 0.004854 | Recon Loss: 0.003998 | Commit Loss: 0.001712 | Perplexity: 1840.246823
2025-09-28 03:22:36,438 Stage: Train 0.5 | Epoch: 218 | Iter: 332400 | Total Loss: 0.004876 | Recon Loss: 0.004016 | Commit Loss: 0.001719 | Perplexity: 1843.507745
2025-09-28 03:23:03,046 Stage: Train 0.5 | Epoch: 218 | Iter: 332600 | Total Loss: 0.004916 | Recon Loss: 0.004062 | Commit Loss: 0.001709 | Perplexity: 1839.181433
Trainning Epoch:  66%|██████▋   | 219/330 [20:35:19<6:15:03, 202.74s/it]2025-09-28 03:23:29,952 Stage: Train 0.5 | Epoch: 219 | Iter: 332800 | Total Loss: 0.004871 | Recon Loss: 0.004012 | Commit Loss: 0.001717 | Perplexity: 1842.222175
2025-09-28 03:23:56,640 Stage: Train 0.5 | Epoch: 219 | Iter: 333000 | Total Loss: 0.004837 | Recon Loss: 0.003983 | Commit Loss: 0.001708 | Perplexity: 1839.463341
2025-09-28 03:24:23,312 Stage: Train 0.5 | Epoch: 219 | Iter: 333200 | Total Loss: 0.004862 | Recon Loss: 0.004006 | Commit Loss: 0.001711 | Perplexity: 1842.939211
2025-09-28 03:24:50,001 Stage: Train 0.5 | Epoch: 219 | Iter: 333400 | Total Loss: 0.004871 | Recon Loss: 0.004012 | Commit Loss: 0.001718 | Perplexity: 1841.286721
2025-09-28 03:25:16,695 Stage: Train 0.5 | Epoch: 219 | Iter: 333600 | Total Loss: 0.004869 | Recon Loss: 0.004014 | Commit Loss: 0.001710 | Perplexity: 1841.097661
2025-09-28 03:25:43,358 Stage: Train 0.5 | Epoch: 219 | Iter: 333800 | Total Loss: 0.004892 | Recon Loss: 0.004036 | Commit Loss: 0.001712 | Perplexity: 1841.860266
2025-09-28 03:26:10,058 Stage: Train 0.5 | Epoch: 219 | Iter: 334000 | Total Loss: 0.004894 | Recon Loss: 0.004035 | Commit Loss: 0.001717 | Perplexity: 1844.734453
Trainning Epoch:  67%|██████▋   | 220/330 [20:38:42<6:11:48, 202.80s/it]2025-09-28 03:26:36,999 Stage: Train 0.5 | Epoch: 220 | Iter: 334200 | Total Loss: 0.004908 | Recon Loss: 0.004053 | Commit Loss: 0.001709 | Perplexity: 1840.775477
2025-09-28 03:27:03,617 Stage: Train 0.5 | Epoch: 220 | Iter: 334400 | Total Loss: 0.004825 | Recon Loss: 0.003974 | Commit Loss: 0.001704 | Perplexity: 1839.202524
2025-09-28 03:27:30,333 Stage: Train 0.5 | Epoch: 220 | Iter: 334600 | Total Loss: 0.004875 | Recon Loss: 0.004019 | Commit Loss: 0.001713 | Perplexity: 1842.765654
2025-09-28 03:27:57,027 Stage: Train 0.5 | Epoch: 220 | Iter: 334800 | Total Loss: 0.004878 | Recon Loss: 0.004022 | Commit Loss: 0.001712 | Perplexity: 1842.403895
2025-09-28 03:28:23,636 Stage: Train 0.5 | Epoch: 220 | Iter: 335000 | Total Loss: 0.004856 | Recon Loss: 0.004002 | Commit Loss: 0.001707 | Perplexity: 1840.697233
2025-09-28 03:28:50,388 Stage: Train 0.5 | Epoch: 220 | Iter: 335200 | Total Loss: 0.004900 | Recon Loss: 0.004044 | Commit Loss: 0.001711 | Perplexity: 1840.329011
2025-09-28 03:29:17,094 Stage: Train 0.5 | Epoch: 220 | Iter: 335400 | Total Loss: 0.004864 | Recon Loss: 0.004002 | Commit Loss: 0.001723 | Perplexity: 1842.941620
2025-09-28 03:29:43,873 Stage: Train 0.5 | Epoch: 220 | Iter: 335600 | Total Loss: 0.004875 | Recon Loss: 0.004018 | Commit Loss: 0.001715 | Perplexity: 1842.238189
Trainning Epoch:  67%|██████▋   | 221/330 [20:42:05<6:08:33, 202.88s/it]2025-09-28 03:30:10,914 Stage: Train 0.5 | Epoch: 221 | Iter: 335800 | Total Loss: 0.004839 | Recon Loss: 0.003986 | Commit Loss: 0.001706 | Perplexity: 1839.491317
2025-09-28 03:30:37,638 Stage: Train 0.5 | Epoch: 221 | Iter: 336000 | Total Loss: 0.004894 | Recon Loss: 0.004038 | Commit Loss: 0.001712 | Perplexity: 1842.094146
2025-09-28 03:31:04,381 Stage: Train 0.5 | Epoch: 221 | Iter: 336200 | Total Loss: 0.004880 | Recon Loss: 0.004017 | Commit Loss: 0.001727 | Perplexity: 1849.647418
2025-09-28 03:31:31,254 Stage: Train 0.5 | Epoch: 221 | Iter: 336400 | Total Loss: 0.004864 | Recon Loss: 0.004008 | Commit Loss: 0.001711 | Perplexity: 1838.706383
2025-09-28 03:31:57,901 Stage: Train 0.5 | Epoch: 221 | Iter: 336600 | Total Loss: 0.004857 | Recon Loss: 0.004005 | Commit Loss: 0.001704 | Perplexity: 1838.750195
2025-09-28 03:32:24,325 Stage: Train 0.5 | Epoch: 221 | Iter: 336800 | Total Loss: 0.004871 | Recon Loss: 0.004015 | Commit Loss: 0.001712 | Perplexity: 1843.592222
2025-09-28 03:32:51,112 Stage: Train 0.5 | Epoch: 221 | Iter: 337000 | Total Loss: 0.004864 | Recon Loss: 0.004002 | Commit Loss: 0.001723 | Perplexity: 1841.827151
2025-09-28 03:33:17,876 Stage: Train 0.5 | Epoch: 221 | Iter: 337200 | Total Loss: 0.004875 | Recon Loss: 0.004020 | Commit Loss: 0.001709 | Perplexity: 1840.690690
Trainning Epoch:  67%|██████▋   | 222/330 [20:45:29<6:05:20, 202.97s/it]2025-09-28 03:33:44,866 Stage: Train 0.5 | Epoch: 222 | Iter: 337400 | Total Loss: 0.004855 | Recon Loss: 0.004000 | Commit Loss: 0.001709 | Perplexity: 1842.590179
2025-09-28 03:34:11,471 Stage: Train 0.5 | Epoch: 222 | Iter: 337600 | Total Loss: 0.004818 | Recon Loss: 0.003962 | Commit Loss: 0.001713 | Perplexity: 1844.566356
2025-09-28 03:34:38,153 Stage: Train 0.5 | Epoch: 222 | Iter: 337800 | Total Loss: 0.004813 | Recon Loss: 0.003959 | Commit Loss: 0.001708 | Perplexity: 1842.975961
2025-09-28 03:35:04,854 Stage: Train 0.5 | Epoch: 222 | Iter: 338000 | Total Loss: 0.004894 | Recon Loss: 0.004039 | Commit Loss: 0.001710 | Perplexity: 1839.096085
2025-09-28 03:35:31,634 Stage: Train 0.5 | Epoch: 222 | Iter: 338200 | Total Loss: 0.004856 | Recon Loss: 0.004000 | Commit Loss: 0.001711 | Perplexity: 1843.238563
2025-09-28 03:35:58,243 Stage: Train 0.5 | Epoch: 222 | Iter: 338400 | Total Loss: 0.004886 | Recon Loss: 0.004028 | Commit Loss: 0.001715 | Perplexity: 1841.453942
2025-09-28 03:36:24,885 Stage: Train 0.5 | Epoch: 222 | Iter: 338600 | Total Loss: 0.004840 | Recon Loss: 0.003985 | Commit Loss: 0.001711 | Perplexity: 1842.215634
Trainning Epoch:  68%|██████▊   | 223/330 [20:48:52<6:01:58, 202.98s/it]2025-09-28 03:36:52,034 Stage: Train 0.5 | Epoch: 223 | Iter: 338800 | Total Loss: 0.004802 | Recon Loss: 0.003948 | Commit Loss: 0.001707 | Perplexity: 1840.315919
2025-09-28 03:37:18,749 Stage: Train 0.5 | Epoch: 223 | Iter: 339000 | Total Loss: 0.004903 | Recon Loss: 0.004048 | Commit Loss: 0.001710 | Perplexity: 1841.331015
2025-09-28 03:37:45,422 Stage: Train 0.5 | Epoch: 223 | Iter: 339200 | Total Loss: 0.004805 | Recon Loss: 0.003950 | Commit Loss: 0.001711 | Perplexity: 1844.341884
2025-09-28 03:38:12,085 Stage: Train 0.5 | Epoch: 223 | Iter: 339400 | Total Loss: 0.004853 | Recon Loss: 0.004000 | Commit Loss: 0.001707 | Perplexity: 1840.502755
2025-09-28 03:38:38,688 Stage: Train 0.5 | Epoch: 223 | Iter: 339600 | Total Loss: 0.004858 | Recon Loss: 0.004004 | Commit Loss: 0.001708 | Perplexity: 1841.973876
2025-09-28 03:39:05,462 Stage: Train 0.5 | Epoch: 223 | Iter: 339800 | Total Loss: 0.004801 | Recon Loss: 0.003945 | Commit Loss: 0.001713 | Perplexity: 1843.976931
2025-09-28 03:39:32,043 Stage: Train 0.5 | Epoch: 223 | Iter: 340000 | Total Loss: 0.004843 | Recon Loss: 0.003984 | Commit Loss: 0.001718 | Perplexity: 1841.192350
2025-09-28 03:39:32,043 Saving model at iteration 340000
2025-09-28 03:39:32,704 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000
2025-09-28 03:39:33,228 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/model.safetensors
2025-09-28 03:39:33,785 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/optimizer.bin
2025-09-28 03:39:33,786 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/scheduler.bin
2025-09-28 03:39:33,786 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/sampler.bin
2025-09-28 03:39:33,787 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_224_step_340000/random_states_0.pkl
2025-09-28 03:40:00,559 Stage: Train 0.5 | Epoch: 223 | Iter: 340200 | Total Loss: 0.004841 | Recon Loss: 0.003981 | Commit Loss: 0.001719 | Perplexity: 1845.138582
Trainning Epoch:  68%|██████▊   | 224/330 [20:52:16<5:59:32, 203.51s/it]2025-09-28 03:40:27,621 Stage: Train 0.5 | Epoch: 224 | Iter: 340400 | Total Loss: 0.004846 | Recon Loss: 0.003987 | Commit Loss: 0.001718 | Perplexity: 1842.923652
2025-09-28 03:40:54,228 Stage: Train 0.5 | Epoch: 224 | Iter: 340600 | Total Loss: 0.004792 | Recon Loss: 0.003936 | Commit Loss: 0.001712 | Perplexity: 1845.830689
2025-09-28 03:41:20,857 Stage: Train 0.5 | Epoch: 224 | Iter: 340800 | Total Loss: 0.004862 | Recon Loss: 0.004009 | Commit Loss: 0.001705 | Perplexity: 1843.364297
2025-09-28 03:41:47,512 Stage: Train 0.5 | Epoch: 224 | Iter: 341000 | Total Loss: 0.004889 | Recon Loss: 0.004032 | Commit Loss: 0.001715 | Perplexity: 1843.708936
2025-09-28 03:42:14,272 Stage: Train 0.5 | Epoch: 224 | Iter: 341200 | Total Loss: 0.004846 | Recon Loss: 0.003988 | Commit Loss: 0.001716 | Perplexity: 1844.543600
2025-09-28 03:42:40,904 Stage: Train 0.5 | Epoch: 224 | Iter: 341400 | Total Loss: 0.004834 | Recon Loss: 0.003976 | Commit Loss: 0.001716 | Perplexity: 1842.445837
2025-09-28 03:43:07,837 Stage: Train 0.5 | Epoch: 224 | Iter: 341600 | Total Loss: 0.004860 | Recon Loss: 0.004002 | Commit Loss: 0.001716 | Perplexity: 1839.493124
Trainning Epoch:  68%|██████▊   | 225/330 [20:55:39<5:55:55, 203.39s/it]2025-09-28 03:43:34,787 Stage: Train 0.5 | Epoch: 225 | Iter: 341800 | Total Loss: 0.004867 | Recon Loss: 0.004009 | Commit Loss: 0.001717 | Perplexity: 1843.196129
2025-09-28 03:44:01,501 Stage: Train 0.5 | Epoch: 225 | Iter: 342000 | Total Loss: 0.004839 | Recon Loss: 0.003984 | Commit Loss: 0.001708 | Perplexity: 1841.498078
2025-09-28 03:44:28,160 Stage: Train 0.5 | Epoch: 225 | Iter: 342200 | Total Loss: 0.004836 | Recon Loss: 0.003981 | Commit Loss: 0.001711 | Perplexity: 1844.781528
2025-09-28 03:44:54,871 Stage: Train 0.5 | Epoch: 225 | Iter: 342400 | Total Loss: 0.004832 | Recon Loss: 0.003978 | Commit Loss: 0.001708 | Perplexity: 1846.261365
2025-09-28 03:45:21,413 Stage: Train 0.5 | Epoch: 225 | Iter: 342600 | Total Loss: 0.004860 | Recon Loss: 0.004001 | Commit Loss: 0.001719 | Perplexity: 1842.662656
2025-09-28 03:45:48,158 Stage: Train 0.5 | Epoch: 225 | Iter: 342800 | Total Loss: 0.004857 | Recon Loss: 0.004001 | Commit Loss: 0.001711 | Perplexity: 1840.850994
2025-09-28 03:46:14,781 Stage: Train 0.5 | Epoch: 225 | Iter: 343000 | Total Loss: 0.004864 | Recon Loss: 0.004003 | Commit Loss: 0.001723 | Perplexity: 1852.012656
2025-09-28 03:46:41,445 Stage: Train 0.5 | Epoch: 225 | Iter: 343200 | Total Loss: 0.004856 | Recon Loss: 0.003996 | Commit Loss: 0.001719 | Perplexity: 1845.492811
Trainning Epoch:  68%|██████▊   | 226/330 [20:59:02<5:52:14, 203.21s/it]2025-09-28 03:47:08,398 Stage: Train 0.5 | Epoch: 226 | Iter: 343400 | Total Loss: 0.004858 | Recon Loss: 0.004001 | Commit Loss: 0.001714 | Perplexity: 1843.093406
2025-09-28 03:47:35,018 Stage: Train 0.5 | Epoch: 226 | Iter: 343600 | Total Loss: 0.004803 | Recon Loss: 0.003950 | Commit Loss: 0.001707 | Perplexity: 1843.361319
2025-09-28 03:48:01,915 Stage: Train 0.5 | Epoch: 226 | Iter: 343800 | Total Loss: 0.004861 | Recon Loss: 0.004009 | Commit Loss: 0.001704 | Perplexity: 1840.054564
2025-09-28 03:48:28,568 Stage: Train 0.5 | Epoch: 226 | Iter: 344000 | Total Loss: 0.004832 | Recon Loss: 0.003971 | Commit Loss: 0.001722 | Perplexity: 1844.447307
2025-09-28 03:48:55,211 Stage: Train 0.5 | Epoch: 226 | Iter: 344200 | Total Loss: 0.004826 | Recon Loss: 0.003968 | Commit Loss: 0.001716 | Perplexity: 1846.173770
2025-09-28 03:49:21,840 Stage: Train 0.5 | Epoch: 226 | Iter: 344400 | Total Loss: 0.004848 | Recon Loss: 0.003992 | Commit Loss: 0.001712 | Perplexity: 1844.566843
2025-09-28 03:49:48,439 Stage: Train 0.5 | Epoch: 226 | Iter: 344600 | Total Loss: 0.004863 | Recon Loss: 0.004006 | Commit Loss: 0.001714 | Perplexity: 1844.767306
2025-09-28 03:50:14,972 Stage: Train 0.5 | Epoch: 226 | Iter: 344800 | Total Loss: 0.004843 | Recon Loss: 0.003986 | Commit Loss: 0.001714 | Perplexity: 1842.831473
Trainning Epoch:  69%|██████▉   | 227/330 [21:02:25<5:48:36, 203.07s/it]2025-09-28 03:50:41,847 Stage: Train 0.5 | Epoch: 227 | Iter: 345000 | Total Loss: 0.004872 | Recon Loss: 0.004016 | Commit Loss: 0.001712 | Perplexity: 1844.041376
2025-09-28 03:51:08,556 Stage: Train 0.5 | Epoch: 227 | Iter: 345200 | Total Loss: 0.004889 | Recon Loss: 0.004033 | Commit Loss: 0.001712 | Perplexity: 1842.566943
2025-09-28 03:51:35,177 Stage: Train 0.5 | Epoch: 227 | Iter: 345400 | Total Loss: 0.004780 | Recon Loss: 0.003921 | Commit Loss: 0.001716 | Perplexity: 1842.937872
2025-09-28 03:52:01,799 Stage: Train 0.5 | Epoch: 227 | Iter: 345600 | Total Loss: 0.004859 | Recon Loss: 0.004007 | Commit Loss: 0.001704 | Perplexity: 1841.910427
2025-09-28 03:52:28,339 Stage: Train 0.5 | Epoch: 227 | Iter: 345800 | Total Loss: 0.004825 | Recon Loss: 0.003968 | Commit Loss: 0.001714 | Perplexity: 1844.561711
2025-09-28 03:52:54,874 Stage: Train 0.5 | Epoch: 227 | Iter: 346000 | Total Loss: 0.004797 | Recon Loss: 0.003940 | Commit Loss: 0.001713 | Perplexity: 1847.802267
2025-09-28 03:53:21,540 Stage: Train 0.5 | Epoch: 227 | Iter: 346200 | Total Loss: 0.004874 | Recon Loss: 0.004016 | Commit Loss: 0.001717 | Perplexity: 1841.927606
Trainning Epoch:  69%|██████▉   | 228/330 [21:05:47<5:44:51, 202.85s/it]2025-09-28 03:53:48,378 Stage: Train 0.5 | Epoch: 228 | Iter: 346400 | Total Loss: 0.004865 | Recon Loss: 0.004008 | Commit Loss: 0.001714 | Perplexity: 1843.141845
2025-09-28 03:54:15,023 Stage: Train 0.5 | Epoch: 228 | Iter: 346600 | Total Loss: 0.004777 | Recon Loss: 0.003921 | Commit Loss: 0.001712 | Perplexity: 1846.742932
2025-09-28 03:54:41,661 Stage: Train 0.5 | Epoch: 228 | Iter: 346800 | Total Loss: 0.004861 | Recon Loss: 0.004005 | Commit Loss: 0.001712 | Perplexity: 1845.745747
2025-09-28 03:55:08,315 Stage: Train 0.5 | Epoch: 228 | Iter: 347000 | Total Loss: 0.004816 | Recon Loss: 0.003962 | Commit Loss: 0.001708 | Perplexity: 1843.997415
2025-09-28 03:55:35,000 Stage: Train 0.5 | Epoch: 228 | Iter: 347200 | Total Loss: 0.004771 | Recon Loss: 0.003919 | Commit Loss: 0.001704 | Perplexity: 1843.909282
2025-09-28 03:56:01,757 Stage: Train 0.5 | Epoch: 228 | Iter: 347400 | Total Loss: 0.004846 | Recon Loss: 0.003988 | Commit Loss: 0.001717 | Perplexity: 1842.349979
2025-09-28 03:56:28,350 Stage: Train 0.5 | Epoch: 228 | Iter: 347600 | Total Loss: 0.004864 | Recon Loss: 0.004003 | Commit Loss: 0.001723 | Perplexity: 1850.111351
2025-09-28 03:56:54,909 Stage: Train 0.5 | Epoch: 228 | Iter: 347800 | Total Loss: 0.004849 | Recon Loss: 0.003993 | Commit Loss: 0.001713 | Perplexity: 1842.247132
Trainning Epoch:  69%|██████▉   | 229/330 [21:09:10<5:41:20, 202.77s/it]2025-09-28 03:57:21,752 Stage: Train 0.5 | Epoch: 229 | Iter: 348000 | Total Loss: 0.004802 | Recon Loss: 0.003946 | Commit Loss: 0.001712 | Perplexity: 1842.483910
2025-09-28 03:57:48,381 Stage: Train 0.5 | Epoch: 229 | Iter: 348200 | Total Loss: 0.004805 | Recon Loss: 0.003952 | Commit Loss: 0.001707 | Perplexity: 1845.461971
2025-09-28 03:58:15,016 Stage: Train 0.5 | Epoch: 229 | Iter: 348400 | Total Loss: 0.004825 | Recon Loss: 0.003966 | Commit Loss: 0.001718 | Perplexity: 1843.314465
2025-09-28 03:58:41,512 Stage: Train 0.5 | Epoch: 229 | Iter: 348600 | Total Loss: 0.004874 | Recon Loss: 0.004015 | Commit Loss: 0.001718 | Perplexity: 1843.900306
2025-09-28 03:59:08,115 Stage: Train 0.5 | Epoch: 229 | Iter: 348800 | Total Loss: 0.004824 | Recon Loss: 0.003970 | Commit Loss: 0.001707 | Perplexity: 1842.579391
2025-09-28 03:59:34,583 Stage: Train 0.5 | Epoch: 229 | Iter: 349000 | Total Loss: 0.004855 | Recon Loss: 0.003998 | Commit Loss: 0.001715 | Perplexity: 1846.776549
2025-09-28 04:00:01,298 Stage: Train 0.5 | Epoch: 229 | Iter: 349200 | Total Loss: 0.004860 | Recon Loss: 0.004001 | Commit Loss: 0.001717 | Perplexity: 1847.770441
Trainning Epoch:  70%|██████▉   | 230/330 [21:12:32<5:37:47, 202.67s/it]2025-09-28 04:00:28,371 Stage: Train 0.5 | Epoch: 230 | Iter: 349400 | Total Loss: 0.004801 | Recon Loss: 0.003945 | Commit Loss: 0.001712 | Perplexity: 1843.745843
2025-09-28 04:00:55,073 Stage: Train 0.5 | Epoch: 230 | Iter: 349600 | Total Loss: 0.004858 | Recon Loss: 0.004003 | Commit Loss: 0.001710 | Perplexity: 1845.933124
2025-09-28 04:01:21,656 Stage: Train 0.5 | Epoch: 230 | Iter: 349800 | Total Loss: 0.004752 | Recon Loss: 0.003897 | Commit Loss: 0.001711 | Perplexity: 1843.765114
2025-09-28 04:01:48,316 Stage: Train 0.5 | Epoch: 230 | Iter: 350000 | Total Loss: 0.004824 | Recon Loss: 0.003969 | Commit Loss: 0.001710 | Perplexity: 1844.190284
2025-09-28 04:02:15,004 Stage: Train 0.5 | Epoch: 230 | Iter: 350200 | Total Loss: 0.004854 | Recon Loss: 0.003996 | Commit Loss: 0.001715 | Perplexity: 1846.894000
2025-09-28 04:02:41,732 Stage: Train 0.5 | Epoch: 230 | Iter: 350400 | Total Loss: 0.004887 | Recon Loss: 0.004024 | Commit Loss: 0.001726 | Perplexity: 1849.050813
2025-09-28 04:03:08,405 Stage: Train 0.5 | Epoch: 230 | Iter: 350600 | Total Loss: 0.004829 | Recon Loss: 0.003974 | Commit Loss: 0.001709 | Perplexity: 1844.145563
2025-09-28 04:03:35,032 Stage: Train 0.5 | Epoch: 230 | Iter: 350800 | Total Loss: 0.004815 | Recon Loss: 0.003957 | Commit Loss: 0.001716 | Perplexity: 1847.783197
Trainning Epoch:  70%|███████   | 231/330 [21:15:55<5:34:29, 202.72s/it]2025-09-28 04:04:02,042 Stage: Train 0.5 | Epoch: 231 | Iter: 351000 | Total Loss: 0.004829 | Recon Loss: 0.003972 | Commit Loss: 0.001713 | Perplexity: 1841.440934
2025-09-28 04:04:28,719 Stage: Train 0.5 | Epoch: 231 | Iter: 351200 | Total Loss: 0.004812 | Recon Loss: 0.003956 | Commit Loss: 0.001712 | Perplexity: 1845.973356
2025-09-28 04:04:55,500 Stage: Train 0.5 | Epoch: 231 | Iter: 351400 | Total Loss: 0.004794 | Recon Loss: 0.003934 | Commit Loss: 0.001720 | Perplexity: 1843.817123
2025-09-28 04:05:22,205 Stage: Train 0.5 | Epoch: 231 | Iter: 351600 | Total Loss: 0.004837 | Recon Loss: 0.003983 | Commit Loss: 0.001707 | Perplexity: 1844.342113
2025-09-28 04:05:48,925 Stage: Train 0.5 | Epoch: 231 | Iter: 351800 | Total Loss: 0.004874 | Recon Loss: 0.004016 | Commit Loss: 0.001716 | Perplexity: 1846.608667
2025-09-28 04:06:15,688 Stage: Train 0.5 | Epoch: 231 | Iter: 352000 | Total Loss: 0.004770 | Recon Loss: 0.003915 | Commit Loss: 0.001710 | Perplexity: 1843.971989
2025-09-28 04:06:42,292 Stage: Train 0.5 | Epoch: 231 | Iter: 352200 | Total Loss: 0.004878 | Recon Loss: 0.004018 | Commit Loss: 0.001718 | Perplexity: 1843.121465
2025-09-28 04:07:08,986 Stage: Train 0.5 | Epoch: 231 | Iter: 352400 | Total Loss: 0.004827 | Recon Loss: 0.003969 | Commit Loss: 0.001715 | Perplexity: 1847.232159
Trainning Epoch:  70%|███████   | 232/330 [21:19:18<5:31:24, 202.90s/it]2025-09-28 04:07:36,115 Stage: Train 0.5 | Epoch: 232 | Iter: 352600 | Total Loss: 0.004798 | Recon Loss: 0.003943 | Commit Loss: 0.001710 | Perplexity: 1847.259725
2025-09-28 04:08:02,767 Stage: Train 0.5 | Epoch: 232 | Iter: 352800 | Total Loss: 0.004873 | Recon Loss: 0.004022 | Commit Loss: 0.001702 | Perplexity: 1841.068102
2025-09-28 04:08:29,445 Stage: Train 0.5 | Epoch: 232 | Iter: 353000 | Total Loss: 0.004777 | Recon Loss: 0.003922 | Commit Loss: 0.001710 | Perplexity: 1845.483342
2025-09-28 04:08:56,024 Stage: Train 0.5 | Epoch: 232 | Iter: 353200 | Total Loss: 0.004806 | Recon Loss: 0.003952 | Commit Loss: 0.001709 | Perplexity: 1843.727053
2025-09-28 04:09:22,696 Stage: Train 0.5 | Epoch: 232 | Iter: 353400 | Total Loss: 0.004841 | Recon Loss: 0.003988 | Commit Loss: 0.001706 | Perplexity: 1843.543419
2025-09-28 04:09:49,499 Stage: Train 0.5 | Epoch: 232 | Iter: 353600 | Total Loss: 0.004833 | Recon Loss: 0.003973 | Commit Loss: 0.001719 | Perplexity: 1849.449199
2025-09-28 04:10:16,190 Stage: Train 0.5 | Epoch: 232 | Iter: 353800 | Total Loss: 0.004823 | Recon Loss: 0.003968 | Commit Loss: 0.001709 | Perplexity: 1843.786819
Trainning Epoch:  71%|███████   | 233/330 [21:22:41<5:28:00, 202.89s/it]2025-09-28 04:10:43,109 Stage: Train 0.5 | Epoch: 233 | Iter: 354000 | Total Loss: 0.004770 | Recon Loss: 0.003917 | Commit Loss: 0.001707 | Perplexity: 1841.424066
2025-09-28 04:11:09,802 Stage: Train 0.5 | Epoch: 233 | Iter: 354200 | Total Loss: 0.004809 | Recon Loss: 0.003956 | Commit Loss: 0.001706 | Perplexity: 1844.654008
2025-09-28 04:11:36,481 Stage: Train 0.5 | Epoch: 233 | Iter: 354400 | Total Loss: 0.004815 | Recon Loss: 0.003965 | Commit Loss: 0.001701 | Perplexity: 1841.664533
2025-09-28 04:12:02,951 Stage: Train 0.5 | Epoch: 233 | Iter: 354600 | Total Loss: 0.004794 | Recon Loss: 0.003941 | Commit Loss: 0.001706 | Perplexity: 1844.541152
2025-09-28 04:12:29,223 Stage: Train 0.5 | Epoch: 233 | Iter: 354800 | Total Loss: 0.004851 | Recon Loss: 0.003994 | Commit Loss: 0.001714 | Perplexity: 1843.561499
2025-09-28 04:12:55,800 Stage: Train 0.5 | Epoch: 233 | Iter: 355000 | Total Loss: 0.004793 | Recon Loss: 0.003937 | Commit Loss: 0.001713 | Perplexity: 1843.592861
2025-09-28 04:13:22,472 Stage: Train 0.5 | Epoch: 233 | Iter: 355200 | Total Loss: 0.004790 | Recon Loss: 0.003933 | Commit Loss: 0.001714 | Perplexity: 1845.586843
2025-09-28 04:13:49,106 Stage: Train 0.5 | Epoch: 233 | Iter: 355400 | Total Loss: 0.004846 | Recon Loss: 0.003986 | Commit Loss: 0.001719 | Perplexity: 1848.804638
Trainning Epoch:  71%|███████   | 234/330 [21:26:03<5:24:15, 202.67s/it]2025-09-28 04:14:16,025 Stage: Train 0.5 | Epoch: 234 | Iter: 355600 | Total Loss: 0.004805 | Recon Loss: 0.003951 | Commit Loss: 0.001709 | Perplexity: 1846.359283
2025-09-28 04:14:42,605 Stage: Train 0.5 | Epoch: 234 | Iter: 355800 | Total Loss: 0.004832 | Recon Loss: 0.003979 | Commit Loss: 0.001706 | Perplexity: 1843.742051
2025-09-28 04:15:09,125 Stage: Train 0.5 | Epoch: 234 | Iter: 356000 | Total Loss: 0.004799 | Recon Loss: 0.003945 | Commit Loss: 0.001708 | Perplexity: 1843.097573
2025-09-28 04:15:35,697 Stage: Train 0.5 | Epoch: 234 | Iter: 356200 | Total Loss: 0.004838 | Recon Loss: 0.003983 | Commit Loss: 0.001710 | Perplexity: 1844.095524
2025-09-28 04:16:02,344 Stage: Train 0.5 | Epoch: 234 | Iter: 356400 | Total Loss: 0.004794 | Recon Loss: 0.003933 | Commit Loss: 0.001721 | Perplexity: 1848.230648
2025-09-28 04:16:28,985 Stage: Train 0.5 | Epoch: 234 | Iter: 356600 | Total Loss: 0.004864 | Recon Loss: 0.004008 | Commit Loss: 0.001712 | Perplexity: 1841.047721
2025-09-28 04:16:55,644 Stage: Train 0.5 | Epoch: 234 | Iter: 356800 | Total Loss: 0.004781 | Recon Loss: 0.003926 | Commit Loss: 0.001710 | Perplexity: 1843.998469
Trainning Epoch:  71%|███████   | 235/330 [21:29:26<5:20:47, 202.60s/it]2025-09-28 04:17:22,626 Stage: Train 0.5 | Epoch: 235 | Iter: 357000 | Total Loss: 0.004826 | Recon Loss: 0.003972 | Commit Loss: 0.001709 | Perplexity: 1847.068510
2025-09-28 04:17:49,214 Stage: Train 0.5 | Epoch: 235 | Iter: 357200 | Total Loss: 0.004783 | Recon Loss: 0.003929 | Commit Loss: 0.001708 | Perplexity: 1847.214489
2025-09-28 04:18:15,832 Stage: Train 0.5 | Epoch: 235 | Iter: 357400 | Total Loss: 0.004849 | Recon Loss: 0.003993 | Commit Loss: 0.001712 | Perplexity: 1846.982042
2025-09-28 04:18:42,454 Stage: Train 0.5 | Epoch: 235 | Iter: 357600 | Total Loss: 0.004803 | Recon Loss: 0.003949 | Commit Loss: 0.001707 | Perplexity: 1848.375911
2025-09-28 04:19:09,115 Stage: Train 0.5 | Epoch: 235 | Iter: 357800 | Total Loss: 0.004828 | Recon Loss: 0.003969 | Commit Loss: 0.001717 | Perplexity: 1845.699494
2025-09-28 04:19:35,807 Stage: Train 0.5 | Epoch: 235 | Iter: 358000 | Total Loss: 0.004805 | Recon Loss: 0.003950 | Commit Loss: 0.001710 | Perplexity: 1841.668495
2025-09-28 04:20:02,372 Stage: Train 0.5 | Epoch: 235 | Iter: 358200 | Total Loss: 0.004815 | Recon Loss: 0.003957 | Commit Loss: 0.001715 | Perplexity: 1847.442919
2025-09-28 04:20:28,973 Stage: Train 0.5 | Epoch: 235 | Iter: 358400 | Total Loss: 0.004786 | Recon Loss: 0.003927 | Commit Loss: 0.001719 | Perplexity: 1848.386536
Trainning Epoch:  72%|███████▏  | 236/330 [21:32:48<5:17:19, 202.55s/it]2025-09-28 04:20:55,800 Stage: Train 0.5 | Epoch: 236 | Iter: 358600 | Total Loss: 0.004788 | Recon Loss: 0.003931 | Commit Loss: 0.001713 | Perplexity: 1844.398401
2025-09-28 04:21:22,553 Stage: Train 0.5 | Epoch: 236 | Iter: 358800 | Total Loss: 0.004791 | Recon Loss: 0.003937 | Commit Loss: 0.001708 | Perplexity: 1844.608121
2025-09-28 04:21:49,229 Stage: Train 0.5 | Epoch: 236 | Iter: 359000 | Total Loss: 0.004799 | Recon Loss: 0.003945 | Commit Loss: 0.001707 | Perplexity: 1843.819918
2025-09-28 04:22:16,026 Stage: Train 0.5 | Epoch: 236 | Iter: 359200 | Total Loss: 0.004778 | Recon Loss: 0.003924 | Commit Loss: 0.001709 | Perplexity: 1846.529528
2025-09-28 04:22:42,673 Stage: Train 0.5 | Epoch: 236 | Iter: 359400 | Total Loss: 0.004754 | Recon Loss: 0.003897 | Commit Loss: 0.001714 | Perplexity: 1845.115477
2025-09-28 04:23:09,409 Stage: Train 0.5 | Epoch: 236 | Iter: 359600 | Total Loss: 0.004818 | Recon Loss: 0.003960 | Commit Loss: 0.001714 | Perplexity: 1849.169008
2025-09-28 04:23:36,150 Stage: Train 0.5 | Epoch: 236 | Iter: 359800 | Total Loss: 0.004814 | Recon Loss: 0.003957 | Commit Loss: 0.001714 | Perplexity: 1845.090184
2025-09-28 04:24:02,768 Stage: Train 0.5 | Epoch: 236 | Iter: 360000 | Total Loss: 0.004790 | Recon Loss: 0.003929 | Commit Loss: 0.001722 | Perplexity: 1850.455547
2025-09-28 04:24:02,769 Saving model at iteration 360000
2025-09-28 04:24:03,534 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000
2025-09-28 04:24:04,294 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/model.safetensors
2025-09-28 04:24:04,874 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/optimizer.bin
2025-09-28 04:24:04,875 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/scheduler.bin
2025-09-28 04:24:04,875 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/sampler.bin
2025-09-28 04:24:04,876 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_237_step_360000/random_states_0.pkl
Trainning Epoch:  72%|███████▏  | 237/330 [21:36:14<5:15:13, 203.37s/it]2025-09-28 04:24:31,903 Stage: Train 0.5 | Epoch: 237 | Iter: 360200 | Total Loss: 0.004781 | Recon Loss: 0.003924 | Commit Loss: 0.001713 | Perplexity: 1851.698357
2025-09-28 04:24:58,495 Stage: Train 0.5 | Epoch: 237 | Iter: 360400 | Total Loss: 0.004834 | Recon Loss: 0.003980 | Commit Loss: 0.001709 | Perplexity: 1846.491165
2025-09-28 04:25:25,053 Stage: Train 0.5 | Epoch: 237 | Iter: 360600 | Total Loss: 0.004824 | Recon Loss: 0.003969 | Commit Loss: 0.001710 | Perplexity: 1849.061431
2025-09-28 04:25:51,625 Stage: Train 0.5 | Epoch: 237 | Iter: 360800 | Total Loss: 0.004757 | Recon Loss: 0.003901 | Commit Loss: 0.001713 | Perplexity: 1845.926345
2025-09-28 04:26:18,320 Stage: Train 0.5 | Epoch: 237 | Iter: 361000 | Total Loss: 0.004827 | Recon Loss: 0.003971 | Commit Loss: 0.001712 | Perplexity: 1844.062655
2025-09-28 04:26:44,999 Stage: Train 0.5 | Epoch: 237 | Iter: 361200 | Total Loss: 0.004803 | Recon Loss: 0.003947 | Commit Loss: 0.001713 | Perplexity: 1842.156223
2025-09-28 04:27:11,642 Stage: Train 0.5 | Epoch: 237 | Iter: 361400 | Total Loss: 0.004792 | Recon Loss: 0.003940 | Commit Loss: 0.001706 | Perplexity: 1845.813893
Trainning Epoch:  72%|███████▏  | 238/330 [21:39:36<5:11:26, 203.12s/it]2025-09-28 04:27:38,532 Stage: Train 0.5 | Epoch: 238 | Iter: 361600 | Total Loss: 0.004762 | Recon Loss: 0.003908 | Commit Loss: 0.001708 | Perplexity: 1844.943168
2025-09-28 04:28:05,267 Stage: Train 0.5 | Epoch: 238 | Iter: 361800 | Total Loss: 0.004806 | Recon Loss: 0.003948 | Commit Loss: 0.001715 | Perplexity: 1846.889294
2025-09-28 04:28:31,762 Stage: Train 0.5 | Epoch: 238 | Iter: 362000 | Total Loss: 0.004789 | Recon Loss: 0.003935 | Commit Loss: 0.001708 | Perplexity: 1847.876792
2025-09-28 04:28:58,432 Stage: Train 0.5 | Epoch: 238 | Iter: 362200 | Total Loss: 0.004815 | Recon Loss: 0.003959 | Commit Loss: 0.001712 | Perplexity: 1846.221735
2025-09-28 04:29:25,135 Stage: Train 0.5 | Epoch: 238 | Iter: 362400 | Total Loss: 0.004775 | Recon Loss: 0.003922 | Commit Loss: 0.001706 | Perplexity: 1842.419180
2025-09-28 04:29:51,880 Stage: Train 0.5 | Epoch: 238 | Iter: 362600 | Total Loss: 0.004809 | Recon Loss: 0.003950 | Commit Loss: 0.001718 | Perplexity: 1847.120011
2025-09-28 04:30:18,651 Stage: Train 0.5 | Epoch: 238 | Iter: 362800 | Total Loss: 0.004752 | Recon Loss: 0.003897 | Commit Loss: 0.001710 | Perplexity: 1846.620229
2025-09-28 04:30:45,378 Stage: Train 0.5 | Epoch: 238 | Iter: 363000 | Total Loss: 0.004814 | Recon Loss: 0.003958 | Commit Loss: 0.001712 | Perplexity: 1844.735010
Trainning Epoch:  72%|███████▏  | 239/330 [21:42:59<5:07:57, 203.05s/it]2025-09-28 04:31:12,211 Stage: Train 0.5 | Epoch: 239 | Iter: 363200 | Total Loss: 0.004793 | Recon Loss: 0.003939 | Commit Loss: 0.001708 | Perplexity: 1844.641410
2025-09-28 04:31:38,842 Stage: Train 0.5 | Epoch: 239 | Iter: 363400 | Total Loss: 0.004751 | Recon Loss: 0.003896 | Commit Loss: 0.001711 | Perplexity: 1846.932195
2025-09-28 04:32:05,616 Stage: Train 0.5 | Epoch: 239 | Iter: 363600 | Total Loss: 0.004839 | Recon Loss: 0.003983 | Commit Loss: 0.001712 | Perplexity: 1843.216942
2025-09-28 04:32:32,275 Stage: Train 0.5 | Epoch: 239 | Iter: 363800 | Total Loss: 0.004787 | Recon Loss: 0.003927 | Commit Loss: 0.001718 | Perplexity: 1848.457776
2025-09-28 04:32:58,963 Stage: Train 0.5 | Epoch: 239 | Iter: 364000 | Total Loss: 0.004786 | Recon Loss: 0.003930 | Commit Loss: 0.001713 | Perplexity: 1843.052675
2025-09-28 04:33:25,281 Stage: Train 0.5 | Epoch: 239 | Iter: 364200 | Total Loss: 0.004800 | Recon Loss: 0.003949 | Commit Loss: 0.001703 | Perplexity: 1839.729671
2025-09-28 04:33:52,007 Stage: Train 0.5 | Epoch: 239 | Iter: 364400 | Total Loss: 0.004800 | Recon Loss: 0.003945 | Commit Loss: 0.001710 | Perplexity: 1844.129424
Trainning Epoch:  73%|███████▎  | 240/330 [21:46:22<5:04:18, 202.88s/it]2025-09-28 04:34:19,044 Stage: Train 0.5 | Epoch: 240 | Iter: 364600 | Total Loss: 0.004840 | Recon Loss: 0.003983 | Commit Loss: 0.001715 | Perplexity: 1846.738557
2025-09-28 04:34:45,844 Stage: Train 0.5 | Epoch: 240 | Iter: 364800 | Total Loss: 0.004775 | Recon Loss: 0.003924 | Commit Loss: 0.001704 | Perplexity: 1842.281166
2025-09-28 04:35:12,545 Stage: Train 0.5 | Epoch: 240 | Iter: 365000 | Total Loss: 0.004784 | Recon Loss: 0.003926 | Commit Loss: 0.001716 | Perplexity: 1849.944294
2025-09-28 04:35:39,162 Stage: Train 0.5 | Epoch: 240 | Iter: 365200 | Total Loss: 0.004800 | Recon Loss: 0.003942 | Commit Loss: 0.001715 | Perplexity: 1849.763932
2025-09-28 04:36:05,786 Stage: Train 0.5 | Epoch: 240 | Iter: 365400 | Total Loss: 0.004722 | Recon Loss: 0.003866 | Commit Loss: 0.001713 | Perplexity: 1847.406299
2025-09-28 04:36:32,224 Stage: Train 0.5 | Epoch: 240 | Iter: 365600 | Total Loss: 0.004781 | Recon Loss: 0.003925 | Commit Loss: 0.001712 | Perplexity: 1847.046532
2025-09-28 04:36:58,911 Stage: Train 0.5 | Epoch: 240 | Iter: 365800 | Total Loss: 0.004809 | Recon Loss: 0.003953 | Commit Loss: 0.001712 | Perplexity: 1843.699756
2025-09-28 04:37:25,796 Stage: Train 0.5 | Epoch: 240 | Iter: 366000 | Total Loss: 0.004757 | Recon Loss: 0.003900 | Commit Loss: 0.001714 | Perplexity: 1846.789946
Trainning Epoch:  73%|███████▎  | 241/330 [21:49:45<5:00:59, 202.91s/it]2025-09-28 04:37:52,788 Stage: Train 0.5 | Epoch: 241 | Iter: 366200 | Total Loss: 0.004754 | Recon Loss: 0.003901 | Commit Loss: 0.001707 | Perplexity: 1845.746735
2025-09-28 04:38:19,433 Stage: Train 0.5 | Epoch: 241 | Iter: 366400 | Total Loss: 0.004772 | Recon Loss: 0.003916 | Commit Loss: 0.001712 | Perplexity: 1849.056211
2025-09-28 04:38:46,111 Stage: Train 0.5 | Epoch: 241 | Iter: 366600 | Total Loss: 0.004833 | Recon Loss: 0.003974 | Commit Loss: 0.001718 | Perplexity: 1846.668657
2025-09-28 04:39:12,873 Stage: Train 0.5 | Epoch: 241 | Iter: 366800 | Total Loss: 0.004771 | Recon Loss: 0.003916 | Commit Loss: 0.001711 | Perplexity: 1847.914210
2025-09-28 04:39:39,528 Stage: Train 0.5 | Epoch: 241 | Iter: 367000 | Total Loss: 0.004739 | Recon Loss: 0.003881 | Commit Loss: 0.001716 | Perplexity: 1844.844761
2025-09-28 04:40:06,184 Stage: Train 0.5 | Epoch: 241 | Iter: 367200 | Total Loss: 0.004778 | Recon Loss: 0.003924 | Commit Loss: 0.001709 | Perplexity: 1846.701161
2025-09-28 04:40:32,875 Stage: Train 0.5 | Epoch: 241 | Iter: 367400 | Total Loss: 0.004764 | Recon Loss: 0.003908 | Commit Loss: 0.001712 | Perplexity: 1847.019042
Trainning Epoch:  73%|███████▎  | 242/330 [21:53:08<4:57:39, 202.94s/it]2025-09-28 04:40:59,885 Stage: Train 0.5 | Epoch: 242 | Iter: 367600 | Total Loss: 0.004765 | Recon Loss: 0.003910 | Commit Loss: 0.001711 | Perplexity: 1843.693226
2025-09-28 04:41:26,597 Stage: Train 0.5 | Epoch: 242 | Iter: 367800 | Total Loss: 0.004748 | Recon Loss: 0.003894 | Commit Loss: 0.001709 | Perplexity: 1847.937678
2025-09-28 04:41:53,361 Stage: Train 0.5 | Epoch: 242 | Iter: 368000 | Total Loss: 0.004795 | Recon Loss: 0.003943 | Commit Loss: 0.001704 | Perplexity: 1846.908025
2025-09-28 04:42:20,040 Stage: Train 0.5 | Epoch: 242 | Iter: 368200 | Total Loss: 0.004785 | Recon Loss: 0.003928 | Commit Loss: 0.001714 | Perplexity: 1846.676967
2025-09-28 04:42:46,863 Stage: Train 0.5 | Epoch: 242 | Iter: 368400 | Total Loss: 0.004804 | Recon Loss: 0.003949 | Commit Loss: 0.001711 | Perplexity: 1843.912925
2025-09-28 04:43:13,485 Stage: Train 0.5 | Epoch: 242 | Iter: 368600 | Total Loss: 0.004810 | Recon Loss: 0.003954 | Commit Loss: 0.001711 | Perplexity: 1846.421949
2025-09-28 04:43:40,207 Stage: Train 0.5 | Epoch: 242 | Iter: 368800 | Total Loss: 0.004775 | Recon Loss: 0.003918 | Commit Loss: 0.001715 | Perplexity: 1851.864275
2025-09-28 04:44:06,975 Stage: Train 0.5 | Epoch: 242 | Iter: 369000 | Total Loss: 0.004763 | Recon Loss: 0.003905 | Commit Loss: 0.001717 | Perplexity: 1848.780046
Trainning Epoch:  74%|███████▎  | 243/330 [21:56:31<4:54:23, 203.03s/it]2025-09-28 04:44:33,734 Stage: Train 0.5 | Epoch: 243 | Iter: 369200 | Total Loss: 0.004764 | Recon Loss: 0.003910 | Commit Loss: 0.001708 | Perplexity: 1843.507739
2025-09-28 04:45:00,477 Stage: Train 0.5 | Epoch: 243 | Iter: 369400 | Total Loss: 0.004765 | Recon Loss: 0.003910 | Commit Loss: 0.001709 | Perplexity: 1845.721041
2025-09-28 04:45:27,116 Stage: Train 0.5 | Epoch: 243 | Iter: 369600 | Total Loss: 0.004785 | Recon Loss: 0.003933 | Commit Loss: 0.001705 | Perplexity: 1844.197541
2025-09-28 04:45:53,720 Stage: Train 0.5 | Epoch: 243 | Iter: 369800 | Total Loss: 0.004757 | Recon Loss: 0.003902 | Commit Loss: 0.001710 | Perplexity: 1844.293776
2025-09-28 04:46:20,295 Stage: Train 0.5 | Epoch: 243 | Iter: 370000 | Total Loss: 0.004812 | Recon Loss: 0.003957 | Commit Loss: 0.001711 | Perplexity: 1845.625185
2025-09-28 04:46:46,938 Stage: Train 0.5 | Epoch: 243 | Iter: 370200 | Total Loss: 0.004789 | Recon Loss: 0.003931 | Commit Loss: 0.001716 | Perplexity: 1850.631577
2025-09-28 04:47:13,556 Stage: Train 0.5 | Epoch: 243 | Iter: 370400 | Total Loss: 0.004764 | Recon Loss: 0.003909 | Commit Loss: 0.001709 | Perplexity: 1845.345772
2025-09-28 04:47:40,078 Stage: Train 0.5 | Epoch: 243 | Iter: 370600 | Total Loss: 0.004754 | Recon Loss: 0.003895 | Commit Loss: 0.001719 | Perplexity: 1848.375576
Trainning Epoch:  74%|███████▍  | 244/330 [21:59:53<4:50:42, 202.82s/it]2025-09-28 04:48:06,982 Stage: Train 0.5 | Epoch: 244 | Iter: 370800 | Total Loss: 0.004736 | Recon Loss: 0.003885 | Commit Loss: 0.001702 | Perplexity: 1844.016866
2025-09-28 04:48:33,484 Stage: Train 0.5 | Epoch: 244 | Iter: 371000 | Total Loss: 0.004776 | Recon Loss: 0.003921 | Commit Loss: 0.001710 | Perplexity: 1849.316128
2025-09-28 04:49:00,208 Stage: Train 0.5 | Epoch: 244 | Iter: 371200 | Total Loss: 0.004748 | Recon Loss: 0.003896 | Commit Loss: 0.001703 | Perplexity: 1844.914219
2025-09-28 04:49:26,846 Stage: Train 0.5 | Epoch: 244 | Iter: 371400 | Total Loss: 0.004780 | Recon Loss: 0.003917 | Commit Loss: 0.001726 | Perplexity: 1848.207137
2025-09-28 04:49:53,437 Stage: Train 0.5 | Epoch: 244 | Iter: 371600 | Total Loss: 0.004745 | Recon Loss: 0.003889 | Commit Loss: 0.001712 | Perplexity: 1845.724040
2025-09-28 04:50:20,102 Stage: Train 0.5 | Epoch: 244 | Iter: 371800 | Total Loss: 0.004789 | Recon Loss: 0.003929 | Commit Loss: 0.001720 | Perplexity: 1849.523541
2025-09-28 04:50:46,690 Stage: Train 0.5 | Epoch: 244 | Iter: 372000 | Total Loss: 0.004745 | Recon Loss: 0.003890 | Commit Loss: 0.001710 | Perplexity: 1848.361943
Trainning Epoch:  74%|███████▍  | 245/330 [22:03:15<4:47:06, 202.67s/it]2025-09-28 04:51:13,448 Stage: Train 0.5 | Epoch: 245 | Iter: 372200 | Total Loss: 0.004751 | Recon Loss: 0.003897 | Commit Loss: 0.001707 | Perplexity: 1847.323824
2025-09-28 04:51:40,186 Stage: Train 0.5 | Epoch: 245 | Iter: 372400 | Total Loss: 0.004760 | Recon Loss: 0.003907 | Commit Loss: 0.001707 | Perplexity: 1846.360063
2025-09-28 04:52:06,892 Stage: Train 0.5 | Epoch: 245 | Iter: 372600 | Total Loss: 0.004758 | Recon Loss: 0.003907 | Commit Loss: 0.001703 | Perplexity: 1846.495533
2025-09-28 04:52:33,520 Stage: Train 0.5 | Epoch: 245 | Iter: 372800 | Total Loss: 0.004778 | Recon Loss: 0.003922 | Commit Loss: 0.001712 | Perplexity: 1850.589866
2025-09-28 04:53:00,213 Stage: Train 0.5 | Epoch: 245 | Iter: 373000 | Total Loss: 0.004817 | Recon Loss: 0.003961 | Commit Loss: 0.001713 | Perplexity: 1851.745026
2025-09-28 04:53:26,803 Stage: Train 0.5 | Epoch: 245 | Iter: 373200 | Total Loss: 0.004786 | Recon Loss: 0.003933 | Commit Loss: 0.001707 | Perplexity: 1844.556257
2025-09-28 04:53:53,604 Stage: Train 0.5 | Epoch: 245 | Iter: 373400 | Total Loss: 0.004792 | Recon Loss: 0.003939 | Commit Loss: 0.001706 | Perplexity: 1845.562227
2025-09-28 04:54:20,296 Stage: Train 0.5 | Epoch: 245 | Iter: 373600 | Total Loss: 0.004761 | Recon Loss: 0.003904 | Commit Loss: 0.001713 | Perplexity: 1848.399605
Trainning Epoch:  75%|███████▍  | 246/330 [22:06:38<4:43:52, 202.77s/it]2025-09-28 04:54:47,147 Stage: Train 0.5 | Epoch: 246 | Iter: 373800 | Total Loss: 0.004753 | Recon Loss: 0.003899 | Commit Loss: 0.001709 | Perplexity: 1845.360962
2025-09-28 04:55:13,920 Stage: Train 0.5 | Epoch: 246 | Iter: 374000 | Total Loss: 0.004745 | Recon Loss: 0.003891 | Commit Loss: 0.001707 | Perplexity: 1845.819724
2025-09-28 04:55:40,473 Stage: Train 0.5 | Epoch: 246 | Iter: 374200 | Total Loss: 0.004761 | Recon Loss: 0.003908 | Commit Loss: 0.001707 | Perplexity: 1848.551290
2025-09-28 04:56:07,125 Stage: Train 0.5 | Epoch: 246 | Iter: 374400 | Total Loss: 0.004773 | Recon Loss: 0.003916 | Commit Loss: 0.001714 | Perplexity: 1848.602999
2025-09-28 04:56:33,737 Stage: Train 0.5 | Epoch: 246 | Iter: 374600 | Total Loss: 0.004746 | Recon Loss: 0.003888 | Commit Loss: 0.001716 | Perplexity: 1845.031597
2025-09-28 04:57:00,332 Stage: Train 0.5 | Epoch: 246 | Iter: 374800 | Total Loss: 0.004769 | Recon Loss: 0.003914 | Commit Loss: 0.001711 | Perplexity: 1845.266107
2025-09-28 04:57:27,012 Stage: Train 0.5 | Epoch: 246 | Iter: 375000 | Total Loss: 0.004744 | Recon Loss: 0.003889 | Commit Loss: 0.001709 | Perplexity: 1846.179233
Trainning Epoch:  75%|███████▍  | 247/330 [22:10:01<4:40:26, 202.72s/it]2025-09-28 04:57:54,012 Stage: Train 0.5 | Epoch: 247 | Iter: 375200 | Total Loss: 0.004737 | Recon Loss: 0.003885 | Commit Loss: 0.001704 | Perplexity: 1844.330798
2025-09-28 04:58:20,695 Stage: Train 0.5 | Epoch: 247 | Iter: 375400 | Total Loss: 0.004767 | Recon Loss: 0.003911 | Commit Loss: 0.001712 | Perplexity: 1850.226817
2025-09-28 04:58:47,287 Stage: Train 0.5 | Epoch: 247 | Iter: 375600 | Total Loss: 0.004729 | Recon Loss: 0.003874 | Commit Loss: 0.001709 | Perplexity: 1850.091605
2025-09-28 04:59:13,853 Stage: Train 0.5 | Epoch: 247 | Iter: 375800 | Total Loss: 0.004743 | Recon Loss: 0.003889 | Commit Loss: 0.001709 | Perplexity: 1843.120779
2025-09-28 04:59:40,488 Stage: Train 0.5 | Epoch: 247 | Iter: 376000 | Total Loss: 0.004753 | Recon Loss: 0.003899 | Commit Loss: 0.001707 | Perplexity: 1844.263281
2025-09-28 05:00:07,019 Stage: Train 0.5 | Epoch: 247 | Iter: 376200 | Total Loss: 0.004779 | Recon Loss: 0.003922 | Commit Loss: 0.001714 | Perplexity: 1852.390644
2025-09-28 05:00:33,696 Stage: Train 0.5 | Epoch: 247 | Iter: 376400 | Total Loss: 0.004772 | Recon Loss: 0.003918 | Commit Loss: 0.001708 | Perplexity: 1846.017605
2025-09-28 05:01:00,324 Stage: Train 0.5 | Epoch: 247 | Iter: 376600 | Total Loss: 0.004759 | Recon Loss: 0.003906 | Commit Loss: 0.001705 | Perplexity: 1846.959503
Trainning Epoch:  75%|███████▌  | 248/330 [22:13:23<4:36:51, 202.58s/it]2025-09-28 05:01:27,028 Stage: Train 0.5 | Epoch: 248 | Iter: 376800 | Total Loss: 0.004748 | Recon Loss: 0.003886 | Commit Loss: 0.001724 | Perplexity: 1854.216885
2025-09-28 05:01:53,684 Stage: Train 0.5 | Epoch: 248 | Iter: 377000 | Total Loss: 0.004751 | Recon Loss: 0.003898 | Commit Loss: 0.001706 | Perplexity: 1848.248513
2025-09-28 05:02:20,356 Stage: Train 0.5 | Epoch: 248 | Iter: 377200 | Total Loss: 0.004772 | Recon Loss: 0.003910 | Commit Loss: 0.001724 | Perplexity: 1852.848275
2025-09-28 05:02:46,946 Stage: Train 0.5 | Epoch: 248 | Iter: 377400 | Total Loss: 0.004725 | Recon Loss: 0.003871 | Commit Loss: 0.001708 | Perplexity: 1847.396270
2025-09-28 05:03:13,537 Stage: Train 0.5 | Epoch: 248 | Iter: 377600 | Total Loss: 0.004740 | Recon Loss: 0.003887 | Commit Loss: 0.001706 | Perplexity: 1850.670860
2025-09-28 05:03:40,194 Stage: Train 0.5 | Epoch: 248 | Iter: 377800 | Total Loss: 0.004742 | Recon Loss: 0.003887 | Commit Loss: 0.001710 | Perplexity: 1845.811192
2025-09-28 05:04:06,758 Stage: Train 0.5 | Epoch: 248 | Iter: 378000 | Total Loss: 0.004745 | Recon Loss: 0.003888 | Commit Loss: 0.001714 | Perplexity: 1850.310082
2025-09-28 05:04:33,307 Stage: Train 0.5 | Epoch: 248 | Iter: 378200 | Total Loss: 0.004764 | Recon Loss: 0.003912 | Commit Loss: 0.001704 | Perplexity: 1845.486642
Trainning Epoch:  75%|███████▌  | 249/330 [22:16:46<4:33:25, 202.54s/it]2025-09-28 05:05:00,264 Stage: Train 0.5 | Epoch: 249 | Iter: 378400 | Total Loss: 0.004797 | Recon Loss: 0.003945 | Commit Loss: 0.001704 | Perplexity: 1843.643088
2025-09-28 05:05:27,050 Stage: Train 0.5 | Epoch: 249 | Iter: 378600 | Total Loss: 0.004748 | Recon Loss: 0.003892 | Commit Loss: 0.001712 | Perplexity: 1847.275535
2025-09-28 05:05:53,777 Stage: Train 0.5 | Epoch: 249 | Iter: 378800 | Total Loss: 0.004724 | Recon Loss: 0.003872 | Commit Loss: 0.001705 | Perplexity: 1847.922947
2025-09-28 05:06:20,507 Stage: Train 0.5 | Epoch: 249 | Iter: 379000 | Total Loss: 0.004779 | Recon Loss: 0.003924 | Commit Loss: 0.001710 | Perplexity: 1849.773794
2025-09-28 05:06:47,068 Stage: Train 0.5 | Epoch: 249 | Iter: 379200 | Total Loss: 0.004739 | Recon Loss: 0.003883 | Commit Loss: 0.001712 | Perplexity: 1848.951656
2025-09-28 05:07:13,690 Stage: Train 0.5 | Epoch: 249 | Iter: 379400 | Total Loss: 0.004740 | Recon Loss: 0.003888 | Commit Loss: 0.001704 | Perplexity: 1843.739682
2025-09-28 05:07:40,365 Stage: Train 0.5 | Epoch: 249 | Iter: 379600 | Total Loss: 0.004821 | Recon Loss: 0.003968 | Commit Loss: 0.001707 | Perplexity: 1847.754005
Trainning Epoch:  76%|███████▌  | 250/330 [22:20:09<4:30:10, 202.63s/it]2025-09-28 05:08:07,274 Stage: Train 0.5 | Epoch: 250 | Iter: 379800 | Total Loss: 0.004728 | Recon Loss: 0.003875 | Commit Loss: 0.001706 | Perplexity: 1848.888536
2025-09-28 05:08:33,816 Stage: Train 0.5 | Epoch: 250 | Iter: 380000 | Total Loss: 0.004762 | Recon Loss: 0.003908 | Commit Loss: 0.001708 | Perplexity: 1845.383059
2025-09-28 05:08:33,816 Saving model at iteration 380000
2025-09-28 05:08:34,309 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000
2025-09-28 05:08:34,855 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/model.safetensors
2025-09-28 05:08:35,452 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/optimizer.bin
2025-09-28 05:08:35,452 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/scheduler.bin
2025-09-28 05:08:35,452 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/sampler.bin
2025-09-28 05:08:35,453 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_251_step_380000/random_states_0.pkl
2025-09-28 05:09:02,161 Stage: Train 0.5 | Epoch: 250 | Iter: 380200 | Total Loss: 0.004721 | Recon Loss: 0.003870 | Commit Loss: 0.001701 | Perplexity: 1846.622101
2025-09-28 05:09:28,850 Stage: Train 0.5 | Epoch: 250 | Iter: 380400 | Total Loss: 0.004779 | Recon Loss: 0.003925 | Commit Loss: 0.001709 | Perplexity: 1847.399778
2025-09-28 05:09:55,482 Stage: Train 0.5 | Epoch: 250 | Iter: 380600 | Total Loss: 0.004780 | Recon Loss: 0.003925 | Commit Loss: 0.001710 | Perplexity: 1851.056514
2025-09-28 05:10:22,258 Stage: Train 0.5 | Epoch: 250 | Iter: 380800 | Total Loss: 0.004742 | Recon Loss: 0.003882 | Commit Loss: 0.001721 | Perplexity: 1846.455153
2025-09-28 05:10:48,749 Stage: Train 0.5 | Epoch: 250 | Iter: 381000 | Total Loss: 0.004761 | Recon Loss: 0.003907 | Commit Loss: 0.001708 | Perplexity: 1847.286752
2025-09-28 05:11:15,372 Stage: Train 0.5 | Epoch: 250 | Iter: 381200 | Total Loss: 0.004704 | Recon Loss: 0.003850 | Commit Loss: 0.001707 | Perplexity: 1844.406216
Trainning Epoch:  76%|███████▌  | 251/330 [22:23:33<4:27:25, 203.10s/it]2025-09-28 05:11:42,291 Stage: Train 0.5 | Epoch: 251 | Iter: 381400 | Total Loss: 0.004744 | Recon Loss: 0.003890 | Commit Loss: 0.001706 | Perplexity: 1848.913156
2025-09-28 05:12:09,020 Stage: Train 0.5 | Epoch: 251 | Iter: 381600 | Total Loss: 0.004794 | Recon Loss: 0.003937 | Commit Loss: 0.001714 | Perplexity: 1850.526146
2025-09-28 05:12:35,809 Stage: Train 0.5 | Epoch: 251 | Iter: 381800 | Total Loss: 0.004684 | Recon Loss: 0.003831 | Commit Loss: 0.001705 | Perplexity: 1847.835595
2025-09-28 05:13:02,449 Stage: Train 0.5 | Epoch: 251 | Iter: 382000 | Total Loss: 0.004766 | Recon Loss: 0.003909 | Commit Loss: 0.001715 | Perplexity: 1848.544390
2025-09-28 05:13:29,052 Stage: Train 0.5 | Epoch: 251 | Iter: 382200 | Total Loss: 0.004694 | Recon Loss: 0.003843 | Commit Loss: 0.001702 | Perplexity: 1842.280640
2025-09-28 05:13:55,821 Stage: Train 0.5 | Epoch: 251 | Iter: 382400 | Total Loss: 0.004748 | Recon Loss: 0.003895 | Commit Loss: 0.001706 | Perplexity: 1847.354623
2025-09-28 05:14:22,408 Stage: Train 0.5 | Epoch: 251 | Iter: 382600 | Total Loss: 0.004758 | Recon Loss: 0.003903 | Commit Loss: 0.001711 | Perplexity: 1846.271649
Trainning Epoch:  76%|███████▋  | 252/330 [22:26:56<4:23:58, 203.06s/it]2025-09-28 05:14:49,394 Stage: Train 0.5 | Epoch: 252 | Iter: 382800 | Total Loss: 0.004751 | Recon Loss: 0.003895 | Commit Loss: 0.001712 | Perplexity: 1847.958062
2025-09-28 05:15:16,087 Stage: Train 0.5 | Epoch: 252 | Iter: 383000 | Total Loss: 0.004750 | Recon Loss: 0.003899 | Commit Loss: 0.001702 | Perplexity: 1846.365168
2025-09-28 05:15:42,597 Stage: Train 0.5 | Epoch: 252 | Iter: 383200 | Total Loss: 0.004697 | Recon Loss: 0.003844 | Commit Loss: 0.001706 | Perplexity: 1844.849725
2025-09-28 05:16:09,228 Stage: Train 0.5 | Epoch: 252 | Iter: 383400 | Total Loss: 0.004736 | Recon Loss: 0.003880 | Commit Loss: 0.001712 | Perplexity: 1850.696205
2025-09-28 05:16:35,900 Stage: Train 0.5 | Epoch: 252 | Iter: 383600 | Total Loss: 0.004706 | Recon Loss: 0.003853 | Commit Loss: 0.001706 | Perplexity: 1847.660980
2025-09-28 05:17:02,426 Stage: Train 0.5 | Epoch: 252 | Iter: 383800 | Total Loss: 0.004784 | Recon Loss: 0.003932 | Commit Loss: 0.001705 | Perplexity: 1846.141615
2025-09-28 05:17:29,151 Stage: Train 0.5 | Epoch: 252 | Iter: 384000 | Total Loss: 0.004731 | Recon Loss: 0.003877 | Commit Loss: 0.001707 | Perplexity: 1847.674612
2025-09-28 05:17:55,783 Stage: Train 0.5 | Epoch: 252 | Iter: 384200 | Total Loss: 0.004757 | Recon Loss: 0.003904 | Commit Loss: 0.001707 | Perplexity: 1848.582432
Trainning Epoch:  77%|███████▋  | 253/330 [22:30:18<4:20:27, 202.95s/it]2025-09-28 05:18:22,922 Stage: Train 0.5 | Epoch: 253 | Iter: 384400 | Total Loss: 0.004753 | Recon Loss: 0.003895 | Commit Loss: 0.001715 | Perplexity: 1849.756366
2025-09-28 05:18:49,528 Stage: Train 0.5 | Epoch: 253 | Iter: 384600 | Total Loss: 0.004744 | Recon Loss: 0.003891 | Commit Loss: 0.001704 | Perplexity: 1846.643030
2025-09-28 05:19:16,036 Stage: Train 0.5 | Epoch: 253 | Iter: 384800 | Total Loss: 0.004750 | Recon Loss: 0.003897 | Commit Loss: 0.001707 | Perplexity: 1846.555187
2025-09-28 05:19:42,776 Stage: Train 0.5 | Epoch: 253 | Iter: 385000 | Total Loss: 0.004730 | Recon Loss: 0.003878 | Commit Loss: 0.001703 | Perplexity: 1848.108988
2025-09-28 05:20:09,520 Stage: Train 0.5 | Epoch: 253 | Iter: 385200 | Total Loss: 0.004732 | Recon Loss: 0.003882 | Commit Loss: 0.001700 | Perplexity: 1848.204518
2025-09-28 05:20:36,196 Stage: Train 0.5 | Epoch: 253 | Iter: 385400 | Total Loss: 0.004701 | Recon Loss: 0.003846 | Commit Loss: 0.001709 | Perplexity: 1849.033943
2025-09-28 05:21:02,946 Stage: Train 0.5 | Epoch: 253 | Iter: 385600 | Total Loss: 0.004776 | Recon Loss: 0.003918 | Commit Loss: 0.001717 | Perplexity: 1848.140684
2025-09-28 05:21:29,563 Stage: Train 0.5 | Epoch: 253 | Iter: 385800 | Total Loss: 0.004764 | Recon Loss: 0.003907 | Commit Loss: 0.001713 | Perplexity: 1845.292318
Trainning Epoch:  77%|███████▋  | 254/330 [22:33:41<4:17:01, 202.92s/it]2025-09-28 05:21:56,560 Stage: Train 0.5 | Epoch: 254 | Iter: 386000 | Total Loss: 0.004771 | Recon Loss: 0.003917 | Commit Loss: 0.001708 | Perplexity: 1847.120623
2025-09-28 05:22:23,239 Stage: Train 0.5 | Epoch: 254 | Iter: 386200 | Total Loss: 0.004697 | Recon Loss: 0.003845 | Commit Loss: 0.001702 | Perplexity: 1848.498707
2025-09-28 05:22:49,910 Stage: Train 0.5 | Epoch: 254 | Iter: 386400 | Total Loss: 0.004771 | Recon Loss: 0.003917 | Commit Loss: 0.001708 | Perplexity: 1846.116595
2025-09-28 05:23:16,559 Stage: Train 0.5 | Epoch: 254 | Iter: 386600 | Total Loss: 0.004741 | Recon Loss: 0.003886 | Commit Loss: 0.001710 | Perplexity: 1850.721191
2025-09-28 05:23:43,202 Stage: Train 0.5 | Epoch: 254 | Iter: 386800 | Total Loss: 0.004733 | Recon Loss: 0.003877 | Commit Loss: 0.001711 | Perplexity: 1850.501478
2025-09-28 05:24:09,741 Stage: Train 0.5 | Epoch: 254 | Iter: 387000 | Total Loss: 0.004743 | Recon Loss: 0.003884 | Commit Loss: 0.001717 | Perplexity: 1850.219882
2025-09-28 05:24:36,511 Stage: Train 0.5 | Epoch: 254 | Iter: 387200 | Total Loss: 0.004722 | Recon Loss: 0.003866 | Commit Loss: 0.001712 | Perplexity: 1852.321266
Trainning Epoch:  77%|███████▋  | 255/330 [22:37:04<4:13:36, 202.89s/it]2025-09-28 05:25:03,486 Stage: Train 0.5 | Epoch: 255 | Iter: 387400 | Total Loss: 0.004734 | Recon Loss: 0.003882 | Commit Loss: 0.001703 | Perplexity: 1841.603918
2025-09-28 05:25:30,081 Stage: Train 0.5 | Epoch: 255 | Iter: 387600 | Total Loss: 0.004649 | Recon Loss: 0.003800 | Commit Loss: 0.001699 | Perplexity: 1846.874158
2025-09-28 05:25:56,523 Stage: Train 0.5 | Epoch: 255 | Iter: 387800 | Total Loss: 0.004747 | Recon Loss: 0.003892 | Commit Loss: 0.001709 | Perplexity: 1848.184379
2025-09-28 05:26:23,338 Stage: Train 0.5 | Epoch: 255 | Iter: 388000 | Total Loss: 0.004703 | Recon Loss: 0.003848 | Commit Loss: 0.001712 | Perplexity: 1848.550891
2025-09-28 05:26:50,009 Stage: Train 0.5 | Epoch: 255 | Iter: 388200 | Total Loss: 0.004748 | Recon Loss: 0.003896 | Commit Loss: 0.001704 | Perplexity: 1850.592821
2025-09-28 05:27:16,696 Stage: Train 0.5 | Epoch: 255 | Iter: 388400 | Total Loss: 0.004779 | Recon Loss: 0.003920 | Commit Loss: 0.001720 | Perplexity: 1849.807504
2025-09-28 05:27:43,421 Stage: Train 0.5 | Epoch: 255 | Iter: 388600 | Total Loss: 0.004736 | Recon Loss: 0.003883 | Commit Loss: 0.001705 | Perplexity: 1846.938327
2025-09-28 05:28:10,136 Stage: Train 0.5 | Epoch: 255 | Iter: 388800 | Total Loss: 0.004712 | Recon Loss: 0.003858 | Commit Loss: 0.001709 | Perplexity: 1850.039109
Trainning Epoch:  78%|███████▊  | 256/330 [22:40:27<4:10:12, 202.87s/it]2025-09-28 05:28:37,151 Stage: Train 0.5 | Epoch: 256 | Iter: 389000 | Total Loss: 0.004758 | Recon Loss: 0.003903 | Commit Loss: 0.001710 | Perplexity: 1851.328835
2025-09-28 05:29:03,848 Stage: Train 0.5 | Epoch: 256 | Iter: 389200 | Total Loss: 0.004714 | Recon Loss: 0.003859 | Commit Loss: 0.001709 | Perplexity: 1851.456770
2025-09-28 05:29:30,491 Stage: Train 0.5 | Epoch: 256 | Iter: 389400 | Total Loss: 0.004697 | Recon Loss: 0.003844 | Commit Loss: 0.001706 | Perplexity: 1851.461125
2025-09-28 05:29:57,117 Stage: Train 0.5 | Epoch: 256 | Iter: 389600 | Total Loss: 0.004725 | Recon Loss: 0.003873 | Commit Loss: 0.001704 | Perplexity: 1844.943793
2025-09-28 05:30:23,776 Stage: Train 0.5 | Epoch: 256 | Iter: 389800 | Total Loss: 0.004695 | Recon Loss: 0.003841 | Commit Loss: 0.001708 | Perplexity: 1850.670199
2025-09-28 05:30:50,475 Stage: Train 0.5 | Epoch: 256 | Iter: 390000 | Total Loss: 0.004701 | Recon Loss: 0.003847 | Commit Loss: 0.001708 | Perplexity: 1846.996208
2025-09-28 05:31:17,074 Stage: Train 0.5 | Epoch: 256 | Iter: 390200 | Total Loss: 0.004744 | Recon Loss: 0.003888 | Commit Loss: 0.001713 | Perplexity: 1853.891727
Trainning Epoch:  78%|███████▊  | 257/330 [22:43:50<4:06:45, 202.82s/it]2025-09-28 05:31:43,938 Stage: Train 0.5 | Epoch: 257 | Iter: 390400 | Total Loss: 0.004703 | Recon Loss: 0.003852 | Commit Loss: 0.001702 | Perplexity: 1844.317471
2025-09-28 05:32:10,645 Stage: Train 0.5 | Epoch: 257 | Iter: 390600 | Total Loss: 0.004751 | Recon Loss: 0.003902 | Commit Loss: 0.001698 | Perplexity: 1846.721603
2025-09-28 05:32:37,315 Stage: Train 0.5 | Epoch: 257 | Iter: 390800 | Total Loss: 0.004725 | Recon Loss: 0.003871 | Commit Loss: 0.001708 | Perplexity: 1853.716162
2025-09-28 05:33:04,011 Stage: Train 0.5 | Epoch: 257 | Iter: 391000 | Total Loss: 0.004770 | Recon Loss: 0.003915 | Commit Loss: 0.001711 | Perplexity: 1849.788652
2025-09-28 05:33:30,681 Stage: Train 0.5 | Epoch: 257 | Iter: 391200 | Total Loss: 0.004748 | Recon Loss: 0.003893 | Commit Loss: 0.001710 | Perplexity: 1851.094463
2025-09-28 05:33:57,388 Stage: Train 0.5 | Epoch: 257 | Iter: 391400 | Total Loss: 0.004759 | Recon Loss: 0.003907 | Commit Loss: 0.001704 | Perplexity: 1849.104482
2025-09-28 05:34:24,049 Stage: Train 0.5 | Epoch: 257 | Iter: 391600 | Total Loss: 0.004726 | Recon Loss: 0.003870 | Commit Loss: 0.001710 | Perplexity: 1850.957499
2025-09-28 05:34:50,812 Stage: Train 0.5 | Epoch: 257 | Iter: 391800 | Total Loss: 0.004708 | Recon Loss: 0.003853 | Commit Loss: 0.001710 | Perplexity: 1849.815348
Trainning Epoch:  78%|███████▊  | 258/330 [22:47:13<4:03:27, 202.88s/it]2025-09-28 05:35:17,772 Stage: Train 0.5 | Epoch: 258 | Iter: 392000 | Total Loss: 0.004703 | Recon Loss: 0.003854 | Commit Loss: 0.001698 | Perplexity: 1843.675747
2025-09-28 05:35:44,499 Stage: Train 0.5 | Epoch: 258 | Iter: 392200 | Total Loss: 0.004697 | Recon Loss: 0.003845 | Commit Loss: 0.001705 | Perplexity: 1849.168074
2025-09-28 05:36:10,931 Stage: Train 0.5 | Epoch: 258 | Iter: 392400 | Total Loss: 0.004758 | Recon Loss: 0.003904 | Commit Loss: 0.001708 | Perplexity: 1851.708579
2025-09-28 05:36:37,773 Stage: Train 0.5 | Epoch: 258 | Iter: 392600 | Total Loss: 0.004720 | Recon Loss: 0.003865 | Commit Loss: 0.001710 | Perplexity: 1849.830845
2025-09-28 05:37:04,388 Stage: Train 0.5 | Epoch: 258 | Iter: 392800 | Total Loss: 0.004688 | Recon Loss: 0.003837 | Commit Loss: 0.001702 | Perplexity: 1846.357827
2025-09-28 05:37:31,049 Stage: Train 0.5 | Epoch: 258 | Iter: 393000 | Total Loss: 0.004684 | Recon Loss: 0.003833 | Commit Loss: 0.001701 | Perplexity: 1850.199800
2025-09-28 05:37:57,752 Stage: Train 0.5 | Epoch: 258 | Iter: 393200 | Total Loss: 0.004759 | Recon Loss: 0.003897 | Commit Loss: 0.001725 | Perplexity: 1853.375923
2025-09-28 05:38:24,572 Stage: Train 0.5 | Epoch: 258 | Iter: 393400 | Total Loss: 0.004712 | Recon Loss: 0.003859 | Commit Loss: 0.001706 | Perplexity: 1848.558329
Trainning Epoch:  78%|███████▊  | 259/330 [22:50:36<4:00:06, 202.91s/it]2025-09-28 05:38:51,502 Stage: Train 0.5 | Epoch: 259 | Iter: 393600 | Total Loss: 0.004717 | Recon Loss: 0.003864 | Commit Loss: 0.001707 | Perplexity: 1848.959061
2025-09-28 05:39:18,234 Stage: Train 0.5 | Epoch: 259 | Iter: 393800 | Total Loss: 0.004675 | Recon Loss: 0.003824 | Commit Loss: 0.001702 | Perplexity: 1853.557971
2025-09-28 05:39:44,892 Stage: Train 0.5 | Epoch: 259 | Iter: 394000 | Total Loss: 0.004688 | Recon Loss: 0.003833 | Commit Loss: 0.001709 | Perplexity: 1850.112621
2025-09-28 05:40:11,489 Stage: Train 0.5 | Epoch: 259 | Iter: 394200 | Total Loss: 0.004725 | Recon Loss: 0.003873 | Commit Loss: 0.001704 | Perplexity: 1851.999329
2025-09-28 05:40:37,986 Stage: Train 0.5 | Epoch: 259 | Iter: 394400 | Total Loss: 0.004663 | Recon Loss: 0.003812 | Commit Loss: 0.001702 | Perplexity: 1848.747951
2025-09-28 05:41:04,671 Stage: Train 0.5 | Epoch: 259 | Iter: 394600 | Total Loss: 0.004727 | Recon Loss: 0.003870 | Commit Loss: 0.001714 | Perplexity: 1852.074459
2025-09-28 05:41:31,300 Stage: Train 0.5 | Epoch: 259 | Iter: 394800 | Total Loss: 0.004723 | Recon Loss: 0.003869 | Commit Loss: 0.001708 | Perplexity: 1850.965531
Trainning Epoch:  79%|███████▉  | 260/330 [22:53:58<3:56:34, 202.78s/it]2025-09-28 05:41:58,139 Stage: Train 0.5 | Epoch: 260 | Iter: 395000 | Total Loss: 0.004695 | Recon Loss: 0.003845 | Commit Loss: 0.001700 | Perplexity: 1848.886907
2025-09-28 05:42:24,904 Stage: Train 0.5 | Epoch: 260 | Iter: 395200 | Total Loss: 0.004725 | Recon Loss: 0.003875 | Commit Loss: 0.001701 | Perplexity: 1850.156520
2025-09-28 05:42:51,556 Stage: Train 0.5 | Epoch: 260 | Iter: 395400 | Total Loss: 0.004692 | Recon Loss: 0.003843 | Commit Loss: 0.001699 | Perplexity: 1848.035842
2025-09-28 05:43:18,181 Stage: Train 0.5 | Epoch: 260 | Iter: 395600 | Total Loss: 0.004719 | Recon Loss: 0.003867 | Commit Loss: 0.001705 | Perplexity: 1850.729494
2025-09-28 05:43:44,930 Stage: Train 0.5 | Epoch: 260 | Iter: 395800 | Total Loss: 0.004687 | Recon Loss: 0.003837 | Commit Loss: 0.001701 | Perplexity: 1847.402078
2025-09-28 05:44:11,685 Stage: Train 0.5 | Epoch: 260 | Iter: 396000 | Total Loss: 0.004697 | Recon Loss: 0.003845 | Commit Loss: 0.001702 | Perplexity: 1847.169760
2025-09-28 05:44:38,420 Stage: Train 0.5 | Epoch: 260 | Iter: 396200 | Total Loss: 0.004750 | Recon Loss: 0.003892 | Commit Loss: 0.001715 | Perplexity: 1854.181431
2025-09-28 05:45:05,119 Stage: Train 0.5 | Epoch: 260 | Iter: 396400 | Total Loss: 0.004701 | Recon Loss: 0.003846 | Commit Loss: 0.001709 | Perplexity: 1851.411617
Trainning Epoch:  79%|███████▉  | 261/330 [22:57:21<3:53:18, 202.88s/it]2025-09-28 05:45:32,064 Stage: Train 0.5 | Epoch: 261 | Iter: 396600 | Total Loss: 0.004694 | Recon Loss: 0.003842 | Commit Loss: 0.001703 | Perplexity: 1851.830947
2025-09-28 05:45:58,756 Stage: Train 0.5 | Epoch: 261 | Iter: 396800 | Total Loss: 0.004692 | Recon Loss: 0.003837 | Commit Loss: 0.001709 | Perplexity: 1850.401981
2025-09-28 05:46:25,429 Stage: Train 0.5 | Epoch: 261 | Iter: 397000 | Total Loss: 0.004706 | Recon Loss: 0.003855 | Commit Loss: 0.001701 | Perplexity: 1851.951541
2025-09-28 05:46:52,366 Stage: Train 0.5 | Epoch: 261 | Iter: 397200 | Total Loss: 0.004755 | Recon Loss: 0.003902 | Commit Loss: 0.001706 | Perplexity: 1852.032673
2025-09-28 05:47:19,040 Stage: Train 0.5 | Epoch: 261 | Iter: 397400 | Total Loss: 0.004676 | Recon Loss: 0.003824 | Commit Loss: 0.001704 | Perplexity: 1846.745077
2025-09-28 05:47:45,727 Stage: Train 0.5 | Epoch: 261 | Iter: 397600 | Total Loss: 0.004753 | Recon Loss: 0.003894 | Commit Loss: 0.001717 | Perplexity: 1855.257393
2025-09-28 05:48:12,471 Stage: Train 0.5 | Epoch: 261 | Iter: 397800 | Total Loss: 0.004696 | Recon Loss: 0.003840 | Commit Loss: 0.001711 | Perplexity: 1848.468090
Trainning Epoch:  79%|███████▉  | 262/330 [23:00:44<3:50:02, 202.98s/it]2025-09-28 05:48:39,432 Stage: Train 0.5 | Epoch: 262 | Iter: 398000 | Total Loss: 0.004734 | Recon Loss: 0.003880 | Commit Loss: 0.001707 | Perplexity: 1849.217375
2025-09-28 05:49:06,104 Stage: Train 0.5 | Epoch: 262 | Iter: 398200 | Total Loss: 0.004708 | Recon Loss: 0.003857 | Commit Loss: 0.001700 | Perplexity: 1850.701959
2025-09-28 05:49:32,602 Stage: Train 0.5 | Epoch: 262 | Iter: 398400 | Total Loss: 0.004660 | Recon Loss: 0.003809 | Commit Loss: 0.001702 | Perplexity: 1852.848088
2025-09-28 05:49:59,196 Stage: Train 0.5 | Epoch: 262 | Iter: 398600 | Total Loss: 0.004675 | Recon Loss: 0.003819 | Commit Loss: 0.001712 | Perplexity: 1854.748459
2025-09-28 05:50:25,877 Stage: Train 0.5 | Epoch: 262 | Iter: 398800 | Total Loss: 0.004695 | Recon Loss: 0.003838 | Commit Loss: 0.001713 | Perplexity: 1855.287410
2025-09-28 05:50:52,536 Stage: Train 0.5 | Epoch: 262 | Iter: 399000 | Total Loss: 0.004761 | Recon Loss: 0.003909 | Commit Loss: 0.001704 | Perplexity: 1849.528139
2025-09-28 05:51:19,042 Stage: Train 0.5 | Epoch: 262 | Iter: 399200 | Total Loss: 0.004714 | Recon Loss: 0.003861 | Commit Loss: 0.001706 | Perplexity: 1852.075830
2025-09-28 05:51:45,685 Stage: Train 0.5 | Epoch: 262 | Iter: 399400 | Total Loss: 0.004669 | Recon Loss: 0.003818 | Commit Loss: 0.001700 | Perplexity: 1846.639769
Trainning Epoch:  80%|███████▉  | 263/330 [23:04:07<3:46:27, 202.80s/it]2025-09-28 05:52:12,586 Stage: Train 0.5 | Epoch: 263 | Iter: 399600 | Total Loss: 0.004701 | Recon Loss: 0.003848 | Commit Loss: 0.001706 | Perplexity: 1849.283849
2025-09-28 05:52:39,136 Stage: Train 0.5 | Epoch: 263 | Iter: 399800 | Total Loss: 0.004714 | Recon Loss: 0.003862 | Commit Loss: 0.001703 | Perplexity: 1847.560242
2025-09-28 05:53:05,873 Stage: Train 0.5 | Epoch: 263 | Iter: 400000 | Total Loss: 0.004657 | Recon Loss: 0.003805 | Commit Loss: 0.001704 | Perplexity: 1854.725475
2025-09-28 05:53:05,874 Saving model at iteration 400000
2025-09-28 05:53:06,106 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000
2025-09-28 05:53:06,615 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/model.safetensors
2025-09-28 05:53:07,178 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/optimizer.bin
2025-09-28 05:53:07,178 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/scheduler.bin
2025-09-28 05:53:07,178 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/sampler.bin
2025-09-28 05:53:07,179 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_264_step_400000/random_states_0.pkl
2025-09-28 05:53:34,175 Stage: Train 0.5 | Epoch: 263 | Iter: 400200 | Total Loss: 0.004696 | Recon Loss: 0.003847 | Commit Loss: 0.001699 | Perplexity: 1847.929698
2025-09-28 05:54:00,897 Stage: Train 0.5 | Epoch: 263 | Iter: 400400 | Total Loss: 0.004718 | Recon Loss: 0.003866 | Commit Loss: 0.001705 | Perplexity: 1853.458368
2025-09-28 05:54:27,660 Stage: Train 0.5 | Epoch: 263 | Iter: 400600 | Total Loss: 0.004736 | Recon Loss: 0.003877 | Commit Loss: 0.001717 | Perplexity: 1854.990847
2025-09-28 05:54:54,336 Stage: Train 0.5 | Epoch: 263 | Iter: 400800 | Total Loss: 0.004708 | Recon Loss: 0.003860 | Commit Loss: 0.001697 | Perplexity: 1849.373040
2025-09-28 05:55:21,045 Stage: Train 0.5 | Epoch: 263 | Iter: 401000 | Total Loss: 0.004696 | Recon Loss: 0.003841 | Commit Loss: 0.001710 | Perplexity: 1853.001465
Trainning Epoch:  80%|████████  | 264/330 [23:07:31<3:43:41, 203.35s/it]2025-09-28 05:55:48,050 Stage: Train 0.5 | Epoch: 264 | Iter: 401200 | Total Loss: 0.004715 | Recon Loss: 0.003864 | Commit Loss: 0.001701 | Perplexity: 1851.859999
2025-09-28 05:56:14,682 Stage: Train 0.5 | Epoch: 264 | Iter: 401400 | Total Loss: 0.004683 | Recon Loss: 0.003834 | Commit Loss: 0.001697 | Perplexity: 1851.451182
2025-09-28 05:56:41,241 Stage: Train 0.5 | Epoch: 264 | Iter: 401600 | Total Loss: 0.004682 | Recon Loss: 0.003828 | Commit Loss: 0.001706 | Perplexity: 1852.314965
2025-09-28 05:57:07,962 Stage: Train 0.5 | Epoch: 264 | Iter: 401800 | Total Loss: 0.004708 | Recon Loss: 0.003857 | Commit Loss: 0.001703 | Perplexity: 1847.656792
2025-09-28 05:57:34,621 Stage: Train 0.5 | Epoch: 264 | Iter: 402000 | Total Loss: 0.004680 | Recon Loss: 0.003825 | Commit Loss: 0.001709 | Perplexity: 1852.868191
2025-09-28 05:58:01,074 Stage: Train 0.5 | Epoch: 264 | Iter: 402200 | Total Loss: 0.004701 | Recon Loss: 0.003849 | Commit Loss: 0.001703 | Perplexity: 1849.697817
2025-09-28 05:58:27,830 Stage: Train 0.5 | Epoch: 264 | Iter: 402400 | Total Loss: 0.004708 | Recon Loss: 0.003854 | Commit Loss: 0.001708 | Perplexity: 1851.878589
Trainning Epoch:  80%|████████  | 265/330 [23:10:54<3:40:05, 203.16s/it]2025-09-28 05:58:54,907 Stage: Train 0.5 | Epoch: 265 | Iter: 402600 | Total Loss: 0.004748 | Recon Loss: 0.003894 | Commit Loss: 0.001708 | Perplexity: 1853.254981
2025-09-28 05:59:21,552 Stage: Train 0.5 | Epoch: 265 | Iter: 402800 | Total Loss: 0.004625 | Recon Loss: 0.003779 | Commit Loss: 0.001691 | Perplexity: 1847.126401
2025-09-28 05:59:48,263 Stage: Train 0.5 | Epoch: 265 | Iter: 403000 | Total Loss: 0.004692 | Recon Loss: 0.003843 | Commit Loss: 0.001697 | Perplexity: 1847.746930
2025-09-28 06:00:14,937 Stage: Train 0.5 | Epoch: 265 | Iter: 403200 | Total Loss: 0.004696 | Recon Loss: 0.003842 | Commit Loss: 0.001708 | Perplexity: 1854.177031
2025-09-28 06:00:41,673 Stage: Train 0.5 | Epoch: 265 | Iter: 403400 | Total Loss: 0.004729 | Recon Loss: 0.003874 | Commit Loss: 0.001710 | Perplexity: 1856.135350
2025-09-28 06:01:08,405 Stage: Train 0.5 | Epoch: 265 | Iter: 403600 | Total Loss: 0.004712 | Recon Loss: 0.003860 | Commit Loss: 0.001703 | Perplexity: 1851.202002
2025-09-28 06:01:35,242 Stage: Train 0.5 | Epoch: 265 | Iter: 403800 | Total Loss: 0.004659 | Recon Loss: 0.003805 | Commit Loss: 0.001710 | Perplexity: 1850.876099
2025-09-28 06:02:02,000 Stage: Train 0.5 | Epoch: 265 | Iter: 404000 | Total Loss: 0.004689 | Recon Loss: 0.003837 | Commit Loss: 0.001704 | Perplexity: 1848.559047
Trainning Epoch:  81%|████████  | 266/330 [23:14:17<3:36:44, 203.19s/it]2025-09-28 06:02:28,986 Stage: Train 0.5 | Epoch: 266 | Iter: 404200 | Total Loss: 0.004668 | Recon Loss: 0.003821 | Commit Loss: 0.001695 | Perplexity: 1847.876304
2025-09-28 06:02:55,685 Stage: Train 0.5 | Epoch: 266 | Iter: 404400 | Total Loss: 0.004664 | Recon Loss: 0.003816 | Commit Loss: 0.001696 | Perplexity: 1849.674292
2025-09-28 06:03:22,354 Stage: Train 0.5 | Epoch: 266 | Iter: 404600 | Total Loss: 0.004678 | Recon Loss: 0.003826 | Commit Loss: 0.001703 | Perplexity: 1849.205131
2025-09-28 06:03:49,016 Stage: Train 0.5 | Epoch: 266 | Iter: 404800 | Total Loss: 0.004683 | Recon Loss: 0.003836 | Commit Loss: 0.001695 | Perplexity: 1847.664676
2025-09-28 06:04:15,584 Stage: Train 0.5 | Epoch: 266 | Iter: 405000 | Total Loss: 0.004726 | Recon Loss: 0.003873 | Commit Loss: 0.001706 | Perplexity: 1851.616726
2025-09-28 06:04:42,185 Stage: Train 0.5 | Epoch: 266 | Iter: 405200 | Total Loss: 0.004689 | Recon Loss: 0.003838 | Commit Loss: 0.001702 | Perplexity: 1845.265438
2025-09-28 06:05:08,863 Stage: Train 0.5 | Epoch: 266 | Iter: 405400 | Total Loss: 0.004757 | Recon Loss: 0.003903 | Commit Loss: 0.001708 | Perplexity: 1848.990953
Trainning Epoch:  81%|████████  | 267/330 [23:17:40<3:33:12, 203.06s/it]2025-09-28 06:05:35,864 Stage: Train 0.5 | Epoch: 267 | Iter: 405600 | Total Loss: 0.004654 | Recon Loss: 0.003804 | Commit Loss: 0.001700 | Perplexity: 1850.238219
2025-09-28 06:06:02,523 Stage: Train 0.5 | Epoch: 267 | Iter: 405800 | Total Loss: 0.004651 | Recon Loss: 0.003802 | Commit Loss: 0.001699 | Perplexity: 1848.882765
2025-09-28 06:06:29,132 Stage: Train 0.5 | Epoch: 267 | Iter: 406000 | Total Loss: 0.004675 | Recon Loss: 0.003826 | Commit Loss: 0.001698 | Perplexity: 1848.343287
2025-09-28 06:06:55,857 Stage: Train 0.5 | Epoch: 267 | Iter: 406200 | Total Loss: 0.004673 | Recon Loss: 0.003826 | Commit Loss: 0.001694 | Perplexity: 1851.327548
2025-09-28 06:07:22,535 Stage: Train 0.5 | Epoch: 267 | Iter: 406400 | Total Loss: 0.004644 | Recon Loss: 0.003794 | Commit Loss: 0.001699 | Perplexity: 1850.728883
2025-09-28 06:07:49,172 Stage: Train 0.5 | Epoch: 267 | Iter: 406600 | Total Loss: 0.004732 | Recon Loss: 0.003881 | Commit Loss: 0.001703 | Perplexity: 1853.615252
2025-09-28 06:08:15,848 Stage: Train 0.5 | Epoch: 267 | Iter: 406800 | Total Loss: 0.004707 | Recon Loss: 0.003853 | Commit Loss: 0.001708 | Perplexity: 1853.898049
2025-09-28 06:08:42,558 Stage: Train 0.5 | Epoch: 267 | Iter: 407000 | Total Loss: 0.004687 | Recon Loss: 0.003837 | Commit Loss: 0.001699 | Perplexity: 1849.010908
Trainning Epoch:  81%|████████  | 268/330 [23:21:03<3:29:47, 203.02s/it]2025-09-28 06:09:09,640 Stage: Train 0.5 | Epoch: 268 | Iter: 407200 | Total Loss: 0.004692 | Recon Loss: 0.003840 | Commit Loss: 0.001704 | Perplexity: 1851.670754
2025-09-28 06:09:36,317 Stage: Train 0.5 | Epoch: 268 | Iter: 407400 | Total Loss: 0.004702 | Recon Loss: 0.003849 | Commit Loss: 0.001706 | Perplexity: 1856.060799
2025-09-28 06:10:03,048 Stage: Train 0.5 | Epoch: 268 | Iter: 407600 | Total Loss: 0.004675 | Recon Loss: 0.003821 | Commit Loss: 0.001706 | Perplexity: 1850.081167
2025-09-28 06:10:29,698 Stage: Train 0.5 | Epoch: 268 | Iter: 407800 | Total Loss: 0.004661 | Recon Loss: 0.003813 | Commit Loss: 0.001697 | Perplexity: 1849.435339
2025-09-28 06:10:56,376 Stage: Train 0.5 | Epoch: 268 | Iter: 408000 | Total Loss: 0.004656 | Recon Loss: 0.003804 | Commit Loss: 0.001704 | Perplexity: 1853.003078
2025-09-28 06:11:23,046 Stage: Train 0.5 | Epoch: 268 | Iter: 408200 | Total Loss: 0.004722 | Recon Loss: 0.003869 | Commit Loss: 0.001705 | Perplexity: 1852.277925
2025-09-28 06:11:49,755 Stage: Train 0.5 | Epoch: 268 | Iter: 408400 | Total Loss: 0.004685 | Recon Loss: 0.003832 | Commit Loss: 0.001706 | Perplexity: 1851.445074
2025-09-28 06:12:16,296 Stage: Train 0.5 | Epoch: 268 | Iter: 408600 | Total Loss: 0.004674 | Recon Loss: 0.003824 | Commit Loss: 0.001699 | Perplexity: 1847.633055
Trainning Epoch:  82%|████████▏ | 269/330 [23:24:26<3:26:21, 202.98s/it]2025-09-28 06:12:43,353 Stage: Train 0.5 | Epoch: 269 | Iter: 408800 | Total Loss: 0.004633 | Recon Loss: 0.003786 | Commit Loss: 0.001693 | Perplexity: 1848.152910
2025-09-28 06:13:10,043 Stage: Train 0.5 | Epoch: 269 | Iter: 409000 | Total Loss: 0.004657 | Recon Loss: 0.003810 | Commit Loss: 0.001694 | Perplexity: 1845.331271
2025-09-28 06:13:36,698 Stage: Train 0.5 | Epoch: 269 | Iter: 409200 | Total Loss: 0.004721 | Recon Loss: 0.003867 | Commit Loss: 0.001708 | Perplexity: 1853.150846
2025-09-28 06:14:03,367 Stage: Train 0.5 | Epoch: 269 | Iter: 409400 | Total Loss: 0.004672 | Recon Loss: 0.003817 | Commit Loss: 0.001709 | Perplexity: 1854.265178
2025-09-28 06:14:29,666 Stage: Train 0.5 | Epoch: 269 | Iter: 409600 | Total Loss: 0.004675 | Recon Loss: 0.003821 | Commit Loss: 0.001709 | Perplexity: 1851.365035
2025-09-28 06:14:55,869 Stage: Train 0.5 | Epoch: 269 | Iter: 409800 | Total Loss: 0.004690 | Recon Loss: 0.003838 | Commit Loss: 0.001704 | Perplexity: 1853.061631
2025-09-28 06:15:22,733 Stage: Train 0.5 | Epoch: 269 | Iter: 410000 | Total Loss: 0.004703 | Recon Loss: 0.003851 | Commit Loss: 0.001705 | Perplexity: 1852.567469
Trainning Epoch:  82%|████████▏ | 270/330 [23:27:48<3:22:45, 202.77s/it]2025-09-28 06:15:49,641 Stage: Train 0.5 | Epoch: 270 | Iter: 410200 | Total Loss: 0.004735 | Recon Loss: 0.003888 | Commit Loss: 0.001695 | Perplexity: 1847.789419
2025-09-28 06:16:16,374 Stage: Train 0.5 | Epoch: 270 | Iter: 410400 | Total Loss: 0.004635 | Recon Loss: 0.003786 | Commit Loss: 0.001699 | Perplexity: 1852.109512
2025-09-28 06:16:43,012 Stage: Train 0.5 | Epoch: 270 | Iter: 410600 | Total Loss: 0.004691 | Recon Loss: 0.003836 | Commit Loss: 0.001710 | Perplexity: 1849.605533
2025-09-28 06:17:09,722 Stage: Train 0.5 | Epoch: 270 | Iter: 410800 | Total Loss: 0.004659 | Recon Loss: 0.003807 | Commit Loss: 0.001703 | Perplexity: 1850.907688
2025-09-28 06:17:36,418 Stage: Train 0.5 | Epoch: 270 | Iter: 411000 | Total Loss: 0.004672 | Recon Loss: 0.003818 | Commit Loss: 0.001709 | Perplexity: 1854.519477
2025-09-28 06:18:03,197 Stage: Train 0.5 | Epoch: 270 | Iter: 411200 | Total Loss: 0.004629 | Recon Loss: 0.003781 | Commit Loss: 0.001695 | Perplexity: 1851.263542
2025-09-28 06:18:29,939 Stage: Train 0.5 | Epoch: 270 | Iter: 411400 | Total Loss: 0.004668 | Recon Loss: 0.003816 | Commit Loss: 0.001703 | Perplexity: 1855.176771
2025-09-28 06:18:56,632 Stage: Train 0.5 | Epoch: 270 | Iter: 411600 | Total Loss: 0.004712 | Recon Loss: 0.003861 | Commit Loss: 0.001702 | Perplexity: 1849.120623
Trainning Epoch:  82%|████████▏ | 271/330 [23:31:11<3:19:29, 202.87s/it]2025-09-28 06:19:23,565 Stage: Train 0.5 | Epoch: 271 | Iter: 411800 | Total Loss: 0.004640 | Recon Loss: 0.003788 | Commit Loss: 0.001704 | Perplexity: 1853.681705
2025-09-28 06:19:50,151 Stage: Train 0.5 | Epoch: 271 | Iter: 412000 | Total Loss: 0.004657 | Recon Loss: 0.003809 | Commit Loss: 0.001697 | Perplexity: 1852.312308
2025-09-28 06:20:16,777 Stage: Train 0.5 | Epoch: 271 | Iter: 412200 | Total Loss: 0.004634 | Recon Loss: 0.003785 | Commit Loss: 0.001699 | Perplexity: 1850.438856
2025-09-28 06:20:43,417 Stage: Train 0.5 | Epoch: 271 | Iter: 412400 | Total Loss: 0.004663 | Recon Loss: 0.003813 | Commit Loss: 0.001701 | Perplexity: 1850.487422
2025-09-28 06:21:10,007 Stage: Train 0.5 | Epoch: 271 | Iter: 412600 | Total Loss: 0.004676 | Recon Loss: 0.003823 | Commit Loss: 0.001706 | Perplexity: 1852.017094
2025-09-28 06:21:36,722 Stage: Train 0.5 | Epoch: 271 | Iter: 412800 | Total Loss: 0.004690 | Recon Loss: 0.003833 | Commit Loss: 0.001713 | Perplexity: 1856.612228
2025-09-28 06:22:03,366 Stage: Train 0.5 | Epoch: 271 | Iter: 413000 | Total Loss: 0.004653 | Recon Loss: 0.003799 | Commit Loss: 0.001709 | Perplexity: 1853.786139
Trainning Epoch:  82%|████████▏ | 272/330 [23:34:34<3:16:01, 202.78s/it]2025-09-28 06:22:30,279 Stage: Train 0.5 | Epoch: 272 | Iter: 413200 | Total Loss: 0.004682 | Recon Loss: 0.003835 | Commit Loss: 0.001694 | Perplexity: 1845.314733
2025-09-28 06:22:56,947 Stage: Train 0.5 | Epoch: 272 | Iter: 413400 | Total Loss: 0.004672 | Recon Loss: 0.003821 | Commit Loss: 0.001702 | Perplexity: 1850.462932
2025-09-28 06:23:23,659 Stage: Train 0.5 | Epoch: 272 | Iter: 413600 | Total Loss: 0.004700 | Recon Loss: 0.003850 | Commit Loss: 0.001701 | Perplexity: 1850.766672
2025-09-28 06:23:50,341 Stage: Train 0.5 | Epoch: 272 | Iter: 413800 | Total Loss: 0.004653 | Recon Loss: 0.003804 | Commit Loss: 0.001698 | Perplexity: 1848.343994
2025-09-28 06:24:17,083 Stage: Train 0.5 | Epoch: 272 | Iter: 414000 | Total Loss: 0.004666 | Recon Loss: 0.003816 | Commit Loss: 0.001699 | Perplexity: 1853.217112
2025-09-28 06:24:43,713 Stage: Train 0.5 | Epoch: 272 | Iter: 414200 | Total Loss: 0.004681 | Recon Loss: 0.003830 | Commit Loss: 0.001702 | Perplexity: 1851.660842
2025-09-28 06:25:10,406 Stage: Train 0.5 | Epoch: 272 | Iter: 414400 | Total Loss: 0.004730 | Recon Loss: 0.003879 | Commit Loss: 0.001701 | Perplexity: 1851.150433
2025-09-28 06:25:37,044 Stage: Train 0.5 | Epoch: 272 | Iter: 414600 | Total Loss: 0.004647 | Recon Loss: 0.003791 | Commit Loss: 0.001712 | Perplexity: 1852.533273
Trainning Epoch:  83%|████████▎ | 273/330 [23:37:57<3:12:41, 202.84s/it]2025-09-28 06:26:04,168 Stage: Train 0.5 | Epoch: 273 | Iter: 414800 | Total Loss: 0.004668 | Recon Loss: 0.003814 | Commit Loss: 0.001708 | Perplexity: 1855.739893
2025-09-28 06:26:30,788 Stage: Train 0.5 | Epoch: 273 | Iter: 415000 | Total Loss: 0.004620 | Recon Loss: 0.003776 | Commit Loss: 0.001688 | Perplexity: 1846.875381
2025-09-28 06:26:57,533 Stage: Train 0.5 | Epoch: 273 | Iter: 415200 | Total Loss: 0.004675 | Recon Loss: 0.003823 | Commit Loss: 0.001704 | Perplexity: 1852.282324
2025-09-28 06:27:24,110 Stage: Train 0.5 | Epoch: 273 | Iter: 415400 | Total Loss: 0.004680 | Recon Loss: 0.003832 | Commit Loss: 0.001697 | Perplexity: 1850.995105
2025-09-28 06:27:50,716 Stage: Train 0.5 | Epoch: 273 | Iter: 415600 | Total Loss: 0.004679 | Recon Loss: 0.003826 | Commit Loss: 0.001707 | Perplexity: 1850.826975
2025-09-28 06:28:17,635 Stage: Train 0.5 | Epoch: 273 | Iter: 415800 | Total Loss: 0.004664 | Recon Loss: 0.003813 | Commit Loss: 0.001702 | Perplexity: 1855.253702
2025-09-28 06:28:44,439 Stage: Train 0.5 | Epoch: 273 | Iter: 416000 | Total Loss: 0.004653 | Recon Loss: 0.003799 | Commit Loss: 0.001709 | Perplexity: 1849.718367
2025-09-28 06:29:11,113 Stage: Train 0.5 | Epoch: 273 | Iter: 416200 | Total Loss: 0.004686 | Recon Loss: 0.003834 | Commit Loss: 0.001706 | Perplexity: 1854.439406
Trainning Epoch:  83%|████████▎ | 274/330 [23:41:20<3:09:25, 202.95s/it]2025-09-28 06:29:38,056 Stage: Train 0.5 | Epoch: 274 | Iter: 416400 | Total Loss: 0.004646 | Recon Loss: 0.003796 | Commit Loss: 0.001699 | Perplexity: 1850.033472
2025-09-28 06:30:04,706 Stage: Train 0.5 | Epoch: 274 | Iter: 416600 | Total Loss: 0.004674 | Recon Loss: 0.003826 | Commit Loss: 0.001697 | Perplexity: 1850.882458
2025-09-28 06:30:31,400 Stage: Train 0.5 | Epoch: 274 | Iter: 416800 | Total Loss: 0.004699 | Recon Loss: 0.003845 | Commit Loss: 0.001709 | Perplexity: 1856.134365
2025-09-28 06:30:58,128 Stage: Train 0.5 | Epoch: 274 | Iter: 417000 | Total Loss: 0.004633 | Recon Loss: 0.003781 | Commit Loss: 0.001704 | Perplexity: 1850.059447
2025-09-28 06:31:24,601 Stage: Train 0.5 | Epoch: 274 | Iter: 417200 | Total Loss: 0.004710 | Recon Loss: 0.003859 | Commit Loss: 0.001701 | Perplexity: 1852.854610
2025-09-28 06:31:51,329 Stage: Train 0.5 | Epoch: 274 | Iter: 417400 | Total Loss: 0.004641 | Recon Loss: 0.003792 | Commit Loss: 0.001697 | Perplexity: 1855.156432
2025-09-28 06:32:17,935 Stage: Train 0.5 | Epoch: 274 | Iter: 417600 | Total Loss: 0.004653 | Recon Loss: 0.003803 | Commit Loss: 0.001701 | Perplexity: 1850.378175
Trainning Epoch:  83%|████████▎ | 275/330 [23:44:43<3:05:58, 202.89s/it]2025-09-28 06:32:44,986 Stage: Train 0.5 | Epoch: 275 | Iter: 417800 | Total Loss: 0.004661 | Recon Loss: 0.003808 | Commit Loss: 0.001706 | Perplexity: 1852.549137
2025-09-28 06:33:11,635 Stage: Train 0.5 | Epoch: 275 | Iter: 418000 | Total Loss: 0.004637 | Recon Loss: 0.003791 | Commit Loss: 0.001691 | Perplexity: 1852.767646
2025-09-28 06:33:38,359 Stage: Train 0.5 | Epoch: 275 | Iter: 418200 | Total Loss: 0.004658 | Recon Loss: 0.003809 | Commit Loss: 0.001699 | Perplexity: 1851.102094
2025-09-28 06:34:04,961 Stage: Train 0.5 | Epoch: 275 | Iter: 418400 | Total Loss: 0.004646 | Recon Loss: 0.003801 | Commit Loss: 0.001691 | Perplexity: 1848.676560
2025-09-28 06:34:31,501 Stage: Train 0.5 | Epoch: 275 | Iter: 418600 | Total Loss: 0.004692 | Recon Loss: 0.003839 | Commit Loss: 0.001706 | Perplexity: 1854.761840
2025-09-28 06:34:58,131 Stage: Train 0.5 | Epoch: 275 | Iter: 418800 | Total Loss: 0.004672 | Recon Loss: 0.003821 | Commit Loss: 0.001703 | Perplexity: 1850.735214
2025-09-28 06:35:24,769 Stage: Train 0.5 | Epoch: 275 | Iter: 419000 | Total Loss: 0.004646 | Recon Loss: 0.003795 | Commit Loss: 0.001703 | Perplexity: 1851.271205
2025-09-28 06:35:51,501 Stage: Train 0.5 | Epoch: 275 | Iter: 419200 | Total Loss: 0.004699 | Recon Loss: 0.003848 | Commit Loss: 0.001703 | Perplexity: 1854.634506
Trainning Epoch:  84%|████████▎ | 276/330 [23:48:06<3:02:33, 202.85s/it]2025-09-28 06:36:18,552 Stage: Train 0.5 | Epoch: 276 | Iter: 419400 | Total Loss: 0.004647 | Recon Loss: 0.003797 | Commit Loss: 0.001699 | Perplexity: 1851.403546
2025-09-28 06:36:45,296 Stage: Train 0.5 | Epoch: 276 | Iter: 419600 | Total Loss: 0.004702 | Recon Loss: 0.003851 | Commit Loss: 0.001703 | Perplexity: 1854.901691
2025-09-28 06:37:11,906 Stage: Train 0.5 | Epoch: 276 | Iter: 419800 | Total Loss: 0.004644 | Recon Loss: 0.003797 | Commit Loss: 0.001693 | Perplexity: 1853.732139
2025-09-28 06:37:38,582 Stage: Train 0.5 | Epoch: 276 | Iter: 420000 | Total Loss: 0.004672 | Recon Loss: 0.003822 | Commit Loss: 0.001700 | Perplexity: 1852.199208
2025-09-28 06:37:38,582 Saving model at iteration 420000
2025-09-28 06:37:39,120 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000
2025-09-28 06:37:39,641 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/model.safetensors
2025-09-28 06:37:40,188 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/optimizer.bin
2025-09-28 06:37:40,188 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/scheduler.bin
2025-09-28 06:37:40,188 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/sampler.bin
2025-09-28 06:37:40,189 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_277_step_420000/random_states_0.pkl
2025-09-28 06:38:06,760 Stage: Train 0.5 | Epoch: 276 | Iter: 420200 | Total Loss: 0.004666 | Recon Loss: 0.003817 | Commit Loss: 0.001697 | Perplexity: 1854.748684
2025-09-28 06:38:33,544 Stage: Train 0.5 | Epoch: 276 | Iter: 420400 | Total Loss: 0.004670 | Recon Loss: 0.003818 | Commit Loss: 0.001705 | Perplexity: 1851.956898
2025-09-28 06:39:00,303 Stage: Train 0.5 | Epoch: 276 | Iter: 420600 | Total Loss: 0.004614 | Recon Loss: 0.003767 | Commit Loss: 0.001694 | Perplexity: 1852.044879
Trainning Epoch:  84%|████████▍ | 277/330 [23:51:30<2:59:39, 203.39s/it]2025-09-28 06:39:27,287 Stage: Train 0.5 | Epoch: 277 | Iter: 420800 | Total Loss: 0.004646 | Recon Loss: 0.003796 | Commit Loss: 0.001701 | Perplexity: 1850.716438
2025-09-28 06:39:53,945 Stage: Train 0.5 | Epoch: 277 | Iter: 421000 | Total Loss: 0.004596 | Recon Loss: 0.003752 | Commit Loss: 0.001688 | Perplexity: 1852.128325
2025-09-28 06:40:20,655 Stage: Train 0.5 | Epoch: 277 | Iter: 421200 | Total Loss: 0.004675 | Recon Loss: 0.003824 | Commit Loss: 0.001703 | Perplexity: 1855.185539
2025-09-28 06:40:47,373 Stage: Train 0.5 | Epoch: 277 | Iter: 421400 | Total Loss: 0.004677 | Recon Loss: 0.003829 | Commit Loss: 0.001696 | Perplexity: 1849.805588
2025-09-28 06:41:14,045 Stage: Train 0.5 | Epoch: 277 | Iter: 421600 | Total Loss: 0.004610 | Recon Loss: 0.003761 | Commit Loss: 0.001698 | Perplexity: 1855.591934
2025-09-28 06:41:40,650 Stage: Train 0.5 | Epoch: 277 | Iter: 421800 | Total Loss: 0.004700 | Recon Loss: 0.003853 | Commit Loss: 0.001693 | Perplexity: 1855.003419
2025-09-28 06:42:07,279 Stage: Train 0.5 | Epoch: 277 | Iter: 422000 | Total Loss: 0.004595 | Recon Loss: 0.003743 | Commit Loss: 0.001703 | Perplexity: 1855.806957
2025-09-28 06:42:34,077 Stage: Train 0.5 | Epoch: 277 | Iter: 422200 | Total Loss: 0.004673 | Recon Loss: 0.003824 | Commit Loss: 0.001698 | Perplexity: 1853.147038
Trainning Epoch:  84%|████████▍ | 278/330 [23:54:53<2:56:10, 203.28s/it]2025-09-28 06:43:01,080 Stage: Train 0.5 | Epoch: 278 | Iter: 422400 | Total Loss: 0.004660 | Recon Loss: 0.003810 | Commit Loss: 0.001700 | Perplexity: 1852.667394
2025-09-28 06:43:27,847 Stage: Train 0.5 | Epoch: 278 | Iter: 422600 | Total Loss: 0.004591 | Recon Loss: 0.003740 | Commit Loss: 0.001701 | Perplexity: 1856.113557
2025-09-28 06:43:54,484 Stage: Train 0.5 | Epoch: 278 | Iter: 422800 | Total Loss: 0.004689 | Recon Loss: 0.003836 | Commit Loss: 0.001706 | Perplexity: 1852.537604
2025-09-28 06:44:21,247 Stage: Train 0.5 | Epoch: 278 | Iter: 423000 | Total Loss: 0.004623 | Recon Loss: 0.003776 | Commit Loss: 0.001695 | Perplexity: 1852.897409
2025-09-28 06:44:47,969 Stage: Train 0.5 | Epoch: 278 | Iter: 423200 | Total Loss: 0.004688 | Recon Loss: 0.003839 | Commit Loss: 0.001699 | Perplexity: 1852.956462
2025-09-28 06:45:14,775 Stage: Train 0.5 | Epoch: 278 | Iter: 423400 | Total Loss: 0.004636 | Recon Loss: 0.003791 | Commit Loss: 0.001691 | Perplexity: 1853.240418
2025-09-28 06:45:41,428 Stage: Train 0.5 | Epoch: 278 | Iter: 423600 | Total Loss: 0.004668 | Recon Loss: 0.003818 | Commit Loss: 0.001701 | Perplexity: 1854.405447
2025-09-28 06:46:08,026 Stage: Train 0.5 | Epoch: 278 | Iter: 423800 | Total Loss: 0.004644 | Recon Loss: 0.003797 | Commit Loss: 0.001695 | Perplexity: 1850.979119
Trainning Epoch:  85%|████████▍ | 279/330 [23:58:16<2:52:44, 203.22s/it]2025-09-28 06:46:35,129 Stage: Train 0.5 | Epoch: 279 | Iter: 424000 | Total Loss: 0.004634 | Recon Loss: 0.003788 | Commit Loss: 0.001693 | Perplexity: 1855.982872
2025-09-28 06:47:01,842 Stage: Train 0.5 | Epoch: 279 | Iter: 424200 | Total Loss: 0.004610 | Recon Loss: 0.003764 | Commit Loss: 0.001692 | Perplexity: 1852.935385
2025-09-28 06:47:28,456 Stage: Train 0.5 | Epoch: 279 | Iter: 424400 | Total Loss: 0.004591 | Recon Loss: 0.003743 | Commit Loss: 0.001697 | Perplexity: 1850.733410
2025-09-28 06:47:55,237 Stage: Train 0.5 | Epoch: 279 | Iter: 424600 | Total Loss: 0.004643 | Recon Loss: 0.003793 | Commit Loss: 0.001701 | Perplexity: 1848.215734
2025-09-28 06:48:21,576 Stage: Train 0.5 | Epoch: 279 | Iter: 424800 | Total Loss: 0.004681 | Recon Loss: 0.003831 | Commit Loss: 0.001699 | Perplexity: 1853.713847
2025-09-28 06:48:48,369 Stage: Train 0.5 | Epoch: 279 | Iter: 425000 | Total Loss: 0.004653 | Recon Loss: 0.003802 | Commit Loss: 0.001703 | Perplexity: 1856.502253
2025-09-28 06:49:15,180 Stage: Train 0.5 | Epoch: 279 | Iter: 425200 | Total Loss: 0.004616 | Recon Loss: 0.003768 | Commit Loss: 0.001697 | Perplexity: 1854.456773
Trainning Epoch:  85%|████████▍ | 280/330 [24:01:39<2:49:18, 203.17s/it]2025-09-28 06:49:42,160 Stage: Train 0.5 | Epoch: 280 | Iter: 425400 | Total Loss: 0.004678 | Recon Loss: 0.003829 | Commit Loss: 0.001699 | Perplexity: 1857.029194
2025-09-28 06:50:08,852 Stage: Train 0.5 | Epoch: 280 | Iter: 425600 | Total Loss: 0.004617 | Recon Loss: 0.003764 | Commit Loss: 0.001704 | Perplexity: 1856.041995
2025-09-28 06:50:35,515 Stage: Train 0.5 | Epoch: 280 | Iter: 425800 | Total Loss: 0.004605 | Recon Loss: 0.003754 | Commit Loss: 0.001702 | Perplexity: 1856.121900
2025-09-28 06:51:02,204 Stage: Train 0.5 | Epoch: 280 | Iter: 426000 | Total Loss: 0.004638 | Recon Loss: 0.003791 | Commit Loss: 0.001694 | Perplexity: 1852.922037
2025-09-28 06:51:28,868 Stage: Train 0.5 | Epoch: 280 | Iter: 426200 | Total Loss: 0.004655 | Recon Loss: 0.003806 | Commit Loss: 0.001697 | Perplexity: 1855.830685
2025-09-28 06:51:55,424 Stage: Train 0.5 | Epoch: 280 | Iter: 426400 | Total Loss: 0.004605 | Recon Loss: 0.003758 | Commit Loss: 0.001694 | Perplexity: 1849.126707
2025-09-28 06:52:21,946 Stage: Train 0.5 | Epoch: 280 | Iter: 426600 | Total Loss: 0.004622 | Recon Loss: 0.003772 | Commit Loss: 0.001699 | Perplexity: 1853.554205
2025-09-28 06:52:48,673 Stage: Train 0.5 | Epoch: 280 | Iter: 426800 | Total Loss: 0.004677 | Recon Loss: 0.003824 | Commit Loss: 0.001706 | Perplexity: 1857.159673
Trainning Epoch:  85%|████████▌ | 281/330 [24:05:02<2:45:47, 203.02s/it]2025-09-28 06:53:15,588 Stage: Train 0.5 | Epoch: 281 | Iter: 427000 | Total Loss: 0.004625 | Recon Loss: 0.003777 | Commit Loss: 0.001696 | Perplexity: 1854.023610
2025-09-28 06:53:42,321 Stage: Train 0.5 | Epoch: 281 | Iter: 427200 | Total Loss: 0.004632 | Recon Loss: 0.003787 | Commit Loss: 0.001690 | Perplexity: 1853.696263
2025-09-28 06:54:09,021 Stage: Train 0.5 | Epoch: 281 | Iter: 427400 | Total Loss: 0.004633 | Recon Loss: 0.003779 | Commit Loss: 0.001707 | Perplexity: 1857.772239
2025-09-28 06:54:35,686 Stage: Train 0.5 | Epoch: 281 | Iter: 427600 | Total Loss: 0.004630 | Recon Loss: 0.003782 | Commit Loss: 0.001697 | Perplexity: 1854.186927
2025-09-28 06:55:02,352 Stage: Train 0.5 | Epoch: 281 | Iter: 427800 | Total Loss: 0.004646 | Recon Loss: 0.003797 | Commit Loss: 0.001699 | Perplexity: 1855.341183
2025-09-28 06:55:28,962 Stage: Train 0.5 | Epoch: 281 | Iter: 428000 | Total Loss: 0.004624 | Recon Loss: 0.003775 | Commit Loss: 0.001698 | Perplexity: 1851.532076
2025-09-28 06:55:55,556 Stage: Train 0.5 | Epoch: 281 | Iter: 428200 | Total Loss: 0.004661 | Recon Loss: 0.003811 | Commit Loss: 0.001699 | Perplexity: 1856.982234
Trainning Epoch:  85%|████████▌ | 282/330 [24:08:25<2:42:20, 202.93s/it]2025-09-28 06:56:22,480 Stage: Train 0.5 | Epoch: 282 | Iter: 428400 | Total Loss: 0.004642 | Recon Loss: 0.003783 | Commit Loss: 0.001717 | Perplexity: 1859.496948
2025-09-28 06:56:49,216 Stage: Train 0.5 | Epoch: 282 | Iter: 428600 | Total Loss: 0.004664 | Recon Loss: 0.003817 | Commit Loss: 0.001694 | Perplexity: 1857.984984
2025-09-28 06:57:15,790 Stage: Train 0.5 | Epoch: 282 | Iter: 428800 | Total Loss: 0.004621 | Recon Loss: 0.003774 | Commit Loss: 0.001694 | Perplexity: 1853.208338
2025-09-28 06:57:42,490 Stage: Train 0.5 | Epoch: 282 | Iter: 429000 | Total Loss: 0.004630 | Recon Loss: 0.003782 | Commit Loss: 0.001696 | Perplexity: 1853.565399
2025-09-28 06:58:09,136 Stage: Train 0.5 | Epoch: 282 | Iter: 429200 | Total Loss: 0.004588 | Recon Loss: 0.003742 | Commit Loss: 0.001694 | Perplexity: 1856.046985
2025-09-28 06:58:35,728 Stage: Train 0.5 | Epoch: 282 | Iter: 429400 | Total Loss: 0.004610 | Recon Loss: 0.003760 | Commit Loss: 0.001700 | Perplexity: 1854.660793
2025-09-28 06:59:02,613 Stage: Train 0.5 | Epoch: 282 | Iter: 429600 | Total Loss: 0.004657 | Recon Loss: 0.003806 | Commit Loss: 0.001702 | Perplexity: 1853.966152
2025-09-28 06:59:29,255 Stage: Train 0.5 | Epoch: 282 | Iter: 429800 | Total Loss: 0.004605 | Recon Loss: 0.003754 | Commit Loss: 0.001701 | Perplexity: 1855.333502
Trainning Epoch:  86%|████████▌ | 283/330 [24:11:48<2:38:57, 202.94s/it]2025-09-28 06:59:56,292 Stage: Train 0.5 | Epoch: 283 | Iter: 430000 | Total Loss: 0.004628 | Recon Loss: 0.003779 | Commit Loss: 0.001698 | Perplexity: 1853.156767
2025-09-28 07:00:22,909 Stage: Train 0.5 | Epoch: 283 | Iter: 430200 | Total Loss: 0.004595 | Recon Loss: 0.003748 | Commit Loss: 0.001695 | Perplexity: 1856.841163
2025-09-28 07:00:49,629 Stage: Train 0.5 | Epoch: 283 | Iter: 430400 | Total Loss: 0.004586 | Recon Loss: 0.003741 | Commit Loss: 0.001690 | Perplexity: 1850.846264
2025-09-28 07:01:16,342 Stage: Train 0.5 | Epoch: 283 | Iter: 430600 | Total Loss: 0.004644 | Recon Loss: 0.003793 | Commit Loss: 0.001702 | Perplexity: 1859.198270
2025-09-28 07:01:42,867 Stage: Train 0.5 | Epoch: 283 | Iter: 430800 | Total Loss: 0.004624 | Recon Loss: 0.003776 | Commit Loss: 0.001696 | Perplexity: 1851.176766
2025-09-28 07:02:09,315 Stage: Train 0.5 | Epoch: 283 | Iter: 431000 | Total Loss: 0.004667 | Recon Loss: 0.003821 | Commit Loss: 0.001692 | Perplexity: 1851.332338
2025-09-28 07:02:35,662 Stage: Train 0.5 | Epoch: 283 | Iter: 431200 | Total Loss: 0.004602 | Recon Loss: 0.003753 | Commit Loss: 0.001699 | Perplexity: 1855.939705
Trainning Epoch:  86%|████████▌ | 284/330 [24:15:10<2:35:25, 202.73s/it]2025-09-28 07:03:02,602 Stage: Train 0.5 | Epoch: 284 | Iter: 431400 | Total Loss: 0.004654 | Recon Loss: 0.003802 | Commit Loss: 0.001703 | Perplexity: 1853.261141
2025-09-28 07:03:29,228 Stage: Train 0.5 | Epoch: 284 | Iter: 431600 | Total Loss: 0.004593 | Recon Loss: 0.003747 | Commit Loss: 0.001692 | Perplexity: 1852.383424
2025-09-28 07:03:55,692 Stage: Train 0.5 | Epoch: 284 | Iter: 431800 | Total Loss: 0.004638 | Recon Loss: 0.003791 | Commit Loss: 0.001694 | Perplexity: 1851.952907
2025-09-28 07:04:22,405 Stage: Train 0.5 | Epoch: 284 | Iter: 432000 | Total Loss: 0.004644 | Recon Loss: 0.003796 | Commit Loss: 0.001697 | Perplexity: 1856.916357
2025-09-28 07:04:48,987 Stage: Train 0.5 | Epoch: 284 | Iter: 432200 | Total Loss: 0.004606 | Recon Loss: 0.003763 | Commit Loss: 0.001687 | Perplexity: 1851.617284
2025-09-28 07:05:15,588 Stage: Train 0.5 | Epoch: 284 | Iter: 432400 | Total Loss: 0.004613 | Recon Loss: 0.003763 | Commit Loss: 0.001701 | Perplexity: 1856.081645
2025-09-28 07:05:42,229 Stage: Train 0.5 | Epoch: 284 | Iter: 432600 | Total Loss: 0.004640 | Recon Loss: 0.003793 | Commit Loss: 0.001695 | Perplexity: 1852.524883
2025-09-28 07:06:08,905 Stage: Train 0.5 | Epoch: 284 | Iter: 432800 | Total Loss: 0.004606 | Recon Loss: 0.003755 | Commit Loss: 0.001701 | Perplexity: 1852.476847
Trainning Epoch:  86%|████████▋ | 285/330 [24:18:32<2:31:58, 202.64s/it]2025-09-28 07:06:35,917 Stage: Train 0.5 | Epoch: 285 | Iter: 433000 | Total Loss: 0.004626 | Recon Loss: 0.003777 | Commit Loss: 0.001696 | Perplexity: 1852.637723
2025-09-28 07:07:02,532 Stage: Train 0.5 | Epoch: 285 | Iter: 433200 | Total Loss: 0.004587 | Recon Loss: 0.003742 | Commit Loss: 0.001688 | Perplexity: 1856.377730
2025-09-28 07:07:29,214 Stage: Train 0.5 | Epoch: 285 | Iter: 433400 | Total Loss: 0.004608 | Recon Loss: 0.003761 | Commit Loss: 0.001695 | Perplexity: 1855.966747
2025-09-28 07:07:55,914 Stage: Train 0.5 | Epoch: 285 | Iter: 433600 | Total Loss: 0.004604 | Recon Loss: 0.003760 | Commit Loss: 0.001688 | Perplexity: 1853.743167
2025-09-28 07:08:22,449 Stage: Train 0.5 | Epoch: 285 | Iter: 433800 | Total Loss: 0.004629 | Recon Loss: 0.003785 | Commit Loss: 0.001687 | Perplexity: 1851.261205
2025-09-28 07:08:49,058 Stage: Train 0.5 | Epoch: 285 | Iter: 434000 | Total Loss: 0.004682 | Recon Loss: 0.003813 | Commit Loss: 0.001739 | Perplexity: 1854.330089
2025-09-28 07:09:15,800 Stage: Train 0.5 | Epoch: 285 | Iter: 434200 | Total Loss: 0.004646 | Recon Loss: 0.003782 | Commit Loss: 0.001729 | Perplexity: 1858.180468
2025-09-28 07:09:42,457 Stage: Train 0.5 | Epoch: 285 | Iter: 434400 | Total Loss: 0.004650 | Recon Loss: 0.003800 | Commit Loss: 0.001701 | Perplexity: 1853.971999
Trainning Epoch:  87%|████████▋ | 286/330 [24:21:55<2:28:37, 202.68s/it]2025-09-28 07:10:09,375 Stage: Train 0.5 | Epoch: 286 | Iter: 434600 | Total Loss: 0.004621 | Recon Loss: 0.003775 | Commit Loss: 0.001692 | Perplexity: 1857.069470
2025-09-28 07:10:36,027 Stage: Train 0.5 | Epoch: 286 | Iter: 434800 | Total Loss: 0.004642 | Recon Loss: 0.003795 | Commit Loss: 0.001694 | Perplexity: 1856.781508
2025-09-28 07:11:02,576 Stage: Train 0.5 | Epoch: 286 | Iter: 435000 | Total Loss: 0.004642 | Recon Loss: 0.003792 | Commit Loss: 0.001699 | Perplexity: 1856.348094
2025-09-28 07:11:29,329 Stage: Train 0.5 | Epoch: 286 | Iter: 435200 | Total Loss: 0.004556 | Recon Loss: 0.003709 | Commit Loss: 0.001693 | Perplexity: 1857.481933
2025-09-28 07:11:55,933 Stage: Train 0.5 | Epoch: 286 | Iter: 435400 | Total Loss: 0.004616 | Recon Loss: 0.003771 | Commit Loss: 0.001689 | Perplexity: 1854.385664
2025-09-28 07:12:22,420 Stage: Train 0.5 | Epoch: 286 | Iter: 435600 | Total Loss: 0.004617 | Recon Loss: 0.003773 | Commit Loss: 0.001686 | Perplexity: 1854.418157
2025-09-28 07:12:49,093 Stage: Train 0.5 | Epoch: 286 | Iter: 435800 | Total Loss: 0.004627 | Recon Loss: 0.003785 | Commit Loss: 0.001684 | Perplexity: 1850.151671
Trainning Epoch:  87%|████████▋ | 287/330 [24:25:18<2:25:12, 202.61s/it]2025-09-28 07:13:15,928 Stage: Train 0.5 | Epoch: 287 | Iter: 436000 | Total Loss: 0.004627 | Recon Loss: 0.003781 | Commit Loss: 0.001693 | Perplexity: 1852.345606
2025-09-28 07:13:42,570 Stage: Train 0.5 | Epoch: 287 | Iter: 436200 | Total Loss: 0.004640 | Recon Loss: 0.003792 | Commit Loss: 0.001695 | Perplexity: 1858.971594
2025-09-28 07:14:09,184 Stage: Train 0.5 | Epoch: 287 | Iter: 436400 | Total Loss: 0.004622 | Recon Loss: 0.003776 | Commit Loss: 0.001691 | Perplexity: 1856.706075
2025-09-28 07:14:35,912 Stage: Train 0.5 | Epoch: 287 | Iter: 436600 | Total Loss: 0.004615 | Recon Loss: 0.003770 | Commit Loss: 0.001689 | Perplexity: 1855.360436
2025-09-28 07:15:02,693 Stage: Train 0.5 | Epoch: 287 | Iter: 436800 | Total Loss: 0.004582 | Recon Loss: 0.003742 | Commit Loss: 0.001681 | Perplexity: 1853.076649
2025-09-28 07:15:29,353 Stage: Train 0.5 | Epoch: 287 | Iter: 437000 | Total Loss: 0.004613 | Recon Loss: 0.003766 | Commit Loss: 0.001693 | Perplexity: 1852.860790
2025-09-28 07:15:56,006 Stage: Train 0.5 | Epoch: 287 | Iter: 437200 | Total Loss: 0.004610 | Recon Loss: 0.003763 | Commit Loss: 0.001695 | Perplexity: 1855.485773
2025-09-28 07:16:22,544 Stage: Train 0.5 | Epoch: 287 | Iter: 437400 | Total Loss: 0.004656 | Recon Loss: 0.003804 | Commit Loss: 0.001704 | Perplexity: 1855.912834
Trainning Epoch:  87%|████████▋ | 288/330 [24:28:40<2:21:51, 202.65s/it]2025-09-28 07:16:49,522 Stage: Train 0.5 | Epoch: 288 | Iter: 437600 | Total Loss: 0.004626 | Recon Loss: 0.003780 | Commit Loss: 0.001692 | Perplexity: 1856.550611
2025-09-28 07:17:16,186 Stage: Train 0.5 | Epoch: 288 | Iter: 437800 | Total Loss: 0.004614 | Recon Loss: 0.003775 | Commit Loss: 0.001679 | Perplexity: 1854.254208
2025-09-28 07:17:42,835 Stage: Train 0.5 | Epoch: 288 | Iter: 438000 | Total Loss: 0.004620 | Recon Loss: 0.003775 | Commit Loss: 0.001690 | Perplexity: 1855.024905
2025-09-28 07:18:09,302 Stage: Train 0.5 | Epoch: 288 | Iter: 438200 | Total Loss: 0.004633 | Recon Loss: 0.003789 | Commit Loss: 0.001688 | Perplexity: 1856.436589
2025-09-28 07:18:35,890 Stage: Train 0.5 | Epoch: 288 | Iter: 438400 | Total Loss: 0.004588 | Recon Loss: 0.003739 | Commit Loss: 0.001697 | Perplexity: 1857.029918
2025-09-28 07:19:02,519 Stage: Train 0.5 | Epoch: 288 | Iter: 438600 | Total Loss: 0.004593 | Recon Loss: 0.003746 | Commit Loss: 0.001695 | Perplexity: 1853.740854
2025-09-28 07:19:29,049 Stage: Train 0.5 | Epoch: 288 | Iter: 438800 | Total Loss: 0.004601 | Recon Loss: 0.003757 | Commit Loss: 0.001688 | Perplexity: 1853.006776
Trainning Epoch:  88%|████████▊ | 289/330 [24:32:03<2:18:24, 202.55s/it]2025-09-28 07:19:55,969 Stage: Train 0.5 | Epoch: 289 | Iter: 439000 | Total Loss: 0.004583 | Recon Loss: 0.003736 | Commit Loss: 0.001694 | Perplexity: 1853.868344
2025-09-28 07:20:22,738 Stage: Train 0.5 | Epoch: 289 | Iter: 439200 | Total Loss: 0.004610 | Recon Loss: 0.003766 | Commit Loss: 0.001688 | Perplexity: 1858.778071
2025-09-28 07:20:49,401 Stage: Train 0.5 | Epoch: 289 | Iter: 439400 | Total Loss: 0.004626 | Recon Loss: 0.003783 | Commit Loss: 0.001687 | Perplexity: 1856.275851
2025-09-28 07:21:16,058 Stage: Train 0.5 | Epoch: 289 | Iter: 439600 | Total Loss: 0.004530 | Recon Loss: 0.003685 | Commit Loss: 0.001689 | Perplexity: 1857.380366
2025-09-28 07:21:42,726 Stage: Train 0.5 | Epoch: 289 | Iter: 439800 | Total Loss: 0.004613 | Recon Loss: 0.003769 | Commit Loss: 0.001690 | Perplexity: 1856.508446
2025-09-28 07:22:09,422 Stage: Train 0.5 | Epoch: 289 | Iter: 440000 | Total Loss: 0.004619 | Recon Loss: 0.003780 | Commit Loss: 0.001680 | Perplexity: 1853.543542
2025-09-28 07:22:09,422 Saving model at iteration 440000
2025-09-28 07:22:09,928 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000
2025-09-28 07:22:10,457 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/model.safetensors
2025-09-28 07:22:10,992 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/optimizer.bin
2025-09-28 07:22:10,992 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/scheduler.bin
2025-09-28 07:22:10,993 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/sampler.bin
2025-09-28 07:22:10,994 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_290_step_440000/random_states_0.pkl
2025-09-28 07:22:37,641 Stage: Train 0.5 | Epoch: 289 | Iter: 440200 | Total Loss: 0.004561 | Recon Loss: 0.003718 | Commit Loss: 0.001687 | Perplexity: 1851.339941
2025-09-28 07:23:04,341 Stage: Train 0.5 | Epoch: 289 | Iter: 440400 | Total Loss: 0.004596 | Recon Loss: 0.003746 | Commit Loss: 0.001700 | Perplexity: 1856.476942
Trainning Epoch:  88%|████████▊ | 290/330 [24:35:27<2:15:27, 203.18s/it]2025-09-28 07:23:31,603 Stage: Train 0.5 | Epoch: 290 | Iter: 440600 | Total Loss: 0.004613 | Recon Loss: 0.003759 | Commit Loss: 0.001708 | Perplexity: 1853.839989
2025-09-28 07:23:58,275 Stage: Train 0.5 | Epoch: 290 | Iter: 440800 | Total Loss: 0.004589 | Recon Loss: 0.003746 | Commit Loss: 0.001686 | Perplexity: 1859.064106
2025-09-28 07:24:24,995 Stage: Train 0.5 | Epoch: 290 | Iter: 441000 | Total Loss: 0.004643 | Recon Loss: 0.003782 | Commit Loss: 0.001721 | Perplexity: 1856.369849
2025-09-28 07:24:51,757 Stage: Train 0.5 | Epoch: 290 | Iter: 441200 | Total Loss: 0.004609 | Recon Loss: 0.003767 | Commit Loss: 0.001683 | Perplexity: 1857.339175
2025-09-28 07:25:18,514 Stage: Train 0.5 | Epoch: 290 | Iter: 441400 | Total Loss: 0.004598 | Recon Loss: 0.003756 | Commit Loss: 0.001683 | Perplexity: 1857.154391
2025-09-28 07:25:45,178 Stage: Train 0.5 | Epoch: 290 | Iter: 441600 | Total Loss: 0.004580 | Recon Loss: 0.003737 | Commit Loss: 0.001684 | Perplexity: 1853.167145
2025-09-28 07:26:11,904 Stage: Train 0.5 | Epoch: 290 | Iter: 441800 | Total Loss: 0.004614 | Recon Loss: 0.003765 | Commit Loss: 0.001699 | Perplexity: 1857.391908
2025-09-28 07:26:38,562 Stage: Train 0.5 | Epoch: 290 | Iter: 442000 | Total Loss: 0.004607 | Recon Loss: 0.003764 | Commit Loss: 0.001687 | Perplexity: 1858.517841
Trainning Epoch:  88%|████████▊ | 291/330 [24:38:51<2:12:05, 203.21s/it]2025-09-28 07:27:05,552 Stage: Train 0.5 | Epoch: 291 | Iter: 442200 | Total Loss: 0.004573 | Recon Loss: 0.003729 | Commit Loss: 0.001687 | Perplexity: 1856.232522
2025-09-28 07:27:32,322 Stage: Train 0.5 | Epoch: 291 | Iter: 442400 | Total Loss: 0.004633 | Recon Loss: 0.003789 | Commit Loss: 0.001690 | Perplexity: 1860.527363
2025-09-28 07:27:59,016 Stage: Train 0.5 | Epoch: 291 | Iter: 442600 | Total Loss: 0.004562 | Recon Loss: 0.003724 | Commit Loss: 0.001676 | Perplexity: 1856.121505
2025-09-28 07:28:25,754 Stage: Train 0.5 | Epoch: 291 | Iter: 442800 | Total Loss: 0.004615 | Recon Loss: 0.003770 | Commit Loss: 0.001689 | Perplexity: 1857.390730
2025-09-28 07:28:52,466 Stage: Train 0.5 | Epoch: 291 | Iter: 443000 | Total Loss: 0.004601 | Recon Loss: 0.003758 | Commit Loss: 0.001686 | Perplexity: 1856.924946
2025-09-28 07:29:19,037 Stage: Train 0.5 | Epoch: 291 | Iter: 443200 | Total Loss: 0.004583 | Recon Loss: 0.003740 | Commit Loss: 0.001684 | Perplexity: 1851.384802
2025-09-28 07:29:45,605 Stage: Train 0.5 | Epoch: 291 | Iter: 443400 | Total Loss: 0.004599 | Recon Loss: 0.003757 | Commit Loss: 0.001683 | Perplexity: 1852.286969
Trainning Epoch:  88%|████████▊ | 292/330 [24:42:14<2:08:39, 203.16s/it]2025-09-28 07:30:12,696 Stage: Train 0.5 | Epoch: 292 | Iter: 443600 | Total Loss: 0.004647 | Recon Loss: 0.003798 | Commit Loss: 0.001698 | Perplexity: 1850.744982
2025-09-28 07:30:39,376 Stage: Train 0.5 | Epoch: 292 | Iter: 443800 | Total Loss: 0.004562 | Recon Loss: 0.003726 | Commit Loss: 0.001671 | Perplexity: 1855.718401
2025-09-28 07:31:06,048 Stage: Train 0.5 | Epoch: 292 | Iter: 444000 | Total Loss: 0.004570 | Recon Loss: 0.003736 | Commit Loss: 0.001669 | Perplexity: 1852.918703
2025-09-28 07:31:32,689 Stage: Train 0.5 | Epoch: 292 | Iter: 444200 | Total Loss: 0.004757 | Recon Loss: 0.003865 | Commit Loss: 0.001784 | Perplexity: 1857.581068
2025-09-28 07:31:59,380 Stage: Train 0.5 | Epoch: 292 | Iter: 444400 | Total Loss: 0.004555 | Recon Loss: 0.003709 | Commit Loss: 0.001692 | Perplexity: 1854.575517
2025-09-28 07:32:26,001 Stage: Train 0.5 | Epoch: 292 | Iter: 444600 | Total Loss: 0.004582 | Recon Loss: 0.003735 | Commit Loss: 0.001693 | Perplexity: 1855.783362
2025-09-28 07:32:52,760 Stage: Train 0.5 | Epoch: 292 | Iter: 444800 | Total Loss: 0.004615 | Recon Loss: 0.003769 | Commit Loss: 0.001691 | Perplexity: 1855.853920
2025-09-28 07:33:19,514 Stage: Train 0.5 | Epoch: 292 | Iter: 445000 | Total Loss: 0.004572 | Recon Loss: 0.003728 | Commit Loss: 0.001688 | Perplexity: 1855.435577
Trainning Epoch:  89%|████████▉ | 293/330 [24:45:37<2:05:15, 203.11s/it]2025-09-28 07:33:46,453 Stage: Train 0.5 | Epoch: 293 | Iter: 445200 | Total Loss: 0.004576 | Recon Loss: 0.003739 | Commit Loss: 0.001675 | Perplexity: 1856.206908
2025-09-28 07:34:13,103 Stage: Train 0.5 | Epoch: 293 | Iter: 445400 | Total Loss: 0.004554 | Recon Loss: 0.003717 | Commit Loss: 0.001674 | Perplexity: 1855.849421
2025-09-28 07:34:39,888 Stage: Train 0.5 | Epoch: 293 | Iter: 445600 | Total Loss: 0.004559 | Recon Loss: 0.003719 | Commit Loss: 0.001681 | Perplexity: 1855.772180
2025-09-28 07:35:06,612 Stage: Train 0.5 | Epoch: 293 | Iter: 445800 | Total Loss: 0.004566 | Recon Loss: 0.003728 | Commit Loss: 0.001677 | Perplexity: 1856.476455
2025-09-28 07:35:33,265 Stage: Train 0.5 | Epoch: 293 | Iter: 446000 | Total Loss: 0.004631 | Recon Loss: 0.003790 | Commit Loss: 0.001683 | Perplexity: 1855.804999
2025-09-28 07:35:59,947 Stage: Train 0.5 | Epoch: 293 | Iter: 446200 | Total Loss: 0.004562 | Recon Loss: 0.003726 | Commit Loss: 0.001672 | Perplexity: 1852.064807
2025-09-28 07:36:26,782 Stage: Train 0.5 | Epoch: 293 | Iter: 446400 | Total Loss: 0.004581 | Recon Loss: 0.003743 | Commit Loss: 0.001676 | Perplexity: 1852.479714
Trainning Epoch:  89%|████████▉ | 294/330 [24:49:00<2:01:53, 203.16s/it]2025-09-28 07:36:53,911 Stage: Train 0.5 | Epoch: 294 | Iter: 446600 | Total Loss: 0.004603 | Recon Loss: 0.003748 | Commit Loss: 0.001709 | Perplexity: 1855.053563
2025-09-28 07:37:20,640 Stage: Train 0.5 | Epoch: 294 | Iter: 446800 | Total Loss: 0.004544 | Recon Loss: 0.003711 | Commit Loss: 0.001667 | Perplexity: 1851.475826
2025-09-28 07:37:47,321 Stage: Train 0.5 | Epoch: 294 | Iter: 447000 | Total Loss: 0.004559 | Recon Loss: 0.003716 | Commit Loss: 0.001686 | Perplexity: 1854.618417
2025-09-28 07:38:13,964 Stage: Train 0.5 | Epoch: 294 | Iter: 447200 | Total Loss: 0.004554 | Recon Loss: 0.003714 | Commit Loss: 0.001680 | Perplexity: 1855.561335
2025-09-28 07:38:40,617 Stage: Train 0.5 | Epoch: 294 | Iter: 447400 | Total Loss: 0.004632 | Recon Loss: 0.003790 | Commit Loss: 0.001683 | Perplexity: 1857.235344
2025-09-28 07:39:07,373 Stage: Train 0.5 | Epoch: 294 | Iter: 447600 | Total Loss: 0.004590 | Recon Loss: 0.003744 | Commit Loss: 0.001691 | Perplexity: 1852.068654
2025-09-28 07:39:34,032 Stage: Train 0.5 | Epoch: 294 | Iter: 447800 | Total Loss: 0.004570 | Recon Loss: 0.003732 | Commit Loss: 0.001676 | Perplexity: 1854.913148
2025-09-28 07:40:00,563 Stage: Train 0.5 | Epoch: 294 | Iter: 448000 | Total Loss: 0.004604 | Recon Loss: 0.003762 | Commit Loss: 0.001684 | Perplexity: 1856.473643
Trainning Epoch:  89%|████████▉ | 295/330 [24:52:23<1:58:28, 203.11s/it]2025-09-28 07:40:27,730 Stage: Train 0.5 | Epoch: 295 | Iter: 448200 | Total Loss: 0.004566 | Recon Loss: 0.003728 | Commit Loss: 0.001677 | Perplexity: 1856.230332
2025-09-28 07:40:54,424 Stage: Train 0.5 | Epoch: 295 | Iter: 448400 | Total Loss: 0.004598 | Recon Loss: 0.003761 | Commit Loss: 0.001674 | Perplexity: 1854.220274
2025-09-28 07:41:21,066 Stage: Train 0.5 | Epoch: 295 | Iter: 448600 | Total Loss: 0.004535 | Recon Loss: 0.003702 | Commit Loss: 0.001667 | Perplexity: 1851.278761
2025-09-28 07:41:47,656 Stage: Train 0.5 | Epoch: 295 | Iter: 448800 | Total Loss: 0.004546 | Recon Loss: 0.003708 | Commit Loss: 0.001677 | Perplexity: 1855.708248
2025-09-28 07:42:14,275 Stage: Train 0.5 | Epoch: 295 | Iter: 449000 | Total Loss: 0.004580 | Recon Loss: 0.003746 | Commit Loss: 0.001669 | Perplexity: 1851.536489
2025-09-28 07:42:40,877 Stage: Train 0.5 | Epoch: 295 | Iter: 449200 | Total Loss: 0.004888 | Recon Loss: 0.003949 | Commit Loss: 0.001879 | Perplexity: 1861.643329
2025-09-28 07:43:07,518 Stage: Train 0.5 | Epoch: 295 | Iter: 449400 | Total Loss: 0.004728 | Recon Loss: 0.003824 | Commit Loss: 0.001808 | Perplexity: 1861.080806
2025-09-28 07:43:34,138 Stage: Train 0.5 | Epoch: 295 | Iter: 449600 | Total Loss: 0.004655 | Recon Loss: 0.003777 | Commit Loss: 0.001756 | Perplexity: 1854.416115
Trainning Epoch:  90%|████████▉ | 296/330 [24:55:46<1:55:00, 202.96s/it]2025-09-28 07:44:01,048 Stage: Train 0.5 | Epoch: 296 | Iter: 449800 | Total Loss: 0.004619 | Recon Loss: 0.003754 | Commit Loss: 0.001731 | Perplexity: 1854.951273
2025-09-28 07:44:27,664 Stage: Train 0.5 | Epoch: 296 | Iter: 450000 | Total Loss: 0.004571 | Recon Loss: 0.003723 | Commit Loss: 0.001696 | Perplexity: 1855.510047
2025-09-28 07:44:54,273 Stage: Train 0.5 | Epoch: 296 | Iter: 450200 | Total Loss: 0.004621 | Recon Loss: 0.003774 | Commit Loss: 0.001695 | Perplexity: 1860.834343
2025-09-28 07:45:20,839 Stage: Train 0.5 | Epoch: 296 | Iter: 450400 | Total Loss: 0.004581 | Recon Loss: 0.003731 | Commit Loss: 0.001701 | Perplexity: 1854.121968
2025-09-28 07:45:47,478 Stage: Train 0.5 | Epoch: 296 | Iter: 450600 | Total Loss: 0.004548 | Recon Loss: 0.003709 | Commit Loss: 0.001679 | Perplexity: 1854.070552
2025-09-28 07:46:14,109 Stage: Train 0.5 | Epoch: 296 | Iter: 450800 | Total Loss: 0.004598 | Recon Loss: 0.003756 | Commit Loss: 0.001683 | Perplexity: 1854.656299
2025-09-28 07:46:40,734 Stage: Train 0.5 | Epoch: 296 | Iter: 451000 | Total Loss: 0.004573 | Recon Loss: 0.003732 | Commit Loss: 0.001682 | Perplexity: 1859.967469
Trainning Epoch:  90%|█████████ | 297/330 [24:59:08<1:51:32, 202.81s/it]2025-09-28 07:47:07,680 Stage: Train 0.5 | Epoch: 297 | Iter: 451200 | Total Loss: 0.004558 | Recon Loss: 0.003723 | Commit Loss: 0.001670 | Perplexity: 1849.508690
2025-09-28 07:47:34,208 Stage: Train 0.5 | Epoch: 297 | Iter: 451400 | Total Loss: 0.004562 | Recon Loss: 0.003722 | Commit Loss: 0.001681 | Perplexity: 1854.474828
2025-09-28 07:48:00,876 Stage: Train 0.5 | Epoch: 297 | Iter: 451600 | Total Loss: 0.004596 | Recon Loss: 0.003761 | Commit Loss: 0.001670 | Perplexity: 1855.573089
2025-09-28 07:48:27,500 Stage: Train 0.5 | Epoch: 297 | Iter: 451800 | Total Loss: 0.004533 | Recon Loss: 0.003696 | Commit Loss: 0.001673 | Perplexity: 1857.329928
2025-09-28 07:48:54,087 Stage: Train 0.5 | Epoch: 297 | Iter: 452000 | Total Loss: 0.004588 | Recon Loss: 0.003753 | Commit Loss: 0.001670 | Perplexity: 1855.209814
2025-09-28 07:49:20,683 Stage: Train 0.5 | Epoch: 297 | Iter: 452200 | Total Loss: 0.004527 | Recon Loss: 0.003697 | Commit Loss: 0.001661 | Perplexity: 1854.390479
2025-09-28 07:49:47,292 Stage: Train 0.5 | Epoch: 297 | Iter: 452400 | Total Loss: 0.004591 | Recon Loss: 0.003754 | Commit Loss: 0.001674 | Perplexity: 1857.025899
2025-09-28 07:50:13,851 Stage: Train 0.5 | Epoch: 297 | Iter: 452600 | Total Loss: 0.004577 | Recon Loss: 0.003743 | Commit Loss: 0.001669 | Perplexity: 1858.023587
Trainning Epoch:  90%|█████████ | 298/330 [25:02:30<1:48:03, 202.62s/it]2025-09-28 07:50:40,763 Stage: Train 0.5 | Epoch: 298 | Iter: 452800 | Total Loss: 0.004487 | Recon Loss: 0.003652 | Commit Loss: 0.001669 | Perplexity: 1855.626572
2025-09-28 07:51:07,492 Stage: Train 0.5 | Epoch: 298 | Iter: 453000 | Total Loss: 0.004580 | Recon Loss: 0.003743 | Commit Loss: 0.001675 | Perplexity: 1857.396552
2025-09-28 07:51:34,210 Stage: Train 0.5 | Epoch: 298 | Iter: 453200 | Total Loss: 0.004526 | Recon Loss: 0.003694 | Commit Loss: 0.001662 | Perplexity: 1853.460983
2025-09-28 07:52:00,817 Stage: Train 0.5 | Epoch: 298 | Iter: 453400 | Total Loss: 0.004567 | Recon Loss: 0.003739 | Commit Loss: 0.001656 | Perplexity: 1849.788925
2025-09-28 07:52:27,279 Stage: Train 0.5 | Epoch: 298 | Iter: 453600 | Total Loss: 0.004584 | Recon Loss: 0.003745 | Commit Loss: 0.001676 | Perplexity: 1857.340696
2025-09-28 07:52:53,815 Stage: Train 0.5 | Epoch: 298 | Iter: 453800 | Total Loss: 0.004549 | Recon Loss: 0.003718 | Commit Loss: 0.001662 | Perplexity: 1854.544167
2025-09-28 07:53:20,522 Stage: Train 0.5 | Epoch: 298 | Iter: 454000 | Total Loss: 0.004521 | Recon Loss: 0.003685 | Commit Loss: 0.001671 | Perplexity: 1856.356276
Trainning Epoch:  91%|█████████ | 299/330 [25:05:53<1:44:41, 202.63s/it]2025-09-28 07:53:47,448 Stage: Train 0.5 | Epoch: 299 | Iter: 454200 | Total Loss: 0.004584 | Recon Loss: 0.003749 | Commit Loss: 0.001671 | Perplexity: 1856.242434
2025-09-28 07:54:14,186 Stage: Train 0.5 | Epoch: 299 | Iter: 454400 | Total Loss: 0.004502 | Recon Loss: 0.003676 | Commit Loss: 0.001652 | Perplexity: 1853.503565
2025-09-28 07:54:40,828 Stage: Train 0.5 | Epoch: 299 | Iter: 454600 | Total Loss: 0.004540 | Recon Loss: 0.003710 | Commit Loss: 0.001660 | Perplexity: 1854.548590
2025-09-28 07:55:07,636 Stage: Train 0.5 | Epoch: 299 | Iter: 454800 | Total Loss: 0.004574 | Recon Loss: 0.003742 | Commit Loss: 0.001664 | Perplexity: 1855.875173
2025-09-28 07:55:34,321 Stage: Train 0.5 | Epoch: 299 | Iter: 455000 | Total Loss: 0.004566 | Recon Loss: 0.003727 | Commit Loss: 0.001679 | Perplexity: 1854.031592
2025-09-28 07:56:00,872 Stage: Train 0.5 | Epoch: 299 | Iter: 455200 | Total Loss: 0.004662 | Recon Loss: 0.003779 | Commit Loss: 0.001766 | Perplexity: 1859.796838
2025-09-28 07:56:27,553 Stage: Train 0.5 | Epoch: 299 | Iter: 455400 | Total Loss: 0.004565 | Recon Loss: 0.003730 | Commit Loss: 0.001670 | Perplexity: 1854.680218
2025-09-28 07:56:54,210 Stage: Train 0.5 | Epoch: 299 | Iter: 455600 | Total Loss: 0.004536 | Recon Loss: 0.003698 | Commit Loss: 0.001677 | Perplexity: 1857.034684
Trainning Epoch:  91%|█████████ | 300/330 [25:09:16<1:41:21, 202.72s/it]2025-09-28 07:57:21,134 Stage: Train 0.5 | Epoch: 300 | Iter: 455800 | Total Loss: 0.004518 | Recon Loss: 0.003691 | Commit Loss: 0.001654 | Perplexity: 1855.156451
2025-09-28 07:57:47,756 Stage: Train 0.5 | Epoch: 300 | Iter: 456000 | Total Loss: 0.004532 | Recon Loss: 0.003703 | Commit Loss: 0.001658 | Perplexity: 1859.213443
2025-09-28 07:58:14,557 Stage: Train 0.5 | Epoch: 300 | Iter: 456200 | Total Loss: 0.004533 | Recon Loss: 0.003695 | Commit Loss: 0.001676 | Perplexity: 1852.406779
2025-09-28 07:58:41,267 Stage: Train 0.5 | Epoch: 300 | Iter: 456400 | Total Loss: 0.004518 | Recon Loss: 0.003689 | Commit Loss: 0.001657 | Perplexity: 1853.361793
2025-09-28 07:59:07,929 Stage: Train 0.5 | Epoch: 300 | Iter: 456600 | Total Loss: 0.004567 | Recon Loss: 0.003728 | Commit Loss: 0.001677 | Perplexity: 1859.574981
2025-09-28 07:59:34,779 Stage: Train 0.5 | Epoch: 300 | Iter: 456800 | Total Loss: 0.004568 | Recon Loss: 0.003728 | Commit Loss: 0.001679 | Perplexity: 1856.746194
2025-09-28 08:00:01,413 Stage: Train 0.5 | Epoch: 300 | Iter: 457000 | Total Loss: 0.004592 | Recon Loss: 0.003730 | Commit Loss: 0.001724 | Perplexity: 1861.131019
2025-09-28 08:00:28,046 Stage: Train 0.5 | Epoch: 300 | Iter: 457200 | Total Loss: 0.004565 | Recon Loss: 0.003736 | Commit Loss: 0.001657 | Perplexity: 1855.761995
Trainning Epoch:  91%|█████████ | 301/330 [25:12:39<1:38:01, 202.80s/it]2025-09-28 08:00:55,013 Stage: Train 0.5 | Epoch: 301 | Iter: 457400 | Total Loss: 0.004517 | Recon Loss: 0.003693 | Commit Loss: 0.001649 | Perplexity: 1854.114993
2025-09-28 08:01:21,588 Stage: Train 0.5 | Epoch: 301 | Iter: 457600 | Total Loss: 0.004536 | Recon Loss: 0.003705 | Commit Loss: 0.001661 | Perplexity: 1855.765905
2025-09-28 08:01:48,120 Stage: Train 0.5 | Epoch: 301 | Iter: 457800 | Total Loss: 0.004560 | Recon Loss: 0.003732 | Commit Loss: 0.001657 | Perplexity: 1857.210676
2025-09-28 08:02:14,620 Stage: Train 0.5 | Epoch: 301 | Iter: 458000 | Total Loss: 0.004513 | Recon Loss: 0.003680 | Commit Loss: 0.001666 | Perplexity: 1854.765057
2025-09-28 08:02:41,282 Stage: Train 0.5 | Epoch: 301 | Iter: 458200 | Total Loss: 0.004563 | Recon Loss: 0.003739 | Commit Loss: 0.001649 | Perplexity: 1853.190067
2025-09-28 08:03:07,910 Stage: Train 0.5 | Epoch: 301 | Iter: 458400 | Total Loss: 0.004600 | Recon Loss: 0.003751 | Commit Loss: 0.001697 | Perplexity: 1857.812489
2025-09-28 08:03:34,645 Stage: Train 0.5 | Epoch: 301 | Iter: 458600 | Total Loss: 0.004516 | Recon Loss: 0.003688 | Commit Loss: 0.001657 | Perplexity: 1852.474109
Trainning Epoch:  92%|█████████▏| 302/330 [25:16:01<1:34:36, 202.73s/it]2025-09-28 08:04:01,703 Stage: Train 0.5 | Epoch: 302 | Iter: 458800 | Total Loss: 0.004545 | Recon Loss: 0.003706 | Commit Loss: 0.001677 | Perplexity: 1857.087522
2025-09-28 08:04:28,341 Stage: Train 0.5 | Epoch: 302 | Iter: 459000 | Total Loss: 0.004503 | Recon Loss: 0.003679 | Commit Loss: 0.001649 | Perplexity: 1856.038428
2025-09-28 08:04:55,031 Stage: Train 0.5 | Epoch: 302 | Iter: 459200 | Total Loss: 0.004573 | Recon Loss: 0.003732 | Commit Loss: 0.001684 | Perplexity: 1859.626023
2025-09-28 08:05:21,636 Stage: Train 0.5 | Epoch: 302 | Iter: 459400 | Total Loss: 0.004565 | Recon Loss: 0.003735 | Commit Loss: 0.001658 | Perplexity: 1854.768053
2025-09-28 08:05:48,389 Stage: Train 0.5 | Epoch: 302 | Iter: 459600 | Total Loss: 0.004542 | Recon Loss: 0.003716 | Commit Loss: 0.001652 | Perplexity: 1852.202487
2025-09-28 08:06:15,075 Stage: Train 0.5 | Epoch: 302 | Iter: 459800 | Total Loss: 0.004551 | Recon Loss: 0.003727 | Commit Loss: 0.001647 | Perplexity: 1859.989712
2025-09-28 08:06:41,686 Stage: Train 0.5 | Epoch: 302 | Iter: 460000 | Total Loss: 0.004623 | Recon Loss: 0.003760 | Commit Loss: 0.001725 | Perplexity: 1856.679612
2025-09-28 08:06:41,686 Saving model at iteration 460000
2025-09-28 08:06:41,935 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000
2025-09-28 08:06:42,523 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/model.safetensors
2025-09-28 08:06:43,062 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/optimizer.bin
2025-09-28 08:06:43,063 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/scheduler.bin
2025-09-28 08:06:43,063 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/sampler.bin
2025-09-28 08:06:43,064 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_303_step_460000/random_states_0.pkl
2025-09-28 08:07:10,179 Stage: Train 0.5 | Epoch: 302 | Iter: 460200 | Total Loss: 0.004527 | Recon Loss: 0.003706 | Commit Loss: 0.001643 | Perplexity: 1858.566237
Trainning Epoch:  92%|█████████▏| 303/330 [25:19:26<1:31:29, 203.32s/it]2025-09-28 08:07:37,232 Stage: Train 0.5 | Epoch: 303 | Iter: 460400 | Total Loss: 0.004517 | Recon Loss: 0.003690 | Commit Loss: 0.001654 | Perplexity: 1853.572507
2025-09-28 08:08:03,890 Stage: Train 0.5 | Epoch: 303 | Iter: 460600 | Total Loss: 0.004579 | Recon Loss: 0.003755 | Commit Loss: 0.001648 | Perplexity: 1860.154357
2025-09-28 08:08:30,611 Stage: Train 0.5 | Epoch: 303 | Iter: 460800 | Total Loss: 0.004516 | Recon Loss: 0.003691 | Commit Loss: 0.001651 | Perplexity: 1857.246953
2025-09-28 08:08:57,371 Stage: Train 0.5 | Epoch: 303 | Iter: 461000 | Total Loss: 0.004532 | Recon Loss: 0.003707 | Commit Loss: 0.001649 | Perplexity: 1857.282089
2025-09-28 08:09:23,972 Stage: Train 0.5 | Epoch: 303 | Iter: 461200 | Total Loss: 0.004546 | Recon Loss: 0.003717 | Commit Loss: 0.001659 | Perplexity: 1851.812338
2025-09-28 08:09:50,721 Stage: Train 0.5 | Epoch: 303 | Iter: 461400 | Total Loss: 0.004501 | Recon Loss: 0.003678 | Commit Loss: 0.001645 | Perplexity: 1856.615104
2025-09-28 08:10:17,471 Stage: Train 0.5 | Epoch: 303 | Iter: 461600 | Total Loss: 0.004467 | Recon Loss: 0.003643 | Commit Loss: 0.001647 | Perplexity: 1854.416913
Trainning Epoch:  92%|█████████▏| 304/330 [25:22:49<1:28:05, 203.27s/it]2025-09-28 08:10:44,454 Stage: Train 0.5 | Epoch: 304 | Iter: 461800 | Total Loss: 0.004505 | Recon Loss: 0.003675 | Commit Loss: 0.001660 | Perplexity: 1856.335555
2025-09-28 08:11:11,147 Stage: Train 0.5 | Epoch: 304 | Iter: 462000 | Total Loss: 0.004585 | Recon Loss: 0.003720 | Commit Loss: 0.001730 | Perplexity: 1857.976406
2025-09-28 08:11:37,839 Stage: Train 0.5 | Epoch: 304 | Iter: 462200 | Total Loss: 0.004503 | Recon Loss: 0.003679 | Commit Loss: 0.001647 | Perplexity: 1855.020975
2025-09-28 08:12:04,492 Stage: Train 0.5 | Epoch: 304 | Iter: 462400 | Total Loss: 0.004517 | Recon Loss: 0.003690 | Commit Loss: 0.001653 | Perplexity: 1858.101594
2025-09-28 08:12:31,202 Stage: Train 0.5 | Epoch: 304 | Iter: 462600 | Total Loss: 0.004537 | Recon Loss: 0.003707 | Commit Loss: 0.001660 | Perplexity: 1854.666478
2025-09-28 08:12:57,912 Stage: Train 0.5 | Epoch: 304 | Iter: 462800 | Total Loss: 0.004612 | Recon Loss: 0.003725 | Commit Loss: 0.001774 | Perplexity: 1857.942529
2025-09-28 08:13:24,644 Stage: Train 0.5 | Epoch: 304 | Iter: 463000 | Total Loss: 0.004516 | Recon Loss: 0.003685 | Commit Loss: 0.001661 | Perplexity: 1857.990787
2025-09-28 08:13:51,358 Stage: Train 0.5 | Epoch: 304 | Iter: 463200 | Total Loss: 0.004504 | Recon Loss: 0.003678 | Commit Loss: 0.001653 | Perplexity: 1855.952257
Trainning Epoch:  92%|█████████▏| 305/330 [25:26:12<1:24:40, 203.22s/it]2025-09-28 08:14:18,272 Stage: Train 0.5 | Epoch: 305 | Iter: 463400 | Total Loss: 0.004548 | Recon Loss: 0.003720 | Commit Loss: 0.001657 | Perplexity: 1859.195350
2025-09-28 08:14:44,873 Stage: Train 0.5 | Epoch: 305 | Iter: 463600 | Total Loss: 0.004479 | Recon Loss: 0.003657 | Commit Loss: 0.001644 | Perplexity: 1855.438912
2025-09-28 08:15:11,651 Stage: Train 0.5 | Epoch: 305 | Iter: 463800 | Total Loss: 0.004540 | Recon Loss: 0.003713 | Commit Loss: 0.001655 | Perplexity: 1855.184555
2025-09-28 08:15:38,288 Stage: Train 0.5 | Epoch: 305 | Iter: 464000 | Total Loss: 0.004579 | Recon Loss: 0.003730 | Commit Loss: 0.001698 | Perplexity: 1857.718594
2025-09-28 08:16:05,044 Stage: Train 0.5 | Epoch: 305 | Iter: 464200 | Total Loss: 0.004485 | Recon Loss: 0.003662 | Commit Loss: 0.001645 | Perplexity: 1851.503862
2025-09-28 08:16:31,722 Stage: Train 0.5 | Epoch: 305 | Iter: 464400 | Total Loss: 0.004509 | Recon Loss: 0.003680 | Commit Loss: 0.001658 | Perplexity: 1857.354648
2025-09-28 08:16:58,356 Stage: Train 0.5 | Epoch: 305 | Iter: 464600 | Total Loss: 0.004571 | Recon Loss: 0.003724 | Commit Loss: 0.001694 | Perplexity: 1858.362313
2025-09-28 08:17:25,030 Stage: Train 0.5 | Epoch: 305 | Iter: 464800 | Total Loss: 0.004490 | Recon Loss: 0.003660 | Commit Loss: 0.001661 | Perplexity: 1857.397365
Trainning Epoch:  93%|█████████▎| 306/330 [25:29:35<1:21:14, 203.10s/it]2025-09-28 08:17:52,077 Stage: Train 0.5 | Epoch: 306 | Iter: 465000 | Total Loss: 0.004490 | Recon Loss: 0.003672 | Commit Loss: 0.001636 | Perplexity: 1853.846126
2025-09-28 08:18:18,772 Stage: Train 0.5 | Epoch: 306 | Iter: 465200 | Total Loss: 0.004493 | Recon Loss: 0.003666 | Commit Loss: 0.001654 | Perplexity: 1859.607584
2025-09-28 08:18:45,364 Stage: Train 0.5 | Epoch: 306 | Iter: 465400 | Total Loss: 0.004515 | Recon Loss: 0.003683 | Commit Loss: 0.001664 | Perplexity: 1859.256337
2025-09-28 08:19:11,952 Stage: Train 0.5 | Epoch: 306 | Iter: 465600 | Total Loss: 0.004520 | Recon Loss: 0.003686 | Commit Loss: 0.001667 | Perplexity: 1857.150040
2025-09-28 08:19:38,596 Stage: Train 0.5 | Epoch: 306 | Iter: 465800 | Total Loss: 0.004519 | Recon Loss: 0.003694 | Commit Loss: 0.001649 | Perplexity: 1855.858453
2025-09-28 08:20:05,258 Stage: Train 0.5 | Epoch: 306 | Iter: 466000 | Total Loss: 0.004509 | Recon Loss: 0.003679 | Commit Loss: 0.001660 | Perplexity: 1853.232181
2025-09-28 08:20:31,919 Stage: Train 0.5 | Epoch: 306 | Iter: 466200 | Total Loss: 0.004491 | Recon Loss: 0.003661 | Commit Loss: 0.001659 | Perplexity: 1856.708069
Trainning Epoch:  93%|█████████▎| 307/330 [25:32:58<1:17:49, 203.01s/it]2025-09-28 08:20:58,914 Stage: Train 0.5 | Epoch: 307 | Iter: 466400 | Total Loss: 0.004582 | Recon Loss: 0.003738 | Commit Loss: 0.001688 | Perplexity: 1852.660378
2025-09-28 08:21:25,476 Stage: Train 0.5 | Epoch: 307 | Iter: 466600 | Total Loss: 0.004480 | Recon Loss: 0.003663 | Commit Loss: 0.001634 | Perplexity: 1854.270085
2025-09-28 08:21:52,415 Stage: Train 0.5 | Epoch: 307 | Iter: 466800 | Total Loss: 0.004516 | Recon Loss: 0.003693 | Commit Loss: 0.001646 | Perplexity: 1859.748647
2025-09-28 08:22:19,072 Stage: Train 0.5 | Epoch: 307 | Iter: 467000 | Total Loss: 0.004596 | Recon Loss: 0.003750 | Commit Loss: 0.001692 | Perplexity: 1856.615856
2025-09-28 08:22:45,442 Stage: Train 0.5 | Epoch: 307 | Iter: 467200 | Total Loss: 0.004532 | Recon Loss: 0.003709 | Commit Loss: 0.001648 | Perplexity: 1856.008024
2025-09-28 08:23:12,133 Stage: Train 0.5 | Epoch: 307 | Iter: 467400 | Total Loss: 0.004510 | Recon Loss: 0.003688 | Commit Loss: 0.001644 | Perplexity: 1857.483546
2025-09-28 08:23:38,877 Stage: Train 0.5 | Epoch: 307 | Iter: 467600 | Total Loss: 0.004496 | Recon Loss: 0.003670 | Commit Loss: 0.001650 | Perplexity: 1856.856329
2025-09-28 08:24:05,482 Stage: Train 0.5 | Epoch: 307 | Iter: 467800 | Total Loss: 0.004446 | Recon Loss: 0.003618 | Commit Loss: 0.001657 | Perplexity: 1856.127573
Trainning Epoch:  93%|█████████▎| 308/330 [25:36:21<1:14:24, 202.91s/it]2025-09-28 08:24:32,410 Stage: Train 0.5 | Epoch: 308 | Iter: 468000 | Total Loss: 0.004525 | Recon Loss: 0.003700 | Commit Loss: 0.001650 | Perplexity: 1855.476122
2025-09-28 08:24:59,092 Stage: Train 0.5 | Epoch: 308 | Iter: 468200 | Total Loss: 0.004481 | Recon Loss: 0.003662 | Commit Loss: 0.001637 | Perplexity: 1856.115067
2025-09-28 08:25:25,694 Stage: Train 0.5 | Epoch: 308 | Iter: 468400 | Total Loss: 0.004505 | Recon Loss: 0.003675 | Commit Loss: 0.001659 | Perplexity: 1858.361004
2025-09-28 08:25:52,342 Stage: Train 0.5 | Epoch: 308 | Iter: 468600 | Total Loss: 0.004529 | Recon Loss: 0.003711 | Commit Loss: 0.001636 | Perplexity: 1855.210558
2025-09-28 08:26:19,087 Stage: Train 0.5 | Epoch: 308 | Iter: 468800 | Total Loss: 0.004469 | Recon Loss: 0.003648 | Commit Loss: 0.001641 | Perplexity: 1856.967087
2025-09-28 08:26:45,810 Stage: Train 0.5 | Epoch: 308 | Iter: 469000 | Total Loss: 0.004488 | Recon Loss: 0.003668 | Commit Loss: 0.001640 | Perplexity: 1859.045616
2025-09-28 08:27:12,454 Stage: Train 0.5 | Epoch: 308 | Iter: 469200 | Total Loss: 0.004540 | Recon Loss: 0.003700 | Commit Loss: 0.001680 | Perplexity: 1855.852799
Trainning Epoch:  94%|█████████▎| 309/330 [25:39:43<1:11:00, 202.90s/it]2025-09-28 08:27:39,452 Stage: Train 0.5 | Epoch: 309 | Iter: 469400 | Total Loss: 0.004586 | Recon Loss: 0.003705 | Commit Loss: 0.001763 | Perplexity: 1859.105595
2025-09-28 08:28:06,130 Stage: Train 0.5 | Epoch: 309 | Iter: 469600 | Total Loss: 0.004530 | Recon Loss: 0.003696 | Commit Loss: 0.001669 | Perplexity: 1854.921120
2025-09-28 08:28:32,745 Stage: Train 0.5 | Epoch: 309 | Iter: 469800 | Total Loss: 0.004522 | Recon Loss: 0.003692 | Commit Loss: 0.001659 | Perplexity: 1864.093821
2025-09-28 08:28:59,365 Stage: Train 0.5 | Epoch: 309 | Iter: 470000 | Total Loss: 0.004511 | Recon Loss: 0.003687 | Commit Loss: 0.001648 | Perplexity: 1858.629487
2025-09-28 08:29:25,993 Stage: Train 0.5 | Epoch: 309 | Iter: 470200 | Total Loss: 0.004499 | Recon Loss: 0.003683 | Commit Loss: 0.001634 | Perplexity: 1852.085211
2025-09-28 08:29:52,701 Stage: Train 0.5 | Epoch: 309 | Iter: 470400 | Total Loss: 0.004544 | Recon Loss: 0.003724 | Commit Loss: 0.001639 | Perplexity: 1854.080730
2025-09-28 08:30:19,323 Stage: Train 0.5 | Epoch: 309 | Iter: 470600 | Total Loss: 0.004467 | Recon Loss: 0.003645 | Commit Loss: 0.001642 | Perplexity: 1853.630531
2025-09-28 08:30:45,928 Stage: Train 0.5 | Epoch: 309 | Iter: 470800 | Total Loss: 0.004564 | Recon Loss: 0.003726 | Commit Loss: 0.001675 | Perplexity: 1854.980908
Trainning Epoch:  94%|█████████▍| 310/330 [25:43:06<1:07:36, 202.84s/it]2025-09-28 08:31:12,908 Stage: Train 0.5 | Epoch: 310 | Iter: 471000 | Total Loss: 0.004509 | Recon Loss: 0.003695 | Commit Loss: 0.001626 | Perplexity: 1852.023767
2025-09-28 08:31:39,601 Stage: Train 0.5 | Epoch: 310 | Iter: 471200 | Total Loss: 0.004522 | Recon Loss: 0.003701 | Commit Loss: 0.001643 | Perplexity: 1857.840244
2025-09-28 08:32:06,521 Stage: Train 0.5 | Epoch: 310 | Iter: 471400 | Total Loss: 0.004469 | Recon Loss: 0.003645 | Commit Loss: 0.001647 | Perplexity: 1858.275241
2025-09-28 08:32:33,140 Stage: Train 0.5 | Epoch: 310 | Iter: 471600 | Total Loss: 0.004521 | Recon Loss: 0.003693 | Commit Loss: 0.001655 | Perplexity: 1860.078992
2025-09-28 08:32:59,832 Stage: Train 0.5 | Epoch: 310 | Iter: 471800 | Total Loss: 0.004487 | Recon Loss: 0.003662 | Commit Loss: 0.001651 | Perplexity: 1854.562920
2025-09-28 08:33:26,519 Stage: Train 0.5 | Epoch: 310 | Iter: 472000 | Total Loss: 0.004495 | Recon Loss: 0.003672 | Commit Loss: 0.001645 | Perplexity: 1854.624569
2025-09-28 08:33:53,111 Stage: Train 0.5 | Epoch: 310 | Iter: 472200 | Total Loss: 0.004466 | Recon Loss: 0.003647 | Commit Loss: 0.001638 | Perplexity: 1854.431422
2025-09-28 08:34:19,888 Stage: Train 0.5 | Epoch: 310 | Iter: 472400 | Total Loss: 0.004719 | Recon Loss: 0.003773 | Commit Loss: 0.001893 | Perplexity: 1860.011263
Trainning Epoch:  94%|█████████▍| 311/330 [25:46:29<1:04:15, 202.93s/it]2025-09-28 08:34:46,891 Stage: Train 0.5 | Epoch: 311 | Iter: 472600 | Total Loss: 0.004438 | Recon Loss: 0.003617 | Commit Loss: 0.001643 | Perplexity: 1854.859844
2025-09-28 08:35:13,563 Stage: Train 0.5 | Epoch: 311 | Iter: 472800 | Total Loss: 0.004507 | Recon Loss: 0.003685 | Commit Loss: 0.001645 | Perplexity: 1855.267889
2025-09-28 08:35:40,181 Stage: Train 0.5 | Epoch: 311 | Iter: 473000 | Total Loss: 0.004478 | Recon Loss: 0.003656 | Commit Loss: 0.001643 | Perplexity: 1855.829448
2025-09-28 08:36:06,862 Stage: Train 0.5 | Epoch: 311 | Iter: 473200 | Total Loss: 0.004483 | Recon Loss: 0.003663 | Commit Loss: 0.001641 | Perplexity: 1852.550124
2025-09-28 08:36:33,593 Stage: Train 0.5 | Epoch: 311 | Iter: 473400 | Total Loss: 0.004510 | Recon Loss: 0.003695 | Commit Loss: 0.001631 | Perplexity: 1857.050193
2025-09-28 08:37:00,220 Stage: Train 0.5 | Epoch: 311 | Iter: 473600 | Total Loss: 0.004470 | Recon Loss: 0.003644 | Commit Loss: 0.001652 | Perplexity: 1861.237156
2025-09-28 08:37:26,491 Stage: Train 0.5 | Epoch: 311 | Iter: 473800 | Total Loss: 0.004552 | Recon Loss: 0.003715 | Commit Loss: 0.001673 | Perplexity: 1854.079942
Trainning Epoch:  95%|█████████▍| 312/330 [25:49:52<1:00:48, 202.72s/it]2025-09-28 08:37:53,251 Stage: Train 0.5 | Epoch: 312 | Iter: 474000 | Total Loss: 0.004488 | Recon Loss: 0.003670 | Commit Loss: 0.001636 | Perplexity: 1851.819094
2025-09-28 08:38:19,805 Stage: Train 0.5 | Epoch: 312 | Iter: 474200 | Total Loss: 0.004499 | Recon Loss: 0.003681 | Commit Loss: 0.001636 | Perplexity: 1853.664273
2025-09-28 08:38:46,305 Stage: Train 0.5 | Epoch: 312 | Iter: 474400 | Total Loss: 0.004548 | Recon Loss: 0.003721 | Commit Loss: 0.001656 | Perplexity: 1856.300460
2025-09-28 08:39:13,021 Stage: Train 0.5 | Epoch: 312 | Iter: 474600 | Total Loss: 0.004457 | Recon Loss: 0.003637 | Commit Loss: 0.001641 | Perplexity: 1856.909042
2025-09-28 08:39:39,686 Stage: Train 0.5 | Epoch: 312 | Iter: 474800 | Total Loss: 0.004506 | Recon Loss: 0.003679 | Commit Loss: 0.001652 | Perplexity: 1856.975393
2025-09-28 08:40:06,254 Stage: Train 0.5 | Epoch: 312 | Iter: 475000 | Total Loss: 0.004481 | Recon Loss: 0.003658 | Commit Loss: 0.001647 | Perplexity: 1857.923884
2025-09-28 08:40:33,020 Stage: Train 0.5 | Epoch: 312 | Iter: 475200 | Total Loss: 0.004476 | Recon Loss: 0.003655 | Commit Loss: 0.001643 | Perplexity: 1852.617961
2025-09-28 08:40:59,656 Stage: Train 0.5 | Epoch: 312 | Iter: 475400 | Total Loss: 0.004507 | Recon Loss: 0.003687 | Commit Loss: 0.001640 | Perplexity: 1856.061639
Trainning Epoch:  95%|█████████▍| 313/330 [25:53:14<57:25, 202.68s/it]  2025-09-28 08:41:26,634 Stage: Train 0.5 | Epoch: 313 | Iter: 475600 | Total Loss: 0.004461 | Recon Loss: 0.003648 | Commit Loss: 0.001626 | Perplexity: 1850.633640
2025-09-28 08:41:53,246 Stage: Train 0.5 | Epoch: 313 | Iter: 475800 | Total Loss: 0.004484 | Recon Loss: 0.003659 | Commit Loss: 0.001650 | Perplexity: 1859.085439
2025-09-28 08:42:20,089 Stage: Train 0.5 | Epoch: 313 | Iter: 476000 | Total Loss: 0.004501 | Recon Loss: 0.003685 | Commit Loss: 0.001632 | Perplexity: 1854.239452
2025-09-28 08:42:46,759 Stage: Train 0.5 | Epoch: 313 | Iter: 476200 | Total Loss: 0.004490 | Recon Loss: 0.003669 | Commit Loss: 0.001643 | Perplexity: 1854.817339
2025-09-28 08:43:13,507 Stage: Train 0.5 | Epoch: 313 | Iter: 476400 | Total Loss: 0.004479 | Recon Loss: 0.003662 | Commit Loss: 0.001634 | Perplexity: 1856.471715
2025-09-28 08:43:40,189 Stage: Train 0.5 | Epoch: 313 | Iter: 476600 | Total Loss: 0.004514 | Recon Loss: 0.003692 | Commit Loss: 0.001643 | Perplexity: 1856.187284
2025-09-28 08:44:06,858 Stage: Train 0.5 | Epoch: 313 | Iter: 476800 | Total Loss: 0.004545 | Recon Loss: 0.003712 | Commit Loss: 0.001665 | Perplexity: 1857.845648
Trainning Epoch:  95%|█████████▌| 314/330 [25:56:37<54:04, 202.80s/it]2025-09-28 08:44:33,854 Stage: Train 0.5 | Epoch: 314 | Iter: 477000 | Total Loss: 0.004465 | Recon Loss: 0.003645 | Commit Loss: 0.001639 | Perplexity: 1853.342019
2025-09-28 08:45:00,571 Stage: Train 0.5 | Epoch: 314 | Iter: 477200 | Total Loss: 0.004538 | Recon Loss: 0.003703 | Commit Loss: 0.001669 | Perplexity: 1858.667812
2025-09-28 08:45:27,219 Stage: Train 0.5 | Epoch: 314 | Iter: 477400 | Total Loss: 0.004546 | Recon Loss: 0.003702 | Commit Loss: 0.001687 | Perplexity: 1857.459030
2025-09-28 08:45:53,839 Stage: Train 0.5 | Epoch: 314 | Iter: 477600 | Total Loss: 0.004467 | Recon Loss: 0.003647 | Commit Loss: 0.001640 | Perplexity: 1855.558679
2025-09-28 08:46:20,515 Stage: Train 0.5 | Epoch: 314 | Iter: 477800 | Total Loss: 0.004472 | Recon Loss: 0.003653 | Commit Loss: 0.001638 | Perplexity: 1856.999506
2025-09-28 08:46:47,057 Stage: Train 0.5 | Epoch: 314 | Iter: 478000 | Total Loss: 0.004516 | Recon Loss: 0.003695 | Commit Loss: 0.001640 | Perplexity: 1858.521967
2025-09-28 08:47:13,732 Stage: Train 0.5 | Epoch: 314 | Iter: 478200 | Total Loss: 0.004468 | Recon Loss: 0.003646 | Commit Loss: 0.001643 | Perplexity: 1854.632424
2025-09-28 08:47:40,363 Stage: Train 0.5 | Epoch: 314 | Iter: 478400 | Total Loss: 0.004472 | Recon Loss: 0.003655 | Commit Loss: 0.001633 | Perplexity: 1853.465411
Trainning Epoch:  95%|█████████▌| 315/330 [26:00:00<50:41, 202.77s/it]2025-09-28 08:48:07,343 Stage: Train 0.5 | Epoch: 315 | Iter: 478600 | Total Loss: 0.004489 | Recon Loss: 0.003656 | Commit Loss: 0.001666 | Perplexity: 1856.671804
2025-09-28 08:48:33,884 Stage: Train 0.5 | Epoch: 315 | Iter: 478800 | Total Loss: 0.004474 | Recon Loss: 0.003654 | Commit Loss: 0.001640 | Perplexity: 1862.275247
2025-09-28 08:49:00,609 Stage: Train 0.5 | Epoch: 315 | Iter: 479000 | Total Loss: 0.004473 | Recon Loss: 0.003650 | Commit Loss: 0.001646 | Perplexity: 1860.362565
2025-09-28 08:49:27,274 Stage: Train 0.5 | Epoch: 315 | Iter: 479200 | Total Loss: 0.004528 | Recon Loss: 0.003702 | Commit Loss: 0.001652 | Perplexity: 1859.468582
2025-09-28 08:49:54,119 Stage: Train 0.5 | Epoch: 315 | Iter: 479400 | Total Loss: 0.004626 | Recon Loss: 0.003755 | Commit Loss: 0.001742 | Perplexity: 1858.051939
2025-09-28 08:50:20,891 Stage: Train 0.5 | Epoch: 315 | Iter: 479600 | Total Loss: 0.004484 | Recon Loss: 0.003644 | Commit Loss: 0.001679 | Perplexity: 1860.738332
2025-09-28 08:50:47,564 Stage: Train 0.5 | Epoch: 315 | Iter: 479800 | Total Loss: 0.004456 | Recon Loss: 0.003632 | Commit Loss: 0.001648 | Perplexity: 1857.107977
2025-09-28 08:51:14,308 Stage: Train 0.5 | Epoch: 315 | Iter: 480000 | Total Loss: 0.004492 | Recon Loss: 0.003668 | Commit Loss: 0.001648 | Perplexity: 1856.749298
2025-09-28 08:51:14,309 Saving model at iteration 480000
2025-09-28 08:51:14,690 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000
2025-09-28 08:51:15,202 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/model.safetensors
2025-09-28 08:51:15,811 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/optimizer.bin
2025-09-28 08:51:15,812 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/scheduler.bin
2025-09-28 08:51:15,812 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/sampler.bin
2025-09-28 08:51:15,813 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_316_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 316/330 [26:03:25<47:26, 203.34s/it]2025-09-28 08:51:42,715 Stage: Train 0.5 | Epoch: 316 | Iter: 480200 | Total Loss: 0.004487 | Recon Loss: 0.003666 | Commit Loss: 0.001642 | Perplexity: 1857.708315
2025-09-28 08:52:09,451 Stage: Train 0.5 | Epoch: 316 | Iter: 480400 | Total Loss: 0.004436 | Recon Loss: 0.003615 | Commit Loss: 0.001641 | Perplexity: 1859.885732
2025-09-28 08:52:36,192 Stage: Train 0.5 | Epoch: 316 | Iter: 480600 | Total Loss: 0.004489 | Recon Loss: 0.003669 | Commit Loss: 0.001640 | Perplexity: 1859.989031
2025-09-28 08:53:02,828 Stage: Train 0.5 | Epoch: 316 | Iter: 480800 | Total Loss: 0.004458 | Recon Loss: 0.003637 | Commit Loss: 0.001640 | Perplexity: 1858.194634
2025-09-28 08:53:29,643 Stage: Train 0.5 | Epoch: 316 | Iter: 481000 | Total Loss: 0.004500 | Recon Loss: 0.003671 | Commit Loss: 0.001658 | Perplexity: 1856.395436
2025-09-28 08:53:56,288 Stage: Train 0.5 | Epoch: 316 | Iter: 481200 | Total Loss: 0.004472 | Recon Loss: 0.003638 | Commit Loss: 0.001668 | Perplexity: 1856.879361
2025-09-28 08:54:23,059 Stage: Train 0.5 | Epoch: 316 | Iter: 481400 | Total Loss: 0.004489 | Recon Loss: 0.003666 | Commit Loss: 0.001644 | Perplexity: 1859.301550
Trainning Epoch:  96%|█████████▌| 317/330 [26:06:48<44:02, 203.26s/it]2025-09-28 08:54:50,084 Stage: Train 0.5 | Epoch: 317 | Iter: 481600 | Total Loss: 0.004494 | Recon Loss: 0.003674 | Commit Loss: 0.001639 | Perplexity: 1854.883708
2025-09-28 08:55:16,700 Stage: Train 0.5 | Epoch: 317 | Iter: 481800 | Total Loss: 0.004483 | Recon Loss: 0.003666 | Commit Loss: 0.001633 | Perplexity: 1855.158644
2025-09-28 08:55:43,343 Stage: Train 0.5 | Epoch: 317 | Iter: 482000 | Total Loss: 0.004533 | Recon Loss: 0.003685 | Commit Loss: 0.001696 | Perplexity: 1859.144713
2025-09-28 08:56:09,986 Stage: Train 0.5 | Epoch: 317 | Iter: 482200 | Total Loss: 0.004455 | Recon Loss: 0.003635 | Commit Loss: 0.001640 | Perplexity: 1856.970421
2025-09-28 08:56:36,683 Stage: Train 0.5 | Epoch: 317 | Iter: 482400 | Total Loss: 0.004485 | Recon Loss: 0.003662 | Commit Loss: 0.001645 | Perplexity: 1859.328388
2025-09-28 08:57:03,367 Stage: Train 0.5 | Epoch: 317 | Iter: 482600 | Total Loss: 0.004465 | Recon Loss: 0.003643 | Commit Loss: 0.001643 | Perplexity: 1862.314568
2025-09-28 08:57:30,096 Stage: Train 0.5 | Epoch: 317 | Iter: 482800 | Total Loss: 0.004482 | Recon Loss: 0.003661 | Commit Loss: 0.001642 | Perplexity: 1856.900555
2025-09-28 08:57:56,749 Stage: Train 0.5 | Epoch: 317 | Iter: 483000 | Total Loss: 0.004510 | Recon Loss: 0.003689 | Commit Loss: 0.001640 | Perplexity: 1857.080753
Trainning Epoch:  96%|█████████▋| 318/330 [26:10:11<40:37, 203.15s/it]2025-09-28 08:58:23,663 Stage: Train 0.5 | Epoch: 318 | Iter: 483200 | Total Loss: 0.004479 | Recon Loss: 0.003651 | Commit Loss: 0.001655 | Perplexity: 1851.287802
2025-09-28 08:58:50,243 Stage: Train 0.5 | Epoch: 318 | Iter: 483400 | Total Loss: 0.004453 | Recon Loss: 0.003632 | Commit Loss: 0.001643 | Perplexity: 1860.192976
2025-09-28 08:59:16,974 Stage: Train 0.5 | Epoch: 318 | Iter: 483600 | Total Loss: 0.004473 | Recon Loss: 0.003654 | Commit Loss: 0.001638 | Perplexity: 1859.558160
2025-09-28 08:59:43,576 Stage: Train 0.5 | Epoch: 318 | Iter: 483800 | Total Loss: 0.004492 | Recon Loss: 0.003669 | Commit Loss: 0.001646 | Perplexity: 1855.668817
2025-09-28 09:00:10,284 Stage: Train 0.5 | Epoch: 318 | Iter: 484000 | Total Loss: 0.004458 | Recon Loss: 0.003636 | Commit Loss: 0.001644 | Perplexity: 1860.274026
2025-09-28 09:00:36,972 Stage: Train 0.5 | Epoch: 318 | Iter: 484200 | Total Loss: 0.004506 | Recon Loss: 0.003679 | Commit Loss: 0.001654 | Perplexity: 1855.847802
2025-09-28 09:01:03,627 Stage: Train 0.5 | Epoch: 318 | Iter: 484400 | Total Loss: 0.004490 | Recon Loss: 0.003668 | Commit Loss: 0.001644 | Perplexity: 1857.218763
Trainning Epoch:  97%|█████████▋| 319/330 [26:13:33<37:13, 203.03s/it]2025-09-28 09:01:30,567 Stage: Train 0.5 | Epoch: 319 | Iter: 484600 | Total Loss: 0.004442 | Recon Loss: 0.003616 | Commit Loss: 0.001652 | Perplexity: 1859.988099
2025-09-28 09:01:57,283 Stage: Train 0.5 | Epoch: 319 | Iter: 484800 | Total Loss: 0.004473 | Recon Loss: 0.003660 | Commit Loss: 0.001626 | Perplexity: 1858.166268
2025-09-28 09:02:24,055 Stage: Train 0.5 | Epoch: 319 | Iter: 485000 | Total Loss: 0.004484 | Recon Loss: 0.003665 | Commit Loss: 0.001637 | Perplexity: 1861.313722
2025-09-28 09:02:50,984 Stage: Train 0.5 | Epoch: 319 | Iter: 485200 | Total Loss: 0.004414 | Recon Loss: 0.003597 | Commit Loss: 0.001633 | Perplexity: 1854.210283
2025-09-28 09:03:17,736 Stage: Train 0.5 | Epoch: 319 | Iter: 485400 | Total Loss: 0.004462 | Recon Loss: 0.003645 | Commit Loss: 0.001635 | Perplexity: 1858.539101
2025-09-28 09:03:44,460 Stage: Train 0.5 | Epoch: 319 | Iter: 485600 | Total Loss: 0.004420 | Recon Loss: 0.003600 | Commit Loss: 0.001640 | Perplexity: 1857.418469
2025-09-28 09:04:11,058 Stage: Train 0.5 | Epoch: 319 | Iter: 485800 | Total Loss: 0.004455 | Recon Loss: 0.003632 | Commit Loss: 0.001647 | Perplexity: 1859.538409
2025-09-28 09:04:37,717 Stage: Train 0.5 | Epoch: 319 | Iter: 486000 | Total Loss: 0.004446 | Recon Loss: 0.003626 | Commit Loss: 0.001640 | Perplexity: 1857.654616
Trainning Epoch:  97%|█████████▋| 320/330 [26:16:57<33:51, 203.14s/it]2025-09-28 09:05:04,696 Stage: Train 0.5 | Epoch: 320 | Iter: 486200 | Total Loss: 0.004476 | Recon Loss: 0.003646 | Commit Loss: 0.001661 | Perplexity: 1856.333128
2025-09-28 09:05:31,288 Stage: Train 0.5 | Epoch: 320 | Iter: 486400 | Total Loss: 0.004454 | Recon Loss: 0.003631 | Commit Loss: 0.001647 | Perplexity: 1859.109440
2025-09-28 09:05:58,027 Stage: Train 0.5 | Epoch: 320 | Iter: 486600 | Total Loss: 0.004475 | Recon Loss: 0.003654 | Commit Loss: 0.001643 | Perplexity: 1859.275961
2025-09-28 09:06:24,670 Stage: Train 0.5 | Epoch: 320 | Iter: 486800 | Total Loss: 0.004439 | Recon Loss: 0.003626 | Commit Loss: 0.001626 | Perplexity: 1854.801351
2025-09-28 09:06:51,382 Stage: Train 0.5 | Epoch: 320 | Iter: 487000 | Total Loss: 0.004441 | Recon Loss: 0.003624 | Commit Loss: 0.001634 | Perplexity: 1855.810267
2025-09-28 09:07:18,099 Stage: Train 0.5 | Epoch: 320 | Iter: 487200 | Total Loss: 0.004486 | Recon Loss: 0.003664 | Commit Loss: 0.001642 | Perplexity: 1861.657564
2025-09-28 09:07:44,744 Stage: Train 0.5 | Epoch: 320 | Iter: 487400 | Total Loss: 0.004454 | Recon Loss: 0.003632 | Commit Loss: 0.001644 | Perplexity: 1862.085460
Trainning Epoch:  97%|█████████▋| 321/330 [26:20:19<30:27, 203.03s/it]2025-09-28 09:08:11,666 Stage: Train 0.5 | Epoch: 321 | Iter: 487600 | Total Loss: 0.004474 | Recon Loss: 0.003653 | Commit Loss: 0.001643 | Perplexity: 1857.453945
2025-09-28 09:08:38,346 Stage: Train 0.5 | Epoch: 321 | Iter: 487800 | Total Loss: 0.004499 | Recon Loss: 0.003650 | Commit Loss: 0.001699 | Perplexity: 1861.526999
2025-09-28 09:09:04,969 Stage: Train 0.5 | Epoch: 321 | Iter: 488000 | Total Loss: 0.004507 | Recon Loss: 0.003640 | Commit Loss: 0.001733 | Perplexity: 1865.194846
2025-09-28 09:09:31,624 Stage: Train 0.5 | Epoch: 321 | Iter: 488200 | Total Loss: 0.004447 | Recon Loss: 0.003624 | Commit Loss: 0.001646 | Perplexity: 1856.887769
2025-09-28 09:09:58,345 Stage: Train 0.5 | Epoch: 321 | Iter: 488400 | Total Loss: 0.004406 | Recon Loss: 0.003587 | Commit Loss: 0.001636 | Perplexity: 1857.713598
2025-09-28 09:10:25,032 Stage: Train 0.5 | Epoch: 321 | Iter: 488600 | Total Loss: 0.004470 | Recon Loss: 0.003647 | Commit Loss: 0.001646 | Perplexity: 1863.063436
2025-09-28 09:10:51,351 Stage: Train 0.5 | Epoch: 321 | Iter: 488800 | Total Loss: 0.004532 | Recon Loss: 0.003684 | Commit Loss: 0.001698 | Perplexity: 1858.947341
2025-09-28 09:11:17,839 Stage: Train 0.5 | Epoch: 321 | Iter: 489000 | Total Loss: 0.004436 | Recon Loss: 0.003618 | Commit Loss: 0.001636 | Perplexity: 1857.639930
Trainning Epoch:  98%|█████████▊| 322/330 [26:23:42<27:02, 202.83s/it]2025-09-28 09:11:44,837 Stage: Train 0.5 | Epoch: 322 | Iter: 489200 | Total Loss: 0.004516 | Recon Loss: 0.003695 | Commit Loss: 0.001642 | Perplexity: 1858.101522
2025-09-28 09:12:11,499 Stage: Train 0.5 | Epoch: 322 | Iter: 489400 | Total Loss: 0.004401 | Recon Loss: 0.003584 | Commit Loss: 0.001635 | Perplexity: 1858.936140
2025-09-28 09:12:38,190 Stage: Train 0.5 | Epoch: 322 | Iter: 489600 | Total Loss: 0.004432 | Recon Loss: 0.003610 | Commit Loss: 0.001645 | Perplexity: 1861.319879
2025-09-28 09:13:04,863 Stage: Train 0.5 | Epoch: 322 | Iter: 489800 | Total Loss: 0.004453 | Recon Loss: 0.003629 | Commit Loss: 0.001648 | Perplexity: 1860.817756
2025-09-28 09:13:31,717 Stage: Train 0.5 | Epoch: 322 | Iter: 490000 | Total Loss: 0.004486 | Recon Loss: 0.003653 | Commit Loss: 0.001667 | Perplexity: 1857.077737
2025-09-28 09:13:58,374 Stage: Train 0.5 | Epoch: 322 | Iter: 490200 | Total Loss: 0.004432 | Recon Loss: 0.003611 | Commit Loss: 0.001641 | Perplexity: 1856.329109
2025-09-28 09:14:25,103 Stage: Train 0.5 | Epoch: 322 | Iter: 490400 | Total Loss: 0.004597 | Recon Loss: 0.003742 | Commit Loss: 0.001708 | Perplexity: 1860.859759
2025-09-28 09:14:51,795 Stage: Train 0.5 | Epoch: 322 | Iter: 490600 | Total Loss: 0.004453 | Recon Loss: 0.003626 | Commit Loss: 0.001654 | Perplexity: 1861.622186
Trainning Epoch:  98%|█████████▊| 323/330 [26:27:05<23:40, 202.92s/it]2025-09-28 09:15:18,714 Stage: Train 0.5 | Epoch: 323 | Iter: 490800 | Total Loss: 0.004386 | Recon Loss: 0.003572 | Commit Loss: 0.001629 | Perplexity: 1852.893568
2025-09-28 09:15:45,406 Stage: Train 0.5 | Epoch: 323 | Iter: 491000 | Total Loss: 0.004457 | Recon Loss: 0.003644 | Commit Loss: 0.001627 | Perplexity: 1854.388759
2025-09-28 09:16:12,170 Stage: Train 0.5 | Epoch: 323 | Iter: 491200 | Total Loss: 0.004477 | Recon Loss: 0.003656 | Commit Loss: 0.001642 | Perplexity: 1861.067220
2025-09-28 09:16:39,000 Stage: Train 0.5 | Epoch: 323 | Iter: 491400 | Total Loss: 0.004494 | Recon Loss: 0.003661 | Commit Loss: 0.001665 | Perplexity: 1859.817841
2025-09-28 09:17:05,876 Stage: Train 0.5 | Epoch: 323 | Iter: 491600 | Total Loss: 0.004507 | Recon Loss: 0.003650 | Commit Loss: 0.001714 | Perplexity: 1859.741116
2025-09-28 09:17:32,648 Stage: Train 0.5 | Epoch: 323 | Iter: 491800 | Total Loss: 0.004441 | Recon Loss: 0.003621 | Commit Loss: 0.001640 | Perplexity: 1857.871617
2025-09-28 09:17:59,390 Stage: Train 0.5 | Epoch: 323 | Iter: 492000 | Total Loss: 0.004448 | Recon Loss: 0.003630 | Commit Loss: 0.001636 | Perplexity: 1857.552600
Trainning Epoch:  98%|█████████▊| 324/330 [26:30:28<20:18, 203.07s/it]2025-09-28 09:18:26,285 Stage: Train 0.5 | Epoch: 324 | Iter: 492200 | Total Loss: 0.004511 | Recon Loss: 0.003692 | Commit Loss: 0.001640 | Perplexity: 1857.930344
2025-09-28 09:18:52,908 Stage: Train 0.5 | Epoch: 324 | Iter: 492400 | Total Loss: 0.004417 | Recon Loss: 0.003598 | Commit Loss: 0.001637 | Perplexity: 1860.599897
2025-09-28 09:19:19,695 Stage: Train 0.5 | Epoch: 324 | Iter: 492600 | Total Loss: 0.004561 | Recon Loss: 0.003684 | Commit Loss: 0.001755 | Perplexity: 1859.788912
2025-09-28 09:19:46,374 Stage: Train 0.5 | Epoch: 324 | Iter: 492800 | Total Loss: 0.004456 | Recon Loss: 0.003636 | Commit Loss: 0.001640 | Perplexity: 1857.699938
2025-09-28 09:20:13,229 Stage: Train 0.5 | Epoch: 324 | Iter: 493000 | Total Loss: 0.004436 | Recon Loss: 0.003618 | Commit Loss: 0.001636 | Perplexity: 1859.623696
2025-09-28 09:20:39,992 Stage: Train 0.5 | Epoch: 324 | Iter: 493200 | Total Loss: 0.004509 | Recon Loss: 0.003691 | Commit Loss: 0.001636 | Perplexity: 1858.847256
2025-09-28 09:21:06,688 Stage: Train 0.5 | Epoch: 324 | Iter: 493400 | Total Loss: 0.004410 | Recon Loss: 0.003588 | Commit Loss: 0.001644 | Perplexity: 1855.828475
2025-09-28 09:21:33,326 Stage: Train 0.5 | Epoch: 324 | Iter: 493600 | Total Loss: 0.004454 | Recon Loss: 0.003633 | Commit Loss: 0.001644 | Perplexity: 1860.330480
Trainning Epoch:  98%|█████████▊| 325/330 [26:33:52<16:55, 203.10s/it]2025-09-28 09:22:00,362 Stage: Train 0.5 | Epoch: 325 | Iter: 493800 | Total Loss: 0.004448 | Recon Loss: 0.003631 | Commit Loss: 0.001634 | Perplexity: 1855.430483
2025-09-28 09:22:27,000 Stage: Train 0.5 | Epoch: 325 | Iter: 494000 | Total Loss: 0.004462 | Recon Loss: 0.003642 | Commit Loss: 0.001640 | Perplexity: 1866.056393
2025-09-28 09:22:53,704 Stage: Train 0.5 | Epoch: 325 | Iter: 494200 | Total Loss: 0.004478 | Recon Loss: 0.003663 | Commit Loss: 0.001631 | Perplexity: 1860.090082
2025-09-28 09:23:20,238 Stage: Train 0.5 | Epoch: 325 | Iter: 494400 | Total Loss: 0.004435 | Recon Loss: 0.003619 | Commit Loss: 0.001633 | Perplexity: 1858.264116
2025-09-28 09:23:47,053 Stage: Train 0.5 | Epoch: 325 | Iter: 494600 | Total Loss: 0.004450 | Recon Loss: 0.003630 | Commit Loss: 0.001640 | Perplexity: 1857.239634
2025-09-28 09:24:13,649 Stage: Train 0.5 | Epoch: 325 | Iter: 494800 | Total Loss: 0.004431 | Recon Loss: 0.003617 | Commit Loss: 0.001629 | Perplexity: 1856.615557
2025-09-28 09:24:40,309 Stage: Train 0.5 | Epoch: 325 | Iter: 495000 | Total Loss: 0.004460 | Recon Loss: 0.003642 | Commit Loss: 0.001635 | Perplexity: 1858.259391
Trainning Epoch:  99%|█████████▉| 326/330 [26:37:14<13:31, 202.94s/it]2025-09-28 09:25:06,953 Stage: Train 0.5 | Epoch: 326 | Iter: 495200 | Total Loss: 0.004443 | Recon Loss: 0.003628 | Commit Loss: 0.001631 | Perplexity: 1857.065499
2025-09-28 09:25:33,615 Stage: Train 0.5 | Epoch: 326 | Iter: 495400 | Total Loss: 0.004598 | Recon Loss: 0.003723 | Commit Loss: 0.001751 | Perplexity: 1863.115258
2025-09-28 09:26:00,227 Stage: Train 0.5 | Epoch: 326 | Iter: 495600 | Total Loss: 0.004382 | Recon Loss: 0.003565 | Commit Loss: 0.001634 | Perplexity: 1859.100088
2025-09-28 09:26:26,973 Stage: Train 0.5 | Epoch: 326 | Iter: 495800 | Total Loss: 0.004431 | Recon Loss: 0.003614 | Commit Loss: 0.001634 | Perplexity: 1858.467906
2025-09-28 09:26:53,650 Stage: Train 0.5 | Epoch: 326 | Iter: 496000 | Total Loss: 0.004433 | Recon Loss: 0.003612 | Commit Loss: 0.001642 | Perplexity: 1859.859886
2025-09-28 09:27:20,328 Stage: Train 0.5 | Epoch: 326 | Iter: 496200 | Total Loss: 0.004485 | Recon Loss: 0.003663 | Commit Loss: 0.001645 | Perplexity: 1857.462196
2025-09-28 09:27:46,976 Stage: Train 0.5 | Epoch: 326 | Iter: 496400 | Total Loss: 0.004472 | Recon Loss: 0.003655 | Commit Loss: 0.001634 | Perplexity: 1859.751594
2025-09-28 09:28:13,612 Stage: Train 0.5 | Epoch: 326 | Iter: 496600 | Total Loss: 0.004435 | Recon Loss: 0.003611 | Commit Loss: 0.001648 | Perplexity: 1858.229070
Trainning Epoch:  99%|█████████▉| 327/330 [26:40:37<10:08, 202.90s/it]2025-09-28 09:28:40,556 Stage: Train 0.5 | Epoch: 327 | Iter: 496800 | Total Loss: 0.004461 | Recon Loss: 0.003641 | Commit Loss: 0.001640 | Perplexity: 1859.609724
2025-09-28 09:29:07,190 Stage: Train 0.5 | Epoch: 327 | Iter: 497000 | Total Loss: 0.004437 | Recon Loss: 0.003622 | Commit Loss: 0.001629 | Perplexity: 1856.906877
2025-09-28 09:29:33,795 Stage: Train 0.5 | Epoch: 327 | Iter: 497200 | Total Loss: 0.004433 | Recon Loss: 0.003618 | Commit Loss: 0.001630 | Perplexity: 1860.662774
2025-09-28 09:30:00,512 Stage: Train 0.5 | Epoch: 327 | Iter: 497400 | Total Loss: 0.004406 | Recon Loss: 0.003589 | Commit Loss: 0.001634 | Perplexity: 1858.205018
2025-09-28 09:30:27,224 Stage: Train 0.5 | Epoch: 327 | Iter: 497600 | Total Loss: 0.004406 | Recon Loss: 0.003586 | Commit Loss: 0.001640 | Perplexity: 1859.813120
2025-09-28 09:30:53,848 Stage: Train 0.5 | Epoch: 327 | Iter: 497800 | Total Loss: 0.004488 | Recon Loss: 0.003672 | Commit Loss: 0.001633 | Perplexity: 1858.626252
2025-09-28 09:31:20,469 Stage: Train 0.5 | Epoch: 327 | Iter: 498000 | Total Loss: 0.004449 | Recon Loss: 0.003623 | Commit Loss: 0.001651 | Perplexity: 1857.761918
2025-09-28 09:31:47,096 Stage: Train 0.5 | Epoch: 327 | Iter: 498200 | Total Loss: 0.004448 | Recon Loss: 0.003628 | Commit Loss: 0.001640 | Perplexity: 1858.459913
Trainning Epoch:  99%|█████████▉| 328/330 [26:44:00<06:45, 202.83s/it]2025-09-28 09:32:14,026 Stage: Train 0.5 | Epoch: 328 | Iter: 498400 | Total Loss: 0.004391 | Recon Loss: 0.003576 | Commit Loss: 0.001631 | Perplexity: 1856.480585
2025-09-28 09:32:40,796 Stage: Train 0.5 | Epoch: 328 | Iter: 498600 | Total Loss: 0.004428 | Recon Loss: 0.003612 | Commit Loss: 0.001633 | Perplexity: 1861.331559
2025-09-28 09:33:07,471 Stage: Train 0.5 | Epoch: 328 | Iter: 498800 | Total Loss: 0.004488 | Recon Loss: 0.003670 | Commit Loss: 0.001637 | Perplexity: 1858.307659
2025-09-28 09:33:34,144 Stage: Train 0.5 | Epoch: 328 | Iter: 499000 | Total Loss: 0.004461 | Recon Loss: 0.003646 | Commit Loss: 0.001631 | Perplexity: 1859.382114
2025-09-28 09:34:01,010 Stage: Train 0.5 | Epoch: 328 | Iter: 499200 | Total Loss: 0.004530 | Recon Loss: 0.003686 | Commit Loss: 0.001687 | Perplexity: 1859.062714
2025-09-28 09:34:27,674 Stage: Train 0.5 | Epoch: 328 | Iter: 499400 | Total Loss: 0.004417 | Recon Loss: 0.003601 | Commit Loss: 0.001632 | Perplexity: 1859.704055
2025-09-28 09:34:54,246 Stage: Train 0.5 | Epoch: 328 | Iter: 499600 | Total Loss: 0.004437 | Recon Loss: 0.003616 | Commit Loss: 0.001642 | Perplexity: 1859.538579
Trainning Epoch: 100%|█████████▉| 329/330 [26:47:23<03:22, 202.88s/it]2025-09-28 09:35:21,154 Stage: Train 0.5 | Epoch: 329 | Iter: 499800 | Total Loss: 0.004388 | Recon Loss: 0.003572 | Commit Loss: 0.001633 | Perplexity: 1856.449146
2025-09-28 09:35:47,795 Stage: Train 0.5 | Epoch: 329 | Iter: 500000 | Total Loss: 0.004457 | Recon Loss: 0.003641 | Commit Loss: 0.001631 | Perplexity: 1858.686710
2025-09-28 09:35:47,796 Saving model at iteration 500000
2025-09-28 09:35:48,013 Saving current state to vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000
2025-09-28 09:35:48,521 Model weights saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/model.safetensors
2025-09-28 09:35:49,037 Optimizer state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/optimizer.bin
2025-09-28 09:35:49,038 Scheduler state saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/scheduler.bin
2025-09-28 09:35:49,038 Sampler state for dataloader 0 saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/sampler.bin
2025-09-28 09:35:49,039 Random states saved in vqvae_experiment/joint_only/joint3d_cam_rootrel_meter/f16s1d16_cb8192x3072_mpjpe_Tdown1-2/models/checkpoint_epoch_330_step_500000/random_states_0.pkl
Trainning Epoch: 100%|█████████▉| 329/330 [26:47:57<04:53, 293.25s/it]
2025-09-28 09:35:49,107 Training finished
