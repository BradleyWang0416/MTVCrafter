Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-09-08 04:56:01,038 
python train_vqvae.py --data_mode joint3d --load_data_file /data2/wxs/DATASETS/PW3D_ByBradley/all_data.pkl,/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl,/data2/wxs/DATASETS/AMASS_ByBradley/ --num_frames 64 --sample_stride 2 --project_dir vqvae_experiment/all_datasets_j3d_f64s2 --not_find_unused_parameters
2025-09-08 04:57:25,212 Data loaded with 51958 samples and 3578331 frames
2025-09-08 04:57:28,681 Number of trainable parameters: 48.323075 M
2025-09-08 04:57:28,682 Args: Namespace(data_root='./', num_frames=64, batch_size=32, max_epoch=1000000000.0, total_iter=500000, world_size=1, rank=0, save_interval=20000, warm_up_iter=5000, print_iter=200, learning_rate=0.0002, lr_schedule=[300000], gamma=0.05, weight_decay=0.0001, resume_pth='', device='cuda', project_config='', allow_tf32=False, project_dir='vqvae_experiment/all_datasets_j3d_f64s2', seed=6666, commit_ratio=0.5, nb_code=8192, codebook_dim=3072, load_data_file='/data2/wxs/DATASETS/PW3D_ByBradley/all_data.pkl,/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl,/data2/wxs/DATASETS/AMASS_ByBradley/', data_mode='joint3d', not_find_unused_parameters=True, sample_stride=2)
Trainning Epoch:   0%|          | 0/308 [00:00<?, ?it/s]2025-09-08 04:59:13,330 current_lr 0.000008 at iteration 200
2025-09-08 04:59:13,850 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.080866 | Recon Loss: 0.079146 | Commit Loss: 0.003440 | Perplexity: 1292.637190
2025-09-08 05:00:55,780 current_lr 0.000016 at iteration 400
2025-09-08 05:00:56,286 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.044254 | Recon Loss: 0.039812 | Commit Loss: 0.008883 | Perplexity: 1192.232452
2025-09-08 05:02:38,263 current_lr 0.000024 at iteration 600
2025-09-08 05:02:38,776 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.043058 | Recon Loss: 0.033501 | Commit Loss: 0.019116 | Perplexity: 1152.026367
2025-09-08 05:04:20,615 current_lr 0.000032 at iteration 800
2025-09-08 05:04:21,130 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.044791 | Recon Loss: 0.031517 | Commit Loss: 0.026548 | Perplexity: 1195.144385
2025-09-08 05:06:02,944 current_lr 0.000040 at iteration 1000
2025-09-08 05:06:03,427 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.044137 | Recon Loss: 0.030190 | Commit Loss: 0.027894 | Perplexity: 1216.011674
2025-09-08 05:07:45,276 current_lr 0.000048 at iteration 1200
2025-09-08 05:07:45,788 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.041880 | Recon Loss: 0.028524 | Commit Loss: 0.026712 | Perplexity: 1217.709381
2025-09-08 05:09:27,924 current_lr 0.000056 at iteration 1400
2025-09-08 05:09:28,445 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.037444 | Recon Loss: 0.025333 | Commit Loss: 0.024223 | Perplexity: 1230.322808
2025-09-08 05:11:10,296 current_lr 0.000064 at iteration 1600
2025-09-08 05:11:10,812 Stage: Warm Up | Epoch: 0 | Iter: 1600 | Total Loss: 0.033672 | Recon Loss: 0.023855 | Commit Loss: 0.019633 | Perplexity: 1237.050300
Trainning Epoch:   0%|          | 1/308 [13:54<71:09:08, 834.36s/it]2025-09-08 05:12:52,736 current_lr 0.000072 at iteration 1800
2025-09-08 05:12:53,244 Stage: Warm Up | Epoch: 1 | Iter: 1800 | Total Loss: 0.028953 | Recon Loss: 0.021081 | Commit Loss: 0.015744 | Perplexity: 1249.159598
2025-09-08 05:14:35,181 current_lr 0.000080 at iteration 2000
2025-09-08 05:14:35,714 Stage: Warm Up | Epoch: 1 | Iter: 2000 | Total Loss: 0.026097 | Recon Loss: 0.019832 | Commit Loss: 0.012529 | Perplexity: 1272.172147
2025-09-08 05:16:17,675 current_lr 0.000088 at iteration 2200
2025-09-08 05:16:18,199 Stage: Warm Up | Epoch: 1 | Iter: 2200 | Total Loss: 0.023727 | Recon Loss: 0.018768 | Commit Loss: 0.009918 | Perplexity: 1291.423620
2025-09-08 05:18:00,081 current_lr 0.000096 at iteration 2400
2025-09-08 05:18:00,588 Stage: Warm Up | Epoch: 1 | Iter: 2400 | Total Loss: 0.021260 | Recon Loss: 0.017180 | Commit Loss: 0.008159 | Perplexity: 1322.457704
2025-09-08 05:19:42,753 current_lr 0.000104 at iteration 2600
2025-09-08 05:19:43,267 Stage: Warm Up | Epoch: 1 | Iter: 2600 | Total Loss: 0.020216 | Recon Loss: 0.016974 | Commit Loss: 0.006483 | Perplexity: 1353.801705
2025-09-08 05:21:25,410 current_lr 0.000112 at iteration 2800
2025-09-08 05:21:25,908 Stage: Warm Up | Epoch: 1 | Iter: 2800 | Total Loss: 0.019301 | Recon Loss: 0.016532 | Commit Loss: 0.005537 | Perplexity: 1380.407418
2025-09-08 05:23:07,784 current_lr 0.000120 at iteration 3000
2025-09-08 05:23:08,308 Stage: Warm Up | Epoch: 1 | Iter: 3000 | Total Loss: 0.018749 | Recon Loss: 0.016394 | Commit Loss: 0.004710 | Perplexity: 1391.219506
2025-09-08 05:24:50,282 current_lr 0.000128 at iteration 3200
2025-09-08 05:24:50,799 Stage: Warm Up | Epoch: 1 | Iter: 3200 | Total Loss: 0.018939 | Recon Loss: 0.016828 | Commit Loss: 0.004223 | Perplexity: 1412.720477
Trainning Epoch:   1%|          | 2/308 [27:46<70:48:57, 833.13s/it]2025-09-08 05:26:32,610 current_lr 0.000136 at iteration 3400
2025-09-08 05:26:33,130 Stage: Warm Up | Epoch: 2 | Iter: 3400 | Total Loss: 0.017172 | Recon Loss: 0.015198 | Commit Loss: 0.003947 | Perplexity: 1432.727885
2025-09-08 05:28:15,146 current_lr 0.000144 at iteration 3600
2025-09-08 05:28:15,651 Stage: Warm Up | Epoch: 2 | Iter: 3600 | Total Loss: 0.016626 | Recon Loss: 0.014789 | Commit Loss: 0.003675 | Perplexity: 1453.034070
2025-09-08 05:29:57,549 current_lr 0.000152 at iteration 3800
2025-09-08 05:29:58,065 Stage: Warm Up | Epoch: 2 | Iter: 3800 | Total Loss: 0.016519 | Recon Loss: 0.014727 | Commit Loss: 0.003585 | Perplexity: 1469.781326
2025-09-08 05:31:40,113 current_lr 0.000160 at iteration 4000
2025-09-08 05:31:40,633 Stage: Warm Up | Epoch: 2 | Iter: 4000 | Total Loss: 0.015712 | Recon Loss: 0.014013 | Commit Loss: 0.003398 | Perplexity: 1475.682990
2025-09-08 05:33:22,625 current_lr 0.000168 at iteration 4200
2025-09-08 05:33:23,141 Stage: Warm Up | Epoch: 2 | Iter: 4200 | Total Loss: 0.015500 | Recon Loss: 0.013890 | Commit Loss: 0.003220 | Perplexity: 1476.302733
2025-09-08 05:35:05,348 current_lr 0.000176 at iteration 4400
2025-09-08 05:35:05,854 Stage: Warm Up | Epoch: 2 | Iter: 4400 | Total Loss: 0.015281 | Recon Loss: 0.013719 | Commit Loss: 0.003125 | Perplexity: 1510.418176
2025-09-08 05:36:48,553 current_lr 0.000184 at iteration 4600
2025-09-08 05:36:49,059 Stage: Warm Up | Epoch: 2 | Iter: 4600 | Total Loss: 0.014874 | Recon Loss: 0.013485 | Commit Loss: 0.002779 | Perplexity: 1506.795236
2025-09-08 05:38:31,156 current_lr 0.000192 at iteration 4800
2025-09-08 05:38:31,682 Stage: Warm Up | Epoch: 2 | Iter: 4800 | Total Loss: 0.013556 | Recon Loss: 0.012146 | Commit Loss: 0.002821 | Perplexity: 1534.208328
Trainning Epoch:   1%|          | 3/308 [41:39<70:35:10, 833.15s/it]2025-09-08 05:40:13,669 current_lr 0.000200 at iteration 5000
2025-09-08 05:40:14,180 Stage: Warm Up | Epoch: 3 | Iter: 5000 | Total Loss: 0.013692 | Recon Loss: 0.012422 | Commit Loss: 0.002540 | Perplexity: 1513.457596
2025-09-08 05:41:56,802 Stage: Train 0.5 | Epoch: 3 | Iter: 5200 | Total Loss: 0.013873 | Recon Loss: 0.012687 | Commit Loss: 0.002372 | Perplexity: 1509.011085
2025-09-08 05:43:39,374 Stage: Train 0.5 | Epoch: 3 | Iter: 5400 | Total Loss: 0.012528 | Recon Loss: 0.011355 | Commit Loss: 0.002345 | Perplexity: 1516.204547
2025-09-08 05:45:22,111 Stage: Train 0.5 | Epoch: 3 | Iter: 5600 | Total Loss: 0.011954 | Recon Loss: 0.010807 | Commit Loss: 0.002294 | Perplexity: 1518.518741
2025-09-08 05:47:04,661 Stage: Train 0.5 | Epoch: 3 | Iter: 5800 | Total Loss: 0.011779 | Recon Loss: 0.010666 | Commit Loss: 0.002227 | Perplexity: 1529.382082
2025-09-08 05:48:47,312 Stage: Train 0.5 | Epoch: 3 | Iter: 6000 | Total Loss: 0.011122 | Recon Loss: 0.010026 | Commit Loss: 0.002192 | Perplexity: 1529.890117
2025-09-08 05:50:29,713 Stage: Train 0.5 | Epoch: 3 | Iter: 6200 | Total Loss: 0.011004 | Recon Loss: 0.009957 | Commit Loss: 0.002093 | Perplexity: 1521.027945
2025-09-08 05:52:12,206 Stage: Train 0.5 | Epoch: 3 | Iter: 6400 | Total Loss: 0.011006 | Recon Loss: 0.010009 | Commit Loss: 0.001994 | Perplexity: 1521.969607
Trainning Epoch:   1%|▏         | 4/308 [55:32<70:20:29, 832.99s/it]2025-09-08 05:53:54,696 Stage: Train 0.5 | Epoch: 4 | Iter: 6600 | Total Loss: 0.010461 | Recon Loss: 0.009505 | Commit Loss: 0.001913 | Perplexity: 1508.418259
2025-09-08 05:55:37,311 Stage: Train 0.5 | Epoch: 4 | Iter: 6800 | Total Loss: 0.009690 | Recon Loss: 0.008755 | Commit Loss: 0.001870 | Perplexity: 1544.995328
2025-09-08 05:57:19,810 Stage: Train 0.5 | Epoch: 4 | Iter: 7000 | Total Loss: 0.009620 | Recon Loss: 0.008683 | Commit Loss: 0.001875 | Perplexity: 1554.270765
2025-09-08 05:59:02,380 Stage: Train 0.5 | Epoch: 4 | Iter: 7200 | Total Loss: 0.009763 | Recon Loss: 0.008889 | Commit Loss: 0.001748 | Perplexity: 1565.824940
2025-09-08 06:00:44,972 Stage: Train 0.5 | Epoch: 4 | Iter: 7400 | Total Loss: 0.009256 | Recon Loss: 0.008409 | Commit Loss: 0.001694 | Perplexity: 1564.141959
2025-09-08 06:02:27,766 Stage: Train 0.5 | Epoch: 4 | Iter: 7600 | Total Loss: 0.009525 | Recon Loss: 0.008631 | Commit Loss: 0.001788 | Perplexity: 1543.152544
2025-09-08 06:04:10,362 Stage: Train 0.5 | Epoch: 4 | Iter: 7800 | Total Loss: 0.010025 | Recon Loss: 0.009172 | Commit Loss: 0.001705 | Perplexity: 1529.183338
2025-09-08 06:05:52,941 Stage: Train 0.5 | Epoch: 4 | Iter: 8000 | Total Loss: 0.008798 | Recon Loss: 0.007967 | Commit Loss: 0.001663 | Perplexity: 1542.218278
Trainning Epoch:   2%|▏         | 5/308 [1:09:25<70:06:54, 833.05s/it]2025-09-08 06:07:35,521 Stage: Train 0.5 | Epoch: 5 | Iter: 8200 | Total Loss: 0.008507 | Recon Loss: 0.007679 | Commit Loss: 0.001656 | Perplexity: 1537.609550
2025-09-08 06:09:18,371 Stage: Train 0.5 | Epoch: 5 | Iter: 8400 | Total Loss: 0.008826 | Recon Loss: 0.007983 | Commit Loss: 0.001687 | Perplexity: 1526.283179
2025-09-08 06:11:00,942 Stage: Train 0.5 | Epoch: 5 | Iter: 8600 | Total Loss: 0.008428 | Recon Loss: 0.007602 | Commit Loss: 0.001651 | Perplexity: 1520.196966
2025-09-08 06:12:43,627 Stage: Train 0.5 | Epoch: 5 | Iter: 8800 | Total Loss: 0.008549 | Recon Loss: 0.007740 | Commit Loss: 0.001618 | Perplexity: 1520.168502
2025-09-08 06:14:26,270 Stage: Train 0.5 | Epoch: 5 | Iter: 9000 | Total Loss: 0.008276 | Recon Loss: 0.007475 | Commit Loss: 0.001604 | Perplexity: 1533.609286
2025-09-08 06:16:08,930 Stage: Train 0.5 | Epoch: 5 | Iter: 9200 | Total Loss: 0.008356 | Recon Loss: 0.007515 | Commit Loss: 0.001682 | Perplexity: 1536.364721
2025-09-08 06:17:51,678 Stage: Train 0.5 | Epoch: 5 | Iter: 9400 | Total Loss: 0.007880 | Recon Loss: 0.007082 | Commit Loss: 0.001596 | Perplexity: 1506.296094
2025-09-08 06:19:34,336 Stage: Train 0.5 | Epoch: 5 | Iter: 9600 | Total Loss: 0.008194 | Recon Loss: 0.007404 | Commit Loss: 0.001580 | Perplexity: 1505.014092
Trainning Epoch:   2%|▏         | 6/308 [1:23:19<69:53:46, 833.20s/it]2025-09-08 06:21:16,641 Stage: Train 0.5 | Epoch: 6 | Iter: 9800 | Total Loss: 0.007875 | Recon Loss: 0.007063 | Commit Loss: 0.001624 | Perplexity: 1503.945266
2025-09-08 06:22:59,387 Stage: Train 0.5 | Epoch: 6 | Iter: 10000 | Total Loss: 0.008142 | Recon Loss: 0.007348 | Commit Loss: 0.001587 | Perplexity: 1509.054554
2025-09-08 06:24:42,012 Stage: Train 0.5 | Epoch: 6 | Iter: 10200 | Total Loss: 0.007850 | Recon Loss: 0.007049 | Commit Loss: 0.001602 | Perplexity: 1510.583010
2025-09-08 06:26:24,707 Stage: Train 0.5 | Epoch: 6 | Iter: 10400 | Total Loss: 0.007749 | Recon Loss: 0.006980 | Commit Loss: 0.001539 | Perplexity: 1493.080224
2025-09-08 06:28:07,303 Stage: Train 0.5 | Epoch: 6 | Iter: 10600 | Total Loss: 0.008184 | Recon Loss: 0.007415 | Commit Loss: 0.001538 | Perplexity: 1472.411860
2025-09-08 06:29:49,837 Stage: Train 0.5 | Epoch: 6 | Iter: 10800 | Total Loss: 0.007314 | Recon Loss: 0.006507 | Commit Loss: 0.001612 | Perplexity: 1483.207502
2025-09-08 06:31:32,218 Stage: Train 0.5 | Epoch: 6 | Iter: 11000 | Total Loss: 0.007363 | Recon Loss: 0.006580 | Commit Loss: 0.001565 | Perplexity: 1484.297158
2025-09-08 06:33:14,551 Stage: Train 0.5 | Epoch: 6 | Iter: 11200 | Total Loss: 0.007585 | Recon Loss: 0.006779 | Commit Loss: 0.001613 | Perplexity: 1480.511647
Trainning Epoch:   2%|▏         | 7/308 [1:37:11<69:39:12, 833.07s/it]2025-09-08 06:34:57,039 Stage: Train 0.5 | Epoch: 7 | Iter: 11400 | Total Loss: 0.007250 | Recon Loss: 0.006467 | Commit Loss: 0.001566 | Perplexity: 1483.076612
2025-09-08 06:36:39,593 Stage: Train 0.5 | Epoch: 7 | Iter: 11600 | Total Loss: 0.007403 | Recon Loss: 0.006633 | Commit Loss: 0.001540 | Perplexity: 1472.231000
2025-09-08 06:38:22,023 Stage: Train 0.5 | Epoch: 7 | Iter: 11800 | Total Loss: 0.007270 | Recon Loss: 0.006511 | Commit Loss: 0.001517 | Perplexity: 1488.407055
2025-09-08 06:40:04,711 Stage: Train 0.5 | Epoch: 7 | Iter: 12000 | Total Loss: 0.007125 | Recon Loss: 0.006371 | Commit Loss: 0.001506 | Perplexity: 1472.545878
2025-09-08 06:41:47,577 Stage: Train 0.5 | Epoch: 7 | Iter: 12200 | Total Loss: 0.007072 | Recon Loss: 0.006288 | Commit Loss: 0.001569 | Perplexity: 1474.997191
2025-09-08 06:43:30,103 Stage: Train 0.5 | Epoch: 7 | Iter: 12400 | Total Loss: 0.007007 | Recon Loss: 0.006288 | Commit Loss: 0.001436 | Perplexity: 1464.693351
2025-09-08 06:45:13,179 Stage: Train 0.5 | Epoch: 7 | Iter: 12600 | Total Loss: 0.007366 | Recon Loss: 0.006650 | Commit Loss: 0.001433 | Perplexity: 1454.706804
2025-09-08 06:46:56,117 Stage: Train 0.5 | Epoch: 7 | Iter: 12800 | Total Loss: 0.007095 | Recon Loss: 0.006359 | Commit Loss: 0.001473 | Perplexity: 1465.078582
Trainning Epoch:   3%|▎         | 8/308 [1:51:05<69:26:48, 833.36s/it]2025-09-08 06:48:38,789 Stage: Train 0.5 | Epoch: 8 | Iter: 13000 | Total Loss: 0.007044 | Recon Loss: 0.006321 | Commit Loss: 0.001445 | Perplexity: 1438.701286
2025-09-08 06:50:21,909 Stage: Train 0.5 | Epoch: 8 | Iter: 13200 | Total Loss: 0.006935 | Recon Loss: 0.006242 | Commit Loss: 0.001387 | Perplexity: 1437.591198
2025-09-08 06:52:04,538 Stage: Train 0.5 | Epoch: 8 | Iter: 13400 | Total Loss: 0.006729 | Recon Loss: 0.006025 | Commit Loss: 0.001407 | Perplexity: 1470.968890
2025-09-08 06:53:47,123 Stage: Train 0.5 | Epoch: 8 | Iter: 13600 | Total Loss: 0.007171 | Recon Loss: 0.006465 | Commit Loss: 0.001412 | Perplexity: 1474.811636
2025-09-08 06:55:29,794 Stage: Train 0.5 | Epoch: 8 | Iter: 13800 | Total Loss: 0.006940 | Recon Loss: 0.006280 | Commit Loss: 0.001321 | Perplexity: 1476.681265
2025-09-08 06:57:12,419 Stage: Train 0.5 | Epoch: 8 | Iter: 14000 | Total Loss: 0.006656 | Recon Loss: 0.005936 | Commit Loss: 0.001439 | Perplexity: 1475.443939
2025-09-08 06:58:55,022 Stage: Train 0.5 | Epoch: 8 | Iter: 14200 | Total Loss: 0.006499 | Recon Loss: 0.005822 | Commit Loss: 0.001354 | Perplexity: 1484.374560
2025-09-08 07:00:37,754 Stage: Train 0.5 | Epoch: 8 | Iter: 14400 | Total Loss: 0.006331 | Recon Loss: 0.005635 | Commit Loss: 0.001392 | Perplexity: 1473.355132
2025-09-08 07:02:20,388 Stage: Train 0.5 | Epoch: 8 | Iter: 14600 | Total Loss: 0.006754 | Recon Loss: 0.006074 | Commit Loss: 0.001360 | Perplexity: 1470.391752
Trainning Epoch:   3%|▎         | 9/308 [2:04:59<69:13:50, 833.55s/it]2025-09-08 07:04:03,137 Stage: Train 0.5 | Epoch: 9 | Iter: 14800 | Total Loss: 0.006735 | Recon Loss: 0.006048 | Commit Loss: 0.001375 | Perplexity: 1469.782370
2025-09-08 07:05:45,816 Stage: Train 0.5 | Epoch: 9 | Iter: 15000 | Total Loss: 0.006529 | Recon Loss: 0.005833 | Commit Loss: 0.001392 | Perplexity: 1472.213566
2025-09-08 07:07:28,295 Stage: Train 0.5 | Epoch: 9 | Iter: 15200 | Total Loss: 0.006386 | Recon Loss: 0.005717 | Commit Loss: 0.001338 | Perplexity: 1470.852108
2025-09-08 07:09:10,822 Stage: Train 0.5 | Epoch: 9 | Iter: 15400 | Total Loss: 0.006343 | Recon Loss: 0.005630 | Commit Loss: 0.001426 | Perplexity: 1474.006514
2025-09-08 07:10:53,557 Stage: Train 0.5 | Epoch: 9 | Iter: 15600 | Total Loss: 0.006262 | Recon Loss: 0.005573 | Commit Loss: 0.001379 | Perplexity: 1467.052387
2025-09-08 07:12:36,338 Stage: Train 0.5 | Epoch: 9 | Iter: 15800 | Total Loss: 0.006479 | Recon Loss: 0.005805 | Commit Loss: 0.001347 | Perplexity: 1474.132289
2025-09-08 07:14:19,095 Stage: Train 0.5 | Epoch: 9 | Iter: 16000 | Total Loss: 0.006471 | Recon Loss: 0.005801 | Commit Loss: 0.001340 | Perplexity: 1467.374926
2025-09-08 07:16:01,955 Stage: Train 0.5 | Epoch: 9 | Iter: 16200 | Total Loss: 0.006111 | Recon Loss: 0.005397 | Commit Loss: 0.001429 | Perplexity: 1456.821634
Trainning Epoch:   3%|▎         | 10/308 [2:18:53<69:00:18, 833.62s/it]2025-09-08 07:17:44,644 Stage: Train 0.5 | Epoch: 10 | Iter: 16400 | Total Loss: 0.006248 | Recon Loss: 0.005582 | Commit Loss: 0.001334 | Perplexity: 1462.627283
2025-09-08 07:19:27,448 Stage: Train 0.5 | Epoch: 10 | Iter: 16600 | Total Loss: 0.006310 | Recon Loss: 0.005620 | Commit Loss: 0.001379 | Perplexity: 1465.979890
2025-09-08 07:21:09,400 Stage: Train 0.5 | Epoch: 10 | Iter: 16800 | Total Loss: 0.006212 | Recon Loss: 0.005546 | Commit Loss: 0.001332 | Perplexity: 1467.917017
2025-09-08 07:22:52,097 Stage: Train 0.5 | Epoch: 10 | Iter: 17000 | Total Loss: 0.006208 | Recon Loss: 0.005505 | Commit Loss: 0.001406 | Perplexity: 1474.540746
2025-09-08 07:24:34,842 Stage: Train 0.5 | Epoch: 10 | Iter: 17200 | Total Loss: 0.005953 | Recon Loss: 0.005275 | Commit Loss: 0.001355 | Perplexity: 1458.025880
2025-09-08 07:26:18,059 Stage: Train 0.5 | Epoch: 10 | Iter: 17400 | Total Loss: 0.006114 | Recon Loss: 0.005408 | Commit Loss: 0.001411 | Perplexity: 1458.512789
2025-09-08 07:28:01,133 Stage: Train 0.5 | Epoch: 10 | Iter: 17600 | Total Loss: 0.005911 | Recon Loss: 0.005237 | Commit Loss: 0.001349 | Perplexity: 1460.520725
2025-09-08 07:29:43,932 Stage: Train 0.5 | Epoch: 10 | Iter: 17800 | Total Loss: 0.006224 | Recon Loss: 0.005585 | Commit Loss: 0.001278 | Perplexity: 1443.078351
Trainning Epoch:   4%|▎         | 11/308 [2:32:48<68:47:33, 833.85s/it]2025-09-08 07:31:26,614 Stage: Train 0.5 | Epoch: 11 | Iter: 18000 | Total Loss: 0.005886 | Recon Loss: 0.005245 | Commit Loss: 0.001281 | Perplexity: 1465.370164
2025-09-08 07:33:09,312 Stage: Train 0.5 | Epoch: 11 | Iter: 18200 | Total Loss: 0.006197 | Recon Loss: 0.005539 | Commit Loss: 0.001315 | Perplexity: 1472.011475
2025-09-08 07:34:51,903 Stage: Train 0.5 | Epoch: 11 | Iter: 18400 | Total Loss: 0.005928 | Recon Loss: 0.005290 | Commit Loss: 0.001275 | Perplexity: 1474.753557
2025-09-08 07:36:34,492 Stage: Train 0.5 | Epoch: 11 | Iter: 18600 | Total Loss: 0.006290 | Recon Loss: 0.005660 | Commit Loss: 0.001260 | Perplexity: 1465.428990
2025-09-08 07:38:17,235 Stage: Train 0.5 | Epoch: 11 | Iter: 18800 | Total Loss: 0.005740 | Recon Loss: 0.005104 | Commit Loss: 0.001273 | Perplexity: 1480.405083
2025-09-08 07:39:59,900 Stage: Train 0.5 | Epoch: 11 | Iter: 19000 | Total Loss: 0.005902 | Recon Loss: 0.005257 | Commit Loss: 0.001290 | Perplexity: 1479.046188
2025-09-08 07:41:42,526 Stage: Train 0.5 | Epoch: 11 | Iter: 19200 | Total Loss: 0.006009 | Recon Loss: 0.005362 | Commit Loss: 0.001295 | Perplexity: 1482.960167
2025-09-08 07:43:25,177 Stage: Train 0.5 | Epoch: 11 | Iter: 19400 | Total Loss: 0.005755 | Recon Loss: 0.005113 | Commit Loss: 0.001286 | Perplexity: 1481.798595
Trainning Epoch:   4%|▍         | 12/308 [2:46:41<68:33:08, 833.74s/it]2025-09-08 07:45:07,749 Stage: Train 0.5 | Epoch: 12 | Iter: 19600 | Total Loss: 0.005717 | Recon Loss: 0.005075 | Commit Loss: 0.001285 | Perplexity: 1495.692311
2025-09-08 07:46:50,548 Stage: Train 0.5 | Epoch: 12 | Iter: 19800 | Total Loss: 0.005774 | Recon Loss: 0.005157 | Commit Loss: 0.001234 | Perplexity: 1491.138655
2025-09-08 07:48:33,281 Stage: Train 0.5 | Epoch: 12 | Iter: 20000 | Total Loss: 0.005717 | Recon Loss: 0.005084 | Commit Loss: 0.001267 | Perplexity: 1485.071331
2025-09-08 07:48:33,282 Saving model at iteration 20000
2025-09-08 07:48:34,214 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000
2025-09-08 07:48:34,619 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000/model.safetensors
2025-09-08 07:48:35,009 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000/optimizer.bin
2025-09-08 07:48:35,009 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000/scheduler.bin
2025-09-08 07:48:35,009 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000/sampler.bin
2025-09-08 07:48:35,010 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_13_step_20000/random_states_0.pkl
2025-09-08 07:50:18,982 Stage: Train 0.5 | Epoch: 12 | Iter: 20200 | Total Loss: 0.005623 | Recon Loss: 0.005015 | Commit Loss: 0.001215 | Perplexity: 1483.183557
2025-09-08 07:52:01,636 Stage: Train 0.5 | Epoch: 12 | Iter: 20400 | Total Loss: 0.005708 | Recon Loss: 0.005076 | Commit Loss: 0.001264 | Perplexity: 1491.275436
2025-09-08 07:53:44,327 Stage: Train 0.5 | Epoch: 12 | Iter: 20600 | Total Loss: 0.005765 | Recon Loss: 0.005142 | Commit Loss: 0.001244 | Perplexity: 1498.356410
2025-09-08 07:55:26,868 Stage: Train 0.5 | Epoch: 12 | Iter: 20800 | Total Loss: 0.005601 | Recon Loss: 0.004979 | Commit Loss: 0.001244 | Perplexity: 1482.240560
2025-09-08 07:57:09,503 Stage: Train 0.5 | Epoch: 12 | Iter: 21000 | Total Loss: 0.005589 | Recon Loss: 0.004944 | Commit Loss: 0.001289 | Perplexity: 1494.690262
Trainning Epoch:   4%|▍         | 13/308 [3:00:38<68:23:29, 834.61s/it]2025-09-08 07:58:52,114 Stage: Train 0.5 | Epoch: 13 | Iter: 21200 | Total Loss: 0.005576 | Recon Loss: 0.004950 | Commit Loss: 0.001252 | Perplexity: 1485.370114
2025-09-08 08:00:34,671 Stage: Train 0.5 | Epoch: 13 | Iter: 21400 | Total Loss: 0.005941 | Recon Loss: 0.005312 | Commit Loss: 0.001257 | Perplexity: 1476.696949
2025-09-08 08:02:17,298 Stage: Train 0.5 | Epoch: 13 | Iter: 21600 | Total Loss: 0.005347 | Recon Loss: 0.004703 | Commit Loss: 0.001288 | Perplexity: 1489.758015
2025-09-08 08:03:59,718 Stage: Train 0.5 | Epoch: 13 | Iter: 21800 | Total Loss: 0.005429 | Recon Loss: 0.004813 | Commit Loss: 0.001232 | Perplexity: 1488.272465
2025-09-08 08:05:42,353 Stage: Train 0.5 | Epoch: 13 | Iter: 22000 | Total Loss: 0.005652 | Recon Loss: 0.005002 | Commit Loss: 0.001300 | Perplexity: 1496.296944
2025-09-08 08:07:24,718 Stage: Train 0.5 | Epoch: 13 | Iter: 22200 | Total Loss: 0.005595 | Recon Loss: 0.004966 | Commit Loss: 0.001259 | Perplexity: 1475.039429
2025-09-08 08:09:07,453 Stage: Train 0.5 | Epoch: 13 | Iter: 22400 | Total Loss: 0.005456 | Recon Loss: 0.004834 | Commit Loss: 0.001244 | Perplexity: 1476.954836
2025-09-08 08:10:50,185 Stage: Train 0.5 | Epoch: 13 | Iter: 22600 | Total Loss: 0.005394 | Recon Loss: 0.004759 | Commit Loss: 0.001271 | Perplexity: 1481.087629
Trainning Epoch:   5%|▍         | 14/308 [3:14:31<68:07:12, 834.13s/it]2025-09-08 08:12:32,516 Stage: Train 0.5 | Epoch: 14 | Iter: 22800 | Total Loss: 0.005402 | Recon Loss: 0.004762 | Commit Loss: 0.001281 | Perplexity: 1474.534117
2025-09-08 08:14:15,267 Stage: Train 0.5 | Epoch: 14 | Iter: 23000 | Total Loss: 0.005468 | Recon Loss: 0.004829 | Commit Loss: 0.001278 | Perplexity: 1479.705605
2025-09-08 08:15:57,971 Stage: Train 0.5 | Epoch: 14 | Iter: 23200 | Total Loss: 0.005406 | Recon Loss: 0.004771 | Commit Loss: 0.001268 | Perplexity: 1479.975826
2025-09-08 08:17:40,538 Stage: Train 0.5 | Epoch: 14 | Iter: 23400 | Total Loss: 0.005488 | Recon Loss: 0.004831 | Commit Loss: 0.001315 | Perplexity: 1471.903463
2025-09-08 08:19:23,545 Stage: Train 0.5 | Epoch: 14 | Iter: 23600 | Total Loss: 0.005439 | Recon Loss: 0.004827 | Commit Loss: 0.001222 | Perplexity: 1462.955510
2025-09-08 08:21:06,301 Stage: Train 0.5 | Epoch: 14 | Iter: 23800 | Total Loss: 0.005352 | Recon Loss: 0.004708 | Commit Loss: 0.001288 | Perplexity: 1472.987585
2025-09-08 08:22:49,155 Stage: Train 0.5 | Epoch: 14 | Iter: 24000 | Total Loss: 0.005603 | Recon Loss: 0.004961 | Commit Loss: 0.001286 | Perplexity: 1465.502098
2025-09-08 08:24:31,950 Stage: Train 0.5 | Epoch: 14 | Iter: 24200 | Total Loss: 0.005200 | Recon Loss: 0.004587 | Commit Loss: 0.001224 | Perplexity: 1452.837332
Trainning Epoch:   5%|▍         | 15/308 [3:28:25<67:53:18, 834.12s/it]2025-09-08 08:26:14,518 Stage: Train 0.5 | Epoch: 15 | Iter: 24400 | Total Loss: 0.005325 | Recon Loss: 0.004697 | Commit Loss: 0.001257 | Perplexity: 1462.789728
2025-09-08 08:27:57,111 Stage: Train 0.5 | Epoch: 15 | Iter: 24600 | Total Loss: 0.005351 | Recon Loss: 0.004744 | Commit Loss: 0.001214 | Perplexity: 1458.368483
2025-09-08 08:29:39,736 Stage: Train 0.5 | Epoch: 15 | Iter: 24800 | Total Loss: 0.005134 | Recon Loss: 0.004501 | Commit Loss: 0.001266 | Perplexity: 1462.834693
2025-09-08 08:31:22,343 Stage: Train 0.5 | Epoch: 15 | Iter: 25000 | Total Loss: 0.005807 | Recon Loss: 0.005175 | Commit Loss: 0.001264 | Perplexity: 1438.278630
2025-09-08 08:33:04,992 Stage: Train 0.5 | Epoch: 15 | Iter: 25200 | Total Loss: 0.005010 | Recon Loss: 0.004380 | Commit Loss: 0.001259 | Perplexity: 1465.353516
2025-09-08 08:34:47,505 Stage: Train 0.5 | Epoch: 15 | Iter: 25400 | Total Loss: 0.005383 | Recon Loss: 0.004743 | Commit Loss: 0.001279 | Perplexity: 1464.783008
2025-09-08 08:36:30,138 Stage: Train 0.5 | Epoch: 15 | Iter: 25600 | Total Loss: 0.005557 | Recon Loss: 0.004925 | Commit Loss: 0.001265 | Perplexity: 1453.098041
2025-09-08 08:38:12,871 Stage: Train 0.5 | Epoch: 15 | Iter: 25800 | Total Loss: 0.005370 | Recon Loss: 0.004754 | Commit Loss: 0.001231 | Perplexity: 1447.590308
Trainning Epoch:   5%|▌         | 16/308 [3:42:18<67:37:42, 833.77s/it]2025-09-08 08:39:55,176 Stage: Train 0.5 | Epoch: 16 | Iter: 26000 | Total Loss: 0.005482 | Recon Loss: 0.004886 | Commit Loss: 0.001192 | Perplexity: 1442.050429
2025-09-08 08:41:37,954 Stage: Train 0.5 | Epoch: 16 | Iter: 26200 | Total Loss: 0.005045 | Recon Loss: 0.004434 | Commit Loss: 0.001221 | Perplexity: 1449.244410
2025-09-08 08:43:20,351 Stage: Train 0.5 | Epoch: 16 | Iter: 26400 | Total Loss: 0.005116 | Recon Loss: 0.004484 | Commit Loss: 0.001264 | Perplexity: 1451.695549
2025-09-08 08:45:02,928 Stage: Train 0.5 | Epoch: 16 | Iter: 26600 | Total Loss: 0.005118 | Recon Loss: 0.004484 | Commit Loss: 0.001270 | Perplexity: 1452.842314
2025-09-08 08:46:45,656 Stage: Train 0.5 | Epoch: 16 | Iter: 26800 | Total Loss: 0.005119 | Recon Loss: 0.004491 | Commit Loss: 0.001255 | Perplexity: 1443.785988
2025-09-08 08:48:28,250 Stage: Train 0.5 | Epoch: 16 | Iter: 27000 | Total Loss: 0.005305 | Recon Loss: 0.004665 | Commit Loss: 0.001281 | Perplexity: 1459.051641
2025-09-08 08:50:10,750 Stage: Train 0.5 | Epoch: 16 | Iter: 27200 | Total Loss: 0.005449 | Recon Loss: 0.004815 | Commit Loss: 0.001270 | Perplexity: 1431.688151
2025-09-08 08:51:53,480 Stage: Train 0.5 | Epoch: 16 | Iter: 27400 | Total Loss: 0.005231 | Recon Loss: 0.004596 | Commit Loss: 0.001270 | Perplexity: 1447.213369
2025-09-08 08:53:36,111 Stage: Train 0.5 | Epoch: 16 | Iter: 27600 | Total Loss: 0.005148 | Recon Loss: 0.004528 | Commit Loss: 0.001240 | Perplexity: 1446.847496
Trainning Epoch:   6%|▌         | 17/308 [3:56:11<67:22:48, 833.57s/it]2025-09-08 08:55:18,834 Stage: Train 0.5 | Epoch: 17 | Iter: 27800 | Total Loss: 0.005007 | Recon Loss: 0.004342 | Commit Loss: 0.001329 | Perplexity: 1453.800377
2025-09-08 08:57:01,656 Stage: Train 0.5 | Epoch: 17 | Iter: 28000 | Total Loss: 0.005105 | Recon Loss: 0.004483 | Commit Loss: 0.001244 | Perplexity: 1451.103486
2025-09-08 08:58:44,289 Stage: Train 0.5 | Epoch: 17 | Iter: 28200 | Total Loss: 0.005009 | Recon Loss: 0.004382 | Commit Loss: 0.001253 | Perplexity: 1446.068920
2025-09-08 09:00:26,870 Stage: Train 0.5 | Epoch: 17 | Iter: 28400 | Total Loss: 0.005041 | Recon Loss: 0.004402 | Commit Loss: 0.001279 | Perplexity: 1442.884593
2025-09-08 09:02:09,409 Stage: Train 0.5 | Epoch: 17 | Iter: 28600 | Total Loss: 0.005086 | Recon Loss: 0.004454 | Commit Loss: 0.001264 | Perplexity: 1437.173707
2025-09-08 09:03:52,097 Stage: Train 0.5 | Epoch: 17 | Iter: 28800 | Total Loss: 0.004969 | Recon Loss: 0.004315 | Commit Loss: 0.001308 | Perplexity: 1438.370530
2025-09-08 09:05:30,439 Stage: Train 0.5 | Epoch: 17 | Iter: 29000 | Total Loss: 0.004974 | Recon Loss: 0.004350 | Commit Loss: 0.001248 | Perplexity: 1438.815581
2025-09-08 09:06:17,210 Stage: Train 0.5 | Epoch: 17 | Iter: 29200 | Total Loss: 0.004962 | Recon Loss: 0.004330 | Commit Loss: 0.001266 | Perplexity: 1450.546777
Trainning Epoch:   6%|▌         | 18/308 [4:08:55<65:28:43, 812.84s/it]2025-09-08 09:07:03,924 Stage: Train 0.5 | Epoch: 18 | Iter: 29400 | Total Loss: 0.005196 | Recon Loss: 0.004561 | Commit Loss: 0.001270 | Perplexity: 1432.647516
2025-09-08 09:07:50,531 Stage: Train 0.5 | Epoch: 18 | Iter: 29600 | Total Loss: 0.005081 | Recon Loss: 0.004461 | Commit Loss: 0.001239 | Perplexity: 1421.520951
2025-09-08 09:08:37,287 Stage: Train 0.5 | Epoch: 18 | Iter: 29800 | Total Loss: 0.004884 | Recon Loss: 0.004242 | Commit Loss: 0.001285 | Perplexity: 1453.638931
2025-09-08 09:09:23,988 Stage: Train 0.5 | Epoch: 18 | Iter: 30000 | Total Loss: 0.004802 | Recon Loss: 0.004153 | Commit Loss: 0.001297 | Perplexity: 1445.749956
2025-09-08 09:10:10,615 Stage: Train 0.5 | Epoch: 18 | Iter: 30200 | Total Loss: 0.005212 | Recon Loss: 0.004573 | Commit Loss: 0.001278 | Perplexity: 1433.217077
2025-09-08 09:10:57,316 Stage: Train 0.5 | Epoch: 18 | Iter: 30400 | Total Loss: 0.005009 | Recon Loss: 0.004354 | Commit Loss: 0.001310 | Perplexity: 1442.481911
2025-09-08 09:11:44,399 Stage: Train 0.5 | Epoch: 18 | Iter: 30600 | Total Loss: 0.004965 | Recon Loss: 0.004338 | Commit Loss: 0.001254 | Perplexity: 1429.366281
2025-09-08 09:13:21,424 Stage: Train 0.5 | Epoch: 18 | Iter: 30800 | Total Loss: 0.004862 | Recon Loss: 0.004240 | Commit Loss: 0.001244 | Perplexity: 1437.702805
Trainning Epoch:   6%|▌         | 19/308 [4:16:21<56:23:38, 702.49s/it]2025-09-08 09:15:04,133 Stage: Train 0.5 | Epoch: 19 | Iter: 31000 | Total Loss: 0.005190 | Recon Loss: 0.004558 | Commit Loss: 0.001264 | Perplexity: 1421.556794
2025-09-08 09:16:46,923 Stage: Train 0.5 | Epoch: 19 | Iter: 31200 | Total Loss: 0.004997 | Recon Loss: 0.004365 | Commit Loss: 0.001262 | Perplexity: 1431.703332
2025-09-08 09:18:29,723 Stage: Train 0.5 | Epoch: 19 | Iter: 31400 | Total Loss: 0.004782 | Recon Loss: 0.004157 | Commit Loss: 0.001250 | Perplexity: 1442.410862
2025-09-08 09:20:12,395 Stage: Train 0.5 | Epoch: 19 | Iter: 31600 | Total Loss: 0.004879 | Recon Loss: 0.004240 | Commit Loss: 0.001279 | Perplexity: 1449.097214
2025-09-08 09:21:55,214 Stage: Train 0.5 | Epoch: 19 | Iter: 31800 | Total Loss: 0.005174 | Recon Loss: 0.004528 | Commit Loss: 0.001293 | Perplexity: 1431.307538
2025-09-08 09:23:38,093 Stage: Train 0.5 | Epoch: 19 | Iter: 32000 | Total Loss: 0.005082 | Recon Loss: 0.004420 | Commit Loss: 0.001325 | Perplexity: 1430.139255
2025-09-08 09:25:21,060 Stage: Train 0.5 | Epoch: 19 | Iter: 32200 | Total Loss: 0.004787 | Recon Loss: 0.004184 | Commit Loss: 0.001207 | Perplexity: 1421.903076
2025-09-08 09:27:03,682 Stage: Train 0.5 | Epoch: 19 | Iter: 32400 | Total Loss: 0.005028 | Recon Loss: 0.004398 | Commit Loss: 0.001260 | Perplexity: 1427.225408
Trainning Epoch:   6%|▋         | 20/308 [4:30:16<59:22:29, 742.19s/it]2025-09-08 09:28:46,359 Stage: Train 0.5 | Epoch: 20 | Iter: 32600 | Total Loss: 0.004903 | Recon Loss: 0.004287 | Commit Loss: 0.001231 | Perplexity: 1432.845002
2025-09-08 09:30:29,240 Stage: Train 0.5 | Epoch: 20 | Iter: 32800 | Total Loss: 0.004816 | Recon Loss: 0.004169 | Commit Loss: 0.001295 | Perplexity: 1445.823436
2025-09-08 09:32:12,417 Stage: Train 0.5 | Epoch: 20 | Iter: 33000 | Total Loss: 0.004935 | Recon Loss: 0.004318 | Commit Loss: 0.001234 | Perplexity: 1427.944675
2025-09-08 09:33:55,329 Stage: Train 0.5 | Epoch: 20 | Iter: 33200 | Total Loss: 0.004856 | Recon Loss: 0.004194 | Commit Loss: 0.001324 | Perplexity: 1420.522995
2025-09-08 09:35:38,765 Stage: Train 0.5 | Epoch: 20 | Iter: 33400 | Total Loss: 0.004770 | Recon Loss: 0.004160 | Commit Loss: 0.001221 | Perplexity: 1436.281512
2025-09-08 09:37:21,894 Stage: Train 0.5 | Epoch: 20 | Iter: 33600 | Total Loss: 0.004869 | Recon Loss: 0.004241 | Commit Loss: 0.001257 | Perplexity: 1433.980119
2025-09-08 09:39:05,161 Stage: Train 0.5 | Epoch: 20 | Iter: 33800 | Total Loss: 0.004885 | Recon Loss: 0.004268 | Commit Loss: 0.001234 | Perplexity: 1436.971932
2025-09-08 09:40:48,487 Stage: Train 0.5 | Epoch: 20 | Iter: 34000 | Total Loss: 0.004741 | Recon Loss: 0.004141 | Commit Loss: 0.001201 | Perplexity: 1442.612784
Trainning Epoch:   7%|▋         | 21/308 [4:44:13<61:26:53, 770.78s/it]2025-09-08 09:42:31,728 Stage: Train 0.5 | Epoch: 21 | Iter: 34200 | Total Loss: 0.004742 | Recon Loss: 0.004125 | Commit Loss: 0.001233 | Perplexity: 1438.385415
2025-09-08 09:44:15,171 Stage: Train 0.5 | Epoch: 21 | Iter: 34400 | Total Loss: 0.004784 | Recon Loss: 0.004181 | Commit Loss: 0.001206 | Perplexity: 1463.533018
2025-09-08 09:45:58,386 Stage: Train 0.5 | Epoch: 21 | Iter: 34600 | Total Loss: 0.004658 | Recon Loss: 0.004047 | Commit Loss: 0.001222 | Perplexity: 1475.367606
2025-09-08 09:47:41,445 Stage: Train 0.5 | Epoch: 21 | Iter: 34800 | Total Loss: 0.004723 | Recon Loss: 0.004105 | Commit Loss: 0.001236 | Perplexity: 1463.800477
2025-09-08 09:49:24,362 Stage: Train 0.5 | Epoch: 21 | Iter: 35000 | Total Loss: 0.004622 | Recon Loss: 0.004026 | Commit Loss: 0.001192 | Perplexity: 1467.079851
2025-09-08 09:51:07,612 Stage: Train 0.5 | Epoch: 21 | Iter: 35200 | Total Loss: 0.005080 | Recon Loss: 0.004459 | Commit Loss: 0.001241 | Perplexity: 1453.002642
2025-09-08 09:52:50,616 Stage: Train 0.5 | Epoch: 21 | Iter: 35400 | Total Loss: 0.004630 | Recon Loss: 0.004042 | Commit Loss: 0.001176 | Perplexity: 1462.677341
2025-09-08 09:54:33,533 Stage: Train 0.5 | Epoch: 21 | Iter: 35600 | Total Loss: 0.004652 | Recon Loss: 0.004046 | Commit Loss: 0.001213 | Perplexity: 1463.066561
Trainning Epoch:   7%|▋         | 22/308 [4:58:10<62:49:00, 790.70s/it]2025-09-08 09:56:16,398 Stage: Train 0.5 | Epoch: 22 | Iter: 35800 | Total Loss: 0.004781 | Recon Loss: 0.004172 | Commit Loss: 0.001218 | Perplexity: 1462.183121
2025-09-08 09:57:59,330 Stage: Train 0.5 | Epoch: 22 | Iter: 36000 | Total Loss: 0.004683 | Recon Loss: 0.004065 | Commit Loss: 0.001237 | Perplexity: 1461.292779
2025-09-08 09:59:42,362 Stage: Train 0.5 | Epoch: 22 | Iter: 36200 | Total Loss: 0.004733 | Recon Loss: 0.004150 | Commit Loss: 0.001166 | Perplexity: 1462.415173
2025-09-08 10:01:25,141 Stage: Train 0.5 | Epoch: 22 | Iter: 36400 | Total Loss: 0.004736 | Recon Loss: 0.004142 | Commit Loss: 0.001187 | Perplexity: 1465.858741
2025-09-08 10:03:08,089 Stage: Train 0.5 | Epoch: 22 | Iter: 36600 | Total Loss: 0.004614 | Recon Loss: 0.004039 | Commit Loss: 0.001151 | Perplexity: 1456.609444
2025-09-08 10:04:51,114 Stage: Train 0.5 | Epoch: 22 | Iter: 36800 | Total Loss: 0.004632 | Recon Loss: 0.004010 | Commit Loss: 0.001244 | Perplexity: 1467.888912
2025-09-08 10:06:33,955 Stage: Train 0.5 | Epoch: 22 | Iter: 37000 | Total Loss: 0.004618 | Recon Loss: 0.004035 | Commit Loss: 0.001165 | Perplexity: 1457.936961
2025-09-08 10:08:17,019 Stage: Train 0.5 | Epoch: 22 | Iter: 37200 | Total Loss: 0.004452 | Recon Loss: 0.003846 | Commit Loss: 0.001210 | Perplexity: 1472.652066
Trainning Epoch:   7%|▋         | 23/308 [5:12:06<63:40:17, 804.27s/it]2025-09-08 10:10:00,034 Stage: Train 0.5 | Epoch: 23 | Iter: 37400 | Total Loss: 0.004627 | Recon Loss: 0.004049 | Commit Loss: 0.001157 | Perplexity: 1450.798445
2025-09-08 10:11:43,118 Stage: Train 0.5 | Epoch: 23 | Iter: 37600 | Total Loss: 0.004654 | Recon Loss: 0.004052 | Commit Loss: 0.001203 | Perplexity: 1466.855818
2025-09-08 10:13:26,191 Stage: Train 0.5 | Epoch: 23 | Iter: 37800 | Total Loss: 0.004521 | Recon Loss: 0.003927 | Commit Loss: 0.001188 | Perplexity: 1468.966291
2025-09-08 10:15:09,358 Stage: Train 0.5 | Epoch: 23 | Iter: 38000 | Total Loss: 0.004543 | Recon Loss: 0.003961 | Commit Loss: 0.001164 | Perplexity: 1454.170148
2025-09-08 10:16:52,194 Stage: Train 0.5 | Epoch: 23 | Iter: 38200 | Total Loss: 0.004584 | Recon Loss: 0.003997 | Commit Loss: 0.001175 | Perplexity: 1466.589803
2025-09-08 10:18:35,645 Stage: Train 0.5 | Epoch: 23 | Iter: 38400 | Total Loss: 0.004478 | Recon Loss: 0.003898 | Commit Loss: 0.001160 | Perplexity: 1471.539197
2025-09-08 10:20:18,715 Stage: Train 0.5 | Epoch: 23 | Iter: 38600 | Total Loss: 0.004773 | Recon Loss: 0.004147 | Commit Loss: 0.001252 | Perplexity: 1458.882456
2025-09-08 10:22:01,691 Stage: Train 0.5 | Epoch: 23 | Iter: 38800 | Total Loss: 0.004612 | Recon Loss: 0.004039 | Commit Loss: 0.001147 | Perplexity: 1454.530490
Trainning Epoch:   8%|▊         | 24/308 [5:26:03<64:13:21, 814.09s/it]2025-09-08 10:23:44,670 Stage: Train 0.5 | Epoch: 24 | Iter: 39000 | Total Loss: 0.004486 | Recon Loss: 0.003908 | Commit Loss: 0.001156 | Perplexity: 1465.497841
2025-09-08 10:25:27,601 Stage: Train 0.5 | Epoch: 24 | Iter: 39200 | Total Loss: 0.004552 | Recon Loss: 0.003956 | Commit Loss: 0.001192 | Perplexity: 1463.600213
2025-09-08 10:27:10,699 Stage: Train 0.5 | Epoch: 24 | Iter: 39400 | Total Loss: 0.004420 | Recon Loss: 0.003857 | Commit Loss: 0.001127 | Perplexity: 1462.300157
2025-09-08 10:28:53,807 Stage: Train 0.5 | Epoch: 24 | Iter: 39600 | Total Loss: 0.004640 | Recon Loss: 0.004051 | Commit Loss: 0.001178 | Perplexity: 1465.201518
2025-09-08 10:30:36,707 Stage: Train 0.5 | Epoch: 24 | Iter: 39800 | Total Loss: 0.004491 | Recon Loss: 0.003918 | Commit Loss: 0.001148 | Perplexity: 1457.904904
2025-09-08 10:32:19,942 Stage: Train 0.5 | Epoch: 24 | Iter: 40000 | Total Loss: 0.004450 | Recon Loss: 0.003867 | Commit Loss: 0.001166 | Perplexity: 1471.403498
2025-09-08 10:32:19,942 Saving model at iteration 40000
2025-09-08 10:32:20,749 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000
2025-09-08 10:32:21,158 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000/model.safetensors
2025-09-08 10:32:21,555 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000/optimizer.bin
2025-09-08 10:32:21,555 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000/scheduler.bin
2025-09-08 10:32:21,555 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000/sampler.bin
2025-09-08 10:32:21,556 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_25_step_40000/random_states_0.pkl
2025-09-08 10:34:05,321 Stage: Train 0.5 | Epoch: 24 | Iter: 40200 | Total Loss: 0.004434 | Recon Loss: 0.003859 | Commit Loss: 0.001151 | Perplexity: 1464.490444
2025-09-08 10:35:48,272 Stage: Train 0.5 | Epoch: 24 | Iter: 40400 | Total Loss: 0.004722 | Recon Loss: 0.004147 | Commit Loss: 0.001151 | Perplexity: 1448.391978
2025-09-08 10:37:30,988 Stage: Train 0.5 | Epoch: 24 | Iter: 40600 | Total Loss: 0.004397 | Recon Loss: 0.003824 | Commit Loss: 0.001147 | Perplexity: 1469.945098
Trainning Epoch:   8%|▊         | 25/308 [5:40:02<64:34:37, 821.47s/it]2025-09-08 10:39:13,884 Stage: Train 0.5 | Epoch: 25 | Iter: 40800 | Total Loss: 0.004517 | Recon Loss: 0.003936 | Commit Loss: 0.001161 | Perplexity: 1467.016297
2025-09-08 10:40:56,697 Stage: Train 0.5 | Epoch: 25 | Iter: 41000 | Total Loss: 0.004525 | Recon Loss: 0.003908 | Commit Loss: 0.001234 | Perplexity: 1464.899900
2025-09-08 10:42:39,721 Stage: Train 0.5 | Epoch: 25 | Iter: 41200 | Total Loss: 0.004373 | Recon Loss: 0.003809 | Commit Loss: 0.001129 | Perplexity: 1453.366426
2025-09-08 10:44:22,838 Stage: Train 0.5 | Epoch: 25 | Iter: 41400 | Total Loss: 0.004437 | Recon Loss: 0.003876 | Commit Loss: 0.001122 | Perplexity: 1468.348104
2025-09-08 10:46:05,930 Stage: Train 0.5 | Epoch: 25 | Iter: 41600 | Total Loss: 0.004552 | Recon Loss: 0.003987 | Commit Loss: 0.001130 | Perplexity: 1453.651246
2025-09-08 10:47:49,096 Stage: Train 0.5 | Epoch: 25 | Iter: 41800 | Total Loss: 0.004373 | Recon Loss: 0.003784 | Commit Loss: 0.001178 | Perplexity: 1470.653307
2025-09-08 10:49:32,432 Stage: Train 0.5 | Epoch: 25 | Iter: 42000 | Total Loss: 0.004362 | Recon Loss: 0.003798 | Commit Loss: 0.001128 | Perplexity: 1449.930883
2025-09-08 10:51:15,859 Stage: Train 0.5 | Epoch: 25 | Iter: 42200 | Total Loss: 0.004381 | Recon Loss: 0.003791 | Commit Loss: 0.001179 | Perplexity: 1468.176926
Trainning Epoch:   8%|▊         | 26/308 [5:53:59<64:43:00, 826.17s/it]2025-09-08 10:52:59,106 Stage: Train 0.5 | Epoch: 26 | Iter: 42400 | Total Loss: 0.004448 | Recon Loss: 0.003868 | Commit Loss: 0.001159 | Perplexity: 1465.607554
2025-09-08 10:54:42,655 Stage: Train 0.5 | Epoch: 26 | Iter: 42600 | Total Loss: 0.004341 | Recon Loss: 0.003777 | Commit Loss: 0.001128 | Perplexity: 1451.472421
2025-09-08 10:56:26,212 Stage: Train 0.5 | Epoch: 26 | Iter: 42800 | Total Loss: 0.004541 | Recon Loss: 0.003958 | Commit Loss: 0.001165 | Perplexity: 1459.584821
2025-09-08 10:58:09,318 Stage: Train 0.5 | Epoch: 26 | Iter: 43000 | Total Loss: 0.004276 | Recon Loss: 0.003707 | Commit Loss: 0.001136 | Perplexity: 1466.770886
2025-09-08 10:59:52,375 Stage: Train 0.5 | Epoch: 26 | Iter: 43200 | Total Loss: 0.004408 | Recon Loss: 0.003842 | Commit Loss: 0.001132 | Perplexity: 1464.250580
2025-09-08 11:01:35,329 Stage: Train 0.5 | Epoch: 26 | Iter: 43400 | Total Loss: 0.004342 | Recon Loss: 0.003751 | Commit Loss: 0.001183 | Perplexity: 1465.593851
2025-09-08 11:03:18,075 Stage: Train 0.5 | Epoch: 26 | Iter: 43600 | Total Loss: 0.004596 | Recon Loss: 0.003983 | Commit Loss: 0.001226 | Perplexity: 1454.262343
2025-09-08 11:05:00,980 Stage: Train 0.5 | Epoch: 26 | Iter: 43800 | Total Loss: 0.004383 | Recon Loss: 0.003817 | Commit Loss: 0.001133 | Perplexity: 1451.681798
Trainning Epoch:   9%|▉         | 27/308 [6:07:57<64:45:35, 829.66s/it]2025-09-08 11:06:44,007 Stage: Train 0.5 | Epoch: 27 | Iter: 44000 | Total Loss: 0.004547 | Recon Loss: 0.003979 | Commit Loss: 0.001136 | Perplexity: 1448.993885
2025-09-08 11:08:27,010 Stage: Train 0.5 | Epoch: 27 | Iter: 44200 | Total Loss: 0.004346 | Recon Loss: 0.003758 | Commit Loss: 0.001176 | Perplexity: 1456.356641
2025-09-08 11:10:09,874 Stage: Train 0.5 | Epoch: 27 | Iter: 44400 | Total Loss: 0.004312 | Recon Loss: 0.003750 | Commit Loss: 0.001125 | Perplexity: 1459.063702
2025-09-08 11:11:52,979 Stage: Train 0.5 | Epoch: 27 | Iter: 44600 | Total Loss: 0.004270 | Recon Loss: 0.003694 | Commit Loss: 0.001151 | Perplexity: 1465.014638
2025-09-08 11:13:35,977 Stage: Train 0.5 | Epoch: 27 | Iter: 44800 | Total Loss: 0.004292 | Recon Loss: 0.003719 | Commit Loss: 0.001146 | Perplexity: 1462.275103
2025-09-08 11:15:19,270 Stage: Train 0.5 | Epoch: 27 | Iter: 45000 | Total Loss: 0.004355 | Recon Loss: 0.003790 | Commit Loss: 0.001129 | Perplexity: 1459.624778
2025-09-08 11:17:02,455 Stage: Train 0.5 | Epoch: 27 | Iter: 45200 | Total Loss: 0.004354 | Recon Loss: 0.003765 | Commit Loss: 0.001180 | Perplexity: 1457.819951
2025-09-08 11:18:45,767 Stage: Train 0.5 | Epoch: 27 | Iter: 45400 | Total Loss: 0.004419 | Recon Loss: 0.003850 | Commit Loss: 0.001138 | Perplexity: 1460.033460
Trainning Epoch:   9%|▉         | 28/308 [6:21:54<64:41:46, 831.81s/it]2025-09-08 11:20:28,692 Stage: Train 0.5 | Epoch: 28 | Iter: 45600 | Total Loss: 0.004271 | Recon Loss: 0.003711 | Commit Loss: 0.001120 | Perplexity: 1451.715418
2025-09-08 11:22:11,949 Stage: Train 0.5 | Epoch: 28 | Iter: 45800 | Total Loss: 0.004426 | Recon Loss: 0.003854 | Commit Loss: 0.001144 | Perplexity: 1458.736652
2025-09-08 11:23:55,546 Stage: Train 0.5 | Epoch: 28 | Iter: 46000 | Total Loss: 0.004374 | Recon Loss: 0.003779 | Commit Loss: 0.001189 | Perplexity: 1461.764042
2025-09-08 11:25:38,892 Stage: Train 0.5 | Epoch: 28 | Iter: 46200 | Total Loss: 0.004248 | Recon Loss: 0.003675 | Commit Loss: 0.001146 | Perplexity: 1462.021105
2025-09-08 11:27:22,133 Stage: Train 0.5 | Epoch: 28 | Iter: 46400 | Total Loss: 0.004367 | Recon Loss: 0.003789 | Commit Loss: 0.001155 | Perplexity: 1467.302802
2025-09-08 11:29:05,598 Stage: Train 0.5 | Epoch: 28 | Iter: 46600 | Total Loss: 0.004147 | Recon Loss: 0.003581 | Commit Loss: 0.001131 | Perplexity: 1453.123517
2025-09-08 11:30:48,608 Stage: Train 0.5 | Epoch: 28 | Iter: 46800 | Total Loss: 0.004348 | Recon Loss: 0.003764 | Commit Loss: 0.001167 | Perplexity: 1455.346998
2025-09-08 11:32:31,349 Stage: Train 0.5 | Epoch: 28 | Iter: 47000 | Total Loss: 0.004294 | Recon Loss: 0.003733 | Commit Loss: 0.001122 | Perplexity: 1444.696469
Trainning Epoch:   9%|▉         | 29/308 [6:35:51<64:36:23, 833.63s/it]2025-09-08 11:34:14,351 Stage: Train 0.5 | Epoch: 29 | Iter: 47200 | Total Loss: 0.004206 | Recon Loss: 0.003644 | Commit Loss: 0.001125 | Perplexity: 1467.454283
2025-09-08 11:35:57,242 Stage: Train 0.5 | Epoch: 29 | Iter: 47400 | Total Loss: 0.004227 | Recon Loss: 0.003657 | Commit Loss: 0.001139 | Perplexity: 1456.428369
2025-09-08 11:37:40,236 Stage: Train 0.5 | Epoch: 29 | Iter: 47600 | Total Loss: 0.004271 | Recon Loss: 0.003677 | Commit Loss: 0.001189 | Perplexity: 1461.970872
2025-09-08 11:39:23,101 Stage: Train 0.5 | Epoch: 29 | Iter: 47800 | Total Loss: 0.004413 | Recon Loss: 0.003843 | Commit Loss: 0.001139 | Perplexity: 1452.191508
2025-09-08 11:41:06,084 Stage: Train 0.5 | Epoch: 29 | Iter: 48000 | Total Loss: 0.004299 | Recon Loss: 0.003745 | Commit Loss: 0.001109 | Perplexity: 1455.119607
2025-09-08 11:42:49,019 Stage: Train 0.5 | Epoch: 29 | Iter: 48200 | Total Loss: 0.004151 | Recon Loss: 0.003571 | Commit Loss: 0.001160 | Perplexity: 1465.477956
2025-09-08 11:44:31,966 Stage: Train 0.5 | Epoch: 29 | Iter: 48400 | Total Loss: 0.004200 | Recon Loss: 0.003628 | Commit Loss: 0.001145 | Perplexity: 1458.851461
2025-09-08 11:46:14,940 Stage: Train 0.5 | Epoch: 29 | Iter: 48600 | Total Loss: 0.004233 | Recon Loss: 0.003657 | Commit Loss: 0.001151 | Perplexity: 1452.010778
Trainning Epoch:  10%|▉         | 30/308 [6:49:47<64:25:32, 834.29s/it]2025-09-08 11:47:57,437 Stage: Train 0.5 | Epoch: 30 | Iter: 48800 | Total Loss: 0.004386 | Recon Loss: 0.003781 | Commit Loss: 0.001211 | Perplexity: 1462.824756
2025-09-08 11:49:40,274 Stage: Train 0.5 | Epoch: 30 | Iter: 49000 | Total Loss: 0.004205 | Recon Loss: 0.003624 | Commit Loss: 0.001162 | Perplexity: 1456.570052
2025-09-08 11:51:23,150 Stage: Train 0.5 | Epoch: 30 | Iter: 49200 | Total Loss: 0.004193 | Recon Loss: 0.003627 | Commit Loss: 0.001131 | Perplexity: 1458.688952
2025-09-08 11:53:06,141 Stage: Train 0.5 | Epoch: 30 | Iter: 49400 | Total Loss: 0.004269 | Recon Loss: 0.003683 | Commit Loss: 0.001172 | Perplexity: 1459.098985
2025-09-08 11:54:49,088 Stage: Train 0.5 | Epoch: 30 | Iter: 49600 | Total Loss: 0.004306 | Recon Loss: 0.003744 | Commit Loss: 0.001125 | Perplexity: 1435.615569
2025-09-08 11:56:32,118 Stage: Train 0.5 | Epoch: 30 | Iter: 49800 | Total Loss: 0.004105 | Recon Loss: 0.003531 | Commit Loss: 0.001149 | Perplexity: 1459.421002
2025-09-08 11:58:15,195 Stage: Train 0.5 | Epoch: 30 | Iter: 50000 | Total Loss: 0.004307 | Recon Loss: 0.003732 | Commit Loss: 0.001150 | Perplexity: 1460.035108
2025-09-08 11:59:58,241 Stage: Train 0.5 | Epoch: 30 | Iter: 50200 | Total Loss: 0.004196 | Recon Loss: 0.003625 | Commit Loss: 0.001142 | Perplexity: 1455.920980
Trainning Epoch:  10%|█         | 31/308 [7:03:43<64:13:44, 834.75s/it]2025-09-08 12:01:41,018 Stage: Train 0.5 | Epoch: 31 | Iter: 50400 | Total Loss: 0.004144 | Recon Loss: 0.003557 | Commit Loss: 0.001176 | Perplexity: 1466.303548
2025-09-08 12:03:24,097 Stage: Train 0.5 | Epoch: 31 | Iter: 50600 | Total Loss: 0.004179 | Recon Loss: 0.003593 | Commit Loss: 0.001172 | Perplexity: 1457.021578
2025-09-08 12:05:06,987 Stage: Train 0.5 | Epoch: 31 | Iter: 50800 | Total Loss: 0.004304 | Recon Loss: 0.003740 | Commit Loss: 0.001128 | Perplexity: 1451.362059
2025-09-08 12:06:49,893 Stage: Train 0.5 | Epoch: 31 | Iter: 51000 | Total Loss: 0.004126 | Recon Loss: 0.003549 | Commit Loss: 0.001153 | Perplexity: 1454.775455
2025-09-08 12:08:32,604 Stage: Train 0.5 | Epoch: 31 | Iter: 51200 | Total Loss: 0.004095 | Recon Loss: 0.003523 | Commit Loss: 0.001144 | Perplexity: 1457.811721
2025-09-08 12:10:15,628 Stage: Train 0.5 | Epoch: 31 | Iter: 51400 | Total Loss: 0.004156 | Recon Loss: 0.003588 | Commit Loss: 0.001135 | Perplexity: 1450.094606
2025-09-08 12:11:58,601 Stage: Train 0.5 | Epoch: 31 | Iter: 51600 | Total Loss: 0.004166 | Recon Loss: 0.003585 | Commit Loss: 0.001161 | Perplexity: 1455.571994
2025-09-08 12:13:41,356 Stage: Train 0.5 | Epoch: 31 | Iter: 51800 | Total Loss: 0.004677 | Recon Loss: 0.004080 | Commit Loss: 0.001194 | Perplexity: 1440.795931
Trainning Epoch:  10%|█         | 32/308 [7:17:38<64:00:36, 834.91s/it]2025-09-08 12:15:24,008 Stage: Train 0.5 | Epoch: 32 | Iter: 52000 | Total Loss: 0.004027 | Recon Loss: 0.003459 | Commit Loss: 0.001137 | Perplexity: 1445.337156
2025-09-08 12:17:06,884 Stage: Train 0.5 | Epoch: 32 | Iter: 52200 | Total Loss: 0.004298 | Recon Loss: 0.003747 | Commit Loss: 0.001103 | Perplexity: 1443.852624
2025-09-08 12:18:49,616 Stage: Train 0.5 | Epoch: 32 | Iter: 52400 | Total Loss: 0.004106 | Recon Loss: 0.003542 | Commit Loss: 0.001129 | Perplexity: 1451.349307
2025-09-08 12:20:32,173 Stage: Train 0.5 | Epoch: 32 | Iter: 52600 | Total Loss: 0.004282 | Recon Loss: 0.003679 | Commit Loss: 0.001207 | Perplexity: 1464.793824
2025-09-08 12:22:15,245 Stage: Train 0.5 | Epoch: 32 | Iter: 52800 | Total Loss: 0.004203 | Recon Loss: 0.003645 | Commit Loss: 0.001116 | Perplexity: 1442.324292
2025-09-08 12:23:58,057 Stage: Train 0.5 | Epoch: 32 | Iter: 53000 | Total Loss: 0.004184 | Recon Loss: 0.003615 | Commit Loss: 0.001139 | Perplexity: 1449.393938
2025-09-08 12:25:40,862 Stage: Train 0.5 | Epoch: 32 | Iter: 53200 | Total Loss: 0.004069 | Recon Loss: 0.003502 | Commit Loss: 0.001135 | Perplexity: 1448.160999
2025-09-08 12:27:23,639 Stage: Train 0.5 | Epoch: 32 | Iter: 53400 | Total Loss: 0.004113 | Recon Loss: 0.003549 | Commit Loss: 0.001127 | Perplexity: 1450.091473
Trainning Epoch:  11%|█         | 33/308 [7:31:33<63:46:24, 834.85s/it]2025-09-08 12:29:06,419 Stage: Train 0.5 | Epoch: 33 | Iter: 53600 | Total Loss: 0.004129 | Recon Loss: 0.003542 | Commit Loss: 0.001175 | Perplexity: 1459.294764
2025-09-08 12:30:49,340 Stage: Train 0.5 | Epoch: 33 | Iter: 53800 | Total Loss: 0.004023 | Recon Loss: 0.003455 | Commit Loss: 0.001135 | Perplexity: 1457.653477
2025-09-08 12:32:32,251 Stage: Train 0.5 | Epoch: 33 | Iter: 54000 | Total Loss: 0.004199 | Recon Loss: 0.003620 | Commit Loss: 0.001159 | Perplexity: 1446.272493
2025-09-08 12:34:15,305 Stage: Train 0.5 | Epoch: 33 | Iter: 54200 | Total Loss: 0.004180 | Recon Loss: 0.003583 | Commit Loss: 0.001194 | Perplexity: 1448.691010
2025-09-08 12:35:58,409 Stage: Train 0.5 | Epoch: 33 | Iter: 54400 | Total Loss: 0.004183 | Recon Loss: 0.003621 | Commit Loss: 0.001124 | Perplexity: 1443.410024
2025-09-08 12:37:41,534 Stage: Train 0.5 | Epoch: 33 | Iter: 54600 | Total Loss: 0.004048 | Recon Loss: 0.003472 | Commit Loss: 0.001151 | Perplexity: 1462.923719
2025-09-08 12:39:24,728 Stage: Train 0.5 | Epoch: 33 | Iter: 54800 | Total Loss: 0.004078 | Recon Loss: 0.003509 | Commit Loss: 0.001138 | Perplexity: 1450.834374
2025-09-08 12:41:07,931 Stage: Train 0.5 | Epoch: 33 | Iter: 55000 | Total Loss: 0.004180 | Recon Loss: 0.003600 | Commit Loss: 0.001159 | Perplexity: 1449.532404
2025-09-08 12:42:50,861 Stage: Train 0.5 | Epoch: 33 | Iter: 55200 | Total Loss: 0.004111 | Recon Loss: 0.003530 | Commit Loss: 0.001161 | Perplexity: 1449.005519
Trainning Epoch:  11%|█         | 34/308 [7:45:30<63:34:54, 835.38s/it]2025-09-08 12:44:33,151 Stage: Train 0.5 | Epoch: 34 | Iter: 55400 | Total Loss: 0.004095 | Recon Loss: 0.003521 | Commit Loss: 0.001149 | Perplexity: 1454.368372
2025-09-08 12:46:15,793 Stage: Train 0.5 | Epoch: 34 | Iter: 55600 | Total Loss: 0.004124 | Recon Loss: 0.003556 | Commit Loss: 0.001137 | Perplexity: 1448.910435
2025-09-08 12:47:58,683 Stage: Train 0.5 | Epoch: 34 | Iter: 55800 | Total Loss: 0.004194 | Recon Loss: 0.003626 | Commit Loss: 0.001136 | Perplexity: 1449.359993
2025-09-08 12:49:41,220 Stage: Train 0.5 | Epoch: 34 | Iter: 56000 | Total Loss: 0.004133 | Recon Loss: 0.003548 | Commit Loss: 0.001170 | Perplexity: 1446.202465
2025-09-08 12:51:23,587 Stage: Train 0.5 | Epoch: 34 | Iter: 56200 | Total Loss: 0.004063 | Recon Loss: 0.003478 | Commit Loss: 0.001170 | Perplexity: 1445.157370
2025-09-08 12:53:06,163 Stage: Train 0.5 | Epoch: 34 | Iter: 56400 | Total Loss: 0.004087 | Recon Loss: 0.003522 | Commit Loss: 0.001128 | Perplexity: 1452.775591
2025-09-08 12:54:48,686 Stage: Train 0.5 | Epoch: 34 | Iter: 56600 | Total Loss: 0.003989 | Recon Loss: 0.003410 | Commit Loss: 0.001159 | Perplexity: 1444.243580
2025-09-08 12:56:31,325 Stage: Train 0.5 | Epoch: 34 | Iter: 56800 | Total Loss: 0.004312 | Recon Loss: 0.003706 | Commit Loss: 0.001212 | Perplexity: 1443.297846
Trainning Epoch:  11%|█▏        | 35/308 [7:59:23<63:17:27, 834.60s/it]2025-09-08 12:58:13,765 Stage: Train 0.5 | Epoch: 35 | Iter: 57000 | Total Loss: 0.003962 | Recon Loss: 0.003406 | Commit Loss: 0.001111 | Perplexity: 1444.475295
2025-09-08 12:59:56,450 Stage: Train 0.5 | Epoch: 35 | Iter: 57200 | Total Loss: 0.004077 | Recon Loss: 0.003475 | Commit Loss: 0.001203 | Perplexity: 1447.389551
2025-09-08 13:01:39,000 Stage: Train 0.5 | Epoch: 35 | Iter: 57400 | Total Loss: 0.004074 | Recon Loss: 0.003502 | Commit Loss: 0.001144 | Perplexity: 1449.755379
2025-09-08 13:03:21,431 Stage: Train 0.5 | Epoch: 35 | Iter: 57600 | Total Loss: 0.003985 | Recon Loss: 0.003415 | Commit Loss: 0.001141 | Perplexity: 1443.173231
2025-09-08 13:05:03,850 Stage: Train 0.5 | Epoch: 35 | Iter: 57800 | Total Loss: 0.004147 | Recon Loss: 0.003571 | Commit Loss: 0.001152 | Perplexity: 1431.590801
2025-09-08 13:06:46,303 Stage: Train 0.5 | Epoch: 35 | Iter: 58000 | Total Loss: 0.003969 | Recon Loss: 0.003395 | Commit Loss: 0.001148 | Perplexity: 1453.623036
2025-09-08 13:08:28,945 Stage: Train 0.5 | Epoch: 35 | Iter: 58200 | Total Loss: 0.004023 | Recon Loss: 0.003458 | Commit Loss: 0.001130 | Perplexity: 1455.897284
2025-09-08 13:10:11,638 Stage: Train 0.5 | Epoch: 35 | Iter: 58400 | Total Loss: 0.004119 | Recon Loss: 0.003533 | Commit Loss: 0.001172 | Perplexity: 1444.650096
Trainning Epoch:  12%|█▏        | 36/308 [8:13:15<63:00:52, 834.02s/it]2025-09-08 13:11:54,186 Stage: Train 0.5 | Epoch: 36 | Iter: 58600 | Total Loss: 0.003918 | Recon Loss: 0.003357 | Commit Loss: 0.001122 | Perplexity: 1436.764792
2025-09-08 13:13:37,211 Stage: Train 0.5 | Epoch: 36 | Iter: 58800 | Total Loss: 0.004199 | Recon Loss: 0.003621 | Commit Loss: 0.001157 | Perplexity: 1433.477912
2025-09-08 13:15:20,101 Stage: Train 0.5 | Epoch: 36 | Iter: 59000 | Total Loss: 0.004123 | Recon Loss: 0.003520 | Commit Loss: 0.001205 | Perplexity: 1450.774432
2025-09-08 13:17:02,851 Stage: Train 0.5 | Epoch: 36 | Iter: 59200 | Total Loss: 0.003909 | Recon Loss: 0.003354 | Commit Loss: 0.001111 | Perplexity: 1433.144111
2025-09-08 13:18:44,617 Stage: Train 0.5 | Epoch: 36 | Iter: 59400 | Total Loss: 0.003985 | Recon Loss: 0.003411 | Commit Loss: 0.001148 | Perplexity: 1445.292620
2025-09-08 13:20:26,341 Stage: Train 0.5 | Epoch: 36 | Iter: 59600 | Total Loss: 0.004202 | Recon Loss: 0.003644 | Commit Loss: 0.001114 | Perplexity: 1440.623766
2025-09-08 13:22:08,967 Stage: Train 0.5 | Epoch: 36 | Iter: 59800 | Total Loss: 0.004089 | Recon Loss: 0.003504 | Commit Loss: 0.001169 | Perplexity: 1433.859728
2025-09-08 13:23:52,005 Stage: Train 0.5 | Epoch: 36 | Iter: 60000 | Total Loss: 0.004013 | Recon Loss: 0.003438 | Commit Loss: 0.001152 | Perplexity: 1455.904150
2025-09-08 13:23:52,006 Saving model at iteration 60000
2025-09-08 13:23:52,121 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000
2025-09-08 13:23:52,524 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000/model.safetensors
2025-09-08 13:23:52,914 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000/optimizer.bin
2025-09-08 13:23:52,915 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000/scheduler.bin
2025-09-08 13:23:52,915 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000/sampler.bin
2025-09-08 13:23:52,916 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_37_step_60000/random_states_0.pkl
Trainning Epoch:  12%|█▏        | 37/308 [8:27:11<62:49:04, 834.48s/it]2025-09-08 13:25:37,485 Stage: Train 0.5 | Epoch: 37 | Iter: 60200 | Total Loss: 0.004035 | Recon Loss: 0.003464 | Commit Loss: 0.001142 | Perplexity: 1451.667410
2025-09-08 13:27:20,044 Stage: Train 0.5 | Epoch: 37 | Iter: 60400 | Total Loss: 0.004117 | Recon Loss: 0.003517 | Commit Loss: 0.001200 | Perplexity: 1444.584323
2025-09-08 13:29:02,488 Stage: Train 0.5 | Epoch: 37 | Iter: 60600 | Total Loss: 0.004078 | Recon Loss: 0.003502 | Commit Loss: 0.001153 | Perplexity: 1446.958383
2025-09-08 13:30:45,300 Stage: Train 0.5 | Epoch: 37 | Iter: 60800 | Total Loss: 0.003986 | Recon Loss: 0.003416 | Commit Loss: 0.001139 | Perplexity: 1449.738936
2025-09-08 13:32:28,225 Stage: Train 0.5 | Epoch: 37 | Iter: 61000 | Total Loss: 0.003897 | Recon Loss: 0.003329 | Commit Loss: 0.001135 | Perplexity: 1435.091517
2025-09-08 13:34:11,025 Stage: Train 0.5 | Epoch: 37 | Iter: 61200 | Total Loss: 0.004167 | Recon Loss: 0.003593 | Commit Loss: 0.001148 | Perplexity: 1428.456107
2025-09-08 13:35:53,820 Stage: Train 0.5 | Epoch: 37 | Iter: 61400 | Total Loss: 0.003903 | Recon Loss: 0.003334 | Commit Loss: 0.001138 | Perplexity: 1432.819765
2025-09-08 13:37:36,412 Stage: Train 0.5 | Epoch: 37 | Iter: 61600 | Total Loss: 0.004014 | Recon Loss: 0.003437 | Commit Loss: 0.001154 | Perplexity: 1430.326904
Trainning Epoch:  12%|█▏        | 38/308 [8:41:05<62:34:20, 834.30s/it]2025-09-08 13:39:19,074 Stage: Train 0.5 | Epoch: 38 | Iter: 61800 | Total Loss: 0.004037 | Recon Loss: 0.003441 | Commit Loss: 0.001191 | Perplexity: 1457.686630
2025-09-08 13:41:02,207 Stage: Train 0.5 | Epoch: 38 | Iter: 62000 | Total Loss: 0.003972 | Recon Loss: 0.003419 | Commit Loss: 0.001106 | Perplexity: 1434.218195
2025-09-08 13:42:44,935 Stage: Train 0.5 | Epoch: 38 | Iter: 62200 | Total Loss: 0.004092 | Recon Loss: 0.003526 | Commit Loss: 0.001132 | Perplexity: 1436.860148
2025-09-08 13:44:27,800 Stage: Train 0.5 | Epoch: 38 | Iter: 62400 | Total Loss: 0.004018 | Recon Loss: 0.003413 | Commit Loss: 0.001209 | Perplexity: 1436.908065
2025-09-08 13:46:10,672 Stage: Train 0.5 | Epoch: 38 | Iter: 62600 | Total Loss: 0.004102 | Recon Loss: 0.003535 | Commit Loss: 0.001134 | Perplexity: 1428.178605
2025-09-08 13:47:53,055 Stage: Train 0.5 | Epoch: 38 | Iter: 62800 | Total Loss: 0.003888 | Recon Loss: 0.003316 | Commit Loss: 0.001143 | Perplexity: 1437.749500
2025-09-08 13:49:35,924 Stage: Train 0.5 | Epoch: 38 | Iter: 63000 | Total Loss: 0.004047 | Recon Loss: 0.003471 | Commit Loss: 0.001153 | Perplexity: 1438.920353
2025-09-08 13:51:18,642 Stage: Train 0.5 | Epoch: 38 | Iter: 63200 | Total Loss: 0.003927 | Recon Loss: 0.003355 | Commit Loss: 0.001144 | Perplexity: 1439.910398
Trainning Epoch:  13%|█▎        | 39/308 [8:54:59<62:20:54, 834.40s/it]2025-09-08 13:53:01,185 Stage: Train 0.5 | Epoch: 39 | Iter: 63400 | Total Loss: 0.004151 | Recon Loss: 0.003541 | Commit Loss: 0.001222 | Perplexity: 1426.210168
2025-09-08 13:54:43,873 Stage: Train 0.5 | Epoch: 39 | Iter: 63600 | Total Loss: 0.003896 | Recon Loss: 0.003341 | Commit Loss: 0.001111 | Perplexity: 1431.157570
2025-09-08 13:56:26,556 Stage: Train 0.5 | Epoch: 39 | Iter: 63800 | Total Loss: 0.003952 | Recon Loss: 0.003389 | Commit Loss: 0.001126 | Perplexity: 1432.924821
2025-09-08 13:58:09,373 Stage: Train 0.5 | Epoch: 39 | Iter: 64000 | Total Loss: 0.003966 | Recon Loss: 0.003382 | Commit Loss: 0.001168 | Perplexity: 1441.146425
2025-09-08 13:59:52,048 Stage: Train 0.5 | Epoch: 39 | Iter: 64200 | Total Loss: 0.004055 | Recon Loss: 0.003486 | Commit Loss: 0.001138 | Perplexity: 1444.935210
2025-09-08 14:01:34,706 Stage: Train 0.5 | Epoch: 39 | Iter: 64400 | Total Loss: 0.003839 | Recon Loss: 0.003271 | Commit Loss: 0.001136 | Perplexity: 1433.701280
2025-09-08 14:03:17,231 Stage: Train 0.5 | Epoch: 39 | Iter: 64600 | Total Loss: 0.004095 | Recon Loss: 0.003517 | Commit Loss: 0.001157 | Perplexity: 1420.625684
2025-09-08 14:04:59,697 Stage: Train 0.5 | Epoch: 39 | Iter: 64800 | Total Loss: 0.003885 | Recon Loss: 0.003304 | Commit Loss: 0.001160 | Perplexity: 1440.745381
Trainning Epoch:  13%|█▎        | 40/308 [9:08:53<62:05:34, 834.09s/it]2025-09-08 14:06:42,192 Stage: Train 0.5 | Epoch: 40 | Iter: 65000 | Total Loss: 0.003957 | Recon Loss: 0.003394 | Commit Loss: 0.001127 | Perplexity: 1433.889730
2025-09-08 14:08:24,721 Stage: Train 0.5 | Epoch: 40 | Iter: 65200 | Total Loss: 0.003926 | Recon Loss: 0.003363 | Commit Loss: 0.001125 | Perplexity: 1439.723779
2025-09-08 14:10:07,114 Stage: Train 0.5 | Epoch: 40 | Iter: 65400 | Total Loss: 0.003920 | Recon Loss: 0.003351 | Commit Loss: 0.001139 | Perplexity: 1439.483914
2025-09-08 14:11:49,337 Stage: Train 0.5 | Epoch: 40 | Iter: 65600 | Total Loss: 0.003971 | Recon Loss: 0.003399 | Commit Loss: 0.001144 | Perplexity: 1434.854692
2025-09-08 14:13:31,799 Stage: Train 0.5 | Epoch: 40 | Iter: 65800 | Total Loss: 0.003980 | Recon Loss: 0.003407 | Commit Loss: 0.001145 | Perplexity: 1423.308885
2025-09-08 14:15:14,329 Stage: Train 0.5 | Epoch: 40 | Iter: 66000 | Total Loss: 0.003977 | Recon Loss: 0.003419 | Commit Loss: 0.001117 | Perplexity: 1419.319285
2025-09-08 14:16:56,802 Stage: Train 0.5 | Epoch: 40 | Iter: 66200 | Total Loss: 0.003989 | Recon Loss: 0.003410 | Commit Loss: 0.001159 | Perplexity: 1433.902835
2025-09-08 14:18:39,222 Stage: Train 0.5 | Epoch: 40 | Iter: 66400 | Total Loss: 0.004034 | Recon Loss: 0.003421 | Commit Loss: 0.001226 | Perplexity: 1417.675095
Trainning Epoch:  13%|█▎        | 41/308 [9:22:44<61:48:34, 833.39s/it]2025-09-08 14:20:21,666 Stage: Train 0.5 | Epoch: 41 | Iter: 66600 | Total Loss: 0.003975 | Recon Loss: 0.003401 | Commit Loss: 0.001147 | Perplexity: 1427.461862
2025-09-08 14:22:04,148 Stage: Train 0.5 | Epoch: 41 | Iter: 66800 | Total Loss: 0.003899 | Recon Loss: 0.003334 | Commit Loss: 0.001130 | Perplexity: 1422.798680
2025-09-08 14:23:47,047 Stage: Train 0.5 | Epoch: 41 | Iter: 67000 | Total Loss: 0.003854 | Recon Loss: 0.003282 | Commit Loss: 0.001144 | Perplexity: 1435.664747
2025-09-08 14:25:29,655 Stage: Train 0.5 | Epoch: 41 | Iter: 67200 | Total Loss: 0.003935 | Recon Loss: 0.003364 | Commit Loss: 0.001143 | Perplexity: 1426.217345
2025-09-08 14:27:12,279 Stage: Train 0.5 | Epoch: 41 | Iter: 67400 | Total Loss: 0.003952 | Recon Loss: 0.003367 | Commit Loss: 0.001170 | Perplexity: 1431.844534
2025-09-08 14:28:54,783 Stage: Train 0.5 | Epoch: 41 | Iter: 67600 | Total Loss: 0.003934 | Recon Loss: 0.003376 | Commit Loss: 0.001117 | Perplexity: 1420.851431
2025-09-08 14:30:37,415 Stage: Train 0.5 | Epoch: 41 | Iter: 67800 | Total Loss: 0.003962 | Recon Loss: 0.003398 | Commit Loss: 0.001128 | Perplexity: 1433.720743
2025-09-08 14:32:20,022 Stage: Train 0.5 | Epoch: 41 | Iter: 68000 | Total Loss: 0.003886 | Recon Loss: 0.003311 | Commit Loss: 0.001150 | Perplexity: 1436.245106
2025-09-08 14:34:02,446 Stage: Train 0.5 | Epoch: 41 | Iter: 68200 | Total Loss: 0.003829 | Recon Loss: 0.003268 | Commit Loss: 0.001122 | Perplexity: 1421.295616
Trainning Epoch:  14%|█▎        | 42/308 [9:36:37<61:34:04, 833.25s/it]2025-09-08 14:35:45,035 Stage: Train 0.5 | Epoch: 42 | Iter: 68400 | Total Loss: 0.003863 | Recon Loss: 0.003294 | Commit Loss: 0.001137 | Perplexity: 1433.843263
2025-09-08 14:37:27,652 Stage: Train 0.5 | Epoch: 42 | Iter: 68600 | Total Loss: 0.003955 | Recon Loss: 0.003392 | Commit Loss: 0.001125 | Perplexity: 1421.964771
2025-09-08 14:39:10,175 Stage: Train 0.5 | Epoch: 42 | Iter: 68800 | Total Loss: 0.003820 | Recon Loss: 0.003254 | Commit Loss: 0.001132 | Perplexity: 1437.805674
2025-09-08 14:40:52,640 Stage: Train 0.5 | Epoch: 42 | Iter: 69000 | Total Loss: 0.003962 | Recon Loss: 0.003361 | Commit Loss: 0.001202 | Perplexity: 1429.318548
2025-09-08 14:42:35,067 Stage: Train 0.5 | Epoch: 42 | Iter: 69200 | Total Loss: 0.003870 | Recon Loss: 0.003307 | Commit Loss: 0.001127 | Perplexity: 1420.178327
2025-09-08 14:44:17,532 Stage: Train 0.5 | Epoch: 42 | Iter: 69400 | Total Loss: 0.003931 | Recon Loss: 0.003342 | Commit Loss: 0.001178 | Perplexity: 1442.605417
2025-09-08 14:45:59,981 Stage: Train 0.5 | Epoch: 42 | Iter: 69600 | Total Loss: 0.003861 | Recon Loss: 0.003290 | Commit Loss: 0.001142 | Perplexity: 1424.214669
2025-09-08 14:47:42,556 Stage: Train 0.5 | Epoch: 42 | Iter: 69800 | Total Loss: 0.003891 | Recon Loss: 0.003315 | Commit Loss: 0.001153 | Perplexity: 1426.177134
Trainning Epoch:  14%|█▍        | 43/308 [9:50:30<61:19:10, 833.02s/it]2025-09-08 14:49:24,831 Stage: Train 0.5 | Epoch: 43 | Iter: 70000 | Total Loss: 0.003980 | Recon Loss: 0.003424 | Commit Loss: 0.001110 | Perplexity: 1422.328217
2025-09-08 14:51:07,395 Stage: Train 0.5 | Epoch: 43 | Iter: 70200 | Total Loss: 0.003746 | Recon Loss: 0.003182 | Commit Loss: 0.001128 | Perplexity: 1433.272694
2025-09-08 14:52:49,784 Stage: Train 0.5 | Epoch: 43 | Iter: 70400 | Total Loss: 0.003849 | Recon Loss: 0.003267 | Commit Loss: 0.001164 | Perplexity: 1436.754378
2025-09-08 14:54:32,238 Stage: Train 0.5 | Epoch: 43 | Iter: 70600 | Total Loss: 0.003973 | Recon Loss: 0.003391 | Commit Loss: 0.001165 | Perplexity: 1408.953683
2025-09-08 14:56:15,209 Stage: Train 0.5 | Epoch: 43 | Iter: 70800 | Total Loss: 0.003829 | Recon Loss: 0.003258 | Commit Loss: 0.001143 | Perplexity: 1430.937186
2025-09-08 14:57:57,920 Stage: Train 0.5 | Epoch: 43 | Iter: 71000 | Total Loss: 0.003963 | Recon Loss: 0.003390 | Commit Loss: 0.001145 | Perplexity: 1430.140734
2025-09-08 14:59:40,654 Stage: Train 0.5 | Epoch: 43 | Iter: 71200 | Total Loss: 0.003848 | Recon Loss: 0.003279 | Commit Loss: 0.001138 | Perplexity: 1436.795229
2025-09-08 15:01:23,409 Stage: Train 0.5 | Epoch: 43 | Iter: 71400 | Total Loss: 0.003899 | Recon Loss: 0.003327 | Commit Loss: 0.001144 | Perplexity: 1428.860532
Trainning Epoch:  14%|█▍        | 44/308 [10:04:23<61:05:29, 833.07s/it]2025-09-08 15:03:06,006 Stage: Train 0.5 | Epoch: 44 | Iter: 71600 | Total Loss: 0.003879 | Recon Loss: 0.003314 | Commit Loss: 0.001130 | Perplexity: 1430.341821
2025-09-08 15:04:48,842 Stage: Train 0.5 | Epoch: 44 | Iter: 71800 | Total Loss: 0.003708 | Recon Loss: 0.003153 | Commit Loss: 0.001112 | Perplexity: 1426.949391
2025-09-08 15:06:31,500 Stage: Train 0.5 | Epoch: 44 | Iter: 72000 | Total Loss: 0.003988 | Recon Loss: 0.003383 | Commit Loss: 0.001211 | Perplexity: 1420.628577
2025-09-08 15:08:14,108 Stage: Train 0.5 | Epoch: 44 | Iter: 72200 | Total Loss: 0.003757 | Recon Loss: 0.003202 | Commit Loss: 0.001110 | Perplexity: 1418.592031
2025-09-08 15:09:56,642 Stage: Train 0.5 | Epoch: 44 | Iter: 72400 | Total Loss: 0.003913 | Recon Loss: 0.003353 | Commit Loss: 0.001119 | Perplexity: 1416.726820
2025-09-08 15:11:39,152 Stage: Train 0.5 | Epoch: 44 | Iter: 72600 | Total Loss: 0.003882 | Recon Loss: 0.003319 | Commit Loss: 0.001126 | Perplexity: 1438.610287
2025-09-08 15:13:21,481 Stage: Train 0.5 | Epoch: 44 | Iter: 72800 | Total Loss: 0.003924 | Recon Loss: 0.003356 | Commit Loss: 0.001138 | Perplexity: 1427.633817
2025-09-08 15:15:04,338 Stage: Train 0.5 | Epoch: 44 | Iter: 73000 | Total Loss: 0.003849 | Recon Loss: 0.003276 | Commit Loss: 0.001147 | Perplexity: 1424.299142
Trainning Epoch:  15%|█▍        | 45/308 [10:18:16<60:51:38, 833.07s/it]2025-09-08 15:16:46,265 Stage: Train 0.5 | Epoch: 45 | Iter: 73200 | Total Loss: 0.003793 | Recon Loss: 0.003226 | Commit Loss: 0.001134 | Perplexity: 1426.349588
2025-09-08 15:18:28,835 Stage: Train 0.5 | Epoch: 45 | Iter: 73400 | Total Loss: 0.003857 | Recon Loss: 0.003288 | Commit Loss: 0.001139 | Perplexity: 1425.456527
2025-09-08 15:20:11,403 Stage: Train 0.5 | Epoch: 45 | Iter: 73600 | Total Loss: 0.003803 | Recon Loss: 0.003230 | Commit Loss: 0.001145 | Perplexity: 1427.961786
2025-09-08 15:21:53,860 Stage: Train 0.5 | Epoch: 45 | Iter: 73800 | Total Loss: 0.003831 | Recon Loss: 0.003266 | Commit Loss: 0.001130 | Perplexity: 1435.895378
2025-09-08 15:23:36,166 Stage: Train 0.5 | Epoch: 45 | Iter: 74000 | Total Loss: 0.003825 | Recon Loss: 0.003263 | Commit Loss: 0.001122 | Perplexity: 1425.468798
2025-09-08 15:25:18,693 Stage: Train 0.5 | Epoch: 45 | Iter: 74200 | Total Loss: 0.003844 | Recon Loss: 0.003263 | Commit Loss: 0.001161 | Perplexity: 1424.941714
2025-09-08 15:27:01,540 Stage: Train 0.5 | Epoch: 45 | Iter: 74400 | Total Loss: 0.003907 | Recon Loss: 0.003354 | Commit Loss: 0.001105 | Perplexity: 1423.665248
2025-09-08 15:28:44,194 Stage: Train 0.5 | Epoch: 45 | Iter: 74600 | Total Loss: 0.003763 | Recon Loss: 0.003167 | Commit Loss: 0.001193 | Perplexity: 1427.397101
Trainning Epoch:  15%|█▍        | 46/308 [10:32:08<60:36:37, 832.82s/it]2025-09-08 15:30:26,723 Stage: Train 0.5 | Epoch: 46 | Iter: 74800 | Total Loss: 0.003822 | Recon Loss: 0.003265 | Commit Loss: 0.001114 | Perplexity: 1428.292321
2025-09-08 15:32:09,517 Stage: Train 0.5 | Epoch: 46 | Iter: 75000 | Total Loss: 0.003784 | Recon Loss: 0.003209 | Commit Loss: 0.001149 | Perplexity: 1427.003199
2025-09-08 15:33:52,421 Stage: Train 0.5 | Epoch: 46 | Iter: 75200 | Total Loss: 0.003896 | Recon Loss: 0.003339 | Commit Loss: 0.001115 | Perplexity: 1421.235830
2025-09-08 15:35:35,028 Stage: Train 0.5 | Epoch: 46 | Iter: 75400 | Total Loss: 0.003647 | Recon Loss: 0.003093 | Commit Loss: 0.001109 | Perplexity: 1433.089832
2025-09-08 15:37:17,733 Stage: Train 0.5 | Epoch: 46 | Iter: 75600 | Total Loss: 0.003765 | Recon Loss: 0.003207 | Commit Loss: 0.001115 | Perplexity: 1431.217398
2025-09-08 15:39:00,452 Stage: Train 0.5 | Epoch: 46 | Iter: 75800 | Total Loss: 0.003765 | Recon Loss: 0.003214 | Commit Loss: 0.001101 | Perplexity: 1413.427057
2025-09-08 15:40:43,090 Stage: Train 0.5 | Epoch: 46 | Iter: 76000 | Total Loss: 0.003750 | Recon Loss: 0.003179 | Commit Loss: 0.001142 | Perplexity: 1421.848771
2025-09-08 15:42:25,848 Stage: Train 0.5 | Epoch: 46 | Iter: 76200 | Total Loss: 0.003873 | Recon Loss: 0.003289 | Commit Loss: 0.001168 | Perplexity: 1406.158173
Trainning Epoch:  15%|█▌        | 47/308 [10:46:02<60:24:15, 833.16s/it]2025-09-08 15:44:08,386 Stage: Train 0.5 | Epoch: 47 | Iter: 76400 | Total Loss: 0.003798 | Recon Loss: 0.003237 | Commit Loss: 0.001121 | Perplexity: 1415.888278
2025-09-08 15:45:51,235 Stage: Train 0.5 | Epoch: 47 | Iter: 76600 | Total Loss: 0.003759 | Recon Loss: 0.003197 | Commit Loss: 0.001126 | Perplexity: 1428.980655
2025-09-08 15:47:34,064 Stage: Train 0.5 | Epoch: 47 | Iter: 76800 | Total Loss: 0.003770 | Recon Loss: 0.003207 | Commit Loss: 0.001127 | Perplexity: 1426.583593
2025-09-08 15:49:16,893 Stage: Train 0.5 | Epoch: 47 | Iter: 77000 | Total Loss: 0.003861 | Recon Loss: 0.003260 | Commit Loss: 0.001202 | Perplexity: 1411.909181
2025-09-08 15:50:59,686 Stage: Train 0.5 | Epoch: 47 | Iter: 77200 | Total Loss: 0.003727 | Recon Loss: 0.003179 | Commit Loss: 0.001095 | Perplexity: 1411.705348
2025-09-08 15:52:42,348 Stage: Train 0.5 | Epoch: 47 | Iter: 77400 | Total Loss: 0.003777 | Recon Loss: 0.003214 | Commit Loss: 0.001125 | Perplexity: 1417.648167
2025-09-08 15:54:25,053 Stage: Train 0.5 | Epoch: 47 | Iter: 77600 | Total Loss: 0.003827 | Recon Loss: 0.003254 | Commit Loss: 0.001146 | Perplexity: 1424.282167
2025-09-08 15:56:07,572 Stage: Train 0.5 | Epoch: 47 | Iter: 77800 | Total Loss: 0.003900 | Recon Loss: 0.003347 | Commit Loss: 0.001106 | Perplexity: 1412.068474
Trainning Epoch:  16%|█▌        | 48/308 [10:59:56<60:11:08, 833.34s/it]2025-09-08 15:57:49,645 Stage: Train 0.5 | Epoch: 48 | Iter: 78000 | Total Loss: 0.003702 | Recon Loss: 0.003155 | Commit Loss: 0.001094 | Perplexity: 1406.533439
2025-09-08 15:59:32,011 Stage: Train 0.5 | Epoch: 48 | Iter: 78200 | Total Loss: 0.003733 | Recon Loss: 0.003174 | Commit Loss: 0.001117 | Perplexity: 1418.737318
2025-09-08 16:01:14,573 Stage: Train 0.5 | Epoch: 48 | Iter: 78400 | Total Loss: 0.003903 | Recon Loss: 0.003342 | Commit Loss: 0.001121 | Perplexity: 1402.566746
2025-09-08 16:02:57,235 Stage: Train 0.5 | Epoch: 48 | Iter: 78600 | Total Loss: 0.003739 | Recon Loss: 0.003152 | Commit Loss: 0.001173 | Perplexity: 1410.310645
2025-09-08 16:04:39,860 Stage: Train 0.5 | Epoch: 48 | Iter: 78800 | Total Loss: 0.003673 | Recon Loss: 0.003118 | Commit Loss: 0.001108 | Perplexity: 1415.655225
2025-09-08 16:06:22,401 Stage: Train 0.5 | Epoch: 48 | Iter: 79000 | Total Loss: 0.003820 | Recon Loss: 0.003251 | Commit Loss: 0.001138 | Perplexity: 1416.860999
2025-09-08 16:08:04,898 Stage: Train 0.5 | Epoch: 48 | Iter: 79200 | Total Loss: 0.003675 | Recon Loss: 0.003114 | Commit Loss: 0.001122 | Perplexity: 1405.945220
2025-09-08 16:09:47,581 Stage: Train 0.5 | Epoch: 48 | Iter: 79400 | Total Loss: 0.003769 | Recon Loss: 0.003207 | Commit Loss: 0.001125 | Perplexity: 1412.136273
Trainning Epoch:  16%|█▌        | 49/308 [11:13:48<59:55:59, 833.05s/it]2025-09-08 16:11:29,829 Stage: Train 0.5 | Epoch: 49 | Iter: 79600 | Total Loss: 0.003692 | Recon Loss: 0.003131 | Commit Loss: 0.001123 | Perplexity: 1395.845019
2025-09-08 16:13:12,367 Stage: Train 0.5 | Epoch: 49 | Iter: 79800 | Total Loss: 0.003828 | Recon Loss: 0.003264 | Commit Loss: 0.001126 | Perplexity: 1398.359072
2025-09-08 16:14:54,899 Stage: Train 0.5 | Epoch: 49 | Iter: 80000 | Total Loss: 0.003692 | Recon Loss: 0.003150 | Commit Loss: 0.001085 | Perplexity: 1394.400331
2025-09-08 16:14:54,899 Saving model at iteration 80000
2025-09-08 16:14:55,016 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000
2025-09-08 16:14:55,416 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000/model.safetensors
2025-09-08 16:14:55,806 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000/optimizer.bin
2025-09-08 16:14:55,807 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000/scheduler.bin
2025-09-08 16:14:55,807 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000/sampler.bin
2025-09-08 16:14:55,807 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_50_step_80000/random_states_0.pkl
2025-09-08 16:16:39,474 Stage: Train 0.5 | Epoch: 49 | Iter: 80200 | Total Loss: 0.003678 | Recon Loss: 0.003115 | Commit Loss: 0.001124 | Perplexity: 1410.142326
2025-09-08 16:18:24,812 Stage: Train 0.5 | Epoch: 49 | Iter: 80400 | Total Loss: 0.003767 | Recon Loss: 0.003208 | Commit Loss: 0.001117 | Perplexity: 1408.532053
2025-09-08 16:20:00,013 Stage: Train 0.5 | Epoch: 49 | Iter: 80600 | Total Loss: 0.003761 | Recon Loss: 0.003181 | Commit Loss: 0.001159 | Perplexity: 1390.284101
2025-09-08 16:21:33,062 Stage: Train 0.5 | Epoch: 49 | Iter: 80800 | Total Loss: 0.003749 | Recon Loss: 0.003187 | Commit Loss: 0.001124 | Perplexity: 1390.875501
2025-09-08 16:23:28,509 Stage: Train 0.5 | Epoch: 49 | Iter: 81000 | Total Loss: 0.003842 | Recon Loss: 0.003272 | Commit Loss: 0.001140 | Perplexity: 1394.322604
2025-09-08 16:25:08,049 Stage: Train 0.5 | Epoch: 49 | Iter: 81200 | Total Loss: 0.003732 | Recon Loss: 0.003172 | Commit Loss: 0.001120 | Perplexity: 1397.039649
Trainning Epoch:  16%|█▌        | 50/308 [11:27:39<59:38:52, 832.30s/it]2025-09-08 16:26:56,682 Stage: Train 0.5 | Epoch: 50 | Iter: 81400 | Total Loss: 0.003634 | Recon Loss: 0.003086 | Commit Loss: 0.001097 | Perplexity: 1401.459659
2025-09-08 16:28:39,438 Stage: Train 0.5 | Epoch: 50 | Iter: 81600 | Total Loss: 0.003764 | Recon Loss: 0.003197 | Commit Loss: 0.001134 | Perplexity: 1395.765004
2025-09-08 16:30:21,807 Stage: Train 0.5 | Epoch: 50 | Iter: 81800 | Total Loss: 0.003767 | Recon Loss: 0.003208 | Commit Loss: 0.001117 | Perplexity: 1394.803761
2025-09-08 16:32:04,392 Stage: Train 0.5 | Epoch: 50 | Iter: 82000 | Total Loss: 0.003656 | Recon Loss: 0.003100 | Commit Loss: 0.001113 | Perplexity: 1401.065883
2025-09-08 16:33:46,895 Stage: Train 0.5 | Epoch: 50 | Iter: 82200 | Total Loss: 0.003799 | Recon Loss: 0.003234 | Commit Loss: 0.001130 | Perplexity: 1401.474566
2025-09-08 16:35:29,474 Stage: Train 0.5 | Epoch: 50 | Iter: 82400 | Total Loss: 0.003729 | Recon Loss: 0.003161 | Commit Loss: 0.001137 | Perplexity: 1398.175493
2025-09-08 16:37:12,104 Stage: Train 0.5 | Epoch: 50 | Iter: 82600 | Total Loss: 0.003736 | Recon Loss: 0.003167 | Commit Loss: 0.001137 | Perplexity: 1397.131596
2025-09-08 16:38:54,531 Stage: Train 0.5 | Epoch: 50 | Iter: 82800 | Total Loss: 0.003759 | Recon Loss: 0.003179 | Commit Loss: 0.001160 | Perplexity: 1387.426392
Trainning Epoch:  17%|█▋        | 51/308 [11:41:38<59:33:12, 834.21s/it]2025-09-08 16:40:37,254 Stage: Train 0.5 | Epoch: 51 | Iter: 83000 | Total Loss: 0.003659 | Recon Loss: 0.003101 | Commit Loss: 0.001118 | Perplexity: 1389.587183
2025-09-08 16:42:19,932 Stage: Train 0.5 | Epoch: 51 | Iter: 83200 | Total Loss: 0.003719 | Recon Loss: 0.003147 | Commit Loss: 0.001144 | Perplexity: 1400.593241
2025-09-08 16:44:02,841 Stage: Train 0.5 | Epoch: 51 | Iter: 83400 | Total Loss: 0.003625 | Recon Loss: 0.003074 | Commit Loss: 0.001102 | Perplexity: 1392.572797
2025-09-08 16:45:45,465 Stage: Train 0.5 | Epoch: 51 | Iter: 83600 | Total Loss: 0.003729 | Recon Loss: 0.003160 | Commit Loss: 0.001139 | Perplexity: 1407.463105
2025-09-08 16:47:28,093 Stage: Train 0.5 | Epoch: 51 | Iter: 83800 | Total Loss: 0.003733 | Recon Loss: 0.003138 | Commit Loss: 0.001190 | Perplexity: 1401.593905
2025-09-08 16:49:10,679 Stage: Train 0.5 | Epoch: 51 | Iter: 84000 | Total Loss: 0.003717 | Recon Loss: 0.003152 | Commit Loss: 0.001130 | Perplexity: 1395.720175
2025-09-08 16:50:53,636 Stage: Train 0.5 | Epoch: 51 | Iter: 84200 | Total Loss: 0.003694 | Recon Loss: 0.003146 | Commit Loss: 0.001096 | Perplexity: 1385.768330
2025-09-08 16:52:36,304 Stage: Train 0.5 | Epoch: 51 | Iter: 84400 | Total Loss: 0.003749 | Recon Loss: 0.003178 | Commit Loss: 0.001142 | Perplexity: 1395.679571
Trainning Epoch:  17%|█▋        | 52/308 [11:55:32<59:19:11, 834.19s/it]2025-09-08 16:54:19,084 Stage: Train 0.5 | Epoch: 52 | Iter: 84600 | Total Loss: 0.003660 | Recon Loss: 0.003105 | Commit Loss: 0.001112 | Perplexity: 1387.733080
2025-09-08 16:56:01,766 Stage: Train 0.5 | Epoch: 52 | Iter: 84800 | Total Loss: 0.003726 | Recon Loss: 0.003137 | Commit Loss: 0.001179 | Perplexity: 1400.950876
2025-09-08 16:57:44,752 Stage: Train 0.5 | Epoch: 52 | Iter: 85000 | Total Loss: 0.003573 | Recon Loss: 0.003020 | Commit Loss: 0.001106 | Perplexity: 1396.787147
2025-09-08 16:59:27,440 Stage: Train 0.5 | Epoch: 52 | Iter: 85200 | Total Loss: 0.003717 | Recon Loss: 0.003144 | Commit Loss: 0.001145 | Perplexity: 1396.700434
2025-09-08 17:01:10,193 Stage: Train 0.5 | Epoch: 52 | Iter: 85400 | Total Loss: 0.003609 | Recon Loss: 0.003052 | Commit Loss: 0.001114 | Perplexity: 1389.438665
2025-09-08 17:02:52,794 Stage: Train 0.5 | Epoch: 52 | Iter: 85600 | Total Loss: 0.003752 | Recon Loss: 0.003178 | Commit Loss: 0.001147 | Perplexity: 1396.933082
2025-09-08 17:04:35,634 Stage: Train 0.5 | Epoch: 52 | Iter: 85800 | Total Loss: 0.003606 | Recon Loss: 0.003048 | Commit Loss: 0.001117 | Perplexity: 1397.688702
2025-09-08 17:06:18,690 Stage: Train 0.5 | Epoch: 52 | Iter: 86000 | Total Loss: 0.003682 | Recon Loss: 0.003115 | Commit Loss: 0.001134 | Perplexity: 1391.174954
Trainning Epoch:  17%|█▋        | 53/308 [12:09:26<59:05:55, 834.34s/it]2025-09-08 17:08:01,311 Stage: Train 0.5 | Epoch: 53 | Iter: 86200 | Total Loss: 0.003667 | Recon Loss: 0.003095 | Commit Loss: 0.001145 | Perplexity: 1398.021807
2025-09-08 17:09:43,980 Stage: Train 0.5 | Epoch: 53 | Iter: 86400 | Total Loss: 0.003681 | Recon Loss: 0.003109 | Commit Loss: 0.001143 | Perplexity: 1387.749533
2025-09-08 17:11:26,643 Stage: Train 0.5 | Epoch: 53 | Iter: 86600 | Total Loss: 0.003709 | Recon Loss: 0.003154 | Commit Loss: 0.001110 | Perplexity: 1385.857808
2025-09-08 17:13:09,213 Stage: Train 0.5 | Epoch: 53 | Iter: 86800 | Total Loss: 0.003633 | Recon Loss: 0.003072 | Commit Loss: 0.001121 | Perplexity: 1397.140344
2025-09-08 17:14:51,719 Stage: Train 0.5 | Epoch: 53 | Iter: 87000 | Total Loss: 0.003690 | Recon Loss: 0.003128 | Commit Loss: 0.001123 | Perplexity: 1388.611243
2025-09-08 17:16:34,295 Stage: Train 0.5 | Epoch: 53 | Iter: 87200 | Total Loss: 0.003769 | Recon Loss: 0.003165 | Commit Loss: 0.001209 | Perplexity: 1395.592202
2025-09-08 17:18:16,884 Stage: Train 0.5 | Epoch: 53 | Iter: 87400 | Total Loss: 0.003644 | Recon Loss: 0.003092 | Commit Loss: 0.001104 | Perplexity: 1379.585364
2025-09-08 17:19:59,672 Stage: Train 0.5 | Epoch: 53 | Iter: 87600 | Total Loss: 0.003717 | Recon Loss: 0.003144 | Commit Loss: 0.001145 | Perplexity: 1397.914462
Trainning Epoch:  18%|█▊        | 54/308 [12:23:20<58:50:42, 834.02s/it]2025-09-08 17:21:42,124 Stage: Train 0.5 | Epoch: 54 | Iter: 87800 | Total Loss: 0.003603 | Recon Loss: 0.003051 | Commit Loss: 0.001105 | Perplexity: 1387.054142
2025-09-08 17:23:24,635 Stage: Train 0.5 | Epoch: 54 | Iter: 88000 | Total Loss: 0.003584 | Recon Loss: 0.003028 | Commit Loss: 0.001111 | Perplexity: 1396.080512
2025-09-08 17:25:07,259 Stage: Train 0.5 | Epoch: 54 | Iter: 88200 | Total Loss: 0.003666 | Recon Loss: 0.003111 | Commit Loss: 0.001110 | Perplexity: 1387.940836
2025-09-08 17:26:50,129 Stage: Train 0.5 | Epoch: 54 | Iter: 88400 | Total Loss: 0.003637 | Recon Loss: 0.003065 | Commit Loss: 0.001144 | Perplexity: 1399.057201
2025-09-08 17:28:32,873 Stage: Train 0.5 | Epoch: 54 | Iter: 88600 | Total Loss: 0.003626 | Recon Loss: 0.003054 | Commit Loss: 0.001144 | Perplexity: 1393.887732
2025-09-08 17:30:15,496 Stage: Train 0.5 | Epoch: 54 | Iter: 88800 | Total Loss: 0.003640 | Recon Loss: 0.003078 | Commit Loss: 0.001124 | Perplexity: 1399.613073
2025-09-08 17:31:58,138 Stage: Train 0.5 | Epoch: 54 | Iter: 89000 | Total Loss: 0.003637 | Recon Loss: 0.003053 | Commit Loss: 0.001167 | Perplexity: 1395.795717
2025-09-08 17:33:40,825 Stage: Train 0.5 | Epoch: 54 | Iter: 89200 | Total Loss: 0.003773 | Recon Loss: 0.003186 | Commit Loss: 0.001174 | Perplexity: 1387.500812
Trainning Epoch:  18%|█▊        | 55/308 [12:37:13<58:36:03, 833.85s/it]2025-09-08 17:35:23,404 Stage: Train 0.5 | Epoch: 55 | Iter: 89400 | Total Loss: 0.003605 | Recon Loss: 0.003041 | Commit Loss: 0.001128 | Perplexity: 1388.046933
2025-09-08 17:37:05,948 Stage: Train 0.5 | Epoch: 55 | Iter: 89600 | Total Loss: 0.003662 | Recon Loss: 0.003106 | Commit Loss: 0.001111 | Perplexity: 1386.476928
2025-09-08 17:38:48,871 Stage: Train 0.5 | Epoch: 55 | Iter: 89800 | Total Loss: 0.003702 | Recon Loss: 0.003136 | Commit Loss: 0.001132 | Perplexity: 1384.998768
2025-09-08 17:40:31,459 Stage: Train 0.5 | Epoch: 55 | Iter: 90000 | Total Loss: 0.003697 | Recon Loss: 0.003135 | Commit Loss: 0.001125 | Perplexity: 1382.812378
2025-09-08 17:42:14,196 Stage: Train 0.5 | Epoch: 55 | Iter: 90200 | Total Loss: 0.003704 | Recon Loss: 0.003128 | Commit Loss: 0.001152 | Perplexity: 1386.275167
2025-09-08 17:43:57,066 Stage: Train 0.5 | Epoch: 55 | Iter: 90400 | Total Loss: 0.003557 | Recon Loss: 0.003001 | Commit Loss: 0.001112 | Perplexity: 1396.157608
2025-09-08 17:45:39,778 Stage: Train 0.5 | Epoch: 55 | Iter: 90600 | Total Loss: 0.003785 | Recon Loss: 0.003204 | Commit Loss: 0.001162 | Perplexity: 1389.433448
2025-09-08 17:47:22,161 Stage: Train 0.5 | Epoch: 55 | Iter: 90800 | Total Loss: 0.003584 | Recon Loss: 0.003023 | Commit Loss: 0.001123 | Perplexity: 1402.852642
Trainning Epoch:  18%|█▊        | 56/308 [12:51:07<58:22:02, 833.82s/it]2025-09-08 17:49:04,729 Stage: Train 0.5 | Epoch: 56 | Iter: 91000 | Total Loss: 0.003596 | Recon Loss: 0.003032 | Commit Loss: 0.001127 | Perplexity: 1403.550425
2025-09-08 17:50:47,487 Stage: Train 0.5 | Epoch: 56 | Iter: 91200 | Total Loss: 0.003719 | Recon Loss: 0.003139 | Commit Loss: 0.001160 | Perplexity: 1395.039761
2025-09-08 17:52:30,089 Stage: Train 0.5 | Epoch: 56 | Iter: 91400 | Total Loss: 0.003574 | Recon Loss: 0.003005 | Commit Loss: 0.001138 | Perplexity: 1405.897516
2025-09-08 17:54:12,961 Stage: Train 0.5 | Epoch: 56 | Iter: 91600 | Total Loss: 0.003671 | Recon Loss: 0.003124 | Commit Loss: 0.001093 | Perplexity: 1387.328128
2025-09-08 17:55:55,626 Stage: Train 0.5 | Epoch: 56 | Iter: 91800 | Total Loss: 0.003662 | Recon Loss: 0.003103 | Commit Loss: 0.001119 | Perplexity: 1383.353528
2025-09-08 17:57:38,256 Stage: Train 0.5 | Epoch: 56 | Iter: 92000 | Total Loss: 0.003604 | Recon Loss: 0.003029 | Commit Loss: 0.001149 | Perplexity: 1390.076587
2025-09-08 17:59:21,187 Stage: Train 0.5 | Epoch: 56 | Iter: 92200 | Total Loss: 0.003699 | Recon Loss: 0.003125 | Commit Loss: 0.001149 | Perplexity: 1390.091527
2025-09-08 18:01:03,981 Stage: Train 0.5 | Epoch: 56 | Iter: 92400 | Total Loss: 0.003556 | Recon Loss: 0.002997 | Commit Loss: 0.001117 | Perplexity: 1395.846423
Trainning Epoch:  19%|█▊        | 57/308 [13:05:01<58:08:41, 833.95s/it]2025-09-08 18:02:46,671 Stage: Train 0.5 | Epoch: 57 | Iter: 92600 | Total Loss: 0.003707 | Recon Loss: 0.003149 | Commit Loss: 0.001117 | Perplexity: 1382.971075
2025-09-08 18:04:29,573 Stage: Train 0.5 | Epoch: 57 | Iter: 92800 | Total Loss: 0.003596 | Recon Loss: 0.003019 | Commit Loss: 0.001154 | Perplexity: 1411.494349
2025-09-08 18:06:12,606 Stage: Train 0.5 | Epoch: 57 | Iter: 93000 | Total Loss: 0.003658 | Recon Loss: 0.003104 | Commit Loss: 0.001106 | Perplexity: 1392.440389
2025-09-08 18:07:55,323 Stage: Train 0.5 | Epoch: 57 | Iter: 93200 | Total Loss: 0.003605 | Recon Loss: 0.003041 | Commit Loss: 0.001128 | Perplexity: 1393.223463
2025-09-08 18:09:37,939 Stage: Train 0.5 | Epoch: 57 | Iter: 93400 | Total Loss: 0.003750 | Recon Loss: 0.003154 | Commit Loss: 0.001191 | Perplexity: 1392.776565
2025-09-08 18:11:20,453 Stage: Train 0.5 | Epoch: 57 | Iter: 93600 | Total Loss: 0.003590 | Recon Loss: 0.003021 | Commit Loss: 0.001137 | Perplexity: 1395.348196
2025-09-08 18:13:03,138 Stage: Train 0.5 | Epoch: 57 | Iter: 93800 | Total Loss: 0.003601 | Recon Loss: 0.003051 | Commit Loss: 0.001099 | Perplexity: 1390.030644
2025-09-08 18:14:45,886 Stage: Train 0.5 | Epoch: 57 | Iter: 94000 | Total Loss: 0.003560 | Recon Loss: 0.003010 | Commit Loss: 0.001100 | Perplexity: 1386.649216
Trainning Epoch:  19%|█▉        | 58/308 [13:18:55<57:54:41, 833.93s/it]2025-09-08 18:16:28,254 Stage: Train 0.5 | Epoch: 58 | Iter: 94200 | Total Loss: 0.003616 | Recon Loss: 0.003047 | Commit Loss: 0.001138 | Perplexity: 1392.844601
2025-09-08 18:18:10,848 Stage: Train 0.5 | Epoch: 58 | Iter: 94400 | Total Loss: 0.003639 | Recon Loss: 0.003070 | Commit Loss: 0.001137 | Perplexity: 1395.430604
2025-09-08 18:19:53,343 Stage: Train 0.5 | Epoch: 58 | Iter: 94600 | Total Loss: 0.003583 | Recon Loss: 0.003021 | Commit Loss: 0.001124 | Perplexity: 1397.937739
2025-09-08 18:21:35,696 Stage: Train 0.5 | Epoch: 58 | Iter: 94800 | Total Loss: 0.003581 | Recon Loss: 0.003005 | Commit Loss: 0.001151 | Perplexity: 1404.658989
2025-09-08 18:23:18,258 Stage: Train 0.5 | Epoch: 58 | Iter: 95000 | Total Loss: 0.003583 | Recon Loss: 0.003030 | Commit Loss: 0.001105 | Perplexity: 1385.839211
2025-09-08 18:25:00,917 Stage: Train 0.5 | Epoch: 58 | Iter: 95200 | Total Loss: 0.003620 | Recon Loss: 0.003060 | Commit Loss: 0.001120 | Perplexity: 1383.581691
2025-09-08 18:26:43,907 Stage: Train 0.5 | Epoch: 58 | Iter: 95400 | Total Loss: 0.003576 | Recon Loss: 0.003008 | Commit Loss: 0.001134 | Perplexity: 1404.250311
2025-09-08 18:28:27,169 Stage: Train 0.5 | Epoch: 58 | Iter: 95600 | Total Loss: 0.003799 | Recon Loss: 0.003211 | Commit Loss: 0.001176 | Perplexity: 1386.960470
2025-09-08 18:30:09,853 Stage: Train 0.5 | Epoch: 58 | Iter: 95800 | Total Loss: 0.003540 | Recon Loss: 0.002983 | Commit Loss: 0.001115 | Perplexity: 1391.284409
Trainning Epoch:  19%|█▉        | 59/308 [13:32:49<57:40:34, 833.87s/it]2025-09-08 18:31:52,442 Stage: Train 0.5 | Epoch: 59 | Iter: 96000 | Total Loss: 0.003825 | Recon Loss: 0.003240 | Commit Loss: 0.001170 | Perplexity: 1396.943035
2025-09-08 18:33:35,129 Stage: Train 0.5 | Epoch: 59 | Iter: 96200 | Total Loss: 0.003521 | Recon Loss: 0.002961 | Commit Loss: 0.001120 | Perplexity: 1391.813871
2025-09-08 18:35:18,056 Stage: Train 0.5 | Epoch: 59 | Iter: 96400 | Total Loss: 0.003594 | Recon Loss: 0.003025 | Commit Loss: 0.001139 | Perplexity: 1395.734064
2025-09-08 18:37:01,054 Stage: Train 0.5 | Epoch: 59 | Iter: 96600 | Total Loss: 0.003642 | Recon Loss: 0.003083 | Commit Loss: 0.001118 | Perplexity: 1395.043519
2025-09-08 18:38:43,900 Stage: Train 0.5 | Epoch: 59 | Iter: 96800 | Total Loss: 0.003510 | Recon Loss: 0.002965 | Commit Loss: 0.001090 | Perplexity: 1389.141990
2025-09-08 18:40:26,745 Stage: Train 0.5 | Epoch: 59 | Iter: 97000 | Total Loss: 0.003643 | Recon Loss: 0.003079 | Commit Loss: 0.001128 | Perplexity: 1393.152786
2025-09-08 18:42:09,679 Stage: Train 0.5 | Epoch: 59 | Iter: 97200 | Total Loss: 0.003641 | Recon Loss: 0.003078 | Commit Loss: 0.001125 | Perplexity: 1377.775441
2025-09-08 18:43:52,486 Stage: Train 0.5 | Epoch: 59 | Iter: 97400 | Total Loss: 0.003552 | Recon Loss: 0.002985 | Commit Loss: 0.001133 | Perplexity: 1396.929150
Trainning Epoch:  19%|█▉        | 60/308 [13:46:44<57:28:02, 834.21s/it]2025-09-08 18:45:35,326 Stage: Train 0.5 | Epoch: 60 | Iter: 97600 | Total Loss: 0.003672 | Recon Loss: 0.003124 | Commit Loss: 0.001098 | Perplexity: 1375.903600
2025-09-08 18:47:18,355 Stage: Train 0.5 | Epoch: 60 | Iter: 97800 | Total Loss: 0.003622 | Recon Loss: 0.003042 | Commit Loss: 0.001159 | Perplexity: 1404.825651
2025-09-08 18:49:01,211 Stage: Train 0.5 | Epoch: 60 | Iter: 98000 | Total Loss: 0.003621 | Recon Loss: 0.003062 | Commit Loss: 0.001116 | Perplexity: 1385.484827
2025-09-08 18:50:44,335 Stage: Train 0.5 | Epoch: 60 | Iter: 98200 | Total Loss: 0.003560 | Recon Loss: 0.002999 | Commit Loss: 0.001124 | Perplexity: 1386.435723
2025-09-08 18:52:27,526 Stage: Train 0.5 | Epoch: 60 | Iter: 98400 | Total Loss: 0.003660 | Recon Loss: 0.003074 | Commit Loss: 0.001172 | Perplexity: 1385.145842
2025-09-08 18:54:10,131 Stage: Train 0.5 | Epoch: 60 | Iter: 98600 | Total Loss: 0.003576 | Recon Loss: 0.003020 | Commit Loss: 0.001114 | Perplexity: 1394.761348
2025-09-08 18:55:52,743 Stage: Train 0.5 | Epoch: 60 | Iter: 98800 | Total Loss: 0.003543 | Recon Loss: 0.002990 | Commit Loss: 0.001107 | Perplexity: 1385.770072
2025-09-08 18:57:35,283 Stage: Train 0.5 | Epoch: 60 | Iter: 99000 | Total Loss: 0.003506 | Recon Loss: 0.002945 | Commit Loss: 0.001122 | Perplexity: 1396.330672
Trainning Epoch:  20%|█▉        | 61/308 [14:00:39<57:15:15, 834.48s/it]2025-09-08 18:59:18,027 Stage: Train 0.5 | Epoch: 61 | Iter: 99200 | Total Loss: 0.003633 | Recon Loss: 0.003070 | Commit Loss: 0.001126 | Perplexity: 1381.950569
2025-09-08 19:01:00,661 Stage: Train 0.5 | Epoch: 61 | Iter: 99400 | Total Loss: 0.003532 | Recon Loss: 0.002961 | Commit Loss: 0.001143 | Perplexity: 1394.775521
2025-09-08 19:02:43,154 Stage: Train 0.5 | Epoch: 61 | Iter: 99600 | Total Loss: 0.003622 | Recon Loss: 0.003055 | Commit Loss: 0.001134 | Perplexity: 1402.412748
2025-09-08 19:04:25,616 Stage: Train 0.5 | Epoch: 61 | Iter: 99800 | Total Loss: 0.003679 | Recon Loss: 0.003127 | Commit Loss: 0.001104 | Perplexity: 1381.847472
2025-09-08 19:06:08,182 Stage: Train 0.5 | Epoch: 61 | Iter: 100000 | Total Loss: 0.003521 | Recon Loss: 0.002952 | Commit Loss: 0.001137 | Perplexity: 1397.727328
2025-09-08 19:06:08,182 Saving model at iteration 100000
2025-09-08 19:06:08,484 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000
2025-09-08 19:06:08,886 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000/model.safetensors
2025-09-08 19:06:09,278 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000/optimizer.bin
2025-09-08 19:06:09,279 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000/scheduler.bin
2025-09-08 19:06:09,279 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000/sampler.bin
2025-09-08 19:06:09,279 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_62_step_100000/random_states_0.pkl
2025-09-08 19:07:53,552 Stage: Train 0.5 | Epoch: 61 | Iter: 100200 | Total Loss: 0.003584 | Recon Loss: 0.003013 | Commit Loss: 0.001143 | Perplexity: 1382.833203
2025-09-08 19:09:36,266 Stage: Train 0.5 | Epoch: 61 | Iter: 100400 | Total Loss: 0.003602 | Recon Loss: 0.003040 | Commit Loss: 0.001124 | Perplexity: 1410.483312
2025-09-08 19:11:18,812 Stage: Train 0.5 | Epoch: 61 | Iter: 100600 | Total Loss: 0.003475 | Recon Loss: 0.002927 | Commit Loss: 0.001096 | Perplexity: 1383.798033
Trainning Epoch:  20%|██        | 62/308 [14:14:35<57:02:54, 834.85s/it]2025-09-08 19:13:01,061 Stage: Train 0.5 | Epoch: 62 | Iter: 100800 | Total Loss: 0.003540 | Recon Loss: 0.002975 | Commit Loss: 0.001131 | Perplexity: 1404.001337
2025-09-08 19:14:43,689 Stage: Train 0.5 | Epoch: 62 | Iter: 101000 | Total Loss: 0.003538 | Recon Loss: 0.002972 | Commit Loss: 0.001132 | Perplexity: 1387.043068
2025-09-08 19:16:26,275 Stage: Train 0.5 | Epoch: 62 | Iter: 101200 | Total Loss: 0.003643 | Recon Loss: 0.003069 | Commit Loss: 0.001149 | Perplexity: 1389.604904
2025-09-08 19:18:08,862 Stage: Train 0.5 | Epoch: 62 | Iter: 101400 | Total Loss: 0.003529 | Recon Loss: 0.002955 | Commit Loss: 0.001149 | Perplexity: 1393.408027
2025-09-08 19:19:51,389 Stage: Train 0.5 | Epoch: 62 | Iter: 101600 | Total Loss: 0.003512 | Recon Loss: 0.002959 | Commit Loss: 0.001106 | Perplexity: 1384.202377
2025-09-08 19:21:33,832 Stage: Train 0.5 | Epoch: 62 | Iter: 101800 | Total Loss: 0.003670 | Recon Loss: 0.003104 | Commit Loss: 0.001131 | Perplexity: 1388.629004
2025-09-08 19:23:16,427 Stage: Train 0.5 | Epoch: 62 | Iter: 102000 | Total Loss: 0.003547 | Recon Loss: 0.002990 | Commit Loss: 0.001115 | Perplexity: 1392.932342
2025-09-08 19:24:58,897 Stage: Train 0.5 | Epoch: 62 | Iter: 102200 | Total Loss: 0.003575 | Recon Loss: 0.003010 | Commit Loss: 0.001129 | Perplexity: 1398.784095
Trainning Epoch:  20%|██        | 63/308 [14:28:27<56:45:42, 834.05s/it]2025-09-08 19:26:40,974 Stage: Train 0.5 | Epoch: 63 | Iter: 102400 | Total Loss: 0.003457 | Recon Loss: 0.002902 | Commit Loss: 0.001110 | Perplexity: 1395.487119
2025-09-08 19:28:23,560 Stage: Train 0.5 | Epoch: 63 | Iter: 102600 | Total Loss: 0.003607 | Recon Loss: 0.003019 | Commit Loss: 0.001176 | Perplexity: 1394.278860
2025-09-08 19:30:06,102 Stage: Train 0.5 | Epoch: 63 | Iter: 102800 | Total Loss: 0.003550 | Recon Loss: 0.002984 | Commit Loss: 0.001132 | Perplexity: 1393.999734
2025-09-08 19:31:48,679 Stage: Train 0.5 | Epoch: 63 | Iter: 103000 | Total Loss: 0.003638 | Recon Loss: 0.003079 | Commit Loss: 0.001118 | Perplexity: 1399.481607
2025-09-08 19:33:31,449 Stage: Train 0.5 | Epoch: 63 | Iter: 103200 | Total Loss: 0.003447 | Recon Loss: 0.002893 | Commit Loss: 0.001107 | Perplexity: 1387.630849
2025-09-08 19:35:13,767 Stage: Train 0.5 | Epoch: 63 | Iter: 103400 | Total Loss: 0.003554 | Recon Loss: 0.002989 | Commit Loss: 0.001129 | Perplexity: 1399.379044
2025-09-08 19:36:56,244 Stage: Train 0.5 | Epoch: 63 | Iter: 103600 | Total Loss: 0.003611 | Recon Loss: 0.003042 | Commit Loss: 0.001138 | Perplexity: 1397.486268
2025-09-08 19:38:38,774 Stage: Train 0.5 | Epoch: 63 | Iter: 103800 | Total Loss: 0.003533 | Recon Loss: 0.002971 | Commit Loss: 0.001124 | Perplexity: 1388.805801
Trainning Epoch:  21%|██        | 64/308 [14:42:19<56:29:51, 833.57s/it]2025-09-08 19:40:21,160 Stage: Train 0.5 | Epoch: 64 | Iter: 104000 | Total Loss: 0.003565 | Recon Loss: 0.003017 | Commit Loss: 0.001096 | Perplexity: 1380.330433
2025-09-08 19:42:03,512 Stage: Train 0.5 | Epoch: 64 | Iter: 104200 | Total Loss: 0.003477 | Recon Loss: 0.002907 | Commit Loss: 0.001141 | Perplexity: 1402.790499
2025-09-08 19:43:46,155 Stage: Train 0.5 | Epoch: 64 | Iter: 104400 | Total Loss: 0.003477 | Recon Loss: 0.002920 | Commit Loss: 0.001114 | Perplexity: 1406.388297
2025-09-08 19:45:28,995 Stage: Train 0.5 | Epoch: 64 | Iter: 104600 | Total Loss: 0.003515 | Recon Loss: 0.002952 | Commit Loss: 0.001127 | Perplexity: 1403.804144
2025-09-08 19:47:11,500 Stage: Train 0.5 | Epoch: 64 | Iter: 104800 | Total Loss: 0.003600 | Recon Loss: 0.003029 | Commit Loss: 0.001143 | Perplexity: 1391.703614
2025-09-08 19:48:54,066 Stage: Train 0.5 | Epoch: 64 | Iter: 105000 | Total Loss: 0.003521 | Recon Loss: 0.002951 | Commit Loss: 0.001140 | Perplexity: 1397.409824
2025-09-08 19:50:36,472 Stage: Train 0.5 | Epoch: 64 | Iter: 105200 | Total Loss: 0.003484 | Recon Loss: 0.002929 | Commit Loss: 0.001109 | Perplexity: 1382.721489
2025-09-08 19:52:19,227 Stage: Train 0.5 | Epoch: 64 | Iter: 105400 | Total Loss: 0.003667 | Recon Loss: 0.003085 | Commit Loss: 0.001163 | Perplexity: 1390.705875
Trainning Epoch:  21%|██        | 65/308 [14:56:12<56:15:07, 833.36s/it]2025-09-08 19:54:01,742 Stage: Train 0.5 | Epoch: 65 | Iter: 105600 | Total Loss: 0.003476 | Recon Loss: 0.002917 | Commit Loss: 0.001118 | Perplexity: 1394.783152
2025-09-08 19:55:44,229 Stage: Train 0.5 | Epoch: 65 | Iter: 105800 | Total Loss: 0.003521 | Recon Loss: 0.002970 | Commit Loss: 0.001104 | Perplexity: 1396.085612
2025-09-08 19:57:26,727 Stage: Train 0.5 | Epoch: 65 | Iter: 106000 | Total Loss: 0.003601 | Recon Loss: 0.003012 | Commit Loss: 0.001177 | Perplexity: 1391.268069
2025-09-08 19:59:09,274 Stage: Train 0.5 | Epoch: 65 | Iter: 106200 | Total Loss: 0.003528 | Recon Loss: 0.002963 | Commit Loss: 0.001129 | Perplexity: 1396.754882
2025-09-08 20:00:51,719 Stage: Train 0.5 | Epoch: 65 | Iter: 106400 | Total Loss: 0.003475 | Recon Loss: 0.002910 | Commit Loss: 0.001131 | Perplexity: 1400.779101
2025-09-08 20:02:34,277 Stage: Train 0.5 | Epoch: 65 | Iter: 106600 | Total Loss: 0.003408 | Recon Loss: 0.002848 | Commit Loss: 0.001121 | Perplexity: 1404.631147
2025-09-08 20:04:17,023 Stage: Train 0.5 | Epoch: 65 | Iter: 106800 | Total Loss: 0.003622 | Recon Loss: 0.003058 | Commit Loss: 0.001127 | Perplexity: 1404.694688
2025-09-08 20:05:59,761 Stage: Train 0.5 | Epoch: 65 | Iter: 107000 | Total Loss: 0.003596 | Recon Loss: 0.003035 | Commit Loss: 0.001121 | Perplexity: 1385.207933
Trainning Epoch:  21%|██▏       | 66/308 [15:10:05<56:00:17, 833.13s/it]2025-09-08 20:07:42,064 Stage: Train 0.5 | Epoch: 66 | Iter: 107200 | Total Loss: 0.003435 | Recon Loss: 0.002870 | Commit Loss: 0.001129 | Perplexity: 1402.652305
2025-09-08 20:09:24,565 Stage: Train 0.5 | Epoch: 66 | Iter: 107400 | Total Loss: 0.003552 | Recon Loss: 0.002983 | Commit Loss: 0.001139 | Perplexity: 1392.709352
2025-09-08 20:11:06,984 Stage: Train 0.5 | Epoch: 66 | Iter: 107600 | Total Loss: 0.003607 | Recon Loss: 0.003032 | Commit Loss: 0.001151 | Perplexity: 1392.219418
2025-09-08 20:12:49,646 Stage: Train 0.5 | Epoch: 66 | Iter: 107800 | Total Loss: 0.003455 | Recon Loss: 0.002889 | Commit Loss: 0.001133 | Perplexity: 1399.738691
2025-09-08 20:14:31,999 Stage: Train 0.5 | Epoch: 66 | Iter: 108000 | Total Loss: 0.003474 | Recon Loss: 0.002904 | Commit Loss: 0.001140 | Perplexity: 1400.727056
2025-09-08 20:16:14,315 Stage: Train 0.5 | Epoch: 66 | Iter: 108200 | Total Loss: 0.003509 | Recon Loss: 0.002951 | Commit Loss: 0.001115 | Perplexity: 1395.975331
2025-09-08 20:17:56,598 Stage: Train 0.5 | Epoch: 66 | Iter: 108400 | Total Loss: 0.003395 | Recon Loss: 0.002837 | Commit Loss: 0.001116 | Perplexity: 1400.269429
2025-09-08 20:19:39,040 Stage: Train 0.5 | Epoch: 66 | Iter: 108600 | Total Loss: 0.003578 | Recon Loss: 0.003009 | Commit Loss: 0.001138 | Perplexity: 1388.888178
2025-09-08 20:21:21,377 Stage: Train 0.5 | Epoch: 66 | Iter: 108800 | Total Loss: 0.003499 | Recon Loss: 0.002946 | Commit Loss: 0.001105 | Perplexity: 1393.962797
Trainning Epoch:  22%|██▏       | 67/308 [15:23:56<55:44:29, 832.65s/it]2025-09-08 20:23:03,270 Stage: Train 0.5 | Epoch: 67 | Iter: 109000 | Total Loss: 0.003565 | Recon Loss: 0.003003 | Commit Loss: 0.001125 | Perplexity: 1409.510220
2025-09-08 20:24:45,524 Stage: Train 0.5 | Epoch: 67 | Iter: 109200 | Total Loss: 0.003421 | Recon Loss: 0.002860 | Commit Loss: 0.001121 | Perplexity: 1406.608812
2025-09-08 20:26:27,821 Stage: Train 0.5 | Epoch: 67 | Iter: 109400 | Total Loss: 0.003601 | Recon Loss: 0.003032 | Commit Loss: 0.001137 | Perplexity: 1394.161831
2025-09-08 20:28:10,092 Stage: Train 0.5 | Epoch: 67 | Iter: 109600 | Total Loss: 0.003449 | Recon Loss: 0.002878 | Commit Loss: 0.001143 | Perplexity: 1398.427327
2025-09-08 20:29:52,424 Stage: Train 0.5 | Epoch: 67 | Iter: 109800 | Total Loss: 0.003525 | Recon Loss: 0.002949 | Commit Loss: 0.001151 | Perplexity: 1397.733129
2025-09-08 20:31:34,783 Stage: Train 0.5 | Epoch: 67 | Iter: 110000 | Total Loss: 0.003437 | Recon Loss: 0.002893 | Commit Loss: 0.001088 | Perplexity: 1395.681381
2025-09-08 20:33:17,249 Stage: Train 0.5 | Epoch: 67 | Iter: 110200 | Total Loss: 0.003534 | Recon Loss: 0.002980 | Commit Loss: 0.001108 | Perplexity: 1389.204827
2025-09-08 20:34:59,650 Stage: Train 0.5 | Epoch: 67 | Iter: 110400 | Total Loss: 0.003454 | Recon Loss: 0.002893 | Commit Loss: 0.001121 | Perplexity: 1387.250882
Trainning Epoch:  22%|██▏       | 68/308 [15:37:47<55:28:08, 832.03s/it]2025-09-08 20:36:42,149 Stage: Train 0.5 | Epoch: 68 | Iter: 110600 | Total Loss: 0.003539 | Recon Loss: 0.002959 | Commit Loss: 0.001160 | Perplexity: 1388.955432
2025-09-08 20:38:24,906 Stage: Train 0.5 | Epoch: 68 | Iter: 110800 | Total Loss: 0.003413 | Recon Loss: 0.002863 | Commit Loss: 0.001099 | Perplexity: 1393.952697
2025-09-08 20:40:07,568 Stage: Train 0.5 | Epoch: 68 | Iter: 111000 | Total Loss: 0.003467 | Recon Loss: 0.002903 | Commit Loss: 0.001129 | Perplexity: 1414.196362
2025-09-08 20:41:50,411 Stage: Train 0.5 | Epoch: 68 | Iter: 111200 | Total Loss: 0.003554 | Recon Loss: 0.002979 | Commit Loss: 0.001150 | Perplexity: 1407.416469
2025-09-08 20:43:33,224 Stage: Train 0.5 | Epoch: 68 | Iter: 111400 | Total Loss: 0.003513 | Recon Loss: 0.002955 | Commit Loss: 0.001116 | Perplexity: 1400.463241
2025-09-08 20:45:15,882 Stage: Train 0.5 | Epoch: 68 | Iter: 111600 | Total Loss: 0.003424 | Recon Loss: 0.002880 | Commit Loss: 0.001089 | Perplexity: 1390.200754
2025-09-08 20:46:58,711 Stage: Train 0.5 | Epoch: 68 | Iter: 111800 | Total Loss: 0.003533 | Recon Loss: 0.002968 | Commit Loss: 0.001130 | Perplexity: 1399.097013
2025-09-08 20:48:41,636 Stage: Train 0.5 | Epoch: 68 | Iter: 112000 | Total Loss: 0.003520 | Recon Loss: 0.002942 | Commit Loss: 0.001155 | Perplexity: 1399.097527
Trainning Epoch:  22%|██▏       | 69/308 [15:51:41<55:17:08, 832.76s/it]2025-09-08 20:50:24,627 Stage: Train 0.5 | Epoch: 69 | Iter: 112200 | Total Loss: 0.003521 | Recon Loss: 0.002968 | Commit Loss: 0.001105 | Perplexity: 1387.174521
2025-09-08 20:52:07,395 Stage: Train 0.5 | Epoch: 69 | Iter: 112400 | Total Loss: 0.003489 | Recon Loss: 0.002930 | Commit Loss: 0.001118 | Perplexity: 1411.801332
2025-09-08 20:53:50,185 Stage: Train 0.5 | Epoch: 69 | Iter: 112600 | Total Loss: 0.003472 | Recon Loss: 0.002919 | Commit Loss: 0.001108 | Perplexity: 1387.637921
2025-09-08 20:55:32,811 Stage: Train 0.5 | Epoch: 69 | Iter: 112800 | Total Loss: 0.003516 | Recon Loss: 0.002943 | Commit Loss: 0.001146 | Perplexity: 1400.601786
2025-09-08 20:57:15,190 Stage: Train 0.5 | Epoch: 69 | Iter: 113000 | Total Loss: 0.003502 | Recon Loss: 0.002927 | Commit Loss: 0.001149 | Perplexity: 1398.613998
2025-09-08 20:58:57,636 Stage: Train 0.5 | Epoch: 69 | Iter: 113200 | Total Loss: 0.003376 | Recon Loss: 0.002823 | Commit Loss: 0.001107 | Perplexity: 1401.956805
2025-09-08 21:00:40,264 Stage: Train 0.5 | Epoch: 69 | Iter: 113400 | Total Loss: 0.003447 | Recon Loss: 0.002889 | Commit Loss: 0.001116 | Perplexity: 1404.355444
2025-09-08 21:02:22,861 Stage: Train 0.5 | Epoch: 69 | Iter: 113600 | Total Loss: 0.003458 | Recon Loss: 0.002895 | Commit Loss: 0.001125 | Perplexity: 1398.558322
Trainning Epoch:  23%|██▎       | 70/308 [16:05:35<55:03:56, 832.93s/it]2025-09-08 21:04:05,114 Stage: Train 0.5 | Epoch: 70 | Iter: 113800 | Total Loss: 0.003505 | Recon Loss: 0.002951 | Commit Loss: 0.001108 | Perplexity: 1406.807313
2025-09-08 21:05:47,567 Stage: Train 0.5 | Epoch: 70 | Iter: 114000 | Total Loss: 0.003423 | Recon Loss: 0.002863 | Commit Loss: 0.001121 | Perplexity: 1402.711198
2025-09-08 21:07:30,134 Stage: Train 0.5 | Epoch: 70 | Iter: 114200 | Total Loss: 0.003363 | Recon Loss: 0.002814 | Commit Loss: 0.001099 | Perplexity: 1396.242619
2025-09-08 21:09:12,706 Stage: Train 0.5 | Epoch: 70 | Iter: 114400 | Total Loss: 0.003544 | Recon Loss: 0.002966 | Commit Loss: 0.001156 | Perplexity: 1395.812501
2025-09-08 21:10:55,171 Stage: Train 0.5 | Epoch: 70 | Iter: 114600 | Total Loss: 0.003427 | Recon Loss: 0.002868 | Commit Loss: 0.001119 | Perplexity: 1404.936547
2025-09-08 21:12:37,702 Stage: Train 0.5 | Epoch: 70 | Iter: 114800 | Total Loss: 0.003422 | Recon Loss: 0.002860 | Commit Loss: 0.001124 | Perplexity: 1403.267204
2025-09-08 21:14:20,053 Stage: Train 0.5 | Epoch: 70 | Iter: 115000 | Total Loss: 0.003485 | Recon Loss: 0.002911 | Commit Loss: 0.001146 | Perplexity: 1403.614437
2025-09-08 21:16:02,607 Stage: Train 0.5 | Epoch: 70 | Iter: 115200 | Total Loss: 0.003416 | Recon Loss: 0.002852 | Commit Loss: 0.001129 | Perplexity: 1407.083619
Trainning Epoch:  23%|██▎       | 71/308 [16:19:27<54:49:04, 832.68s/it]2025-09-08 21:17:44,939 Stage: Train 0.5 | Epoch: 71 | Iter: 115400 | Total Loss: 0.003346 | Recon Loss: 0.002787 | Commit Loss: 0.001118 | Perplexity: 1403.106107
2025-09-08 21:19:27,653 Stage: Train 0.5 | Epoch: 71 | Iter: 115600 | Total Loss: 0.003533 | Recon Loss: 0.002975 | Commit Loss: 0.001116 | Perplexity: 1405.655924
2025-09-08 21:21:10,287 Stage: Train 0.5 | Epoch: 71 | Iter: 115800 | Total Loss: 0.003397 | Recon Loss: 0.002839 | Commit Loss: 0.001115 | Perplexity: 1404.851360
2025-09-08 21:22:52,816 Stage: Train 0.5 | Epoch: 71 | Iter: 116000 | Total Loss: 0.003427 | Recon Loss: 0.002874 | Commit Loss: 0.001105 | Perplexity: 1401.562865
2025-09-08 21:24:35,300 Stage: Train 0.5 | Epoch: 71 | Iter: 116200 | Total Loss: 0.003491 | Recon Loss: 0.002919 | Commit Loss: 0.001144 | Perplexity: 1406.085717
2025-09-08 21:26:18,022 Stage: Train 0.5 | Epoch: 71 | Iter: 116400 | Total Loss: 0.003479 | Recon Loss: 0.002924 | Commit Loss: 0.001111 | Perplexity: 1388.016719
2025-09-08 21:28:00,784 Stage: Train 0.5 | Epoch: 71 | Iter: 116600 | Total Loss: 0.003407 | Recon Loss: 0.002850 | Commit Loss: 0.001115 | Perplexity: 1399.161940
2025-09-08 21:29:43,557 Stage: Train 0.5 | Epoch: 71 | Iter: 116800 | Total Loss: 0.003403 | Recon Loss: 0.002843 | Commit Loss: 0.001120 | Perplexity: 1405.961241
Trainning Epoch:  23%|██▎       | 72/308 [16:33:20<54:35:49, 832.84s/it]2025-09-08 21:31:25,933 Stage: Train 0.5 | Epoch: 72 | Iter: 117000 | Total Loss: 0.003610 | Recon Loss: 0.003029 | Commit Loss: 0.001164 | Perplexity: 1405.093992
2025-09-08 21:33:08,595 Stage: Train 0.5 | Epoch: 72 | Iter: 117200 | Total Loss: 0.003519 | Recon Loss: 0.002949 | Commit Loss: 0.001141 | Perplexity: 1405.861730
2025-09-08 21:34:51,219 Stage: Train 0.5 | Epoch: 72 | Iter: 117400 | Total Loss: 0.003521 | Recon Loss: 0.002944 | Commit Loss: 0.001155 | Perplexity: 1407.432651
2025-09-08 21:36:33,695 Stage: Train 0.5 | Epoch: 72 | Iter: 117600 | Total Loss: 0.003452 | Recon Loss: 0.002902 | Commit Loss: 0.001099 | Perplexity: 1392.742877
2025-09-08 21:38:16,166 Stage: Train 0.5 | Epoch: 72 | Iter: 117800 | Total Loss: 0.003379 | Recon Loss: 0.002831 | Commit Loss: 0.001095 | Perplexity: 1401.517582
2025-09-08 21:39:58,720 Stage: Train 0.5 | Epoch: 72 | Iter: 118000 | Total Loss: 0.003437 | Recon Loss: 0.002879 | Commit Loss: 0.001116 | Perplexity: 1406.984419
2025-09-08 21:41:41,175 Stage: Train 0.5 | Epoch: 72 | Iter: 118200 | Total Loss: 0.003451 | Recon Loss: 0.002897 | Commit Loss: 0.001108 | Perplexity: 1401.860105
2025-09-08 21:43:23,729 Stage: Train 0.5 | Epoch: 72 | Iter: 118400 | Total Loss: 0.003385 | Recon Loss: 0.002834 | Commit Loss: 0.001102 | Perplexity: 1398.366959
Trainning Epoch:  24%|██▎       | 73/308 [16:47:12<54:21:33, 832.74s/it]2025-09-08 21:45:06,103 Stage: Train 0.5 | Epoch: 73 | Iter: 118600 | Total Loss: 0.003397 | Recon Loss: 0.002848 | Commit Loss: 0.001098 | Perplexity: 1390.715905
2025-09-08 21:46:48,471 Stage: Train 0.5 | Epoch: 73 | Iter: 118800 | Total Loss: 0.003574 | Recon Loss: 0.002996 | Commit Loss: 0.001156 | Perplexity: 1402.550930
2025-09-08 21:48:30,811 Stage: Train 0.5 | Epoch: 73 | Iter: 119000 | Total Loss: 0.003405 | Recon Loss: 0.002850 | Commit Loss: 0.001111 | Perplexity: 1402.786989
2025-09-08 21:50:13,355 Stage: Train 0.5 | Epoch: 73 | Iter: 119200 | Total Loss: 0.003427 | Recon Loss: 0.002873 | Commit Loss: 0.001108 | Perplexity: 1394.668709
2025-09-08 21:51:55,887 Stage: Train 0.5 | Epoch: 73 | Iter: 119400 | Total Loss: 0.003442 | Recon Loss: 0.002891 | Commit Loss: 0.001102 | Perplexity: 1398.069048
2025-09-08 21:53:38,465 Stage: Train 0.5 | Epoch: 73 | Iter: 119600 | Total Loss: 0.003480 | Recon Loss: 0.002922 | Commit Loss: 0.001116 | Perplexity: 1397.468024
2025-09-08 21:55:21,108 Stage: Train 0.5 | Epoch: 73 | Iter: 119800 | Total Loss: 0.003377 | Recon Loss: 0.002824 | Commit Loss: 0.001107 | Perplexity: 1401.533690
2025-09-08 21:57:03,611 Stage: Train 0.5 | Epoch: 73 | Iter: 120000 | Total Loss: 0.003432 | Recon Loss: 0.002879 | Commit Loss: 0.001106 | Perplexity: 1403.983515
2025-09-08 21:57:03,611 Saving model at iteration 120000
2025-09-08 21:57:03,724 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000
2025-09-08 21:57:04,098 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000/model.safetensors
2025-09-08 21:57:04,473 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000/optimizer.bin
2025-09-08 21:57:04,474 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000/scheduler.bin
2025-09-08 21:57:04,474 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000/sampler.bin
2025-09-08 21:57:04,475 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_74_step_120000/random_states_0.pkl
Trainning Epoch:  24%|██▍       | 74/308 [17:01:05<54:08:05, 832.84s/it]2025-09-08 21:58:46,909 Stage: Train 0.5 | Epoch: 74 | Iter: 120200 | Total Loss: 0.003370 | Recon Loss: 0.002814 | Commit Loss: 0.001111 | Perplexity: 1413.161337
2025-09-08 22:00:29,244 Stage: Train 0.5 | Epoch: 74 | Iter: 120400 | Total Loss: 0.003502 | Recon Loss: 0.002932 | Commit Loss: 0.001141 | Perplexity: 1395.076472
2025-09-08 22:02:11,633 Stage: Train 0.5 | Epoch: 74 | Iter: 120600 | Total Loss: 0.003334 | Recon Loss: 0.002780 | Commit Loss: 0.001109 | Perplexity: 1407.827381
2025-09-08 22:03:54,050 Stage: Train 0.5 | Epoch: 74 | Iter: 120800 | Total Loss: 0.003418 | Recon Loss: 0.002879 | Commit Loss: 0.001080 | Perplexity: 1395.634159
2025-09-08 22:05:36,529 Stage: Train 0.5 | Epoch: 74 | Iter: 121000 | Total Loss: 0.003473 | Recon Loss: 0.002893 | Commit Loss: 0.001159 | Perplexity: 1425.969923
2025-09-08 22:07:19,031 Stage: Train 0.5 | Epoch: 74 | Iter: 121200 | Total Loss: 0.003469 | Recon Loss: 0.002914 | Commit Loss: 0.001108 | Perplexity: 1391.111398
2025-09-08 22:09:01,525 Stage: Train 0.5 | Epoch: 74 | Iter: 121400 | Total Loss: 0.003351 | Recon Loss: 0.002806 | Commit Loss: 0.001090 | Perplexity: 1392.575266
2025-09-08 22:10:43,976 Stage: Train 0.5 | Epoch: 74 | Iter: 121600 | Total Loss: 0.003467 | Recon Loss: 0.002918 | Commit Loss: 0.001098 | Perplexity: 1393.383062
2025-09-08 22:12:26,461 Stage: Train 0.5 | Epoch: 74 | Iter: 121800 | Total Loss: 0.003371 | Recon Loss: 0.002813 | Commit Loss: 0.001115 | Perplexity: 1408.041030
Trainning Epoch:  24%|██▍       | 75/308 [17:14:57<53:53:02, 832.54s/it]2025-09-08 22:14:08,877 Stage: Train 0.5 | Epoch: 75 | Iter: 122000 | Total Loss: 0.003388 | Recon Loss: 0.002833 | Commit Loss: 0.001109 | Perplexity: 1401.063281
2025-09-08 22:15:51,358 Stage: Train 0.5 | Epoch: 75 | Iter: 122200 | Total Loss: 0.003464 | Recon Loss: 0.002921 | Commit Loss: 0.001086 | Perplexity: 1395.658204
2025-09-08 22:17:33,991 Stage: Train 0.5 | Epoch: 75 | Iter: 122400 | Total Loss: 0.003403 | Recon Loss: 0.002852 | Commit Loss: 0.001103 | Perplexity: 1395.582603
2025-09-08 22:19:16,240 Stage: Train 0.5 | Epoch: 75 | Iter: 122600 | Total Loss: 0.003417 | Recon Loss: 0.002856 | Commit Loss: 0.001122 | Perplexity: 1403.501030
2025-09-08 22:20:58,615 Stage: Train 0.5 | Epoch: 75 | Iter: 122800 | Total Loss: 0.003444 | Recon Loss: 0.002892 | Commit Loss: 0.001104 | Perplexity: 1406.852704
2025-09-08 22:22:41,058 Stage: Train 0.5 | Epoch: 75 | Iter: 123000 | Total Loss: 0.003433 | Recon Loss: 0.002892 | Commit Loss: 0.001084 | Perplexity: 1410.942399
2025-09-08 22:24:23,528 Stage: Train 0.5 | Epoch: 75 | Iter: 123200 | Total Loss: 0.003435 | Recon Loss: 0.002857 | Commit Loss: 0.001157 | Perplexity: 1396.514799
2025-09-08 22:26:06,122 Stage: Train 0.5 | Epoch: 75 | Iter: 123400 | Total Loss: 0.003394 | Recon Loss: 0.002832 | Commit Loss: 0.001123 | Perplexity: 1403.346188
Trainning Epoch:  25%|██▍       | 76/308 [17:28:49<53:38:22, 832.34s/it]2025-09-08 22:27:48,525 Stage: Train 0.5 | Epoch: 76 | Iter: 123600 | Total Loss: 0.003375 | Recon Loss: 0.002819 | Commit Loss: 0.001112 | Perplexity: 1398.386025
2025-09-08 22:29:30,988 Stage: Train 0.5 | Epoch: 76 | Iter: 123800 | Total Loss: 0.003443 | Recon Loss: 0.002891 | Commit Loss: 0.001104 | Perplexity: 1399.369921
2025-09-08 22:31:13,735 Stage: Train 0.5 | Epoch: 76 | Iter: 124000 | Total Loss: 0.003336 | Recon Loss: 0.002778 | Commit Loss: 0.001115 | Perplexity: 1404.972380
2025-09-08 22:32:56,223 Stage: Train 0.5 | Epoch: 76 | Iter: 124200 | Total Loss: 0.003400 | Recon Loss: 0.002845 | Commit Loss: 0.001110 | Perplexity: 1405.228896
2025-09-08 22:34:38,571 Stage: Train 0.5 | Epoch: 76 | Iter: 124400 | Total Loss: 0.003377 | Recon Loss: 0.002832 | Commit Loss: 0.001090 | Perplexity: 1397.875637
2025-09-08 22:36:21,096 Stage: Train 0.5 | Epoch: 76 | Iter: 124600 | Total Loss: 0.003345 | Recon Loss: 0.002798 | Commit Loss: 0.001095 | Perplexity: 1408.330213
2025-09-08 22:38:03,552 Stage: Train 0.5 | Epoch: 76 | Iter: 124800 | Total Loss: 0.003439 | Recon Loss: 0.002886 | Commit Loss: 0.001107 | Perplexity: 1399.569044
2025-09-08 22:39:45,948 Stage: Train 0.5 | Epoch: 76 | Iter: 125000 | Total Loss: 0.003463 | Recon Loss: 0.002883 | Commit Loss: 0.001161 | Perplexity: 1407.143193
Trainning Epoch:  25%|██▌       | 77/308 [17:42:41<53:24:10, 832.25s/it]2025-09-08 22:41:28,257 Stage: Train 0.5 | Epoch: 77 | Iter: 125200 | Total Loss: 0.003383 | Recon Loss: 0.002825 | Commit Loss: 0.001117 | Perplexity: 1408.751955
2025-09-08 22:43:10,415 Stage: Train 0.5 | Epoch: 77 | Iter: 125400 | Total Loss: 0.003455 | Recon Loss: 0.002900 | Commit Loss: 0.001111 | Perplexity: 1405.341689
2025-09-08 22:44:52,942 Stage: Train 0.5 | Epoch: 77 | Iter: 125600 | Total Loss: 0.003322 | Recon Loss: 0.002784 | Commit Loss: 0.001076 | Perplexity: 1399.536173
2025-09-08 22:46:35,219 Stage: Train 0.5 | Epoch: 77 | Iter: 125800 | Total Loss: 0.003446 | Recon Loss: 0.002887 | Commit Loss: 0.001117 | Perplexity: 1400.963881
2025-09-08 22:48:17,475 Stage: Train 0.5 | Epoch: 77 | Iter: 126000 | Total Loss: 0.003385 | Recon Loss: 0.002831 | Commit Loss: 0.001108 | Perplexity: 1409.730062
2025-09-08 22:49:59,955 Stage: Train 0.5 | Epoch: 77 | Iter: 126200 | Total Loss: 0.003403 | Recon Loss: 0.002860 | Commit Loss: 0.001085 | Perplexity: 1397.957513
2025-09-08 22:51:42,606 Stage: Train 0.5 | Epoch: 77 | Iter: 126400 | Total Loss: 0.003352 | Recon Loss: 0.002801 | Commit Loss: 0.001101 | Perplexity: 1397.479988
2025-09-08 22:53:25,279 Stage: Train 0.5 | Epoch: 77 | Iter: 126600 | Total Loss: 0.003447 | Recon Loss: 0.002879 | Commit Loss: 0.001136 | Perplexity: 1396.303312
Trainning Epoch:  25%|██▌       | 78/308 [17:56:33<53:09:39, 832.08s/it]2025-09-08 22:55:07,665 Stage: Train 0.5 | Epoch: 78 | Iter: 126800 | Total Loss: 0.003379 | Recon Loss: 0.002830 | Commit Loss: 0.001099 | Perplexity: 1401.989575
2025-09-08 22:56:50,109 Stage: Train 0.5 | Epoch: 78 | Iter: 127000 | Total Loss: 0.003432 | Recon Loss: 0.002878 | Commit Loss: 0.001108 | Perplexity: 1415.786010
2025-09-08 22:58:32,784 Stage: Train 0.5 | Epoch: 78 | Iter: 127200 | Total Loss: 0.003358 | Recon Loss: 0.002813 | Commit Loss: 0.001089 | Perplexity: 1409.575412
2025-09-08 23:00:15,598 Stage: Train 0.5 | Epoch: 78 | Iter: 127400 | Total Loss: 0.003324 | Recon Loss: 0.002772 | Commit Loss: 0.001103 | Perplexity: 1399.023575
2025-09-08 23:01:58,622 Stage: Train 0.5 | Epoch: 78 | Iter: 127600 | Total Loss: 0.003448 | Recon Loss: 0.002894 | Commit Loss: 0.001107 | Perplexity: 1401.848352
2025-09-08 23:03:41,532 Stage: Train 0.5 | Epoch: 78 | Iter: 127800 | Total Loss: 0.003400 | Recon Loss: 0.002845 | Commit Loss: 0.001108 | Perplexity: 1405.307435
2025-09-08 23:05:24,506 Stage: Train 0.5 | Epoch: 78 | Iter: 128000 | Total Loss: 0.003390 | Recon Loss: 0.002846 | Commit Loss: 0.001089 | Perplexity: 1396.431463
2025-09-08 23:07:07,304 Stage: Train 0.5 | Epoch: 78 | Iter: 128200 | Total Loss: 0.003282 | Recon Loss: 0.002734 | Commit Loss: 0.001095 | Perplexity: 1406.369709
Trainning Epoch:  26%|██▌       | 79/308 [18:10:27<52:58:22, 832.76s/it]2025-09-08 23:08:49,902 Stage: Train 0.5 | Epoch: 79 | Iter: 128400 | Total Loss: 0.003379 | Recon Loss: 0.002837 | Commit Loss: 0.001085 | Perplexity: 1394.399205
2025-09-08 23:10:32,308 Stage: Train 0.5 | Epoch: 79 | Iter: 128600 | Total Loss: 0.003358 | Recon Loss: 0.002805 | Commit Loss: 0.001105 | Perplexity: 1404.407956
2025-09-08 23:12:14,768 Stage: Train 0.5 | Epoch: 79 | Iter: 128800 | Total Loss: 0.003435 | Recon Loss: 0.002892 | Commit Loss: 0.001085 | Perplexity: 1401.816964
2025-09-08 23:13:57,154 Stage: Train 0.5 | Epoch: 79 | Iter: 129000 | Total Loss: 0.003376 | Recon Loss: 0.002829 | Commit Loss: 0.001094 | Perplexity: 1408.804498
2025-09-08 23:15:39,525 Stage: Train 0.5 | Epoch: 79 | Iter: 129200 | Total Loss: 0.003409 | Recon Loss: 0.002860 | Commit Loss: 0.001099 | Perplexity: 1406.315726
2025-09-08 23:17:21,841 Stage: Train 0.5 | Epoch: 79 | Iter: 129400 | Total Loss: 0.003412 | Recon Loss: 0.002852 | Commit Loss: 0.001120 | Perplexity: 1405.273467
2025-09-08 23:19:04,113 Stage: Train 0.5 | Epoch: 79 | Iter: 129600 | Total Loss: 0.003384 | Recon Loss: 0.002830 | Commit Loss: 0.001109 | Perplexity: 1397.245596
2025-09-08 23:20:46,686 Stage: Train 0.5 | Epoch: 79 | Iter: 129800 | Total Loss: 0.003472 | Recon Loss: 0.002930 | Commit Loss: 0.001083 | Perplexity: 1408.861743
Trainning Epoch:  26%|██▌       | 80/308 [18:24:19<52:43:04, 832.39s/it]2025-09-08 23:22:28,944 Stage: Train 0.5 | Epoch: 80 | Iter: 130000 | Total Loss: 0.003283 | Recon Loss: 0.002737 | Commit Loss: 0.001093 | Perplexity: 1408.628036
2025-09-08 23:24:11,209 Stage: Train 0.5 | Epoch: 80 | Iter: 130200 | Total Loss: 0.003283 | Recon Loss: 0.002743 | Commit Loss: 0.001078 | Perplexity: 1404.297189
2025-09-08 23:25:53,966 Stage: Train 0.5 | Epoch: 80 | Iter: 130400 | Total Loss: 0.003331 | Recon Loss: 0.002792 | Commit Loss: 0.001078 | Perplexity: 1407.556072
2025-09-08 23:27:36,425 Stage: Train 0.5 | Epoch: 80 | Iter: 130600 | Total Loss: 0.003345 | Recon Loss: 0.002793 | Commit Loss: 0.001104 | Perplexity: 1408.702757
2025-09-08 23:29:18,978 Stage: Train 0.5 | Epoch: 80 | Iter: 130800 | Total Loss: 0.003328 | Recon Loss: 0.002792 | Commit Loss: 0.001071 | Perplexity: 1402.847409
2025-09-08 23:31:01,214 Stage: Train 0.5 | Epoch: 80 | Iter: 131000 | Total Loss: 0.003331 | Recon Loss: 0.002783 | Commit Loss: 0.001097 | Perplexity: 1401.512899
2025-09-08 23:32:43,553 Stage: Train 0.5 | Epoch: 80 | Iter: 131200 | Total Loss: 0.003371 | Recon Loss: 0.002808 | Commit Loss: 0.001124 | Perplexity: 1423.267652
2025-09-08 23:34:26,408 Stage: Train 0.5 | Epoch: 80 | Iter: 131400 | Total Loss: 0.003368 | Recon Loss: 0.002799 | Commit Loss: 0.001137 | Perplexity: 1403.516950
Trainning Epoch:  26%|██▋       | 81/308 [18:38:11<52:29:04, 832.36s/it]2025-09-08 23:36:09,092 Stage: Train 0.5 | Epoch: 81 | Iter: 131600 | Total Loss: 0.003320 | Recon Loss: 0.002774 | Commit Loss: 0.001092 | Perplexity: 1410.661197
2025-09-08 23:37:51,980 Stage: Train 0.5 | Epoch: 81 | Iter: 131800 | Total Loss: 0.003313 | Recon Loss: 0.002768 | Commit Loss: 0.001092 | Perplexity: 1401.568358
2025-09-08 23:39:34,742 Stage: Train 0.5 | Epoch: 81 | Iter: 132000 | Total Loss: 0.003490 | Recon Loss: 0.002906 | Commit Loss: 0.001167 | Perplexity: 1416.099919
2025-09-08 23:41:17,696 Stage: Train 0.5 | Epoch: 81 | Iter: 132200 | Total Loss: 0.003329 | Recon Loss: 0.002778 | Commit Loss: 0.001102 | Perplexity: 1411.874947
2025-09-08 23:43:00,397 Stage: Train 0.5 | Epoch: 81 | Iter: 132400 | Total Loss: 0.003408 | Recon Loss: 0.002864 | Commit Loss: 0.001088 | Perplexity: 1399.269141
2025-09-08 23:44:42,711 Stage: Train 0.5 | Epoch: 81 | Iter: 132600 | Total Loss: 0.003303 | Recon Loss: 0.002764 | Commit Loss: 0.001078 | Perplexity: 1406.086949
2025-09-08 23:46:25,068 Stage: Train 0.5 | Epoch: 81 | Iter: 132800 | Total Loss: 0.003278 | Recon Loss: 0.002732 | Commit Loss: 0.001091 | Perplexity: 1407.248101
2025-09-08 23:48:07,563 Stage: Train 0.5 | Epoch: 81 | Iter: 133000 | Total Loss: 0.003355 | Recon Loss: 0.002816 | Commit Loss: 0.001078 | Perplexity: 1412.649371
Trainning Epoch:  27%|██▋       | 82/308 [18:52:04<52:16:06, 832.60s/it]2025-09-08 23:49:49,771 Stage: Train 0.5 | Epoch: 82 | Iter: 133200 | Total Loss: 0.003262 | Recon Loss: 0.002716 | Commit Loss: 0.001091 | Perplexity: 1419.455420
2025-09-08 23:51:32,113 Stage: Train 0.5 | Epoch: 82 | Iter: 133400 | Total Loss: 0.003456 | Recon Loss: 0.002889 | Commit Loss: 0.001134 | Perplexity: 1395.136970
2025-09-08 23:53:14,705 Stage: Train 0.5 | Epoch: 82 | Iter: 133600 | Total Loss: 0.003302 | Recon Loss: 0.002755 | Commit Loss: 0.001095 | Perplexity: 1425.779305
2025-09-08 23:54:57,151 Stage: Train 0.5 | Epoch: 82 | Iter: 133800 | Total Loss: 0.003333 | Recon Loss: 0.002793 | Commit Loss: 0.001079 | Perplexity: 1420.077731
2025-09-08 23:56:39,657 Stage: Train 0.5 | Epoch: 82 | Iter: 134000 | Total Loss: 0.003318 | Recon Loss: 0.002776 | Commit Loss: 0.001084 | Perplexity: 1406.635968
2025-09-08 23:58:22,261 Stage: Train 0.5 | Epoch: 82 | Iter: 134200 | Total Loss: 0.003441 | Recon Loss: 0.002885 | Commit Loss: 0.001110 | Perplexity: 1410.803447
2025-09-09 00:00:04,853 Stage: Train 0.5 | Epoch: 82 | Iter: 134400 | Total Loss: 0.003227 | Recon Loss: 0.002701 | Commit Loss: 0.001053 | Perplexity: 1399.219018
2025-09-09 00:01:47,276 Stage: Train 0.5 | Epoch: 82 | Iter: 134600 | Total Loss: 0.003289 | Recon Loss: 0.002740 | Commit Loss: 0.001098 | Perplexity: 1412.685485
Trainning Epoch:  27%|██▋       | 83/308 [19:05:56<52:01:35, 832.43s/it]2025-09-09 00:03:29,494 Stage: Train 0.5 | Epoch: 83 | Iter: 134800 | Total Loss: 0.003330 | Recon Loss: 0.002792 | Commit Loss: 0.001076 | Perplexity: 1409.840394
2025-09-09 00:05:11,894 Stage: Train 0.5 | Epoch: 83 | Iter: 135000 | Total Loss: 0.003298 | Recon Loss: 0.002757 | Commit Loss: 0.001082 | Perplexity: 1410.870348
2025-09-09 00:06:54,278 Stage: Train 0.5 | Epoch: 83 | Iter: 135200 | Total Loss: 0.003414 | Recon Loss: 0.002868 | Commit Loss: 0.001091 | Perplexity: 1417.222686
2025-09-09 00:08:36,727 Stage: Train 0.5 | Epoch: 83 | Iter: 135400 | Total Loss: 0.003408 | Recon Loss: 0.002852 | Commit Loss: 0.001112 | Perplexity: 1398.108160
2025-09-09 00:10:19,128 Stage: Train 0.5 | Epoch: 83 | Iter: 135600 | Total Loss: 0.003374 | Recon Loss: 0.002824 | Commit Loss: 0.001099 | Perplexity: 1417.414102
2025-09-09 00:12:01,715 Stage: Train 0.5 | Epoch: 83 | Iter: 135800 | Total Loss: 0.003351 | Recon Loss: 0.002807 | Commit Loss: 0.001087 | Perplexity: 1414.979771
2025-09-09 00:13:44,141 Stage: Train 0.5 | Epoch: 83 | Iter: 136000 | Total Loss: 0.003256 | Recon Loss: 0.002718 | Commit Loss: 0.001077 | Perplexity: 1396.946485
2025-09-09 00:15:26,666 Stage: Train 0.5 | Epoch: 83 | Iter: 136200 | Total Loss: 0.003441 | Recon Loss: 0.002904 | Commit Loss: 0.001074 | Perplexity: 1399.295969
2025-09-09 00:17:09,163 Stage: Train 0.5 | Epoch: 83 | Iter: 136400 | Total Loss: 0.003298 | Recon Loss: 0.002757 | Commit Loss: 0.001081 | Perplexity: 1424.382637
Trainning Epoch:  27%|██▋       | 84/308 [19:19:48<51:47:05, 832.26s/it]2025-09-09 00:18:51,434 Stage: Train 0.5 | Epoch: 84 | Iter: 136600 | Total Loss: 0.003307 | Recon Loss: 0.002753 | Commit Loss: 0.001109 | Perplexity: 1420.189836
2025-09-09 00:20:34,016 Stage: Train 0.5 | Epoch: 84 | Iter: 136800 | Total Loss: 0.003230 | Recon Loss: 0.002692 | Commit Loss: 0.001077 | Perplexity: 1401.490513
2025-09-09 00:22:16,365 Stage: Train 0.5 | Epoch: 84 | Iter: 137000 | Total Loss: 0.003207 | Recon Loss: 0.002662 | Commit Loss: 0.001089 | Perplexity: 1418.479657
2025-09-09 00:23:58,682 Stage: Train 0.5 | Epoch: 84 | Iter: 137200 | Total Loss: 0.003307 | Recon Loss: 0.002763 | Commit Loss: 0.001089 | Perplexity: 1414.890541
2025-09-09 00:25:41,202 Stage: Train 0.5 | Epoch: 84 | Iter: 137400 | Total Loss: 0.003263 | Recon Loss: 0.002731 | Commit Loss: 0.001065 | Perplexity: 1414.822470
2025-09-09 00:27:23,710 Stage: Train 0.5 | Epoch: 84 | Iter: 137600 | Total Loss: 0.003361 | Recon Loss: 0.002823 | Commit Loss: 0.001077 | Perplexity: 1412.024276
2025-09-09 00:29:06,079 Stage: Train 0.5 | Epoch: 84 | Iter: 137800 | Total Loss: 0.003251 | Recon Loss: 0.002705 | Commit Loss: 0.001091 | Perplexity: 1424.606506
2025-09-09 00:30:48,439 Stage: Train 0.5 | Epoch: 84 | Iter: 138000 | Total Loss: 0.003434 | Recon Loss: 0.002862 | Commit Loss: 0.001145 | Perplexity: 1410.491171
Trainning Epoch:  28%|██▊       | 85/308 [19:33:40<51:32:27, 832.05s/it]2025-09-09 00:32:30,795 Stage: Train 0.5 | Epoch: 85 | Iter: 138200 | Total Loss: 0.003335 | Recon Loss: 0.002781 | Commit Loss: 0.001108 | Perplexity: 1409.262252
2025-09-09 00:34:13,254 Stage: Train 0.5 | Epoch: 85 | Iter: 138400 | Total Loss: 0.003261 | Recon Loss: 0.002718 | Commit Loss: 0.001086 | Perplexity: 1417.601177
2025-09-09 00:35:55,536 Stage: Train 0.5 | Epoch: 85 | Iter: 138600 | Total Loss: 0.003296 | Recon Loss: 0.002743 | Commit Loss: 0.001107 | Perplexity: 1413.881308
2025-09-09 00:37:37,991 Stage: Train 0.5 | Epoch: 85 | Iter: 138800 | Total Loss: 0.003296 | Recon Loss: 0.002754 | Commit Loss: 0.001083 | Perplexity: 1427.016987
2025-09-09 00:39:20,487 Stage: Train 0.5 | Epoch: 85 | Iter: 139000 | Total Loss: 0.003290 | Recon Loss: 0.002747 | Commit Loss: 0.001086 | Perplexity: 1410.527465
2025-09-09 00:41:02,993 Stage: Train 0.5 | Epoch: 85 | Iter: 139200 | Total Loss: 0.003298 | Recon Loss: 0.002754 | Commit Loss: 0.001088 | Perplexity: 1412.406688
2025-09-09 00:42:45,223 Stage: Train 0.5 | Epoch: 85 | Iter: 139400 | Total Loss: 0.003256 | Recon Loss: 0.002715 | Commit Loss: 0.001083 | Perplexity: 1408.846679
2025-09-09 00:44:27,646 Stage: Train 0.5 | Epoch: 85 | Iter: 139600 | Total Loss: 0.003322 | Recon Loss: 0.002782 | Commit Loss: 0.001080 | Perplexity: 1420.209833
Trainning Epoch:  28%|██▊       | 86/308 [19:47:31<51:17:55, 831.87s/it]2025-09-09 00:46:09,982 Stage: Train 0.5 | Epoch: 86 | Iter: 139800 | Total Loss: 0.003295 | Recon Loss: 0.002753 | Commit Loss: 0.001084 | Perplexity: 1406.386635
2025-09-09 00:47:52,382 Stage: Train 0.5 | Epoch: 86 | Iter: 140000 | Total Loss: 0.003257 | Recon Loss: 0.002724 | Commit Loss: 0.001065 | Perplexity: 1414.274104
2025-09-09 00:47:52,382 Saving model at iteration 140000
2025-09-09 00:47:52,493 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000
2025-09-09 00:47:52,852 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000/model.safetensors
2025-09-09 00:47:53,210 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000/optimizer.bin
2025-09-09 00:47:53,210 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000/scheduler.bin
2025-09-09 00:47:53,210 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000/sampler.bin
2025-09-09 00:47:53,259 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_87_step_140000/random_states_0.pkl
2025-09-09 00:49:35,683 Stage: Train 0.5 | Epoch: 86 | Iter: 140200 | Total Loss: 0.003413 | Recon Loss: 0.002863 | Commit Loss: 0.001100 | Perplexity: 1415.205001
2025-09-09 00:51:17,991 Stage: Train 0.5 | Epoch: 86 | Iter: 140400 | Total Loss: 0.003257 | Recon Loss: 0.002706 | Commit Loss: 0.001102 | Perplexity: 1409.970482
2025-09-09 00:53:00,352 Stage: Train 0.5 | Epoch: 86 | Iter: 140600 | Total Loss: 0.003296 | Recon Loss: 0.002755 | Commit Loss: 0.001081 | Perplexity: 1420.161566
2025-09-09 00:54:42,868 Stage: Train 0.5 | Epoch: 86 | Iter: 140800 | Total Loss: 0.003304 | Recon Loss: 0.002764 | Commit Loss: 0.001079 | Perplexity: 1410.688467
2025-09-09 00:56:25,184 Stage: Train 0.5 | Epoch: 86 | Iter: 141000 | Total Loss: 0.003271 | Recon Loss: 0.002719 | Commit Loss: 0.001105 | Perplexity: 1422.897827
2025-09-09 00:58:07,467 Stage: Train 0.5 | Epoch: 86 | Iter: 141200 | Total Loss: 0.003238 | Recon Loss: 0.002707 | Commit Loss: 0.001063 | Perplexity: 1409.195611
Trainning Epoch:  28%|██▊       | 87/308 [20:01:23<51:04:21, 831.95s/it]2025-09-09 00:59:49,752 Stage: Train 0.5 | Epoch: 87 | Iter: 141400 | Total Loss: 0.003266 | Recon Loss: 0.002723 | Commit Loss: 0.001086 | Perplexity: 1407.494648
2025-09-09 01:01:32,367 Stage: Train 0.5 | Epoch: 87 | Iter: 141600 | Total Loss: 0.003355 | Recon Loss: 0.002794 | Commit Loss: 0.001122 | Perplexity: 1416.364910
2025-09-09 01:03:14,985 Stage: Train 0.5 | Epoch: 87 | Iter: 141800 | Total Loss: 0.003398 | Recon Loss: 0.002850 | Commit Loss: 0.001095 | Perplexity: 1406.646836
2025-09-09 01:04:57,473 Stage: Train 0.5 | Epoch: 87 | Iter: 142000 | Total Loss: 0.003305 | Recon Loss: 0.002759 | Commit Loss: 0.001092 | Perplexity: 1415.785518
2025-09-09 01:06:39,952 Stage: Train 0.5 | Epoch: 87 | Iter: 142200 | Total Loss: 0.003276 | Recon Loss: 0.002744 | Commit Loss: 0.001064 | Perplexity: 1412.304430
2025-09-09 01:08:22,400 Stage: Train 0.5 | Epoch: 87 | Iter: 142400 | Total Loss: 0.003206 | Recon Loss: 0.002666 | Commit Loss: 0.001079 | Perplexity: 1419.820545
2025-09-09 01:10:04,870 Stage: Train 0.5 | Epoch: 87 | Iter: 142600 | Total Loss: 0.003249 | Recon Loss: 0.002723 | Commit Loss: 0.001053 | Perplexity: 1406.796916
2025-09-09 01:11:47,260 Stage: Train 0.5 | Epoch: 87 | Iter: 142800 | Total Loss: 0.003244 | Recon Loss: 0.002697 | Commit Loss: 0.001094 | Perplexity: 1422.121119
Trainning Epoch:  29%|██▊       | 88/308 [20:15:15<50:50:40, 832.00s/it]2025-09-09 01:13:29,431 Stage: Train 0.5 | Epoch: 88 | Iter: 143000 | Total Loss: 0.003257 | Recon Loss: 0.002721 | Commit Loss: 0.001072 | Perplexity: 1422.679405
2025-09-09 01:15:11,974 Stage: Train 0.5 | Epoch: 88 | Iter: 143200 | Total Loss: 0.003350 | Recon Loss: 0.002798 | Commit Loss: 0.001105 | Perplexity: 1415.367504
2025-09-09 01:16:54,414 Stage: Train 0.5 | Epoch: 88 | Iter: 143400 | Total Loss: 0.003252 | Recon Loss: 0.002717 | Commit Loss: 0.001070 | Perplexity: 1414.575881
2025-09-09 01:18:36,603 Stage: Train 0.5 | Epoch: 88 | Iter: 143600 | Total Loss: 0.003248 | Recon Loss: 0.002713 | Commit Loss: 0.001071 | Perplexity: 1423.585391
2025-09-09 01:20:19,218 Stage: Train 0.5 | Epoch: 88 | Iter: 143800 | Total Loss: 0.003309 | Recon Loss: 0.002766 | Commit Loss: 0.001086 | Perplexity: 1415.634002
2025-09-09 01:22:01,869 Stage: Train 0.5 | Epoch: 88 | Iter: 144000 | Total Loss: 0.003296 | Recon Loss: 0.002736 | Commit Loss: 0.001120 | Perplexity: 1419.950809
2025-09-09 01:23:44,513 Stage: Train 0.5 | Epoch: 88 | Iter: 144200 | Total Loss: 0.003282 | Recon Loss: 0.002744 | Commit Loss: 0.001076 | Perplexity: 1411.812398
2025-09-09 01:25:26,939 Stage: Train 0.5 | Epoch: 88 | Iter: 144400 | Total Loss: 0.003282 | Recon Loss: 0.002747 | Commit Loss: 0.001071 | Perplexity: 1407.772242
Trainning Epoch:  29%|██▉       | 89/308 [20:29:07<50:36:55, 832.03s/it]2025-09-09 01:27:09,537 Stage: Train 0.5 | Epoch: 89 | Iter: 144600 | Total Loss: 0.003251 | Recon Loss: 0.002721 | Commit Loss: 0.001059 | Perplexity: 1404.775722
2025-09-09 01:28:52,303 Stage: Train 0.5 | Epoch: 89 | Iter: 144800 | Total Loss: 0.003347 | Recon Loss: 0.002785 | Commit Loss: 0.001125 | Perplexity: 1415.989352
2025-09-09 01:30:34,954 Stage: Train 0.5 | Epoch: 89 | Iter: 145000 | Total Loss: 0.003282 | Recon Loss: 0.002750 | Commit Loss: 0.001063 | Perplexity: 1405.003007
2025-09-09 01:32:17,943 Stage: Train 0.5 | Epoch: 89 | Iter: 145200 | Total Loss: 0.003224 | Recon Loss: 0.002693 | Commit Loss: 0.001062 | Perplexity: 1418.808796
2025-09-09 01:34:00,634 Stage: Train 0.5 | Epoch: 89 | Iter: 145400 | Total Loss: 0.003272 | Recon Loss: 0.002738 | Commit Loss: 0.001068 | Perplexity: 1408.135300
2025-09-09 01:35:43,359 Stage: Train 0.5 | Epoch: 89 | Iter: 145600 | Total Loss: 0.003330 | Recon Loss: 0.002780 | Commit Loss: 0.001100 | Perplexity: 1422.956115
2025-09-09 01:37:26,159 Stage: Train 0.5 | Epoch: 89 | Iter: 145800 | Total Loss: 0.003265 | Recon Loss: 0.002723 | Commit Loss: 0.001085 | Perplexity: 1424.143221
2025-09-09 01:39:09,019 Stage: Train 0.5 | Epoch: 89 | Iter: 146000 | Total Loss: 0.003212 | Recon Loss: 0.002677 | Commit Loss: 0.001071 | Perplexity: 1419.597610
Trainning Epoch:  29%|██▉       | 90/308 [20:43:02<50:25:46, 832.78s/it]2025-09-09 01:40:51,649 Stage: Train 0.5 | Epoch: 90 | Iter: 146200 | Total Loss: 0.003202 | Recon Loss: 0.002656 | Commit Loss: 0.001092 | Perplexity: 1432.545854
2025-09-09 01:42:34,220 Stage: Train 0.5 | Epoch: 90 | Iter: 146400 | Total Loss: 0.003334 | Recon Loss: 0.002801 | Commit Loss: 0.001065 | Perplexity: 1419.492742
2025-09-09 01:44:16,852 Stage: Train 0.5 | Epoch: 90 | Iter: 146600 | Total Loss: 0.003255 | Recon Loss: 0.002702 | Commit Loss: 0.001106 | Perplexity: 1424.587192
2025-09-09 01:45:59,374 Stage: Train 0.5 | Epoch: 90 | Iter: 146800 | Total Loss: 0.003238 | Recon Loss: 0.002700 | Commit Loss: 0.001076 | Perplexity: 1427.013659
2025-09-09 01:47:41,775 Stage: Train 0.5 | Epoch: 90 | Iter: 147000 | Total Loss: 0.003285 | Recon Loss: 0.002750 | Commit Loss: 0.001071 | Perplexity: 1410.370514
2025-09-09 01:49:24,127 Stage: Train 0.5 | Epoch: 90 | Iter: 147200 | Total Loss: 0.003177 | Recon Loss: 0.002638 | Commit Loss: 0.001080 | Perplexity: 1427.206695
2025-09-09 01:51:06,475 Stage: Train 0.5 | Epoch: 90 | Iter: 147400 | Total Loss: 0.003211 | Recon Loss: 0.002674 | Commit Loss: 0.001074 | Perplexity: 1424.318859
2025-09-09 01:52:48,909 Stage: Train 0.5 | Epoch: 90 | Iter: 147600 | Total Loss: 0.003275 | Recon Loss: 0.002723 | Commit Loss: 0.001105 | Perplexity: 1415.289672
Trainning Epoch:  30%|██▉       | 91/308 [20:56:54<50:10:58, 832.53s/it]2025-09-09 01:54:31,215 Stage: Train 0.5 | Epoch: 91 | Iter: 147800 | Total Loss: 0.003292 | Recon Loss: 0.002760 | Commit Loss: 0.001063 | Perplexity: 1409.304750
2025-09-09 01:56:13,675 Stage: Train 0.5 | Epoch: 91 | Iter: 148000 | Total Loss: 0.003275 | Recon Loss: 0.002729 | Commit Loss: 0.001092 | Perplexity: 1413.069612
2025-09-09 01:57:56,069 Stage: Train 0.5 | Epoch: 91 | Iter: 148200 | Total Loss: 0.003277 | Recon Loss: 0.002740 | Commit Loss: 0.001074 | Perplexity: 1430.809707
2025-09-09 01:59:38,395 Stage: Train 0.5 | Epoch: 91 | Iter: 148400 | Total Loss: 0.003287 | Recon Loss: 0.002739 | Commit Loss: 0.001096 | Perplexity: 1418.418191
2025-09-09 02:01:20,817 Stage: Train 0.5 | Epoch: 91 | Iter: 148600 | Total Loss: 0.003204 | Recon Loss: 0.002686 | Commit Loss: 0.001037 | Perplexity: 1407.092410
2025-09-09 02:03:03,174 Stage: Train 0.5 | Epoch: 91 | Iter: 148800 | Total Loss: 0.003284 | Recon Loss: 0.002750 | Commit Loss: 0.001068 | Perplexity: 1413.541605
2025-09-09 02:04:45,638 Stage: Train 0.5 | Epoch: 91 | Iter: 149000 | Total Loss: 0.003206 | Recon Loss: 0.002669 | Commit Loss: 0.001072 | Perplexity: 1427.544791
2025-09-09 02:06:28,142 Stage: Train 0.5 | Epoch: 91 | Iter: 149200 | Total Loss: 0.003240 | Recon Loss: 0.002706 | Commit Loss: 0.001068 | Perplexity: 1423.384771
2025-09-09 02:08:10,608 Stage: Train 0.5 | Epoch: 91 | Iter: 149400 | Total Loss: 0.003280 | Recon Loss: 0.002734 | Commit Loss: 0.001091 | Perplexity: 1425.231036
Trainning Epoch:  30%|██▉       | 92/308 [21:10:45<49:56:00, 832.22s/it]2025-09-09 02:09:53,100 Stage: Train 0.5 | Epoch: 92 | Iter: 149600 | Total Loss: 0.003285 | Recon Loss: 0.002747 | Commit Loss: 0.001077 | Perplexity: 1426.311404
2025-09-09 02:11:35,641 Stage: Train 0.5 | Epoch: 92 | Iter: 149800 | Total Loss: 0.003214 | Recon Loss: 0.002681 | Commit Loss: 0.001066 | Perplexity: 1421.239666
2025-09-09 02:13:17,879 Stage: Train 0.5 | Epoch: 92 | Iter: 150000 | Total Loss: 0.003203 | Recon Loss: 0.002664 | Commit Loss: 0.001077 | Perplexity: 1421.842156
2025-09-09 02:15:00,428 Stage: Train 0.5 | Epoch: 92 | Iter: 150200 | Total Loss: 0.003206 | Recon Loss: 0.002674 | Commit Loss: 0.001064 | Perplexity: 1420.265615
2025-09-09 02:16:42,985 Stage: Train 0.5 | Epoch: 92 | Iter: 150400 | Total Loss: 0.003142 | Recon Loss: 0.002612 | Commit Loss: 0.001060 | Perplexity: 1411.435272
2025-09-09 02:18:25,607 Stage: Train 0.5 | Epoch: 92 | Iter: 150600 | Total Loss: 0.003225 | Recon Loss: 0.002676 | Commit Loss: 0.001098 | Perplexity: 1420.288720
2025-09-09 02:20:08,183 Stage: Train 0.5 | Epoch: 92 | Iter: 150800 | Total Loss: 0.003247 | Recon Loss: 0.002705 | Commit Loss: 0.001084 | Perplexity: 1421.880426
2025-09-09 02:21:50,621 Stage: Train 0.5 | Epoch: 92 | Iter: 151000 | Total Loss: 0.003310 | Recon Loss: 0.002770 | Commit Loss: 0.001079 | Perplexity: 1417.384179
Trainning Epoch:  30%|███       | 93/308 [21:24:38<49:42:09, 832.23s/it]2025-09-09 02:23:33,128 Stage: Train 0.5 | Epoch: 93 | Iter: 151200 | Total Loss: 0.003200 | Recon Loss: 0.002670 | Commit Loss: 0.001061 | Perplexity: 1419.623848
2025-09-09 02:25:15,469 Stage: Train 0.5 | Epoch: 93 | Iter: 151400 | Total Loss: 0.003247 | Recon Loss: 0.002713 | Commit Loss: 0.001067 | Perplexity: 1427.985682
2025-09-09 02:26:57,904 Stage: Train 0.5 | Epoch: 93 | Iter: 151600 | Total Loss: 0.003246 | Recon Loss: 0.002712 | Commit Loss: 0.001068 | Perplexity: 1419.587177
2025-09-09 02:28:40,335 Stage: Train 0.5 | Epoch: 93 | Iter: 151800 | Total Loss: 0.003209 | Recon Loss: 0.002676 | Commit Loss: 0.001067 | Perplexity: 1413.450199
2025-09-09 02:30:22,825 Stage: Train 0.5 | Epoch: 93 | Iter: 152000 | Total Loss: 0.003180 | Recon Loss: 0.002645 | Commit Loss: 0.001070 | Perplexity: 1426.840190
2025-09-09 02:32:05,489 Stage: Train 0.5 | Epoch: 93 | Iter: 152200 | Total Loss: 0.003279 | Recon Loss: 0.002713 | Commit Loss: 0.001133 | Perplexity: 1431.534752
2025-09-09 02:33:48,269 Stage: Train 0.5 | Epoch: 93 | Iter: 152400 | Total Loss: 0.003297 | Recon Loss: 0.002765 | Commit Loss: 0.001066 | Perplexity: 1425.077042
2025-09-09 02:35:30,903 Stage: Train 0.5 | Epoch: 93 | Iter: 152600 | Total Loss: 0.003201 | Recon Loss: 0.002666 | Commit Loss: 0.001069 | Perplexity: 1422.506736
Trainning Epoch:  31%|███       | 94/308 [21:38:30<49:28:47, 832.37s/it]2025-09-09 02:37:13,415 Stage: Train 0.5 | Epoch: 94 | Iter: 152800 | Total Loss: 0.003185 | Recon Loss: 0.002635 | Commit Loss: 0.001101 | Perplexity: 1419.412085
2025-09-09 02:38:55,963 Stage: Train 0.5 | Epoch: 94 | Iter: 153000 | Total Loss: 0.003235 | Recon Loss: 0.002696 | Commit Loss: 0.001079 | Perplexity: 1426.451254
2025-09-09 02:40:38,424 Stage: Train 0.5 | Epoch: 94 | Iter: 153200 | Total Loss: 0.003197 | Recon Loss: 0.002656 | Commit Loss: 0.001082 | Perplexity: 1423.441522
2025-09-09 02:42:20,794 Stage: Train 0.5 | Epoch: 94 | Iter: 153400 | Total Loss: 0.003245 | Recon Loss: 0.002710 | Commit Loss: 0.001069 | Perplexity: 1426.922444
2025-09-09 02:44:03,336 Stage: Train 0.5 | Epoch: 94 | Iter: 153600 | Total Loss: 0.003190 | Recon Loss: 0.002654 | Commit Loss: 0.001073 | Perplexity: 1416.098908
2025-09-09 02:45:45,956 Stage: Train 0.5 | Epoch: 94 | Iter: 153800 | Total Loss: 0.003182 | Recon Loss: 0.002660 | Commit Loss: 0.001043 | Perplexity: 1418.253043
2025-09-09 02:47:28,577 Stage: Train 0.5 | Epoch: 94 | Iter: 154000 | Total Loss: 0.003217 | Recon Loss: 0.002674 | Commit Loss: 0.001086 | Perplexity: 1436.834426
2025-09-09 02:49:11,069 Stage: Train 0.5 | Epoch: 94 | Iter: 154200 | Total Loss: 0.003257 | Recon Loss: 0.002708 | Commit Loss: 0.001098 | Perplexity: 1421.409896
Trainning Epoch:  31%|███       | 95/308 [21:52:23<49:14:47, 832.33s/it]2025-09-09 02:50:53,392 Stage: Train 0.5 | Epoch: 95 | Iter: 154400 | Total Loss: 0.003173 | Recon Loss: 0.002648 | Commit Loss: 0.001050 | Perplexity: 1424.932519
2025-09-09 02:52:35,733 Stage: Train 0.5 | Epoch: 95 | Iter: 154600 | Total Loss: 0.003238 | Recon Loss: 0.002700 | Commit Loss: 0.001075 | Perplexity: 1418.087784
2025-09-09 02:54:18,100 Stage: Train 0.5 | Epoch: 95 | Iter: 154800 | Total Loss: 0.003200 | Recon Loss: 0.002676 | Commit Loss: 0.001048 | Perplexity: 1416.885663
2025-09-09 02:56:00,304 Stage: Train 0.5 | Epoch: 95 | Iter: 155000 | Total Loss: 0.003266 | Recon Loss: 0.002721 | Commit Loss: 0.001089 | Perplexity: 1443.043292
2025-09-09 02:57:42,549 Stage: Train 0.5 | Epoch: 95 | Iter: 155200 | Total Loss: 0.003221 | Recon Loss: 0.002685 | Commit Loss: 0.001072 | Perplexity: 1421.800950
2025-09-09 02:59:24,966 Stage: Train 0.5 | Epoch: 95 | Iter: 155400 | Total Loss: 0.003181 | Recon Loss: 0.002650 | Commit Loss: 0.001061 | Perplexity: 1424.382980
2025-09-09 03:01:07,384 Stage: Train 0.5 | Epoch: 95 | Iter: 155600 | Total Loss: 0.003274 | Recon Loss: 0.002727 | Commit Loss: 0.001093 | Perplexity: 1431.382456
2025-09-09 03:02:49,545 Stage: Train 0.5 | Epoch: 95 | Iter: 155800 | Total Loss: 0.003159 | Recon Loss: 0.002632 | Commit Loss: 0.001054 | Perplexity: 1425.076148
Trainning Epoch:  31%|███       | 96/308 [22:06:13<48:59:19, 831.88s/it]2025-09-09 03:04:32,043 Stage: Train 0.5 | Epoch: 96 | Iter: 156000 | Total Loss: 0.003249 | Recon Loss: 0.002702 | Commit Loss: 0.001094 | Perplexity: 1422.544285
2025-09-09 03:06:14,862 Stage: Train 0.5 | Epoch: 96 | Iter: 156200 | Total Loss: 0.003238 | Recon Loss: 0.002695 | Commit Loss: 0.001086 | Perplexity: 1438.099312
2025-09-09 03:07:57,890 Stage: Train 0.5 | Epoch: 96 | Iter: 156400 | Total Loss: 0.003172 | Recon Loss: 0.002643 | Commit Loss: 0.001059 | Perplexity: 1427.077656
2025-09-09 03:09:40,353 Stage: Train 0.5 | Epoch: 96 | Iter: 156600 | Total Loss: 0.003210 | Recon Loss: 0.002674 | Commit Loss: 0.001071 | Perplexity: 1428.868533
2025-09-09 03:11:23,140 Stage: Train 0.5 | Epoch: 96 | Iter: 156800 | Total Loss: 0.003232 | Recon Loss: 0.002705 | Commit Loss: 0.001053 | Perplexity: 1414.747437
2025-09-09 03:13:05,956 Stage: Train 0.5 | Epoch: 96 | Iter: 157000 | Total Loss: 0.003124 | Recon Loss: 0.002592 | Commit Loss: 0.001064 | Perplexity: 1424.254360
2025-09-09 03:14:49,014 Stage: Train 0.5 | Epoch: 96 | Iter: 157200 | Total Loss: 0.003265 | Recon Loss: 0.002711 | Commit Loss: 0.001108 | Perplexity: 1424.169155
2025-09-09 03:16:31,691 Stage: Train 0.5 | Epoch: 96 | Iter: 157400 | Total Loss: 0.003191 | Recon Loss: 0.002671 | Commit Loss: 0.001039 | Perplexity: 1423.463832
Trainning Epoch:  31%|███▏      | 97/308 [22:20:08<48:48:29, 832.74s/it]2025-09-09 03:18:14,298 Stage: Train 0.5 | Epoch: 97 | Iter: 157600 | Total Loss: 0.003161 | Recon Loss: 0.002635 | Commit Loss: 0.001052 | Perplexity: 1429.645056
2025-09-09 03:19:57,221 Stage: Train 0.5 | Epoch: 97 | Iter: 157800 | Total Loss: 0.003186 | Recon Loss: 0.002659 | Commit Loss: 0.001055 | Perplexity: 1422.446600
2025-09-09 03:21:40,310 Stage: Train 0.5 | Epoch: 97 | Iter: 158000 | Total Loss: 0.003139 | Recon Loss: 0.002602 | Commit Loss: 0.001073 | Perplexity: 1432.828054
2025-09-09 03:23:23,309 Stage: Train 0.5 | Epoch: 97 | Iter: 158200 | Total Loss: 0.003131 | Recon Loss: 0.002603 | Commit Loss: 0.001054 | Perplexity: 1432.887388
2025-09-09 03:25:06,183 Stage: Train 0.5 | Epoch: 97 | Iter: 158400 | Total Loss: 0.003247 | Recon Loss: 0.002723 | Commit Loss: 0.001048 | Perplexity: 1421.388900
2025-09-09 03:26:49,084 Stage: Train 0.5 | Epoch: 97 | Iter: 158600 | Total Loss: 0.003252 | Recon Loss: 0.002690 | Commit Loss: 0.001123 | Perplexity: 1429.409496
2025-09-09 03:28:31,741 Stage: Train 0.5 | Epoch: 97 | Iter: 158800 | Total Loss: 0.003207 | Recon Loss: 0.002672 | Commit Loss: 0.001068 | Perplexity: 1414.666207
2025-09-09 03:30:14,146 Stage: Train 0.5 | Epoch: 97 | Iter: 159000 | Total Loss: 0.003209 | Recon Loss: 0.002670 | Commit Loss: 0.001077 | Perplexity: 1431.287146
Trainning Epoch:  32%|███▏      | 98/308 [22:34:03<48:36:30, 833.29s/it]2025-09-09 03:31:56,475 Stage: Train 0.5 | Epoch: 98 | Iter: 159200 | Total Loss: 0.003124 | Recon Loss: 0.002590 | Commit Loss: 0.001067 | Perplexity: 1422.477187
2025-09-09 03:33:39,145 Stage: Train 0.5 | Epoch: 98 | Iter: 159400 | Total Loss: 0.003221 | Recon Loss: 0.002687 | Commit Loss: 0.001067 | Perplexity: 1423.700330
2025-09-09 03:35:21,808 Stage: Train 0.5 | Epoch: 98 | Iter: 159600 | Total Loss: 0.003213 | Recon Loss: 0.002679 | Commit Loss: 0.001068 | Perplexity: 1432.472112
2025-09-09 03:37:04,156 Stage: Train 0.5 | Epoch: 98 | Iter: 159800 | Total Loss: 0.003178 | Recon Loss: 0.002632 | Commit Loss: 0.001092 | Perplexity: 1431.018181
2025-09-09 03:38:46,581 Stage: Train 0.5 | Epoch: 98 | Iter: 160000 | Total Loss: 0.003128 | Recon Loss: 0.002599 | Commit Loss: 0.001058 | Perplexity: 1433.848582
2025-09-09 03:38:46,581 Saving model at iteration 160000
2025-09-09 03:38:46,692 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000
2025-09-09 03:38:47,042 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000/model.safetensors
2025-09-09 03:38:47,399 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000/optimizer.bin
2025-09-09 03:38:47,399 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000/scheduler.bin
2025-09-09 03:38:47,399 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000/sampler.bin
2025-09-09 03:38:47,400 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_99_step_160000/random_states_0.pkl
2025-09-09 03:40:29,868 Stage: Train 0.5 | Epoch: 98 | Iter: 160200 | Total Loss: 0.003136 | Recon Loss: 0.002609 | Commit Loss: 0.001054 | Perplexity: 1423.858023
2025-09-09 03:42:12,401 Stage: Train 0.5 | Epoch: 98 | Iter: 160400 | Total Loss: 0.003211 | Recon Loss: 0.002680 | Commit Loss: 0.001060 | Perplexity: 1430.837309
2025-09-09 03:43:54,880 Stage: Train 0.5 | Epoch: 98 | Iter: 160600 | Total Loss: 0.003269 | Recon Loss: 0.002743 | Commit Loss: 0.001054 | Perplexity: 1420.910593
Trainning Epoch:  32%|███▏      | 99/308 [22:47:56<48:22:50, 833.35s/it]2025-09-09 03:45:37,829 Stage: Train 0.5 | Epoch: 99 | Iter: 160800 | Total Loss: 0.003133 | Recon Loss: 0.002592 | Commit Loss: 0.001083 | Perplexity: 1439.596735
2025-09-09 03:47:20,388 Stage: Train 0.5 | Epoch: 99 | Iter: 161000 | Total Loss: 0.003207 | Recon Loss: 0.002680 | Commit Loss: 0.001054 | Perplexity: 1425.944203
2025-09-09 03:49:03,365 Stage: Train 0.5 | Epoch: 99 | Iter: 161200 | Total Loss: 0.003127 | Recon Loss: 0.002601 | Commit Loss: 0.001053 | Perplexity: 1428.776542
2025-09-09 03:50:45,864 Stage: Train 0.5 | Epoch: 99 | Iter: 161400 | Total Loss: 0.003337 | Recon Loss: 0.002786 | Commit Loss: 0.001101 | Perplexity: 1424.431038
2025-09-09 03:52:28,362 Stage: Train 0.5 | Epoch: 99 | Iter: 161600 | Total Loss: 0.003157 | Recon Loss: 0.002623 | Commit Loss: 0.001068 | Perplexity: 1431.840908
2025-09-09 03:54:11,099 Stage: Train 0.5 | Epoch: 99 | Iter: 161800 | Total Loss: 0.003179 | Recon Loss: 0.002652 | Commit Loss: 0.001055 | Perplexity: 1428.001140
2025-09-09 03:55:53,768 Stage: Train 0.5 | Epoch: 99 | Iter: 162000 | Total Loss: 0.003096 | Recon Loss: 0.002564 | Commit Loss: 0.001063 | Perplexity: 1434.963851
2025-09-09 03:57:36,434 Stage: Train 0.5 | Epoch: 99 | Iter: 162200 | Total Loss: 0.003219 | Recon Loss: 0.002673 | Commit Loss: 0.001093 | Perplexity: 1427.254223
2025-09-09 03:59:18,678 Stage: Train 0.5 | Epoch: 99 | Iter: 162400 | Total Loss: 0.003157 | Recon Loss: 0.002635 | Commit Loss: 0.001044 | Perplexity: 1422.197460
Trainning Epoch:  32%|███▏      | 100/308 [23:01:49<48:08:48, 833.31s/it]2025-09-09 04:01:01,407 Stage: Train 0.5 | Epoch: 100 | Iter: 162600 | Total Loss: 0.003176 | Recon Loss: 0.002643 | Commit Loss: 0.001068 | Perplexity: 1423.023185
2025-09-09 04:02:44,163 Stage: Train 0.5 | Epoch: 100 | Iter: 162800 | Total Loss: 0.003296 | Recon Loss: 0.002753 | Commit Loss: 0.001086 | Perplexity: 1430.677419
2025-09-09 04:04:26,903 Stage: Train 0.5 | Epoch: 100 | Iter: 163000 | Total Loss: 0.003082 | Recon Loss: 0.002556 | Commit Loss: 0.001052 | Perplexity: 1430.630405
2025-09-09 04:06:09,501 Stage: Train 0.5 | Epoch: 100 | Iter: 163200 | Total Loss: 0.003184 | Recon Loss: 0.002645 | Commit Loss: 0.001078 | Perplexity: 1448.949297
2025-09-09 04:07:52,049 Stage: Train 0.5 | Epoch: 100 | Iter: 163400 | Total Loss: 0.003157 | Recon Loss: 0.002627 | Commit Loss: 0.001059 | Perplexity: 1437.457451
2025-09-09 04:09:34,599 Stage: Train 0.5 | Epoch: 100 | Iter: 163600 | Total Loss: 0.003199 | Recon Loss: 0.002669 | Commit Loss: 0.001061 | Perplexity: 1435.092669
2025-09-09 04:11:17,266 Stage: Train 0.5 | Epoch: 100 | Iter: 163800 | Total Loss: 0.003110 | Recon Loss: 0.002590 | Commit Loss: 0.001040 | Perplexity: 1420.657514
2025-09-09 04:13:00,075 Stage: Train 0.5 | Epoch: 100 | Iter: 164000 | Total Loss: 0.003176 | Recon Loss: 0.002639 | Commit Loss: 0.001074 | Perplexity: 1418.910627
Trainning Epoch:  33%|███▎      | 101/308 [23:15:43<47:55:11, 833.39s/it]2025-09-09 04:14:42,463 Stage: Train 0.5 | Epoch: 101 | Iter: 164200 | Total Loss: 0.003135 | Recon Loss: 0.002608 | Commit Loss: 0.001053 | Perplexity: 1441.815491
2025-09-09 04:16:24,899 Stage: Train 0.5 | Epoch: 101 | Iter: 164400 | Total Loss: 0.003194 | Recon Loss: 0.002666 | Commit Loss: 0.001056 | Perplexity: 1435.229741
2025-09-09 04:18:07,493 Stage: Train 0.5 | Epoch: 101 | Iter: 164600 | Total Loss: 0.003148 | Recon Loss: 0.002626 | Commit Loss: 0.001044 | Perplexity: 1417.021221
2025-09-09 04:19:50,443 Stage: Train 0.5 | Epoch: 101 | Iter: 164800 | Total Loss: 0.003235 | Recon Loss: 0.002689 | Commit Loss: 0.001093 | Perplexity: 1423.038271
2025-09-09 04:21:32,675 Stage: Train 0.5 | Epoch: 101 | Iter: 165000 | Total Loss: 0.003141 | Recon Loss: 0.002621 | Commit Loss: 0.001040 | Perplexity: 1432.204556
2025-09-09 04:23:15,192 Stage: Train 0.5 | Epoch: 101 | Iter: 165200 | Total Loss: 0.003197 | Recon Loss: 0.002666 | Commit Loss: 0.001061 | Perplexity: 1427.221968
2025-09-09 04:24:57,932 Stage: Train 0.5 | Epoch: 101 | Iter: 165400 | Total Loss: 0.003167 | Recon Loss: 0.002638 | Commit Loss: 0.001058 | Perplexity: 1430.919321
2025-09-09 04:26:40,750 Stage: Train 0.5 | Epoch: 101 | Iter: 165600 | Total Loss: 0.003150 | Recon Loss: 0.002615 | Commit Loss: 0.001072 | Perplexity: 1425.365547
Trainning Epoch:  33%|███▎      | 102/308 [23:29:36<47:40:51, 833.26s/it]2025-09-09 04:28:23,063 Stage: Train 0.5 | Epoch: 102 | Iter: 165800 | Total Loss: 0.003160 | Recon Loss: 0.002629 | Commit Loss: 0.001061 | Perplexity: 1430.666074
2025-09-09 04:30:05,589 Stage: Train 0.5 | Epoch: 102 | Iter: 166000 | Total Loss: 0.003167 | Recon Loss: 0.002639 | Commit Loss: 0.001057 | Perplexity: 1422.075610
2025-09-09 04:31:47,944 Stage: Train 0.5 | Epoch: 102 | Iter: 166200 | Total Loss: 0.003172 | Recon Loss: 0.002641 | Commit Loss: 0.001063 | Perplexity: 1424.791732
2025-09-09 04:33:30,454 Stage: Train 0.5 | Epoch: 102 | Iter: 166400 | Total Loss: 0.003101 | Recon Loss: 0.002578 | Commit Loss: 0.001047 | Perplexity: 1420.066157
2025-09-09 04:35:12,678 Stage: Train 0.5 | Epoch: 102 | Iter: 166600 | Total Loss: 0.003225 | Recon Loss: 0.002690 | Commit Loss: 0.001069 | Perplexity: 1429.662315
2025-09-09 04:36:55,081 Stage: Train 0.5 | Epoch: 102 | Iter: 166800 | Total Loss: 0.003124 | Recon Loss: 0.002600 | Commit Loss: 0.001048 | Perplexity: 1435.564303
2025-09-09 04:38:37,478 Stage: Train 0.5 | Epoch: 102 | Iter: 167000 | Total Loss: 0.003205 | Recon Loss: 0.002675 | Commit Loss: 0.001059 | Perplexity: 1428.343637
2025-09-09 04:40:19,889 Stage: Train 0.5 | Epoch: 102 | Iter: 167200 | Total Loss: 0.003136 | Recon Loss: 0.002600 | Commit Loss: 0.001071 | Perplexity: 1439.570831
Trainning Epoch:  33%|███▎      | 103/308 [23:43:27<47:25:04, 832.71s/it]2025-09-09 04:42:02,216 Stage: Train 0.5 | Epoch: 103 | Iter: 167400 | Total Loss: 0.003120 | Recon Loss: 0.002587 | Commit Loss: 0.001065 | Perplexity: 1428.485840
2025-09-09 04:43:44,712 Stage: Train 0.5 | Epoch: 103 | Iter: 167600 | Total Loss: 0.003156 | Recon Loss: 0.002626 | Commit Loss: 0.001060 | Perplexity: 1432.226693
2025-09-09 04:45:27,225 Stage: Train 0.5 | Epoch: 103 | Iter: 167800 | Total Loss: 0.003089 | Recon Loss: 0.002564 | Commit Loss: 0.001051 | Perplexity: 1424.969595
2025-09-09 04:47:10,022 Stage: Train 0.5 | Epoch: 103 | Iter: 168000 | Total Loss: 0.003156 | Recon Loss: 0.002626 | Commit Loss: 0.001061 | Perplexity: 1424.487289
2025-09-09 04:48:52,789 Stage: Train 0.5 | Epoch: 103 | Iter: 168200 | Total Loss: 0.003077 | Recon Loss: 0.002564 | Commit Loss: 0.001027 | Perplexity: 1427.428701
2025-09-09 04:50:35,555 Stage: Train 0.5 | Epoch: 103 | Iter: 168400 | Total Loss: 0.003192 | Recon Loss: 0.002655 | Commit Loss: 0.001074 | Perplexity: 1432.991453
2025-09-09 04:52:18,337 Stage: Train 0.5 | Epoch: 103 | Iter: 168600 | Total Loss: 0.003221 | Recon Loss: 0.002675 | Commit Loss: 0.001092 | Perplexity: 1441.417487
2025-09-09 04:54:01,192 Stage: Train 0.5 | Epoch: 103 | Iter: 168800 | Total Loss: 0.003134 | Recon Loss: 0.002610 | Commit Loss: 0.001049 | Perplexity: 1427.253763
Trainning Epoch:  34%|███▍      | 104/308 [23:57:21<47:12:20, 833.04s/it]2025-09-09 04:55:43,985 Stage: Train 0.5 | Epoch: 104 | Iter: 169000 | Total Loss: 0.003112 | Recon Loss: 0.002582 | Commit Loss: 0.001060 | Perplexity: 1426.752230
2025-09-09 04:57:26,788 Stage: Train 0.5 | Epoch: 104 | Iter: 169200 | Total Loss: 0.003206 | Recon Loss: 0.002667 | Commit Loss: 0.001078 | Perplexity: 1421.802177
2025-09-09 04:59:09,728 Stage: Train 0.5 | Epoch: 104 | Iter: 169400 | Total Loss: 0.003070 | Recon Loss: 0.002552 | Commit Loss: 0.001036 | Perplexity: 1433.483871
2025-09-09 05:00:52,330 Stage: Train 0.5 | Epoch: 104 | Iter: 169600 | Total Loss: 0.003174 | Recon Loss: 0.002640 | Commit Loss: 0.001069 | Perplexity: 1442.948149
2025-09-09 05:02:35,147 Stage: Train 0.5 | Epoch: 104 | Iter: 169800 | Total Loss: 0.003129 | Recon Loss: 0.002602 | Commit Loss: 0.001054 | Perplexity: 1432.958384
2025-09-09 05:04:17,639 Stage: Train 0.5 | Epoch: 104 | Iter: 170000 | Total Loss: 0.003149 | Recon Loss: 0.002620 | Commit Loss: 0.001058 | Perplexity: 1417.030398
2025-09-09 05:06:00,206 Stage: Train 0.5 | Epoch: 104 | Iter: 170200 | Total Loss: 0.003135 | Recon Loss: 0.002595 | Commit Loss: 0.001080 | Perplexity: 1437.546658
2025-09-09 05:07:42,640 Stage: Train 0.5 | Epoch: 104 | Iter: 170400 | Total Loss: 0.003184 | Recon Loss: 0.002647 | Commit Loss: 0.001074 | Perplexity: 1438.014341
Trainning Epoch:  34%|███▍      | 105/308 [24:11:15<46:58:48, 833.15s/it]2025-09-09 05:09:24,887 Stage: Train 0.5 | Epoch: 105 | Iter: 170600 | Total Loss: 0.003061 | Recon Loss: 0.002542 | Commit Loss: 0.001038 | Perplexity: 1429.745818
2025-09-09 05:11:07,212 Stage: Train 0.5 | Epoch: 105 | Iter: 170800 | Total Loss: 0.003110 | Recon Loss: 0.002594 | Commit Loss: 0.001031 | Perplexity: 1421.373774
2025-09-09 05:12:49,682 Stage: Train 0.5 | Epoch: 105 | Iter: 171000 | Total Loss: 0.003170 | Recon Loss: 0.002646 | Commit Loss: 0.001048 | Perplexity: 1420.887908
2025-09-09 05:14:32,026 Stage: Train 0.5 | Epoch: 105 | Iter: 171200 | Total Loss: 0.003111 | Recon Loss: 0.002576 | Commit Loss: 0.001070 | Perplexity: 1445.876069
2025-09-09 05:16:14,541 Stage: Train 0.5 | Epoch: 105 | Iter: 171400 | Total Loss: 0.003221 | Recon Loss: 0.002683 | Commit Loss: 0.001074 | Perplexity: 1435.274165
2025-09-09 05:17:56,929 Stage: Train 0.5 | Epoch: 105 | Iter: 171600 | Total Loss: 0.003142 | Recon Loss: 0.002601 | Commit Loss: 0.001083 | Perplexity: 1433.483960
2025-09-09 05:19:39,148 Stage: Train 0.5 | Epoch: 105 | Iter: 171800 | Total Loss: 0.003168 | Recon Loss: 0.002630 | Commit Loss: 0.001075 | Perplexity: 1439.238556
2025-09-09 05:21:21,497 Stage: Train 0.5 | Epoch: 105 | Iter: 172000 | Total Loss: 0.003116 | Recon Loss: 0.002594 | Commit Loss: 0.001043 | Perplexity: 1426.818677
Trainning Epoch:  34%|███▍      | 106/308 [24:25:06<46:43:15, 832.65s/it]2025-09-09 05:23:04,004 Stage: Train 0.5 | Epoch: 106 | Iter: 172200 | Total Loss: 0.003101 | Recon Loss: 0.002578 | Commit Loss: 0.001048 | Perplexity: 1439.513886
2025-09-09 05:24:46,429 Stage: Train 0.5 | Epoch: 106 | Iter: 172400 | Total Loss: 0.003177 | Recon Loss: 0.002644 | Commit Loss: 0.001065 | Perplexity: 1439.469324
2025-09-09 05:26:28,971 Stage: Train 0.5 | Epoch: 106 | Iter: 172600 | Total Loss: 0.003069 | Recon Loss: 0.002544 | Commit Loss: 0.001052 | Perplexity: 1442.440265
2025-09-09 05:28:11,374 Stage: Train 0.5 | Epoch: 106 | Iter: 172800 | Total Loss: 0.003044 | Recon Loss: 0.002520 | Commit Loss: 0.001048 | Perplexity: 1436.231346
2025-09-09 05:29:54,099 Stage: Train 0.5 | Epoch: 106 | Iter: 173000 | Total Loss: 0.003246 | Recon Loss: 0.002697 | Commit Loss: 0.001098 | Perplexity: 1418.835400
2025-09-09 05:31:36,500 Stage: Train 0.5 | Epoch: 106 | Iter: 173200 | Total Loss: 0.003180 | Recon Loss: 0.002652 | Commit Loss: 0.001058 | Perplexity: 1439.451515
2025-09-09 05:33:19,301 Stage: Train 0.5 | Epoch: 106 | Iter: 173400 | Total Loss: 0.003100 | Recon Loss: 0.002570 | Commit Loss: 0.001061 | Perplexity: 1423.690618
2025-09-09 05:35:02,020 Stage: Train 0.5 | Epoch: 106 | Iter: 173600 | Total Loss: 0.003052 | Recon Loss: 0.002531 | Commit Loss: 0.001043 | Perplexity: 1423.313348
Trainning Epoch:  35%|███▍      | 107/308 [24:38:59<46:29:55, 832.81s/it]2025-09-09 05:36:45,037 Stage: Train 0.5 | Epoch: 107 | Iter: 173800 | Total Loss: 0.003095 | Recon Loss: 0.002569 | Commit Loss: 0.001051 | Perplexity: 1426.181953
2025-09-09 05:38:27,864 Stage: Train 0.5 | Epoch: 107 | Iter: 174000 | Total Loss: 0.003128 | Recon Loss: 0.002605 | Commit Loss: 0.001045 | Perplexity: 1429.516173
2025-09-09 05:40:10,484 Stage: Train 0.5 | Epoch: 107 | Iter: 174200 | Total Loss: 0.003088 | Recon Loss: 0.002563 | Commit Loss: 0.001050 | Perplexity: 1435.041219
2025-09-09 05:41:53,276 Stage: Train 0.5 | Epoch: 107 | Iter: 174400 | Total Loss: 0.003069 | Recon Loss: 0.002552 | Commit Loss: 0.001033 | Perplexity: 1431.703035
2025-09-09 05:43:35,956 Stage: Train 0.5 | Epoch: 107 | Iter: 174600 | Total Loss: 0.003107 | Recon Loss: 0.002580 | Commit Loss: 0.001055 | Perplexity: 1429.923815
2025-09-09 05:45:18,773 Stage: Train 0.5 | Epoch: 107 | Iter: 174800 | Total Loss: 0.003107 | Recon Loss: 0.002576 | Commit Loss: 0.001063 | Perplexity: 1440.891196
2025-09-09 05:47:01,548 Stage: Train 0.5 | Epoch: 107 | Iter: 175000 | Total Loss: 0.003131 | Recon Loss: 0.002597 | Commit Loss: 0.001069 | Perplexity: 1434.580378
2025-09-09 05:48:44,390 Stage: Train 0.5 | Epoch: 107 | Iter: 175200 | Total Loss: 0.003180 | Recon Loss: 0.002645 | Commit Loss: 0.001070 | Perplexity: 1429.690853
Trainning Epoch:  35%|███▌      | 108/308 [24:52:53<46:17:22, 833.21s/it]2025-09-09 05:50:26,798 Stage: Train 0.5 | Epoch: 108 | Iter: 175400 | Total Loss: 0.003160 | Recon Loss: 0.002618 | Commit Loss: 0.001084 | Perplexity: 1434.314636
2025-09-09 05:52:09,176 Stage: Train 0.5 | Epoch: 108 | Iter: 175600 | Total Loss: 0.003082 | Recon Loss: 0.002565 | Commit Loss: 0.001033 | Perplexity: 1430.228566
2025-09-09 05:53:51,699 Stage: Train 0.5 | Epoch: 108 | Iter: 175800 | Total Loss: 0.003141 | Recon Loss: 0.002616 | Commit Loss: 0.001048 | Perplexity: 1435.421543
2025-09-09 05:55:34,336 Stage: Train 0.5 | Epoch: 108 | Iter: 176000 | Total Loss: 0.003092 | Recon Loss: 0.002568 | Commit Loss: 0.001048 | Perplexity: 1434.999979
2025-09-09 05:57:16,852 Stage: Train 0.5 | Epoch: 108 | Iter: 176200 | Total Loss: 0.003102 | Recon Loss: 0.002571 | Commit Loss: 0.001062 | Perplexity: 1419.762289
2025-09-09 05:58:59,436 Stage: Train 0.5 | Epoch: 108 | Iter: 176400 | Total Loss: 0.003110 | Recon Loss: 0.002572 | Commit Loss: 0.001075 | Perplexity: 1429.957307
2025-09-09 06:00:41,833 Stage: Train 0.5 | Epoch: 108 | Iter: 176600 | Total Loss: 0.003108 | Recon Loss: 0.002589 | Commit Loss: 0.001039 | Perplexity: 1428.101445
2025-09-09 06:02:24,286 Stage: Train 0.5 | Epoch: 108 | Iter: 176800 | Total Loss: 0.003185 | Recon Loss: 0.002652 | Commit Loss: 0.001066 | Perplexity: 1438.770456
2025-09-09 06:04:06,640 Stage: Train 0.5 | Epoch: 108 | Iter: 177000 | Total Loss: 0.003074 | Recon Loss: 0.002546 | Commit Loss: 0.001057 | Perplexity: 1430.637272
Trainning Epoch:  35%|███▌      | 109/308 [25:06:46<46:02:18, 832.86s/it]2025-09-09 06:05:48,893 Stage: Train 0.5 | Epoch: 109 | Iter: 177200 | Total Loss: 0.003206 | Recon Loss: 0.002654 | Commit Loss: 0.001104 | Perplexity: 1434.866083
2025-09-09 06:07:31,381 Stage: Train 0.5 | Epoch: 109 | Iter: 177400 | Total Loss: 0.003061 | Recon Loss: 0.002543 | Commit Loss: 0.001036 | Perplexity: 1426.943241
2025-09-09 06:09:14,061 Stage: Train 0.5 | Epoch: 109 | Iter: 177600 | Total Loss: 0.003223 | Recon Loss: 0.002698 | Commit Loss: 0.001050 | Perplexity: 1409.785817
2025-09-09 06:10:56,444 Stage: Train 0.5 | Epoch: 109 | Iter: 177800 | Total Loss: 0.003106 | Recon Loss: 0.002573 | Commit Loss: 0.001066 | Perplexity: 1445.596797
2025-09-09 06:12:39,126 Stage: Train 0.5 | Epoch: 109 | Iter: 178000 | Total Loss: 0.003050 | Recon Loss: 0.002524 | Commit Loss: 0.001052 | Perplexity: 1429.459312
2025-09-09 06:14:21,679 Stage: Train 0.5 | Epoch: 109 | Iter: 178200 | Total Loss: 0.003119 | Recon Loss: 0.002597 | Commit Loss: 0.001043 | Perplexity: 1435.979318
2025-09-09 06:16:03,927 Stage: Train 0.5 | Epoch: 109 | Iter: 178400 | Total Loss: 0.003117 | Recon Loss: 0.002586 | Commit Loss: 0.001062 | Perplexity: 1427.714536
2025-09-09 06:17:46,271 Stage: Train 0.5 | Epoch: 109 | Iter: 178600 | Total Loss: 0.003104 | Recon Loss: 0.002578 | Commit Loss: 0.001052 | Perplexity: 1426.473838
Trainning Epoch:  36%|███▌      | 110/308 [25:20:37<45:47:30, 832.58s/it]2025-09-09 06:19:28,345 Stage: Train 0.5 | Epoch: 110 | Iter: 178800 | Total Loss: 0.003051 | Recon Loss: 0.002527 | Commit Loss: 0.001048 | Perplexity: 1431.953688
2025-09-09 06:21:10,918 Stage: Train 0.5 | Epoch: 110 | Iter: 179000 | Total Loss: 0.003056 | Recon Loss: 0.002532 | Commit Loss: 0.001047 | Perplexity: 1438.384850
2025-09-09 06:22:53,377 Stage: Train 0.5 | Epoch: 110 | Iter: 179200 | Total Loss: 0.003097 | Recon Loss: 0.002581 | Commit Loss: 0.001033 | Perplexity: 1439.877749
2025-09-09 06:24:35,413 Stage: Train 0.5 | Epoch: 110 | Iter: 179400 | Total Loss: 0.003064 | Recon Loss: 0.002539 | Commit Loss: 0.001049 | Perplexity: 1423.650297
2025-09-09 06:26:17,713 Stage: Train 0.5 | Epoch: 110 | Iter: 179600 | Total Loss: 0.003091 | Recon Loss: 0.002559 | Commit Loss: 0.001064 | Perplexity: 1444.879040
2025-09-09 06:27:59,970 Stage: Train 0.5 | Epoch: 110 | Iter: 179800 | Total Loss: 0.003130 | Recon Loss: 0.002603 | Commit Loss: 0.001054 | Perplexity: 1428.799829
2025-09-09 06:29:42,586 Stage: Train 0.5 | Epoch: 110 | Iter: 180000 | Total Loss: 0.003195 | Recon Loss: 0.002655 | Commit Loss: 0.001080 | Perplexity: 1424.292308
2025-09-09 06:29:42,587 Saving model at iteration 180000
2025-09-09 06:29:42,716 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000
2025-09-09 06:29:43,124 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000/model.safetensors
2025-09-09 06:29:43,519 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000/optimizer.bin
2025-09-09 06:29:43,520 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000/scheduler.bin
2025-09-09 06:29:43,520 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000/sampler.bin
2025-09-09 06:29:43,520 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_111_step_180000/random_states_0.pkl
2025-09-09 06:31:26,739 Stage: Train 0.5 | Epoch: 110 | Iter: 180200 | Total Loss: 0.003113 | Recon Loss: 0.002581 | Commit Loss: 0.001063 | Perplexity: 1427.553116
Trainning Epoch:  36%|███▌      | 111/308 [25:34:30<45:33:54, 832.66s/it]2025-09-09 06:33:09,187 Stage: Train 0.5 | Epoch: 111 | Iter: 180400 | Total Loss: 0.003080 | Recon Loss: 0.002556 | Commit Loss: 0.001048 | Perplexity: 1431.243883
2025-09-09 06:34:51,680 Stage: Train 0.5 | Epoch: 111 | Iter: 180600 | Total Loss: 0.003057 | Recon Loss: 0.002534 | Commit Loss: 0.001045 | Perplexity: 1440.735473
2025-09-09 06:36:34,292 Stage: Train 0.5 | Epoch: 111 | Iter: 180800 | Total Loss: 0.003106 | Recon Loss: 0.002583 | Commit Loss: 0.001044 | Perplexity: 1437.954304
2025-09-09 06:38:16,720 Stage: Train 0.5 | Epoch: 111 | Iter: 181000 | Total Loss: 0.003060 | Recon Loss: 0.002533 | Commit Loss: 0.001054 | Perplexity: 1423.584818
2025-09-09 06:39:59,068 Stage: Train 0.5 | Epoch: 111 | Iter: 181200 | Total Loss: 0.003143 | Recon Loss: 0.002613 | Commit Loss: 0.001058 | Perplexity: 1427.780273
2025-09-09 06:41:41,644 Stage: Train 0.5 | Epoch: 111 | Iter: 181400 | Total Loss: 0.003061 | Recon Loss: 0.002533 | Commit Loss: 0.001056 | Perplexity: 1439.726385
2025-09-09 06:43:23,963 Stage: Train 0.5 | Epoch: 111 | Iter: 181600 | Total Loss: 0.003104 | Recon Loss: 0.002581 | Commit Loss: 0.001045 | Perplexity: 1436.246970
2025-09-09 06:45:06,275 Stage: Train 0.5 | Epoch: 111 | Iter: 181800 | Total Loss: 0.003130 | Recon Loss: 0.002603 | Commit Loss: 0.001053 | Perplexity: 1432.670082
Trainning Epoch:  36%|███▋      | 112/308 [25:48:22<45:19:09, 832.40s/it]2025-09-09 06:46:48,659 Stage: Train 0.5 | Epoch: 112 | Iter: 182000 | Total Loss: 0.003117 | Recon Loss: 0.002584 | Commit Loss: 0.001067 | Perplexity: 1428.079266
2025-09-09 06:48:31,167 Stage: Train 0.5 | Epoch: 112 | Iter: 182200 | Total Loss: 0.003057 | Recon Loss: 0.002540 | Commit Loss: 0.001034 | Perplexity: 1429.591665
2025-09-09 06:50:13,714 Stage: Train 0.5 | Epoch: 112 | Iter: 182400 | Total Loss: 0.003081 | Recon Loss: 0.002552 | Commit Loss: 0.001058 | Perplexity: 1435.329158
2025-09-09 06:51:55,954 Stage: Train 0.5 | Epoch: 112 | Iter: 182600 | Total Loss: 0.003109 | Recon Loss: 0.002586 | Commit Loss: 0.001046 | Perplexity: 1429.140797
2025-09-09 06:53:38,527 Stage: Train 0.5 | Epoch: 112 | Iter: 182800 | Total Loss: 0.003065 | Recon Loss: 0.002543 | Commit Loss: 0.001043 | Perplexity: 1439.417865
2025-09-09 06:55:21,216 Stage: Train 0.5 | Epoch: 112 | Iter: 183000 | Total Loss: 0.003103 | Recon Loss: 0.002564 | Commit Loss: 0.001079 | Perplexity: 1427.699915
2025-09-09 06:57:03,665 Stage: Train 0.5 | Epoch: 112 | Iter: 183200 | Total Loss: 0.003035 | Recon Loss: 0.002506 | Commit Loss: 0.001059 | Perplexity: 1446.630163
2025-09-09 06:58:46,035 Stage: Train 0.5 | Epoch: 112 | Iter: 183400 | Total Loss: 0.003127 | Recon Loss: 0.002597 | Commit Loss: 0.001060 | Perplexity: 1434.749875
Trainning Epoch:  37%|███▋      | 113/308 [26:02:14<45:04:41, 832.21s/it]2025-09-09 07:00:28,143 Stage: Train 0.5 | Epoch: 113 | Iter: 183600 | Total Loss: 0.003137 | Recon Loss: 0.002607 | Commit Loss: 0.001060 | Perplexity: 1432.552998
2025-09-09 07:02:10,613 Stage: Train 0.5 | Epoch: 113 | Iter: 183800 | Total Loss: 0.003052 | Recon Loss: 0.002520 | Commit Loss: 0.001063 | Perplexity: 1438.314812
2025-09-09 07:03:53,311 Stage: Train 0.5 | Epoch: 113 | Iter: 184000 | Total Loss: 0.003116 | Recon Loss: 0.002586 | Commit Loss: 0.001061 | Perplexity: 1444.604620
2025-09-09 07:05:35,769 Stage: Train 0.5 | Epoch: 113 | Iter: 184200 | Total Loss: 0.003029 | Recon Loss: 0.002505 | Commit Loss: 0.001048 | Perplexity: 1438.097562
2025-09-09 07:07:18,426 Stage: Train 0.5 | Epoch: 113 | Iter: 184400 | Total Loss: 0.003111 | Recon Loss: 0.002583 | Commit Loss: 0.001056 | Perplexity: 1441.418923
2025-09-09 07:09:00,542 Stage: Train 0.5 | Epoch: 113 | Iter: 184600 | Total Loss: 0.003085 | Recon Loss: 0.002553 | Commit Loss: 0.001066 | Perplexity: 1428.711609
2025-09-09 07:10:42,993 Stage: Train 0.5 | Epoch: 113 | Iter: 184800 | Total Loss: 0.003055 | Recon Loss: 0.002532 | Commit Loss: 0.001045 | Perplexity: 1424.750100
2025-09-09 07:12:25,712 Stage: Train 0.5 | Epoch: 113 | Iter: 185000 | Total Loss: 0.003072 | Recon Loss: 0.002536 | Commit Loss: 0.001072 | Perplexity: 1427.468229
Trainning Epoch:  37%|███▋      | 114/308 [26:16:06<44:50:54, 832.24s/it]2025-09-09 07:14:08,211 Stage: Train 0.5 | Epoch: 114 | Iter: 185200 | Total Loss: 0.003075 | Recon Loss: 0.002549 | Commit Loss: 0.001051 | Perplexity: 1428.174294
2025-09-09 07:15:50,356 Stage: Train 0.5 | Epoch: 114 | Iter: 185400 | Total Loss: 0.003155 | Recon Loss: 0.002609 | Commit Loss: 0.001092 | Perplexity: 1435.018857
2025-09-09 07:17:32,768 Stage: Train 0.5 | Epoch: 114 | Iter: 185600 | Total Loss: 0.002979 | Recon Loss: 0.002472 | Commit Loss: 0.001015 | Perplexity: 1417.712941
2025-09-09 07:19:15,018 Stage: Train 0.5 | Epoch: 114 | Iter: 185800 | Total Loss: 0.003070 | Recon Loss: 0.002551 | Commit Loss: 0.001038 | Perplexity: 1433.283206
2025-09-09 07:20:57,513 Stage: Train 0.5 | Epoch: 114 | Iter: 186000 | Total Loss: 0.003098 | Recon Loss: 0.002572 | Commit Loss: 0.001052 | Perplexity: 1425.074717
2025-09-09 07:22:39,906 Stage: Train 0.5 | Epoch: 114 | Iter: 186200 | Total Loss: 0.003122 | Recon Loss: 0.002596 | Commit Loss: 0.001053 | Perplexity: 1446.871247
2025-09-09 07:24:22,509 Stage: Train 0.5 | Epoch: 114 | Iter: 186400 | Total Loss: 0.003066 | Recon Loss: 0.002543 | Commit Loss: 0.001045 | Perplexity: 1426.788712
2025-09-09 07:26:05,115 Stage: Train 0.5 | Epoch: 114 | Iter: 186600 | Total Loss: 0.003169 | Recon Loss: 0.002644 | Commit Loss: 0.001049 | Perplexity: 1434.852336
Trainning Epoch:  37%|███▋      | 115/308 [26:29:58<44:36:17, 832.01s/it]2025-09-09 07:27:47,371 Stage: Train 0.5 | Epoch: 115 | Iter: 186800 | Total Loss: 0.003000 | Recon Loss: 0.002485 | Commit Loss: 0.001031 | Perplexity: 1433.796128
2025-09-09 07:29:29,871 Stage: Train 0.5 | Epoch: 115 | Iter: 187000 | Total Loss: 0.003048 | Recon Loss: 0.002526 | Commit Loss: 0.001045 | Perplexity: 1435.241267
2025-09-09 07:31:12,135 Stage: Train 0.5 | Epoch: 115 | Iter: 187200 | Total Loss: 0.003046 | Recon Loss: 0.002513 | Commit Loss: 0.001066 | Perplexity: 1431.040046
2025-09-09 07:32:54,550 Stage: Train 0.5 | Epoch: 115 | Iter: 187400 | Total Loss: 0.003025 | Recon Loss: 0.002516 | Commit Loss: 0.001018 | Perplexity: 1424.840727
2025-09-09 07:34:37,053 Stage: Train 0.5 | Epoch: 115 | Iter: 187600 | Total Loss: 0.003084 | Recon Loss: 0.002548 | Commit Loss: 0.001072 | Perplexity: 1435.806044
2025-09-09 07:36:20,028 Stage: Train 0.5 | Epoch: 115 | Iter: 187800 | Total Loss: 0.003053 | Recon Loss: 0.002532 | Commit Loss: 0.001043 | Perplexity: 1441.915508
2025-09-09 07:38:02,335 Stage: Train 0.5 | Epoch: 115 | Iter: 188000 | Total Loss: 0.003082 | Recon Loss: 0.002556 | Commit Loss: 0.001051 | Perplexity: 1437.208043
2025-09-09 07:39:44,633 Stage: Train 0.5 | Epoch: 115 | Iter: 188200 | Total Loss: 0.003089 | Recon Loss: 0.002564 | Commit Loss: 0.001050 | Perplexity: 1428.391158
Trainning Epoch:  38%|███▊      | 116/308 [26:43:50<44:22:28, 832.02s/it]2025-09-09 07:41:27,183 Stage: Train 0.5 | Epoch: 116 | Iter: 188400 | Total Loss: 0.003167 | Recon Loss: 0.002634 | Commit Loss: 0.001066 | Perplexity: 1433.001521
2025-09-09 07:43:09,680 Stage: Train 0.5 | Epoch: 116 | Iter: 188600 | Total Loss: 0.003081 | Recon Loss: 0.002567 | Commit Loss: 0.001029 | Perplexity: 1431.536094
2025-09-09 07:44:52,061 Stage: Train 0.5 | Epoch: 116 | Iter: 188800 | Total Loss: 0.003049 | Recon Loss: 0.002528 | Commit Loss: 0.001042 | Perplexity: 1428.010819
2025-09-09 07:46:34,548 Stage: Train 0.5 | Epoch: 116 | Iter: 189000 | Total Loss: 0.002992 | Recon Loss: 0.002473 | Commit Loss: 0.001037 | Perplexity: 1435.459885
2025-09-09 07:48:17,215 Stage: Train 0.5 | Epoch: 116 | Iter: 189200 | Total Loss: 0.003095 | Recon Loss: 0.002565 | Commit Loss: 0.001060 | Perplexity: 1435.251138
2025-09-09 07:49:59,600 Stage: Train 0.5 | Epoch: 116 | Iter: 189400 | Total Loss: 0.003061 | Recon Loss: 0.002537 | Commit Loss: 0.001049 | Perplexity: 1447.998232
2025-09-09 07:51:41,915 Stage: Train 0.5 | Epoch: 116 | Iter: 189600 | Total Loss: 0.003035 | Recon Loss: 0.002505 | Commit Loss: 0.001059 | Perplexity: 1424.817258
2025-09-09 07:53:24,401 Stage: Train 0.5 | Epoch: 116 | Iter: 189800 | Total Loss: 0.003202 | Recon Loss: 0.002659 | Commit Loss: 0.001086 | Perplexity: 1437.487849
2025-09-09 07:55:06,794 Stage: Train 0.5 | Epoch: 116 | Iter: 190000 | Total Loss: 0.003030 | Recon Loss: 0.002507 | Commit Loss: 0.001045 | Perplexity: 1430.062464
Trainning Epoch:  38%|███▊      | 117/308 [26:57:42<44:08:25, 831.97s/it]2025-09-09 07:56:48,923 Stage: Train 0.5 | Epoch: 117 | Iter: 190200 | Total Loss: 0.003045 | Recon Loss: 0.002520 | Commit Loss: 0.001050 | Perplexity: 1442.458771
2025-09-09 07:58:31,404 Stage: Train 0.5 | Epoch: 117 | Iter: 190400 | Total Loss: 0.003146 | Recon Loss: 0.002612 | Commit Loss: 0.001067 | Perplexity: 1439.673093
2025-09-09 08:00:13,869 Stage: Train 0.5 | Epoch: 117 | Iter: 190600 | Total Loss: 0.003059 | Recon Loss: 0.002527 | Commit Loss: 0.001063 | Perplexity: 1435.455087
2025-09-09 08:01:56,335 Stage: Train 0.5 | Epoch: 117 | Iter: 190800 | Total Loss: 0.003103 | Recon Loss: 0.002573 | Commit Loss: 0.001060 | Perplexity: 1430.878951
2025-09-09 08:03:38,914 Stage: Train 0.5 | Epoch: 117 | Iter: 191000 | Total Loss: 0.003067 | Recon Loss: 0.002550 | Commit Loss: 0.001034 | Perplexity: 1428.260598
2025-09-09 08:05:21,488 Stage: Train 0.5 | Epoch: 117 | Iter: 191200 | Total Loss: 0.003021 | Recon Loss: 0.002496 | Commit Loss: 0.001049 | Perplexity: 1444.666104
2025-09-09 08:07:03,842 Stage: Train 0.5 | Epoch: 117 | Iter: 191400 | Total Loss: 0.003017 | Recon Loss: 0.002504 | Commit Loss: 0.001026 | Perplexity: 1418.648625
2025-09-09 08:08:46,303 Stage: Train 0.5 | Epoch: 117 | Iter: 191600 | Total Loss: 0.003062 | Recon Loss: 0.002537 | Commit Loss: 0.001049 | Perplexity: 1433.440100
Trainning Epoch:  38%|███▊      | 118/308 [27:11:33<43:54:28, 831.94s/it]2025-09-09 08:10:28,835 Stage: Train 0.5 | Epoch: 118 | Iter: 191800 | Total Loss: 0.003017 | Recon Loss: 0.002495 | Commit Loss: 0.001043 | Perplexity: 1436.441505
2025-09-09 08:12:11,131 Stage: Train 0.5 | Epoch: 118 | Iter: 192000 | Total Loss: 0.003049 | Recon Loss: 0.002527 | Commit Loss: 0.001045 | Perplexity: 1450.099594
2025-09-09 08:13:53,637 Stage: Train 0.5 | Epoch: 118 | Iter: 192200 | Total Loss: 0.003168 | Recon Loss: 0.002634 | Commit Loss: 0.001067 | Perplexity: 1433.905135
2025-09-09 08:15:36,181 Stage: Train 0.5 | Epoch: 118 | Iter: 192400 | Total Loss: 0.003033 | Recon Loss: 0.002514 | Commit Loss: 0.001038 | Perplexity: 1430.960273
2025-09-09 08:17:18,594 Stage: Train 0.5 | Epoch: 118 | Iter: 192600 | Total Loss: 0.003051 | Recon Loss: 0.002530 | Commit Loss: 0.001041 | Perplexity: 1439.125165
2025-09-09 08:19:00,751 Stage: Train 0.5 | Epoch: 118 | Iter: 192800 | Total Loss: 0.003035 | Recon Loss: 0.002520 | Commit Loss: 0.001031 | Perplexity: 1435.800086
2025-09-09 08:20:43,157 Stage: Train 0.5 | Epoch: 118 | Iter: 193000 | Total Loss: 0.003114 | Recon Loss: 0.002581 | Commit Loss: 0.001068 | Perplexity: 1421.765486
2025-09-09 08:22:25,650 Stage: Train 0.5 | Epoch: 118 | Iter: 193200 | Total Loss: 0.003036 | Recon Loss: 0.002514 | Commit Loss: 0.001042 | Perplexity: 1431.165361
Trainning Epoch:  39%|███▊      | 119/308 [27:25:25<43:40:12, 831.81s/it]2025-09-09 08:24:07,780 Stage: Train 0.5 | Epoch: 119 | Iter: 193400 | Total Loss: 0.003032 | Recon Loss: 0.002508 | Commit Loss: 0.001047 | Perplexity: 1444.059776
2025-09-09 08:25:49,954 Stage: Train 0.5 | Epoch: 119 | Iter: 193600 | Total Loss: 0.002903 | Recon Loss: 0.002393 | Commit Loss: 0.001021 | Perplexity: 1425.949979
2025-09-09 08:27:32,254 Stage: Train 0.5 | Epoch: 119 | Iter: 193800 | Total Loss: 0.003058 | Recon Loss: 0.002532 | Commit Loss: 0.001051 | Perplexity: 1433.664857
2025-09-09 08:29:14,748 Stage: Train 0.5 | Epoch: 119 | Iter: 194000 | Total Loss: 0.003107 | Recon Loss: 0.002579 | Commit Loss: 0.001058 | Perplexity: 1422.012908
2025-09-09 08:30:57,366 Stage: Train 0.5 | Epoch: 119 | Iter: 194200 | Total Loss: 0.003032 | Recon Loss: 0.002522 | Commit Loss: 0.001021 | Perplexity: 1425.247573
2025-09-09 08:32:40,075 Stage: Train 0.5 | Epoch: 119 | Iter: 194400 | Total Loss: 0.003040 | Recon Loss: 0.002514 | Commit Loss: 0.001053 | Perplexity: 1441.568724
2025-09-09 08:34:22,820 Stage: Train 0.5 | Epoch: 119 | Iter: 194600 | Total Loss: 0.003051 | Recon Loss: 0.002524 | Commit Loss: 0.001055 | Perplexity: 1451.659949
2025-09-09 08:36:05,321 Stage: Train 0.5 | Epoch: 119 | Iter: 194800 | Total Loss: 0.003089 | Recon Loss: 0.002557 | Commit Loss: 0.001064 | Perplexity: 1448.336981
Trainning Epoch:  39%|███▉      | 120/308 [27:39:17<43:26:32, 831.87s/it]2025-09-09 08:37:47,619 Stage: Train 0.5 | Epoch: 120 | Iter: 195000 | Total Loss: 0.003155 | Recon Loss: 0.002636 | Commit Loss: 0.001040 | Perplexity: 1431.522504
2025-09-09 08:39:30,083 Stage: Train 0.5 | Epoch: 120 | Iter: 195200 | Total Loss: 0.003068 | Recon Loss: 0.002539 | Commit Loss: 0.001059 | Perplexity: 1438.383788
2025-09-09 08:41:12,456 Stage: Train 0.5 | Epoch: 120 | Iter: 195400 | Total Loss: 0.003119 | Recon Loss: 0.002594 | Commit Loss: 0.001052 | Perplexity: 1443.556683
2025-09-09 08:42:54,825 Stage: Train 0.5 | Epoch: 120 | Iter: 195600 | Total Loss: 0.002989 | Recon Loss: 0.002474 | Commit Loss: 0.001031 | Perplexity: 1438.570383
2025-09-09 08:44:37,396 Stage: Train 0.5 | Epoch: 120 | Iter: 195800 | Total Loss: 0.003101 | Recon Loss: 0.002585 | Commit Loss: 0.001032 | Perplexity: 1423.361024
2025-09-09 08:46:19,936 Stage: Train 0.5 | Epoch: 120 | Iter: 196000 | Total Loss: 0.003076 | Recon Loss: 0.002561 | Commit Loss: 0.001032 | Perplexity: 1444.663060
2025-09-09 08:48:02,809 Stage: Train 0.5 | Epoch: 120 | Iter: 196200 | Total Loss: 0.003057 | Recon Loss: 0.002527 | Commit Loss: 0.001059 | Perplexity: 1434.455806
2025-09-09 08:49:45,560 Stage: Train 0.5 | Epoch: 120 | Iter: 196400 | Total Loss: 0.003032 | Recon Loss: 0.002508 | Commit Loss: 0.001049 | Perplexity: 1433.117057
Trainning Epoch:  39%|███▉      | 121/308 [27:53:10<43:13:23, 832.10s/it]2025-09-09 08:51:27,874 Stage: Train 0.5 | Epoch: 121 | Iter: 196600 | Total Loss: 0.003000 | Recon Loss: 0.002481 | Commit Loss: 0.001039 | Perplexity: 1436.579999
2025-09-09 08:53:10,262 Stage: Train 0.5 | Epoch: 121 | Iter: 196800 | Total Loss: 0.003066 | Recon Loss: 0.002545 | Commit Loss: 0.001042 | Perplexity: 1444.221588
2025-09-09 08:54:52,554 Stage: Train 0.5 | Epoch: 121 | Iter: 197000 | Total Loss: 0.003090 | Recon Loss: 0.002560 | Commit Loss: 0.001061 | Perplexity: 1437.636903
2025-09-09 08:56:35,233 Stage: Train 0.5 | Epoch: 121 | Iter: 197200 | Total Loss: 0.003006 | Recon Loss: 0.002496 | Commit Loss: 0.001020 | Perplexity: 1436.883374
2025-09-09 08:58:17,640 Stage: Train 0.5 | Epoch: 121 | Iter: 197400 | Total Loss: 0.002998 | Recon Loss: 0.002471 | Commit Loss: 0.001054 | Perplexity: 1439.540695
2025-09-09 08:59:59,799 Stage: Train 0.5 | Epoch: 121 | Iter: 197600 | Total Loss: 0.003067 | Recon Loss: 0.002547 | Commit Loss: 0.001041 | Perplexity: 1441.622020
2025-09-09 09:01:42,299 Stage: Train 0.5 | Epoch: 121 | Iter: 197800 | Total Loss: 0.003077 | Recon Loss: 0.002555 | Commit Loss: 0.001044 | Perplexity: 1436.905112
2025-09-09 09:03:24,537 Stage: Train 0.5 | Epoch: 121 | Iter: 198000 | Total Loss: 0.003087 | Recon Loss: 0.002561 | Commit Loss: 0.001051 | Perplexity: 1435.276277
Trainning Epoch:  40%|███▉      | 122/308 [28:07:01<42:58:30, 831.78s/it]2025-09-09 09:05:06,822 Stage: Train 0.5 | Epoch: 122 | Iter: 198200 | Total Loss: 0.002956 | Recon Loss: 0.002439 | Commit Loss: 0.001033 | Perplexity: 1432.944061
2025-09-09 09:06:48,519 Stage: Train 0.5 | Epoch: 122 | Iter: 198400 | Total Loss: 0.003040 | Recon Loss: 0.002522 | Commit Loss: 0.001035 | Perplexity: 1445.879868
2025-09-09 09:08:30,503 Stage: Train 0.5 | Epoch: 122 | Iter: 198600 | Total Loss: 0.003063 | Recon Loss: 0.002541 | Commit Loss: 0.001042 | Perplexity: 1436.454637
2025-09-09 09:10:12,476 Stage: Train 0.5 | Epoch: 122 | Iter: 198800 | Total Loss: 0.003071 | Recon Loss: 0.002545 | Commit Loss: 0.001051 | Perplexity: 1435.779280
2025-09-09 09:11:54,525 Stage: Train 0.5 | Epoch: 122 | Iter: 199000 | Total Loss: 0.003182 | Recon Loss: 0.002665 | Commit Loss: 0.001035 | Perplexity: 1428.405680
2025-09-09 09:13:36,948 Stage: Train 0.5 | Epoch: 122 | Iter: 199200 | Total Loss: 0.002971 | Recon Loss: 0.002447 | Commit Loss: 0.001048 | Perplexity: 1442.806219
2025-09-09 09:15:19,248 Stage: Train 0.5 | Epoch: 122 | Iter: 199400 | Total Loss: 0.003062 | Recon Loss: 0.002537 | Commit Loss: 0.001051 | Perplexity: 1450.250480
2025-09-09 09:17:01,694 Stage: Train 0.5 | Epoch: 122 | Iter: 199600 | Total Loss: 0.002968 | Recon Loss: 0.002456 | Commit Loss: 0.001025 | Perplexity: 1422.890164
Trainning Epoch:  40%|███▉      | 123/308 [28:20:50<42:42:42, 831.15s/it]2025-09-09 09:18:43,999 Stage: Train 0.5 | Epoch: 123 | Iter: 199800 | Total Loss: 0.003072 | Recon Loss: 0.002549 | Commit Loss: 0.001045 | Perplexity: 1434.597014
2025-09-09 09:20:26,854 Stage: Train 0.5 | Epoch: 123 | Iter: 200000 | Total Loss: 0.002970 | Recon Loss: 0.002455 | Commit Loss: 0.001030 | Perplexity: 1436.219813
2025-09-09 09:20:26,854 Saving model at iteration 200000
2025-09-09 09:20:26,972 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000
2025-09-09 09:20:27,364 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000/model.safetensors
2025-09-09 09:20:27,752 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000/optimizer.bin
2025-09-09 09:20:27,752 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000/scheduler.bin
2025-09-09 09:20:27,752 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000/sampler.bin
2025-09-09 09:20:27,753 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_124_step_200000/random_states_0.pkl
2025-09-09 09:22:11,381 Stage: Train 0.5 | Epoch: 123 | Iter: 200200 | Total Loss: 0.003028 | Recon Loss: 0.002513 | Commit Loss: 0.001030 | Perplexity: 1445.116415
2025-09-09 09:23:53,985 Stage: Train 0.5 | Epoch: 123 | Iter: 200400 | Total Loss: 0.002995 | Recon Loss: 0.002479 | Commit Loss: 0.001033 | Perplexity: 1443.717872
2025-09-09 09:25:36,497 Stage: Train 0.5 | Epoch: 123 | Iter: 200600 | Total Loss: 0.003004 | Recon Loss: 0.002476 | Commit Loss: 0.001055 | Perplexity: 1431.363331
2025-09-09 09:27:19,152 Stage: Train 0.5 | Epoch: 123 | Iter: 200800 | Total Loss: 0.003021 | Recon Loss: 0.002495 | Commit Loss: 0.001050 | Perplexity: 1448.810598
2025-09-09 09:29:01,558 Stage: Train 0.5 | Epoch: 123 | Iter: 201000 | Total Loss: 0.003008 | Recon Loss: 0.002480 | Commit Loss: 0.001056 | Perplexity: 1431.120041
2025-09-09 09:30:44,124 Stage: Train 0.5 | Epoch: 123 | Iter: 201200 | Total Loss: 0.003078 | Recon Loss: 0.002560 | Commit Loss: 0.001035 | Perplexity: 1432.430110
Trainning Epoch:  40%|████      | 124/308 [28:34:45<42:32:26, 832.32s/it]2025-09-09 09:32:26,802 Stage: Train 0.5 | Epoch: 124 | Iter: 201400 | Total Loss: 0.003103 | Recon Loss: 0.002572 | Commit Loss: 0.001061 | Perplexity: 1440.130360
2025-09-09 09:34:09,473 Stage: Train 0.5 | Epoch: 124 | Iter: 201600 | Total Loss: 0.003042 | Recon Loss: 0.002518 | Commit Loss: 0.001050 | Perplexity: 1432.732624
2025-09-09 09:35:51,627 Stage: Train 0.5 | Epoch: 124 | Iter: 201800 | Total Loss: 0.003096 | Recon Loss: 0.002558 | Commit Loss: 0.001076 | Perplexity: 1448.464976
2025-09-09 09:37:34,110 Stage: Train 0.5 | Epoch: 124 | Iter: 202000 | Total Loss: 0.002942 | Recon Loss: 0.002426 | Commit Loss: 0.001032 | Perplexity: 1440.395050
2025-09-09 09:39:16,543 Stage: Train 0.5 | Epoch: 124 | Iter: 202200 | Total Loss: 0.003058 | Recon Loss: 0.002536 | Commit Loss: 0.001045 | Perplexity: 1430.083580
2025-09-09 09:40:59,051 Stage: Train 0.5 | Epoch: 124 | Iter: 202400 | Total Loss: 0.003006 | Recon Loss: 0.002485 | Commit Loss: 0.001042 | Perplexity: 1444.104807
2025-09-09 09:42:41,708 Stage: Train 0.5 | Epoch: 124 | Iter: 202600 | Total Loss: 0.002963 | Recon Loss: 0.002455 | Commit Loss: 0.001018 | Perplexity: 1438.869098
2025-09-09 09:44:24,109 Stage: Train 0.5 | Epoch: 124 | Iter: 202800 | Total Loss: 0.002985 | Recon Loss: 0.002471 | Commit Loss: 0.001028 | Perplexity: 1443.167711
2025-09-09 09:46:06,613 Stage: Train 0.5 | Epoch: 124 | Iter: 203000 | Total Loss: 0.003049 | Recon Loss: 0.002520 | Commit Loss: 0.001058 | Perplexity: 1441.889086
Trainning Epoch:  41%|████      | 125/308 [28:48:37<42:18:23, 832.26s/it]2025-09-09 09:47:49,163 Stage: Train 0.5 | Epoch: 125 | Iter: 203200 | Total Loss: 0.002998 | Recon Loss: 0.002480 | Commit Loss: 0.001035 | Perplexity: 1445.805613
2025-09-09 09:49:31,751 Stage: Train 0.5 | Epoch: 125 | Iter: 203400 | Total Loss: 0.003014 | Recon Loss: 0.002501 | Commit Loss: 0.001026 | Perplexity: 1437.808384
2025-09-09 09:51:14,299 Stage: Train 0.5 | Epoch: 125 | Iter: 203600 | Total Loss: 0.002978 | Recon Loss: 0.002457 | Commit Loss: 0.001042 | Perplexity: 1442.301553
2025-09-09 09:52:56,844 Stage: Train 0.5 | Epoch: 125 | Iter: 203800 | Total Loss: 0.003039 | Recon Loss: 0.002515 | Commit Loss: 0.001048 | Perplexity: 1429.508563
2025-09-09 09:54:39,386 Stage: Train 0.5 | Epoch: 125 | Iter: 204000 | Total Loss: 0.003053 | Recon Loss: 0.002534 | Commit Loss: 0.001039 | Perplexity: 1438.060120
2025-09-09 09:56:21,806 Stage: Train 0.5 | Epoch: 125 | Iter: 204200 | Total Loss: 0.003072 | Recon Loss: 0.002549 | Commit Loss: 0.001047 | Perplexity: 1436.820269
2025-09-09 09:58:04,231 Stage: Train 0.5 | Epoch: 125 | Iter: 204400 | Total Loss: 0.003030 | Recon Loss: 0.002492 | Commit Loss: 0.001075 | Perplexity: 1452.673871
2025-09-09 09:59:46,478 Stage: Train 0.5 | Epoch: 125 | Iter: 204600 | Total Loss: 0.002986 | Recon Loss: 0.002469 | Commit Loss: 0.001035 | Perplexity: 1436.634949
Trainning Epoch:  41%|████      | 126/308 [29:02:29<42:04:19, 832.20s/it]2025-09-09 10:01:28,845 Stage: Train 0.5 | Epoch: 126 | Iter: 204800 | Total Loss: 0.002981 | Recon Loss: 0.002464 | Commit Loss: 0.001034 | Perplexity: 1452.975054
2025-09-09 10:03:11,503 Stage: Train 0.5 | Epoch: 126 | Iter: 205000 | Total Loss: 0.003019 | Recon Loss: 0.002502 | Commit Loss: 0.001033 | Perplexity: 1436.630505
2025-09-09 10:04:53,880 Stage: Train 0.5 | Epoch: 126 | Iter: 205200 | Total Loss: 0.003050 | Recon Loss: 0.002518 | Commit Loss: 0.001065 | Perplexity: 1437.387022
2025-09-09 10:06:36,210 Stage: Train 0.5 | Epoch: 126 | Iter: 205400 | Total Loss: 0.002975 | Recon Loss: 0.002456 | Commit Loss: 0.001037 | Perplexity: 1449.709433
2025-09-09 10:08:18,835 Stage: Train 0.5 | Epoch: 126 | Iter: 205600 | Total Loss: 0.003025 | Recon Loss: 0.002507 | Commit Loss: 0.001037 | Perplexity: 1434.471747
2025-09-09 10:10:01,341 Stage: Train 0.5 | Epoch: 126 | Iter: 205800 | Total Loss: 0.003032 | Recon Loss: 0.002510 | Commit Loss: 0.001045 | Perplexity: 1447.623119
2025-09-09 10:11:43,767 Stage: Train 0.5 | Epoch: 126 | Iter: 206000 | Total Loss: 0.002962 | Recon Loss: 0.002441 | Commit Loss: 0.001042 | Perplexity: 1430.191448
2025-09-09 10:13:26,158 Stage: Train 0.5 | Epoch: 126 | Iter: 206200 | Total Loss: 0.003029 | Recon Loss: 0.002513 | Commit Loss: 0.001033 | Perplexity: 1431.832327
Trainning Epoch:  41%|████      | 127/308 [29:16:21<41:50:13, 832.12s/it]2025-09-09 10:15:08,668 Stage: Train 0.5 | Epoch: 127 | Iter: 206400 | Total Loss: 0.002973 | Recon Loss: 0.002459 | Commit Loss: 0.001027 | Perplexity: 1442.357671
2025-09-09 10:16:51,202 Stage: Train 0.5 | Epoch: 127 | Iter: 206600 | Total Loss: 0.002992 | Recon Loss: 0.002468 | Commit Loss: 0.001047 | Perplexity: 1439.620139
2025-09-09 10:18:33,643 Stage: Train 0.5 | Epoch: 127 | Iter: 206800 | Total Loss: 0.003058 | Recon Loss: 0.002530 | Commit Loss: 0.001056 | Perplexity: 1431.878430
2025-09-09 10:20:16,011 Stage: Train 0.5 | Epoch: 127 | Iter: 207000 | Total Loss: 0.002932 | Recon Loss: 0.002415 | Commit Loss: 0.001033 | Perplexity: 1445.022231
2025-09-09 10:21:58,174 Stage: Train 0.5 | Epoch: 127 | Iter: 207200 | Total Loss: 0.002976 | Recon Loss: 0.002459 | Commit Loss: 0.001033 | Perplexity: 1446.737816
2025-09-09 10:23:40,735 Stage: Train 0.5 | Epoch: 127 | Iter: 207400 | Total Loss: 0.003060 | Recon Loss: 0.002533 | Commit Loss: 0.001053 | Perplexity: 1440.623189
2025-09-09 10:25:23,110 Stage: Train 0.5 | Epoch: 127 | Iter: 207600 | Total Loss: 0.002959 | Recon Loss: 0.002440 | Commit Loss: 0.001038 | Perplexity: 1438.622645
2025-09-09 10:27:05,755 Stage: Train 0.5 | Epoch: 127 | Iter: 207800 | Total Loss: 0.003023 | Recon Loss: 0.002500 | Commit Loss: 0.001046 | Perplexity: 1437.731461
Trainning Epoch:  42%|████▏     | 128/308 [29:30:13<41:36:14, 832.08s/it]2025-09-09 10:28:48,304 Stage: Train 0.5 | Epoch: 128 | Iter: 208000 | Total Loss: 0.003044 | Recon Loss: 0.002511 | Commit Loss: 0.001067 | Perplexity: 1446.167540
2025-09-09 10:30:30,738 Stage: Train 0.5 | Epoch: 128 | Iter: 208200 | Total Loss: 0.002974 | Recon Loss: 0.002456 | Commit Loss: 0.001035 | Perplexity: 1440.455439
2025-09-09 10:32:13,272 Stage: Train 0.5 | Epoch: 128 | Iter: 208400 | Total Loss: 0.003001 | Recon Loss: 0.002481 | Commit Loss: 0.001039 | Perplexity: 1437.084982
2025-09-09 10:33:55,548 Stage: Train 0.5 | Epoch: 128 | Iter: 208600 | Total Loss: 0.002978 | Recon Loss: 0.002464 | Commit Loss: 0.001028 | Perplexity: 1439.132734
2025-09-09 10:35:37,900 Stage: Train 0.5 | Epoch: 128 | Iter: 208800 | Total Loss: 0.002962 | Recon Loss: 0.002453 | Commit Loss: 0.001018 | Perplexity: 1436.679458
2025-09-09 10:37:20,414 Stage: Train 0.5 | Epoch: 128 | Iter: 209000 | Total Loss: 0.002984 | Recon Loss: 0.002468 | Commit Loss: 0.001032 | Perplexity: 1443.490275
2025-09-09 10:39:03,279 Stage: Train 0.5 | Epoch: 128 | Iter: 209200 | Total Loss: 0.003044 | Recon Loss: 0.002516 | Commit Loss: 0.001056 | Perplexity: 1444.234787
2025-09-09 10:40:45,546 Stage: Train 0.5 | Epoch: 128 | Iter: 209400 | Total Loss: 0.002988 | Recon Loss: 0.002465 | Commit Loss: 0.001046 | Perplexity: 1447.765590
Trainning Epoch:  42%|████▏     | 129/308 [29:44:05<41:22:14, 832.04s/it]2025-09-09 10:42:27,752 Stage: Train 0.5 | Epoch: 129 | Iter: 209600 | Total Loss: 0.002939 | Recon Loss: 0.002423 | Commit Loss: 0.001031 | Perplexity: 1437.048734
2025-09-09 10:44:10,226 Stage: Train 0.5 | Epoch: 129 | Iter: 209800 | Total Loss: 0.002960 | Recon Loss: 0.002450 | Commit Loss: 0.001021 | Perplexity: 1440.163871
2025-09-09 10:45:52,649 Stage: Train 0.5 | Epoch: 129 | Iter: 210000 | Total Loss: 0.002950 | Recon Loss: 0.002425 | Commit Loss: 0.001050 | Perplexity: 1439.712633
2025-09-09 10:47:35,089 Stage: Train 0.5 | Epoch: 129 | Iter: 210200 | Total Loss: 0.002991 | Recon Loss: 0.002469 | Commit Loss: 0.001042 | Perplexity: 1444.349340
2025-09-09 10:49:17,779 Stage: Train 0.5 | Epoch: 129 | Iter: 210400 | Total Loss: 0.003003 | Recon Loss: 0.002488 | Commit Loss: 0.001031 | Perplexity: 1442.592014
2025-09-09 10:51:00,238 Stage: Train 0.5 | Epoch: 129 | Iter: 210600 | Total Loss: 0.003106 | Recon Loss: 0.002584 | Commit Loss: 0.001043 | Perplexity: 1451.206743
2025-09-09 10:52:42,770 Stage: Train 0.5 | Epoch: 129 | Iter: 210800 | Total Loss: 0.002990 | Recon Loss: 0.002463 | Commit Loss: 0.001054 | Perplexity: 1437.108895
2025-09-09 10:54:25,143 Stage: Train 0.5 | Epoch: 129 | Iter: 211000 | Total Loss: 0.002997 | Recon Loss: 0.002473 | Commit Loss: 0.001047 | Perplexity: 1447.537565
Trainning Epoch:  42%|████▏     | 130/308 [29:57:57<41:08:18, 832.01s/it]2025-09-09 10:56:07,441 Stage: Train 0.5 | Epoch: 130 | Iter: 211200 | Total Loss: 0.003069 | Recon Loss: 0.002549 | Commit Loss: 0.001039 | Perplexity: 1450.574199
2025-09-09 10:57:49,955 Stage: Train 0.5 | Epoch: 130 | Iter: 211400 | Total Loss: 0.003025 | Recon Loss: 0.002499 | Commit Loss: 0.001054 | Perplexity: 1440.912430
2025-09-09 10:59:32,433 Stage: Train 0.5 | Epoch: 130 | Iter: 211600 | Total Loss: 0.002954 | Recon Loss: 0.002432 | Commit Loss: 0.001044 | Perplexity: 1447.271443
2025-09-09 11:01:14,865 Stage: Train 0.5 | Epoch: 130 | Iter: 211800 | Total Loss: 0.002994 | Recon Loss: 0.002485 | Commit Loss: 0.001017 | Perplexity: 1447.715797
2025-09-09 11:02:57,242 Stage: Train 0.5 | Epoch: 130 | Iter: 212000 | Total Loss: 0.002976 | Recon Loss: 0.002456 | Commit Loss: 0.001039 | Perplexity: 1444.774260
2025-09-09 11:04:39,503 Stage: Train 0.5 | Epoch: 130 | Iter: 212200 | Total Loss: 0.003048 | Recon Loss: 0.002526 | Commit Loss: 0.001043 | Perplexity: 1438.173008
2025-09-09 11:06:21,984 Stage: Train 0.5 | Epoch: 130 | Iter: 212400 | Total Loss: 0.002943 | Recon Loss: 0.002434 | Commit Loss: 0.001017 | Perplexity: 1430.490314
2025-09-09 11:08:04,370 Stage: Train 0.5 | Epoch: 130 | Iter: 212600 | Total Loss: 0.002948 | Recon Loss: 0.002437 | Commit Loss: 0.001022 | Perplexity: 1447.242857
Trainning Epoch:  43%|████▎     | 131/308 [30:11:49<40:54:01, 831.87s/it]2025-09-09 11:09:46,601 Stage: Train 0.5 | Epoch: 131 | Iter: 212800 | Total Loss: 0.003061 | Recon Loss: 0.002537 | Commit Loss: 0.001047 | Perplexity: 1446.684539
2025-09-09 11:11:29,108 Stage: Train 0.5 | Epoch: 131 | Iter: 213000 | Total Loss: 0.002914 | Recon Loss: 0.002407 | Commit Loss: 0.001014 | Perplexity: 1444.907489
2025-09-09 11:13:11,608 Stage: Train 0.5 | Epoch: 131 | Iter: 213200 | Total Loss: 0.002959 | Recon Loss: 0.002453 | Commit Loss: 0.001012 | Perplexity: 1435.031214
2025-09-09 11:14:54,037 Stage: Train 0.5 | Epoch: 131 | Iter: 213400 | Total Loss: 0.002931 | Recon Loss: 0.002409 | Commit Loss: 0.001045 | Perplexity: 1444.455906
2025-09-09 11:16:36,435 Stage: Train 0.5 | Epoch: 131 | Iter: 213600 | Total Loss: 0.002973 | Recon Loss: 0.002454 | Commit Loss: 0.001038 | Perplexity: 1452.661638
2025-09-09 11:18:18,668 Stage: Train 0.5 | Epoch: 131 | Iter: 213800 | Total Loss: 0.003079 | Recon Loss: 0.002535 | Commit Loss: 0.001089 | Perplexity: 1447.126577
2025-09-09 11:20:01,189 Stage: Train 0.5 | Epoch: 131 | Iter: 214000 | Total Loss: 0.002948 | Recon Loss: 0.002434 | Commit Loss: 0.001029 | Perplexity: 1443.438471
2025-09-09 11:21:43,668 Stage: Train 0.5 | Epoch: 131 | Iter: 214200 | Total Loss: 0.002994 | Recon Loss: 0.002480 | Commit Loss: 0.001027 | Perplexity: 1441.532134
Trainning Epoch:  43%|████▎     | 132/308 [30:25:41<40:40:00, 831.82s/it]2025-09-09 11:23:26,155 Stage: Train 0.5 | Epoch: 132 | Iter: 214400 | Total Loss: 0.002967 | Recon Loss: 0.002453 | Commit Loss: 0.001029 | Perplexity: 1447.311097
2025-09-09 11:25:08,795 Stage: Train 0.5 | Epoch: 132 | Iter: 214600 | Total Loss: 0.002941 | Recon Loss: 0.002429 | Commit Loss: 0.001025 | Perplexity: 1447.680083
2025-09-09 11:26:51,523 Stage: Train 0.5 | Epoch: 132 | Iter: 214800 | Total Loss: 0.002993 | Recon Loss: 0.002459 | Commit Loss: 0.001069 | Perplexity: 1447.309617
2025-09-09 11:28:33,907 Stage: Train 0.5 | Epoch: 132 | Iter: 215000 | Total Loss: 0.002961 | Recon Loss: 0.002447 | Commit Loss: 0.001027 | Perplexity: 1444.397321
2025-09-09 11:30:16,294 Stage: Train 0.5 | Epoch: 132 | Iter: 215200 | Total Loss: 0.002994 | Recon Loss: 0.002478 | Commit Loss: 0.001032 | Perplexity: 1445.808516
2025-09-09 11:31:58,898 Stage: Train 0.5 | Epoch: 132 | Iter: 215400 | Total Loss: 0.002960 | Recon Loss: 0.002444 | Commit Loss: 0.001032 | Perplexity: 1451.986693
2025-09-09 11:33:41,102 Stage: Train 0.5 | Epoch: 132 | Iter: 215600 | Total Loss: 0.003044 | Recon Loss: 0.002521 | Commit Loss: 0.001046 | Perplexity: 1454.272618
2025-09-09 11:35:23,527 Stage: Train 0.5 | Epoch: 132 | Iter: 215800 | Total Loss: 0.002977 | Recon Loss: 0.002464 | Commit Loss: 0.001026 | Perplexity: 1439.899421
Trainning Epoch:  43%|████▎     | 133/308 [30:39:33<40:26:31, 831.95s/it]2025-09-09 11:37:06,026 Stage: Train 0.5 | Epoch: 133 | Iter: 216000 | Total Loss: 0.003005 | Recon Loss: 0.002479 | Commit Loss: 0.001053 | Perplexity: 1438.929747
2025-09-09 11:38:48,610 Stage: Train 0.5 | Epoch: 133 | Iter: 216200 | Total Loss: 0.002979 | Recon Loss: 0.002465 | Commit Loss: 0.001028 | Perplexity: 1447.500959
2025-09-09 11:40:31,062 Stage: Train 0.5 | Epoch: 133 | Iter: 216400 | Total Loss: 0.002993 | Recon Loss: 0.002470 | Commit Loss: 0.001046 | Perplexity: 1451.118020
2025-09-09 11:42:13,355 Stage: Train 0.5 | Epoch: 133 | Iter: 216600 | Total Loss: 0.002928 | Recon Loss: 0.002410 | Commit Loss: 0.001037 | Perplexity: 1438.170383
2025-09-09 11:43:55,631 Stage: Train 0.5 | Epoch: 133 | Iter: 216800 | Total Loss: 0.002980 | Recon Loss: 0.002459 | Commit Loss: 0.001042 | Perplexity: 1441.291483
2025-09-09 11:45:37,988 Stage: Train 0.5 | Epoch: 133 | Iter: 217000 | Total Loss: 0.003016 | Recon Loss: 0.002504 | Commit Loss: 0.001023 | Perplexity: 1437.192456
2025-09-09 11:47:20,307 Stage: Train 0.5 | Epoch: 133 | Iter: 217200 | Total Loss: 0.002927 | Recon Loss: 0.002410 | Commit Loss: 0.001035 | Perplexity: 1443.467157
2025-09-09 11:49:02,666 Stage: Train 0.5 | Epoch: 133 | Iter: 217400 | Total Loss: 0.003038 | Recon Loss: 0.002513 | Commit Loss: 0.001050 | Perplexity: 1452.974622
2025-09-09 11:50:44,939 Stage: Train 0.5 | Epoch: 133 | Iter: 217600 | Total Loss: 0.002914 | Recon Loss: 0.002398 | Commit Loss: 0.001032 | Perplexity: 1451.381427
Trainning Epoch:  44%|████▎     | 134/308 [30:53:24<40:11:52, 831.68s/it]2025-09-09 11:52:27,344 Stage: Train 0.5 | Epoch: 134 | Iter: 217800 | Total Loss: 0.002957 | Recon Loss: 0.002443 | Commit Loss: 0.001028 | Perplexity: 1442.744671
2025-09-09 11:54:09,733 Stage: Train 0.5 | Epoch: 134 | Iter: 218000 | Total Loss: 0.002925 | Recon Loss: 0.002413 | Commit Loss: 0.001023 | Perplexity: 1435.962630
2025-09-09 11:55:52,209 Stage: Train 0.5 | Epoch: 134 | Iter: 218200 | Total Loss: 0.002996 | Recon Loss: 0.002467 | Commit Loss: 0.001059 | Perplexity: 1446.609047
2025-09-09 11:57:34,672 Stage: Train 0.5 | Epoch: 134 | Iter: 218400 | Total Loss: 0.002956 | Recon Loss: 0.002436 | Commit Loss: 0.001039 | Perplexity: 1438.895400
2025-09-09 11:59:17,147 Stage: Train 0.5 | Epoch: 134 | Iter: 218600 | Total Loss: 0.002954 | Recon Loss: 0.002442 | Commit Loss: 0.001024 | Perplexity: 1446.527439
2025-09-09 12:00:59,229 Stage: Train 0.5 | Epoch: 134 | Iter: 218800 | Total Loss: 0.002976 | Recon Loss: 0.002458 | Commit Loss: 0.001035 | Perplexity: 1447.658831
2025-09-09 12:02:41,619 Stage: Train 0.5 | Epoch: 134 | Iter: 219000 | Total Loss: 0.002934 | Recon Loss: 0.002413 | Commit Loss: 0.001040 | Perplexity: 1454.038847
2025-09-09 12:04:23,983 Stage: Train 0.5 | Epoch: 134 | Iter: 219200 | Total Loss: 0.003005 | Recon Loss: 0.002470 | Commit Loss: 0.001070 | Perplexity: 1450.660919
Trainning Epoch:  44%|████▍     | 135/308 [31:07:15<39:57:44, 831.59s/it]2025-09-09 12:06:05,881 Stage: Train 0.5 | Epoch: 135 | Iter: 219400 | Total Loss: 0.003037 | Recon Loss: 0.002518 | Commit Loss: 0.001038 | Perplexity: 1444.765887
2025-09-09 12:07:48,210 Stage: Train 0.5 | Epoch: 135 | Iter: 219600 | Total Loss: 0.002948 | Recon Loss: 0.002438 | Commit Loss: 0.001019 | Perplexity: 1441.635218
2025-09-09 12:09:30,178 Stage: Train 0.5 | Epoch: 135 | Iter: 219800 | Total Loss: 0.003047 | Recon Loss: 0.002527 | Commit Loss: 0.001040 | Perplexity: 1444.864689
2025-09-09 12:11:12,355 Stage: Train 0.5 | Epoch: 135 | Iter: 220000 | Total Loss: 0.003052 | Recon Loss: 0.002520 | Commit Loss: 0.001063 | Perplexity: 1451.955437
2025-09-09 12:11:12,355 Saving model at iteration 220000
2025-09-09 12:11:12,636 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000
2025-09-09 12:11:12,984 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000/model.safetensors
2025-09-09 12:11:13,346 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000/optimizer.bin
2025-09-09 12:11:13,346 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000/scheduler.bin
2025-09-09 12:11:13,346 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000/sampler.bin
2025-09-09 12:11:13,347 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_136_step_220000/random_states_0.pkl
2025-09-09 12:12:56,435 Stage: Train 0.5 | Epoch: 135 | Iter: 220200 | Total Loss: 0.002890 | Recon Loss: 0.002380 | Commit Loss: 0.001019 | Perplexity: 1441.291359
2025-09-09 12:14:38,822 Stage: Train 0.5 | Epoch: 135 | Iter: 220400 | Total Loss: 0.002882 | Recon Loss: 0.002374 | Commit Loss: 0.001016 | Perplexity: 1435.592994
2025-09-09 12:16:21,195 Stage: Train 0.5 | Epoch: 135 | Iter: 220600 | Total Loss: 0.002966 | Recon Loss: 0.002453 | Commit Loss: 0.001026 | Perplexity: 1448.509319
2025-09-09 12:18:03,571 Stage: Train 0.5 | Epoch: 135 | Iter: 220800 | Total Loss: 0.002966 | Recon Loss: 0.002448 | Commit Loss: 0.001036 | Perplexity: 1452.321423
Trainning Epoch:  44%|████▍     | 136/308 [31:21:07<39:44:04, 831.66s/it]2025-09-09 12:19:45,804 Stage: Train 0.5 | Epoch: 136 | Iter: 221000 | Total Loss: 0.002975 | Recon Loss: 0.002454 | Commit Loss: 0.001042 | Perplexity: 1446.437693
2025-09-09 12:21:28,261 Stage: Train 0.5 | Epoch: 136 | Iter: 221200 | Total Loss: 0.002891 | Recon Loss: 0.002384 | Commit Loss: 0.001016 | Perplexity: 1452.569860
2025-09-09 12:23:10,536 Stage: Train 0.5 | Epoch: 136 | Iter: 221400 | Total Loss: 0.003058 | Recon Loss: 0.002538 | Commit Loss: 0.001042 | Perplexity: 1446.727562
2025-09-09 12:24:52,908 Stage: Train 0.5 | Epoch: 136 | Iter: 221600 | Total Loss: 0.002992 | Recon Loss: 0.002458 | Commit Loss: 0.001068 | Perplexity: 1460.137833
2025-09-09 12:26:35,160 Stage: Train 0.5 | Epoch: 136 | Iter: 221800 | Total Loss: 0.002883 | Recon Loss: 0.002381 | Commit Loss: 0.001005 | Perplexity: 1438.382076
2025-09-09 12:28:17,461 Stage: Train 0.5 | Epoch: 136 | Iter: 222000 | Total Loss: 0.003030 | Recon Loss: 0.002500 | Commit Loss: 0.001060 | Perplexity: 1443.154913
2025-09-09 12:30:00,077 Stage: Train 0.5 | Epoch: 136 | Iter: 222200 | Total Loss: 0.003119 | Recon Loss: 0.002612 | Commit Loss: 0.001013 | Perplexity: 1432.931008
2025-09-09 12:31:42,401 Stage: Train 0.5 | Epoch: 136 | Iter: 222400 | Total Loss: 0.002849 | Recon Loss: 0.002341 | Commit Loss: 0.001018 | Perplexity: 1437.863414
Trainning Epoch:  44%|████▍     | 137/308 [31:34:58<39:29:49, 831.52s/it]2025-09-09 12:33:24,549 Stage: Train 0.5 | Epoch: 137 | Iter: 222600 | Total Loss: 0.002986 | Recon Loss: 0.002465 | Commit Loss: 0.001043 | Perplexity: 1445.327035
2025-09-09 12:35:06,974 Stage: Train 0.5 | Epoch: 137 | Iter: 222800 | Total Loss: 0.002921 | Recon Loss: 0.002397 | Commit Loss: 0.001049 | Perplexity: 1451.783995
2025-09-09 12:36:49,466 Stage: Train 0.5 | Epoch: 137 | Iter: 223000 | Total Loss: 0.002950 | Recon Loss: 0.002439 | Commit Loss: 0.001022 | Perplexity: 1444.780224
2025-09-09 12:38:24,541 Stage: Train 0.5 | Epoch: 137 | Iter: 223200 | Total Loss: 0.002928 | Recon Loss: 0.002413 | Commit Loss: 0.001030 | Perplexity: 1445.783994
2025-09-09 12:39:11,077 Stage: Train 0.5 | Epoch: 137 | Iter: 223400 | Total Loss: 0.003037 | Recon Loss: 0.002511 | Commit Loss: 0.001052 | Perplexity: 1435.885125
2025-09-09 12:40:17,666 Stage: Train 0.5 | Epoch: 137 | Iter: 223600 | Total Loss: 0.002945 | Recon Loss: 0.002435 | Commit Loss: 0.001020 | Perplexity: 1444.710892
2025-09-09 12:42:00,047 Stage: Train 0.5 | Epoch: 137 | Iter: 223800 | Total Loss: 0.002960 | Recon Loss: 0.002443 | Commit Loss: 0.001033 | Perplexity: 1449.602627
2025-09-09 12:43:42,540 Stage: Train 0.5 | Epoch: 137 | Iter: 224000 | Total Loss: 0.002937 | Recon Loss: 0.002426 | Commit Loss: 0.001021 | Perplexity: 1452.723817
Trainning Epoch:  45%|████▍     | 138/308 [31:47:11<37:51:42, 801.78s/it]2025-09-09 12:45:24,877 Stage: Train 0.5 | Epoch: 138 | Iter: 224200 | Total Loss: 0.003015 | Recon Loss: 0.002505 | Commit Loss: 0.001020 | Perplexity: 1433.676216
2025-09-09 12:47:07,421 Stage: Train 0.5 | Epoch: 138 | Iter: 224400 | Total Loss: 0.002906 | Recon Loss: 0.002394 | Commit Loss: 0.001023 | Perplexity: 1448.977086
2025-09-09 12:48:49,839 Stage: Train 0.5 | Epoch: 138 | Iter: 224600 | Total Loss: 0.003108 | Recon Loss: 0.002585 | Commit Loss: 0.001047 | Perplexity: 1446.435851
2025-09-09 12:50:32,281 Stage: Train 0.5 | Epoch: 138 | Iter: 224800 | Total Loss: 0.002962 | Recon Loss: 0.002434 | Commit Loss: 0.001055 | Perplexity: 1458.623032
2025-09-09 12:52:14,691 Stage: Train 0.5 | Epoch: 138 | Iter: 225000 | Total Loss: 0.002856 | Recon Loss: 0.002348 | Commit Loss: 0.001017 | Perplexity: 1451.294214
2025-09-09 12:53:57,010 Stage: Train 0.5 | Epoch: 138 | Iter: 225200 | Total Loss: 0.003017 | Recon Loss: 0.002509 | Commit Loss: 0.001016 | Perplexity: 1431.971940
2025-09-09 12:55:39,490 Stage: Train 0.5 | Epoch: 138 | Iter: 225400 | Total Loss: 0.002947 | Recon Loss: 0.002423 | Commit Loss: 0.001048 | Perplexity: 1461.098334
2025-09-09 12:57:21,632 Stage: Train 0.5 | Epoch: 138 | Iter: 225600 | Total Loss: 0.002922 | Recon Loss: 0.002408 | Commit Loss: 0.001027 | Perplexity: 1444.772869
Trainning Epoch:  45%|████▌     | 139/308 [32:01:02<38:03:25, 810.69s/it]2025-09-09 12:59:04,178 Stage: Train 0.5 | Epoch: 139 | Iter: 225800 | Total Loss: 0.002959 | Recon Loss: 0.002436 | Commit Loss: 0.001046 | Perplexity: 1447.831917
2025-09-09 13:00:46,617 Stage: Train 0.5 | Epoch: 139 | Iter: 226000 | Total Loss: 0.002953 | Recon Loss: 0.002435 | Commit Loss: 0.001036 | Perplexity: 1458.119854
2025-09-09 13:02:28,874 Stage: Train 0.5 | Epoch: 139 | Iter: 226200 | Total Loss: 0.002971 | Recon Loss: 0.002459 | Commit Loss: 0.001023 | Perplexity: 1443.524343
2025-09-09 13:04:11,223 Stage: Train 0.5 | Epoch: 139 | Iter: 226400 | Total Loss: 0.002962 | Recon Loss: 0.002429 | Commit Loss: 0.001066 | Perplexity: 1449.711976
2025-09-09 13:05:53,596 Stage: Train 0.5 | Epoch: 139 | Iter: 226600 | Total Loss: 0.002949 | Recon Loss: 0.002442 | Commit Loss: 0.001014 | Perplexity: 1443.868024
2025-09-09 13:07:35,787 Stage: Train 0.5 | Epoch: 139 | Iter: 226800 | Total Loss: 0.002971 | Recon Loss: 0.002460 | Commit Loss: 0.001021 | Perplexity: 1434.522290
2025-09-09 13:09:18,200 Stage: Train 0.5 | Epoch: 139 | Iter: 227000 | Total Loss: 0.002921 | Recon Loss: 0.002411 | Commit Loss: 0.001019 | Perplexity: 1447.093726
2025-09-09 13:11:00,293 Stage: Train 0.5 | Epoch: 139 | Iter: 227200 | Total Loss: 0.002957 | Recon Loss: 0.002442 | Commit Loss: 0.001031 | Perplexity: 1454.534680
Trainning Epoch:  45%|████▌     | 140/308 [32:14:53<38:06:55, 816.76s/it]2025-09-09 13:12:42,764 Stage: Train 0.5 | Epoch: 140 | Iter: 227400 | Total Loss: 0.002904 | Recon Loss: 0.002389 | Commit Loss: 0.001031 | Perplexity: 1440.062064
2025-09-09 13:14:24,867 Stage: Train 0.5 | Epoch: 140 | Iter: 227600 | Total Loss: 0.002999 | Recon Loss: 0.002467 | Commit Loss: 0.001065 | Perplexity: 1450.312109
2025-09-09 13:16:06,920 Stage: Train 0.5 | Epoch: 140 | Iter: 227800 | Total Loss: 0.002940 | Recon Loss: 0.002426 | Commit Loss: 0.001028 | Perplexity: 1444.453225
2025-09-09 13:17:49,319 Stage: Train 0.5 | Epoch: 140 | Iter: 228000 | Total Loss: 0.002949 | Recon Loss: 0.002432 | Commit Loss: 0.001035 | Perplexity: 1451.974453
2025-09-09 13:19:31,823 Stage: Train 0.5 | Epoch: 140 | Iter: 228200 | Total Loss: 0.002928 | Recon Loss: 0.002414 | Commit Loss: 0.001029 | Perplexity: 1451.289429
2025-09-09 13:21:14,113 Stage: Train 0.5 | Epoch: 140 | Iter: 228400 | Total Loss: 0.002909 | Recon Loss: 0.002397 | Commit Loss: 0.001024 | Perplexity: 1457.864639
2025-09-09 13:22:56,385 Stage: Train 0.5 | Epoch: 140 | Iter: 228600 | Total Loss: 0.002907 | Recon Loss: 0.002397 | Commit Loss: 0.001020 | Perplexity: 1448.348767
2025-09-09 13:24:38,699 Stage: Train 0.5 | Epoch: 140 | Iter: 228800 | Total Loss: 0.002939 | Recon Loss: 0.002425 | Commit Loss: 0.001026 | Perplexity: 1459.948275
Trainning Epoch:  46%|████▌     | 141/308 [32:28:44<38:04:47, 820.88s/it]2025-09-09 13:26:20,969 Stage: Train 0.5 | Epoch: 141 | Iter: 229000 | Total Loss: 0.002923 | Recon Loss: 0.002408 | Commit Loss: 0.001030 | Perplexity: 1437.568002
2025-09-09 13:28:03,349 Stage: Train 0.5 | Epoch: 141 | Iter: 229200 | Total Loss: 0.002940 | Recon Loss: 0.002428 | Commit Loss: 0.001024 | Perplexity: 1449.439473
2025-09-09 13:29:45,552 Stage: Train 0.5 | Epoch: 141 | Iter: 229400 | Total Loss: 0.002938 | Recon Loss: 0.002427 | Commit Loss: 0.001021 | Perplexity: 1448.302220
2025-09-09 13:31:28,169 Stage: Train 0.5 | Epoch: 141 | Iter: 229600 | Total Loss: 0.002972 | Recon Loss: 0.002451 | Commit Loss: 0.001042 | Perplexity: 1444.733912
2025-09-09 13:33:10,532 Stage: Train 0.5 | Epoch: 141 | Iter: 229800 | Total Loss: 0.002951 | Recon Loss: 0.002439 | Commit Loss: 0.001024 | Perplexity: 1441.877055
2025-09-09 13:34:52,802 Stage: Train 0.5 | Epoch: 141 | Iter: 230000 | Total Loss: 0.002966 | Recon Loss: 0.002455 | Commit Loss: 0.001022 | Perplexity: 1445.654706
2025-09-09 13:36:35,214 Stage: Train 0.5 | Epoch: 141 | Iter: 230200 | Total Loss: 0.002938 | Recon Loss: 0.002423 | Commit Loss: 0.001030 | Perplexity: 1457.460657
2025-09-09 13:38:17,683 Stage: Train 0.5 | Epoch: 141 | Iter: 230400 | Total Loss: 0.002928 | Recon Loss: 0.002416 | Commit Loss: 0.001023 | Perplexity: 1451.247973
2025-09-09 13:40:00,053 Stage: Train 0.5 | Epoch: 141 | Iter: 230600 | Total Loss: 0.002931 | Recon Loss: 0.002423 | Commit Loss: 0.001015 | Perplexity: 1452.364676
Trainning Epoch:  46%|████▌     | 142/308 [32:42:35<37:59:42, 823.99s/it]2025-09-09 13:41:42,470 Stage: Train 0.5 | Epoch: 142 | Iter: 230800 | Total Loss: 0.002912 | Recon Loss: 0.002402 | Commit Loss: 0.001020 | Perplexity: 1453.197333
2025-09-09 13:43:24,889 Stage: Train 0.5 | Epoch: 142 | Iter: 231000 | Total Loss: 0.002879 | Recon Loss: 0.002365 | Commit Loss: 0.001027 | Perplexity: 1461.706179
2025-09-09 13:45:07,360 Stage: Train 0.5 | Epoch: 142 | Iter: 231200 | Total Loss: 0.002902 | Recon Loss: 0.002394 | Commit Loss: 0.001014 | Perplexity: 1446.732863
2025-09-09 13:46:49,926 Stage: Train 0.5 | Epoch: 142 | Iter: 231400 | Total Loss: 0.002942 | Recon Loss: 0.002431 | Commit Loss: 0.001023 | Perplexity: 1447.145113
2025-09-09 13:48:32,324 Stage: Train 0.5 | Epoch: 142 | Iter: 231600 | Total Loss: 0.002949 | Recon Loss: 0.002424 | Commit Loss: 0.001051 | Perplexity: 1459.550233
2025-09-09 13:50:14,794 Stage: Train 0.5 | Epoch: 142 | Iter: 231800 | Total Loss: 0.002956 | Recon Loss: 0.002431 | Commit Loss: 0.001050 | Perplexity: 1447.876724
2025-09-09 13:51:57,479 Stage: Train 0.5 | Epoch: 142 | Iter: 232000 | Total Loss: 0.002932 | Recon Loss: 0.002420 | Commit Loss: 0.001024 | Perplexity: 1452.085181
2025-09-09 13:53:39,914 Stage: Train 0.5 | Epoch: 142 | Iter: 232200 | Total Loss: 0.002952 | Recon Loss: 0.002437 | Commit Loss: 0.001031 | Perplexity: 1439.951201
Trainning Epoch:  46%|████▋     | 143/308 [32:56:27<37:52:44, 826.45s/it]2025-09-09 13:55:22,269 Stage: Train 0.5 | Epoch: 143 | Iter: 232400 | Total Loss: 0.002985 | Recon Loss: 0.002454 | Commit Loss: 0.001063 | Perplexity: 1449.892396
2025-09-09 13:57:04,729 Stage: Train 0.5 | Epoch: 143 | Iter: 232600 | Total Loss: 0.002878 | Recon Loss: 0.002375 | Commit Loss: 0.001006 | Perplexity: 1452.591485
2025-09-09 13:58:47,349 Stage: Train 0.5 | Epoch: 143 | Iter: 232800 | Total Loss: 0.002893 | Recon Loss: 0.002389 | Commit Loss: 0.001006 | Perplexity: 1445.391896
2025-09-09 14:00:30,086 Stage: Train 0.5 | Epoch: 143 | Iter: 233000 | Total Loss: 0.002861 | Recon Loss: 0.002341 | Commit Loss: 0.001039 | Perplexity: 1457.368642
2025-09-09 14:02:12,645 Stage: Train 0.5 | Epoch: 143 | Iter: 233200 | Total Loss: 0.002946 | Recon Loss: 0.002427 | Commit Loss: 0.001037 | Perplexity: 1452.113766
2025-09-09 14:03:55,416 Stage: Train 0.5 | Epoch: 143 | Iter: 233400 | Total Loss: 0.002996 | Recon Loss: 0.002472 | Commit Loss: 0.001046 | Perplexity: 1445.880418
2025-09-09 14:05:38,043 Stage: Train 0.5 | Epoch: 143 | Iter: 233600 | Total Loss: 0.002912 | Recon Loss: 0.002396 | Commit Loss: 0.001031 | Perplexity: 1451.420015
2025-09-09 14:07:20,553 Stage: Train 0.5 | Epoch: 143 | Iter: 233800 | Total Loss: 0.002906 | Recon Loss: 0.002396 | Commit Loss: 0.001019 | Perplexity: 1448.148140
Trainning Epoch:  47%|████▋     | 144/308 [33:10:20<37:44:17, 828.40s/it]2025-09-09 14:09:02,992 Stage: Train 0.5 | Epoch: 144 | Iter: 234000 | Total Loss: 0.002926 | Recon Loss: 0.002410 | Commit Loss: 0.001031 | Perplexity: 1450.321956
2025-09-09 14:10:45,263 Stage: Train 0.5 | Epoch: 144 | Iter: 234200 | Total Loss: 0.002899 | Recon Loss: 0.002388 | Commit Loss: 0.001022 | Perplexity: 1449.958858
2025-09-09 14:12:27,607 Stage: Train 0.5 | Epoch: 144 | Iter: 234400 | Total Loss: 0.002936 | Recon Loss: 0.002426 | Commit Loss: 0.001020 | Perplexity: 1450.923705
2025-09-09 14:14:10,134 Stage: Train 0.5 | Epoch: 144 | Iter: 234600 | Total Loss: 0.002962 | Recon Loss: 0.002436 | Commit Loss: 0.001051 | Perplexity: 1448.224140
2025-09-09 14:15:52,410 Stage: Train 0.5 | Epoch: 144 | Iter: 234800 | Total Loss: 0.002874 | Recon Loss: 0.002369 | Commit Loss: 0.001010 | Perplexity: 1442.616583
2025-09-09 14:17:34,846 Stage: Train 0.5 | Epoch: 144 | Iter: 235000 | Total Loss: 0.002896 | Recon Loss: 0.002386 | Commit Loss: 0.001020 | Perplexity: 1442.034379
2025-09-09 14:19:17,433 Stage: Train 0.5 | Epoch: 144 | Iter: 235200 | Total Loss: 0.002953 | Recon Loss: 0.002437 | Commit Loss: 0.001033 | Perplexity: 1456.524965
2025-09-09 14:20:59,946 Stage: Train 0.5 | Epoch: 144 | Iter: 235400 | Total Loss: 0.002952 | Recon Loss: 0.002435 | Commit Loss: 0.001035 | Perplexity: 1450.201511
Trainning Epoch:  47%|████▋     | 145/308 [33:24:12<37:33:13, 829.41s/it]2025-09-09 14:22:42,406 Stage: Train 0.5 | Epoch: 145 | Iter: 235600 | Total Loss: 0.002902 | Recon Loss: 0.002395 | Commit Loss: 0.001014 | Perplexity: 1446.175732
2025-09-09 14:24:24,734 Stage: Train 0.5 | Epoch: 145 | Iter: 235800 | Total Loss: 0.002910 | Recon Loss: 0.002400 | Commit Loss: 0.001019 | Perplexity: 1447.699976
2025-09-09 14:26:07,103 Stage: Train 0.5 | Epoch: 145 | Iter: 236000 | Total Loss: 0.002862 | Recon Loss: 0.002353 | Commit Loss: 0.001018 | Perplexity: 1458.184423
2025-09-09 14:27:49,758 Stage: Train 0.5 | Epoch: 145 | Iter: 236200 | Total Loss: 0.003005 | Recon Loss: 0.002487 | Commit Loss: 0.001037 | Perplexity: 1453.904317
2025-09-09 14:29:32,340 Stage: Train 0.5 | Epoch: 145 | Iter: 236400 | Total Loss: 0.002976 | Recon Loss: 0.002460 | Commit Loss: 0.001032 | Perplexity: 1431.020640
2025-09-09 14:31:14,762 Stage: Train 0.5 | Epoch: 145 | Iter: 236600 | Total Loss: 0.002864 | Recon Loss: 0.002362 | Commit Loss: 0.001004 | Perplexity: 1442.052627
2025-09-09 14:32:56,802 Stage: Train 0.5 | Epoch: 145 | Iter: 236800 | Total Loss: 0.002927 | Recon Loss: 0.002419 | Commit Loss: 0.001017 | Perplexity: 1450.004745
2025-09-09 14:34:39,170 Stage: Train 0.5 | Epoch: 145 | Iter: 237000 | Total Loss: 0.002915 | Recon Loss: 0.002386 | Commit Loss: 0.001057 | Perplexity: 1460.935546
Trainning Epoch:  47%|████▋     | 146/308 [33:38:03<37:21:06, 830.04s/it]2025-09-09 14:36:21,534 Stage: Train 0.5 | Epoch: 146 | Iter: 237200 | Total Loss: 0.003304 | Recon Loss: 0.002797 | Commit Loss: 0.001015 | Perplexity: 1441.271563
2025-09-09 14:38:04,022 Stage: Train 0.5 | Epoch: 146 | Iter: 237400 | Total Loss: 0.002791 | Recon Loss: 0.002282 | Commit Loss: 0.001018 | Perplexity: 1456.334729
2025-09-09 14:39:46,577 Stage: Train 0.5 | Epoch: 146 | Iter: 237600 | Total Loss: 0.002886 | Recon Loss: 0.002372 | Commit Loss: 0.001028 | Perplexity: 1442.562604
2025-09-09 14:41:29,173 Stage: Train 0.5 | Epoch: 146 | Iter: 237800 | Total Loss: 0.002951 | Recon Loss: 0.002430 | Commit Loss: 0.001043 | Perplexity: 1441.822513
2025-09-09 14:43:11,750 Stage: Train 0.5 | Epoch: 146 | Iter: 238000 | Total Loss: 0.002922 | Recon Loss: 0.002406 | Commit Loss: 0.001032 | Perplexity: 1450.138842
2025-09-09 14:44:54,177 Stage: Train 0.5 | Epoch: 146 | Iter: 238200 | Total Loss: 0.002936 | Recon Loss: 0.002424 | Commit Loss: 0.001024 | Perplexity: 1450.118552
2025-09-09 14:46:36,747 Stage: Train 0.5 | Epoch: 146 | Iter: 238400 | Total Loss: 0.002897 | Recon Loss: 0.002383 | Commit Loss: 0.001026 | Perplexity: 1452.267950
2025-09-09 14:48:19,291 Stage: Train 0.5 | Epoch: 146 | Iter: 238600 | Total Loss: 0.002918 | Recon Loss: 0.002407 | Commit Loss: 0.001020 | Perplexity: 1452.882503
Trainning Epoch:  48%|████▊     | 147/308 [33:51:55<37:09:05, 830.71s/it]2025-09-09 14:50:01,563 Stage: Train 0.5 | Epoch: 147 | Iter: 238800 | Total Loss: 0.002844 | Recon Loss: 0.002336 | Commit Loss: 0.001016 | Perplexity: 1449.045156
2025-09-09 14:51:43,898 Stage: Train 0.5 | Epoch: 147 | Iter: 239000 | Total Loss: 0.002920 | Recon Loss: 0.002414 | Commit Loss: 0.001012 | Perplexity: 1446.020403
2025-09-09 14:53:26,553 Stage: Train 0.5 | Epoch: 147 | Iter: 239200 | Total Loss: 0.002937 | Recon Loss: 0.002421 | Commit Loss: 0.001032 | Perplexity: 1452.735737
2025-09-09 14:55:09,079 Stage: Train 0.5 | Epoch: 147 | Iter: 239400 | Total Loss: 0.002927 | Recon Loss: 0.002416 | Commit Loss: 0.001020 | Perplexity: 1443.515058
2025-09-09 14:56:51,756 Stage: Train 0.5 | Epoch: 147 | Iter: 239600 | Total Loss: 0.002943 | Recon Loss: 0.002415 | Commit Loss: 0.001056 | Perplexity: 1456.190643
2025-09-09 14:58:34,167 Stage: Train 0.5 | Epoch: 147 | Iter: 239800 | Total Loss: 0.002889 | Recon Loss: 0.002380 | Commit Loss: 0.001018 | Perplexity: 1455.459975
2025-09-09 15:00:16,548 Stage: Train 0.5 | Epoch: 147 | Iter: 240000 | Total Loss: 0.002924 | Recon Loss: 0.002409 | Commit Loss: 0.001031 | Perplexity: 1445.660900
2025-09-09 15:00:16,548 Saving model at iteration 240000
2025-09-09 15:00:16,659 Saving current state to vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000
2025-09-09 15:00:17,030 Model weights saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000/model.safetensors
2025-09-09 15:00:17,409 Optimizer state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000/optimizer.bin
2025-09-09 15:00:17,409 Scheduler state saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000/scheduler.bin
2025-09-09 15:00:17,409 Sampler state for dataloader 0 saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000/sampler.bin
2025-09-09 15:00:17,410 Random states saved in vqvae_experiment/all_datasets_j3d_f64s2/models/checkpoint_epoch_148_step_240000/random_states_0.pkl
2025-09-09 15:02:01,063 Stage: Train 0.5 | Epoch: 147 | Iter: 240200 | Total Loss: 0.002878 | Recon Loss: 0.002374 | Commit Loss: 0.001010 | Perplexity: 1450.443771
Trainning Epoch:  48%|████▊     | 148/308 [34:05:50<36:57:57, 831.74s/it]2025-09-09 15:03:43,397 Stage: Train 0.5 | Epoch: 148 | Iter: 240400 | Total Loss: 0.002914 | Recon Loss: 0.002398 | Commit Loss: 0.001031 | Perplexity: 1454.114344
2025-09-09 15:05:25,943 Stage: Train 0.5 | Epoch: 148 | Iter: 240600 | Total Loss: 0.002959 | Recon Loss: 0.002440 | Commit Loss: 0.001039 | Perplexity: 1441.408751
2025-09-09 15:07:08,359 Stage: Train 0.5 | Epoch: 148 | Iter: 240800 | Total Loss: 0.002961 | Recon Loss: 0.002454 | Commit Loss: 0.001014 | Perplexity: 1440.528729
2025-09-09 15:08:50,845 Stage: Train 0.5 | Epoch: 148 | Iter: 241000 | Total Loss: 0.002912 | Recon Loss: 0.002397 | Commit Loss: 0.001030 | Perplexity: 1459.701759
2025-09-09 15:10:33,355 Stage: Train 0.5 | Epoch: 148 | Iter: 241200 | Total Loss: 0.002858 | Recon Loss: 0.002358 | Commit Loss: 0.000999 | Perplexity: 1445.342590
2025-09-09 15:12:15,911 Stage: Train 0.5 | Epoch: 148 | Iter: 241400 | Total Loss: 0.002873 | Recon Loss: 0.002362 | Commit Loss: 0.001022 | Perplexity: 1463.211283
2025-09-09 15:13:58,378 Stage: Train 0.5 | Epoch: 148 | Iter: 241600 | Total Loss: 0.002920 | Recon Loss: 0.002407 | Commit Loss: 0.001026 | Perplexity: 1454.205311
2025-09-09 15:15:40,699 Stage: Train 0.5 | Epoch: 148 | Iter: 241800 | Total Loss: 0.002893 | Recon Loss: 0.002384 | Commit Loss: 0.001018 | Perplexity: 1445.559356
Trainning Epoch:  48%|████▊     | 149/308 [34:19:42<36:44:16, 831.80s/it]2025-09-09 15:17:22,946 Stage: Train 0.5 | Epoch: 149 | Iter: 242000 | Total Loss: 0.002906 | Recon Loss: 0.002398 | Commit Loss: 0.001017 | Perplexity: 1443.445076
2025-09-09 15:19:05,288 Stage: Train 0.5 | Epoch: 149 | Iter: 242200 | Total Loss: 0.002926 | Recon Loss: 0.002425 | Commit Loss: 0.001001 | Perplexity: 1440.641307
2025-09-09 15:20:47,503 Stage: Train 0.5 | Epoch: 149 | Iter: 242400 | Total Loss: 0.002923 | Recon Loss: 0.002408 | Commit Loss: 0.001030 | Perplexity: 1450.148065
2025-09-09 15:22:29,861 Stage: Train 0.5 | Epoch: 149 | Iter: 242600 | Total Loss: 0.002960 | Recon Loss: 0.002435 | Commit Loss: 0.001050 | Perplexity: 1451.065567
2025-09-09 15:24:12,404 Stage: Train 0.5 | Epoch: 149 | Iter: 242800 | Total Loss: 0.002923 | Recon Loss: 0.002414 | Commit Loss: 0.001018 | Perplexity: 1450.068998
2025-09-09 15:25:54,750 Stage: Train 0.5 | Epoch: 149 | Iter: 243000 | Total Loss: 0.002811 | Recon Loss: 0.002300 | Commit Loss: 0.001021 | Perplexity: 1448.285844
2025-09-09 15:27:37,290 Stage: Train 0.5 | Epoch: 149 | Iter: 243200 | Total Loss: 0.002899 | Recon Loss: 0.002391 | Commit Loss: 0.001016 | Perplexity: 1456.405319
2025-09-09 15:29:19,796 Stage: Train 0.5 | Epoch: 149 | Iter: 243400 | Total Loss: 0.002901 | Recon Loss: 0.002391 | Commit Loss: 0.001021 | Perplexity: 1449.637689
2025-09-09 15:31:02,234 Stage: Train 0.5 | Epoch: 149 | Iter: 243600 | Total Loss: 0.002939 | Recon Loss: 0.002420 | Commit Loss: 0.001039 | Perplexity: 1453.910289
Trainning Epoch:  49%|████▊     | 150/308 [34:33:33<36:30:10, 831.71s/it]2025-09-09 15:32:44,929 Stage: Train 0.5 | Epoch: 150 | Iter: 243800 | Total Loss: 0.002898 | Recon Loss: 0.002389 | Commit Loss: 0.001018 | Perplexity: 1446.678165
2025-09-09 15:34:27,449 Stage: Train 0.5 | Epoch: 150 | Iter: 244000 | Total Loss: 0.002876 | Recon Loss: 0.002367 | Commit Loss: 0.001017 | Perplexity: 1452.104661
2025-09-09 15:36:09,906 Stage: Train 0.5 | Epoch: 150 | Iter: 244200 | Total Loss: 0.002891 | Recon Loss: 0.002370 | Commit Loss: 0.001041 | Perplexity: 1451.408118
2025-09-09 15:37:52,400 Stage: Train 0.5 | Epoch: 150 | Iter: 244400 | Total Loss: 0.002895 | Recon Loss: 0.002391 | Commit Loss: 0.001009 | Perplexity: 1449.028528
2025-09-09 15:39:35,049 Stage: Train 0.5 | Epoch: 150 | Iter: 244600 | Total Loss: 0.002866 | Recon Loss: 0.002354 | Commit Loss: 0.001023 | Perplexity: 1452.245325
2025-09-09 15:41:17,394 Stage: Train 0.5 | Epoch: 150 | Iter: 244800 | Total Loss: 0.002863 | Recon Loss: 0.002360 | Commit Loss: 0.001006 | Perplexity: 1445.328802
2025-09-09 15:42:59,920 Stage: Train 0.5 | Epoch: 150 | Iter: 245000 | Total Loss: 0.002927 | Recon Loss: 0.002410 | Commit Loss: 0.001033 | Perplexity: 1453.931052
2025-09-09 15:44:42,310 Stage: Train 0.5 | Epoch: 150 | Iter: 245200 | Total Loss: 0.002914 | Recon Loss: 0.002402 | Commit Loss: 0.001022 | Perplexity: 1456.022897
Trainning Epoch:  49%|████▉     | 151/308 [34:47:25<36:16:46, 831.89s/it]2025-09-09 15:46:24,778 Stage: Train 0.5 | Epoch: 151 | Iter: 245400 | Total Loss: 0.002909 | Recon Loss: 0.002396 | Commit Loss: 0.001027 | Perplexity: 1443.220303
2025-09-09 15:48:07,510 Stage: Train 0.5 | Epoch: 151 | Iter: 245600 | Total Loss: 0.002822 | Recon Loss: 0.002321 | Commit Loss: 0.001003 | Perplexity: 1454.116746
2025-09-09 15:49:49,804 Stage: Train 0.5 | Epoch: 151 | Iter: 245800 | Total Loss: 0.002905 | Recon Loss: 0.002391 | Commit Loss: 0.001029 | Perplexity: 1463.168214
2025-09-09 15:51:32,037 Stage: Train 0.5 | Epoch: 151 | Iter: 246000 | Total Loss: 0.002863 | Recon Loss: 0.002352 | Commit Loss: 0.001021 | Perplexity: 1451.800886
2025-09-09 15:53:14,361 Stage: Train 0.5 | Epoch: 151 | Iter: 246200 | Total Loss: 0.002891 | Recon Loss: 0.002376 | Commit Loss: 0.001030 | Perplexity: 1452.993737
2025-09-09 15:54:56,702 Stage: Train 0.5 | Epoch: 151 | Iter: 246400 | Total Loss: 0.002932 | Recon Loss: 0.002425 | Commit Loss: 0.001015 | Perplexity: 1455.470329
2025-09-09 15:56:39,323 Stage: Train 0.5 | Epoch: 151 | Iter: 246600 | Total Loss: 0.002903 | Recon Loss: 0.002388 | Commit Loss: 0.001029 | Perplexity: 1446.560186
2025-09-09 15:58:21,839 Stage: Train 0.5 | Epoch: 151 | Iter: 246800 | Total Loss: 0.002910 | Recon Loss: 0.002394 | Commit Loss: 0.001033 | Perplexity: 1446.837505
Trainning Epoch:  49%|████▉     | 152/308 [35:01:17<36:02:49, 831.86s/it]2025-09-09 16:00:04,280 Stage: Train 0.5 | Epoch: 152 | Iter: 247000 | Total Loss: 0.002842 | Recon Loss: 0.002336 | Commit Loss: 0.001013 | Perplexity: 1453.871707
2025-09-09 16:01:46,943 Stage: Train 0.5 | Epoch: 152 | Iter: 247200 | Total Loss: 0.002853 | Recon Loss: 0.002342 | Commit Loss: 0.001022 | Perplexity: 1438.103375
2025-09-09 16:03:29,493 Stage: Train 0.5 | Epoch: 152 | Iter: 247400 | Total Loss: 0.002959 | Recon Loss: 0.002445 | Commit Loss: 0.001028 | Perplexity: 1460.244509
2025-09-09 16:05:12,209 Stage: Train 0.5 | Epoch: 152 | Iter: 247600 | Total Loss: 0.002838 | Recon Loss: 0.002336 | Commit Loss: 0.001004 | Perplexity: 1457.463474
2025-09-09 16:06:54,784 Stage: Train 0.5 | Epoch: 152 | Iter: 247800 | Total Loss: 0.002910 | Recon Loss: 0.002397 | Commit Loss: 0.001027 | Perplexity: 1459.084033
2025-09-09 16:08:37,637 Stage: Train 0.5 | Epoch: 152 | Iter: 248000 | Total Loss: 0.002912 | Recon Loss: 0.002396 | Commit Loss: 0.001031 | Perplexity: 1464.803307
2025-09-09 16:10:20,287 Stage: Train 0.5 | Epoch: 152 | Iter: 248200 | Total Loss: 0.002884 | Recon Loss: 0.002374 | Commit Loss: 0.001021 | Perplexity: 1448.291481
2025-09-09 16:12:02,818 Stage: Train 0.5 | Epoch: 152 | Iter: 248400 | Total Loss: 0.002890 | Recon Loss: 0.002382 | Commit Loss: 0.001016 | Perplexity: 1443.218593
Trainning Epoch:  50%|████▉     | 153/308 [35:15:11<35:50:08, 832.31s/it]2025-09-09 16:13:45,440 Stage: Train 0.5 | Epoch: 153 | Iter: 248600 | Total Loss: 0.002873 | Recon Loss: 0.002370 | Commit Loss: 0.001007 | Perplexity: 1452.950510
2025-09-09 16:15:28,293 Stage: Train 0.5 | Epoch: 153 | Iter: 248800 | Total Loss: 0.002848 | Recon Loss: 0.002331 | Commit Loss: 0.001034 | Perplexity: 1462.164796
2025-09-09 16:17:10,765 Stage: Train 0.5 | Epoch: 153 | Iter: 249000 | Total Loss: 0.002875 | Recon Loss: 0.002356 | Commit Loss: 0.001036 | Perplexity: 1462.578736
2025-09-09 16:18:53,312 Stage: Train 0.5 | Epoch: 153 | Iter: 249200 | Total Loss: 0.002834 | Recon Loss: 0.002326 | Commit Loss: 0.001015 | Perplexity: 1448.331706
2025-09-09 16:20:35,764 Stage: Train 0.5 | Epoch: 153 | Iter: 249400 | Total Loss: 0.002880 | Recon Loss: 0.002379 | Commit Loss: 0.001002 | Perplexity: 1449.388248
2025-09-09 16:22:18,045 Stage: Train 0.5 | Epoch: 153 | Iter: 249600 | Total Loss: 0.002830 | Recon Loss: 0.002328 | Commit Loss: 0.001004 | Perplexity: 1440.195161
2025-09-09 16:24:00,685 Stage: Train 0.5 | Epoch: 153 | Iter: 249800 | Total Loss: 0.002996 | Recon Loss: 0.002482 | Commit Loss: 0.001027 | Perplexity: 1454.879877
2025-09-09 16:25:43,268 Stage: Train 0.5 | Epoch: 153 | Iter: 250000 | Total Loss: 0.002931 | Recon Loss: 0.002418 | Commit Loss: 0.001026 | Perplexity: 1444.822573
Trainning Epoch:  50%|█████     | 154/308 [35:29:03<35:36:29, 832.40s/it]2025-09-09 16:27:25,600 Stage: Train 0.5 | Epoch: 154 | Iter: 250200 | Total Loss: 0.002823 | Recon Loss: 0.002321 | Commit Loss: 0.001004 | Perplexity: 1449.955452
2025-09-09 16:29:08,004 Stage: Train 0.5 | Epoch: 154 | Iter: 250400 | Total Loss: 0.002892 | Recon Loss: 0.002381 | Commit Loss: 0.001022 | Perplexity: 1438.860201
2025-09-09 16:30:50,453 Stage: Train 0.5 | Epoch: 154 | Iter: 250600 | Total Loss: 0.002922 | Recon Loss: 0.002407 | Commit Loss: 0.001029 | Perplexity: 1452.356038
2025-09-09 16:32:32,778 Stage: Train 0.5 | Epoch: 154 | Iter: 250800 | Total Loss: 0.002933 | Recon Loss: 0.002434 | Commit Loss: 0.000999 | Perplexity: 1454.959116
2025-09-09 16:34:15,189 Stage: Train 0.5 | Epoch: 154 | Iter: 251000 | Total Loss: 0.002815 | Recon Loss: 0.002307 | Commit Loss: 0.001017 | Perplexity: 1464.085938
2025-09-09 16:35:57,702 Stage: Train 0.5 | Epoch: 154 | Iter: 251200 | Total Loss: 0.002995 | Recon Loss: 0.002470 | Commit Loss: 0.001052 | Perplexity: 1452.244564
2025-09-09 16:37:40,488 Stage: Train 0.5 | Epoch: 154 | Iter: 251400 | Total Loss: 0.002820 | Recon Loss: 0.002320 | Commit Loss: 0.000998 | Perplexity: 1443.506071
2025-09-09 16:39:22,888 Stage: Train 0.5 | Epoch: 154 | Iter: 251600 | Total Loss: 0.002888 | Recon Loss: 0.002374 | Commit Loss: 0.001028 | Perplexity: 1464.727321
Trainning Epoch:  50%|█████     | 155/308 [35:42:55<35:22:13, 832.24s/it]2025-09-09 16:41:05,250 Stage: Train 0.5 | Epoch: 155 | Iter: 251800 | Total Loss: 0.002882 | Recon Loss: 0.002374 | Commit Loss: 0.001017 | Perplexity: 1447.094327
2025-09-09 16:42:47,742 Stage: Train 0.5 | Epoch: 155 | Iter: 252000 | Total Loss: 0.002862 | Recon Loss: 0.002355 | Commit Loss: 0.001013 | Perplexity: 1444.313125
2025-09-09 16:44:30,394 Stage: Train 0.5 | Epoch: 155 | Iter: 252200 | Total Loss: 0.002862 | Recon Loss: 0.002361 | Commit Loss: 0.001002 | Perplexity: 1456.993349
2025-09-09 16:46:12,757 Stage: Train 0.5 | Epoch: 155 | Iter: 252400 | Total Loss: 0.002951 | Recon Loss: 0.002420 | Commit Loss: 0.001061 | Perplexity: 1453.532662
2025-09-09 16:47:55,334 Stage: Train 0.5 | Epoch: 155 | Iter: 252600 | Total Loss: 0.002899 | Recon Loss: 0.002386 | Commit Loss: 0.001026 | Perplexity: 1451.029790
2025-09-09 16:49:37,768 Stage: Train 0.5 | Epoch: 155 | Iter: 252800 | Total Loss: 0.002823 | Recon Loss: 0.002324 | Commit Loss: 0.000997 | Perplexity: 1456.113395
2025-09-09 16:51:20,089 Stage: Train 0.5 | Epoch: 155 | Iter: 253000 | Total Loss: 0.002853 | Recon Loss: 0.002344 | Commit Loss: 0.001016 | Perplexity: 1454.785807
2025-09-09 16:53:02,703 Stage: Train 0.5 | Epoch: 155 | Iter: 253200 | Total Loss: 0.002913 | Recon Loss: 0.002400 | Commit Loss: 0.001026 | Perplexity: 1449.343422
Trainning Epoch:  51%|█████     | 156/308 [35:56:47<35:08:21, 832.24s/it]2025-09-09 16:54:45,091 Stage: Train 0.5 | Epoch: 156 | Iter: 253400 | Total Loss: 0.002848 | Recon Loss: 0.002342 | Commit Loss: 0.001012 | Perplexity: 1453.127771
2025-09-09 16:56:27,391 Stage: Train 0.5 | Epoch: 156 | Iter: 253600 | Total Loss: 0.002801 | Recon Loss: 0.002302 | Commit Loss: 0.000998 | Perplexity: 1463.784636
2025-09-09 16:58:10,037 Stage: Train 0.5 | Epoch: 156 | Iter: 253800 | Total Loss: 0.002930 | Recon Loss: 0.002416 | Commit Loss: 0.001027 | Perplexity: 1454.625154
2025-09-09 16:59:52,610 Stage: Train 0.5 | Epoch: 156 | Iter: 254000 | Total Loss: 0.002870 | Recon Loss: 0.002369 | Commit Loss: 0.001001 | Perplexity: 1451.039423
2025-09-09 17:01:34,856 Stage: Train 0.5 | Epoch: 156 | Iter: 254200 | Total Loss: 0.002818 | Recon Loss: 0.002309 | Commit Loss: 0.001019 | Perplexity: 1448.497666
2025-09-09 17:03:17,175 Stage: Train 0.5 | Epoch: 156 | Iter: 254400 | Total Loss: 0.002903 | Recon Loss: 0.002387 | Commit Loss: 0.001032 | Perplexity: 1458.890245
2025-09-09 17:04:59,774 Stage: Train 0.5 | Epoch: 156 | Iter: 254600 | Total Loss: 0.002824 | Recon Loss: 0.002314 | Commit Loss: 0.001019 | Perplexity: 1455.620293
2025-09-09 17:06:42,142 Stage: Train 0.5 | Epoch: 156 | Iter: 254800 | Total Loss: 0.002900 | Recon Loss: 0.002382 | Commit Loss: 0.001035 | Perplexity: 1448.788221
Trainning Epoch:  51%|█████     | 157/308 [36:10:39<34:53:54, 832.02s/it]2025-09-09 17:08:24,292 Stage: Train 0.5 | Epoch: 157 | Iter: 255000 | Total Loss: 0.002876 | Recon Loss: 0.002373 | Commit Loss: 0.001006 | Perplexity: 1448.620015
2025-09-09 17:10:06,722 Stage: Train 0.5 | Epoch: 157 | Iter: 255200 | Total Loss: 0.002833 | Recon Loss: 0.002330 | Commit Loss: 0.001005 | Perplexity: 1458.835577
2025-09-09 17:11:49,385 Stage: Train 0.5 | Epoch: 157 | Iter: 255400 | Total Loss: 0.002872 | Recon Loss: 0.002364 | Commit Loss: 0.001017 | Perplexity: 1455.278184
2025-09-09 17:13:31,889 Stage: Train 0.5 | Epoch: 157 | Iter: 255600 | Total Loss: 0.002874 | Recon Loss: 0.002365 | Commit Loss: 0.001018 | Perplexity: 1460.999113
2025-09-09 17:15:14,147 Stage: Train 0.5 | Epoch: 157 | Iter: 255800 | Total Loss: 0.002831 | Recon Loss: 0.002326 | Commit Loss: 0.001010 | Perplexity: 1434.154915
2025-09-09 17:16:56,774 Stage: Train 0.5 | Epoch: 157 | Iter: 256000 | Total Loss: 0.002884 | Recon Loss: 0.002372 | Commit Loss: 0.001023 | Perplexity: 1451.428018
2025-09-09 17:18:39,236 Stage: Train 0.5 | Epoch: 157 | Iter: 256200 | Total Loss: 0.002904 | Recon Loss: 0.002382 | Commit Loss: 0.001044 | Perplexity: 1464.816038
2025-09-09 17:20:21,507 Stage: Train 0.5 | Epoch: 157 | Iter: 256400 | Total Loss: 0.002875 | Recon Loss: 0.002370 | Commit Loss: 0.001011 | Perplexity: 1455.570303
Trainning Epoch:  51%|█████▏    | 158/308 [36:24:31<34:39:54, 831.96s/it]2025-09-09 17:22:03,847 Stage: Train 0.5 | Epoch: 158 | Iter: 256600 | Total Loss: 0.002891 | Recon Loss: 0.002386 | Commit Loss: 0.001011 | Perplexity: 1457.055322
2025-09-09 17:23:46,437 Stage: Train 0.5 | Epoch: 158 | Iter: 256800 | Total Loss: 0.002911 | Recon Loss: 0.002390 | Commit Loss: 0.001041 | Perplexity: 1458.412639
2025-09-09 17:25:28,215 Stage: Train 0.5 | Epoch: 158 | Iter: 257000 | Total Loss: 0.002819 | Recon Loss: 0.002316 | Commit Loss: 0.001005 | Perplexity: 1448.375092
2025-09-09 17:27:10,737 Stage: Train 0.5 | Epoch: 158 | Iter: 257200 | Total Loss: 0.002849 | Recon Loss: 0.002346 | Commit Loss: 0.001005 | Perplexity: 1456.592887
2025-09-09 17:28:53,143 Stage: Train 0.5 | Epoch: 158 | Iter: 257400 | Total Loss: 0.002904 | Recon Loss: 0.002396 | Commit Loss: 0.001015 | Perplexity: 1454.982740
2025-09-09 17:30:35,459 Stage: Train 0.5 | Epoch: 158 | Iter: 257600 | Total Loss: 0.002781 | Recon Loss: 0.002284 | Commit Loss: 0.000994 | Perplexity: 1452.282155
2025-09-09 17:32:17,898 Stage: Train 0.5 | Epoch: 158 | Iter: 257800 | Total Loss: 0.002991 | Recon Loss: 0.002479 | Commit Loss: 0.001025 | Perplexity: 1444.293645
2025-09-09 17:34:00,249 Stage: Train 0.5 | Epoch: 158 | Iter: 258000 | Total Loss: 0.002814 | Recon Loss: 0.002315 | Commit Loss: 0.000997 | Perplexity: 1466.178875
2025-09-09 17:35:42,623 Stage: Train 0.5 | Epoch: 158 | Iter: 258200 | Total Loss: 0.002991 | Recon Loss: 0.002479 | Commit Loss: 0.001024 | Perplexity: 1453.059067
Trainning Epoch:  52%|█████▏    | 159/308 [36:38:22<34:25:17, 831.66s/it]2025-09-09 17:36:40,269 Stage: Train 0.5 | Epoch: 159 | Iter: 258400 | Total Loss: 0.002830 | Recon Loss: 0.002305 | Commit Loss: 0.001050 | Perplexity: 1468.244831
2025-09-09 17:37:59,186 Stage: Train 0.5 | Epoch: 159 | Iter: 258600 | Total Loss: 0.002885 | Recon Loss: 0.002380 | Commit Loss: 0.001011 | Perplexity: 1447.551504
