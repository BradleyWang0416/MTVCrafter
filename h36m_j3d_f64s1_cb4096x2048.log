[2025-09-14 16:32:22,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-09-14 16:32:23,925 
python train_vqvae.py --data_mode joint3d --load_data_file /data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl --num_frames 64 --sample_stride 1 --project_dir vqvae_experiment/h36m_j3d_f64s1_cb4096x2048 --not_find_unused_parameters --nb_code 4096 --codebook_dim 2048
2025-09-14 16:32:43,167 Data loaded with 24076 samples and 1559752 frames
2025-09-14 16:32:43,960 Number of trainable parameters: 38.884867 M
2025-09-14 16:32:43,960 Args: Namespace(data_root='./', num_frames=64, batch_size=32, max_epoch=1000000000.0, total_iter=500000, world_size=1, rank=0, save_interval=20000, warm_up_iter=5000, print_iter=200, learning_rate=0.0002, lr_schedule=[300000], gamma=0.05, weight_decay=0.0001, resume_pth='', device='cuda', project_config='', allow_tf32=False, project_dir='vqvae_experiment/h36m_j3d_f64s1_cb4096x2048', seed=6666, commit_ratio=0.5, nb_code=4096, codebook_dim=2048, load_data_file='/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl', data_mode='joint3d', not_find_unused_parameters=True, sample_stride=1)
Trainning Epoch:   0%|          | 0/665 [00:00<?, ?it/s]2025-09-14 16:33:11,875 current_lr 0.000008 at iteration 200
2025-09-14 16:33:12,011 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.080443 | Recon Loss: 0.078332 | Commit Loss: 0.004223 | Perplexity: 753.825026
2025-09-14 16:33:39,095 current_lr 0.000016 at iteration 400
2025-09-14 16:33:39,230 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.047191 | Recon Loss: 0.040801 | Commit Loss: 0.012780 | Perplexity: 748.205002
2025-09-14 16:34:06,524 current_lr 0.000024 at iteration 600
2025-09-14 16:34:06,660 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.048047 | Recon Loss: 0.034531 | Commit Loss: 0.027031 | Perplexity: 758.727398
Trainning Epoch:   0%|          | 1/665 [01:44<19:11:53, 104.09s/it]2025-09-14 16:34:34,350 current_lr 0.000032 at iteration 800
2025-09-14 16:34:34,487 Stage: Warm Up | Epoch: 1 | Iter: 800 | Total Loss: 0.049295 | Recon Loss: 0.030811 | Commit Loss: 0.036968 | Perplexity: 798.686188
2025-09-14 16:35:01,775 current_lr 0.000040 at iteration 1000
2025-09-14 16:35:01,912 Stage: Warm Up | Epoch: 1 | Iter: 1000 | Total Loss: 0.046885 | Recon Loss: 0.027862 | Commit Loss: 0.038046 | Perplexity: 814.249886
2025-09-14 16:35:29,528 current_lr 0.000048 at iteration 1200
2025-09-14 16:35:29,667 Stage: Warm Up | Epoch: 1 | Iter: 1200 | Total Loss: 0.042316 | Recon Loss: 0.025098 | Commit Loss: 0.034436 | Perplexity: 825.155738
2025-09-14 16:35:57,320 current_lr 0.000056 at iteration 1400
2025-09-14 16:35:57,457 Stage: Warm Up | Epoch: 1 | Iter: 1400 | Total Loss: 0.036865 | Recon Loss: 0.022897 | Commit Loss: 0.027936 | Perplexity: 826.904672
Trainning Epoch:   0%|          | 2/665 [03:28<19:09:19, 104.01s/it]2025-09-14 16:36:24,817 current_lr 0.000064 at iteration 1600
2025-09-14 16:36:24,953 Stage: Warm Up | Epoch: 2 | Iter: 1600 | Total Loss: 0.031677 | Recon Loss: 0.020685 | Commit Loss: 0.021984 | Perplexity: 830.301602
2025-09-14 16:36:52,354 current_lr 0.000072 at iteration 1800
2025-09-14 16:36:52,490 Stage: Warm Up | Epoch: 2 | Iter: 1800 | Total Loss: 0.028354 | Recon Loss: 0.019675 | Commit Loss: 0.017359 | Perplexity: 844.824070
2025-09-14 16:37:19,960 current_lr 0.000080 at iteration 2000
2025-09-14 16:37:20,099 Stage: Warm Up | Epoch: 2 | Iter: 2000 | Total Loss: 0.025474 | Recon Loss: 0.018493 | Commit Loss: 0.013963 | Perplexity: 852.720074
2025-09-14 16:37:47,605 current_lr 0.000088 at iteration 2200
2025-09-14 16:37:47,742 Stage: Warm Up | Epoch: 2 | Iter: 2200 | Total Loss: 0.023215 | Recon Loss: 0.017690 | Commit Loss: 0.011049 | Perplexity: 864.280403
Trainning Epoch:   0%|          | 3/665 [05:11<19:06:29, 103.91s/it]2025-09-14 16:38:15,026 current_lr 0.000096 at iteration 2400
2025-09-14 16:38:15,164 Stage: Warm Up | Epoch: 3 | Iter: 2400 | Total Loss: 0.020743 | Recon Loss: 0.016238 | Commit Loss: 0.009010 | Perplexity: 871.001632
2025-09-14 16:38:42,513 current_lr 0.000104 at iteration 2600
2025-09-14 16:38:42,649 Stage: Warm Up | Epoch: 3 | Iter: 2600 | Total Loss: 0.019733 | Recon Loss: 0.016132 | Commit Loss: 0.007202 | Perplexity: 883.791912
2025-09-14 16:39:10,000 current_lr 0.000112 at iteration 2800
2025-09-14 16:39:10,136 Stage: Warm Up | Epoch: 3 | Iter: 2800 | Total Loss: 0.017905 | Recon Loss: 0.014886 | Commit Loss: 0.006038 | Perplexity: 900.527904
2025-09-14 16:39:37,495 current_lr 0.000120 at iteration 3000
2025-09-14 16:39:37,632 Stage: Warm Up | Epoch: 3 | Iter: 3000 | Total Loss: 0.016724 | Recon Loss: 0.014205 | Commit Loss: 0.005038 | Perplexity: 917.117244
Trainning Epoch:   1%|          | 4/665 [06:55<19:02:42, 103.73s/it]2025-09-14 16:40:05,341 current_lr 0.000128 at iteration 3200
2025-09-14 16:40:05,477 Stage: Warm Up | Epoch: 4 | Iter: 3200 | Total Loss: 0.016647 | Recon Loss: 0.014567 | Commit Loss: 0.004159 | Perplexity: 922.329690
2025-09-14 16:40:32,961 current_lr 0.000136 at iteration 3400
2025-09-14 16:40:33,099 Stage: Warm Up | Epoch: 4 | Iter: 3400 | Total Loss: 0.015149 | Recon Loss: 0.013399 | Commit Loss: 0.003499 | Perplexity: 941.136393
2025-09-14 16:41:00,708 current_lr 0.000144 at iteration 3600
2025-09-14 16:41:00,845 Stage: Warm Up | Epoch: 4 | Iter: 3600 | Total Loss: 0.015205 | Recon Loss: 0.013658 | Commit Loss: 0.003094 | Perplexity: 938.729827
Trainning Epoch:   1%|          | 5/665 [08:39<19:03:10, 103.92s/it]2025-09-14 16:41:28,192 current_lr 0.000152 at iteration 3800
2025-09-14 16:41:28,328 Stage: Warm Up | Epoch: 5 | Iter: 3800 | Total Loss: 0.014583 | Recon Loss: 0.013223 | Commit Loss: 0.002720 | Perplexity: 938.836053
2025-09-14 16:41:55,647 current_lr 0.000160 at iteration 4000
2025-09-14 16:41:55,784 Stage: Warm Up | Epoch: 5 | Iter: 4000 | Total Loss: 0.013189 | Recon Loss: 0.011921 | Commit Loss: 0.002536 | Perplexity: 950.505506
2025-09-14 16:42:23,114 current_lr 0.000168 at iteration 4200
2025-09-14 16:42:23,251 Stage: Warm Up | Epoch: 5 | Iter: 4200 | Total Loss: 0.013950 | Recon Loss: 0.012805 | Commit Loss: 0.002289 | Perplexity: 956.993783
2025-09-14 16:42:50,599 current_lr 0.000176 at iteration 4400
2025-09-14 16:42:50,735 Stage: Warm Up | Epoch: 5 | Iter: 4400 | Total Loss: 0.012838 | Recon Loss: 0.011778 | Commit Loss: 0.002119 | Perplexity: 964.728285
Trainning Epoch:   1%|          | 6/665 [10:22<18:59:23, 103.74s/it]2025-09-14 16:43:18,015 current_lr 0.000184 at iteration 4600
2025-09-14 16:43:18,152 Stage: Warm Up | Epoch: 6 | Iter: 4600 | Total Loss: 0.013370 | Recon Loss: 0.012373 | Commit Loss: 0.001994 | Perplexity: 970.545398
2025-09-14 16:43:45,574 current_lr 0.000192 at iteration 4800
2025-09-14 16:43:45,712 Stage: Warm Up | Epoch: 6 | Iter: 4800 | Total Loss: 0.012496 | Recon Loss: 0.011563 | Commit Loss: 0.001866 | Perplexity: 979.704763
2025-09-14 16:44:13,111 current_lr 0.000200 at iteration 5000
2025-09-14 16:44:13,248 Stage: Warm Up | Epoch: 6 | Iter: 5000 | Total Loss: 0.012352 | Recon Loss: 0.011520 | Commit Loss: 0.001664 | Perplexity: 956.758483
2025-09-14 16:44:40,878 Stage: Train 0.5 | Epoch: 6 | Iter: 5200 | Total Loss: 0.011907 | Recon Loss: 0.011133 | Commit Loss: 0.001548 | Perplexity: 965.848694
Trainning Epoch:   1%|          | 7/665 [12:06<18:57:35, 103.73s/it]2025-09-14 16:45:08,363 Stage: Train 0.5 | Epoch: 7 | Iter: 5400 | Total Loss: 0.011177 | Recon Loss: 0.010424 | Commit Loss: 0.001505 | Perplexity: 985.877516
2025-09-14 16:45:35,909 Stage: Train 0.5 | Epoch: 7 | Iter: 5600 | Total Loss: 0.011012 | Recon Loss: 0.010249 | Commit Loss: 0.001527 | Perplexity: 979.952544
2025-09-14 16:46:03,444 Stage: Train 0.5 | Epoch: 7 | Iter: 5800 | Total Loss: 0.011261 | Recon Loss: 0.010522 | Commit Loss: 0.001480 | Perplexity: 995.725893
2025-09-14 16:46:31,114 Stage: Train 0.5 | Epoch: 7 | Iter: 6000 | Total Loss: 0.010227 | Recon Loss: 0.009481 | Commit Loss: 0.001491 | Perplexity: 1002.129313
Trainning Epoch:   1%|          | 8/665 [13:50<18:56:07, 103.76s/it]2025-09-14 16:46:58,962 Stage: Train 0.5 | Epoch: 8 | Iter: 6200 | Total Loss: 0.010215 | Recon Loss: 0.009483 | Commit Loss: 0.001463 | Perplexity: 1007.245411
2025-09-14 16:47:26,558 Stage: Train 0.5 | Epoch: 8 | Iter: 6400 | Total Loss: 0.010210 | Recon Loss: 0.009447 | Commit Loss: 0.001526 | Perplexity: 1002.003021
2025-09-14 16:47:54,269 Stage: Train 0.5 | Epoch: 8 | Iter: 6600 | Total Loss: 0.010127 | Recon Loss: 0.009413 | Commit Loss: 0.001429 | Perplexity: 997.492569
Trainning Epoch:   1%|▏         | 9/665 [15:34<18:56:03, 103.91s/it]2025-09-14 16:48:21,841 Stage: Train 0.5 | Epoch: 9 | Iter: 6800 | Total Loss: 0.009320 | Recon Loss: 0.008621 | Commit Loss: 0.001398 | Perplexity: 995.500651
2025-09-14 16:48:49,560 Stage: Train 0.5 | Epoch: 9 | Iter: 7000 | Total Loss: 0.009562 | Recon Loss: 0.008838 | Commit Loss: 0.001448 | Perplexity: 1005.049719
2025-09-14 16:49:17,154 Stage: Train 0.5 | Epoch: 9 | Iter: 7200 | Total Loss: 0.009169 | Recon Loss: 0.008468 | Commit Loss: 0.001401 | Perplexity: 1016.320290
2025-09-14 16:49:44,823 Stage: Train 0.5 | Epoch: 9 | Iter: 7400 | Total Loss: 0.008985 | Recon Loss: 0.008293 | Commit Loss: 0.001384 | Perplexity: 1015.363724
Trainning Epoch:   2%|▏         | 10/665 [17:18<18:54:52, 103.96s/it]2025-09-14 16:50:12,359 Stage: Train 0.5 | Epoch: 10 | Iter: 7600 | Total Loss: 0.008669 | Recon Loss: 0.007950 | Commit Loss: 0.001438 | Perplexity: 1016.231196
2025-09-14 16:50:39,822 Stage: Train 0.5 | Epoch: 10 | Iter: 7800 | Total Loss: 0.008886 | Recon Loss: 0.008181 | Commit Loss: 0.001409 | Perplexity: 1022.600497
2025-09-14 16:51:07,275 Stage: Train 0.5 | Epoch: 10 | Iter: 8000 | Total Loss: 0.008603 | Recon Loss: 0.007874 | Commit Loss: 0.001458 | Perplexity: 1019.457239
2025-09-14 16:51:34,805 Stage: Train 0.5 | Epoch: 10 | Iter: 8200 | Total Loss: 0.008970 | Recon Loss: 0.008271 | Commit Loss: 0.001398 | Perplexity: 1014.910745
Trainning Epoch:   2%|▏         | 11/665 [19:02<18:51:23, 103.80s/it]2025-09-14 16:52:02,223 Stage: Train 0.5 | Epoch: 11 | Iter: 8400 | Total Loss: 0.008344 | Recon Loss: 0.007592 | Commit Loss: 0.001504 | Perplexity: 1022.409734
2025-09-14 16:52:29,641 Stage: Train 0.5 | Epoch: 11 | Iter: 8600 | Total Loss: 0.008459 | Recon Loss: 0.007717 | Commit Loss: 0.001484 | Perplexity: 1017.051161
2025-09-14 16:52:57,067 Stage: Train 0.5 | Epoch: 11 | Iter: 8800 | Total Loss: 0.008130 | Recon Loss: 0.007378 | Commit Loss: 0.001503 | Perplexity: 1013.600589
2025-09-14 16:53:24,489 Stage: Train 0.5 | Epoch: 11 | Iter: 9000 | Total Loss: 0.008035 | Recon Loss: 0.007292 | Commit Loss: 0.001486 | Perplexity: 1005.305275
Trainning Epoch:   2%|▏         | 12/665 [20:45<18:47:42, 103.62s/it]2025-09-14 16:53:51,862 Stage: Train 0.5 | Epoch: 12 | Iter: 9200 | Total Loss: 0.008085 | Recon Loss: 0.007370 | Commit Loss: 0.001430 | Perplexity: 1018.182993
2025-09-14 16:54:19,296 Stage: Train 0.5 | Epoch: 12 | Iter: 9400 | Total Loss: 0.007555 | Recon Loss: 0.006812 | Commit Loss: 0.001487 | Perplexity: 1023.627048
2025-09-14 16:54:46,885 Stage: Train 0.5 | Epoch: 12 | Iter: 9600 | Total Loss: 0.007822 | Recon Loss: 0.007101 | Commit Loss: 0.001442 | Perplexity: 1008.223899
Trainning Epoch:   2%|▏         | 13/665 [22:28<18:45:44, 103.60s/it]2025-09-14 16:55:14,427 Stage: Train 0.5 | Epoch: 13 | Iter: 9800 | Total Loss: 0.008300 | Recon Loss: 0.007619 | Commit Loss: 0.001360 | Perplexity: 1003.968991
2025-09-14 16:55:42,034 Stage: Train 0.5 | Epoch: 13 | Iter: 10000 | Total Loss: 0.007315 | Recon Loss: 0.006611 | Commit Loss: 0.001408 | Perplexity: 1021.303047
2025-09-14 16:56:09,553 Stage: Train 0.5 | Epoch: 13 | Iter: 10200 | Total Loss: 0.007568 | Recon Loss: 0.006875 | Commit Loss: 0.001387 | Perplexity: 1020.134604
2025-09-14 16:56:37,061 Stage: Train 0.5 | Epoch: 13 | Iter: 10400 | Total Loss: 0.007563 | Recon Loss: 0.006852 | Commit Loss: 0.001421 | Perplexity: 1021.183019
Trainning Epoch:   2%|▏         | 14/665 [24:12<18:44:07, 103.61s/it]2025-09-14 16:57:04,523 Stage: Train 0.5 | Epoch: 14 | Iter: 10600 | Total Loss: 0.007553 | Recon Loss: 0.006858 | Commit Loss: 0.001390 | Perplexity: 1026.372607
2025-09-14 16:57:32,009 Stage: Train 0.5 | Epoch: 14 | Iter: 10800 | Total Loss: 0.007073 | Recon Loss: 0.006370 | Commit Loss: 0.001407 | Perplexity: 1025.155168
2025-09-14 16:57:59,549 Stage: Train 0.5 | Epoch: 14 | Iter: 11000 | Total Loss: 0.007310 | Recon Loss: 0.006605 | Commit Loss: 0.001410 | Perplexity: 1029.202103
2025-09-14 16:58:27,158 Stage: Train 0.5 | Epoch: 14 | Iter: 11200 | Total Loss: 0.007278 | Recon Loss: 0.006589 | Commit Loss: 0.001378 | Perplexity: 1027.751280
Trainning Epoch:   2%|▏         | 15/665 [25:56<18:42:29, 103.61s/it]2025-09-14 16:58:54,592 Stage: Train 0.5 | Epoch: 15 | Iter: 11400 | Total Loss: 0.007310 | Recon Loss: 0.006631 | Commit Loss: 0.001358 | Perplexity: 1032.740788
2025-09-14 16:59:22,074 Stage: Train 0.5 | Epoch: 15 | Iter: 11600 | Total Loss: 0.006763 | Recon Loss: 0.006066 | Commit Loss: 0.001395 | Perplexity: 1033.370461
2025-09-14 16:59:49,608 Stage: Train 0.5 | Epoch: 15 | Iter: 11800 | Total Loss: 0.007167 | Recon Loss: 0.006506 | Commit Loss: 0.001323 | Perplexity: 1029.135648
2025-09-14 17:00:17,540 Stage: Train 0.5 | Epoch: 15 | Iter: 12000 | Total Loss: 0.006755 | Recon Loss: 0.006062 | Commit Loss: 0.001385 | Perplexity: 1027.898752
Trainning Epoch:   2%|▏         | 16/665 [27:40<18:42:12, 103.75s/it]2025-09-14 17:00:45,491 Stage: Train 0.5 | Epoch: 16 | Iter: 12200 | Total Loss: 0.006593 | Recon Loss: 0.005889 | Commit Loss: 0.001409 | Perplexity: 1033.444807
2025-09-14 17:01:13,466 Stage: Train 0.5 | Epoch: 16 | Iter: 12400 | Total Loss: 0.006718 | Recon Loss: 0.006021 | Commit Loss: 0.001394 | Perplexity: 1037.435903
2025-09-14 17:01:41,540 Stage: Train 0.5 | Epoch: 16 | Iter: 12600 | Total Loss: 0.006475 | Recon Loss: 0.005782 | Commit Loss: 0.001386 | Perplexity: 1036.561052
2025-09-14 17:02:08,952 Stage: Train 0.5 | Epoch: 16 | Iter: 12800 | Total Loss: 0.006652 | Recon Loss: 0.005977 | Commit Loss: 0.001350 | Perplexity: 1030.638637
Trainning Epoch:   3%|▎         | 17/665 [29:25<18:43:53, 104.06s/it]2025-09-14 17:02:36,302 Stage: Train 0.5 | Epoch: 17 | Iter: 13000 | Total Loss: 0.006728 | Recon Loss: 0.006067 | Commit Loss: 0.001321 | Perplexity: 1033.719149
2025-09-14 17:03:03,692 Stage: Train 0.5 | Epoch: 17 | Iter: 13200 | Total Loss: 0.006564 | Recon Loss: 0.005881 | Commit Loss: 0.001364 | Perplexity: 1039.667653
2025-09-14 17:03:31,077 Stage: Train 0.5 | Epoch: 17 | Iter: 13400 | Total Loss: 0.006621 | Recon Loss: 0.005967 | Commit Loss: 0.001309 | Perplexity: 1039.723647
Trainning Epoch:   3%|▎         | 18/665 [31:08<18:38:45, 103.75s/it]2025-09-14 17:03:58,340 Stage: Train 0.5 | Epoch: 18 | Iter: 13600 | Total Loss: 0.006344 | Recon Loss: 0.005656 | Commit Loss: 0.001375 | Perplexity: 1039.706066
2025-09-14 17:04:25,660 Stage: Train 0.5 | Epoch: 18 | Iter: 13800 | Total Loss: 0.006420 | Recon Loss: 0.005762 | Commit Loss: 0.001315 | Perplexity: 1036.793674
2025-09-14 17:04:53,440 Stage: Train 0.5 | Epoch: 18 | Iter: 14000 | Total Loss: 0.006438 | Recon Loss: 0.005761 | Commit Loss: 0.001353 | Perplexity: 1043.105684
2025-09-14 17:05:21,145 Stage: Train 0.5 | Epoch: 18 | Iter: 14200 | Total Loss: 0.006517 | Recon Loss: 0.005862 | Commit Loss: 0.001311 | Perplexity: 1038.148767
Trainning Epoch:   3%|▎         | 19/665 [32:51<18:37:02, 103.75s/it]2025-09-14 17:05:48,623 Stage: Train 0.5 | Epoch: 19 | Iter: 14400 | Total Loss: 0.006245 | Recon Loss: 0.005578 | Commit Loss: 0.001335 | Perplexity: 1041.353799
2025-09-14 17:06:16,166 Stage: Train 0.5 | Epoch: 19 | Iter: 14600 | Total Loss: 0.006080 | Recon Loss: 0.005406 | Commit Loss: 0.001348 | Perplexity: 1041.066472
2025-09-14 17:06:43,702 Stage: Train 0.5 | Epoch: 19 | Iter: 14800 | Total Loss: 0.006346 | Recon Loss: 0.005681 | Commit Loss: 0.001331 | Perplexity: 1037.971549
2025-09-14 17:07:11,433 Stage: Train 0.5 | Epoch: 19 | Iter: 15000 | Total Loss: 0.006195 | Recon Loss: 0.005528 | Commit Loss: 0.001335 | Perplexity: 1038.932243
Trainning Epoch:   3%|▎         | 20/665 [34:35<18:35:44, 103.79s/it]2025-09-14 17:07:38,986 Stage: Train 0.5 | Epoch: 20 | Iter: 15200 | Total Loss: 0.006141 | Recon Loss: 0.005481 | Commit Loss: 0.001322 | Perplexity: 1035.307405
2025-09-14 17:08:06,432 Stage: Train 0.5 | Epoch: 20 | Iter: 15400 | Total Loss: 0.006192 | Recon Loss: 0.005535 | Commit Loss: 0.001314 | Perplexity: 1042.519548
2025-09-14 17:08:33,992 Stage: Train 0.5 | Epoch: 20 | Iter: 15600 | Total Loss: 0.005984 | Recon Loss: 0.005330 | Commit Loss: 0.001309 | Perplexity: 1039.126339
2025-09-14 17:09:01,527 Stage: Train 0.5 | Epoch: 20 | Iter: 15800 | Total Loss: 0.005974 | Recon Loss: 0.005305 | Commit Loss: 0.001338 | Perplexity: 1043.105581
Trainning Epoch:   3%|▎         | 21/665 [36:19<18:33:20, 103.73s/it]2025-09-14 17:09:29,021 Stage: Train 0.5 | Epoch: 21 | Iter: 16000 | Total Loss: 0.006312 | Recon Loss: 0.005672 | Commit Loss: 0.001279 | Perplexity: 1038.293942
2025-09-14 17:09:56,592 Stage: Train 0.5 | Epoch: 21 | Iter: 16200 | Total Loss: 0.005969 | Recon Loss: 0.005337 | Commit Loss: 0.001264 | Perplexity: 1037.757252
2025-09-14 17:10:24,140 Stage: Train 0.5 | Epoch: 21 | Iter: 16400 | Total Loss: 0.005858 | Recon Loss: 0.005199 | Commit Loss: 0.001317 | Perplexity: 1041.855183
Trainning Epoch:   3%|▎         | 22/665 [38:03<18:31:33, 103.72s/it]2025-09-14 17:10:51,660 Stage: Train 0.5 | Epoch: 22 | Iter: 16600 | Total Loss: 0.005781 | Recon Loss: 0.005126 | Commit Loss: 0.001310 | Perplexity: 1042.685979
2025-09-14 17:11:19,177 Stage: Train 0.5 | Epoch: 22 | Iter: 16800 | Total Loss: 0.005861 | Recon Loss: 0.005213 | Commit Loss: 0.001295 | Perplexity: 1037.693502
2025-09-14 17:11:46,822 Stage: Train 0.5 | Epoch: 22 | Iter: 17000 | Total Loss: 0.005848 | Recon Loss: 0.005199 | Commit Loss: 0.001299 | Perplexity: 1040.362039
2025-09-14 17:12:14,246 Stage: Train 0.5 | Epoch: 22 | Iter: 17200 | Total Loss: 0.005731 | Recon Loss: 0.005079 | Commit Loss: 0.001304 | Perplexity: 1037.551054
Trainning Epoch:   3%|▎         | 23/665 [39:46<18:29:12, 103.66s/it]2025-09-14 17:12:41,616 Stage: Train 0.5 | Epoch: 23 | Iter: 17400 | Total Loss: 0.005865 | Recon Loss: 0.005237 | Commit Loss: 0.001256 | Perplexity: 1035.747001
2025-09-14 17:13:09,043 Stage: Train 0.5 | Epoch: 23 | Iter: 17600 | Total Loss: 0.005706 | Recon Loss: 0.005064 | Commit Loss: 0.001284 | Perplexity: 1037.659666
2025-09-14 17:13:36,463 Stage: Train 0.5 | Epoch: 23 | Iter: 17800 | Total Loss: 0.005813 | Recon Loss: 0.005182 | Commit Loss: 0.001263 | Perplexity: 1032.789654
2025-09-14 17:14:04,034 Stage: Train 0.5 | Epoch: 23 | Iter: 18000 | Total Loss: 0.005895 | Recon Loss: 0.005270 | Commit Loss: 0.001250 | Perplexity: 1039.370021
Trainning Epoch:   4%|▎         | 24/665 [41:29<18:26:33, 103.58s/it]2025-09-14 17:14:31,483 Stage: Train 0.5 | Epoch: 24 | Iter: 18200 | Total Loss: 0.005440 | Recon Loss: 0.004803 | Commit Loss: 0.001272 | Perplexity: 1039.091352
2025-09-14 17:14:59,055 Stage: Train 0.5 | Epoch: 24 | Iter: 18400 | Total Loss: 0.005949 | Recon Loss: 0.005338 | Commit Loss: 0.001223 | Perplexity: 1033.108716
2025-09-14 17:15:26,732 Stage: Train 0.5 | Epoch: 24 | Iter: 18600 | Total Loss: 0.005448 | Recon Loss: 0.004816 | Commit Loss: 0.001264 | Perplexity: 1042.787125
2025-09-14 17:15:54,235 Stage: Train 0.5 | Epoch: 24 | Iter: 18800 | Total Loss: 0.005680 | Recon Loss: 0.005070 | Commit Loss: 0.001219 | Perplexity: 1039.253770
Trainning Epoch:   4%|▍         | 25/665 [43:13<18:25:18, 103.62s/it]2025-09-14 17:16:21,601 Stage: Train 0.5 | Epoch: 25 | Iter: 19000 | Total Loss: 0.005519 | Recon Loss: 0.004901 | Commit Loss: 0.001236 | Perplexity: 1042.089594
2025-09-14 17:16:49,028 Stage: Train 0.5 | Epoch: 25 | Iter: 19200 | Total Loss: 0.005369 | Recon Loss: 0.004747 | Commit Loss: 0.001245 | Perplexity: 1046.471430
2025-09-14 17:17:16,463 Stage: Train 0.5 | Epoch: 25 | Iter: 19400 | Total Loss: 0.005487 | Recon Loss: 0.004878 | Commit Loss: 0.001218 | Perplexity: 1035.494033
Trainning Epoch:   4%|▍         | 26/665 [44:56<18:22:14, 103.50s/it]2025-09-14 17:17:43,833 Stage: Train 0.5 | Epoch: 26 | Iter: 19600 | Total Loss: 0.005419 | Recon Loss: 0.004803 | Commit Loss: 0.001232 | Perplexity: 1036.178533
2025-09-14 17:18:11,252 Stage: Train 0.5 | Epoch: 26 | Iter: 19800 | Total Loss: 0.005153 | Recon Loss: 0.004536 | Commit Loss: 0.001236 | Perplexity: 1038.823232
2025-09-14 17:18:38,906 Stage: Train 0.5 | Epoch: 26 | Iter: 20000 | Total Loss: 0.005437 | Recon Loss: 0.004831 | Commit Loss: 0.001212 | Perplexity: 1039.333333
2025-09-14 17:18:38,907 Saving model at iteration 20000
2025-09-14 17:18:39,048 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_27_step_20000
2025-09-14 17:18:39,257 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_27_step_20000/pytorch_model.bin
2025-09-14 17:18:39,583 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_27_step_20000/optimizer.bin
2025-09-14 17:18:39,584 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_27_step_20000/scheduler.bin
2025-09-14 17:18:39,584 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_27_step_20000/random_states_0.pkl
2025-09-14 17:19:07,459 Stage: Train 0.5 | Epoch: 26 | Iter: 20200 | Total Loss: 0.005446 | Recon Loss: 0.004856 | Commit Loss: 0.001180 | Perplexity: 1036.455606
Trainning Epoch:   4%|▍         | 27/665 [46:41<18:25:15, 103.94s/it]2025-09-14 17:19:35,304 Stage: Train 0.5 | Epoch: 27 | Iter: 20400 | Total Loss: 0.005269 | Recon Loss: 0.004659 | Commit Loss: 0.001220 | Perplexity: 1039.403937
2025-09-14 17:20:02,863 Stage: Train 0.5 | Epoch: 27 | Iter: 20600 | Total Loss: 0.005228 | Recon Loss: 0.004645 | Commit Loss: 0.001165 | Perplexity: 1037.927525
2025-09-14 17:20:30,389 Stage: Train 0.5 | Epoch: 27 | Iter: 20800 | Total Loss: 0.005331 | Recon Loss: 0.004740 | Commit Loss: 0.001181 | Perplexity: 1045.994208
2025-09-14 17:20:57,903 Stage: Train 0.5 | Epoch: 27 | Iter: 21000 | Total Loss: 0.005388 | Recon Loss: 0.004811 | Commit Loss: 0.001154 | Perplexity: 1041.631759
Trainning Epoch:   4%|▍         | 28/665 [48:25<18:22:26, 103.84s/it]2025-09-14 17:21:25,396 Stage: Train 0.5 | Epoch: 28 | Iter: 21200 | Total Loss: 0.005150 | Recon Loss: 0.004570 | Commit Loss: 0.001159 | Perplexity: 1047.031030
2025-09-14 17:21:52,853 Stage: Train 0.5 | Epoch: 28 | Iter: 21400 | Total Loss: 0.005165 | Recon Loss: 0.004584 | Commit Loss: 0.001162 | Perplexity: 1047.468275
2025-09-14 17:22:20,415 Stage: Train 0.5 | Epoch: 28 | Iter: 21600 | Total Loss: 0.005083 | Recon Loss: 0.004508 | Commit Loss: 0.001151 | Perplexity: 1039.952498
2025-09-14 17:22:47,884 Stage: Train 0.5 | Epoch: 28 | Iter: 21800 | Total Loss: 0.005527 | Recon Loss: 0.004972 | Commit Loss: 0.001110 | Perplexity: 1042.702767
Trainning Epoch:   4%|▍         | 29/665 [50:08<18:19:39, 103.74s/it]2025-09-14 17:23:15,299 Stage: Train 0.5 | Epoch: 29 | Iter: 22000 | Total Loss: 0.005222 | Recon Loss: 0.004663 | Commit Loss: 0.001118 | Perplexity: 1045.361290
2025-09-14 17:23:43,060 Stage: Train 0.5 | Epoch: 29 | Iter: 22200 | Total Loss: 0.004886 | Recon Loss: 0.004329 | Commit Loss: 0.001115 | Perplexity: 1050.001344
2025-09-14 17:24:10,700 Stage: Train 0.5 | Epoch: 29 | Iter: 22400 | Total Loss: 0.005228 | Recon Loss: 0.004679 | Commit Loss: 0.001099 | Perplexity: 1043.102114
Trainning Epoch:   5%|▍         | 30/665 [51:52<18:18:13, 103.77s/it]2025-09-14 17:24:38,121 Stage: Train 0.5 | Epoch: 30 | Iter: 22600 | Total Loss: 0.004972 | Recon Loss: 0.004422 | Commit Loss: 0.001100 | Perplexity: 1045.018331
2025-09-14 17:25:05,626 Stage: Train 0.5 | Epoch: 30 | Iter: 22800 | Total Loss: 0.005191 | Recon Loss: 0.004643 | Commit Loss: 0.001097 | Perplexity: 1041.434232
2025-09-14 17:25:33,322 Stage: Train 0.5 | Epoch: 30 | Iter: 23000 | Total Loss: 0.004915 | Recon Loss: 0.004363 | Commit Loss: 0.001104 | Perplexity: 1048.871886
2025-09-14 17:26:00,949 Stage: Train 0.5 | Epoch: 30 | Iter: 23200 | Total Loss: 0.005036 | Recon Loss: 0.004488 | Commit Loss: 0.001097 | Perplexity: 1048.185287
Trainning Epoch:   5%|▍         | 31/665 [53:36<18:17:07, 103.83s/it]2025-09-14 17:26:28,587 Stage: Train 0.5 | Epoch: 31 | Iter: 23400 | Total Loss: 0.005203 | Recon Loss: 0.004663 | Commit Loss: 0.001080 | Perplexity: 1038.390655
2025-09-14 17:26:56,052 Stage: Train 0.5 | Epoch: 31 | Iter: 23600 | Total Loss: 0.004818 | Recon Loss: 0.004274 | Commit Loss: 0.001089 | Perplexity: 1044.335200
2025-09-14 17:27:23,486 Stage: Train 0.5 | Epoch: 31 | Iter: 23800 | Total Loss: 0.005031 | Recon Loss: 0.004493 | Commit Loss: 0.001076 | Perplexity: 1042.121622
2025-09-14 17:27:51,139 Stage: Train 0.5 | Epoch: 31 | Iter: 24000 | Total Loss: 0.005082 | Recon Loss: 0.004544 | Commit Loss: 0.001075 | Perplexity: 1038.052415
Trainning Epoch:   5%|▍         | 32/665 [55:20<18:14:42, 103.76s/it]2025-09-14 17:28:18,707 Stage: Train 0.5 | Epoch: 32 | Iter: 24200 | Total Loss: 0.004849 | Recon Loss: 0.004300 | Commit Loss: 0.001098 | Perplexity: 1046.961892
2025-09-14 17:28:46,182 Stage: Train 0.5 | Epoch: 32 | Iter: 24400 | Total Loss: 0.004898 | Recon Loss: 0.004351 | Commit Loss: 0.001095 | Perplexity: 1041.985146
2025-09-14 17:29:13,931 Stage: Train 0.5 | Epoch: 32 | Iter: 24600 | Total Loss: 0.004819 | Recon Loss: 0.004287 | Commit Loss: 0.001064 | Perplexity: 1038.105298
2025-09-14 17:29:41,598 Stage: Train 0.5 | Epoch: 32 | Iter: 24800 | Total Loss: 0.004885 | Recon Loss: 0.004353 | Commit Loss: 0.001064 | Perplexity: 1039.860021
Trainning Epoch:   5%|▍         | 33/665 [57:04<18:13:38, 103.83s/it]2025-09-14 17:30:09,118 Stage: Train 0.5 | Epoch: 33 | Iter: 25000 | Total Loss: 0.004951 | Recon Loss: 0.004411 | Commit Loss: 0.001080 | Perplexity: 1037.662704
2025-09-14 17:30:36,612 Stage: Train 0.5 | Epoch: 33 | Iter: 25200 | Total Loss: 0.004800 | Recon Loss: 0.004268 | Commit Loss: 0.001064 | Perplexity: 1043.888352
2025-09-14 17:31:04,100 Stage: Train 0.5 | Epoch: 33 | Iter: 25400 | Total Loss: 0.004929 | Recon Loss: 0.004399 | Commit Loss: 0.001059 | Perplexity: 1039.914024
2025-09-14 17:31:31,584 Stage: Train 0.5 | Epoch: 33 | Iter: 25600 | Total Loss: 0.004787 | Recon Loss: 0.004262 | Commit Loss: 0.001049 | Perplexity: 1042.221468
Trainning Epoch:   5%|▌         | 34/665 [58:47<18:10:54, 103.73s/it]2025-09-14 17:31:58,992 Stage: Train 0.5 | Epoch: 34 | Iter: 25800 | Total Loss: 0.004843 | Recon Loss: 0.004317 | Commit Loss: 0.001051 | Perplexity: 1037.883793
2025-09-14 17:32:26,590 Stage: Train 0.5 | Epoch: 34 | Iter: 26000 | Total Loss: 0.004682 | Recon Loss: 0.004150 | Commit Loss: 0.001063 | Perplexity: 1044.671819
2025-09-14 17:32:54,030 Stage: Train 0.5 | Epoch: 34 | Iter: 26200 | Total Loss: 0.004932 | Recon Loss: 0.004407 | Commit Loss: 0.001051 | Perplexity: 1035.309457
Trainning Epoch:   5%|▌         | 35/665 [1:00:31<18:08:22, 103.65s/it]2025-09-14 17:33:21,459 Stage: Train 0.5 | Epoch: 35 | Iter: 26400 | Total Loss: 0.004827 | Recon Loss: 0.004293 | Commit Loss: 0.001068 | Perplexity: 1038.441914
2025-09-14 17:33:49,216 Stage: Train 0.5 | Epoch: 35 | Iter: 26600 | Total Loss: 0.004861 | Recon Loss: 0.004342 | Commit Loss: 0.001038 | Perplexity: 1038.602019
2025-09-14 17:34:17,212 Stage: Train 0.5 | Epoch: 35 | Iter: 26800 | Total Loss: 0.004675 | Recon Loss: 0.004139 | Commit Loss: 0.001073 | Perplexity: 1042.005862
2025-09-14 17:34:44,911 Stage: Train 0.5 | Epoch: 35 | Iter: 27000 | Total Loss: 0.004658 | Recon Loss: 0.004132 | Commit Loss: 0.001052 | Perplexity: 1042.069339
Trainning Epoch:   5%|▌         | 36/665 [1:02:15<18:08:53, 103.87s/it]2025-09-14 17:35:12,362 Stage: Train 0.5 | Epoch: 36 | Iter: 27200 | Total Loss: 0.004634 | Recon Loss: 0.004104 | Commit Loss: 0.001059 | Perplexity: 1039.049183
2025-09-14 17:35:39,988 Stage: Train 0.5 | Epoch: 36 | Iter: 27400 | Total Loss: 0.004757 | Recon Loss: 0.004227 | Commit Loss: 0.001060 | Perplexity: 1040.858830
2025-09-14 17:36:07,601 Stage: Train 0.5 | Epoch: 36 | Iter: 27600 | Total Loss: 0.004818 | Recon Loss: 0.004288 | Commit Loss: 0.001060 | Perplexity: 1042.597828
2025-09-14 17:36:35,039 Stage: Train 0.5 | Epoch: 36 | Iter: 27800 | Total Loss: 0.004799 | Recon Loss: 0.004278 | Commit Loss: 0.001042 | Perplexity: 1037.901573
Trainning Epoch:   6%|▌         | 37/665 [1:03:59<18:06:38, 103.82s/it]2025-09-14 17:37:02,441 Stage: Train 0.5 | Epoch: 37 | Iter: 28000 | Total Loss: 0.004605 | Recon Loss: 0.004098 | Commit Loss: 0.001014 | Perplexity: 1040.076666
2025-09-14 17:37:30,003 Stage: Train 0.5 | Epoch: 37 | Iter: 28200 | Total Loss: 0.004618 | Recon Loss: 0.004088 | Commit Loss: 0.001062 | Perplexity: 1047.777666
2025-09-14 17:37:57,470 Stage: Train 0.5 | Epoch: 37 | Iter: 28400 | Total Loss: 0.004528 | Recon Loss: 0.003998 | Commit Loss: 0.001060 | Perplexity: 1046.509718
2025-09-14 17:38:24,990 Stage: Train 0.5 | Epoch: 37 | Iter: 28600 | Total Loss: 0.004692 | Recon Loss: 0.004175 | Commit Loss: 0.001033 | Perplexity: 1039.320020
Trainning Epoch:   6%|▌         | 38/665 [1:05:42<18:03:55, 103.72s/it]2025-09-14 17:38:52,372 Stage: Train 0.5 | Epoch: 38 | Iter: 28800 | Total Loss: 0.004659 | Recon Loss: 0.004140 | Commit Loss: 0.001038 | Perplexity: 1042.687214
2025-09-14 17:39:19,875 Stage: Train 0.5 | Epoch: 38 | Iter: 29000 | Total Loss: 0.004490 | Recon Loss: 0.003965 | Commit Loss: 0.001049 | Perplexity: 1044.851095
2025-09-14 17:39:47,395 Stage: Train 0.5 | Epoch: 38 | Iter: 29200 | Total Loss: 0.004657 | Recon Loss: 0.004139 | Commit Loss: 0.001036 | Perplexity: 1042.401126
Trainning Epoch:   6%|▌         | 39/665 [1:07:26<18:01:03, 103.62s/it]2025-09-14 17:40:14,741 Stage: Train 0.5 | Epoch: 39 | Iter: 29400 | Total Loss: 0.004502 | Recon Loss: 0.003976 | Commit Loss: 0.001052 | Perplexity: 1043.481628
2025-09-14 17:40:42,149 Stage: Train 0.5 | Epoch: 39 | Iter: 29600 | Total Loss: 0.004617 | Recon Loss: 0.004102 | Commit Loss: 0.001030 | Perplexity: 1044.515129
2025-09-14 17:41:09,554 Stage: Train 0.5 | Epoch: 39 | Iter: 29800 | Total Loss: 0.004437 | Recon Loss: 0.003906 | Commit Loss: 0.001060 | Perplexity: 1048.033957
2025-09-14 17:41:36,991 Stage: Train 0.5 | Epoch: 39 | Iter: 30000 | Total Loss: 0.004423 | Recon Loss: 0.003910 | Commit Loss: 0.001026 | Perplexity: 1045.087063
Trainning Epoch:   6%|▌         | 40/665 [1:09:09<17:58:04, 103.49s/it]2025-09-14 17:42:04,417 Stage: Train 0.5 | Epoch: 40 | Iter: 30200 | Total Loss: 0.004585 | Recon Loss: 0.004062 | Commit Loss: 0.001047 | Perplexity: 1048.799631
2025-09-14 17:42:31,880 Stage: Train 0.5 | Epoch: 40 | Iter: 30400 | Total Loss: 0.004617 | Recon Loss: 0.004106 | Commit Loss: 0.001023 | Perplexity: 1043.277326
2025-09-14 17:42:59,438 Stage: Train 0.5 | Epoch: 40 | Iter: 30600 | Total Loss: 0.004322 | Recon Loss: 0.003796 | Commit Loss: 0.001051 | Perplexity: 1049.023586
2025-09-14 17:43:26,898 Stage: Train 0.5 | Epoch: 40 | Iter: 30800 | Total Loss: 0.004459 | Recon Loss: 0.003936 | Commit Loss: 0.001045 | Perplexity: 1048.055712
Trainning Epoch:   6%|▌         | 41/665 [1:10:52<17:56:10, 103.48s/it]2025-09-14 17:43:54,308 Stage: Train 0.5 | Epoch: 41 | Iter: 31000 | Total Loss: 0.004492 | Recon Loss: 0.003973 | Commit Loss: 0.001039 | Perplexity: 1045.418387
2025-09-14 17:44:21,844 Stage: Train 0.5 | Epoch: 41 | Iter: 31200 | Total Loss: 0.004523 | Recon Loss: 0.004001 | Commit Loss: 0.001044 | Perplexity: 1042.767995
2025-09-14 17:44:49,387 Stage: Train 0.5 | Epoch: 41 | Iter: 31400 | Total Loss: 0.004320 | Recon Loss: 0.003799 | Commit Loss: 0.001041 | Perplexity: 1049.008701
2025-09-14 17:45:17,001 Stage: Train 0.5 | Epoch: 41 | Iter: 31600 | Total Loss: 0.004429 | Recon Loss: 0.003897 | Commit Loss: 0.001063 | Perplexity: 1051.420291
Trainning Epoch:   6%|▋         | 42/665 [1:12:36<17:55:05, 103.54s/it]2025-09-14 17:45:44,437 Stage: Train 0.5 | Epoch: 42 | Iter: 31800 | Total Loss: 0.004419 | Recon Loss: 0.003904 | Commit Loss: 0.001031 | Perplexity: 1045.673930
2025-09-14 17:46:11,978 Stage: Train 0.5 | Epoch: 42 | Iter: 32000 | Total Loss: 0.004493 | Recon Loss: 0.003970 | Commit Loss: 0.001048 | Perplexity: 1050.196937
2025-09-14 17:46:39,430 Stage: Train 0.5 | Epoch: 42 | Iter: 32200 | Total Loss: 0.004359 | Recon Loss: 0.003841 | Commit Loss: 0.001036 | Perplexity: 1049.271718
Trainning Epoch:   6%|▋         | 43/665 [1:14:19<17:52:52, 103.49s/it]2025-09-14 17:47:06,833 Stage: Train 0.5 | Epoch: 43 | Iter: 32400 | Total Loss: 0.004389 | Recon Loss: 0.003863 | Commit Loss: 0.001051 | Perplexity: 1051.393513
2025-09-14 17:47:34,259 Stage: Train 0.5 | Epoch: 43 | Iter: 32600 | Total Loss: 0.004425 | Recon Loss: 0.003916 | Commit Loss: 0.001018 | Perplexity: 1051.290255
2025-09-14 17:48:01,668 Stage: Train 0.5 | Epoch: 43 | Iter: 32800 | Total Loss: 0.004535 | Recon Loss: 0.004013 | Commit Loss: 0.001045 | Perplexity: 1051.168911
2025-09-14 17:48:29,090 Stage: Train 0.5 | Epoch: 43 | Iter: 33000 | Total Loss: 0.004408 | Recon Loss: 0.003893 | Commit Loss: 0.001030 | Perplexity: 1053.665206
Trainning Epoch:   7%|▋         | 44/665 [1:16:03<17:50:14, 103.40s/it]2025-09-14 17:48:56,473 Stage: Train 0.5 | Epoch: 44 | Iter: 33200 | Total Loss: 0.004472 | Recon Loss: 0.003960 | Commit Loss: 0.001024 | Perplexity: 1051.439030
2025-09-14 17:49:24,023 Stage: Train 0.5 | Epoch: 44 | Iter: 33400 | Total Loss: 0.004323 | Recon Loss: 0.003801 | Commit Loss: 0.001044 | Perplexity: 1054.603609
2025-09-14 17:49:51,404 Stage: Train 0.5 | Epoch: 44 | Iter: 33600 | Total Loss: 0.004499 | Recon Loss: 0.003986 | Commit Loss: 0.001026 | Perplexity: 1051.036991
2025-09-14 17:50:18,818 Stage: Train 0.5 | Epoch: 44 | Iter: 33800 | Total Loss: 0.004275 | Recon Loss: 0.003764 | Commit Loss: 0.001022 | Perplexity: 1054.087010
Trainning Epoch:   7%|▋         | 45/665 [1:17:46<17:48:06, 103.37s/it]2025-09-14 17:50:46,195 Stage: Train 0.5 | Epoch: 45 | Iter: 34000 | Total Loss: 0.004198 | Recon Loss: 0.003673 | Commit Loss: 0.001049 | Perplexity: 1056.795098
2025-09-14 17:51:13,713 Stage: Train 0.5 | Epoch: 45 | Iter: 34200 | Total Loss: 0.004271 | Recon Loss: 0.003751 | Commit Loss: 0.001038 | Perplexity: 1055.062159
2025-09-14 17:51:41,228 Stage: Train 0.5 | Epoch: 45 | Iter: 34400 | Total Loss: 0.004448 | Recon Loss: 0.003934 | Commit Loss: 0.001029 | Perplexity: 1054.472932
2025-09-14 17:52:08,921 Stage: Train 0.5 | Epoch: 45 | Iter: 34600 | Total Loss: 0.004325 | Recon Loss: 0.003808 | Commit Loss: 0.001035 | Perplexity: 1053.709877
Trainning Epoch:   7%|▋         | 46/665 [1:19:30<17:47:26, 103.47s/it]2025-09-14 17:52:36,375 Stage: Train 0.5 | Epoch: 46 | Iter: 34800 | Total Loss: 0.004300 | Recon Loss: 0.003781 | Commit Loss: 0.001037 | Perplexity: 1053.596964
2025-09-14 17:53:03,920 Stage: Train 0.5 | Epoch: 46 | Iter: 35000 | Total Loss: 0.004244 | Recon Loss: 0.003720 | Commit Loss: 0.001049 | Perplexity: 1062.804341
2025-09-14 17:53:31,353 Stage: Train 0.5 | Epoch: 46 | Iter: 35200 | Total Loss: 0.004162 | Recon Loss: 0.003649 | Commit Loss: 0.001026 | Perplexity: 1056.940396
Trainning Epoch:   7%|▋         | 47/665 [1:21:13<17:45:23, 103.44s/it]2025-09-14 17:53:58,720 Stage: Train 0.5 | Epoch: 47 | Iter: 35400 | Total Loss: 0.004432 | Recon Loss: 0.003919 | Commit Loss: 0.001026 | Perplexity: 1054.265485
2025-09-14 17:54:26,155 Stage: Train 0.5 | Epoch: 47 | Iter: 35600 | Total Loss: 0.004156 | Recon Loss: 0.003639 | Commit Loss: 0.001034 | Perplexity: 1059.683778
2025-09-14 17:54:53,569 Stage: Train 0.5 | Epoch: 47 | Iter: 35800 | Total Loss: 0.004225 | Recon Loss: 0.003701 | Commit Loss: 0.001048 | Perplexity: 1062.586268
2025-09-14 17:55:21,075 Stage: Train 0.5 | Epoch: 47 | Iter: 36000 | Total Loss: 0.004312 | Recon Loss: 0.003801 | Commit Loss: 0.001023 | Perplexity: 1058.298219
Trainning Epoch:   7%|▋         | 48/665 [1:22:56<17:43:14, 103.40s/it]2025-09-14 17:55:48,460 Stage: Train 0.5 | Epoch: 48 | Iter: 36200 | Total Loss: 0.004396 | Recon Loss: 0.003888 | Commit Loss: 0.001016 | Perplexity: 1053.322592
2025-09-14 17:56:15,989 Stage: Train 0.5 | Epoch: 48 | Iter: 36400 | Total Loss: 0.004093 | Recon Loss: 0.003588 | Commit Loss: 0.001011 | Perplexity: 1059.566822
2025-09-14 17:56:43,440 Stage: Train 0.5 | Epoch: 48 | Iter: 36600 | Total Loss: 0.004350 | Recon Loss: 0.003846 | Commit Loss: 0.001007 | Perplexity: 1058.065748
2025-09-14 17:57:10,858 Stage: Train 0.5 | Epoch: 48 | Iter: 36800 | Total Loss: 0.004233 | Recon Loss: 0.003715 | Commit Loss: 0.001036 | Perplexity: 1057.174658
Trainning Epoch:   7%|▋         | 49/665 [1:24:40<17:41:19, 103.38s/it]2025-09-14 17:57:38,228 Stage: Train 0.5 | Epoch: 49 | Iter: 37000 | Total Loss: 0.004192 | Recon Loss: 0.003682 | Commit Loss: 0.001021 | Perplexity: 1062.286291
2025-09-14 17:58:05,678 Stage: Train 0.5 | Epoch: 49 | Iter: 37200 | Total Loss: 0.004201 | Recon Loss: 0.003681 | Commit Loss: 0.001039 | Perplexity: 1061.920126
2025-09-14 17:58:33,091 Stage: Train 0.5 | Epoch: 49 | Iter: 37400 | Total Loss: 0.004115 | Recon Loss: 0.003602 | Commit Loss: 0.001028 | Perplexity: 1060.706077
2025-09-14 17:59:00,499 Stage: Train 0.5 | Epoch: 49 | Iter: 37600 | Total Loss: 0.004280 | Recon Loss: 0.003766 | Commit Loss: 0.001028 | Perplexity: 1056.691202
Trainning Epoch:   8%|▊         | 50/665 [1:26:23<17:39:01, 103.32s/it]2025-09-14 17:59:27,832 Stage: Train 0.5 | Epoch: 50 | Iter: 37800 | Total Loss: 0.004219 | Recon Loss: 0.003709 | Commit Loss: 0.001019 | Perplexity: 1054.071130
2025-09-14 17:59:55,308 Stage: Train 0.5 | Epoch: 50 | Iter: 38000 | Total Loss: 0.004241 | Recon Loss: 0.003742 | Commit Loss: 0.000998 | Perplexity: 1059.770271
2025-09-14 18:00:22,700 Stage: Train 0.5 | Epoch: 50 | Iter: 38200 | Total Loss: 0.004108 | Recon Loss: 0.003597 | Commit Loss: 0.001023 | Perplexity: 1061.608318
2025-09-14 18:00:50,077 Stage: Train 0.5 | Epoch: 50 | Iter: 38400 | Total Loss: 0.004111 | Recon Loss: 0.003593 | Commit Loss: 0.001035 | Perplexity: 1060.600255
Trainning Epoch:   8%|▊         | 51/665 [1:28:06<17:36:44, 103.26s/it]2025-09-14 18:01:17,443 Stage: Train 0.5 | Epoch: 51 | Iter: 38600 | Total Loss: 0.004210 | Recon Loss: 0.003696 | Commit Loss: 0.001028 | Perplexity: 1055.715099
2025-09-14 18:01:44,761 Stage: Train 0.5 | Epoch: 51 | Iter: 38800 | Total Loss: 0.004180 | Recon Loss: 0.003672 | Commit Loss: 0.001015 | Perplexity: 1061.713605
2025-09-14 18:02:12,114 Stage: Train 0.5 | Epoch: 51 | Iter: 39000 | Total Loss: 0.004163 | Recon Loss: 0.003650 | Commit Loss: 0.001026 | Perplexity: 1055.638382
Trainning Epoch:   8%|▊         | 52/665 [1:29:49<17:34:08, 103.18s/it]2025-09-14 18:02:39,442 Stage: Train 0.5 | Epoch: 52 | Iter: 39200 | Total Loss: 0.004072 | Recon Loss: 0.003562 | Commit Loss: 0.001019 | Perplexity: 1058.128814
2025-09-14 18:03:06,915 Stage: Train 0.5 | Epoch: 52 | Iter: 39400 | Total Loss: 0.004096 | Recon Loss: 0.003593 | Commit Loss: 0.001007 | Perplexity: 1055.032182
2025-09-14 18:03:34,251 Stage: Train 0.5 | Epoch: 52 | Iter: 39600 | Total Loss: 0.004077 | Recon Loss: 0.003552 | Commit Loss: 0.001049 | Perplexity: 1068.363453
2025-09-14 18:04:01,612 Stage: Train 0.5 | Epoch: 52 | Iter: 39800 | Total Loss: 0.004021 | Recon Loss: 0.003506 | Commit Loss: 0.001029 | Perplexity: 1062.400459
Trainning Epoch:   8%|▊         | 53/665 [1:31:32<17:32:09, 103.15s/it]2025-09-14 18:04:28,983 Stage: Train 0.5 | Epoch: 53 | Iter: 40000 | Total Loss: 0.004163 | Recon Loss: 0.003654 | Commit Loss: 0.001019 | Perplexity: 1055.925484
2025-09-14 18:04:28,983 Saving model at iteration 40000
2025-09-14 18:04:29,154 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_54_step_40000
2025-09-14 18:04:29,369 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_54_step_40000/pytorch_model.bin
2025-09-14 18:04:29,712 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_54_step_40000/optimizer.bin
2025-09-14 18:04:29,712 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_54_step_40000/scheduler.bin
2025-09-14 18:04:29,713 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_54_step_40000/random_states_0.pkl
2025-09-14 18:04:57,375 Stage: Train 0.5 | Epoch: 53 | Iter: 40200 | Total Loss: 0.004262 | Recon Loss: 0.003758 | Commit Loss: 0.001008 | Perplexity: 1053.363692
2025-09-14 18:05:24,790 Stage: Train 0.5 | Epoch: 53 | Iter: 40400 | Total Loss: 0.004006 | Recon Loss: 0.003493 | Commit Loss: 0.001025 | Perplexity: 1062.319405
2025-09-14 18:05:52,231 Stage: Train 0.5 | Epoch: 53 | Iter: 40600 | Total Loss: 0.003937 | Recon Loss: 0.003423 | Commit Loss: 0.001029 | Perplexity: 1058.758688
Trainning Epoch:   8%|▊         | 54/665 [1:33:17<17:34:42, 103.57s/it]2025-09-14 18:06:20,051 Stage: Train 0.5 | Epoch: 54 | Iter: 40800 | Total Loss: 0.004072 | Recon Loss: 0.003564 | Commit Loss: 0.001017 | Perplexity: 1056.735935
2025-09-14 18:06:47,642 Stage: Train 0.5 | Epoch: 54 | Iter: 41000 | Total Loss: 0.004096 | Recon Loss: 0.003585 | Commit Loss: 0.001023 | Perplexity: 1063.545313
2025-09-14 18:07:15,121 Stage: Train 0.5 | Epoch: 54 | Iter: 41200 | Total Loss: 0.004076 | Recon Loss: 0.003568 | Commit Loss: 0.001016 | Perplexity: 1057.945686
2025-09-14 18:07:42,709 Stage: Train 0.5 | Epoch: 54 | Iter: 41400 | Total Loss: 0.004123 | Recon Loss: 0.003614 | Commit Loss: 0.001017 | Perplexity: 1056.179219
Trainning Epoch:   8%|▊         | 55/665 [1:35:00<17:33:22, 103.61s/it]2025-09-14 18:08:10,484 Stage: Train 0.5 | Epoch: 55 | Iter: 41600 | Total Loss: 0.003991 | Recon Loss: 0.003484 | Commit Loss: 0.001014 | Perplexity: 1059.326982
2025-09-14 18:08:38,045 Stage: Train 0.5 | Epoch: 55 | Iter: 41800 | Total Loss: 0.003965 | Recon Loss: 0.003456 | Commit Loss: 0.001017 | Perplexity: 1058.579535
2025-09-14 18:09:05,452 Stage: Train 0.5 | Epoch: 55 | Iter: 42000 | Total Loss: 0.004132 | Recon Loss: 0.003621 | Commit Loss: 0.001022 | Perplexity: 1060.365334
Trainning Epoch:   8%|▊         | 56/665 [1:36:44<17:31:42, 103.62s/it]2025-09-14 18:09:32,767 Stage: Train 0.5 | Epoch: 56 | Iter: 42200 | Total Loss: 0.004067 | Recon Loss: 0.003562 | Commit Loss: 0.001010 | Perplexity: 1054.155152
2025-09-14 18:10:00,279 Stage: Train 0.5 | Epoch: 56 | Iter: 42400 | Total Loss: 0.004075 | Recon Loss: 0.003569 | Commit Loss: 0.001012 | Perplexity: 1059.810243
2025-09-14 18:10:27,671 Stage: Train 0.5 | Epoch: 56 | Iter: 42600 | Total Loss: 0.004058 | Recon Loss: 0.003555 | Commit Loss: 0.001007 | Perplexity: 1054.945443
2025-09-14 18:10:55,049 Stage: Train 0.5 | Epoch: 56 | Iter: 42800 | Total Loss: 0.003883 | Recon Loss: 0.003373 | Commit Loss: 0.001021 | Perplexity: 1060.990391
Trainning Epoch:   9%|▊         | 57/665 [1:38:27<17:28:40, 103.49s/it]2025-09-14 18:11:22,500 Stage: Train 0.5 | Epoch: 57 | Iter: 43000 | Total Loss: 0.004024 | Recon Loss: 0.003523 | Commit Loss: 0.001002 | Perplexity: 1060.374033
2025-09-14 18:11:49,877 Stage: Train 0.5 | Epoch: 57 | Iter: 43200 | Total Loss: 0.004007 | Recon Loss: 0.003500 | Commit Loss: 0.001013 | Perplexity: 1061.682776
2025-09-14 18:12:17,331 Stage: Train 0.5 | Epoch: 57 | Iter: 43400 | Total Loss: 0.003966 | Recon Loss: 0.003464 | Commit Loss: 0.001003 | Perplexity: 1060.217594
2025-09-14 18:12:44,697 Stage: Train 0.5 | Epoch: 57 | Iter: 43600 | Total Loss: 0.004050 | Recon Loss: 0.003545 | Commit Loss: 0.001009 | Perplexity: 1055.989555
Trainning Epoch:   9%|▊         | 58/665 [1:40:10<17:26:02, 103.40s/it]2025-09-14 18:13:12,002 Stage: Train 0.5 | Epoch: 58 | Iter: 43800 | Total Loss: 0.004034 | Recon Loss: 0.003536 | Commit Loss: 0.000996 | Perplexity: 1063.514982
2025-09-14 18:13:39,351 Stage: Train 0.5 | Epoch: 58 | Iter: 44000 | Total Loss: 0.004034 | Recon Loss: 0.003538 | Commit Loss: 0.000992 | Perplexity: 1059.317283
2025-09-14 18:14:06,692 Stage: Train 0.5 | Epoch: 58 | Iter: 44200 | Total Loss: 0.003948 | Recon Loss: 0.003449 | Commit Loss: 0.000998 | Perplexity: 1065.058331
2025-09-14 18:14:34,110 Stage: Train 0.5 | Epoch: 58 | Iter: 44400 | Total Loss: 0.004039 | Recon Loss: 0.003544 | Commit Loss: 0.000991 | Perplexity: 1061.877260
Trainning Epoch:   9%|▉         | 59/665 [1:41:53<17:23:06, 103.28s/it]2025-09-14 18:15:01,600 Stage: Train 0.5 | Epoch: 59 | Iter: 44600 | Total Loss: 0.003951 | Recon Loss: 0.003463 | Commit Loss: 0.000977 | Perplexity: 1061.846211
2025-09-14 18:15:29,053 Stage: Train 0.5 | Epoch: 59 | Iter: 44800 | Total Loss: 0.003850 | Recon Loss: 0.003358 | Commit Loss: 0.000983 | Perplexity: 1069.589641
2025-09-14 18:15:56,576 Stage: Train 0.5 | Epoch: 59 | Iter: 45000 | Total Loss: 0.003989 | Recon Loss: 0.003488 | Commit Loss: 0.001002 | Perplexity: 1069.026010
Trainning Epoch:   9%|▉         | 60/665 [1:43:37<17:22:16, 103.37s/it]2025-09-14 18:16:24,091 Stage: Train 0.5 | Epoch: 60 | Iter: 45200 | Total Loss: 0.003848 | Recon Loss: 0.003352 | Commit Loss: 0.000992 | Perplexity: 1065.370707
2025-09-14 18:16:51,666 Stage: Train 0.5 | Epoch: 60 | Iter: 45400 | Total Loss: 0.003929 | Recon Loss: 0.003436 | Commit Loss: 0.000987 | Perplexity: 1065.837934
2025-09-14 18:17:19,380 Stage: Train 0.5 | Epoch: 60 | Iter: 45600 | Total Loss: 0.003908 | Recon Loss: 0.003406 | Commit Loss: 0.001003 | Perplexity: 1065.962271
2025-09-14 18:17:46,937 Stage: Train 0.5 | Epoch: 60 | Iter: 45800 | Total Loss: 0.003880 | Recon Loss: 0.003390 | Commit Loss: 0.000981 | Perplexity: 1066.377537
Trainning Epoch:   9%|▉         | 61/665 [1:45:21<17:22:06, 103.52s/it]2025-09-14 18:18:14,580 Stage: Train 0.5 | Epoch: 61 | Iter: 46000 | Total Loss: 0.004375 | Recon Loss: 0.003887 | Commit Loss: 0.000977 | Perplexity: 1056.344967
2025-09-14 18:18:42,106 Stage: Train 0.5 | Epoch: 61 | Iter: 46200 | Total Loss: 0.003841 | Recon Loss: 0.003357 | Commit Loss: 0.000968 | Perplexity: 1066.652484
2025-09-14 18:19:09,641 Stage: Train 0.5 | Epoch: 61 | Iter: 46400 | Total Loss: 0.004482 | Recon Loss: 0.004001 | Commit Loss: 0.000961 | Perplexity: 1051.399714
2025-09-14 18:19:37,242 Stage: Train 0.5 | Epoch: 61 | Iter: 46600 | Total Loss: 0.003690 | Recon Loss: 0.003205 | Commit Loss: 0.000969 | Perplexity: 1066.717340
Trainning Epoch:   9%|▉         | 62/665 [1:47:05<17:21:23, 103.62s/it]2025-09-14 18:20:04,789 Stage: Train 0.5 | Epoch: 62 | Iter: 46800 | Total Loss: 0.004040 | Recon Loss: 0.003550 | Commit Loss: 0.000980 | Perplexity: 1063.009775
2025-09-14 18:20:32,304 Stage: Train 0.5 | Epoch: 62 | Iter: 47000 | Total Loss: 0.003875 | Recon Loss: 0.003389 | Commit Loss: 0.000972 | Perplexity: 1064.605367
2025-09-14 18:20:59,777 Stage: Train 0.5 | Epoch: 62 | Iter: 47200 | Total Loss: 0.003866 | Recon Loss: 0.003374 | Commit Loss: 0.000984 | Perplexity: 1059.798513
2025-09-14 18:21:27,173 Stage: Train 0.5 | Epoch: 62 | Iter: 47400 | Total Loss: 0.003803 | Recon Loss: 0.003310 | Commit Loss: 0.000986 | Perplexity: 1061.878046
Trainning Epoch:   9%|▉         | 63/665 [1:48:48<17:19:15, 103.58s/it]2025-09-14 18:21:54,746 Stage: Train 0.5 | Epoch: 63 | Iter: 47600 | Total Loss: 0.003820 | Recon Loss: 0.003327 | Commit Loss: 0.000987 | Perplexity: 1062.859880
2025-09-14 18:22:22,130 Stage: Train 0.5 | Epoch: 63 | Iter: 47800 | Total Loss: 0.003821 | Recon Loss: 0.003325 | Commit Loss: 0.000992 | Perplexity: 1063.766356
2025-09-14 18:22:49,676 Stage: Train 0.5 | Epoch: 63 | Iter: 48000 | Total Loss: 0.003983 | Recon Loss: 0.003498 | Commit Loss: 0.000970 | Perplexity: 1054.624017
Trainning Epoch:  10%|▉         | 64/665 [1:50:32<17:17:30, 103.58s/it]2025-09-14 18:23:17,230 Stage: Train 0.5 | Epoch: 64 | Iter: 48200 | Total Loss: 0.003833 | Recon Loss: 0.003341 | Commit Loss: 0.000983 | Perplexity: 1056.958410
2025-09-14 18:23:44,625 Stage: Train 0.5 | Epoch: 64 | Iter: 48400 | Total Loss: 0.003828 | Recon Loss: 0.003334 | Commit Loss: 0.000987 | Perplexity: 1062.599804
2025-09-14 18:24:12,010 Stage: Train 0.5 | Epoch: 64 | Iter: 48600 | Total Loss: 0.004112 | Recon Loss: 0.003627 | Commit Loss: 0.000970 | Perplexity: 1052.290392
2025-09-14 18:24:39,389 Stage: Train 0.5 | Epoch: 64 | Iter: 48800 | Total Loss: 0.003854 | Recon Loss: 0.003364 | Commit Loss: 0.000979 | Perplexity: 1060.608227
Trainning Epoch:  10%|▉         | 65/665 [1:52:15<17:14:59, 103.50s/it]2025-09-14 18:25:06,976 Stage: Train 0.5 | Epoch: 65 | Iter: 49000 | Total Loss: 0.003755 | Recon Loss: 0.003263 | Commit Loss: 0.000985 | Perplexity: 1058.746495
2025-09-14 18:25:34,356 Stage: Train 0.5 | Epoch: 65 | Iter: 49200 | Total Loss: 0.003841 | Recon Loss: 0.003342 | Commit Loss: 0.000998 | Perplexity: 1064.852102
2025-09-14 18:26:01,749 Stage: Train 0.5 | Epoch: 65 | Iter: 49400 | Total Loss: 0.003708 | Recon Loss: 0.003224 | Commit Loss: 0.000968 | Perplexity: 1054.056028
2025-09-14 18:26:29,162 Stage: Train 0.5 | Epoch: 65 | Iter: 49600 | Total Loss: 0.003903 | Recon Loss: 0.003407 | Commit Loss: 0.000992 | Perplexity: 1058.118913
Trainning Epoch:  10%|▉         | 66/665 [1:53:58<17:12:03, 103.38s/it]2025-09-14 18:26:56,648 Stage: Train 0.5 | Epoch: 66 | Iter: 49800 | Total Loss: 0.003851 | Recon Loss: 0.003363 | Commit Loss: 0.000976 | Perplexity: 1054.005346
2025-09-14 18:27:24,051 Stage: Train 0.5 | Epoch: 66 | Iter: 50000 | Total Loss: 0.003841 | Recon Loss: 0.003344 | Commit Loss: 0.000993 | Perplexity: 1056.204986
2025-09-14 18:27:51,475 Stage: Train 0.5 | Epoch: 66 | Iter: 50200 | Total Loss: 0.003760 | Recon Loss: 0.003267 | Commit Loss: 0.000986 | Perplexity: 1064.643990
2025-09-14 18:28:18,965 Stage: Train 0.5 | Epoch: 66 | Iter: 50400 | Total Loss: 0.003859 | Recon Loss: 0.003367 | Commit Loss: 0.000982 | Perplexity: 1054.906053
Trainning Epoch:  10%|█         | 67/665 [1:55:42<17:10:38, 103.41s/it]2025-09-14 18:28:46,451 Stage: Train 0.5 | Epoch: 67 | Iter: 50600 | Total Loss: 0.003809 | Recon Loss: 0.003313 | Commit Loss: 0.000992 | Perplexity: 1059.292684
2025-09-14 18:29:13,887 Stage: Train 0.5 | Epoch: 67 | Iter: 50800 | Total Loss: 0.003873 | Recon Loss: 0.003388 | Commit Loss: 0.000970 | Perplexity: 1053.360913
2025-09-14 18:29:41,325 Stage: Train 0.5 | Epoch: 67 | Iter: 51000 | Total Loss: 0.003771 | Recon Loss: 0.003283 | Commit Loss: 0.000975 | Perplexity: 1055.003783
2025-09-14 18:30:08,952 Stage: Train 0.5 | Epoch: 67 | Iter: 51200 | Total Loss: 0.003744 | Recon Loss: 0.003248 | Commit Loss: 0.000990 | Perplexity: 1061.094467
Trainning Epoch:  10%|█         | 68/665 [1:57:25<17:08:59, 103.42s/it]2025-09-14 18:30:36,400 Stage: Train 0.5 | Epoch: 68 | Iter: 51400 | Total Loss: 0.003840 | Recon Loss: 0.003348 | Commit Loss: 0.000984 | Perplexity: 1058.573520
2025-09-14 18:31:04,048 Stage: Train 0.5 | Epoch: 68 | Iter: 51600 | Total Loss: 0.003631 | Recon Loss: 0.003132 | Commit Loss: 0.000997 | Perplexity: 1062.553799
2025-09-14 18:31:31,461 Stage: Train 0.5 | Epoch: 68 | Iter: 51800 | Total Loss: 0.004013 | Recon Loss: 0.003521 | Commit Loss: 0.000984 | Perplexity: 1051.681309
Trainning Epoch:  10%|█         | 69/665 [1:59:09<17:07:46, 103.47s/it]2025-09-14 18:31:58,933 Stage: Train 0.5 | Epoch: 69 | Iter: 52000 | Total Loss: 0.003755 | Recon Loss: 0.003257 | Commit Loss: 0.000995 | Perplexity: 1056.370351
2025-09-14 18:32:26,725 Stage: Train 0.5 | Epoch: 69 | Iter: 52200 | Total Loss: 0.003871 | Recon Loss: 0.003384 | Commit Loss: 0.000974 | Perplexity: 1055.042719
2025-09-14 18:32:54,131 Stage: Train 0.5 | Epoch: 69 | Iter: 52400 | Total Loss: 0.003807 | Recon Loss: 0.003315 | Commit Loss: 0.000984 | Perplexity: 1055.229023
2025-09-14 18:33:21,468 Stage: Train 0.5 | Epoch: 69 | Iter: 52600 | Total Loss: 0.003791 | Recon Loss: 0.003299 | Commit Loss: 0.000984 | Perplexity: 1054.978478
Trainning Epoch:  11%|█         | 70/665 [2:00:52<17:06:00, 103.46s/it]2025-09-14 18:33:48,842 Stage: Train 0.5 | Epoch: 70 | Iter: 52800 | Total Loss: 0.003697 | Recon Loss: 0.003205 | Commit Loss: 0.000983 | Perplexity: 1062.199807
2025-09-14 18:34:16,257 Stage: Train 0.5 | Epoch: 70 | Iter: 53000 | Total Loss: 0.003836 | Recon Loss: 0.003346 | Commit Loss: 0.000981 | Perplexity: 1057.013349
2025-09-14 18:34:43,712 Stage: Train 0.5 | Epoch: 70 | Iter: 53200 | Total Loss: 0.003772 | Recon Loss: 0.003281 | Commit Loss: 0.000981 | Perplexity: 1060.473297
2025-09-14 18:35:11,130 Stage: Train 0.5 | Epoch: 70 | Iter: 53400 | Total Loss: 0.003826 | Recon Loss: 0.003337 | Commit Loss: 0.000978 | Perplexity: 1055.733091
Trainning Epoch:  11%|█         | 71/665 [2:02:35<17:03:32, 103.39s/it]2025-09-14 18:35:38,472 Stage: Train 0.5 | Epoch: 71 | Iter: 53600 | Total Loss: 0.003842 | Recon Loss: 0.003362 | Commit Loss: 0.000961 | Perplexity: 1052.736414
2025-09-14 18:36:05,858 Stage: Train 0.5 | Epoch: 71 | Iter: 53800 | Total Loss: 0.003609 | Recon Loss: 0.003113 | Commit Loss: 0.000992 | Perplexity: 1056.349791
2025-09-14 18:36:33,250 Stage: Train 0.5 | Epoch: 71 | Iter: 54000 | Total Loss: 0.003834 | Recon Loss: 0.003342 | Commit Loss: 0.000984 | Perplexity: 1060.032204
2025-09-14 18:37:00,658 Stage: Train 0.5 | Epoch: 71 | Iter: 54200 | Total Loss: 0.003833 | Recon Loss: 0.003341 | Commit Loss: 0.000984 | Perplexity: 1059.010156
Trainning Epoch:  11%|█         | 72/665 [2:04:18<17:00:57, 103.30s/it]2025-09-14 18:37:28,015 Stage: Train 0.5 | Epoch: 72 | Iter: 54400 | Total Loss: 0.003624 | Recon Loss: 0.003131 | Commit Loss: 0.000986 | Perplexity: 1058.510133
2025-09-14 18:37:55,421 Stage: Train 0.5 | Epoch: 72 | Iter: 54600 | Total Loss: 0.003674 | Recon Loss: 0.003181 | Commit Loss: 0.000986 | Perplexity: 1059.753238
2025-09-14 18:38:22,899 Stage: Train 0.5 | Epoch: 72 | Iter: 54800 | Total Loss: 0.003756 | Recon Loss: 0.003262 | Commit Loss: 0.000987 | Perplexity: 1058.223441
Trainning Epoch:  11%|█         | 73/665 [2:06:02<16:59:15, 103.30s/it]2025-09-14 18:38:50,359 Stage: Train 0.5 | Epoch: 73 | Iter: 55000 | Total Loss: 0.003607 | Recon Loss: 0.003108 | Commit Loss: 0.000998 | Perplexity: 1059.780952
2025-09-14 18:39:17,755 Stage: Train 0.5 | Epoch: 73 | Iter: 55200 | Total Loss: 0.003636 | Recon Loss: 0.003139 | Commit Loss: 0.000994 | Perplexity: 1056.901905
2025-09-14 18:39:45,230 Stage: Train 0.5 | Epoch: 73 | Iter: 55400 | Total Loss: 0.003798 | Recon Loss: 0.003301 | Commit Loss: 0.000992 | Perplexity: 1055.861348
2025-09-14 18:40:12,732 Stage: Train 0.5 | Epoch: 73 | Iter: 55600 | Total Loss: 0.003709 | Recon Loss: 0.003216 | Commit Loss: 0.000985 | Perplexity: 1057.035909
Trainning Epoch:  11%|█         | 74/665 [2:07:45<16:57:40, 103.32s/it]2025-09-14 18:40:40,188 Stage: Train 0.5 | Epoch: 74 | Iter: 55800 | Total Loss: 0.003705 | Recon Loss: 0.003214 | Commit Loss: 0.000981 | Perplexity: 1057.329492
2025-09-14 18:41:07,620 Stage: Train 0.5 | Epoch: 74 | Iter: 56000 | Total Loss: 0.003757 | Recon Loss: 0.003273 | Commit Loss: 0.000968 | Perplexity: 1054.377014
2025-09-14 18:41:35,142 Stage: Train 0.5 | Epoch: 74 | Iter: 56200 | Total Loss: 0.003743 | Recon Loss: 0.003253 | Commit Loss: 0.000979 | Perplexity: 1056.328802
2025-09-14 18:42:02,690 Stage: Train 0.5 | Epoch: 74 | Iter: 56400 | Total Loss: 0.003619 | Recon Loss: 0.003125 | Commit Loss: 0.000988 | Perplexity: 1060.829389
Trainning Epoch:  11%|█▏        | 75/665 [2:09:28<16:56:23, 103.36s/it]2025-09-14 18:42:30,071 Stage: Train 0.5 | Epoch: 75 | Iter: 56600 | Total Loss: 0.003925 | Recon Loss: 0.003435 | Commit Loss: 0.000981 | Perplexity: 1052.176370
2025-09-14 18:42:57,507 Stage: Train 0.5 | Epoch: 75 | Iter: 56800 | Total Loss: 0.003685 | Recon Loss: 0.003198 | Commit Loss: 0.000973 | Perplexity: 1054.708645
2025-09-14 18:43:24,907 Stage: Train 0.5 | Epoch: 75 | Iter: 57000 | Total Loss: 0.003690 | Recon Loss: 0.003198 | Commit Loss: 0.000983 | Perplexity: 1057.622961
2025-09-14 18:43:52,302 Stage: Train 0.5 | Epoch: 75 | Iter: 57200 | Total Loss: 0.003683 | Recon Loss: 0.003191 | Commit Loss: 0.000984 | Perplexity: 1058.772127
Trainning Epoch:  11%|█▏        | 76/665 [2:11:12<16:54:04, 103.30s/it]2025-09-14 18:44:19,651 Stage: Train 0.5 | Epoch: 76 | Iter: 57400 | Total Loss: 0.003820 | Recon Loss: 0.003340 | Commit Loss: 0.000960 | Perplexity: 1047.683305
2025-09-14 18:44:47,031 Stage: Train 0.5 | Epoch: 76 | Iter: 57600 | Total Loss: 0.003662 | Recon Loss: 0.003173 | Commit Loss: 0.000978 | Perplexity: 1058.502725
2025-09-14 18:45:14,410 Stage: Train 0.5 | Epoch: 76 | Iter: 57800 | Total Loss: 0.003591 | Recon Loss: 0.003098 | Commit Loss: 0.000986 | Perplexity: 1056.474008
Trainning Epoch:  12%|█▏        | 77/665 [2:12:55<16:51:55, 103.26s/it]2025-09-14 18:45:41,847 Stage: Train 0.5 | Epoch: 77 | Iter: 58000 | Total Loss: 0.003751 | Recon Loss: 0.003256 | Commit Loss: 0.000989 | Perplexity: 1055.567366
2025-09-14 18:46:09,231 Stage: Train 0.5 | Epoch: 77 | Iter: 58200 | Total Loss: 0.003633 | Recon Loss: 0.003143 | Commit Loss: 0.000979 | Perplexity: 1053.880050
2025-09-14 18:46:36,622 Stage: Train 0.5 | Epoch: 77 | Iter: 58400 | Total Loss: 0.003622 | Recon Loss: 0.003133 | Commit Loss: 0.000978 | Perplexity: 1059.845462
2025-09-14 18:47:04,022 Stage: Train 0.5 | Epoch: 77 | Iter: 58600 | Total Loss: 0.003693 | Recon Loss: 0.003197 | Commit Loss: 0.000991 | Perplexity: 1057.790672
Trainning Epoch:  12%|█▏        | 78/665 [2:14:38<16:49:38, 103.20s/it]2025-09-14 18:47:31,346 Stage: Train 0.5 | Epoch: 78 | Iter: 58800 | Total Loss: 0.003635 | Recon Loss: 0.003143 | Commit Loss: 0.000983 | Perplexity: 1055.119425
2025-09-14 18:47:58,744 Stage: Train 0.5 | Epoch: 78 | Iter: 59000 | Total Loss: 0.003658 | Recon Loss: 0.003162 | Commit Loss: 0.000991 | Perplexity: 1051.289681
2025-09-14 18:48:26,216 Stage: Train 0.5 | Epoch: 78 | Iter: 59200 | Total Loss: 0.003610 | Recon Loss: 0.003118 | Commit Loss: 0.000983 | Perplexity: 1056.500730
2025-09-14 18:48:53,787 Stage: Train 0.5 | Epoch: 78 | Iter: 59400 | Total Loss: 0.003609 | Recon Loss: 0.003122 | Commit Loss: 0.000975 | Perplexity: 1057.552757
Trainning Epoch:  12%|█▏        | 79/665 [2:16:21<16:48:30, 103.26s/it]2025-09-14 18:49:21,247 Stage: Train 0.5 | Epoch: 79 | Iter: 59600 | Total Loss: 0.003588 | Recon Loss: 0.003091 | Commit Loss: 0.000995 | Perplexity: 1054.067165
2025-09-14 18:49:48,725 Stage: Train 0.5 | Epoch: 79 | Iter: 59800 | Total Loss: 0.003822 | Recon Loss: 0.003335 | Commit Loss: 0.000975 | Perplexity: 1049.038278
2025-09-14 18:50:16,177 Stage: Train 0.5 | Epoch: 79 | Iter: 60000 | Total Loss: 0.003560 | Recon Loss: 0.003071 | Commit Loss: 0.000979 | Perplexity: 1056.343540
2025-09-14 18:50:16,177 Saving model at iteration 60000
2025-09-14 18:50:16,350 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_80_step_60000
2025-09-14 18:50:16,555 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_80_step_60000/pytorch_model.bin
2025-09-14 18:50:16,879 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_80_step_60000/optimizer.bin
2025-09-14 18:50:16,879 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_80_step_60000/scheduler.bin
2025-09-14 18:50:16,880 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_80_step_60000/random_states_0.pkl
2025-09-14 18:50:44,514 Stage: Train 0.5 | Epoch: 79 | Iter: 60200 | Total Loss: 0.003530 | Recon Loss: 0.003031 | Commit Loss: 0.000998 | Perplexity: 1056.420898
Trainning Epoch:  12%|█▏        | 80/665 [2:18:06<16:50:06, 103.60s/it]2025-09-14 18:51:12,100 Stage: Train 0.5 | Epoch: 80 | Iter: 60400 | Total Loss: 0.003687 | Recon Loss: 0.003197 | Commit Loss: 0.000981 | Perplexity: 1047.750815
2025-09-14 18:51:39,553 Stage: Train 0.5 | Epoch: 80 | Iter: 60600 | Total Loss: 0.003545 | Recon Loss: 0.003052 | Commit Loss: 0.000987 | Perplexity: 1052.538795
2025-09-14 18:52:06,960 Stage: Train 0.5 | Epoch: 80 | Iter: 60800 | Total Loss: 0.003701 | Recon Loss: 0.003212 | Commit Loss: 0.000979 | Perplexity: 1053.068295
Trainning Epoch:  12%|█▏        | 81/665 [2:19:49<16:47:20, 103.49s/it]2025-09-14 18:52:34,312 Stage: Train 0.5 | Epoch: 81 | Iter: 61000 | Total Loss: 0.003520 | Recon Loss: 0.003024 | Commit Loss: 0.000993 | Perplexity: 1054.403282
2025-09-14 18:53:01,769 Stage: Train 0.5 | Epoch: 81 | Iter: 61200 | Total Loss: 0.003626 | Recon Loss: 0.003134 | Commit Loss: 0.000983 | Perplexity: 1054.083186
2025-09-14 18:53:29,191 Stage: Train 0.5 | Epoch: 81 | Iter: 61400 | Total Loss: 0.003661 | Recon Loss: 0.003169 | Commit Loss: 0.000984 | Perplexity: 1052.560203
2025-09-14 18:53:56,763 Stage: Train 0.5 | Epoch: 81 | Iter: 61600 | Total Loss: 0.003611 | Recon Loss: 0.003125 | Commit Loss: 0.000973 | Perplexity: 1052.640853
Trainning Epoch:  12%|█▏        | 82/665 [2:21:32<16:45:38, 103.50s/it]2025-09-14 18:54:24,301 Stage: Train 0.5 | Epoch: 82 | Iter: 61800 | Total Loss: 0.003758 | Recon Loss: 0.003264 | Commit Loss: 0.000987 | Perplexity: 1051.240187
2025-09-14 18:54:51,752 Stage: Train 0.5 | Epoch: 82 | Iter: 62000 | Total Loss: 0.003698 | Recon Loss: 0.003206 | Commit Loss: 0.000982 | Perplexity: 1051.631075
2025-09-14 18:55:19,241 Stage: Train 0.5 | Epoch: 82 | Iter: 62200 | Total Loss: 0.003510 | Recon Loss: 0.003017 | Commit Loss: 0.000986 | Perplexity: 1055.591863
2025-09-14 18:55:46,969 Stage: Train 0.5 | Epoch: 82 | Iter: 62400 | Total Loss: 0.003710 | Recon Loss: 0.003216 | Commit Loss: 0.000989 | Perplexity: 1052.797636
Trainning Epoch:  12%|█▏        | 83/665 [2:23:16<16:44:59, 103.61s/it]2025-09-14 18:56:14,572 Stage: Train 0.5 | Epoch: 83 | Iter: 62600 | Total Loss: 0.003497 | Recon Loss: 0.003009 | Commit Loss: 0.000974 | Perplexity: 1050.202290
2025-09-14 18:56:42,128 Stage: Train 0.5 | Epoch: 83 | Iter: 62800 | Total Loss: 0.003576 | Recon Loss: 0.003085 | Commit Loss: 0.000982 | Perplexity: 1054.246146
2025-09-14 18:57:09,737 Stage: Train 0.5 | Epoch: 83 | Iter: 63000 | Total Loss: 0.003662 | Recon Loss: 0.003170 | Commit Loss: 0.000985 | Perplexity: 1051.863088
2025-09-14 18:57:37,155 Stage: Train 0.5 | Epoch: 83 | Iter: 63200 | Total Loss: 0.003605 | Recon Loss: 0.003112 | Commit Loss: 0.000986 | Perplexity: 1051.996910
Trainning Epoch:  13%|█▎        | 84/665 [2:25:00<16:43:00, 103.58s/it]2025-09-14 18:58:04,593 Stage: Train 0.5 | Epoch: 84 | Iter: 63400 | Total Loss: 0.003560 | Recon Loss: 0.003063 | Commit Loss: 0.000993 | Perplexity: 1053.029467
2025-09-14 18:58:32,067 Stage: Train 0.5 | Epoch: 84 | Iter: 63600 | Total Loss: 0.003524 | Recon Loss: 0.003027 | Commit Loss: 0.000994 | Perplexity: 1055.337654
2025-09-14 18:58:59,607 Stage: Train 0.5 | Epoch: 84 | Iter: 63800 | Total Loss: 0.003608 | Recon Loss: 0.003124 | Commit Loss: 0.000968 | Perplexity: 1049.732113
2025-09-14 18:59:27,082 Stage: Train 0.5 | Epoch: 84 | Iter: 64000 | Total Loss: 0.003597 | Recon Loss: 0.003102 | Commit Loss: 0.000991 | Perplexity: 1051.809629
Trainning Epoch:  13%|█▎        | 85/665 [2:26:43<16:40:59, 103.55s/it]2025-09-14 18:59:54,458 Stage: Train 0.5 | Epoch: 85 | Iter: 64200 | Total Loss: 0.003539 | Recon Loss: 0.003051 | Commit Loss: 0.000974 | Perplexity: 1046.333344
2025-09-14 19:00:21,964 Stage: Train 0.5 | Epoch: 85 | Iter: 64400 | Total Loss: 0.003575 | Recon Loss: 0.003084 | Commit Loss: 0.000982 | Perplexity: 1057.164541
2025-09-14 19:00:49,612 Stage: Train 0.5 | Epoch: 85 | Iter: 64600 | Total Loss: 0.003627 | Recon Loss: 0.003137 | Commit Loss: 0.000978 | Perplexity: 1048.458510
Trainning Epoch:  13%|█▎        | 86/665 [2:28:27<16:39:23, 103.56s/it]2025-09-14 19:01:17,104 Stage: Train 0.5 | Epoch: 86 | Iter: 64800 | Total Loss: 0.003552 | Recon Loss: 0.003050 | Commit Loss: 0.001004 | Perplexity: 1052.841574
2025-09-14 19:01:44,630 Stage: Train 0.5 | Epoch: 86 | Iter: 65000 | Total Loss: 0.003585 | Recon Loss: 0.003098 | Commit Loss: 0.000973 | Perplexity: 1048.325610
2025-09-14 19:02:12,190 Stage: Train 0.5 | Epoch: 86 | Iter: 65200 | Total Loss: 0.003441 | Recon Loss: 0.002948 | Commit Loss: 0.000985 | Perplexity: 1052.205213
2025-09-14 19:02:39,715 Stage: Train 0.5 | Epoch: 86 | Iter: 65400 | Total Loss: 0.003596 | Recon Loss: 0.003099 | Commit Loss: 0.000995 | Perplexity: 1053.429232
Trainning Epoch:  13%|█▎        | 87/665 [2:30:10<16:37:51, 103.58s/it]2025-09-14 19:03:07,215 Stage: Train 0.5 | Epoch: 87 | Iter: 65600 | Total Loss: 0.003651 | Recon Loss: 0.003152 | Commit Loss: 0.000998 | Perplexity: 1048.712445
2025-09-14 19:03:34,759 Stage: Train 0.5 | Epoch: 87 | Iter: 65800 | Total Loss: 0.003561 | Recon Loss: 0.003067 | Commit Loss: 0.000989 | Perplexity: 1044.875505
2025-09-14 19:04:02,417 Stage: Train 0.5 | Epoch: 87 | Iter: 66000 | Total Loss: 0.003495 | Recon Loss: 0.003001 | Commit Loss: 0.000987 | Perplexity: 1053.649473
2025-09-14 19:04:29,950 Stage: Train 0.5 | Epoch: 87 | Iter: 66200 | Total Loss: 0.003545 | Recon Loss: 0.003058 | Commit Loss: 0.000975 | Perplexity: 1047.672887
Trainning Epoch:  13%|█▎        | 88/665 [2:31:54<16:36:35, 103.63s/it]2025-09-14 19:04:57,376 Stage: Train 0.5 | Epoch: 88 | Iter: 66400 | Total Loss: 0.003532 | Recon Loss: 0.003041 | Commit Loss: 0.000982 | Perplexity: 1045.451794
2025-09-14 19:05:25,181 Stage: Train 0.5 | Epoch: 88 | Iter: 66600 | Total Loss: 0.003734 | Recon Loss: 0.003247 | Commit Loss: 0.000975 | Perplexity: 1045.877384
2025-09-14 19:05:52,823 Stage: Train 0.5 | Epoch: 88 | Iter: 66800 | Total Loss: 0.003439 | Recon Loss: 0.002941 | Commit Loss: 0.000995 | Perplexity: 1054.844277
2025-09-14 19:06:20,680 Stage: Train 0.5 | Epoch: 88 | Iter: 67000 | Total Loss: 0.003554 | Recon Loss: 0.003064 | Commit Loss: 0.000979 | Perplexity: 1049.323192
Trainning Epoch:  13%|█▎        | 89/665 [2:33:39<16:36:50, 103.84s/it]2025-09-14 19:06:48,502 Stage: Train 0.5 | Epoch: 89 | Iter: 67200 | Total Loss: 0.003478 | Recon Loss: 0.002985 | Commit Loss: 0.000985 | Perplexity: 1051.673524
2025-09-14 19:07:16,331 Stage: Train 0.5 | Epoch: 89 | Iter: 67400 | Total Loss: 0.003511 | Recon Loss: 0.003015 | Commit Loss: 0.000992 | Perplexity: 1055.246891
2025-09-14 19:07:44,166 Stage: Train 0.5 | Epoch: 89 | Iter: 67600 | Total Loss: 0.003596 | Recon Loss: 0.003108 | Commit Loss: 0.000977 | Perplexity: 1048.210732
Trainning Epoch:  14%|█▎        | 90/665 [2:35:23<16:37:28, 104.09s/it]2025-09-14 19:08:11,810 Stage: Train 0.5 | Epoch: 90 | Iter: 67800 | Total Loss: 0.003588 | Recon Loss: 0.003092 | Commit Loss: 0.000990 | Perplexity: 1047.172517
2025-09-14 19:08:39,289 Stage: Train 0.5 | Epoch: 90 | Iter: 68000 | Total Loss: 0.003478 | Recon Loss: 0.002991 | Commit Loss: 0.000973 | Perplexity: 1049.506682
2025-09-14 19:09:06,910 Stage: Train 0.5 | Epoch: 90 | Iter: 68200 | Total Loss: 0.003559 | Recon Loss: 0.003062 | Commit Loss: 0.000994 | Perplexity: 1050.609389
2025-09-14 19:09:34,507 Stage: Train 0.5 | Epoch: 90 | Iter: 68400 | Total Loss: 0.003525 | Recon Loss: 0.003032 | Commit Loss: 0.000985 | Perplexity: 1046.622089
Trainning Epoch:  14%|█▎        | 91/665 [2:37:07<16:35:07, 104.02s/it]2025-09-14 19:10:02,289 Stage: Train 0.5 | Epoch: 91 | Iter: 68600 | Total Loss: 0.003531 | Recon Loss: 0.003043 | Commit Loss: 0.000977 | Perplexity: 1047.380048
2025-09-14 19:10:30,101 Stage: Train 0.5 | Epoch: 91 | Iter: 68800 | Total Loss: 0.003509 | Recon Loss: 0.003021 | Commit Loss: 0.000977 | Perplexity: 1044.618816
2025-09-14 19:10:57,808 Stage: Train 0.5 | Epoch: 91 | Iter: 69000 | Total Loss: 0.003660 | Recon Loss: 0.003168 | Commit Loss: 0.000983 | Perplexity: 1046.033434
2025-09-14 19:11:25,223 Stage: Train 0.5 | Epoch: 91 | Iter: 69200 | Total Loss: 0.003493 | Recon Loss: 0.002999 | Commit Loss: 0.000988 | Perplexity: 1047.173390
Trainning Epoch:  14%|█▍        | 92/665 [2:38:51<16:33:29, 104.03s/it]2025-09-14 19:11:52,591 Stage: Train 0.5 | Epoch: 92 | Iter: 69400 | Total Loss: 0.003402 | Recon Loss: 0.002911 | Commit Loss: 0.000983 | Perplexity: 1052.764597
2025-09-14 19:12:20,047 Stage: Train 0.5 | Epoch: 92 | Iter: 69600 | Total Loss: 0.003631 | Recon Loss: 0.003140 | Commit Loss: 0.000982 | Perplexity: 1046.171860
2025-09-14 19:12:47,508 Stage: Train 0.5 | Epoch: 92 | Iter: 69800 | Total Loss: 0.003518 | Recon Loss: 0.003026 | Commit Loss: 0.000984 | Perplexity: 1044.505246
2025-09-14 19:13:14,955 Stage: Train 0.5 | Epoch: 92 | Iter: 70000 | Total Loss: 0.003571 | Recon Loss: 0.003077 | Commit Loss: 0.000988 | Perplexity: 1051.499295
Trainning Epoch:  14%|█▍        | 93/665 [2:40:34<16:29:41, 103.81s/it]2025-09-14 19:13:42,397 Stage: Train 0.5 | Epoch: 93 | Iter: 70200 | Total Loss: 0.003479 | Recon Loss: 0.002991 | Commit Loss: 0.000975 | Perplexity: 1045.400135
2025-09-14 19:14:09,816 Stage: Train 0.5 | Epoch: 93 | Iter: 70400 | Total Loss: 0.003472 | Recon Loss: 0.002975 | Commit Loss: 0.000995 | Perplexity: 1050.487061
2025-09-14 19:14:37,421 Stage: Train 0.5 | Epoch: 93 | Iter: 70600 | Total Loss: 0.003519 | Recon Loss: 0.003029 | Commit Loss: 0.000979 | Perplexity: 1049.189222
Trainning Epoch:  14%|█▍        | 94/665 [2:42:18<16:27:04, 103.72s/it]2025-09-14 19:15:04,881 Stage: Train 0.5 | Epoch: 94 | Iter: 70800 | Total Loss: 0.003391 | Recon Loss: 0.002896 | Commit Loss: 0.000991 | Perplexity: 1046.905191
2025-09-14 19:15:32,413 Stage: Train 0.5 | Epoch: 94 | Iter: 71000 | Total Loss: 0.003560 | Recon Loss: 0.003065 | Commit Loss: 0.000989 | Perplexity: 1043.585035
2025-09-14 19:15:59,914 Stage: Train 0.5 | Epoch: 94 | Iter: 71200 | Total Loss: 0.003496 | Recon Loss: 0.003004 | Commit Loss: 0.000984 | Perplexity: 1053.529046
2025-09-14 19:16:27,413 Stage: Train 0.5 | Epoch: 94 | Iter: 71400 | Total Loss: 0.003412 | Recon Loss: 0.002920 | Commit Loss: 0.000985 | Perplexity: 1048.970870
Trainning Epoch:  14%|█▍        | 95/665 [2:44:01<16:24:40, 103.65s/it]2025-09-14 19:16:54,794 Stage: Train 0.5 | Epoch: 95 | Iter: 71600 | Total Loss: 0.003495 | Recon Loss: 0.002999 | Commit Loss: 0.000992 | Perplexity: 1043.366728
2025-09-14 19:17:22,191 Stage: Train 0.5 | Epoch: 95 | Iter: 71800 | Total Loss: 0.003536 | Recon Loss: 0.003045 | Commit Loss: 0.000982 | Perplexity: 1046.953885
2025-09-14 19:17:49,717 Stage: Train 0.5 | Epoch: 95 | Iter: 72000 | Total Loss: 0.003447 | Recon Loss: 0.002963 | Commit Loss: 0.000967 | Perplexity: 1044.611674
2025-09-14 19:18:17,123 Stage: Train 0.5 | Epoch: 95 | Iter: 72200 | Total Loss: 0.003539 | Recon Loss: 0.003040 | Commit Loss: 0.000998 | Perplexity: 1049.762552
Trainning Epoch:  14%|█▍        | 96/665 [2:45:45<16:21:46, 103.53s/it]2025-09-14 19:18:44,490 Stage: Train 0.5 | Epoch: 96 | Iter: 72400 | Total Loss: 0.003475 | Recon Loss: 0.002988 | Commit Loss: 0.000974 | Perplexity: 1044.378051
2025-09-14 19:19:11,884 Stage: Train 0.5 | Epoch: 96 | Iter: 72600 | Total Loss: 0.003519 | Recon Loss: 0.003028 | Commit Loss: 0.000983 | Perplexity: 1043.069237
2025-09-14 19:19:39,264 Stage: Train 0.5 | Epoch: 96 | Iter: 72800 | Total Loss: 0.003403 | Recon Loss: 0.002908 | Commit Loss: 0.000990 | Perplexity: 1052.362156
2025-09-14 19:20:06,657 Stage: Train 0.5 | Epoch: 96 | Iter: 73000 | Total Loss: 0.003464 | Recon Loss: 0.002969 | Commit Loss: 0.000990 | Perplexity: 1049.250310
Trainning Epoch:  15%|█▍        | 97/665 [2:47:28<16:18:49, 103.40s/it]2025-09-14 19:20:34,024 Stage: Train 0.5 | Epoch: 97 | Iter: 73200 | Total Loss: 0.003453 | Recon Loss: 0.002967 | Commit Loss: 0.000973 | Perplexity: 1040.954266
2025-09-14 19:21:01,617 Stage: Train 0.5 | Epoch: 97 | Iter: 73400 | Total Loss: 0.003489 | Recon Loss: 0.002995 | Commit Loss: 0.000988 | Perplexity: 1047.237854
2025-09-14 19:21:29,109 Stage: Train 0.5 | Epoch: 97 | Iter: 73600 | Total Loss: 0.003434 | Recon Loss: 0.002943 | Commit Loss: 0.000982 | Perplexity: 1047.679523
Trainning Epoch:  15%|█▍        | 98/665 [2:49:11<16:17:37, 103.45s/it]2025-09-14 19:21:56,644 Stage: Train 0.5 | Epoch: 98 | Iter: 73800 | Total Loss: 0.003490 | Recon Loss: 0.002997 | Commit Loss: 0.000987 | Perplexity: 1044.659030
2025-09-14 19:22:24,248 Stage: Train 0.5 | Epoch: 98 | Iter: 74000 | Total Loss: 0.003544 | Recon Loss: 0.003057 | Commit Loss: 0.000975 | Perplexity: 1042.209518
2025-09-14 19:22:51,885 Stage: Train 0.5 | Epoch: 98 | Iter: 74200 | Total Loss: 0.003468 | Recon Loss: 0.002979 | Commit Loss: 0.000979 | Perplexity: 1047.812473
2025-09-14 19:23:19,398 Stage: Train 0.5 | Epoch: 98 | Iter: 74400 | Total Loss: 0.003557 | Recon Loss: 0.003070 | Commit Loss: 0.000975 | Perplexity: 1042.856300
Trainning Epoch:  15%|█▍        | 99/665 [2:50:55<16:16:44, 103.54s/it]2025-09-14 19:23:46,839 Stage: Train 0.5 | Epoch: 99 | Iter: 74600 | Total Loss: 0.003444 | Recon Loss: 0.002951 | Commit Loss: 0.000985 | Perplexity: 1046.096952
2025-09-14 19:24:14,410 Stage: Train 0.5 | Epoch: 99 | Iter: 74800 | Total Loss: 0.003394 | Recon Loss: 0.002901 | Commit Loss: 0.000986 | Perplexity: 1046.435339
2025-09-14 19:24:42,081 Stage: Train 0.5 | Epoch: 99 | Iter: 75000 | Total Loss: 0.003409 | Recon Loss: 0.002924 | Commit Loss: 0.000970 | Perplexity: 1045.454673
2025-09-14 19:25:09,933 Stage: Train 0.5 | Epoch: 99 | Iter: 75200 | Total Loss: 0.003405 | Recon Loss: 0.002907 | Commit Loss: 0.000996 | Perplexity: 1050.624794
Trainning Epoch:  15%|█▌        | 100/665 [2:52:39<16:16:28, 103.70s/it]2025-09-14 19:25:37,384 Stage: Train 0.5 | Epoch: 100 | Iter: 75400 | Total Loss: 0.003484 | Recon Loss: 0.002992 | Commit Loss: 0.000982 | Perplexity: 1046.103779
2025-09-14 19:26:04,845 Stage: Train 0.5 | Epoch: 100 | Iter: 75600 | Total Loss: 0.003459 | Recon Loss: 0.002965 | Commit Loss: 0.000989 | Perplexity: 1046.732975
2025-09-14 19:26:32,373 Stage: Train 0.5 | Epoch: 100 | Iter: 75800 | Total Loss: 0.003357 | Recon Loss: 0.002867 | Commit Loss: 0.000980 | Perplexity: 1044.967939
2025-09-14 19:26:59,843 Stage: Train 0.5 | Epoch: 100 | Iter: 76000 | Total Loss: 0.003438 | Recon Loss: 0.002943 | Commit Loss: 0.000990 | Perplexity: 1048.530481
Trainning Epoch:  15%|█▌        | 101/665 [2:54:23<16:14:03, 103.62s/it]2025-09-14 19:27:27,248 Stage: Train 0.5 | Epoch: 101 | Iter: 76200 | Total Loss: 0.003437 | Recon Loss: 0.002947 | Commit Loss: 0.000980 | Perplexity: 1043.462227
2025-09-14 19:27:54,751 Stage: Train 0.5 | Epoch: 101 | Iter: 76400 | Total Loss: 0.003378 | Recon Loss: 0.002884 | Commit Loss: 0.000989 | Perplexity: 1048.081146
2025-09-14 19:28:22,253 Stage: Train 0.5 | Epoch: 101 | Iter: 76600 | Total Loss: 0.003365 | Recon Loss: 0.002875 | Commit Loss: 0.000980 | Perplexity: 1048.167917
2025-09-14 19:28:49,705 Stage: Train 0.5 | Epoch: 101 | Iter: 76800 | Total Loss: 0.003491 | Recon Loss: 0.002992 | Commit Loss: 0.000997 | Perplexity: 1047.258221
Trainning Epoch:  15%|█▌        | 102/665 [2:56:06<16:11:44, 103.56s/it]2025-09-14 19:29:17,100 Stage: Train 0.5 | Epoch: 102 | Iter: 77000 | Total Loss: 0.003372 | Recon Loss: 0.002876 | Commit Loss: 0.000992 | Perplexity: 1052.036957
2025-09-14 19:29:44,546 Stage: Train 0.5 | Epoch: 102 | Iter: 77200 | Total Loss: 0.003335 | Recon Loss: 0.002845 | Commit Loss: 0.000981 | Perplexity: 1045.871335
2025-09-14 19:30:11,988 Stage: Train 0.5 | Epoch: 102 | Iter: 77400 | Total Loss: 0.003381 | Recon Loss: 0.002887 | Commit Loss: 0.000989 | Perplexity: 1049.636936
Trainning Epoch:  15%|█▌        | 103/665 [2:57:49<16:09:07, 103.46s/it]2025-09-14 19:30:39,334 Stage: Train 0.5 | Epoch: 103 | Iter: 77600 | Total Loss: 0.003518 | Recon Loss: 0.003029 | Commit Loss: 0.000980 | Perplexity: 1044.493397
2025-09-14 19:31:06,714 Stage: Train 0.5 | Epoch: 103 | Iter: 77800 | Total Loss: 0.003356 | Recon Loss: 0.002864 | Commit Loss: 0.000985 | Perplexity: 1052.023083
2025-09-14 19:31:34,188 Stage: Train 0.5 | Epoch: 103 | Iter: 78000 | Total Loss: 0.003393 | Recon Loss: 0.002896 | Commit Loss: 0.000994 | Perplexity: 1048.874613
2025-09-14 19:32:01,631 Stage: Train 0.5 | Epoch: 103 | Iter: 78200 | Total Loss: 0.003457 | Recon Loss: 0.002971 | Commit Loss: 0.000973 | Perplexity: 1041.984465
Trainning Epoch:  16%|█▌        | 104/665 [2:59:33<16:06:47, 103.40s/it]2025-09-14 19:32:29,060 Stage: Train 0.5 | Epoch: 104 | Iter: 78400 | Total Loss: 0.003395 | Recon Loss: 0.002905 | Commit Loss: 0.000982 | Perplexity: 1049.003359
2025-09-14 19:32:56,554 Stage: Train 0.5 | Epoch: 104 | Iter: 78600 | Total Loss: 0.003516 | Recon Loss: 0.003024 | Commit Loss: 0.000982 | Perplexity: 1047.580105
2025-09-14 19:33:24,035 Stage: Train 0.5 | Epoch: 104 | Iter: 78800 | Total Loss: 0.003375 | Recon Loss: 0.002884 | Commit Loss: 0.000981 | Perplexity: 1046.117559
2025-09-14 19:33:51,531 Stage: Train 0.5 | Epoch: 104 | Iter: 79000 | Total Loss: 0.003450 | Recon Loss: 0.002959 | Commit Loss: 0.000982 | Perplexity: 1045.608058
Trainning Epoch:  16%|█▌        | 105/665 [3:01:16<16:05:10, 103.41s/it]2025-09-14 19:34:18,926 Stage: Train 0.5 | Epoch: 105 | Iter: 79200 | Total Loss: 0.003385 | Recon Loss: 0.002897 | Commit Loss: 0.000975 | Perplexity: 1042.739761
2025-09-14 19:34:46,472 Stage: Train 0.5 | Epoch: 105 | Iter: 79400 | Total Loss: 0.003345 | Recon Loss: 0.002858 | Commit Loss: 0.000975 | Perplexity: 1046.322306
2025-09-14 19:35:13,957 Stage: Train 0.5 | Epoch: 105 | Iter: 79600 | Total Loss: 0.003424 | Recon Loss: 0.002934 | Commit Loss: 0.000981 | Perplexity: 1045.467071
2025-09-14 19:35:41,549 Stage: Train 0.5 | Epoch: 105 | Iter: 79800 | Total Loss: 0.003293 | Recon Loss: 0.002801 | Commit Loss: 0.000985 | Perplexity: 1050.685274
Trainning Epoch:  16%|█▌        | 106/665 [3:03:00<16:03:54, 103.46s/it]2025-09-14 19:36:08,971 Stage: Train 0.5 | Epoch: 106 | Iter: 80000 | Total Loss: 0.003434 | Recon Loss: 0.002936 | Commit Loss: 0.000996 | Perplexity: 1048.661522
2025-09-14 19:36:08,971 Saving model at iteration 80000
2025-09-14 19:36:09,165 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_107_step_80000
2025-09-14 19:36:09,369 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_107_step_80000/pytorch_model.bin
2025-09-14 19:36:09,687 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_107_step_80000/optimizer.bin
2025-09-14 19:36:09,688 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_107_step_80000/scheduler.bin
2025-09-14 19:36:09,688 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_107_step_80000/random_states_0.pkl
2025-09-14 19:36:37,665 Stage: Train 0.5 | Epoch: 106 | Iter: 80200 | Total Loss: 0.003317 | Recon Loss: 0.002826 | Commit Loss: 0.000981 | Perplexity: 1047.553087
2025-09-14 19:37:05,173 Stage: Train 0.5 | Epoch: 106 | Iter: 80400 | Total Loss: 0.003404 | Recon Loss: 0.002916 | Commit Loss: 0.000977 | Perplexity: 1043.435319
Trainning Epoch:  16%|█▌        | 107/665 [3:04:44<16:05:30, 103.82s/it]2025-09-14 19:37:32,622 Stage: Train 0.5 | Epoch: 107 | Iter: 80600 | Total Loss: 0.003404 | Recon Loss: 0.002914 | Commit Loss: 0.000981 | Perplexity: 1042.413594
2025-09-14 19:38:00,244 Stage: Train 0.5 | Epoch: 107 | Iter: 80800 | Total Loss: 0.003367 | Recon Loss: 0.002875 | Commit Loss: 0.000984 | Perplexity: 1049.256594
2025-09-14 19:38:27,632 Stage: Train 0.5 | Epoch: 107 | Iter: 81000 | Total Loss: 0.003295 | Recon Loss: 0.002805 | Commit Loss: 0.000982 | Perplexity: 1045.940857
2025-09-14 19:38:54,992 Stage: Train 0.5 | Epoch: 107 | Iter: 81200 | Total Loss: 0.003373 | Recon Loss: 0.002882 | Commit Loss: 0.000983 | Perplexity: 1046.019068
Trainning Epoch:  16%|█▌        | 108/665 [3:06:27<16:02:13, 103.65s/it]2025-09-14 19:39:22,287 Stage: Train 0.5 | Epoch: 108 | Iter: 81400 | Total Loss: 0.003429 | Recon Loss: 0.002938 | Commit Loss: 0.000982 | Perplexity: 1047.993318
2025-09-14 19:39:49,762 Stage: Train 0.5 | Epoch: 108 | Iter: 81600 | Total Loss: 0.003340 | Recon Loss: 0.002850 | Commit Loss: 0.000981 | Perplexity: 1044.749291
2025-09-14 19:40:17,182 Stage: Train 0.5 | Epoch: 108 | Iter: 81800 | Total Loss: 0.003278 | Recon Loss: 0.002785 | Commit Loss: 0.000987 | Perplexity: 1046.936810
2025-09-14 19:40:44,740 Stage: Train 0.5 | Epoch: 108 | Iter: 82000 | Total Loss: 0.003416 | Recon Loss: 0.002920 | Commit Loss: 0.000992 | Perplexity: 1050.876673
Trainning Epoch:  16%|█▋        | 109/665 [3:08:11<15:59:41, 103.56s/it]2025-09-14 19:41:12,144 Stage: Train 0.5 | Epoch: 109 | Iter: 82200 | Total Loss: 0.003447 | Recon Loss: 0.002958 | Commit Loss: 0.000979 | Perplexity: 1044.691056
2025-09-14 19:41:39,640 Stage: Train 0.5 | Epoch: 109 | Iter: 82400 | Total Loss: 0.003260 | Recon Loss: 0.002766 | Commit Loss: 0.000988 | Perplexity: 1048.544724
2025-09-14 19:42:07,155 Stage: Train 0.5 | Epoch: 109 | Iter: 82600 | Total Loss: 0.003370 | Recon Loss: 0.002880 | Commit Loss: 0.000982 | Perplexity: 1047.484939
2025-09-14 19:42:34,696 Stage: Train 0.5 | Epoch: 109 | Iter: 82800 | Total Loss: 0.003335 | Recon Loss: 0.002842 | Commit Loss: 0.000986 | Perplexity: 1047.920735
Trainning Epoch:  17%|█▋        | 110/665 [3:09:54<15:57:49, 103.55s/it]2025-09-14 19:43:02,196 Stage: Train 0.5 | Epoch: 110 | Iter: 83000 | Total Loss: 0.003420 | Recon Loss: 0.002933 | Commit Loss: 0.000975 | Perplexity: 1041.910497
2025-09-14 19:43:29,625 Stage: Train 0.5 | Epoch: 110 | Iter: 83200 | Total Loss: 0.003312 | Recon Loss: 0.002820 | Commit Loss: 0.000984 | Perplexity: 1048.232334
2025-09-14 19:43:57,053 Stage: Train 0.5 | Epoch: 110 | Iter: 83400 | Total Loss: 0.003351 | Recon Loss: 0.002856 | Commit Loss: 0.000989 | Perplexity: 1050.317922
Trainning Epoch:  17%|█▋        | 111/665 [3:11:38<15:56:21, 103.58s/it]2025-09-14 19:44:24,747 Stage: Train 0.5 | Epoch: 111 | Iter: 83600 | Total Loss: 0.003285 | Recon Loss: 0.002793 | Commit Loss: 0.000985 | Perplexity: 1046.919027
2025-09-14 19:44:52,166 Stage: Train 0.5 | Epoch: 111 | Iter: 83800 | Total Loss: 0.003425 | Recon Loss: 0.002934 | Commit Loss: 0.000981 | Perplexity: 1045.898991
2025-09-14 19:45:19,570 Stage: Train 0.5 | Epoch: 111 | Iter: 84000 | Total Loss: 0.003280 | Recon Loss: 0.002795 | Commit Loss: 0.000970 | Perplexity: 1041.985667
2025-09-14 19:45:47,001 Stage: Train 0.5 | Epoch: 111 | Iter: 84200 | Total Loss: 0.003361 | Recon Loss: 0.002864 | Commit Loss: 0.000995 | Perplexity: 1054.196764
Trainning Epoch:  17%|█▋        | 112/665 [3:13:21<15:53:32, 103.46s/it]2025-09-14 19:46:14,363 Stage: Train 0.5 | Epoch: 112 | Iter: 84400 | Total Loss: 0.003349 | Recon Loss: 0.002863 | Commit Loss: 0.000973 | Perplexity: 1039.453820
2025-09-14 19:46:41,892 Stage: Train 0.5 | Epoch: 112 | Iter: 84600 | Total Loss: 0.003333 | Recon Loss: 0.002840 | Commit Loss: 0.000986 | Perplexity: 1052.475848
2025-09-14 19:47:09,318 Stage: Train 0.5 | Epoch: 112 | Iter: 84800 | Total Loss: 0.003314 | Recon Loss: 0.002822 | Commit Loss: 0.000982 | Perplexity: 1048.961834
2025-09-14 19:47:36,724 Stage: Train 0.5 | Epoch: 112 | Iter: 85000 | Total Loss: 0.003384 | Recon Loss: 0.002891 | Commit Loss: 0.000985 | Perplexity: 1048.349389
Trainning Epoch:  17%|█▋        | 113/665 [3:15:04<15:51:21, 103.41s/it]2025-09-14 19:48:04,124 Stage: Train 0.5 | Epoch: 113 | Iter: 85200 | Total Loss: 0.003430 | Recon Loss: 0.002945 | Commit Loss: 0.000970 | Perplexity: 1044.326136
2025-09-14 19:48:31,538 Stage: Train 0.5 | Epoch: 113 | Iter: 85400 | Total Loss: 0.003195 | Recon Loss: 0.002707 | Commit Loss: 0.000976 | Perplexity: 1049.652769
2025-09-14 19:48:58,946 Stage: Train 0.5 | Epoch: 113 | Iter: 85600 | Total Loss: 0.003329 | Recon Loss: 0.002835 | Commit Loss: 0.000988 | Perplexity: 1048.839584
2025-09-14 19:49:26,373 Stage: Train 0.5 | Epoch: 113 | Iter: 85800 | Total Loss: 0.003296 | Recon Loss: 0.002809 | Commit Loss: 0.000975 | Perplexity: 1044.922864
Trainning Epoch:  17%|█▋        | 114/665 [3:16:48<15:49:07, 103.35s/it]2025-09-14 19:49:53,943 Stage: Train 0.5 | Epoch: 114 | Iter: 86000 | Total Loss: 0.003333 | Recon Loss: 0.002843 | Commit Loss: 0.000980 | Perplexity: 1046.983079
2025-09-14 19:50:21,447 Stage: Train 0.5 | Epoch: 114 | Iter: 86200 | Total Loss: 0.003287 | Recon Loss: 0.002793 | Commit Loss: 0.000987 | Perplexity: 1047.738798
2025-09-14 19:50:48,944 Stage: Train 0.5 | Epoch: 114 | Iter: 86400 | Total Loss: 0.003335 | Recon Loss: 0.002847 | Commit Loss: 0.000977 | Perplexity: 1047.770770
Trainning Epoch:  17%|█▋        | 115/665 [3:18:32<15:48:48, 103.51s/it]2025-09-14 19:51:16,661 Stage: Train 0.5 | Epoch: 115 | Iter: 86600 | Total Loss: 0.003277 | Recon Loss: 0.002785 | Commit Loss: 0.000986 | Perplexity: 1050.361252
2025-09-14 19:51:44,094 Stage: Train 0.5 | Epoch: 115 | Iter: 86800 | Total Loss: 0.003315 | Recon Loss: 0.002823 | Commit Loss: 0.000984 | Perplexity: 1050.529368
2025-09-14 19:52:11,521 Stage: Train 0.5 | Epoch: 115 | Iter: 87000 | Total Loss: 0.003246 | Recon Loss: 0.002751 | Commit Loss: 0.000989 | Perplexity: 1052.177828
2025-09-14 19:52:38,927 Stage: Train 0.5 | Epoch: 115 | Iter: 87200 | Total Loss: 0.003362 | Recon Loss: 0.002876 | Commit Loss: 0.000973 | Perplexity: 1044.308201
Trainning Epoch:  17%|█▋        | 116/665 [3:20:15<15:46:15, 103.42s/it]2025-09-14 19:53:06,305 Stage: Train 0.5 | Epoch: 116 | Iter: 87400 | Total Loss: 0.003271 | Recon Loss: 0.002785 | Commit Loss: 0.000972 | Perplexity: 1045.377076
2025-09-14 19:53:33,902 Stage: Train 0.5 | Epoch: 116 | Iter: 87600 | Total Loss: 0.003352 | Recon Loss: 0.002863 | Commit Loss: 0.000979 | Perplexity: 1045.011649
2025-09-14 19:54:01,434 Stage: Train 0.5 | Epoch: 116 | Iter: 87800 | Total Loss: 0.003314 | Recon Loss: 0.002824 | Commit Loss: 0.000981 | Perplexity: 1048.529713
2025-09-14 19:54:28,957 Stage: Train 0.5 | Epoch: 116 | Iter: 88000 | Total Loss: 0.003223 | Recon Loss: 0.002735 | Commit Loss: 0.000976 | Perplexity: 1052.160431
Trainning Epoch:  18%|█▊        | 117/665 [3:21:58<15:45:03, 103.47s/it]2025-09-14 19:54:56,388 Stage: Train 0.5 | Epoch: 117 | Iter: 88200 | Total Loss: 0.003492 | Recon Loss: 0.003006 | Commit Loss: 0.000972 | Perplexity: 1041.511367
2025-09-14 19:55:24,220 Stage: Train 0.5 | Epoch: 117 | Iter: 88400 | Total Loss: 0.003173 | Recon Loss: 0.002683 | Commit Loss: 0.000980 | Perplexity: 1051.448758
2025-09-14 19:55:51,746 Stage: Train 0.5 | Epoch: 117 | Iter: 88600 | Total Loss: 0.003330 | Recon Loss: 0.002838 | Commit Loss: 0.000984 | Perplexity: 1048.597655
2025-09-14 19:56:19,265 Stage: Train 0.5 | Epoch: 117 | Iter: 88800 | Total Loss: 0.003329 | Recon Loss: 0.002844 | Commit Loss: 0.000971 | Perplexity: 1046.096352
Trainning Epoch:  18%|█▊        | 118/665 [3:23:42<15:44:18, 103.58s/it]2025-09-14 19:56:46,737 Stage: Train 0.5 | Epoch: 118 | Iter: 89000 | Total Loss: 0.003201 | Recon Loss: 0.002716 | Commit Loss: 0.000971 | Perplexity: 1043.529569
2025-09-14 19:57:14,173 Stage: Train 0.5 | Epoch: 118 | Iter: 89200 | Total Loss: 0.003312 | Recon Loss: 0.002820 | Commit Loss: 0.000984 | Perplexity: 1051.226153
2025-09-14 19:57:41,598 Stage: Train 0.5 | Epoch: 118 | Iter: 89400 | Total Loss: 0.003344 | Recon Loss: 0.002855 | Commit Loss: 0.000979 | Perplexity: 1046.515505
2025-09-14 19:58:09,020 Stage: Train 0.5 | Epoch: 118 | Iter: 89600 | Total Loss: 0.003294 | Recon Loss: 0.002808 | Commit Loss: 0.000972 | Perplexity: 1045.543443
Trainning Epoch:  18%|█▊        | 119/665 [3:25:25<15:41:51, 103.50s/it]2025-09-14 19:58:36,384 Stage: Train 0.5 | Epoch: 119 | Iter: 89800 | Total Loss: 0.003172 | Recon Loss: 0.002691 | Commit Loss: 0.000962 | Perplexity: 1043.825714
2025-09-14 19:59:03,798 Stage: Train 0.5 | Epoch: 119 | Iter: 90000 | Total Loss: 0.003416 | Recon Loss: 0.002925 | Commit Loss: 0.000982 | Perplexity: 1048.012651
2025-09-14 19:59:31,227 Stage: Train 0.5 | Epoch: 119 | Iter: 90200 | Total Loss: 0.003294 | Recon Loss: 0.002805 | Commit Loss: 0.000977 | Perplexity: 1049.105829
Trainning Epoch:  18%|█▊        | 120/665 [3:27:09<15:39:15, 103.41s/it]2025-09-14 19:59:58,594 Stage: Train 0.5 | Epoch: 120 | Iter: 90400 | Total Loss: 0.003326 | Recon Loss: 0.002838 | Commit Loss: 0.000975 | Perplexity: 1044.749174
2025-09-14 20:00:26,112 Stage: Train 0.5 | Epoch: 120 | Iter: 90600 | Total Loss: 0.003272 | Recon Loss: 0.002785 | Commit Loss: 0.000973 | Perplexity: 1049.349972
2025-09-14 20:00:53,523 Stage: Train 0.5 | Epoch: 120 | Iter: 90800 | Total Loss: 0.003279 | Recon Loss: 0.002794 | Commit Loss: 0.000969 | Perplexity: 1045.874258
2025-09-14 20:01:20,934 Stage: Train 0.5 | Epoch: 120 | Iter: 91000 | Total Loss: 0.003362 | Recon Loss: 0.002873 | Commit Loss: 0.000978 | Perplexity: 1045.544584
Trainning Epoch:  18%|█▊        | 121/665 [3:28:52<15:37:07, 103.36s/it]2025-09-14 20:01:48,293 Stage: Train 0.5 | Epoch: 121 | Iter: 91200 | Total Loss: 0.003271 | Recon Loss: 0.002788 | Commit Loss: 0.000966 | Perplexity: 1045.006571
2025-09-14 20:02:15,709 Stage: Train 0.5 | Epoch: 121 | Iter: 91400 | Total Loss: 0.003278 | Recon Loss: 0.002793 | Commit Loss: 0.000970 | Perplexity: 1048.811192
2025-09-14 20:02:43,137 Stage: Train 0.5 | Epoch: 121 | Iter: 91600 | Total Loss: 0.003300 | Recon Loss: 0.002814 | Commit Loss: 0.000973 | Perplexity: 1047.993815
2025-09-14 20:03:10,548 Stage: Train 0.5 | Epoch: 121 | Iter: 91800 | Total Loss: 0.003287 | Recon Loss: 0.002797 | Commit Loss: 0.000980 | Perplexity: 1052.916833
Trainning Epoch:  18%|█▊        | 122/665 [3:30:35<15:34:53, 103.30s/it]2025-09-14 20:03:38,005 Stage: Train 0.5 | Epoch: 122 | Iter: 92000 | Total Loss: 0.003320 | Recon Loss: 0.002834 | Commit Loss: 0.000972 | Perplexity: 1045.077264
2025-09-14 20:04:05,424 Stage: Train 0.5 | Epoch: 122 | Iter: 92200 | Total Loss: 0.003298 | Recon Loss: 0.002816 | Commit Loss: 0.000963 | Perplexity: 1049.073122
2025-09-14 20:04:32,852 Stage: Train 0.5 | Epoch: 122 | Iter: 92400 | Total Loss: 0.003234 | Recon Loss: 0.002747 | Commit Loss: 0.000972 | Perplexity: 1045.834732
2025-09-14 20:05:00,280 Stage: Train 0.5 | Epoch: 122 | Iter: 92600 | Total Loss: 0.003265 | Recon Loss: 0.002778 | Commit Loss: 0.000974 | Perplexity: 1050.847339
Trainning Epoch:  18%|█▊        | 123/665 [3:32:18<15:33:09, 103.30s/it]2025-09-14 20:05:27,675 Stage: Train 0.5 | Epoch: 123 | Iter: 92800 | Total Loss: 0.003217 | Recon Loss: 0.002728 | Commit Loss: 0.000976 | Perplexity: 1049.663315
2025-09-14 20:05:55,099 Stage: Train 0.5 | Epoch: 123 | Iter: 93000 | Total Loss: 0.003176 | Recon Loss: 0.002691 | Commit Loss: 0.000970 | Perplexity: 1047.740266
2025-09-14 20:06:22,606 Stage: Train 0.5 | Epoch: 123 | Iter: 93200 | Total Loss: 0.003288 | Recon Loss: 0.002800 | Commit Loss: 0.000977 | Perplexity: 1052.773171
Trainning Epoch:  19%|█▊        | 124/665 [3:34:02<15:31:43, 103.33s/it]2025-09-14 20:06:50,201 Stage: Train 0.5 | Epoch: 124 | Iter: 93400 | Total Loss: 0.003295 | Recon Loss: 0.002813 | Commit Loss: 0.000965 | Perplexity: 1044.602096
2025-09-14 20:07:17,735 Stage: Train 0.5 | Epoch: 124 | Iter: 93600 | Total Loss: 0.003186 | Recon Loss: 0.002698 | Commit Loss: 0.000974 | Perplexity: 1047.171843
2025-09-14 20:07:45,215 Stage: Train 0.5 | Epoch: 124 | Iter: 93800 | Total Loss: 0.003258 | Recon Loss: 0.002769 | Commit Loss: 0.000978 | Perplexity: 1048.405191
2025-09-14 20:08:13,013 Stage: Train 0.5 | Epoch: 124 | Iter: 94000 | Total Loss: 0.003370 | Recon Loss: 0.002889 | Commit Loss: 0.000964 | Perplexity: 1048.598154
Trainning Epoch:  19%|█▉        | 125/665 [3:35:46<15:31:32, 103.50s/it]2025-09-14 20:08:40,435 Stage: Train 0.5 | Epoch: 125 | Iter: 94200 | Total Loss: 0.003202 | Recon Loss: 0.002717 | Commit Loss: 0.000970 | Perplexity: 1045.629054
2025-09-14 20:09:07,820 Stage: Train 0.5 | Epoch: 125 | Iter: 94400 | Total Loss: 0.003227 | Recon Loss: 0.002745 | Commit Loss: 0.000965 | Perplexity: 1044.332611
2025-09-14 20:09:35,272 Stage: Train 0.5 | Epoch: 125 | Iter: 94600 | Total Loss: 0.003222 | Recon Loss: 0.002742 | Commit Loss: 0.000961 | Perplexity: 1049.579641
2025-09-14 20:10:02,868 Stage: Train 0.5 | Epoch: 125 | Iter: 94800 | Total Loss: 0.003297 | Recon Loss: 0.002808 | Commit Loss: 0.000978 | Perplexity: 1051.949958
Trainning Epoch:  19%|█▉        | 126/665 [3:37:29<15:29:27, 103.47s/it]2025-09-14 20:10:30,399 Stage: Train 0.5 | Epoch: 126 | Iter: 95000 | Total Loss: 0.003219 | Recon Loss: 0.002742 | Commit Loss: 0.000953 | Perplexity: 1044.342000
2025-09-14 20:10:57,878 Stage: Train 0.5 | Epoch: 126 | Iter: 95200 | Total Loss: 0.003186 | Recon Loss: 0.002697 | Commit Loss: 0.000979 | Perplexity: 1053.342752
2025-09-14 20:11:25,283 Stage: Train 0.5 | Epoch: 126 | Iter: 95400 | Total Loss: 0.003267 | Recon Loss: 0.002785 | Commit Loss: 0.000962 | Perplexity: 1045.408404
2025-09-14 20:11:52,766 Stage: Train 0.5 | Epoch: 126 | Iter: 95600 | Total Loss: 0.003191 | Recon Loss: 0.002703 | Commit Loss: 0.000977 | Perplexity: 1056.199131
Trainning Epoch:  19%|█▉        | 127/665 [3:39:13<15:27:44, 103.47s/it]2025-09-14 20:12:20,202 Stage: Train 0.5 | Epoch: 127 | Iter: 95800 | Total Loss: 0.003214 | Recon Loss: 0.002734 | Commit Loss: 0.000961 | Perplexity: 1043.950471
2025-09-14 20:12:47,994 Stage: Train 0.5 | Epoch: 127 | Iter: 96000 | Total Loss: 0.003225 | Recon Loss: 0.002740 | Commit Loss: 0.000970 | Perplexity: 1049.868362
2025-09-14 20:13:15,499 Stage: Train 0.5 | Epoch: 127 | Iter: 96200 | Total Loss: 0.003298 | Recon Loss: 0.002816 | Commit Loss: 0.000964 | Perplexity: 1050.340363
Trainning Epoch:  19%|█▉        | 128/665 [3:40:56<15:26:50, 103.56s/it]2025-09-14 20:13:42,961 Stage: Train 0.5 | Epoch: 128 | Iter: 96400 | Total Loss: 0.003190 | Recon Loss: 0.002705 | Commit Loss: 0.000972 | Perplexity: 1050.431068
2025-09-14 20:14:10,445 Stage: Train 0.5 | Epoch: 128 | Iter: 96600 | Total Loss: 0.003252 | Recon Loss: 0.002771 | Commit Loss: 0.000962 | Perplexity: 1050.477926
2025-09-14 20:14:37,925 Stage: Train 0.5 | Epoch: 128 | Iter: 96800 | Total Loss: 0.003190 | Recon Loss: 0.002709 | Commit Loss: 0.000963 | Perplexity: 1050.190116
2025-09-14 20:15:05,407 Stage: Train 0.5 | Epoch: 128 | Iter: 97000 | Total Loss: 0.003241 | Recon Loss: 0.002761 | Commit Loss: 0.000961 | Perplexity: 1048.814192
Trainning Epoch:  19%|█▉        | 129/665 [3:42:40<15:25:02, 103.55s/it]2025-09-14 20:15:33,076 Stage: Train 0.5 | Epoch: 129 | Iter: 97200 | Total Loss: 0.003241 | Recon Loss: 0.002760 | Commit Loss: 0.000962 | Perplexity: 1049.750886
2025-09-14 20:16:00,736 Stage: Train 0.5 | Epoch: 129 | Iter: 97400 | Total Loss: 0.003159 | Recon Loss: 0.002679 | Commit Loss: 0.000960 | Perplexity: 1052.537524
2025-09-14 20:16:28,287 Stage: Train 0.5 | Epoch: 129 | Iter: 97600 | Total Loss: 0.003223 | Recon Loss: 0.002742 | Commit Loss: 0.000962 | Perplexity: 1046.885909
2025-09-14 20:16:55,967 Stage: Train 0.5 | Epoch: 129 | Iter: 97800 | Total Loss: 0.003251 | Recon Loss: 0.002772 | Commit Loss: 0.000959 | Perplexity: 1049.305480
Trainning Epoch:  20%|█▉        | 130/665 [3:44:24<15:24:48, 103.72s/it]2025-09-14 20:17:23,559 Stage: Train 0.5 | Epoch: 130 | Iter: 98000 | Total Loss: 0.003175 | Recon Loss: 0.002699 | Commit Loss: 0.000952 | Perplexity: 1048.436211
2025-09-14 20:17:51,082 Stage: Train 0.5 | Epoch: 130 | Iter: 98200 | Total Loss: 0.003125 | Recon Loss: 0.002645 | Commit Loss: 0.000960 | Perplexity: 1055.732903
2025-09-14 20:18:18,560 Stage: Train 0.5 | Epoch: 130 | Iter: 98400 | Total Loss: 0.003176 | Recon Loss: 0.002693 | Commit Loss: 0.000966 | Perplexity: 1051.689826
2025-09-14 20:18:46,028 Stage: Train 0.5 | Epoch: 130 | Iter: 98600 | Total Loss: 0.003139 | Recon Loss: 0.002655 | Commit Loss: 0.000968 | Perplexity: 1051.636479
Trainning Epoch:  20%|█▉        | 131/665 [3:46:07<15:22:26, 103.65s/it]2025-09-14 20:19:13,495 Stage: Train 0.5 | Epoch: 131 | Iter: 98800 | Total Loss: 0.003268 | Recon Loss: 0.002792 | Commit Loss: 0.000952 | Perplexity: 1043.430341
2025-09-14 20:19:41,371 Stage: Train 0.5 | Epoch: 131 | Iter: 99000 | Total Loss: 0.003132 | Recon Loss: 0.002653 | Commit Loss: 0.000956 | Perplexity: 1052.713814
2025-09-14 20:20:09,113 Stage: Train 0.5 | Epoch: 131 | Iter: 99200 | Total Loss: 0.003170 | Recon Loss: 0.002690 | Commit Loss: 0.000959 | Perplexity: 1049.011223
Trainning Epoch:  20%|█▉        | 132/665 [3:47:52<15:22:21, 103.83s/it]2025-09-14 20:20:36,692 Stage: Train 0.5 | Epoch: 132 | Iter: 99400 | Total Loss: 0.003281 | Recon Loss: 0.002801 | Commit Loss: 0.000960 | Perplexity: 1048.900636
2025-09-14 20:21:04,183 Stage: Train 0.5 | Epoch: 132 | Iter: 99600 | Total Loss: 0.003084 | Recon Loss: 0.002609 | Commit Loss: 0.000951 | Perplexity: 1054.209355
2025-09-14 20:21:31,843 Stage: Train 0.5 | Epoch: 132 | Iter: 99800 | Total Loss: 0.003244 | Recon Loss: 0.002760 | Commit Loss: 0.000969 | Perplexity: 1053.034444
2025-09-14 20:21:59,634 Stage: Train 0.5 | Epoch: 132 | Iter: 100000 | Total Loss: 0.003159 | Recon Loss: 0.002682 | Commit Loss: 0.000954 | Perplexity: 1046.457057
2025-09-14 20:21:59,634 Saving model at iteration 100000
2025-09-14 20:22:00,217 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_133_step_100000
2025-09-14 20:22:00,424 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_133_step_100000/pytorch_model.bin
2025-09-14 20:22:00,747 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_133_step_100000/optimizer.bin
2025-09-14 20:22:00,747 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_133_step_100000/scheduler.bin
2025-09-14 20:22:00,748 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_133_step_100000/random_states_0.pkl
Trainning Epoch:  20%|██        | 133/665 [3:49:37<15:25:18, 104.36s/it]2025-09-14 20:22:28,794 Stage: Train 0.5 | Epoch: 133 | Iter: 100200 | Total Loss: 0.003145 | Recon Loss: 0.002666 | Commit Loss: 0.000958 | Perplexity: 1043.216487
2025-09-14 20:22:56,477 Stage: Train 0.5 | Epoch: 133 | Iter: 100400 | Total Loss: 0.003210 | Recon Loss: 0.002731 | Commit Loss: 0.000958 | Perplexity: 1048.935354
2025-09-14 20:23:24,085 Stage: Train 0.5 | Epoch: 133 | Iter: 100600 | Total Loss: 0.003165 | Recon Loss: 0.002685 | Commit Loss: 0.000961 | Perplexity: 1048.223684
2025-09-14 20:23:51,694 Stage: Train 0.5 | Epoch: 133 | Iter: 100800 | Total Loss: 0.003198 | Recon Loss: 0.002728 | Commit Loss: 0.000939 | Perplexity: 1043.418420
Trainning Epoch:  20%|██        | 134/665 [3:51:21<15:22:20, 104.22s/it]2025-09-14 20:24:19,059 Stage: Train 0.5 | Epoch: 134 | Iter: 101000 | Total Loss: 0.003098 | Recon Loss: 0.002619 | Commit Loss: 0.000957 | Perplexity: 1046.113352
2025-09-14 20:24:46,589 Stage: Train 0.5 | Epoch: 134 | Iter: 101200 | Total Loss: 0.003243 | Recon Loss: 0.002767 | Commit Loss: 0.000954 | Perplexity: 1048.806399
2025-09-14 20:25:14,113 Stage: Train 0.5 | Epoch: 134 | Iter: 101400 | Total Loss: 0.003240 | Recon Loss: 0.002769 | Commit Loss: 0.000942 | Perplexity: 1043.126316
2025-09-14 20:25:41,641 Stage: Train 0.5 | Epoch: 134 | Iter: 101600 | Total Loss: 0.003113 | Recon Loss: 0.002635 | Commit Loss: 0.000955 | Perplexity: 1042.408687
Trainning Epoch:  20%|██        | 135/665 [3:53:05<15:18:52, 104.02s/it]2025-09-14 20:26:09,133 Stage: Train 0.5 | Epoch: 135 | Iter: 101800 | Total Loss: 0.003112 | Recon Loss: 0.002637 | Commit Loss: 0.000950 | Perplexity: 1044.935044
2025-09-14 20:26:36,761 Stage: Train 0.5 | Epoch: 135 | Iter: 102000 | Total Loss: 0.003179 | Recon Loss: 0.002707 | Commit Loss: 0.000943 | Perplexity: 1045.181447
2025-09-14 20:27:04,417 Stage: Train 0.5 | Epoch: 135 | Iter: 102200 | Total Loss: 0.003121 | Recon Loss: 0.002640 | Commit Loss: 0.000961 | Perplexity: 1050.589404
2025-09-14 20:27:32,054 Stage: Train 0.5 | Epoch: 135 | Iter: 102400 | Total Loss: 0.003164 | Recon Loss: 0.002681 | Commit Loss: 0.000966 | Perplexity: 1042.085540
Trainning Epoch:  20%|██        | 136/665 [3:54:49<15:16:51, 103.99s/it]2025-09-14 20:27:59,646 Stage: Train 0.5 | Epoch: 136 | Iter: 102600 | Total Loss: 0.003158 | Recon Loss: 0.002686 | Commit Loss: 0.000944 | Perplexity: 1042.296227
2025-09-14 20:28:27,336 Stage: Train 0.5 | Epoch: 136 | Iter: 102800 | Total Loss: 0.003162 | Recon Loss: 0.002684 | Commit Loss: 0.000956 | Perplexity: 1046.594591
2025-09-14 20:28:55,214 Stage: Train 0.5 | Epoch: 136 | Iter: 103000 | Total Loss: 0.003138 | Recon Loss: 0.002662 | Commit Loss: 0.000952 | Perplexity: 1046.336199
Trainning Epoch:  21%|██        | 137/665 [3:56:33<15:16:35, 104.16s/it]2025-09-14 20:29:23,083 Stage: Train 0.5 | Epoch: 137 | Iter: 103200 | Total Loss: 0.003143 | Recon Loss: 0.002664 | Commit Loss: 0.000957 | Perplexity: 1046.937656
2025-09-14 20:29:50,689 Stage: Train 0.5 | Epoch: 137 | Iter: 103400 | Total Loss: 0.003113 | Recon Loss: 0.002638 | Commit Loss: 0.000952 | Perplexity: 1041.756244
2025-09-14 20:30:18,369 Stage: Train 0.5 | Epoch: 137 | Iter: 103600 | Total Loss: 0.003096 | Recon Loss: 0.002624 | Commit Loss: 0.000945 | Perplexity: 1047.942671
2025-09-14 20:30:46,010 Stage: Train 0.5 | Epoch: 137 | Iter: 103800 | Total Loss: 0.003228 | Recon Loss: 0.002754 | Commit Loss: 0.000949 | Perplexity: 1044.697647
Trainning Epoch:  21%|██        | 138/665 [3:58:17<15:14:25, 104.11s/it]2025-09-14 20:31:13,492 Stage: Train 0.5 | Epoch: 138 | Iter: 104000 | Total Loss: 0.003117 | Recon Loss: 0.002638 | Commit Loss: 0.000957 | Perplexity: 1045.895294
2025-09-14 20:31:40,974 Stage: Train 0.5 | Epoch: 138 | Iter: 104200 | Total Loss: 0.003142 | Recon Loss: 0.002666 | Commit Loss: 0.000953 | Perplexity: 1048.771031
2025-09-14 20:32:08,455 Stage: Train 0.5 | Epoch: 138 | Iter: 104400 | Total Loss: 0.003174 | Recon Loss: 0.002697 | Commit Loss: 0.000953 | Perplexity: 1047.129865
2025-09-14 20:32:36,054 Stage: Train 0.5 | Epoch: 138 | Iter: 104600 | Total Loss: 0.003121 | Recon Loss: 0.002647 | Commit Loss: 0.000948 | Perplexity: 1047.923631
Trainning Epoch:  21%|██        | 139/665 [4:00:01<15:11:15, 103.95s/it]2025-09-14 20:33:03,500 Stage: Train 0.5 | Epoch: 139 | Iter: 104800 | Total Loss: 0.003099 | Recon Loss: 0.002626 | Commit Loss: 0.000945 | Perplexity: 1045.643849
2025-09-14 20:33:30,944 Stage: Train 0.5 | Epoch: 139 | Iter: 105000 | Total Loss: 0.003017 | Recon Loss: 0.002542 | Commit Loss: 0.000950 | Perplexity: 1045.793803
2025-09-14 20:33:58,450 Stage: Train 0.5 | Epoch: 139 | Iter: 105200 | Total Loss: 0.003182 | Recon Loss: 0.002703 | Commit Loss: 0.000957 | Perplexity: 1049.171558
2025-09-14 20:34:25,939 Stage: Train 0.5 | Epoch: 139 | Iter: 105400 | Total Loss: 0.003210 | Recon Loss: 0.002732 | Commit Loss: 0.000957 | Perplexity: 1046.698586
Trainning Epoch:  21%|██        | 140/665 [4:01:44<15:08:10, 103.79s/it]2025-09-14 20:34:53,484 Stage: Train 0.5 | Epoch: 140 | Iter: 105600 | Total Loss: 0.003132 | Recon Loss: 0.002660 | Commit Loss: 0.000944 | Perplexity: 1047.084039
2025-09-14 20:35:21,019 Stage: Train 0.5 | Epoch: 140 | Iter: 105800 | Total Loss: 0.003138 | Recon Loss: 0.002664 | Commit Loss: 0.000948 | Perplexity: 1047.468803
2025-09-14 20:35:48,631 Stage: Train 0.5 | Epoch: 140 | Iter: 106000 | Total Loss: 0.003216 | Recon Loss: 0.002743 | Commit Loss: 0.000947 | Perplexity: 1044.799444
Trainning Epoch:  21%|██        | 141/665 [4:03:28<15:06:27, 103.79s/it]2025-09-14 20:36:16,162 Stage: Train 0.5 | Epoch: 141 | Iter: 106200 | Total Loss: 0.003105 | Recon Loss: 0.002633 | Commit Loss: 0.000945 | Perplexity: 1038.908207
2025-09-14 20:36:43,637 Stage: Train 0.5 | Epoch: 141 | Iter: 106400 | Total Loss: 0.003103 | Recon Loss: 0.002630 | Commit Loss: 0.000945 | Perplexity: 1041.344575
2025-09-14 20:37:11,109 Stage: Train 0.5 | Epoch: 141 | Iter: 106600 | Total Loss: 0.003220 | Recon Loss: 0.002748 | Commit Loss: 0.000945 | Perplexity: 1048.417881
2025-09-14 20:37:38,558 Stage: Train 0.5 | Epoch: 141 | Iter: 106800 | Total Loss: 0.003074 | Recon Loss: 0.002603 | Commit Loss: 0.000943 | Perplexity: 1048.141976
Trainning Epoch:  21%|██▏       | 142/665 [4:05:11<15:03:39, 103.67s/it]2025-09-14 20:38:05,984 Stage: Train 0.5 | Epoch: 142 | Iter: 107000 | Total Loss: 0.003217 | Recon Loss: 0.002741 | Commit Loss: 0.000952 | Perplexity: 1046.574796
2025-09-14 20:38:33,433 Stage: Train 0.5 | Epoch: 142 | Iter: 107200 | Total Loss: 0.003109 | Recon Loss: 0.002632 | Commit Loss: 0.000953 | Perplexity: 1049.825518
2025-09-14 20:39:00,871 Stage: Train 0.5 | Epoch: 142 | Iter: 107400 | Total Loss: 0.003169 | Recon Loss: 0.002699 | Commit Loss: 0.000939 | Perplexity: 1044.301850
2025-09-14 20:39:28,646 Stage: Train 0.5 | Epoch: 142 | Iter: 107600 | Total Loss: 0.003047 | Recon Loss: 0.002573 | Commit Loss: 0.000949 | Perplexity: 1046.136155
Trainning Epoch:  22%|██▏       | 143/665 [4:06:55<15:01:43, 103.65s/it]2025-09-14 20:39:56,007 Stage: Train 0.5 | Epoch: 143 | Iter: 107800 | Total Loss: 0.003147 | Recon Loss: 0.002677 | Commit Loss: 0.000939 | Perplexity: 1046.793934
2025-09-14 20:40:23,415 Stage: Train 0.5 | Epoch: 143 | Iter: 108000 | Total Loss: 0.003158 | Recon Loss: 0.002684 | Commit Loss: 0.000949 | Perplexity: 1045.953630
2025-09-14 20:40:50,852 Stage: Train 0.5 | Epoch: 143 | Iter: 108200 | Total Loss: 0.003083 | Recon Loss: 0.002614 | Commit Loss: 0.000938 | Perplexity: 1043.668840
2025-09-14 20:41:18,367 Stage: Train 0.5 | Epoch: 143 | Iter: 108400 | Total Loss: 0.003111 | Recon Loss: 0.002639 | Commit Loss: 0.000944 | Perplexity: 1048.575369
Trainning Epoch:  22%|██▏       | 144/665 [4:08:38<14:59:04, 103.54s/it]2025-09-14 20:41:45,759 Stage: Train 0.5 | Epoch: 144 | Iter: 108600 | Total Loss: 0.003050 | Recon Loss: 0.002583 | Commit Loss: 0.000935 | Perplexity: 1048.000045
2025-09-14 20:42:13,174 Stage: Train 0.5 | Epoch: 144 | Iter: 108800 | Total Loss: 0.003131 | Recon Loss: 0.002662 | Commit Loss: 0.000939 | Perplexity: 1042.150597
2025-09-14 20:42:40,652 Stage: Train 0.5 | Epoch: 144 | Iter: 109000 | Total Loss: 0.003177 | Recon Loss: 0.002700 | Commit Loss: 0.000953 | Perplexity: 1045.811934
Trainning Epoch:  22%|██▏       | 145/665 [4:10:21<14:56:27, 103.44s/it]2025-09-14 20:43:07,960 Stage: Train 0.5 | Epoch: 145 | Iter: 109200 | Total Loss: 0.003209 | Recon Loss: 0.002734 | Commit Loss: 0.000950 | Perplexity: 1044.765823
2025-09-14 20:43:35,598 Stage: Train 0.5 | Epoch: 145 | Iter: 109400 | Total Loss: 0.003069 | Recon Loss: 0.002596 | Commit Loss: 0.000946 | Perplexity: 1046.690969
2025-09-14 20:44:03,052 Stage: Train 0.5 | Epoch: 145 | Iter: 109600 | Total Loss: 0.003054 | Recon Loss: 0.002583 | Commit Loss: 0.000941 | Perplexity: 1044.639647
2025-09-14 20:44:30,506 Stage: Train 0.5 | Epoch: 145 | Iter: 109800 | Total Loss: 0.003063 | Recon Loss: 0.002591 | Commit Loss: 0.000944 | Perplexity: 1045.156169
Trainning Epoch:  22%|██▏       | 146/665 [4:12:05<14:54:51, 103.45s/it]2025-09-14 20:44:57,914 Stage: Train 0.5 | Epoch: 146 | Iter: 110000 | Total Loss: 0.003179 | Recon Loss: 0.002709 | Commit Loss: 0.000940 | Perplexity: 1051.721985
2025-09-14 20:45:25,373 Stage: Train 0.5 | Epoch: 146 | Iter: 110200 | Total Loss: 0.003076 | Recon Loss: 0.002606 | Commit Loss: 0.000941 | Perplexity: 1047.607831
2025-09-14 20:45:52,790 Stage: Train 0.5 | Epoch: 146 | Iter: 110400 | Total Loss: 0.003021 | Recon Loss: 0.002549 | Commit Loss: 0.000945 | Perplexity: 1052.670593
2025-09-14 20:46:20,323 Stage: Train 0.5 | Epoch: 146 | Iter: 110600 | Total Loss: 0.003081 | Recon Loss: 0.002608 | Commit Loss: 0.000947 | Perplexity: 1046.438443
Trainning Epoch:  22%|██▏       | 147/665 [4:13:48<14:52:51, 103.42s/it]2025-09-14 20:46:47,688 Stage: Train 0.5 | Epoch: 147 | Iter: 110800 | Total Loss: 0.003203 | Recon Loss: 0.002730 | Commit Loss: 0.000945 | Perplexity: 1043.114295
2025-09-14 20:47:15,240 Stage: Train 0.5 | Epoch: 147 | Iter: 111000 | Total Loss: 0.003048 | Recon Loss: 0.002580 | Commit Loss: 0.000936 | Perplexity: 1043.373665
2025-09-14 20:47:42,655 Stage: Train 0.5 | Epoch: 147 | Iter: 111200 | Total Loss: 0.003069 | Recon Loss: 0.002595 | Commit Loss: 0.000947 | Perplexity: 1051.231679
2025-09-14 20:48:10,081 Stage: Train 0.5 | Epoch: 147 | Iter: 111400 | Total Loss: 0.003122 | Recon Loss: 0.002657 | Commit Loss: 0.000930 | Perplexity: 1044.616464
Trainning Epoch:  22%|██▏       | 148/665 [4:15:32<14:50:56, 103.40s/it]2025-09-14 20:48:37,560 Stage: Train 0.5 | Epoch: 148 | Iter: 111600 | Total Loss: 0.003173 | Recon Loss: 0.002700 | Commit Loss: 0.000945 | Perplexity: 1051.808834
2025-09-14 20:49:05,073 Stage: Train 0.5 | Epoch: 148 | Iter: 111800 | Total Loss: 0.003015 | Recon Loss: 0.002545 | Commit Loss: 0.000939 | Perplexity: 1043.648134
2025-09-14 20:49:32,702 Stage: Train 0.5 | Epoch: 148 | Iter: 112000 | Total Loss: 0.003094 | Recon Loss: 0.002622 | Commit Loss: 0.000943 | Perplexity: 1048.082613
Trainning Epoch:  22%|██▏       | 149/665 [4:17:15<14:50:08, 103.51s/it]2025-09-14 20:50:00,258 Stage: Train 0.5 | Epoch: 149 | Iter: 112200 | Total Loss: 0.003054 | Recon Loss: 0.002582 | Commit Loss: 0.000944 | Perplexity: 1047.761434
2025-09-14 20:50:27,742 Stage: Train 0.5 | Epoch: 149 | Iter: 112400 | Total Loss: 0.003204 | Recon Loss: 0.002737 | Commit Loss: 0.000934 | Perplexity: 1044.109635
2025-09-14 20:50:55,218 Stage: Train 0.5 | Epoch: 149 | Iter: 112600 | Total Loss: 0.002987 | Recon Loss: 0.002512 | Commit Loss: 0.000948 | Perplexity: 1052.893416
2025-09-14 20:51:22,709 Stage: Train 0.5 | Epoch: 149 | Iter: 112800 | Total Loss: 0.003237 | Recon Loss: 0.002769 | Commit Loss: 0.000938 | Perplexity: 1043.868286
Trainning Epoch:  23%|██▎       | 150/665 [4:18:59<14:48:10, 103.48s/it]2025-09-14 20:51:50,111 Stage: Train 0.5 | Epoch: 150 | Iter: 113000 | Total Loss: 0.003018 | Recon Loss: 0.002550 | Commit Loss: 0.000936 | Perplexity: 1049.155121
2025-09-14 20:52:17,540 Stage: Train 0.5 | Epoch: 150 | Iter: 113200 | Total Loss: 0.003083 | Recon Loss: 0.002615 | Commit Loss: 0.000937 | Perplexity: 1048.927294
2025-09-14 20:52:45,196 Stage: Train 0.5 | Epoch: 150 | Iter: 113400 | Total Loss: 0.003128 | Recon Loss: 0.002662 | Commit Loss: 0.000934 | Perplexity: 1043.830198
2025-09-14 20:53:12,754 Stage: Train 0.5 | Epoch: 150 | Iter: 113600 | Total Loss: 0.003058 | Recon Loss: 0.002586 | Commit Loss: 0.000944 | Perplexity: 1047.843908
Trainning Epoch:  23%|██▎       | 151/665 [4:20:42<14:46:57, 103.54s/it]2025-09-14 20:53:40,269 Stage: Train 0.5 | Epoch: 151 | Iter: 113800 | Total Loss: 0.003134 | Recon Loss: 0.002662 | Commit Loss: 0.000944 | Perplexity: 1046.511088
2025-09-14 20:54:07,899 Stage: Train 0.5 | Epoch: 151 | Iter: 114000 | Total Loss: 0.002992 | Recon Loss: 0.002525 | Commit Loss: 0.000934 | Perplexity: 1049.758314
2025-09-14 20:54:35,536 Stage: Train 0.5 | Epoch: 151 | Iter: 114200 | Total Loss: 0.003075 | Recon Loss: 0.002606 | Commit Loss: 0.000937 | Perplexity: 1046.585002
2025-09-14 20:55:03,144 Stage: Train 0.5 | Epoch: 151 | Iter: 114400 | Total Loss: 0.003001 | Recon Loss: 0.002532 | Commit Loss: 0.000938 | Perplexity: 1052.818981
Trainning Epoch:  23%|██▎       | 152/665 [4:22:26<14:46:09, 103.64s/it]2025-09-14 20:55:30,692 Stage: Train 0.5 | Epoch: 152 | Iter: 114600 | Total Loss: 0.003189 | Recon Loss: 0.002720 | Commit Loss: 0.000939 | Perplexity: 1048.371714
2025-09-14 20:55:58,241 Stage: Train 0.5 | Epoch: 152 | Iter: 114800 | Total Loss: 0.003092 | Recon Loss: 0.002626 | Commit Loss: 0.000932 | Perplexity: 1047.548931
2025-09-14 20:56:25,908 Stage: Train 0.5 | Epoch: 152 | Iter: 115000 | Total Loss: 0.003072 | Recon Loss: 0.002608 | Commit Loss: 0.000928 | Perplexity: 1045.562367
2025-09-14 20:56:53,489 Stage: Train 0.5 | Epoch: 152 | Iter: 115200 | Total Loss: 0.003032 | Recon Loss: 0.002557 | Commit Loss: 0.000951 | Perplexity: 1053.357747
Trainning Epoch:  23%|██▎       | 153/665 [4:24:10<14:44:57, 103.71s/it]2025-09-14 20:57:20,967 Stage: Train 0.5 | Epoch: 153 | Iter: 115400 | Total Loss: 0.003107 | Recon Loss: 0.002641 | Commit Loss: 0.000933 | Perplexity: 1043.946846
2025-09-14 20:57:48,514 Stage: Train 0.5 | Epoch: 153 | Iter: 115600 | Total Loss: 0.003032 | Recon Loss: 0.002563 | Commit Loss: 0.000939 | Perplexity: 1051.454515
2025-09-14 20:58:16,074 Stage: Train 0.5 | Epoch: 153 | Iter: 115800 | Total Loss: 0.003019 | Recon Loss: 0.002552 | Commit Loss: 0.000934 | Perplexity: 1049.977019
Trainning Epoch:  23%|██▎       | 154/665 [4:25:54<14:43:04, 103.69s/it]2025-09-14 20:58:43,555 Stage: Train 0.5 | Epoch: 154 | Iter: 116000 | Total Loss: 0.003090 | Recon Loss: 0.002621 | Commit Loss: 0.000937 | Perplexity: 1047.719248
2025-09-14 20:59:11,088 Stage: Train 0.5 | Epoch: 154 | Iter: 116200 | Total Loss: 0.003068 | Recon Loss: 0.002601 | Commit Loss: 0.000935 | Perplexity: 1050.137187
2025-09-14 20:59:38,753 Stage: Train 0.5 | Epoch: 154 | Iter: 116400 | Total Loss: 0.003083 | Recon Loss: 0.002617 | Commit Loss: 0.000932 | Perplexity: 1049.741063
2025-09-14 21:00:06,294 Stage: Train 0.5 | Epoch: 154 | Iter: 116600 | Total Loss: 0.002979 | Recon Loss: 0.002516 | Commit Loss: 0.000927 | Perplexity: 1047.157339
Trainning Epoch:  23%|██▎       | 155/665 [4:27:38<14:41:36, 103.72s/it]2025-09-14 21:00:33,816 Stage: Train 0.5 | Epoch: 155 | Iter: 116800 | Total Loss: 0.003017 | Recon Loss: 0.002549 | Commit Loss: 0.000935 | Perplexity: 1050.215526
2025-09-14 21:01:01,372 Stage: Train 0.5 | Epoch: 155 | Iter: 117000 | Total Loss: 0.003019 | Recon Loss: 0.002555 | Commit Loss: 0.000928 | Perplexity: 1047.272929
2025-09-14 21:01:28,934 Stage: Train 0.5 | Epoch: 155 | Iter: 117200 | Total Loss: 0.003108 | Recon Loss: 0.002638 | Commit Loss: 0.000939 | Perplexity: 1050.250636
2025-09-14 21:01:56,503 Stage: Train 0.5 | Epoch: 155 | Iter: 117400 | Total Loss: 0.003055 | Recon Loss: 0.002588 | Commit Loss: 0.000934 | Perplexity: 1050.651695
Trainning Epoch:  23%|██▎       | 156/665 [4:29:21<14:39:50, 103.71s/it]2025-09-14 21:02:23,974 Stage: Train 0.5 | Epoch: 156 | Iter: 117600 | Total Loss: 0.002958 | Recon Loss: 0.002485 | Commit Loss: 0.000946 | Perplexity: 1052.377323
2025-09-14 21:02:51,539 Stage: Train 0.5 | Epoch: 156 | Iter: 117800 | Total Loss: 0.002960 | Recon Loss: 0.002493 | Commit Loss: 0.000935 | Perplexity: 1050.598293
2025-09-14 21:03:19,202 Stage: Train 0.5 | Epoch: 156 | Iter: 118000 | Total Loss: 0.003050 | Recon Loss: 0.002581 | Commit Loss: 0.000938 | Perplexity: 1053.191116
2025-09-14 21:03:46,747 Stage: Train 0.5 | Epoch: 156 | Iter: 118200 | Total Loss: 0.003047 | Recon Loss: 0.002587 | Commit Loss: 0.000921 | Perplexity: 1047.430366
Trainning Epoch:  24%|██▎       | 157/665 [4:31:05<14:38:17, 103.74s/it]2025-09-14 21:04:14,271 Stage: Train 0.5 | Epoch: 157 | Iter: 118400 | Total Loss: 0.003002 | Recon Loss: 0.002535 | Commit Loss: 0.000933 | Perplexity: 1052.865072
2025-09-14 21:04:41,863 Stage: Train 0.5 | Epoch: 157 | Iter: 118600 | Total Loss: 0.003008 | Recon Loss: 0.002544 | Commit Loss: 0.000930 | Perplexity: 1048.097090
2025-09-14 21:05:09,424 Stage: Train 0.5 | Epoch: 157 | Iter: 118800 | Total Loss: 0.003089 | Recon Loss: 0.002623 | Commit Loss: 0.000932 | Perplexity: 1048.729228
Trainning Epoch:  24%|██▍       | 158/665 [4:32:49<14:36:38, 103.75s/it]2025-09-14 21:05:36,965 Stage: Train 0.5 | Epoch: 158 | Iter: 119000 | Total Loss: 0.003055 | Recon Loss: 0.002586 | Commit Loss: 0.000939 | Perplexity: 1048.205134
2025-09-14 21:06:04,624 Stage: Train 0.5 | Epoch: 158 | Iter: 119200 | Total Loss: 0.003005 | Recon Loss: 0.002543 | Commit Loss: 0.000925 | Perplexity: 1048.059042
2025-09-14 21:06:32,266 Stage: Train 0.5 | Epoch: 158 | Iter: 119400 | Total Loss: 0.003129 | Recon Loss: 0.002661 | Commit Loss: 0.000936 | Perplexity: 1051.864038
2025-09-14 21:06:59,689 Stage: Train 0.5 | Epoch: 158 | Iter: 119600 | Total Loss: 0.003023 | Recon Loss: 0.002561 | Commit Loss: 0.000924 | Perplexity: 1049.794715
Trainning Epoch:  24%|██▍       | 159/665 [4:34:33<14:34:46, 103.73s/it]2025-09-14 21:07:27,063 Stage: Train 0.5 | Epoch: 159 | Iter: 119800 | Total Loss: 0.003111 | Recon Loss: 0.002643 | Commit Loss: 0.000937 | Perplexity: 1046.521937
2025-09-14 21:07:54,480 Stage: Train 0.5 | Epoch: 159 | Iter: 120000 | Total Loss: 0.003074 | Recon Loss: 0.002608 | Commit Loss: 0.000931 | Perplexity: 1050.216915
2025-09-14 21:07:54,480 Saving model at iteration 120000
2025-09-14 21:07:54,652 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_160_step_120000
2025-09-14 21:07:54,859 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_160_step_120000/pytorch_model.bin
2025-09-14 21:07:55,177 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_160_step_120000/optimizer.bin
2025-09-14 21:07:55,177 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_160_step_120000/scheduler.bin
2025-09-14 21:07:55,178 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_160_step_120000/random_states_0.pkl
2025-09-14 21:08:22,746 Stage: Train 0.5 | Epoch: 159 | Iter: 120200 | Total Loss: 0.003003 | Recon Loss: 0.002541 | Commit Loss: 0.000925 | Perplexity: 1051.289645
2025-09-14 21:08:50,261 Stage: Train 0.5 | Epoch: 159 | Iter: 120400 | Total Loss: 0.002960 | Recon Loss: 0.002495 | Commit Loss: 0.000931 | Perplexity: 1052.350885
Trainning Epoch:  24%|██▍       | 160/665 [4:36:17<14:34:28, 103.90s/it]2025-09-14 21:09:17,952 Stage: Train 0.5 | Epoch: 160 | Iter: 120600 | Total Loss: 0.003093 | Recon Loss: 0.002632 | Commit Loss: 0.000922 | Perplexity: 1045.210716
2025-09-14 21:09:45,502 Stage: Train 0.5 | Epoch: 160 | Iter: 120800 | Total Loss: 0.002957 | Recon Loss: 0.002491 | Commit Loss: 0.000932 | Perplexity: 1053.925430
2025-09-14 21:10:13,115 Stage: Train 0.5 | Epoch: 160 | Iter: 121000 | Total Loss: 0.003024 | Recon Loss: 0.002561 | Commit Loss: 0.000926 | Perplexity: 1049.362245
2025-09-14 21:10:40,574 Stage: Train 0.5 | Epoch: 160 | Iter: 121200 | Total Loss: 0.002947 | Recon Loss: 0.002483 | Commit Loss: 0.000929 | Perplexity: 1054.989894
Trainning Epoch:  24%|██▍       | 161/665 [4:38:01<14:32:17, 103.84s/it]2025-09-14 21:11:08,104 Stage: Train 0.5 | Epoch: 161 | Iter: 121400 | Total Loss: 0.003072 | Recon Loss: 0.002608 | Commit Loss: 0.000927 | Perplexity: 1048.974485
2025-09-14 21:11:35,669 Stage: Train 0.5 | Epoch: 161 | Iter: 121600 | Total Loss: 0.002985 | Recon Loss: 0.002519 | Commit Loss: 0.000932 | Perplexity: 1055.519382
2025-09-14 21:12:03,236 Stage: Train 0.5 | Epoch: 161 | Iter: 121800 | Total Loss: 0.002949 | Recon Loss: 0.002483 | Commit Loss: 0.000931 | Perplexity: 1053.053839
Trainning Epoch:  24%|██▍       | 162/665 [4:39:44<14:30:14, 103.81s/it]2025-09-14 21:12:30,719 Stage: Train 0.5 | Epoch: 162 | Iter: 122000 | Total Loss: 0.003076 | Recon Loss: 0.002615 | Commit Loss: 0.000922 | Perplexity: 1044.816056
2025-09-14 21:12:58,249 Stage: Train 0.5 | Epoch: 162 | Iter: 122200 | Total Loss: 0.003044 | Recon Loss: 0.002579 | Commit Loss: 0.000929 | Perplexity: 1053.535869
2025-09-14 21:13:25,850 Stage: Train 0.5 | Epoch: 162 | Iter: 122400 | Total Loss: 0.002914 | Recon Loss: 0.002452 | Commit Loss: 0.000923 | Perplexity: 1052.384881
2025-09-14 21:13:53,627 Stage: Train 0.5 | Epoch: 162 | Iter: 122600 | Total Loss: 0.003152 | Recon Loss: 0.002690 | Commit Loss: 0.000923 | Perplexity: 1048.629150
Trainning Epoch:  25%|██▍       | 163/665 [4:41:28<14:28:46, 103.84s/it]2025-09-14 21:14:21,086 Stage: Train 0.5 | Epoch: 163 | Iter: 122800 | Total Loss: 0.003088 | Recon Loss: 0.002626 | Commit Loss: 0.000924 | Perplexity: 1047.720080
2025-09-14 21:14:48,779 Stage: Train 0.5 | Epoch: 163 | Iter: 123000 | Total Loss: 0.002959 | Recon Loss: 0.002496 | Commit Loss: 0.000925 | Perplexity: 1055.530937
2025-09-14 21:15:16,335 Stage: Train 0.5 | Epoch: 163 | Iter: 123200 | Total Loss: 0.003034 | Recon Loss: 0.002572 | Commit Loss: 0.000923 | Perplexity: 1054.167716
2025-09-14 21:15:43,924 Stage: Train 0.5 | Epoch: 163 | Iter: 123400 | Total Loss: 0.002955 | Recon Loss: 0.002489 | Commit Loss: 0.000931 | Perplexity: 1051.235858
Trainning Epoch:  25%|██▍       | 164/665 [4:43:12<14:27:04, 103.84s/it]2025-09-14 21:16:11,442 Stage: Train 0.5 | Epoch: 164 | Iter: 123600 | Total Loss: 0.002956 | Recon Loss: 0.002492 | Commit Loss: 0.000928 | Perplexity: 1050.751987
2025-09-14 21:16:38,992 Stage: Train 0.5 | Epoch: 164 | Iter: 123800 | Total Loss: 0.003086 | Recon Loss: 0.002624 | Commit Loss: 0.000926 | Perplexity: 1052.181272
2025-09-14 21:17:06,645 Stage: Train 0.5 | Epoch: 164 | Iter: 124000 | Total Loss: 0.002949 | Recon Loss: 0.002490 | Commit Loss: 0.000918 | Perplexity: 1049.863723
2025-09-14 21:17:34,195 Stage: Train 0.5 | Epoch: 164 | Iter: 124200 | Total Loss: 0.003017 | Recon Loss: 0.002551 | Commit Loss: 0.000933 | Perplexity: 1053.212666
Trainning Epoch:  25%|██▍       | 165/665 [4:44:56<14:25:12, 103.82s/it]2025-09-14 21:18:01,787 Stage: Train 0.5 | Epoch: 165 | Iter: 124400 | Total Loss: 0.003029 | Recon Loss: 0.002572 | Commit Loss: 0.000915 | Perplexity: 1047.335646
2025-09-14 21:18:29,610 Stage: Train 0.5 | Epoch: 165 | Iter: 124600 | Total Loss: 0.002985 | Recon Loss: 0.002520 | Commit Loss: 0.000929 | Perplexity: 1052.478872
2025-09-14 21:18:57,045 Stage: Train 0.5 | Epoch: 165 | Iter: 124800 | Total Loss: 0.002962 | Recon Loss: 0.002499 | Commit Loss: 0.000927 | Perplexity: 1051.779105
Trainning Epoch:  25%|██▍       | 166/665 [4:46:40<14:23:30, 103.83s/it]2025-09-14 21:19:24,448 Stage: Train 0.5 | Epoch: 166 | Iter: 125000 | Total Loss: 0.003013 | Recon Loss: 0.002549 | Commit Loss: 0.000928 | Perplexity: 1052.616996
2025-09-14 21:19:52,005 Stage: Train 0.5 | Epoch: 166 | Iter: 125200 | Total Loss: 0.002995 | Recon Loss: 0.002533 | Commit Loss: 0.000925 | Perplexity: 1050.275251
2025-09-14 21:20:19,476 Stage: Train 0.5 | Epoch: 166 | Iter: 125400 | Total Loss: 0.002968 | Recon Loss: 0.002505 | Commit Loss: 0.000926 | Perplexity: 1055.083533
2025-09-14 21:20:46,916 Stage: Train 0.5 | Epoch: 166 | Iter: 125600 | Total Loss: 0.002990 | Recon Loss: 0.002528 | Commit Loss: 0.000923 | Perplexity: 1051.469849
Trainning Epoch:  25%|██▌       | 167/665 [4:48:23<14:20:48, 103.71s/it]2025-09-14 21:21:14,329 Stage: Train 0.5 | Epoch: 167 | Iter: 125800 | Total Loss: 0.002971 | Recon Loss: 0.002508 | Commit Loss: 0.000925 | Perplexity: 1052.491858
2025-09-14 21:21:41,869 Stage: Train 0.5 | Epoch: 167 | Iter: 126000 | Total Loss: 0.002946 | Recon Loss: 0.002483 | Commit Loss: 0.000924 | Perplexity: 1049.943457
2025-09-14 21:22:09,308 Stage: Train 0.5 | Epoch: 167 | Iter: 126200 | Total Loss: 0.003003 | Recon Loss: 0.002538 | Commit Loss: 0.000931 | Perplexity: 1056.554340
2025-09-14 21:22:36,790 Stage: Train 0.5 | Epoch: 167 | Iter: 126400 | Total Loss: 0.002908 | Recon Loss: 0.002449 | Commit Loss: 0.000919 | Perplexity: 1051.756904
Trainning Epoch:  25%|██▌       | 168/665 [4:50:07<14:18:22, 103.63s/it]2025-09-14 21:23:04,257 Stage: Train 0.5 | Epoch: 168 | Iter: 126600 | Total Loss: 0.002983 | Recon Loss: 0.002523 | Commit Loss: 0.000920 | Perplexity: 1048.501588
2025-09-14 21:23:31,809 Stage: Train 0.5 | Epoch: 168 | Iter: 126800 | Total Loss: 0.003023 | Recon Loss: 0.002564 | Commit Loss: 0.000917 | Perplexity: 1050.982844
2025-09-14 21:23:59,260 Stage: Train 0.5 | Epoch: 168 | Iter: 127000 | Total Loss: 0.002918 | Recon Loss: 0.002458 | Commit Loss: 0.000919 | Perplexity: 1056.090716
2025-09-14 21:24:26,768 Stage: Train 0.5 | Epoch: 168 | Iter: 127200 | Total Loss: 0.003049 | Recon Loss: 0.002585 | Commit Loss: 0.000929 | Perplexity: 1053.795256
Trainning Epoch:  25%|██▌       | 169/665 [4:51:50<14:16:23, 103.60s/it]2025-09-14 21:24:54,265 Stage: Train 0.5 | Epoch: 169 | Iter: 127400 | Total Loss: 0.002945 | Recon Loss: 0.002486 | Commit Loss: 0.000917 | Perplexity: 1049.942175
2025-09-14 21:25:21,922 Stage: Train 0.5 | Epoch: 169 | Iter: 127600 | Total Loss: 0.002977 | Recon Loss: 0.002517 | Commit Loss: 0.000919 | Perplexity: 1053.033174
2025-09-14 21:25:49,405 Stage: Train 0.5 | Epoch: 169 | Iter: 127800 | Total Loss: 0.002994 | Recon Loss: 0.002533 | Commit Loss: 0.000922 | Perplexity: 1053.226245
2025-09-14 21:26:16,847 Stage: Train 0.5 | Epoch: 169 | Iter: 128000 | Total Loss: 0.002986 | Recon Loss: 0.002526 | Commit Loss: 0.000919 | Perplexity: 1053.263325
Trainning Epoch:  26%|██▌       | 170/665 [4:53:34<14:14:42, 103.60s/it]2025-09-14 21:26:44,241 Stage: Train 0.5 | Epoch: 170 | Iter: 128200 | Total Loss: 0.002986 | Recon Loss: 0.002529 | Commit Loss: 0.000913 | Perplexity: 1051.290388
2025-09-14 21:27:11,824 Stage: Train 0.5 | Epoch: 170 | Iter: 128400 | Total Loss: 0.002870 | Recon Loss: 0.002408 | Commit Loss: 0.000925 | Perplexity: 1058.617135
2025-09-14 21:27:39,316 Stage: Train 0.5 | Epoch: 170 | Iter: 128600 | Total Loss: 0.002949 | Recon Loss: 0.002485 | Commit Loss: 0.000927 | Perplexity: 1051.353547
Trainning Epoch:  26%|██▌       | 171/665 [4:55:17<14:12:47, 103.58s/it]2025-09-14 21:28:06,777 Stage: Train 0.5 | Epoch: 171 | Iter: 128800 | Total Loss: 0.003013 | Recon Loss: 0.002553 | Commit Loss: 0.000919 | Perplexity: 1053.795387
2025-09-14 21:28:34,357 Stage: Train 0.5 | Epoch: 171 | Iter: 129000 | Total Loss: 0.002940 | Recon Loss: 0.002481 | Commit Loss: 0.000917 | Perplexity: 1055.043889
2025-09-14 21:29:01,793 Stage: Train 0.5 | Epoch: 171 | Iter: 129200 | Total Loss: 0.003004 | Recon Loss: 0.002542 | Commit Loss: 0.000925 | Perplexity: 1053.066443
2025-09-14 21:29:29,272 Stage: Train 0.5 | Epoch: 171 | Iter: 129400 | Total Loss: 0.002940 | Recon Loss: 0.002480 | Commit Loss: 0.000920 | Perplexity: 1050.255846
Trainning Epoch:  26%|██▌       | 172/665 [4:57:01<14:10:47, 103.54s/it]2025-09-14 21:29:56,690 Stage: Train 0.5 | Epoch: 172 | Iter: 129600 | Total Loss: 0.002984 | Recon Loss: 0.002519 | Commit Loss: 0.000930 | Perplexity: 1053.338297
2025-09-14 21:30:24,134 Stage: Train 0.5 | Epoch: 172 | Iter: 129800 | Total Loss: 0.002946 | Recon Loss: 0.002491 | Commit Loss: 0.000910 | Perplexity: 1052.189420
2025-09-14 21:30:51,572 Stage: Train 0.5 | Epoch: 172 | Iter: 130000 | Total Loss: 0.002882 | Recon Loss: 0.002421 | Commit Loss: 0.000922 | Perplexity: 1051.931071
2025-09-14 21:31:19,029 Stage: Train 0.5 | Epoch: 172 | Iter: 130200 | Total Loss: 0.003030 | Recon Loss: 0.002571 | Commit Loss: 0.000916 | Perplexity: 1052.221985
Trainning Epoch:  26%|██▌       | 173/665 [4:58:44<14:08:28, 103.47s/it]2025-09-14 21:31:46,494 Stage: Train 0.5 | Epoch: 173 | Iter: 130400 | Total Loss: 0.002957 | Recon Loss: 0.002503 | Commit Loss: 0.000908 | Perplexity: 1050.877274
2025-09-14 21:32:14,134 Stage: Train 0.5 | Epoch: 173 | Iter: 130600 | Total Loss: 0.002903 | Recon Loss: 0.002444 | Commit Loss: 0.000918 | Perplexity: 1057.771010
2025-09-14 21:32:41,685 Stage: Train 0.5 | Epoch: 173 | Iter: 130800 | Total Loss: 0.003019 | Recon Loss: 0.002560 | Commit Loss: 0.000918 | Perplexity: 1053.556112
2025-09-14 21:33:09,292 Stage: Train 0.5 | Epoch: 173 | Iter: 131000 | Total Loss: 0.002942 | Recon Loss: 0.002484 | Commit Loss: 0.000916 | Perplexity: 1051.153243
Trainning Epoch:  26%|██▌       | 174/665 [5:00:28<14:07:38, 103.58s/it]2025-09-14 21:33:36,842 Stage: Train 0.5 | Epoch: 174 | Iter: 131200 | Total Loss: 0.002897 | Recon Loss: 0.002442 | Commit Loss: 0.000910 | Perplexity: 1050.689473
2025-09-14 21:34:04,354 Stage: Train 0.5 | Epoch: 174 | Iter: 131400 | Total Loss: 0.003068 | Recon Loss: 0.002607 | Commit Loss: 0.000923 | Perplexity: 1056.891777
2025-09-14 21:34:31,956 Stage: Train 0.5 | Epoch: 174 | Iter: 131600 | Total Loss: 0.002912 | Recon Loss: 0.002452 | Commit Loss: 0.000919 | Perplexity: 1054.036357
Trainning Epoch:  26%|██▋       | 175/665 [5:02:11<14:06:04, 103.60s/it]2025-09-14 21:34:59,379 Stage: Train 0.5 | Epoch: 175 | Iter: 131800 | Total Loss: 0.002902 | Recon Loss: 0.002446 | Commit Loss: 0.000912 | Perplexity: 1049.058438
2025-09-14 21:35:27,065 Stage: Train 0.5 | Epoch: 175 | Iter: 132000 | Total Loss: 0.002945 | Recon Loss: 0.002485 | Commit Loss: 0.000919 | Perplexity: 1057.293022
2025-09-14 21:35:54,645 Stage: Train 0.5 | Epoch: 175 | Iter: 132200 | Total Loss: 0.002999 | Recon Loss: 0.002543 | Commit Loss: 0.000912 | Perplexity: 1054.390746
2025-09-14 21:36:22,055 Stage: Train 0.5 | Epoch: 175 | Iter: 132400 | Total Loss: 0.002918 | Recon Loss: 0.002460 | Commit Loss: 0.000916 | Perplexity: 1054.248606
Trainning Epoch:  26%|██▋       | 176/665 [5:03:55<14:04:28, 103.62s/it]2025-09-14 21:36:49,499 Stage: Train 0.5 | Epoch: 176 | Iter: 132600 | Total Loss: 0.002992 | Recon Loss: 0.002534 | Commit Loss: 0.000916 | Perplexity: 1051.657419
2025-09-14 21:37:17,174 Stage: Train 0.5 | Epoch: 176 | Iter: 132800 | Total Loss: 0.002969 | Recon Loss: 0.002512 | Commit Loss: 0.000913 | Perplexity: 1054.666516
2025-09-14 21:37:44,718 Stage: Train 0.5 | Epoch: 176 | Iter: 133000 | Total Loss: 0.002879 | Recon Loss: 0.002423 | Commit Loss: 0.000913 | Perplexity: 1055.158615
2025-09-14 21:38:12,210 Stage: Train 0.5 | Epoch: 176 | Iter: 133200 | Total Loss: 0.002932 | Recon Loss: 0.002474 | Commit Loss: 0.000915 | Perplexity: 1053.044455
Trainning Epoch:  27%|██▋       | 177/665 [5:05:39<14:02:54, 103.64s/it]2025-09-14 21:38:39,761 Stage: Train 0.5 | Epoch: 177 | Iter: 133400 | Total Loss: 0.002872 | Recon Loss: 0.002414 | Commit Loss: 0.000916 | Perplexity: 1053.949070
2025-09-14 21:39:07,163 Stage: Train 0.5 | Epoch: 177 | Iter: 133600 | Total Loss: 0.002925 | Recon Loss: 0.002467 | Commit Loss: 0.000916 | Perplexity: 1057.615159
2025-09-14 21:39:34,612 Stage: Train 0.5 | Epoch: 177 | Iter: 133800 | Total Loss: 0.002926 | Recon Loss: 0.002465 | Commit Loss: 0.000923 | Perplexity: 1057.457717
2025-09-14 21:40:02,181 Stage: Train 0.5 | Epoch: 177 | Iter: 134000 | Total Loss: 0.002941 | Recon Loss: 0.002484 | Commit Loss: 0.000913 | Perplexity: 1047.634052
Trainning Epoch:  27%|██▋       | 178/665 [5:07:22<14:00:57, 103.61s/it]2025-09-14 21:40:29,719 Stage: Train 0.5 | Epoch: 178 | Iter: 134200 | Total Loss: 0.002915 | Recon Loss: 0.002459 | Commit Loss: 0.000911 | Perplexity: 1048.444177
2025-09-14 21:40:57,223 Stage: Train 0.5 | Epoch: 178 | Iter: 134400 | Total Loss: 0.002942 | Recon Loss: 0.002490 | Commit Loss: 0.000904 | Perplexity: 1052.330027
2025-09-14 21:41:24,666 Stage: Train 0.5 | Epoch: 178 | Iter: 134600 | Total Loss: 0.002893 | Recon Loss: 0.002435 | Commit Loss: 0.000915 | Perplexity: 1058.212629
Trainning Epoch:  27%|██▋       | 179/665 [5:09:06<13:58:46, 103.55s/it]2025-09-14 21:41:52,028 Stage: Train 0.5 | Epoch: 179 | Iter: 134800 | Total Loss: 0.002982 | Recon Loss: 0.002521 | Commit Loss: 0.000924 | Perplexity: 1057.062680
2025-09-14 21:42:19,524 Stage: Train 0.5 | Epoch: 179 | Iter: 135000 | Total Loss: 0.002875 | Recon Loss: 0.002420 | Commit Loss: 0.000909 | Perplexity: 1058.043350
2025-09-14 21:42:46,919 Stage: Train 0.5 | Epoch: 179 | Iter: 135200 | Total Loss: 0.002891 | Recon Loss: 0.002438 | Commit Loss: 0.000906 | Perplexity: 1052.016144
2025-09-14 21:43:14,359 Stage: Train 0.5 | Epoch: 179 | Iter: 135400 | Total Loss: 0.002945 | Recon Loss: 0.002484 | Commit Loss: 0.000922 | Perplexity: 1054.024489
Trainning Epoch:  27%|██▋       | 180/665 [5:10:49<13:56:31, 103.49s/it]2025-09-14 21:43:41,786 Stage: Train 0.5 | Epoch: 180 | Iter: 135600 | Total Loss: 0.002950 | Recon Loss: 0.002491 | Commit Loss: 0.000918 | Perplexity: 1055.677946
2025-09-14 21:44:09,329 Stage: Train 0.5 | Epoch: 180 | Iter: 135800 | Total Loss: 0.002899 | Recon Loss: 0.002447 | Commit Loss: 0.000905 | Perplexity: 1051.286122
2025-09-14 21:44:36,960 Stage: Train 0.5 | Epoch: 180 | Iter: 136000 | Total Loss: 0.003021 | Recon Loss: 0.002567 | Commit Loss: 0.000909 | Perplexity: 1053.543233
2025-09-14 21:45:04,499 Stage: Train 0.5 | Epoch: 180 | Iter: 136200 | Total Loss: 0.002896 | Recon Loss: 0.002435 | Commit Loss: 0.000922 | Perplexity: 1059.168820
Trainning Epoch:  27%|██▋       | 181/665 [5:12:33<13:55:18, 103.55s/it]2025-09-14 21:45:32,122 Stage: Train 0.5 | Epoch: 181 | Iter: 136400 | Total Loss: 0.002898 | Recon Loss: 0.002448 | Commit Loss: 0.000899 | Perplexity: 1050.305429
2025-09-14 21:45:59,656 Stage: Train 0.5 | Epoch: 181 | Iter: 136600 | Total Loss: 0.002874 | Recon Loss: 0.002420 | Commit Loss: 0.000908 | Perplexity: 1055.238418
2025-09-14 21:46:27,242 Stage: Train 0.5 | Epoch: 181 | Iter: 136800 | Total Loss: 0.002969 | Recon Loss: 0.002511 | Commit Loss: 0.000916 | Perplexity: 1057.109052
2025-09-14 21:46:54,653 Stage: Train 0.5 | Epoch: 181 | Iter: 137000 | Total Loss: 0.002933 | Recon Loss: 0.002478 | Commit Loss: 0.000910 | Perplexity: 1055.253136
Trainning Epoch:  27%|██▋       | 182/665 [5:14:16<13:53:43, 103.57s/it]2025-09-14 21:47:21,991 Stage: Train 0.5 | Epoch: 182 | Iter: 137200 | Total Loss: 0.002916 | Recon Loss: 0.002458 | Commit Loss: 0.000917 | Perplexity: 1057.001506
2025-09-14 21:47:49,395 Stage: Train 0.5 | Epoch: 182 | Iter: 137400 | Total Loss: 0.003022 | Recon Loss: 0.002568 | Commit Loss: 0.000907 | Perplexity: 1053.082572
2025-09-14 21:48:16,783 Stage: Train 0.5 | Epoch: 182 | Iter: 137600 | Total Loss: 0.002830 | Recon Loss: 0.002375 | Commit Loss: 0.000911 | Perplexity: 1053.840783
Trainning Epoch:  28%|██▊       | 183/665 [5:16:00<13:51:03, 103.45s/it]2025-09-14 21:48:44,215 Stage: Train 0.5 | Epoch: 183 | Iter: 137800 | Total Loss: 0.003005 | Recon Loss: 0.002550 | Commit Loss: 0.000909 | Perplexity: 1053.969195
2025-09-14 21:49:11,804 Stage: Train 0.5 | Epoch: 183 | Iter: 138000 | Total Loss: 0.002948 | Recon Loss: 0.002496 | Commit Loss: 0.000904 | Perplexity: 1057.086241
2025-09-14 21:49:39,340 Stage: Train 0.5 | Epoch: 183 | Iter: 138200 | Total Loss: 0.002937 | Recon Loss: 0.002479 | Commit Loss: 0.000916 | Perplexity: 1057.442470
2025-09-14 21:50:07,156 Stage: Train 0.5 | Epoch: 183 | Iter: 138400 | Total Loss: 0.002846 | Recon Loss: 0.002391 | Commit Loss: 0.000909 | Perplexity: 1056.440804
Trainning Epoch:  28%|██▊       | 184/665 [5:17:44<13:50:55, 103.65s/it]2025-09-14 21:50:34,860 Stage: Train 0.5 | Epoch: 184 | Iter: 138600 | Total Loss: 0.002980 | Recon Loss: 0.002527 | Commit Loss: 0.000906 | Perplexity: 1052.797210
2025-09-14 21:51:02,392 Stage: Train 0.5 | Epoch: 184 | Iter: 138800 | Total Loss: 0.002853 | Recon Loss: 0.002400 | Commit Loss: 0.000905 | Perplexity: 1057.442211
2025-09-14 21:51:29,907 Stage: Train 0.5 | Epoch: 184 | Iter: 139000 | Total Loss: 0.002879 | Recon Loss: 0.002424 | Commit Loss: 0.000911 | Perplexity: 1062.128976
2025-09-14 21:51:57,414 Stage: Train 0.5 | Epoch: 184 | Iter: 139200 | Total Loss: 0.002874 | Recon Loss: 0.002418 | Commit Loss: 0.000911 | Perplexity: 1058.311525
Trainning Epoch:  28%|██▊       | 185/665 [5:19:27<13:49:13, 103.65s/it]2025-09-14 21:52:25,033 Stage: Train 0.5 | Epoch: 185 | Iter: 139400 | Total Loss: 0.002888 | Recon Loss: 0.002433 | Commit Loss: 0.000910 | Perplexity: 1052.329873
2025-09-14 21:52:52,585 Stage: Train 0.5 | Epoch: 185 | Iter: 139600 | Total Loss: 0.002963 | Recon Loss: 0.002509 | Commit Loss: 0.000908 | Perplexity: 1056.128357
2025-09-14 21:53:20,166 Stage: Train 0.5 | Epoch: 185 | Iter: 139800 | Total Loss: 0.002897 | Recon Loss: 0.002444 | Commit Loss: 0.000907 | Perplexity: 1062.914586
2025-09-14 21:53:47,731 Stage: Train 0.5 | Epoch: 185 | Iter: 140000 | Total Loss: 0.002889 | Recon Loss: 0.002438 | Commit Loss: 0.000903 | Perplexity: 1056.307947
2025-09-14 21:53:47,731 Saving model at iteration 140000
2025-09-14 21:53:47,905 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_186_step_140000
2025-09-14 21:53:48,109 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_186_step_140000/pytorch_model.bin
2025-09-14 21:53:48,430 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_186_step_140000/optimizer.bin
2025-09-14 21:53:48,431 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_186_step_140000/scheduler.bin
2025-09-14 21:53:48,431 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_186_step_140000/random_states_0.pkl
Trainning Epoch:  28%|██▊       | 186/665 [5:21:12<13:49:48, 103.94s/it]2025-09-14 21:54:15,989 Stage: Train 0.5 | Epoch: 186 | Iter: 140200 | Total Loss: 0.002938 | Recon Loss: 0.002489 | Commit Loss: 0.000898 | Perplexity: 1056.791059
2025-09-14 21:54:43,467 Stage: Train 0.5 | Epoch: 186 | Iter: 140400 | Total Loss: 0.002880 | Recon Loss: 0.002426 | Commit Loss: 0.000907 | Perplexity: 1065.919675
2025-09-14 21:55:10,944 Stage: Train 0.5 | Epoch: 186 | Iter: 140600 | Total Loss: 0.002917 | Recon Loss: 0.002462 | Commit Loss: 0.000911 | Perplexity: 1057.863203
2025-09-14 21:55:38,428 Stage: Train 0.5 | Epoch: 186 | Iter: 140800 | Total Loss: 0.002872 | Recon Loss: 0.002422 | Commit Loss: 0.000901 | Perplexity: 1053.950944
Trainning Epoch:  28%|██▊       | 187/665 [5:22:55<13:46:50, 103.79s/it]2025-09-14 21:56:05,873 Stage: Train 0.5 | Epoch: 187 | Iter: 141000 | Total Loss: 0.002935 | Recon Loss: 0.002481 | Commit Loss: 0.000908 | Perplexity: 1054.976070
2025-09-14 21:56:33,322 Stage: Train 0.5 | Epoch: 187 | Iter: 141200 | Total Loss: 0.002898 | Recon Loss: 0.002447 | Commit Loss: 0.000904 | Perplexity: 1057.997823
2025-09-14 21:57:00,958 Stage: Train 0.5 | Epoch: 187 | Iter: 141400 | Total Loss: 0.002912 | Recon Loss: 0.002461 | Commit Loss: 0.000902 | Perplexity: 1060.825246
Trainning Epoch:  28%|██▊       | 188/665 [5:24:39<13:44:52, 103.76s/it]2025-09-14 21:57:28,520 Stage: Train 0.5 | Epoch: 188 | Iter: 141600 | Total Loss: 0.002916 | Recon Loss: 0.002466 | Commit Loss: 0.000899 | Perplexity: 1056.041138
2025-09-14 21:57:55,994 Stage: Train 0.5 | Epoch: 188 | Iter: 141800 | Total Loss: 0.002869 | Recon Loss: 0.002417 | Commit Loss: 0.000906 | Perplexity: 1062.649023
2025-09-14 21:58:23,485 Stage: Train 0.5 | Epoch: 188 | Iter: 142000 | Total Loss: 0.002823 | Recon Loss: 0.002373 | Commit Loss: 0.000900 | Perplexity: 1058.468386
2025-09-14 21:58:50,886 Stage: Train 0.5 | Epoch: 188 | Iter: 142200 | Total Loss: 0.002926 | Recon Loss: 0.002467 | Commit Loss: 0.000918 | Perplexity: 1059.751942
Trainning Epoch:  28%|██▊       | 189/665 [5:26:22<13:42:03, 103.62s/it]2025-09-14 21:59:18,255 Stage: Train 0.5 | Epoch: 189 | Iter: 142400 | Total Loss: 0.002871 | Recon Loss: 0.002421 | Commit Loss: 0.000899 | Perplexity: 1053.895615
2025-09-14 21:59:45,682 Stage: Train 0.5 | Epoch: 189 | Iter: 142600 | Total Loss: 0.002919 | Recon Loss: 0.002465 | Commit Loss: 0.000906 | Perplexity: 1060.101472
2025-09-14 22:00:13,121 Stage: Train 0.5 | Epoch: 189 | Iter: 142800 | Total Loss: 0.002931 | Recon Loss: 0.002479 | Commit Loss: 0.000905 | Perplexity: 1057.060733
2025-09-14 22:00:40,746 Stage: Train 0.5 | Epoch: 189 | Iter: 143000 | Total Loss: 0.002826 | Recon Loss: 0.002375 | Commit Loss: 0.000904 | Perplexity: 1059.081637
Trainning Epoch:  29%|██▊       | 190/665 [5:28:06<13:39:55, 103.57s/it]2025-09-14 22:01:08,292 Stage: Train 0.5 | Epoch: 190 | Iter: 143200 | Total Loss: 0.002822 | Recon Loss: 0.002367 | Commit Loss: 0.000908 | Perplexity: 1058.436412
2025-09-14 22:01:35,990 Stage: Train 0.5 | Epoch: 190 | Iter: 143400 | Total Loss: 0.002878 | Recon Loss: 0.002423 | Commit Loss: 0.000912 | Perplexity: 1063.548801
2025-09-14 22:02:03,585 Stage: Train 0.5 | Epoch: 190 | Iter: 143600 | Total Loss: 0.002875 | Recon Loss: 0.002422 | Commit Loss: 0.000905 | Perplexity: 1060.275272
2025-09-14 22:02:31,026 Stage: Train 0.5 | Epoch: 190 | Iter: 143800 | Total Loss: 0.002871 | Recon Loss: 0.002423 | Commit Loss: 0.000896 | Perplexity: 1058.335344
Trainning Epoch:  29%|██▊       | 191/665 [5:29:50<13:38:45, 103.64s/it]2025-09-14 22:02:58,418 Stage: Train 0.5 | Epoch: 191 | Iter: 144000 | Total Loss: 0.002855 | Recon Loss: 0.002402 | Commit Loss: 0.000906 | Perplexity: 1061.818429
2025-09-14 22:03:25,862 Stage: Train 0.5 | Epoch: 191 | Iter: 144200 | Total Loss: 0.002853 | Recon Loss: 0.002404 | Commit Loss: 0.000897 | Perplexity: 1055.915823
2025-09-14 22:03:53,412 Stage: Train 0.5 | Epoch: 191 | Iter: 144400 | Total Loss: 0.002884 | Recon Loss: 0.002431 | Commit Loss: 0.000907 | Perplexity: 1058.422814
Trainning Epoch:  29%|██▉       | 192/665 [5:31:33<13:37:02, 103.64s/it]2025-09-14 22:04:21,102 Stage: Train 0.5 | Epoch: 192 | Iter: 144600 | Total Loss: 0.002851 | Recon Loss: 0.002398 | Commit Loss: 0.000907 | Perplexity: 1058.289004
2025-09-14 22:04:48,618 Stage: Train 0.5 | Epoch: 192 | Iter: 144800 | Total Loss: 0.002913 | Recon Loss: 0.002462 | Commit Loss: 0.000902 | Perplexity: 1060.380960
2025-09-14 22:05:16,085 Stage: Train 0.5 | Epoch: 192 | Iter: 145000 | Total Loss: 0.002887 | Recon Loss: 0.002436 | Commit Loss: 0.000902 | Perplexity: 1054.223769
2025-09-14 22:05:43,578 Stage: Train 0.5 | Epoch: 192 | Iter: 145200 | Total Loss: 0.002907 | Recon Loss: 0.002453 | Commit Loss: 0.000908 | Perplexity: 1062.291611
Trainning Epoch:  29%|██▉       | 193/665 [5:33:17<13:34:59, 103.60s/it]2025-09-14 22:06:11,117 Stage: Train 0.5 | Epoch: 193 | Iter: 145400 | Total Loss: 0.002848 | Recon Loss: 0.002400 | Commit Loss: 0.000894 | Perplexity: 1054.625760
2025-09-14 22:06:38,798 Stage: Train 0.5 | Epoch: 193 | Iter: 145600 | Total Loss: 0.002870 | Recon Loss: 0.002420 | Commit Loss: 0.000901 | Perplexity: 1061.096210
2025-09-14 22:07:06,521 Stage: Train 0.5 | Epoch: 193 | Iter: 145800 | Total Loss: 0.002893 | Recon Loss: 0.002439 | Commit Loss: 0.000908 | Perplexity: 1057.737698
2025-09-14 22:07:34,137 Stage: Train 0.5 | Epoch: 193 | Iter: 146000 | Total Loss: 0.002899 | Recon Loss: 0.002451 | Commit Loss: 0.000896 | Perplexity: 1058.079175
Trainning Epoch:  29%|██▉       | 194/665 [5:35:01<13:34:20, 103.74s/it]2025-09-14 22:08:01,534 Stage: Train 0.5 | Epoch: 194 | Iter: 146200 | Total Loss: 0.002837 | Recon Loss: 0.002386 | Commit Loss: 0.000903 | Perplexity: 1061.085920
2025-09-14 22:08:29,001 Stage: Train 0.5 | Epoch: 194 | Iter: 146400 | Total Loss: 0.002897 | Recon Loss: 0.002443 | Commit Loss: 0.000908 | Perplexity: 1059.620642
2025-09-14 22:08:56,461 Stage: Train 0.5 | Epoch: 194 | Iter: 146600 | Total Loss: 0.002875 | Recon Loss: 0.002431 | Commit Loss: 0.000887 | Perplexity: 1057.236584
2025-09-14 22:09:23,914 Stage: Train 0.5 | Epoch: 194 | Iter: 146800 | Total Loss: 0.002795 | Recon Loss: 0.002344 | Commit Loss: 0.000901 | Perplexity: 1059.504057
Trainning Epoch:  29%|██▉       | 195/665 [5:36:44<13:31:38, 103.61s/it]2025-09-14 22:09:51,319 Stage: Train 0.5 | Epoch: 195 | Iter: 147000 | Total Loss: 0.002849 | Recon Loss: 0.002396 | Commit Loss: 0.000906 | Perplexity: 1060.919478
2025-09-14 22:10:18,768 Stage: Train 0.5 | Epoch: 195 | Iter: 147200 | Total Loss: 0.002860 | Recon Loss: 0.002409 | Commit Loss: 0.000903 | Perplexity: 1059.486410
2025-09-14 22:10:46,202 Stage: Train 0.5 | Epoch: 195 | Iter: 147400 | Total Loss: 0.002938 | Recon Loss: 0.002491 | Commit Loss: 0.000895 | Perplexity: 1057.663527
Trainning Epoch:  29%|██▉       | 196/665 [5:38:28<13:29:19, 103.54s/it]2025-09-14 22:11:13,675 Stage: Train 0.5 | Epoch: 196 | Iter: 147600 | Total Loss: 0.002871 | Recon Loss: 0.002421 | Commit Loss: 0.000901 | Perplexity: 1054.800269
2025-09-14 22:11:41,090 Stage: Train 0.5 | Epoch: 196 | Iter: 147800 | Total Loss: 0.002828 | Recon Loss: 0.002378 | Commit Loss: 0.000901 | Perplexity: 1061.040476
2025-09-14 22:12:08,651 Stage: Train 0.5 | Epoch: 196 | Iter: 148000 | Total Loss: 0.002894 | Recon Loss: 0.002447 | Commit Loss: 0.000893 | Perplexity: 1059.936254
2025-09-14 22:12:36,246 Stage: Train 0.5 | Epoch: 196 | Iter: 148200 | Total Loss: 0.002898 | Recon Loss: 0.002451 | Commit Loss: 0.000895 | Perplexity: 1057.988247
Trainning Epoch:  30%|██▉       | 197/665 [5:40:11<13:27:53, 103.58s/it]2025-09-14 22:13:03,878 Stage: Train 0.5 | Epoch: 197 | Iter: 148400 | Total Loss: 0.002857 | Recon Loss: 0.002404 | Commit Loss: 0.000906 | Perplexity: 1057.058353
2025-09-14 22:13:31,531 Stage: Train 0.5 | Epoch: 197 | Iter: 148600 | Total Loss: 0.002834 | Recon Loss: 0.002382 | Commit Loss: 0.000903 | Perplexity: 1060.483318
2025-09-14 22:13:59,056 Stage: Train 0.5 | Epoch: 197 | Iter: 148800 | Total Loss: 0.002913 | Recon Loss: 0.002470 | Commit Loss: 0.000886 | Perplexity: 1055.473218
2025-09-14 22:14:26,676 Stage: Train 0.5 | Epoch: 197 | Iter: 149000 | Total Loss: 0.002883 | Recon Loss: 0.002431 | Commit Loss: 0.000905 | Perplexity: 1061.758587
Trainning Epoch:  30%|██▉       | 198/665 [5:41:55<13:26:52, 103.67s/it]2025-09-14 22:14:54,182 Stage: Train 0.5 | Epoch: 198 | Iter: 149200 | Total Loss: 0.002846 | Recon Loss: 0.002400 | Commit Loss: 0.000891 | Perplexity: 1059.553745
2025-09-14 22:15:21,722 Stage: Train 0.5 | Epoch: 198 | Iter: 149400 | Total Loss: 0.002866 | Recon Loss: 0.002420 | Commit Loss: 0.000891 | Perplexity: 1058.307180
2025-09-14 22:15:49,271 Stage: Train 0.5 | Epoch: 198 | Iter: 149600 | Total Loss: 0.002780 | Recon Loss: 0.002329 | Commit Loss: 0.000901 | Perplexity: 1062.441000
2025-09-14 22:16:16,813 Stage: Train 0.5 | Epoch: 198 | Iter: 149800 | Total Loss: 0.002885 | Recon Loss: 0.002434 | Commit Loss: 0.000903 | Perplexity: 1061.600126
Trainning Epoch:  30%|██▉       | 199/665 [5:43:39<13:25:08, 103.67s/it]2025-09-14 22:16:44,320 Stage: Train 0.5 | Epoch: 199 | Iter: 150000 | Total Loss: 0.002793 | Recon Loss: 0.002345 | Commit Loss: 0.000896 | Perplexity: 1059.497071
2025-09-14 22:17:11,853 Stage: Train 0.5 | Epoch: 199 | Iter: 150200 | Total Loss: 0.002943 | Recon Loss: 0.002496 | Commit Loss: 0.000895 | Perplexity: 1061.286022
2025-09-14 22:17:39,382 Stage: Train 0.5 | Epoch: 199 | Iter: 150400 | Total Loss: 0.002871 | Recon Loss: 0.002420 | Commit Loss: 0.000901 | Perplexity: 1063.903831
2025-09-14 22:18:06,945 Stage: Train 0.5 | Epoch: 199 | Iter: 150600 | Total Loss: 0.002862 | Recon Loss: 0.002415 | Commit Loss: 0.000894 | Perplexity: 1060.426147
Trainning Epoch:  30%|███       | 200/665 [5:45:22<13:23:30, 103.68s/it]2025-09-14 22:18:34,470 Stage: Train 0.5 | Epoch: 200 | Iter: 150800 | Total Loss: 0.002876 | Recon Loss: 0.002426 | Commit Loss: 0.000900 | Perplexity: 1062.602838
2025-09-14 22:19:01,970 Stage: Train 0.5 | Epoch: 200 | Iter: 151000 | Total Loss: 0.002942 | Recon Loss: 0.002494 | Commit Loss: 0.000895 | Perplexity: 1059.454302
2025-09-14 22:19:29,474 Stage: Train 0.5 | Epoch: 200 | Iter: 151200 | Total Loss: 0.002786 | Recon Loss: 0.002340 | Commit Loss: 0.000892 | Perplexity: 1058.994458
Trainning Epoch:  30%|███       | 201/665 [5:47:06<13:21:25, 103.63s/it]2025-09-14 22:19:56,936 Stage: Train 0.5 | Epoch: 201 | Iter: 151400 | Total Loss: 0.002874 | Recon Loss: 0.002427 | Commit Loss: 0.000895 | Perplexity: 1062.788595
2025-09-14 22:20:24,444 Stage: Train 0.5 | Epoch: 201 | Iter: 151600 | Total Loss: 0.002846 | Recon Loss: 0.002401 | Commit Loss: 0.000890 | Perplexity: 1059.773568
2025-09-14 22:20:51,956 Stage: Train 0.5 | Epoch: 201 | Iter: 151800 | Total Loss: 0.002787 | Recon Loss: 0.002340 | Commit Loss: 0.000895 | Perplexity: 1059.008690
2025-09-14 22:21:19,530 Stage: Train 0.5 | Epoch: 201 | Iter: 152000 | Total Loss: 0.002825 | Recon Loss: 0.002374 | Commit Loss: 0.000902 | Perplexity: 1062.410531
Trainning Epoch:  30%|███       | 202/665 [5:48:50<13:20:06, 103.69s/it]2025-09-14 22:21:47,184 Stage: Train 0.5 | Epoch: 202 | Iter: 152200 | Total Loss: 0.002804 | Recon Loss: 0.002360 | Commit Loss: 0.000888 | Perplexity: 1055.781802
2025-09-14 22:22:14,660 Stage: Train 0.5 | Epoch: 202 | Iter: 152400 | Total Loss: 0.002861 | Recon Loss: 0.002412 | Commit Loss: 0.000899 | Perplexity: 1065.890339
2025-09-14 22:22:42,105 Stage: Train 0.5 | Epoch: 202 | Iter: 152600 | Total Loss: 0.002801 | Recon Loss: 0.002351 | Commit Loss: 0.000900 | Perplexity: 1060.131306
2025-09-14 22:23:09,556 Stage: Train 0.5 | Epoch: 202 | Iter: 152800 | Total Loss: 0.002811 | Recon Loss: 0.002365 | Commit Loss: 0.000892 | Perplexity: 1058.469776
Trainning Epoch:  31%|███       | 203/665 [5:50:33<13:17:33, 103.58s/it]2025-09-14 22:23:36,950 Stage: Train 0.5 | Epoch: 203 | Iter: 153000 | Total Loss: 0.002818 | Recon Loss: 0.002371 | Commit Loss: 0.000892 | Perplexity: 1062.235695
2025-09-14 22:24:04,379 Stage: Train 0.5 | Epoch: 203 | Iter: 153200 | Total Loss: 0.002820 | Recon Loss: 0.002374 | Commit Loss: 0.000892 | Perplexity: 1059.506017
2025-09-14 22:24:32,012 Stage: Train 0.5 | Epoch: 203 | Iter: 153400 | Total Loss: 0.002887 | Recon Loss: 0.002439 | Commit Loss: 0.000895 | Perplexity: 1058.932121
2025-09-14 22:24:59,482 Stage: Train 0.5 | Epoch: 203 | Iter: 153600 | Total Loss: 0.002814 | Recon Loss: 0.002363 | Commit Loss: 0.000901 | Perplexity: 1062.345837
Trainning Epoch:  31%|███       | 204/665 [5:52:17<13:15:35, 103.55s/it]2025-09-14 22:25:26,908 Stage: Train 0.5 | Epoch: 204 | Iter: 153800 | Total Loss: 0.002827 | Recon Loss: 0.002382 | Commit Loss: 0.000890 | Perplexity: 1057.507106
2025-09-14 22:25:54,385 Stage: Train 0.5 | Epoch: 204 | Iter: 154000 | Total Loss: 0.002805 | Recon Loss: 0.002359 | Commit Loss: 0.000892 | Perplexity: 1063.503472
2025-09-14 22:26:21,817 Stage: Train 0.5 | Epoch: 204 | Iter: 154200 | Total Loss: 0.002762 | Recon Loss: 0.002316 | Commit Loss: 0.000890 | Perplexity: 1059.752189
Trainning Epoch:  31%|███       | 205/665 [5:54:00<13:13:16, 103.47s/it]2025-09-14 22:26:49,174 Stage: Train 0.5 | Epoch: 205 | Iter: 154400 | Total Loss: 0.002862 | Recon Loss: 0.002410 | Commit Loss: 0.000904 | Perplexity: 1063.650184
2025-09-14 22:27:16,575 Stage: Train 0.5 | Epoch: 205 | Iter: 154600 | Total Loss: 0.002797 | Recon Loss: 0.002350 | Commit Loss: 0.000893 | Perplexity: 1064.538946
2025-09-14 22:27:43,981 Stage: Train 0.5 | Epoch: 205 | Iter: 154800 | Total Loss: 0.002811 | Recon Loss: 0.002363 | Commit Loss: 0.000896 | Perplexity: 1062.532902
2025-09-14 22:28:11,870 Stage: Train 0.5 | Epoch: 205 | Iter: 155000 | Total Loss: 0.002856 | Recon Loss: 0.002410 | Commit Loss: 0.000892 | Perplexity: 1059.206703
Trainning Epoch:  31%|███       | 206/665 [5:55:44<13:12:11, 103.56s/it]2025-09-14 22:28:39,455 Stage: Train 0.5 | Epoch: 206 | Iter: 155200 | Total Loss: 0.002798 | Recon Loss: 0.002350 | Commit Loss: 0.000895 | Perplexity: 1059.160508
2025-09-14 22:29:07,013 Stage: Train 0.5 | Epoch: 206 | Iter: 155400 | Total Loss: 0.002825 | Recon Loss: 0.002384 | Commit Loss: 0.000883 | Perplexity: 1059.234960
2025-09-14 22:29:34,575 Stage: Train 0.5 | Epoch: 206 | Iter: 155600 | Total Loss: 0.002795 | Recon Loss: 0.002344 | Commit Loss: 0.000901 | Perplexity: 1060.088220
2025-09-14 22:30:02,156 Stage: Train 0.5 | Epoch: 206 | Iter: 155800 | Total Loss: 0.002867 | Recon Loss: 0.002420 | Commit Loss: 0.000895 | Perplexity: 1064.127719
Trainning Epoch:  31%|███       | 207/665 [5:57:27<13:10:57, 103.62s/it]2025-09-14 22:30:29,564 Stage: Train 0.5 | Epoch: 207 | Iter: 156000 | Total Loss: 0.002859 | Recon Loss: 0.002413 | Commit Loss: 0.000892 | Perplexity: 1060.932363
2025-09-14 22:30:57,265 Stage: Train 0.5 | Epoch: 207 | Iter: 156200 | Total Loss: 0.002699 | Recon Loss: 0.002256 | Commit Loss: 0.000886 | Perplexity: 1060.204910
2025-09-14 22:31:24,893 Stage: Train 0.5 | Epoch: 207 | Iter: 156400 | Total Loss: 0.002830 | Recon Loss: 0.002385 | Commit Loss: 0.000889 | Perplexity: 1059.670401
2025-09-14 22:31:52,407 Stage: Train 0.5 | Epoch: 207 | Iter: 156600 | Total Loss: 0.002870 | Recon Loss: 0.002420 | Commit Loss: 0.000899 | Perplexity: 1062.863719
Trainning Epoch:  31%|███▏      | 208/665 [5:59:11<13:09:33, 103.66s/it]2025-09-14 22:32:19,882 Stage: Train 0.5 | Epoch: 208 | Iter: 156800 | Total Loss: 0.002844 | Recon Loss: 0.002401 | Commit Loss: 0.000887 | Perplexity: 1057.487739
2025-09-14 22:32:47,462 Stage: Train 0.5 | Epoch: 208 | Iter: 157000 | Total Loss: 0.002768 | Recon Loss: 0.002323 | Commit Loss: 0.000890 | Perplexity: 1063.299704
2025-09-14 22:33:14,998 Stage: Train 0.5 | Epoch: 208 | Iter: 157200 | Total Loss: 0.002832 | Recon Loss: 0.002383 | Commit Loss: 0.000899 | Perplexity: 1063.211201
Trainning Epoch:  31%|███▏      | 209/665 [6:00:55<13:07:49, 103.66s/it]2025-09-14 22:33:42,477 Stage: Train 0.5 | Epoch: 209 | Iter: 157400 | Total Loss: 0.002835 | Recon Loss: 0.002388 | Commit Loss: 0.000894 | Perplexity: 1059.362838
2025-09-14 22:34:09,993 Stage: Train 0.5 | Epoch: 209 | Iter: 157600 | Total Loss: 0.002822 | Recon Loss: 0.002382 | Commit Loss: 0.000881 | Perplexity: 1058.667240
2025-09-14 22:34:37,500 Stage: Train 0.5 | Epoch: 209 | Iter: 157800 | Total Loss: 0.002859 | Recon Loss: 0.002412 | Commit Loss: 0.000895 | Perplexity: 1063.458693
2025-09-14 22:35:05,150 Stage: Train 0.5 | Epoch: 209 | Iter: 158000 | Total Loss: 0.002782 | Recon Loss: 0.002336 | Commit Loss: 0.000891 | Perplexity: 1064.531025
Trainning Epoch:  32%|███▏      | 210/665 [6:02:38<13:06:00, 103.65s/it]2025-09-14 22:35:32,529 Stage: Train 0.5 | Epoch: 210 | Iter: 158200 | Total Loss: 0.002834 | Recon Loss: 0.002389 | Commit Loss: 0.000889 | Perplexity: 1063.889909
2025-09-14 22:35:59,912 Stage: Train 0.5 | Epoch: 210 | Iter: 158400 | Total Loss: 0.002859 | Recon Loss: 0.002413 | Commit Loss: 0.000892 | Perplexity: 1063.531108
2025-09-14 22:36:27,349 Stage: Train 0.5 | Epoch: 210 | Iter: 158600 | Total Loss: 0.002796 | Recon Loss: 0.002352 | Commit Loss: 0.000886 | Perplexity: 1054.388210
2025-09-14 22:36:54,724 Stage: Train 0.5 | Epoch: 210 | Iter: 158800 | Total Loss: 0.002794 | Recon Loss: 0.002347 | Commit Loss: 0.000895 | Perplexity: 1065.458286
Trainning Epoch:  32%|███▏      | 211/665 [6:04:22<13:03:03, 103.49s/it]2025-09-14 22:37:22,072 Stage: Train 0.5 | Epoch: 211 | Iter: 159000 | Total Loss: 0.002838 | Recon Loss: 0.002394 | Commit Loss: 0.000888 | Perplexity: 1060.817675
2025-09-14 22:37:49,465 Stage: Train 0.5 | Epoch: 211 | Iter: 159200 | Total Loss: 0.002775 | Recon Loss: 0.002325 | Commit Loss: 0.000899 | Perplexity: 1065.034338
2025-09-14 22:38:17,092 Stage: Train 0.5 | Epoch: 211 | Iter: 159400 | Total Loss: 0.002807 | Recon Loss: 0.002366 | Commit Loss: 0.000881 | Perplexity: 1060.128185
2025-09-14 22:38:44,527 Stage: Train 0.5 | Epoch: 211 | Iter: 159600 | Total Loss: 0.002833 | Recon Loss: 0.002390 | Commit Loss: 0.000885 | Perplexity: 1059.233320
Trainning Epoch:  32%|███▏      | 212/665 [6:06:05<13:01:03, 103.45s/it]2025-09-14 22:39:11,931 Stage: Train 0.5 | Epoch: 212 | Iter: 159800 | Total Loss: 0.002783 | Recon Loss: 0.002336 | Commit Loss: 0.000894 | Perplexity: 1059.663699
2025-09-14 22:39:39,401 Stage: Train 0.5 | Epoch: 212 | Iter: 160000 | Total Loss: 0.002900 | Recon Loss: 0.002456 | Commit Loss: 0.000888 | Perplexity: 1064.358594
2025-09-14 22:39:39,401 Saving model at iteration 160000
2025-09-14 22:39:39,570 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_213_step_160000
2025-09-14 22:39:39,783 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_213_step_160000/pytorch_model.bin
2025-09-14 22:39:40,108 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_213_step_160000/optimizer.bin
2025-09-14 22:39:40,108 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_213_step_160000/scheduler.bin
2025-09-14 22:39:40,109 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_213_step_160000/random_states_0.pkl
2025-09-14 22:40:07,905 Stage: Train 0.5 | Epoch: 212 | Iter: 160200 | Total Loss: 0.002757 | Recon Loss: 0.002317 | Commit Loss: 0.000880 | Perplexity: 1060.318478
Trainning Epoch:  32%|███▏      | 213/665 [6:07:50<13:02:06, 103.82s/it]2025-09-14 22:40:35,611 Stage: Train 0.5 | Epoch: 213 | Iter: 160400 | Total Loss: 0.002753 | Recon Loss: 0.002306 | Commit Loss: 0.000893 | Perplexity: 1064.480448
2025-09-14 22:41:03,094 Stage: Train 0.5 | Epoch: 213 | Iter: 160600 | Total Loss: 0.002797 | Recon Loss: 0.002354 | Commit Loss: 0.000886 | Perplexity: 1059.993073
2025-09-14 22:41:30,603 Stage: Train 0.5 | Epoch: 213 | Iter: 160800 | Total Loss: 0.002760 | Recon Loss: 0.002314 | Commit Loss: 0.000893 | Perplexity: 1063.645574
2025-09-14 22:41:58,092 Stage: Train 0.5 | Epoch: 213 | Iter: 161000 | Total Loss: 0.002807 | Recon Loss: 0.002366 | Commit Loss: 0.000882 | Perplexity: 1060.518924
Trainning Epoch:  32%|███▏      | 214/665 [6:09:33<12:59:35, 103.72s/it]2025-09-14 22:42:25,519 Stage: Train 0.5 | Epoch: 214 | Iter: 161200 | Total Loss: 0.002792 | Recon Loss: 0.002348 | Commit Loss: 0.000889 | Perplexity: 1063.011300
2025-09-14 22:42:53,002 Stage: Train 0.5 | Epoch: 214 | Iter: 161400 | Total Loss: 0.002756 | Recon Loss: 0.002312 | Commit Loss: 0.000888 | Perplexity: 1062.437209
2025-09-14 22:43:20,549 Stage: Train 0.5 | Epoch: 214 | Iter: 161600 | Total Loss: 0.002878 | Recon Loss: 0.002432 | Commit Loss: 0.000892 | Perplexity: 1062.737430
2025-09-14 22:43:48,010 Stage: Train 0.5 | Epoch: 214 | Iter: 161800 | Total Loss: 0.002780 | Recon Loss: 0.002339 | Commit Loss: 0.000883 | Perplexity: 1059.605974
Trainning Epoch:  32%|███▏      | 215/665 [6:11:17<12:57:10, 103.62s/it]2025-09-14 22:44:15,361 Stage: Train 0.5 | Epoch: 215 | Iter: 162000 | Total Loss: 0.002803 | Recon Loss: 0.002363 | Commit Loss: 0.000880 | Perplexity: 1059.647307
2025-09-14 22:44:42,745 Stage: Train 0.5 | Epoch: 215 | Iter: 162200 | Total Loss: 0.002787 | Recon Loss: 0.002344 | Commit Loss: 0.000885 | Perplexity: 1058.826399
2025-09-14 22:45:10,048 Stage: Train 0.5 | Epoch: 215 | Iter: 162400 | Total Loss: 0.002726 | Recon Loss: 0.002277 | Commit Loss: 0.000898 | Perplexity: 1069.992495
2025-09-14 22:45:37,371 Stage: Train 0.5 | Epoch: 215 | Iter: 162600 | Total Loss: 0.002735 | Recon Loss: 0.002292 | Commit Loss: 0.000886 | Perplexity: 1063.668774
Trainning Epoch:  32%|███▏      | 216/665 [6:12:59<12:53:50, 103.41s/it]2025-09-14 22:46:04,664 Stage: Train 0.5 | Epoch: 216 | Iter: 162800 | Total Loss: 0.002798 | Recon Loss: 0.002352 | Commit Loss: 0.000891 | Perplexity: 1063.965074
2025-09-14 22:46:32,166 Stage: Train 0.5 | Epoch: 216 | Iter: 163000 | Total Loss: 0.002745 | Recon Loss: 0.002305 | Commit Loss: 0.000880 | Perplexity: 1060.540691
2025-09-14 22:46:59,555 Stage: Train 0.5 | Epoch: 216 | Iter: 163200 | Total Loss: 0.002783 | Recon Loss: 0.002339 | Commit Loss: 0.000888 | Perplexity: 1062.465934
2025-09-14 22:47:26,943 Stage: Train 0.5 | Epoch: 216 | Iter: 163400 | Total Loss: 0.002799 | Recon Loss: 0.002353 | Commit Loss: 0.000892 | Perplexity: 1062.503513
Trainning Epoch:  33%|███▎      | 217/665 [6:14:43<12:51:31, 103.33s/it]2025-09-14 22:47:54,233 Stage: Train 0.5 | Epoch: 217 | Iter: 163600 | Total Loss: 0.002791 | Recon Loss: 0.002347 | Commit Loss: 0.000889 | Perplexity: 1063.172746
2025-09-14 22:48:21,537 Stage: Train 0.5 | Epoch: 217 | Iter: 163800 | Total Loss: 0.002845 | Recon Loss: 0.002401 | Commit Loss: 0.000888 | Perplexity: 1063.518124
2025-09-14 22:48:48,842 Stage: Train 0.5 | Epoch: 217 | Iter: 164000 | Total Loss: 0.002741 | Recon Loss: 0.002301 | Commit Loss: 0.000879 | Perplexity: 1058.984865
Trainning Epoch:  33%|███▎      | 218/665 [6:16:25<12:48:37, 103.17s/it]2025-09-14 22:49:16,113 Stage: Train 0.5 | Epoch: 218 | Iter: 164200 | Total Loss: 0.002825 | Recon Loss: 0.002384 | Commit Loss: 0.000882 | Perplexity: 1063.853777
2025-09-14 22:49:43,508 Stage: Train 0.5 | Epoch: 218 | Iter: 164400 | Total Loss: 0.002750 | Recon Loss: 0.002310 | Commit Loss: 0.000880 | Perplexity: 1060.082310
2025-09-14 22:50:10,997 Stage: Train 0.5 | Epoch: 218 | Iter: 164600 | Total Loss: 0.002769 | Recon Loss: 0.002327 | Commit Loss: 0.000883 | Perplexity: 1060.352254
2025-09-14 22:50:38,385 Stage: Train 0.5 | Epoch: 218 | Iter: 164800 | Total Loss: 0.002878 | Recon Loss: 0.002434 | Commit Loss: 0.000889 | Perplexity: 1061.450508
Trainning Epoch:  33%|███▎      | 219/665 [6:18:09<12:46:52, 103.17s/it]2025-09-14 22:51:05,720 Stage: Train 0.5 | Epoch: 219 | Iter: 165000 | Total Loss: 0.002779 | Recon Loss: 0.002336 | Commit Loss: 0.000886 | Perplexity: 1062.692511
2025-09-14 22:51:33,114 Stage: Train 0.5 | Epoch: 219 | Iter: 165200 | Total Loss: 0.002829 | Recon Loss: 0.002389 | Commit Loss: 0.000880 | Perplexity: 1059.685829
2025-09-14 22:52:00,507 Stage: Train 0.5 | Epoch: 219 | Iter: 165400 | Total Loss: 0.002724 | Recon Loss: 0.002281 | Commit Loss: 0.000887 | Perplexity: 1063.935858
2025-09-14 22:52:27,888 Stage: Train 0.5 | Epoch: 219 | Iter: 165600 | Total Loss: 0.002813 | Recon Loss: 0.002370 | Commit Loss: 0.000886 | Perplexity: 1058.653831
Trainning Epoch:  33%|███▎      | 220/665 [6:19:52<12:44:55, 103.14s/it]2025-09-14 22:52:55,352 Stage: Train 0.5 | Epoch: 220 | Iter: 165800 | Total Loss: 0.002766 | Recon Loss: 0.002325 | Commit Loss: 0.000882 | Perplexity: 1060.561089
2025-09-14 22:53:22,872 Stage: Train 0.5 | Epoch: 220 | Iter: 166000 | Total Loss: 0.002774 | Recon Loss: 0.002334 | Commit Loss: 0.000880 | Perplexity: 1060.457830
2025-09-14 22:53:50,301 Stage: Train 0.5 | Epoch: 220 | Iter: 166200 | Total Loss: 0.002853 | Recon Loss: 0.002413 | Commit Loss: 0.000881 | Perplexity: 1059.762521
2025-09-14 22:54:17,682 Stage: Train 0.5 | Epoch: 220 | Iter: 166400 | Total Loss: 0.002764 | Recon Loss: 0.002324 | Commit Loss: 0.000880 | Perplexity: 1059.432617
Trainning Epoch:  33%|███▎      | 221/665 [6:21:35<12:43:42, 103.20s/it]2025-09-14 22:54:45,016 Stage: Train 0.5 | Epoch: 221 | Iter: 166600 | Total Loss: 0.002799 | Recon Loss: 0.002362 | Commit Loss: 0.000875 | Perplexity: 1057.655097
2025-09-14 22:55:12,596 Stage: Train 0.5 | Epoch: 221 | Iter: 166800 | Total Loss: 0.002807 | Recon Loss: 0.002368 | Commit Loss: 0.000879 | Perplexity: 1061.697906
2025-09-14 22:55:40,362 Stage: Train 0.5 | Epoch: 221 | Iter: 167000 | Total Loss: 0.002744 | Recon Loss: 0.002303 | Commit Loss: 0.000881 | Perplexity: 1062.263208
Trainning Epoch:  33%|███▎      | 222/665 [6:23:19<12:43:04, 103.35s/it]2025-09-14 22:56:07,770 Stage: Train 0.5 | Epoch: 222 | Iter: 167200 | Total Loss: 0.002825 | Recon Loss: 0.002383 | Commit Loss: 0.000884 | Perplexity: 1060.345892
2025-09-14 22:56:35,341 Stage: Train 0.5 | Epoch: 222 | Iter: 167400 | Total Loss: 0.002798 | Recon Loss: 0.002352 | Commit Loss: 0.000894 | Perplexity: 1062.391670
2025-09-14 22:57:02,954 Stage: Train 0.5 | Epoch: 222 | Iter: 167600 | Total Loss: 0.002768 | Recon Loss: 0.002335 | Commit Loss: 0.000867 | Perplexity: 1060.311675
2025-09-14 22:57:30,582 Stage: Train 0.5 | Epoch: 222 | Iter: 167800 | Total Loss: 0.002777 | Recon Loss: 0.002335 | Commit Loss: 0.000885 | Perplexity: 1059.950432
Trainning Epoch:  34%|███▎      | 223/665 [6:25:02<12:42:26, 103.50s/it]2025-09-14 22:57:58,095 Stage: Train 0.5 | Epoch: 223 | Iter: 168000 | Total Loss: 0.002755 | Recon Loss: 0.002315 | Commit Loss: 0.000880 | Perplexity: 1060.026480
2025-09-14 22:58:25,641 Stage: Train 0.5 | Epoch: 223 | Iter: 168200 | Total Loss: 0.002788 | Recon Loss: 0.002346 | Commit Loss: 0.000884 | Perplexity: 1063.685226
2025-09-14 22:58:53,087 Stage: Train 0.5 | Epoch: 223 | Iter: 168400 | Total Loss: 0.002791 | Recon Loss: 0.002352 | Commit Loss: 0.000878 | Perplexity: 1057.899067
2025-09-14 22:59:20,573 Stage: Train 0.5 | Epoch: 223 | Iter: 168600 | Total Loss: 0.002725 | Recon Loss: 0.002288 | Commit Loss: 0.000874 | Perplexity: 1061.357245
Trainning Epoch:  34%|███▎      | 224/665 [6:26:46<12:40:53, 103.52s/it]2025-09-14 22:59:48,199 Stage: Train 0.5 | Epoch: 224 | Iter: 168800 | Total Loss: 0.002826 | Recon Loss: 0.002386 | Commit Loss: 0.000879 | Perplexity: 1056.492591
2025-09-14 23:00:15,772 Stage: Train 0.5 | Epoch: 224 | Iter: 169000 | Total Loss: 0.002775 | Recon Loss: 0.002337 | Commit Loss: 0.000877 | Perplexity: 1063.238587
2025-09-14 23:00:43,312 Stage: Train 0.5 | Epoch: 224 | Iter: 169200 | Total Loss: 0.002829 | Recon Loss: 0.002391 | Commit Loss: 0.000878 | Perplexity: 1060.305217
2025-09-14 23:01:10,746 Stage: Train 0.5 | Epoch: 224 | Iter: 169400 | Total Loss: 0.002699 | Recon Loss: 0.002258 | Commit Loss: 0.000883 | Perplexity: 1060.985379
Trainning Epoch:  34%|███▍      | 225/665 [6:28:30<12:39:19, 103.54s/it]2025-09-14 23:01:38,173 Stage: Train 0.5 | Epoch: 225 | Iter: 169600 | Total Loss: 0.002776 | Recon Loss: 0.002335 | Commit Loss: 0.000882 | Perplexity: 1060.525005
2025-09-14 23:02:05,696 Stage: Train 0.5 | Epoch: 225 | Iter: 169800 | Total Loss: 0.002723 | Recon Loss: 0.002284 | Commit Loss: 0.000878 | Perplexity: 1062.235664
2025-09-14 23:02:33,359 Stage: Train 0.5 | Epoch: 225 | Iter: 170000 | Total Loss: 0.002768 | Recon Loss: 0.002328 | Commit Loss: 0.000879 | Perplexity: 1059.942451
Trainning Epoch:  34%|███▍      | 226/665 [6:30:13<12:38:03, 103.61s/it]2025-09-14 23:03:00,904 Stage: Train 0.5 | Epoch: 226 | Iter: 170200 | Total Loss: 0.002752 | Recon Loss: 0.002311 | Commit Loss: 0.000882 | Perplexity: 1060.823368
2025-09-14 23:03:28,456 Stage: Train 0.5 | Epoch: 226 | Iter: 170400 | Total Loss: 0.002786 | Recon Loss: 0.002347 | Commit Loss: 0.000877 | Perplexity: 1057.907528
2025-09-14 23:03:56,127 Stage: Train 0.5 | Epoch: 226 | Iter: 170600 | Total Loss: 0.002721 | Recon Loss: 0.002280 | Commit Loss: 0.000881 | Perplexity: 1062.887003
2025-09-14 23:04:23,656 Stage: Train 0.5 | Epoch: 226 | Iter: 170800 | Total Loss: 0.002753 | Recon Loss: 0.002314 | Commit Loss: 0.000877 | Perplexity: 1061.804164
Trainning Epoch:  34%|███▍      | 227/665 [6:31:57<12:36:45, 103.66s/it]2025-09-14 23:04:51,174 Stage: Train 0.5 | Epoch: 227 | Iter: 171000 | Total Loss: 0.002734 | Recon Loss: 0.002296 | Commit Loss: 0.000876 | Perplexity: 1059.979156
2025-09-14 23:05:18,725 Stage: Train 0.5 | Epoch: 227 | Iter: 171200 | Total Loss: 0.002767 | Recon Loss: 0.002328 | Commit Loss: 0.000877 | Perplexity: 1064.341378
2025-09-14 23:05:46,276 Stage: Train 0.5 | Epoch: 227 | Iter: 171400 | Total Loss: 0.002760 | Recon Loss: 0.002318 | Commit Loss: 0.000885 | Perplexity: 1065.247943
2025-09-14 23:06:13,879 Stage: Train 0.5 | Epoch: 227 | Iter: 171600 | Total Loss: 0.002818 | Recon Loss: 0.002379 | Commit Loss: 0.000877 | Perplexity: 1055.561685
Trainning Epoch:  34%|███▍      | 228/665 [6:33:41<12:35:06, 103.68s/it]2025-09-14 23:06:41,417 Stage: Train 0.5 | Epoch: 228 | Iter: 171800 | Total Loss: 0.002760 | Recon Loss: 0.002324 | Commit Loss: 0.000872 | Perplexity: 1056.854916
2025-09-14 23:07:09,185 Stage: Train 0.5 | Epoch: 228 | Iter: 172000 | Total Loss: 0.002732 | Recon Loss: 0.002291 | Commit Loss: 0.000882 | Perplexity: 1062.904908
2025-09-14 23:07:36,863 Stage: Train 0.5 | Epoch: 228 | Iter: 172200 | Total Loss: 0.002802 | Recon Loss: 0.002361 | Commit Loss: 0.000880 | Perplexity: 1065.407574
2025-09-14 23:08:04,482 Stage: Train 0.5 | Epoch: 228 | Iter: 172400 | Total Loss: 0.002681 | Recon Loss: 0.002243 | Commit Loss: 0.000875 | Perplexity: 1059.534256
Trainning Epoch:  34%|███▍      | 229/665 [6:35:25<12:34:23, 103.82s/it]2025-09-14 23:08:31,935 Stage: Train 0.5 | Epoch: 229 | Iter: 172600 | Total Loss: 0.002742 | Recon Loss: 0.002305 | Commit Loss: 0.000874 | Perplexity: 1056.596986
2025-09-14 23:08:59,405 Stage: Train 0.5 | Epoch: 229 | Iter: 172800 | Total Loss: 0.002686 | Recon Loss: 0.002248 | Commit Loss: 0.000875 | Perplexity: 1062.338404
2025-09-14 23:09:26,908 Stage: Train 0.5 | Epoch: 229 | Iter: 173000 | Total Loss: 0.002815 | Recon Loss: 0.002374 | Commit Loss: 0.000883 | Perplexity: 1063.003807
Trainning Epoch:  35%|███▍      | 230/665 [6:37:09<12:31:58, 103.72s/it]2025-09-14 23:09:54,398 Stage: Train 0.5 | Epoch: 230 | Iter: 173200 | Total Loss: 0.002714 | Recon Loss: 0.002274 | Commit Loss: 0.000879 | Perplexity: 1064.405913
2025-09-14 23:10:22,016 Stage: Train 0.5 | Epoch: 230 | Iter: 173400 | Total Loss: 0.002737 | Recon Loss: 0.002299 | Commit Loss: 0.000876 | Perplexity: 1064.066696
2025-09-14 23:10:49,557 Stage: Train 0.5 | Epoch: 230 | Iter: 173600 | Total Loss: 0.002715 | Recon Loss: 0.002275 | Commit Loss: 0.000881 | Perplexity: 1063.292372
2025-09-14 23:11:17,083 Stage: Train 0.5 | Epoch: 230 | Iter: 173800 | Total Loss: 0.002711 | Recon Loss: 0.002271 | Commit Loss: 0.000879 | Perplexity: 1058.763356
Trainning Epoch:  35%|███▍      | 231/665 [6:38:52<12:30:12, 103.72s/it]2025-09-14 23:11:44,576 Stage: Train 0.5 | Epoch: 231 | Iter: 174000 | Total Loss: 0.002796 | Recon Loss: 0.002358 | Commit Loss: 0.000877 | Perplexity: 1058.984823
2025-09-14 23:12:12,104 Stage: Train 0.5 | Epoch: 231 | Iter: 174200 | Total Loss: 0.002688 | Recon Loss: 0.002250 | Commit Loss: 0.000876 | Perplexity: 1064.708226
2025-09-14 23:12:39,641 Stage: Train 0.5 | Epoch: 231 | Iter: 174400 | Total Loss: 0.002744 | Recon Loss: 0.002308 | Commit Loss: 0.000871 | Perplexity: 1057.903502
2025-09-14 23:13:07,144 Stage: Train 0.5 | Epoch: 231 | Iter: 174600 | Total Loss: 0.002797 | Recon Loss: 0.002359 | Commit Loss: 0.000875 | Perplexity: 1059.959810
Trainning Epoch:  35%|███▍      | 232/665 [6:40:36<12:28:08, 103.67s/it]2025-09-14 23:13:34,589 Stage: Train 0.5 | Epoch: 232 | Iter: 174800 | Total Loss: 0.002779 | Recon Loss: 0.002341 | Commit Loss: 0.000876 | Perplexity: 1058.602356
2025-09-14 23:14:02,165 Stage: Train 0.5 | Epoch: 232 | Iter: 175000 | Total Loss: 0.002707 | Recon Loss: 0.002269 | Commit Loss: 0.000875 | Perplexity: 1060.559560
2025-09-14 23:14:29,567 Stage: Train 0.5 | Epoch: 232 | Iter: 175200 | Total Loss: 0.002730 | Recon Loss: 0.002290 | Commit Loss: 0.000881 | Perplexity: 1065.904032
2025-09-14 23:14:56,975 Stage: Train 0.5 | Epoch: 232 | Iter: 175400 | Total Loss: 0.002765 | Recon Loss: 0.002325 | Commit Loss: 0.000880 | Perplexity: 1061.892106
Trainning Epoch:  35%|███▌      | 233/665 [6:42:19<12:25:47, 103.58s/it]2025-09-14 23:15:24,455 Stage: Train 0.5 | Epoch: 233 | Iter: 175600 | Total Loss: 0.002703 | Recon Loss: 0.002266 | Commit Loss: 0.000874 | Perplexity: 1062.034115
2025-09-14 23:15:52,199 Stage: Train 0.5 | Epoch: 233 | Iter: 175800 | Total Loss: 0.002723 | Recon Loss: 0.002284 | Commit Loss: 0.000878 | Perplexity: 1061.186287
2025-09-14 23:16:19,919 Stage: Train 0.5 | Epoch: 233 | Iter: 176000 | Total Loss: 0.002691 | Recon Loss: 0.002253 | Commit Loss: 0.000877 | Perplexity: 1063.660293
2025-09-14 23:16:47,471 Stage: Train 0.5 | Epoch: 233 | Iter: 176200 | Total Loss: 0.002723 | Recon Loss: 0.002286 | Commit Loss: 0.000874 | Perplexity: 1059.471814
Trainning Epoch:  35%|███▌      | 234/665 [6:44:03<12:25:02, 103.72s/it]2025-09-14 23:17:15,093 Stage: Train 0.5 | Epoch: 234 | Iter: 176400 | Total Loss: 0.002726 | Recon Loss: 0.002287 | Commit Loss: 0.000878 | Perplexity: 1062.145431
2025-09-14 23:17:42,706 Stage: Train 0.5 | Epoch: 234 | Iter: 176600 | Total Loss: 0.002723 | Recon Loss: 0.002286 | Commit Loss: 0.000873 | Perplexity: 1060.918255
2025-09-14 23:18:10,249 Stage: Train 0.5 | Epoch: 234 | Iter: 176800 | Total Loss: 0.002744 | Recon Loss: 0.002304 | Commit Loss: 0.000880 | Perplexity: 1058.363904
Trainning Epoch:  35%|███▌      | 235/665 [6:45:47<12:23:42, 103.77s/it]2025-09-14 23:18:37,795 Stage: Train 0.5 | Epoch: 235 | Iter: 177000 | Total Loss: 0.002690 | Recon Loss: 0.002253 | Commit Loss: 0.000874 | Perplexity: 1061.286629
2025-09-14 23:19:05,283 Stage: Train 0.5 | Epoch: 235 | Iter: 177200 | Total Loss: 0.002750 | Recon Loss: 0.002311 | Commit Loss: 0.000878 | Perplexity: 1061.819112
2025-09-14 23:19:32,717 Stage: Train 0.5 | Epoch: 235 | Iter: 177400 | Total Loss: 0.002747 | Recon Loss: 0.002310 | Commit Loss: 0.000874 | Perplexity: 1062.373658
2025-09-14 23:20:00,195 Stage: Train 0.5 | Epoch: 235 | Iter: 177600 | Total Loss: 0.002708 | Recon Loss: 0.002269 | Commit Loss: 0.000878 | Perplexity: 1061.632948
Trainning Epoch:  35%|███▌      | 236/665 [6:47:30<12:21:05, 103.65s/it]2025-09-14 23:20:27,566 Stage: Train 0.5 | Epoch: 236 | Iter: 177800 | Total Loss: 0.002758 | Recon Loss: 0.002324 | Commit Loss: 0.000868 | Perplexity: 1055.701816
2025-09-14 23:20:55,114 Stage: Train 0.5 | Epoch: 236 | Iter: 178000 | Total Loss: 0.002726 | Recon Loss: 0.002287 | Commit Loss: 0.000877 | Perplexity: 1063.308723
2025-09-14 23:21:22,604 Stage: Train 0.5 | Epoch: 236 | Iter: 178200 | Total Loss: 0.002660 | Recon Loss: 0.002225 | Commit Loss: 0.000870 | Perplexity: 1059.944026
2025-09-14 23:21:50,019 Stage: Train 0.5 | Epoch: 236 | Iter: 178400 | Total Loss: 0.002744 | Recon Loss: 0.002307 | Commit Loss: 0.000875 | Perplexity: 1060.647126
Trainning Epoch:  36%|███▌      | 237/665 [6:49:14<12:18:44, 103.56s/it]2025-09-14 23:22:17,367 Stage: Train 0.5 | Epoch: 237 | Iter: 178600 | Total Loss: 0.002710 | Recon Loss: 0.002275 | Commit Loss: 0.000871 | Perplexity: 1056.027516
2025-09-14 23:22:44,777 Stage: Train 0.5 | Epoch: 237 | Iter: 178800 | Total Loss: 0.002736 | Recon Loss: 0.002299 | Commit Loss: 0.000873 | Perplexity: 1061.952611
2025-09-14 23:23:12,180 Stage: Train 0.5 | Epoch: 237 | Iter: 179000 | Total Loss: 0.002694 | Recon Loss: 0.002255 | Commit Loss: 0.000878 | Perplexity: 1056.954137
2025-09-14 23:23:39,600 Stage: Train 0.5 | Epoch: 237 | Iter: 179200 | Total Loss: 0.002714 | Recon Loss: 0.002273 | Commit Loss: 0.000882 | Perplexity: 1064.379690
Trainning Epoch:  36%|███▌      | 238/665 [6:50:57<12:16:09, 103.44s/it]2025-09-14 23:24:07,053 Stage: Train 0.5 | Epoch: 238 | Iter: 179400 | Total Loss: 0.002799 | Recon Loss: 0.002361 | Commit Loss: 0.000876 | Perplexity: 1064.015102
2025-09-14 23:24:34,879 Stage: Train 0.5 | Epoch: 238 | Iter: 179600 | Total Loss: 0.002780 | Recon Loss: 0.002348 | Commit Loss: 0.000864 | Perplexity: 1056.100564
2025-09-14 23:25:02,456 Stage: Train 0.5 | Epoch: 238 | Iter: 179800 | Total Loss: 0.002707 | Recon Loss: 0.002268 | Commit Loss: 0.000878 | Perplexity: 1056.436525
Trainning Epoch:  36%|███▌      | 239/665 [6:52:41<12:15:26, 103.58s/it]2025-09-14 23:25:29,947 Stage: Train 0.5 | Epoch: 239 | Iter: 180000 | Total Loss: 0.002732 | Recon Loss: 0.002298 | Commit Loss: 0.000867 | Perplexity: 1059.177940
2025-09-14 23:25:29,947 Saving model at iteration 180000
2025-09-14 23:25:30,118 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_240_step_180000
2025-09-14 23:25:30,329 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_240_step_180000/pytorch_model.bin
2025-09-14 23:25:30,655 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_240_step_180000/optimizer.bin
2025-09-14 23:25:30,656 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_240_step_180000/scheduler.bin
2025-09-14 23:25:30,656 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_240_step_180000/random_states_0.pkl
2025-09-14 23:25:58,242 Stage: Train 0.5 | Epoch: 239 | Iter: 180200 | Total Loss: 0.002722 | Recon Loss: 0.002286 | Commit Loss: 0.000872 | Perplexity: 1063.206362
2025-09-14 23:26:25,724 Stage: Train 0.5 | Epoch: 239 | Iter: 180400 | Total Loss: 0.002737 | Recon Loss: 0.002303 | Commit Loss: 0.000869 | Perplexity: 1058.164316
2025-09-14 23:26:53,116 Stage: Train 0.5 | Epoch: 239 | Iter: 180600 | Total Loss: 0.002716 | Recon Loss: 0.002277 | Commit Loss: 0.000877 | Perplexity: 1060.803994
Trainning Epoch:  36%|███▌      | 240/665 [6:54:25<12:15:04, 103.78s/it]2025-09-14 23:27:20,583 Stage: Train 0.5 | Epoch: 240 | Iter: 180800 | Total Loss: 0.002730 | Recon Loss: 0.002293 | Commit Loss: 0.000874 | Perplexity: 1059.901591
2025-09-14 23:27:48,027 Stage: Train 0.5 | Epoch: 240 | Iter: 181000 | Total Loss: 0.002770 | Recon Loss: 0.002336 | Commit Loss: 0.000868 | Perplexity: 1057.555220
2025-09-14 23:28:15,420 Stage: Train 0.5 | Epoch: 240 | Iter: 181200 | Total Loss: 0.002766 | Recon Loss: 0.002328 | Commit Loss: 0.000877 | Perplexity: 1061.362119
2025-09-14 23:28:42,884 Stage: Train 0.5 | Epoch: 240 | Iter: 181400 | Total Loss: 0.002706 | Recon Loss: 0.002270 | Commit Loss: 0.000874 | Perplexity: 1064.735511
Trainning Epoch:  36%|███▌      | 241/665 [6:56:08<12:12:12, 103.62s/it]2025-09-14 23:29:10,396 Stage: Train 0.5 | Epoch: 241 | Iter: 181600 | Total Loss: 0.002692 | Recon Loss: 0.002259 | Commit Loss: 0.000867 | Perplexity: 1059.604155
2025-09-14 23:29:37,858 Stage: Train 0.5 | Epoch: 241 | Iter: 181800 | Total Loss: 0.002678 | Recon Loss: 0.002241 | Commit Loss: 0.000875 | Perplexity: 1064.512129
2025-09-14 23:30:05,303 Stage: Train 0.5 | Epoch: 241 | Iter: 182000 | Total Loss: 0.002762 | Recon Loss: 0.002323 | Commit Loss: 0.000879 | Perplexity: 1063.803845
2025-09-14 23:30:32,825 Stage: Train 0.5 | Epoch: 241 | Iter: 182200 | Total Loss: 0.002722 | Recon Loss: 0.002289 | Commit Loss: 0.000866 | Perplexity: 1059.095755
Trainning Epoch:  36%|███▋      | 242/665 [6:57:52<12:10:15, 103.58s/it]2025-09-14 23:31:00,327 Stage: Train 0.5 | Epoch: 242 | Iter: 182400 | Total Loss: 0.002726 | Recon Loss: 0.002292 | Commit Loss: 0.000868 | Perplexity: 1058.287021
2025-09-14 23:31:27,919 Stage: Train 0.5 | Epoch: 242 | Iter: 182600 | Total Loss: 0.002736 | Recon Loss: 0.002303 | Commit Loss: 0.000866 | Perplexity: 1058.202981
2025-09-14 23:31:55,470 Stage: Train 0.5 | Epoch: 242 | Iter: 182800 | Total Loss: 0.002718 | Recon Loss: 0.002279 | Commit Loss: 0.000878 | Perplexity: 1063.546183
Trainning Epoch:  37%|███▋      | 243/665 [6:59:36<12:09:03, 103.66s/it]2025-09-14 23:32:23,100 Stage: Train 0.5 | Epoch: 243 | Iter: 183000 | Total Loss: 0.002670 | Recon Loss: 0.002236 | Commit Loss: 0.000868 | Perplexity: 1062.044772
2025-09-14 23:32:50,595 Stage: Train 0.5 | Epoch: 243 | Iter: 183200 | Total Loss: 0.002828 | Recon Loss: 0.002394 | Commit Loss: 0.000866 | Perplexity: 1060.786164
2025-09-14 23:33:18,084 Stage: Train 0.5 | Epoch: 243 | Iter: 183400 | Total Loss: 0.002669 | Recon Loss: 0.002232 | Commit Loss: 0.000872 | Perplexity: 1060.906333
2025-09-14 23:33:45,662 Stage: Train 0.5 | Epoch: 243 | Iter: 183600 | Total Loss: 0.002720 | Recon Loss: 0.002285 | Commit Loss: 0.000870 | Perplexity: 1060.257334
Trainning Epoch:  37%|███▋      | 244/665 [7:01:19<12:06:58, 103.61s/it]2025-09-14 23:34:13,004 Stage: Train 0.5 | Epoch: 244 | Iter: 183800 | Total Loss: 0.002724 | Recon Loss: 0.002286 | Commit Loss: 0.000874 | Perplexity: 1063.763511
2025-09-14 23:34:40,387 Stage: Train 0.5 | Epoch: 244 | Iter: 184000 | Total Loss: 0.002669 | Recon Loss: 0.002236 | Commit Loss: 0.000866 | Perplexity: 1061.177534
2025-09-14 23:35:07,779 Stage: Train 0.5 | Epoch: 244 | Iter: 184200 | Total Loss: 0.002801 | Recon Loss: 0.002367 | Commit Loss: 0.000868 | Perplexity: 1059.852673
2025-09-14 23:35:35,185 Stage: Train 0.5 | Epoch: 244 | Iter: 184400 | Total Loss: 0.002675 | Recon Loss: 0.002242 | Commit Loss: 0.000866 | Perplexity: 1059.534774
Trainning Epoch:  37%|███▋      | 245/665 [7:03:02<12:04:23, 103.48s/it]2025-09-14 23:36:02,636 Stage: Train 0.5 | Epoch: 245 | Iter: 184600 | Total Loss: 0.002851 | Recon Loss: 0.002417 | Commit Loss: 0.000867 | Perplexity: 1058.978276
2025-09-14 23:36:30,029 Stage: Train 0.5 | Epoch: 245 | Iter: 184800 | Total Loss: 0.002675 | Recon Loss: 0.002239 | Commit Loss: 0.000873 | Perplexity: 1063.392594
2025-09-14 23:36:57,424 Stage: Train 0.5 | Epoch: 245 | Iter: 185000 | Total Loss: 0.002722 | Recon Loss: 0.002285 | Commit Loss: 0.000874 | Perplexity: 1066.385963
2025-09-14 23:37:24,814 Stage: Train 0.5 | Epoch: 245 | Iter: 185200 | Total Loss: 0.002664 | Recon Loss: 0.002233 | Commit Loss: 0.000864 | Perplexity: 1059.028059
Trainning Epoch:  37%|███▋      | 246/665 [7:04:46<12:01:50, 103.37s/it]2025-09-14 23:37:52,166 Stage: Train 0.5 | Epoch: 246 | Iter: 185400 | Total Loss: 0.002720 | Recon Loss: 0.002288 | Commit Loss: 0.000863 | Perplexity: 1062.630708
2025-09-14 23:38:19,558 Stage: Train 0.5 | Epoch: 246 | Iter: 185600 | Total Loss: 0.002665 | Recon Loss: 0.002231 | Commit Loss: 0.000868 | Perplexity: 1066.261501
2025-09-14 23:38:46,968 Stage: Train 0.5 | Epoch: 246 | Iter: 185800 | Total Loss: 0.002752 | Recon Loss: 0.002316 | Commit Loss: 0.000872 | Perplexity: 1062.822249
Trainning Epoch:  37%|███▋      | 247/665 [7:06:29<11:59:49, 103.32s/it]2025-09-14 23:39:14,437 Stage: Train 0.5 | Epoch: 247 | Iter: 186000 | Total Loss: 0.002722 | Recon Loss: 0.002288 | Commit Loss: 0.000867 | Perplexity: 1054.727203
2025-09-14 23:39:41,829 Stage: Train 0.5 | Epoch: 247 | Iter: 186200 | Total Loss: 0.002647 | Recon Loss: 0.002210 | Commit Loss: 0.000874 | Perplexity: 1065.699713
2025-09-14 23:40:09,231 Stage: Train 0.5 | Epoch: 247 | Iter: 186400 | Total Loss: 0.002716 | Recon Loss: 0.002285 | Commit Loss: 0.000862 | Perplexity: 1058.833689
2025-09-14 23:40:36,635 Stage: Train 0.5 | Epoch: 247 | Iter: 186600 | Total Loss: 0.002780 | Recon Loss: 0.002347 | Commit Loss: 0.000868 | Perplexity: 1062.616953
Trainning Epoch:  37%|███▋      | 248/665 [7:08:12<11:57:39, 103.26s/it]2025-09-14 23:41:03,988 Stage: Train 0.5 | Epoch: 248 | Iter: 186800 | Total Loss: 0.002682 | Recon Loss: 0.002251 | Commit Loss: 0.000863 | Perplexity: 1057.533822
2025-09-14 23:41:31,484 Stage: Train 0.5 | Epoch: 248 | Iter: 187000 | Total Loss: 0.002693 | Recon Loss: 0.002258 | Commit Loss: 0.000870 | Perplexity: 1065.480018
2025-09-14 23:41:59,287 Stage: Train 0.5 | Epoch: 248 | Iter: 187200 | Total Loss: 0.002721 | Recon Loss: 0.002286 | Commit Loss: 0.000869 | Perplexity: 1061.941522
2025-09-14 23:42:26,786 Stage: Train 0.5 | Epoch: 248 | Iter: 187400 | Total Loss: 0.002726 | Recon Loss: 0.002295 | Commit Loss: 0.000861 | Perplexity: 1057.726288
Trainning Epoch:  37%|███▋      | 249/665 [7:09:56<11:57:19, 103.46s/it]2025-09-14 23:42:54,456 Stage: Train 0.5 | Epoch: 249 | Iter: 187600 | Total Loss: 0.002783 | Recon Loss: 0.002350 | Commit Loss: 0.000866 | Perplexity: 1056.467176
2025-09-14 23:43:22,060 Stage: Train 0.5 | Epoch: 249 | Iter: 187800 | Total Loss: 0.002617 | Recon Loss: 0.002184 | Commit Loss: 0.000864 | Perplexity: 1065.499028
2025-09-14 23:43:49,630 Stage: Train 0.5 | Epoch: 249 | Iter: 188000 | Total Loss: 0.002613 | Recon Loss: 0.002179 | Commit Loss: 0.000869 | Perplexity: 1064.105359
2025-09-14 23:44:17,145 Stage: Train 0.5 | Epoch: 249 | Iter: 188200 | Total Loss: 0.002682 | Recon Loss: 0.002244 | Commit Loss: 0.000876 | Perplexity: 1063.937085
Trainning Epoch:  38%|███▊      | 250/665 [7:11:40<11:56:10, 103.54s/it]2025-09-14 23:44:45,045 Stage: Train 0.5 | Epoch: 250 | Iter: 188400 | Total Loss: 0.002814 | Recon Loss: 0.002378 | Commit Loss: 0.000871 | Perplexity: 1059.479832
2025-09-14 23:45:12,995 Stage: Train 0.5 | Epoch: 250 | Iter: 188600 | Total Loss: 0.002725 | Recon Loss: 0.002294 | Commit Loss: 0.000861 | Perplexity: 1060.482051
2025-09-14 23:45:40,502 Stage: Train 0.5 | Epoch: 250 | Iter: 188800 | Total Loss: 0.002717 | Recon Loss: 0.002284 | Commit Loss: 0.000865 | Perplexity: 1061.653567
2025-09-14 23:46:08,056 Stage: Train 0.5 | Epoch: 250 | Iter: 189000 | Total Loss: 0.002740 | Recon Loss: 0.002310 | Commit Loss: 0.000859 | Perplexity: 1058.651919
Trainning Epoch:  38%|███▊      | 251/665 [7:13:24<11:56:18, 103.81s/it]2025-09-14 23:46:35,449 Stage: Train 0.5 | Epoch: 251 | Iter: 189200 | Total Loss: 0.002668 | Recon Loss: 0.002235 | Commit Loss: 0.000865 | Perplexity: 1063.359397
2025-09-14 23:47:03,009 Stage: Train 0.5 | Epoch: 251 | Iter: 189400 | Total Loss: 0.002707 | Recon Loss: 0.002274 | Commit Loss: 0.000865 | Perplexity: 1058.910591
2025-09-14 23:47:30,473 Stage: Train 0.5 | Epoch: 251 | Iter: 189600 | Total Loss: 0.002691 | Recon Loss: 0.002260 | Commit Loss: 0.000863 | Perplexity: 1059.807732
Trainning Epoch:  38%|███▊      | 252/665 [7:15:07<11:53:50, 103.71s/it]2025-09-14 23:47:57,933 Stage: Train 0.5 | Epoch: 252 | Iter: 189800 | Total Loss: 0.002667 | Recon Loss: 0.002233 | Commit Loss: 0.000867 | Perplexity: 1062.862787
2025-09-14 23:48:25,532 Stage: Train 0.5 | Epoch: 252 | Iter: 190000 | Total Loss: 0.002797 | Recon Loss: 0.002365 | Commit Loss: 0.000864 | Perplexity: 1060.527926
2025-09-14 23:48:52,929 Stage: Train 0.5 | Epoch: 252 | Iter: 190200 | Total Loss: 0.002629 | Recon Loss: 0.002199 | Commit Loss: 0.000860 | Perplexity: 1060.014436
2025-09-14 23:49:20,359 Stage: Train 0.5 | Epoch: 252 | Iter: 190400 | Total Loss: 0.002768 | Recon Loss: 0.002333 | Commit Loss: 0.000870 | Perplexity: 1061.928931
Trainning Epoch:  38%|███▊      | 253/665 [7:16:51<11:51:36, 103.63s/it]2025-09-14 23:49:47,831 Stage: Train 0.5 | Epoch: 253 | Iter: 190600 | Total Loss: 0.002679 | Recon Loss: 0.002243 | Commit Loss: 0.000873 | Perplexity: 1062.503297
2025-09-14 23:50:15,272 Stage: Train 0.5 | Epoch: 253 | Iter: 190800 | Total Loss: 0.002676 | Recon Loss: 0.002244 | Commit Loss: 0.000862 | Perplexity: 1062.855777
2025-09-14 23:50:42,746 Stage: Train 0.5 | Epoch: 253 | Iter: 191000 | Total Loss: 0.002650 | Recon Loss: 0.002219 | Commit Loss: 0.000862 | Perplexity: 1060.261125
2025-09-14 23:51:10,206 Stage: Train 0.5 | Epoch: 253 | Iter: 191200 | Total Loss: 0.002709 | Recon Loss: 0.002278 | Commit Loss: 0.000861 | Perplexity: 1063.157115
Trainning Epoch:  38%|███▊      | 254/665 [7:18:34<11:49:19, 103.55s/it]2025-09-14 23:51:37,660 Stage: Train 0.5 | Epoch: 254 | Iter: 191400 | Total Loss: 0.002732 | Recon Loss: 0.002299 | Commit Loss: 0.000866 | Perplexity: 1060.963175
2025-09-14 23:52:05,142 Stage: Train 0.5 | Epoch: 254 | Iter: 191600 | Total Loss: 0.002747 | Recon Loss: 0.002310 | Commit Loss: 0.000875 | Perplexity: 1064.819596
2025-09-14 23:52:32,624 Stage: Train 0.5 | Epoch: 254 | Iter: 191800 | Total Loss: 0.002607 | Recon Loss: 0.002176 | Commit Loss: 0.000861 | Perplexity: 1059.782575
2025-09-14 23:53:00,207 Stage: Train 0.5 | Epoch: 254 | Iter: 192000 | Total Loss: 0.002721 | Recon Loss: 0.002293 | Commit Loss: 0.000857 | Perplexity: 1060.427117
Trainning Epoch:  38%|███▊      | 255/665 [7:20:18<11:47:31, 103.54s/it]2025-09-14 23:53:27,654 Stage: Train 0.5 | Epoch: 255 | Iter: 192200 | Total Loss: 0.002630 | Recon Loss: 0.002200 | Commit Loss: 0.000858 | Perplexity: 1061.754624
2025-09-14 23:53:55,113 Stage: Train 0.5 | Epoch: 255 | Iter: 192400 | Total Loss: 0.002704 | Recon Loss: 0.002266 | Commit Loss: 0.000877 | Perplexity: 1068.871301
2025-09-14 23:54:22,598 Stage: Train 0.5 | Epoch: 255 | Iter: 192600 | Total Loss: 0.002723 | Recon Loss: 0.002293 | Commit Loss: 0.000860 | Perplexity: 1057.524968
Trainning Epoch:  38%|███▊      | 256/665 [7:22:01<11:45:34, 103.51s/it]2025-09-14 23:54:50,047 Stage: Train 0.5 | Epoch: 256 | Iter: 192800 | Total Loss: 0.002646 | Recon Loss: 0.002220 | Commit Loss: 0.000851 | Perplexity: 1055.634684
2025-09-14 23:55:17,526 Stage: Train 0.5 | Epoch: 256 | Iter: 193000 | Total Loss: 0.002658 | Recon Loss: 0.002225 | Commit Loss: 0.000866 | Perplexity: 1063.786413
2025-09-14 23:55:45,024 Stage: Train 0.5 | Epoch: 256 | Iter: 193200 | Total Loss: 0.002678 | Recon Loss: 0.002249 | Commit Loss: 0.000858 | Perplexity: 1057.892820
2025-09-14 23:56:12,772 Stage: Train 0.5 | Epoch: 256 | Iter: 193400 | Total Loss: 0.002651 | Recon Loss: 0.002221 | Commit Loss: 0.000860 | Perplexity: 1062.131066
Trainning Epoch:  39%|███▊      | 257/665 [7:23:45<11:44:18, 103.57s/it]2025-09-14 23:56:40,252 Stage: Train 0.5 | Epoch: 257 | Iter: 193600 | Total Loss: 0.002712 | Recon Loss: 0.002276 | Commit Loss: 0.000873 | Perplexity: 1065.420332
2025-09-14 23:57:07,803 Stage: Train 0.5 | Epoch: 257 | Iter: 193800 | Total Loss: 0.002642 | Recon Loss: 0.002212 | Commit Loss: 0.000861 | Perplexity: 1059.123605
2025-09-14 23:57:35,329 Stage: Train 0.5 | Epoch: 257 | Iter: 194000 | Total Loss: 0.002651 | Recon Loss: 0.002219 | Commit Loss: 0.000864 | Perplexity: 1062.118014
2025-09-14 23:58:02,894 Stage: Train 0.5 | Epoch: 257 | Iter: 194200 | Total Loss: 0.002735 | Recon Loss: 0.002304 | Commit Loss: 0.000862 | Perplexity: 1060.021331
Trainning Epoch:  39%|███▉      | 258/665 [7:25:29<11:42:45, 103.60s/it]2025-09-14 23:58:30,388 Stage: Train 0.5 | Epoch: 258 | Iter: 194400 | Total Loss: 0.002660 | Recon Loss: 0.002228 | Commit Loss: 0.000865 | Perplexity: 1063.483730
2025-09-14 23:58:57,927 Stage: Train 0.5 | Epoch: 258 | Iter: 194600 | Total Loss: 0.002664 | Recon Loss: 0.002231 | Commit Loss: 0.000867 | Perplexity: 1063.710062
2025-09-14 23:59:25,452 Stage: Train 0.5 | Epoch: 258 | Iter: 194800 | Total Loss: 0.002718 | Recon Loss: 0.002286 | Commit Loss: 0.000865 | Perplexity: 1061.375746
2025-09-14 23:59:53,099 Stage: Train 0.5 | Epoch: 258 | Iter: 195000 | Total Loss: 0.002603 | Recon Loss: 0.002172 | Commit Loss: 0.000863 | Perplexity: 1062.561310
Trainning Epoch:  39%|███▉      | 259/665 [7:27:12<11:41:17, 103.64s/it]2025-09-15 00:00:20,717 Stage: Train 0.5 | Epoch: 259 | Iter: 195200 | Total Loss: 0.002653 | Recon Loss: 0.002225 | Commit Loss: 0.000857 | Perplexity: 1064.522878
2025-09-15 00:00:48,314 Stage: Train 0.5 | Epoch: 259 | Iter: 195400 | Total Loss: 0.002625 | Recon Loss: 0.002190 | Commit Loss: 0.000871 | Perplexity: 1065.577711
2025-09-15 00:01:15,907 Stage: Train 0.5 | Epoch: 259 | Iter: 195600 | Total Loss: 0.002738 | Recon Loss: 0.002307 | Commit Loss: 0.000863 | Perplexity: 1060.883744
Trainning Epoch:  39%|███▉      | 260/665 [7:28:56<11:40:13, 103.74s/it]2025-09-15 00:01:43,494 Stage: Train 0.5 | Epoch: 260 | Iter: 195800 | Total Loss: 0.002647 | Recon Loss: 0.002215 | Commit Loss: 0.000865 | Perplexity: 1060.240752
2025-09-15 00:02:11,096 Stage: Train 0.5 | Epoch: 260 | Iter: 196000 | Total Loss: 0.002666 | Recon Loss: 0.002233 | Commit Loss: 0.000867 | Perplexity: 1065.181263
2025-09-15 00:02:38,662 Stage: Train 0.5 | Epoch: 260 | Iter: 196200 | Total Loss: 0.002637 | Recon Loss: 0.002208 | Commit Loss: 0.000857 | Perplexity: 1060.758954
2025-09-15 00:03:06,318 Stage: Train 0.5 | Epoch: 260 | Iter: 196400 | Total Loss: 0.002657 | Recon Loss: 0.002225 | Commit Loss: 0.000865 | Perplexity: 1062.217539
Trainning Epoch:  39%|███▉      | 261/665 [7:30:40<11:38:43, 103.77s/it]2025-09-15 00:03:33,811 Stage: Train 0.5 | Epoch: 261 | Iter: 196600 | Total Loss: 0.002618 | Recon Loss: 0.002187 | Commit Loss: 0.000862 | Perplexity: 1062.166525
2025-09-15 00:04:01,383 Stage: Train 0.5 | Epoch: 261 | Iter: 196800 | Total Loss: 0.002683 | Recon Loss: 0.002252 | Commit Loss: 0.000862 | Perplexity: 1061.611057
2025-09-15 00:04:28,954 Stage: Train 0.5 | Epoch: 261 | Iter: 197000 | Total Loss: 0.002666 | Recon Loss: 0.002231 | Commit Loss: 0.000870 | Perplexity: 1065.047983
2025-09-15 00:04:56,551 Stage: Train 0.5 | Epoch: 261 | Iter: 197200 | Total Loss: 0.002634 | Recon Loss: 0.002207 | Commit Loss: 0.000855 | Perplexity: 1059.662870
Trainning Epoch:  39%|███▉      | 262/665 [7:32:24<11:37:00, 103.77s/it]2025-09-15 00:05:24,060 Stage: Train 0.5 | Epoch: 262 | Iter: 197400 | Total Loss: 0.002721 | Recon Loss: 0.002292 | Commit Loss: 0.000858 | Perplexity: 1060.282607
2025-09-15 00:05:51,612 Stage: Train 0.5 | Epoch: 262 | Iter: 197600 | Total Loss: 0.002605 | Recon Loss: 0.002177 | Commit Loss: 0.000857 | Perplexity: 1060.701310
2025-09-15 00:06:19,198 Stage: Train 0.5 | Epoch: 262 | Iter: 197800 | Total Loss: 0.002628 | Recon Loss: 0.002194 | Commit Loss: 0.000866 | Perplexity: 1064.648570
2025-09-15 00:06:46,878 Stage: Train 0.5 | Epoch: 262 | Iter: 198000 | Total Loss: 0.002687 | Recon Loss: 0.002255 | Commit Loss: 0.000865 | Perplexity: 1061.093539
Trainning Epoch:  40%|███▉      | 263/665 [7:34:08<11:35:22, 103.79s/it]2025-09-15 00:07:14,291 Stage: Train 0.5 | Epoch: 263 | Iter: 198200 | Total Loss: 0.002708 | Recon Loss: 0.002276 | Commit Loss: 0.000865 | Perplexity: 1062.641666
2025-09-15 00:07:41,759 Stage: Train 0.5 | Epoch: 263 | Iter: 198400 | Total Loss: 0.002638 | Recon Loss: 0.002211 | Commit Loss: 0.000855 | Perplexity: 1061.118538
2025-09-15 00:08:09,232 Stage: Train 0.5 | Epoch: 263 | Iter: 198600 | Total Loss: 0.002709 | Recon Loss: 0.002279 | Commit Loss: 0.000860 | Perplexity: 1059.792816
Trainning Epoch:  40%|███▉      | 264/665 [7:35:51<11:32:50, 103.67s/it]2025-09-15 00:08:36,674 Stage: Train 0.5 | Epoch: 264 | Iter: 198800 | Total Loss: 0.002666 | Recon Loss: 0.002235 | Commit Loss: 0.000861 | Perplexity: 1062.627825
2025-09-15 00:09:04,125 Stage: Train 0.5 | Epoch: 264 | Iter: 199000 | Total Loss: 0.002654 | Recon Loss: 0.002225 | Commit Loss: 0.000859 | Perplexity: 1064.244993
2025-09-15 00:09:31,632 Stage: Train 0.5 | Epoch: 264 | Iter: 199200 | Total Loss: 0.002667 | Recon Loss: 0.002236 | Commit Loss: 0.000862 | Perplexity: 1067.124261
2025-09-15 00:09:59,184 Stage: Train 0.5 | Epoch: 264 | Iter: 199400 | Total Loss: 0.002643 | Recon Loss: 0.002210 | Commit Loss: 0.000865 | Perplexity: 1062.115720
Trainning Epoch:  40%|███▉      | 265/665 [7:37:35<11:30:41, 103.60s/it]2025-09-15 00:10:26,573 Stage: Train 0.5 | Epoch: 265 | Iter: 199600 | Total Loss: 0.002662 | Recon Loss: 0.002238 | Commit Loss: 0.000848 | Perplexity: 1056.060529
2025-09-15 00:10:54,009 Stage: Train 0.5 | Epoch: 265 | Iter: 199800 | Total Loss: 0.002612 | Recon Loss: 0.002185 | Commit Loss: 0.000854 | Perplexity: 1062.975457
2025-09-15 00:11:21,444 Stage: Train 0.5 | Epoch: 265 | Iter: 200000 | Total Loss: 0.002588 | Recon Loss: 0.002157 | Commit Loss: 0.000863 | Perplexity: 1065.244723
2025-09-15 00:11:21,444 Saving model at iteration 200000
2025-09-15 00:11:21,669 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_266_step_200000
2025-09-15 00:11:21,874 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_266_step_200000/pytorch_model.bin
2025-09-15 00:11:22,201 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_266_step_200000/optimizer.bin
2025-09-15 00:11:22,202 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_266_step_200000/scheduler.bin
2025-09-15 00:11:22,202 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_266_step_200000/random_states_0.pkl
2025-09-15 00:11:50,064 Stage: Train 0.5 | Epoch: 265 | Iter: 200200 | Total Loss: 0.002652 | Recon Loss: 0.002220 | Commit Loss: 0.000864 | Perplexity: 1064.103012
Trainning Epoch:  40%|████      | 266/665 [7:39:19<11:31:19, 103.96s/it]2025-09-15 00:12:17,796 Stage: Train 0.5 | Epoch: 266 | Iter: 200400 | Total Loss: 0.002644 | Recon Loss: 0.002210 | Commit Loss: 0.000869 | Perplexity: 1065.665522
2025-09-15 00:12:45,347 Stage: Train 0.5 | Epoch: 266 | Iter: 200600 | Total Loss: 0.002604 | Recon Loss: 0.002176 | Commit Loss: 0.000856 | Perplexity: 1061.116969
2025-09-15 00:13:12,908 Stage: Train 0.5 | Epoch: 266 | Iter: 200800 | Total Loss: 0.002700 | Recon Loss: 0.002268 | Commit Loss: 0.000863 | Perplexity: 1062.274260
2025-09-15 00:13:40,439 Stage: Train 0.5 | Epoch: 266 | Iter: 201000 | Total Loss: 0.002669 | Recon Loss: 0.002237 | Commit Loss: 0.000863 | Perplexity: 1065.466897
Trainning Epoch:  40%|████      | 267/665 [7:41:03<11:28:53, 103.85s/it]2025-09-15 00:14:07,970 Stage: Train 0.5 | Epoch: 267 | Iter: 201200 | Total Loss: 0.002625 | Recon Loss: 0.002197 | Commit Loss: 0.000856 | Perplexity: 1061.180251
2025-09-15 00:14:35,511 Stage: Train 0.5 | Epoch: 267 | Iter: 201400 | Total Loss: 0.002654 | Recon Loss: 0.002221 | Commit Loss: 0.000865 | Perplexity: 1065.347606
2025-09-15 00:15:03,165 Stage: Train 0.5 | Epoch: 267 | Iter: 201600 | Total Loss: 0.002579 | Recon Loss: 0.002149 | Commit Loss: 0.000860 | Perplexity: 1063.176952
2025-09-15 00:15:31,023 Stage: Train 0.5 | Epoch: 267 | Iter: 201800 | Total Loss: 0.002630 | Recon Loss: 0.002201 | Commit Loss: 0.000857 | Perplexity: 1060.133391
Trainning Epoch:  40%|████      | 268/665 [7:42:47<11:27:40, 103.93s/it]2025-09-15 00:15:58,572 Stage: Train 0.5 | Epoch: 268 | Iter: 202000 | Total Loss: 0.002699 | Recon Loss: 0.002273 | Commit Loss: 0.000852 | Perplexity: 1060.435033
2025-09-15 00:16:26,221 Stage: Train 0.5 | Epoch: 268 | Iter: 202200 | Total Loss: 0.002592 | Recon Loss: 0.002164 | Commit Loss: 0.000856 | Perplexity: 1066.538983
2025-09-15 00:16:53,816 Stage: Train 0.5 | Epoch: 268 | Iter: 202400 | Total Loss: 0.002651 | Recon Loss: 0.002219 | Commit Loss: 0.000864 | Perplexity: 1066.368082
Trainning Epoch:  40%|████      | 269/665 [7:44:31<11:26:04, 103.95s/it]2025-09-15 00:17:21,437 Stage: Train 0.5 | Epoch: 269 | Iter: 202600 | Total Loss: 0.002660 | Recon Loss: 0.002228 | Commit Loss: 0.000863 | Perplexity: 1058.533477
2025-09-15 00:17:48,956 Stage: Train 0.5 | Epoch: 269 | Iter: 202800 | Total Loss: 0.002573 | Recon Loss: 0.002148 | Commit Loss: 0.000851 | Perplexity: 1059.849226
2025-09-15 00:18:16,609 Stage: Train 0.5 | Epoch: 269 | Iter: 203000 | Total Loss: 0.002651 | Recon Loss: 0.002218 | Commit Loss: 0.000866 | Perplexity: 1068.017473
2025-09-15 00:18:44,613 Stage: Train 0.5 | Epoch: 269 | Iter: 203200 | Total Loss: 0.002642 | Recon Loss: 0.002213 | Commit Loss: 0.000858 | Perplexity: 1062.441170
Trainning Epoch:  41%|████      | 270/665 [7:46:15<11:24:58, 104.05s/it]2025-09-15 00:19:12,321 Stage: Train 0.5 | Epoch: 270 | Iter: 203400 | Total Loss: 0.002604 | Recon Loss: 0.002173 | Commit Loss: 0.000862 | Perplexity: 1062.655023
2025-09-15 00:19:40,105 Stage: Train 0.5 | Epoch: 270 | Iter: 203600 | Total Loss: 0.002711 | Recon Loss: 0.002283 | Commit Loss: 0.000855 | Perplexity: 1055.630500
2025-09-15 00:20:07,689 Stage: Train 0.5 | Epoch: 270 | Iter: 203800 | Total Loss: 0.002549 | Recon Loss: 0.002123 | Commit Loss: 0.000851 | Perplexity: 1061.325781
2025-09-15 00:20:35,275 Stage: Train 0.5 | Epoch: 270 | Iter: 204000 | Total Loss: 0.002631 | Recon Loss: 0.002202 | Commit Loss: 0.000858 | Perplexity: 1064.243361
Trainning Epoch:  41%|████      | 271/665 [7:47:59<11:23:27, 104.08s/it]2025-09-15 00:21:02,817 Stage: Train 0.5 | Epoch: 271 | Iter: 204200 | Total Loss: 0.002713 | Recon Loss: 0.002282 | Commit Loss: 0.000861 | Perplexity: 1059.827166
2025-09-15 00:21:30,387 Stage: Train 0.5 | Epoch: 271 | Iter: 204400 | Total Loss: 0.002628 | Recon Loss: 0.002199 | Commit Loss: 0.000857 | Perplexity: 1059.466453
2025-09-15 00:21:58,026 Stage: Train 0.5 | Epoch: 271 | Iter: 204600 | Total Loss: 0.002626 | Recon Loss: 0.002199 | Commit Loss: 0.000854 | Perplexity: 1064.050708
2025-09-15 00:22:25,586 Stage: Train 0.5 | Epoch: 271 | Iter: 204800 | Total Loss: 0.002639 | Recon Loss: 0.002209 | Commit Loss: 0.000860 | Perplexity: 1064.746596
Trainning Epoch:  41%|████      | 272/665 [7:49:43<11:21:11, 104.00s/it]2025-09-15 00:22:53,041 Stage: Train 0.5 | Epoch: 272 | Iter: 205000 | Total Loss: 0.002673 | Recon Loss: 0.002250 | Commit Loss: 0.000846 | Perplexity: 1057.099880
2025-09-15 00:23:20,500 Stage: Train 0.5 | Epoch: 272 | Iter: 205200 | Total Loss: 0.002659 | Recon Loss: 0.002230 | Commit Loss: 0.000858 | Perplexity: 1062.360724
2025-09-15 00:23:47,961 Stage: Train 0.5 | Epoch: 272 | Iter: 205400 | Total Loss: 0.002621 | Recon Loss: 0.002190 | Commit Loss: 0.000861 | Perplexity: 1065.395391
Trainning Epoch:  41%|████      | 273/665 [7:51:27<11:18:11, 103.81s/it]2025-09-15 00:24:15,374 Stage: Train 0.5 | Epoch: 273 | Iter: 205600 | Total Loss: 0.002621 | Recon Loss: 0.002191 | Commit Loss: 0.000859 | Perplexity: 1063.009531
2025-09-15 00:24:42,910 Stage: Train 0.5 | Epoch: 273 | Iter: 205800 | Total Loss: 0.002638 | Recon Loss: 0.002213 | Commit Loss: 0.000851 | Perplexity: 1060.017594
2025-09-15 00:25:10,542 Stage: Train 0.5 | Epoch: 273 | Iter: 206000 | Total Loss: 0.002608 | Recon Loss: 0.002179 | Commit Loss: 0.000858 | Perplexity: 1062.567121
2025-09-15 00:25:37,970 Stage: Train 0.5 | Epoch: 273 | Iter: 206200 | Total Loss: 0.002655 | Recon Loss: 0.002226 | Commit Loss: 0.000859 | Perplexity: 1066.244224
Trainning Epoch:  41%|████      | 274/665 [7:53:10<11:15:55, 103.72s/it]2025-09-15 00:26:05,353 Stage: Train 0.5 | Epoch: 274 | Iter: 206400 | Total Loss: 0.002663 | Recon Loss: 0.002235 | Commit Loss: 0.000855 | Perplexity: 1063.625281
2025-09-15 00:26:32,772 Stage: Train 0.5 | Epoch: 274 | Iter: 206600 | Total Loss: 0.002671 | Recon Loss: 0.002244 | Commit Loss: 0.000856 | Perplexity: 1063.702096
2025-09-15 00:27:00,211 Stage: Train 0.5 | Epoch: 274 | Iter: 206800 | Total Loss: 0.002590 | Recon Loss: 0.002165 | Commit Loss: 0.000850 | Perplexity: 1061.448871
2025-09-15 00:27:27,713 Stage: Train 0.5 | Epoch: 274 | Iter: 207000 | Total Loss: 0.002611 | Recon Loss: 0.002182 | Commit Loss: 0.000859 | Perplexity: 1059.928946
Trainning Epoch:  41%|████▏     | 275/665 [7:54:53<11:13:24, 103.60s/it]2025-09-15 00:27:55,093 Stage: Train 0.5 | Epoch: 275 | Iter: 207200 | Total Loss: 0.002603 | Recon Loss: 0.002177 | Commit Loss: 0.000853 | Perplexity: 1063.093375
2025-09-15 00:28:22,501 Stage: Train 0.5 | Epoch: 275 | Iter: 207400 | Total Loss: 0.002627 | Recon Loss: 0.002195 | Commit Loss: 0.000865 | Perplexity: 1067.305874
2025-09-15 00:28:50,012 Stage: Train 0.5 | Epoch: 275 | Iter: 207600 | Total Loss: 0.002600 | Recon Loss: 0.002174 | Commit Loss: 0.000853 | Perplexity: 1061.709663
2025-09-15 00:29:17,417 Stage: Train 0.5 | Epoch: 275 | Iter: 207800 | Total Loss: 0.002600 | Recon Loss: 0.002172 | Commit Loss: 0.000857 | Perplexity: 1063.516774
Trainning Epoch:  42%|████▏     | 276/665 [7:56:37<11:10:58, 103.49s/it]2025-09-15 00:29:44,761 Stage: Train 0.5 | Epoch: 276 | Iter: 208000 | Total Loss: 0.002658 | Recon Loss: 0.002235 | Commit Loss: 0.000845 | Perplexity: 1060.574293
2025-09-15 00:30:12,170 Stage: Train 0.5 | Epoch: 276 | Iter: 208200 | Total Loss: 0.002620 | Recon Loss: 0.002191 | Commit Loss: 0.000857 | Perplexity: 1063.785277
2025-09-15 00:30:39,569 Stage: Train 0.5 | Epoch: 276 | Iter: 208400 | Total Loss: 0.002607 | Recon Loss: 0.002180 | Commit Loss: 0.000854 | Perplexity: 1060.096664
Trainning Epoch:  42%|████▏     | 277/665 [7:58:20<11:08:34, 103.39s/it]2025-09-15 00:31:06,950 Stage: Train 0.5 | Epoch: 277 | Iter: 208600 | Total Loss: 0.002658 | Recon Loss: 0.002227 | Commit Loss: 0.000861 | Perplexity: 1061.647112
2025-09-15 00:31:34,486 Stage: Train 0.5 | Epoch: 277 | Iter: 208800 | Total Loss: 0.002600 | Recon Loss: 0.002174 | Commit Loss: 0.000851 | Perplexity: 1059.905976
2025-09-15 00:32:02,106 Stage: Train 0.5 | Epoch: 277 | Iter: 209000 | Total Loss: 0.002638 | Recon Loss: 0.002211 | Commit Loss: 0.000853 | Perplexity: 1059.521326
2025-09-15 00:32:29,647 Stage: Train 0.5 | Epoch: 277 | Iter: 209200 | Total Loss: 0.002588 | Recon Loss: 0.002158 | Commit Loss: 0.000860 | Perplexity: 1064.534649
Trainning Epoch:  42%|████▏     | 278/665 [8:00:04<11:07:26, 103.48s/it]2025-09-15 00:32:57,087 Stage: Train 0.5 | Epoch: 278 | Iter: 209400 | Total Loss: 0.002704 | Recon Loss: 0.002277 | Commit Loss: 0.000854 | Perplexity: 1056.802161
2025-09-15 00:33:24,535 Stage: Train 0.5 | Epoch: 278 | Iter: 209600 | Total Loss: 0.002568 | Recon Loss: 0.002144 | Commit Loss: 0.000847 | Perplexity: 1059.134041
2025-09-15 00:33:52,039 Stage: Train 0.5 | Epoch: 278 | Iter: 209800 | Total Loss: 0.002586 | Recon Loss: 0.002160 | Commit Loss: 0.000853 | Perplexity: 1062.926933
2025-09-15 00:34:19,462 Stage: Train 0.5 | Epoch: 278 | Iter: 210000 | Total Loss: 0.002623 | Recon Loss: 0.002190 | Commit Loss: 0.000865 | Perplexity: 1067.669293
Trainning Epoch:  42%|████▏     | 279/665 [8:01:47<11:05:24, 103.43s/it]2025-09-15 00:34:46,895 Stage: Train 0.5 | Epoch: 279 | Iter: 210200 | Total Loss: 0.002584 | Recon Loss: 0.002162 | Commit Loss: 0.000845 | Perplexity: 1057.352530
2025-09-15 00:35:14,427 Stage: Train 0.5 | Epoch: 279 | Iter: 210400 | Total Loss: 0.002567 | Recon Loss: 0.002143 | Commit Loss: 0.000848 | Perplexity: 1059.749459
2025-09-15 00:35:41,978 Stage: Train 0.5 | Epoch: 279 | Iter: 210600 | Total Loss: 0.002596 | Recon Loss: 0.002163 | Commit Loss: 0.000868 | Perplexity: 1067.811390
2025-09-15 00:36:09,382 Stage: Train 0.5 | Epoch: 279 | Iter: 210800 | Total Loss: 0.002575 | Recon Loss: 0.002147 | Commit Loss: 0.000857 | Perplexity: 1066.129761
Trainning Epoch:  42%|████▏     | 280/665 [8:03:30<11:03:44, 103.44s/it]2025-09-15 00:36:36,738 Stage: Train 0.5 | Epoch: 280 | Iter: 211000 | Total Loss: 0.002618 | Recon Loss: 0.002193 | Commit Loss: 0.000849 | Perplexity: 1062.179686
2025-09-15 00:37:04,139 Stage: Train 0.5 | Epoch: 280 | Iter: 211200 | Total Loss: 0.002620 | Recon Loss: 0.002193 | Commit Loss: 0.000854 | Perplexity: 1060.230253
2025-09-15 00:37:31,569 Stage: Train 0.5 | Epoch: 280 | Iter: 211400 | Total Loss: 0.002633 | Recon Loss: 0.002209 | Commit Loss: 0.000847 | Perplexity: 1059.774092
Trainning Epoch:  42%|████▏     | 281/665 [8:05:14<11:01:28, 103.35s/it]2025-09-15 00:37:58,931 Stage: Train 0.5 | Epoch: 281 | Iter: 211600 | Total Loss: 0.002639 | Recon Loss: 0.002206 | Commit Loss: 0.000866 | Perplexity: 1060.810220
2025-09-15 00:38:26,336 Stage: Train 0.5 | Epoch: 281 | Iter: 211800 | Total Loss: 0.002599 | Recon Loss: 0.002174 | Commit Loss: 0.000851 | Perplexity: 1061.662546
2025-09-15 00:38:53,841 Stage: Train 0.5 | Epoch: 281 | Iter: 212000 | Total Loss: 0.002572 | Recon Loss: 0.002143 | Commit Loss: 0.000857 | Perplexity: 1064.426113
2025-09-15 00:39:21,268 Stage: Train 0.5 | Epoch: 281 | Iter: 212200 | Total Loss: 0.002599 | Recon Loss: 0.002174 | Commit Loss: 0.000851 | Perplexity: 1060.371240
Trainning Epoch:  42%|████▏     | 282/665 [8:06:57<10:59:36, 103.33s/it]2025-09-15 00:39:48,675 Stage: Train 0.5 | Epoch: 282 | Iter: 212400 | Total Loss: 0.002594 | Recon Loss: 0.002167 | Commit Loss: 0.000855 | Perplexity: 1063.585555
2025-09-15 00:40:16,131 Stage: Train 0.5 | Epoch: 282 | Iter: 212600 | Total Loss: 0.002595 | Recon Loss: 0.002168 | Commit Loss: 0.000855 | Perplexity: 1064.312382
2025-09-15 00:40:43,563 Stage: Train 0.5 | Epoch: 282 | Iter: 212800 | Total Loss: 0.002647 | Recon Loss: 0.002220 | Commit Loss: 0.000855 | Perplexity: 1063.362123
2025-09-15 00:41:11,033 Stage: Train 0.5 | Epoch: 282 | Iter: 213000 | Total Loss: 0.002563 | Recon Loss: 0.002136 | Commit Loss: 0.000854 | Perplexity: 1062.953380
Trainning Epoch:  43%|████▎     | 283/665 [8:08:40<10:57:54, 103.34s/it]2025-09-15 00:41:38,475 Stage: Train 0.5 | Epoch: 283 | Iter: 213200 | Total Loss: 0.002597 | Recon Loss: 0.002174 | Commit Loss: 0.000847 | Perplexity: 1060.857895
2025-09-15 00:42:06,008 Stage: Train 0.5 | Epoch: 283 | Iter: 213400 | Total Loss: 0.002645 | Recon Loss: 0.002219 | Commit Loss: 0.000852 | Perplexity: 1063.006946
2025-09-15 00:42:33,439 Stage: Train 0.5 | Epoch: 283 | Iter: 213600 | Total Loss: 0.002632 | Recon Loss: 0.002207 | Commit Loss: 0.000849 | Perplexity: 1058.915974
2025-09-15 00:43:00,895 Stage: Train 0.5 | Epoch: 283 | Iter: 213800 | Total Loss: 0.002568 | Recon Loss: 0.002137 | Commit Loss: 0.000862 | Perplexity: 1065.518103
Trainning Epoch:  43%|████▎     | 284/665 [8:10:24<10:56:17, 103.35s/it]2025-09-15 00:43:28,315 Stage: Train 0.5 | Epoch: 284 | Iter: 214000 | Total Loss: 0.002587 | Recon Loss: 0.002161 | Commit Loss: 0.000852 | Perplexity: 1061.399638
2025-09-15 00:43:55,791 Stage: Train 0.5 | Epoch: 284 | Iter: 214200 | Total Loss: 0.002695 | Recon Loss: 0.002271 | Commit Loss: 0.000848 | Perplexity: 1059.041095
2025-09-15 00:44:23,181 Stage: Train 0.5 | Epoch: 284 | Iter: 214400 | Total Loss: 0.002564 | Recon Loss: 0.002137 | Commit Loss: 0.000853 | Perplexity: 1060.943473
2025-09-15 00:44:50,567 Stage: Train 0.5 | Epoch: 284 | Iter: 214600 | Total Loss: 0.002585 | Recon Loss: 0.002157 | Commit Loss: 0.000856 | Perplexity: 1063.421459
Trainning Epoch:  43%|████▎     | 285/665 [8:12:07<10:54:18, 103.31s/it]2025-09-15 00:45:17,920 Stage: Train 0.5 | Epoch: 285 | Iter: 214800 | Total Loss: 0.002587 | Recon Loss: 0.002160 | Commit Loss: 0.000853 | Perplexity: 1063.629115
2025-09-15 00:45:45,391 Stage: Train 0.5 | Epoch: 285 | Iter: 215000 | Total Loss: 0.002637 | Recon Loss: 0.002215 | Commit Loss: 0.000844 | Perplexity: 1060.139838
2025-09-15 00:46:12,759 Stage: Train 0.5 | Epoch: 285 | Iter: 215200 | Total Loss: 0.002577 | Recon Loss: 0.002149 | Commit Loss: 0.000857 | Perplexity: 1064.022567
Trainning Epoch:  43%|████▎     | 286/665 [8:13:50<10:52:13, 103.25s/it]2025-09-15 00:46:40,078 Stage: Train 0.5 | Epoch: 286 | Iter: 215400 | Total Loss: 0.002630 | Recon Loss: 0.002202 | Commit Loss: 0.000855 | Perplexity: 1060.728713
2025-09-15 00:47:07,446 Stage: Train 0.5 | Epoch: 286 | Iter: 215600 | Total Loss: 0.002594 | Recon Loss: 0.002167 | Commit Loss: 0.000854 | Perplexity: 1066.582038
2025-09-15 00:47:34,811 Stage: Train 0.5 | Epoch: 286 | Iter: 215800 | Total Loss: 0.002615 | Recon Loss: 0.002190 | Commit Loss: 0.000850 | Perplexity: 1062.995738
2025-09-15 00:48:02,183 Stage: Train 0.5 | Epoch: 286 | Iter: 216000 | Total Loss: 0.002543 | Recon Loss: 0.002115 | Commit Loss: 0.000856 | Perplexity: 1061.826343
Trainning Epoch:  43%|████▎     | 287/665 [8:15:33<10:50:01, 103.18s/it]2025-09-15 00:48:29,517 Stage: Train 0.5 | Epoch: 287 | Iter: 216200 | Total Loss: 0.002637 | Recon Loss: 0.002212 | Commit Loss: 0.000849 | Perplexity: 1060.720455
2025-09-15 00:48:57,009 Stage: Train 0.5 | Epoch: 287 | Iter: 216400 | Total Loss: 0.002586 | Recon Loss: 0.002165 | Commit Loss: 0.000842 | Perplexity: 1058.923027
2025-09-15 00:49:24,455 Stage: Train 0.5 | Epoch: 287 | Iter: 216600 | Total Loss: 0.002621 | Recon Loss: 0.002196 | Commit Loss: 0.000850 | Perplexity: 1061.553337
2025-09-15 00:49:51,945 Stage: Train 0.5 | Epoch: 287 | Iter: 216800 | Total Loss: 0.002606 | Recon Loss: 0.002181 | Commit Loss: 0.000851 | Perplexity: 1064.098398
Trainning Epoch:  43%|████▎     | 288/665 [8:17:16<10:48:53, 103.27s/it]2025-09-15 00:50:19,634 Stage: Train 0.5 | Epoch: 288 | Iter: 217000 | Total Loss: 0.002610 | Recon Loss: 0.002182 | Commit Loss: 0.000855 | Perplexity: 1065.031712
2025-09-15 00:50:47,207 Stage: Train 0.5 | Epoch: 288 | Iter: 217200 | Total Loss: 0.002574 | Recon Loss: 0.002149 | Commit Loss: 0.000851 | Perplexity: 1063.019866
2025-09-15 00:51:14,942 Stage: Train 0.5 | Epoch: 288 | Iter: 217400 | Total Loss: 0.002583 | Recon Loss: 0.002160 | Commit Loss: 0.000846 | Perplexity: 1060.803424
2025-09-15 00:51:42,365 Stage: Train 0.5 | Epoch: 288 | Iter: 217600 | Total Loss: 0.002585 | Recon Loss: 0.002160 | Commit Loss: 0.000848 | Perplexity: 1062.628968
Trainning Epoch:  43%|████▎     | 289/665 [8:19:00<10:48:12, 103.44s/it]2025-09-15 00:52:09,693 Stage: Train 0.5 | Epoch: 289 | Iter: 217800 | Total Loss: 0.002620 | Recon Loss: 0.002196 | Commit Loss: 0.000847 | Perplexity: 1056.579725
2025-09-15 00:52:37,146 Stage: Train 0.5 | Epoch: 289 | Iter: 218000 | Total Loss: 0.002529 | Recon Loss: 0.002103 | Commit Loss: 0.000852 | Perplexity: 1064.547697
2025-09-15 00:53:04,494 Stage: Train 0.5 | Epoch: 289 | Iter: 218200 | Total Loss: 0.002620 | Recon Loss: 0.002195 | Commit Loss: 0.000852 | Perplexity: 1062.507392
Trainning Epoch:  44%|████▎     | 290/665 [8:20:43<10:45:45, 103.32s/it]2025-09-15 00:53:31,824 Stage: Train 0.5 | Epoch: 290 | Iter: 218400 | Total Loss: 0.002620 | Recon Loss: 0.002194 | Commit Loss: 0.000853 | Perplexity: 1063.115378
2025-09-15 00:53:59,274 Stage: Train 0.5 | Epoch: 290 | Iter: 218600 | Total Loss: 0.002562 | Recon Loss: 0.002139 | Commit Loss: 0.000845 | Perplexity: 1061.495182
2025-09-15 00:54:26,682 Stage: Train 0.5 | Epoch: 290 | Iter: 218800 | Total Loss: 0.002572 | Recon Loss: 0.002146 | Commit Loss: 0.000851 | Perplexity: 1064.524919
2025-09-15 00:54:54,075 Stage: Train 0.5 | Epoch: 290 | Iter: 219000 | Total Loss: 0.002664 | Recon Loss: 0.002239 | Commit Loss: 0.000850 | Perplexity: 1059.775282
Trainning Epoch:  44%|████▍     | 291/665 [8:22:26<10:43:45, 103.28s/it]2025-09-15 00:55:21,412 Stage: Train 0.5 | Epoch: 291 | Iter: 219200 | Total Loss: 0.002630 | Recon Loss: 0.002207 | Commit Loss: 0.000846 | Perplexity: 1060.395234
2025-09-15 00:55:49,032 Stage: Train 0.5 | Epoch: 291 | Iter: 219400 | Total Loss: 0.002555 | Recon Loss: 0.002132 | Commit Loss: 0.000846 | Perplexity: 1060.611511
2025-09-15 00:56:16,711 Stage: Train 0.5 | Epoch: 291 | Iter: 219600 | Total Loss: 0.002551 | Recon Loss: 0.002129 | Commit Loss: 0.000845 | Perplexity: 1062.057415
2025-09-15 00:56:44,236 Stage: Train 0.5 | Epoch: 291 | Iter: 219800 | Total Loss: 0.002649 | Recon Loss: 0.002222 | Commit Loss: 0.000853 | Perplexity: 1063.540300
Trainning Epoch:  44%|████▍     | 292/665 [8:24:10<10:42:58, 103.43s/it]2025-09-15 00:57:11,696 Stage: Train 0.5 | Epoch: 292 | Iter: 220000 | Total Loss: 0.002617 | Recon Loss: 0.002190 | Commit Loss: 0.000853 | Perplexity: 1064.746901
2025-09-15 00:57:11,696 Saving model at iteration 220000
2025-09-15 00:57:11,864 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_293_step_220000
2025-09-15 00:57:12,080 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_293_step_220000/pytorch_model.bin
2025-09-15 00:57:12,411 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_293_step_220000/optimizer.bin
2025-09-15 00:57:12,412 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_293_step_220000/scheduler.bin
2025-09-15 00:57:12,413 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_293_step_220000/random_states_0.pkl
2025-09-15 00:57:40,182 Stage: Train 0.5 | Epoch: 292 | Iter: 220200 | Total Loss: 0.002604 | Recon Loss: 0.002182 | Commit Loss: 0.000843 | Perplexity: 1060.054381
2025-09-15 00:58:07,573 Stage: Train 0.5 | Epoch: 292 | Iter: 220400 | Total Loss: 0.002589 | Recon Loss: 0.002166 | Commit Loss: 0.000846 | Perplexity: 1061.112705
2025-09-15 00:58:34,952 Stage: Train 0.5 | Epoch: 292 | Iter: 220600 | Total Loss: 0.002567 | Recon Loss: 0.002142 | Commit Loss: 0.000849 | Perplexity: 1064.956624
Trainning Epoch:  44%|████▍     | 293/665 [8:25:54<10:42:46, 103.67s/it]2025-09-15 00:59:02,499 Stage: Train 0.5 | Epoch: 293 | Iter: 220800 | Total Loss: 0.002618 | Recon Loss: 0.002198 | Commit Loss: 0.000841 | Perplexity: 1060.909994
2025-09-15 00:59:30,029 Stage: Train 0.5 | Epoch: 293 | Iter: 221000 | Total Loss: 0.002555 | Recon Loss: 0.002133 | Commit Loss: 0.000845 | Perplexity: 1066.836888
2025-09-15 00:59:57,517 Stage: Train 0.5 | Epoch: 293 | Iter: 221200 | Total Loss: 0.002570 | Recon Loss: 0.002145 | Commit Loss: 0.000851 | Perplexity: 1063.668847
Trainning Epoch:  44%|████▍     | 294/665 [8:27:38<10:40:57, 103.66s/it]2025-09-15 01:00:25,010 Stage: Train 0.5 | Epoch: 294 | Iter: 221400 | Total Loss: 0.002598 | Recon Loss: 0.002172 | Commit Loss: 0.000852 | Perplexity: 1060.541183
2025-09-15 01:00:52,689 Stage: Train 0.5 | Epoch: 294 | Iter: 221600 | Total Loss: 0.002583 | Recon Loss: 0.002159 | Commit Loss: 0.000847 | Perplexity: 1066.094737
2025-09-15 01:01:20,128 Stage: Train 0.5 | Epoch: 294 | Iter: 221800 | Total Loss: 0.002560 | Recon Loss: 0.002139 | Commit Loss: 0.000842 | Perplexity: 1061.844154
2025-09-15 01:01:47,567 Stage: Train 0.5 | Epoch: 294 | Iter: 222000 | Total Loss: 0.002572 | Recon Loss: 0.002148 | Commit Loss: 0.000849 | Perplexity: 1062.001247
Trainning Epoch:  44%|████▍     | 295/665 [8:29:22<10:38:56, 103.61s/it]2025-09-15 01:02:14,942 Stage: Train 0.5 | Epoch: 295 | Iter: 222200 | Total Loss: 0.002596 | Recon Loss: 0.002174 | Commit Loss: 0.000842 | Perplexity: 1057.955181
2025-09-15 01:02:42,375 Stage: Train 0.5 | Epoch: 295 | Iter: 222400 | Total Loss: 0.002553 | Recon Loss: 0.002131 | Commit Loss: 0.000844 | Perplexity: 1064.506790
2025-09-15 01:03:09,791 Stage: Train 0.5 | Epoch: 295 | Iter: 222600 | Total Loss: 0.002545 | Recon Loss: 0.002119 | Commit Loss: 0.000851 | Perplexity: 1061.665276
2025-09-15 01:03:37,196 Stage: Train 0.5 | Epoch: 295 | Iter: 222800 | Total Loss: 0.002584 | Recon Loss: 0.002158 | Commit Loss: 0.000852 | Perplexity: 1065.762853
Trainning Epoch:  45%|████▍     | 296/665 [8:31:05<10:36:26, 103.49s/it]2025-09-15 01:04:04,672 Stage: Train 0.5 | Epoch: 296 | Iter: 223000 | Total Loss: 0.002634 | Recon Loss: 0.002212 | Commit Loss: 0.000843 | Perplexity: 1058.534725
2025-09-15 01:04:32,062 Stage: Train 0.5 | Epoch: 296 | Iter: 223200 | Total Loss: 0.002547 | Recon Loss: 0.002125 | Commit Loss: 0.000844 | Perplexity: 1063.638882
2025-09-15 01:04:59,460 Stage: Train 0.5 | Epoch: 296 | Iter: 223400 | Total Loss: 0.002604 | Recon Loss: 0.002179 | Commit Loss: 0.000851 | Perplexity: 1063.383569
2025-09-15 01:05:27,065 Stage: Train 0.5 | Epoch: 296 | Iter: 223600 | Total Loss: 0.002593 | Recon Loss: 0.002170 | Commit Loss: 0.000845 | Perplexity: 1058.275566
Trainning Epoch:  45%|████▍     | 297/665 [8:32:48<10:34:41, 103.48s/it]2025-09-15 01:05:54,553 Stage: Train 0.5 | Epoch: 297 | Iter: 223800 | Total Loss: 0.002609 | Recon Loss: 0.002189 | Commit Loss: 0.000840 | Perplexity: 1059.793835
2025-09-15 01:06:22,040 Stage: Train 0.5 | Epoch: 297 | Iter: 224000 | Total Loss: 0.002552 | Recon Loss: 0.002126 | Commit Loss: 0.000851 | Perplexity: 1067.180480
2025-09-15 01:06:49,441 Stage: Train 0.5 | Epoch: 297 | Iter: 224200 | Total Loss: 0.002610 | Recon Loss: 0.002187 | Commit Loss: 0.000846 | Perplexity: 1064.922858
Trainning Epoch:  45%|████▍     | 298/665 [8:34:32<10:32:40, 103.43s/it]2025-09-15 01:07:16,840 Stage: Train 0.5 | Epoch: 298 | Iter: 224400 | Total Loss: 0.002554 | Recon Loss: 0.002134 | Commit Loss: 0.000840 | Perplexity: 1055.436244
2025-09-15 01:07:44,453 Stage: Train 0.5 | Epoch: 298 | Iter: 224600 | Total Loss: 0.002631 | Recon Loss: 0.002205 | Commit Loss: 0.000851 | Perplexity: 1062.934283
2025-09-15 01:08:11,841 Stage: Train 0.5 | Epoch: 298 | Iter: 224800 | Total Loss: 0.002526 | Recon Loss: 0.002105 | Commit Loss: 0.000842 | Perplexity: 1065.449319
2025-09-15 01:08:39,224 Stage: Train 0.5 | Epoch: 298 | Iter: 225000 | Total Loss: 0.002560 | Recon Loss: 0.002135 | Commit Loss: 0.000850 | Perplexity: 1064.457672
Trainning Epoch:  45%|████▍     | 299/665 [8:36:15<10:30:40, 103.39s/it]2025-09-15 01:09:06,559 Stage: Train 0.5 | Epoch: 299 | Iter: 225200 | Total Loss: 0.002575 | Recon Loss: 0.002155 | Commit Loss: 0.000841 | Perplexity: 1061.998955
2025-09-15 01:09:33,946 Stage: Train 0.5 | Epoch: 299 | Iter: 225400 | Total Loss: 0.002553 | Recon Loss: 0.002132 | Commit Loss: 0.000842 | Perplexity: 1060.936397
2025-09-15 01:10:01,398 Stage: Train 0.5 | Epoch: 299 | Iter: 225600 | Total Loss: 0.002586 | Recon Loss: 0.002162 | Commit Loss: 0.000846 | Perplexity: 1063.450752
2025-09-15 01:10:28,904 Stage: Train 0.5 | Epoch: 299 | Iter: 225800 | Total Loss: 0.002570 | Recon Loss: 0.002149 | Commit Loss: 0.000842 | Perplexity: 1062.122196
Trainning Epoch:  45%|████▌     | 300/665 [8:37:58<10:28:50, 103.37s/it]2025-09-15 01:10:56,546 Stage: Train 0.5 | Epoch: 300 | Iter: 226000 | Total Loss: 0.002570 | Recon Loss: 0.002149 | Commit Loss: 0.000842 | Perplexity: 1062.848068
2025-09-15 01:11:24,075 Stage: Train 0.5 | Epoch: 300 | Iter: 226200 | Total Loss: 0.002567 | Recon Loss: 0.002143 | Commit Loss: 0.000849 | Perplexity: 1065.925240
2025-09-15 01:11:51,548 Stage: Train 0.5 | Epoch: 300 | Iter: 226400 | Total Loss: 0.002553 | Recon Loss: 0.002131 | Commit Loss: 0.000843 | Perplexity: 1062.786135
2025-09-15 01:12:18,958 Stage: Train 0.5 | Epoch: 300 | Iter: 226600 | Total Loss: 0.002574 | Recon Loss: 0.002155 | Commit Loss: 0.000839 | Perplexity: 1060.342178
Trainning Epoch:  45%|████▌     | 301/665 [8:39:42<10:27:25, 103.42s/it]2025-09-15 01:12:46,290 Stage: Train 0.5 | Epoch: 301 | Iter: 226800 | Total Loss: 0.002548 | Recon Loss: 0.002127 | Commit Loss: 0.000842 | Perplexity: 1063.801870
2025-09-15 01:13:13,703 Stage: Train 0.5 | Epoch: 301 | Iter: 227000 | Total Loss: 0.002604 | Recon Loss: 0.002183 | Commit Loss: 0.000840 | Perplexity: 1063.420041
2025-09-15 01:13:41,096 Stage: Train 0.5 | Epoch: 301 | Iter: 227200 | Total Loss: 0.002541 | Recon Loss: 0.002121 | Commit Loss: 0.000841 | Perplexity: 1062.885371
2025-09-15 01:14:08,552 Stage: Train 0.5 | Epoch: 301 | Iter: 227400 | Total Loss: 0.002558 | Recon Loss: 0.002130 | Commit Loss: 0.000856 | Perplexity: 1064.829636
Trainning Epoch:  45%|████▌     | 302/665 [8:41:25<10:25:14, 103.34s/it]2025-09-15 01:14:36,116 Stage: Train 0.5 | Epoch: 302 | Iter: 227600 | Total Loss: 0.002582 | Recon Loss: 0.002159 | Commit Loss: 0.000845 | Perplexity: 1061.338000
2025-09-15 01:15:03,618 Stage: Train 0.5 | Epoch: 302 | Iter: 227800 | Total Loss: 0.002527 | Recon Loss: 0.002106 | Commit Loss: 0.000842 | Perplexity: 1064.022006
2025-09-15 01:15:31,120 Stage: Train 0.5 | Epoch: 302 | Iter: 228000 | Total Loss: 0.002557 | Recon Loss: 0.002135 | Commit Loss: 0.000844 | Perplexity: 1061.184200
Trainning Epoch:  46%|████▌     | 303/665 [8:43:08<10:24:00, 103.43s/it]2025-09-15 01:15:58,576 Stage: Train 0.5 | Epoch: 303 | Iter: 228200 | Total Loss: 0.002525 | Recon Loss: 0.002101 | Commit Loss: 0.000848 | Perplexity: 1063.390077
2025-09-15 01:16:26,048 Stage: Train 0.5 | Epoch: 303 | Iter: 228400 | Total Loss: 0.002553 | Recon Loss: 0.002131 | Commit Loss: 0.000843 | Perplexity: 1066.222983
2025-09-15 01:16:53,535 Stage: Train 0.5 | Epoch: 303 | Iter: 228600 | Total Loss: 0.002501 | Recon Loss: 0.002080 | Commit Loss: 0.000842 | Perplexity: 1061.428702
2025-09-15 01:17:21,039 Stage: Train 0.5 | Epoch: 303 | Iter: 228800 | Total Loss: 0.002539 | Recon Loss: 0.002118 | Commit Loss: 0.000842 | Perplexity: 1059.220233
Trainning Epoch:  46%|████▌     | 304/665 [8:44:52<10:22:15, 103.42s/it]2025-09-15 01:17:48,637 Stage: Train 0.5 | Epoch: 304 | Iter: 229000 | Total Loss: 0.002559 | Recon Loss: 0.002136 | Commit Loss: 0.000847 | Perplexity: 1067.953031
2025-09-15 01:18:16,255 Stage: Train 0.5 | Epoch: 304 | Iter: 229200 | Total Loss: 0.002580 | Recon Loss: 0.002159 | Commit Loss: 0.000841 | Perplexity: 1060.648809
2025-09-15 01:18:43,810 Stage: Train 0.5 | Epoch: 304 | Iter: 229400 | Total Loss: 0.002556 | Recon Loss: 0.002134 | Commit Loss: 0.000844 | Perplexity: 1063.909080
2025-09-15 01:19:11,370 Stage: Train 0.5 | Epoch: 304 | Iter: 229600 | Total Loss: 0.002492 | Recon Loss: 0.002067 | Commit Loss: 0.000851 | Perplexity: 1064.493225
Trainning Epoch:  46%|████▌     | 305/665 [8:46:36<10:21:24, 103.57s/it]2025-09-15 01:19:38,848 Stage: Train 0.5 | Epoch: 305 | Iter: 229800 | Total Loss: 0.002517 | Recon Loss: 0.002096 | Commit Loss: 0.000842 | Perplexity: 1062.202188
2025-09-15 01:20:06,365 Stage: Train 0.5 | Epoch: 305 | Iter: 230000 | Total Loss: 0.002535 | Recon Loss: 0.002111 | Commit Loss: 0.000847 | Perplexity: 1063.089504
2025-09-15 01:20:33,931 Stage: Train 0.5 | Epoch: 305 | Iter: 230200 | Total Loss: 0.002568 | Recon Loss: 0.002146 | Commit Loss: 0.000844 | Perplexity: 1064.115489
2025-09-15 01:21:01,503 Stage: Train 0.5 | Epoch: 305 | Iter: 230400 | Total Loss: 0.002539 | Recon Loss: 0.002114 | Commit Loss: 0.000850 | Perplexity: 1063.783164
Trainning Epoch:  46%|████▌     | 306/665 [8:48:20<10:20:03, 103.63s/it]2025-09-15 01:21:29,091 Stage: Train 0.5 | Epoch: 306 | Iter: 230600 | Total Loss: 0.002511 | Recon Loss: 0.002089 | Commit Loss: 0.000846 | Perplexity: 1065.224918
2025-09-15 01:21:56,563 Stage: Train 0.5 | Epoch: 306 | Iter: 230800 | Total Loss: 0.002529 | Recon Loss: 0.002110 | Commit Loss: 0.000839 | Perplexity: 1060.311081
2025-09-15 01:22:23,984 Stage: Train 0.5 | Epoch: 306 | Iter: 231000 | Total Loss: 0.002549 | Recon Loss: 0.002130 | Commit Loss: 0.000838 | Perplexity: 1062.867619
Trainning Epoch:  46%|████▌     | 307/665 [8:50:03<10:17:48, 103.54s/it]2025-09-15 01:22:51,363 Stage: Train 0.5 | Epoch: 307 | Iter: 231200 | Total Loss: 0.002583 | Recon Loss: 0.002159 | Commit Loss: 0.000849 | Perplexity: 1060.854796
2025-09-15 01:23:18,752 Stage: Train 0.5 | Epoch: 307 | Iter: 231400 | Total Loss: 0.002566 | Recon Loss: 0.002143 | Commit Loss: 0.000845 | Perplexity: 1065.893459
2025-09-15 01:23:46,139 Stage: Train 0.5 | Epoch: 307 | Iter: 231600 | Total Loss: 0.002525 | Recon Loss: 0.002103 | Commit Loss: 0.000844 | Perplexity: 1062.898417
2025-09-15 01:24:13,550 Stage: Train 0.5 | Epoch: 307 | Iter: 231800 | Total Loss: 0.002571 | Recon Loss: 0.002150 | Commit Loss: 0.000843 | Perplexity: 1062.003190
Trainning Epoch:  46%|████▋     | 308/665 [8:51:46<10:15:32, 103.45s/it]2025-09-15 01:24:41,063 Stage: Train 0.5 | Epoch: 308 | Iter: 232000 | Total Loss: 0.002609 | Recon Loss: 0.002186 | Commit Loss: 0.000846 | Perplexity: 1061.974882
2025-09-15 01:25:08,468 Stage: Train 0.5 | Epoch: 308 | Iter: 232200 | Total Loss: 0.002552 | Recon Loss: 0.002132 | Commit Loss: 0.000839 | Perplexity: 1061.976366
2025-09-15 01:25:35,875 Stage: Train 0.5 | Epoch: 308 | Iter: 232400 | Total Loss: 0.002497 | Recon Loss: 0.002078 | Commit Loss: 0.000837 | Perplexity: 1062.372510
2025-09-15 01:26:03,355 Stage: Train 0.5 | Epoch: 308 | Iter: 232600 | Total Loss: 0.002562 | Recon Loss: 0.002139 | Commit Loss: 0.000846 | Perplexity: 1064.320594
Trainning Epoch:  46%|████▋     | 309/665 [8:53:29<10:13:22, 103.38s/it]2025-09-15 01:26:30,652 Stage: Train 0.5 | Epoch: 309 | Iter: 232800 | Total Loss: 0.002618 | Recon Loss: 0.002198 | Commit Loss: 0.000841 | Perplexity: 1064.758243
2025-09-15 01:26:57,977 Stage: Train 0.5 | Epoch: 309 | Iter: 233000 | Total Loss: 0.002592 | Recon Loss: 0.002169 | Commit Loss: 0.000846 | Perplexity: 1068.446838
2025-09-15 01:27:25,398 Stage: Train 0.5 | Epoch: 309 | Iter: 233200 | Total Loss: 0.002575 | Recon Loss: 0.002155 | Commit Loss: 0.000841 | Perplexity: 1061.598577
2025-09-15 01:27:52,908 Stage: Train 0.5 | Epoch: 309 | Iter: 233400 | Total Loss: 0.002508 | Recon Loss: 0.002088 | Commit Loss: 0.000839 | Perplexity: 1062.244229
Trainning Epoch:  47%|████▋     | 310/665 [8:55:13<10:11:14, 103.31s/it]2025-09-15 01:28:20,285 Stage: Train 0.5 | Epoch: 310 | Iter: 233600 | Total Loss: 0.002597 | Recon Loss: 0.002177 | Commit Loss: 0.000841 | Perplexity: 1060.243820
2025-09-15 01:28:47,715 Stage: Train 0.5 | Epoch: 310 | Iter: 233800 | Total Loss: 0.002523 | Recon Loss: 0.002104 | Commit Loss: 0.000839 | Perplexity: 1067.503537
2025-09-15 01:29:15,205 Stage: Train 0.5 | Epoch: 310 | Iter: 234000 | Total Loss: 0.002562 | Recon Loss: 0.002143 | Commit Loss: 0.000839 | Perplexity: 1059.436826
Trainning Epoch:  47%|████▋     | 311/665 [8:56:56<10:09:27, 103.30s/it]2025-09-15 01:29:42,582 Stage: Train 0.5 | Epoch: 311 | Iter: 234200 | Total Loss: 0.002579 | Recon Loss: 0.002159 | Commit Loss: 0.000840 | Perplexity: 1063.976032
2025-09-15 01:30:10,008 Stage: Train 0.5 | Epoch: 311 | Iter: 234400 | Total Loss: 0.002538 | Recon Loss: 0.002121 | Commit Loss: 0.000835 | Perplexity: 1064.132492
2025-09-15 01:30:37,582 Stage: Train 0.5 | Epoch: 311 | Iter: 234600 | Total Loss: 0.002547 | Recon Loss: 0.002128 | Commit Loss: 0.000838 | Perplexity: 1063.470327
2025-09-15 01:31:04,993 Stage: Train 0.5 | Epoch: 311 | Iter: 234800 | Total Loss: 0.002600 | Recon Loss: 0.002179 | Commit Loss: 0.000842 | Perplexity: 1063.084281
Trainning Epoch:  47%|████▋     | 312/665 [8:58:39<10:07:58, 103.34s/it]2025-09-15 01:31:32,446 Stage: Train 0.5 | Epoch: 312 | Iter: 235000 | Total Loss: 0.002542 | Recon Loss: 0.002121 | Commit Loss: 0.000841 | Perplexity: 1064.201460
2025-09-15 01:31:59,840 Stage: Train 0.5 | Epoch: 312 | Iter: 235200 | Total Loss: 0.002557 | Recon Loss: 0.002139 | Commit Loss: 0.000836 | Perplexity: 1062.683671
2025-09-15 01:32:27,257 Stage: Train 0.5 | Epoch: 312 | Iter: 235400 | Total Loss: 0.002557 | Recon Loss: 0.002136 | Commit Loss: 0.000843 | Perplexity: 1067.505788
2025-09-15 01:32:54,659 Stage: Train 0.5 | Epoch: 312 | Iter: 235600 | Total Loss: 0.002500 | Recon Loss: 0.002083 | Commit Loss: 0.000834 | Perplexity: 1062.728374
Trainning Epoch:  47%|████▋     | 313/665 [9:00:22<10:05:52, 103.27s/it]2025-09-15 01:33:22,040 Stage: Train 0.5 | Epoch: 313 | Iter: 235800 | Total Loss: 0.002554 | Recon Loss: 0.002136 | Commit Loss: 0.000836 | Perplexity: 1062.364153
2025-09-15 01:33:49,455 Stage: Train 0.5 | Epoch: 313 | Iter: 236000 | Total Loss: 0.002562 | Recon Loss: 0.002143 | Commit Loss: 0.000837 | Perplexity: 1061.503669
2025-09-15 01:34:16,882 Stage: Train 0.5 | Epoch: 313 | Iter: 236200 | Total Loss: 0.002590 | Recon Loss: 0.002173 | Commit Loss: 0.000835 | Perplexity: 1062.169684
2025-09-15 01:34:44,506 Stage: Train 0.5 | Epoch: 313 | Iter: 236400 | Total Loss: 0.002518 | Recon Loss: 0.002097 | Commit Loss: 0.000840 | Perplexity: 1064.531669
Trainning Epoch:  47%|████▋     | 314/665 [9:02:06<10:04:24, 103.32s/it]2025-09-15 01:35:11,970 Stage: Train 0.5 | Epoch: 314 | Iter: 236600 | Total Loss: 0.002557 | Recon Loss: 0.002136 | Commit Loss: 0.000842 | Perplexity: 1064.433550
2025-09-15 01:35:39,481 Stage: Train 0.5 | Epoch: 314 | Iter: 236800 | Total Loss: 0.002485 | Recon Loss: 0.002066 | Commit Loss: 0.000838 | Perplexity: 1065.828263
2025-09-15 01:36:07,138 Stage: Train 0.5 | Epoch: 314 | Iter: 237000 | Total Loss: 0.002564 | Recon Loss: 0.002146 | Commit Loss: 0.000836 | Perplexity: 1060.781251
Trainning Epoch:  47%|████▋     | 315/665 [9:03:49<10:03:20, 103.43s/it]2025-09-15 01:36:34,603 Stage: Train 0.5 | Epoch: 315 | Iter: 237200 | Total Loss: 0.002529 | Recon Loss: 0.002110 | Commit Loss: 0.000838 | Perplexity: 1062.810565
2025-09-15 01:37:02,084 Stage: Train 0.5 | Epoch: 315 | Iter: 237400 | Total Loss: 0.002577 | Recon Loss: 0.002156 | Commit Loss: 0.000842 | Perplexity: 1067.562627
2025-09-15 01:37:29,500 Stage: Train 0.5 | Epoch: 315 | Iter: 237600 | Total Loss: 0.002521 | Recon Loss: 0.002102 | Commit Loss: 0.000838 | Perplexity: 1064.108146
2025-09-15 01:37:56,959 Stage: Train 0.5 | Epoch: 315 | Iter: 237800 | Total Loss: 0.002488 | Recon Loss: 0.002069 | Commit Loss: 0.000839 | Perplexity: 1067.277279
Trainning Epoch:  48%|████▊     | 316/665 [9:05:33<10:01:43, 103.45s/it]2025-09-15 01:38:24,549 Stage: Train 0.5 | Epoch: 316 | Iter: 238000 | Total Loss: 0.002567 | Recon Loss: 0.002149 | Commit Loss: 0.000835 | Perplexity: 1059.961934
2025-09-15 01:38:52,277 Stage: Train 0.5 | Epoch: 316 | Iter: 238200 | Total Loss: 0.002491 | Recon Loss: 0.002071 | Commit Loss: 0.000840 | Perplexity: 1068.348970
2025-09-15 01:39:19,848 Stage: Train 0.5 | Epoch: 316 | Iter: 238400 | Total Loss: 0.002560 | Recon Loss: 0.002140 | Commit Loss: 0.000840 | Perplexity: 1064.926915
2025-09-15 01:39:47,315 Stage: Train 0.5 | Epoch: 316 | Iter: 238600 | Total Loss: 0.002534 | Recon Loss: 0.002114 | Commit Loss: 0.000840 | Perplexity: 1069.352010
Trainning Epoch:  48%|████▊     | 317/665 [9:07:17<10:00:32, 103.54s/it]2025-09-15 01:40:14,764 Stage: Train 0.5 | Epoch: 317 | Iter: 238800 | Total Loss: 0.002510 | Recon Loss: 0.002097 | Commit Loss: 0.000828 | Perplexity: 1058.186309
2025-09-15 01:40:42,496 Stage: Train 0.5 | Epoch: 317 | Iter: 239000 | Total Loss: 0.002546 | Recon Loss: 0.002128 | Commit Loss: 0.000835 | Perplexity: 1064.791031
2025-09-15 01:41:10,187 Stage: Train 0.5 | Epoch: 317 | Iter: 239200 | Total Loss: 0.002496 | Recon Loss: 0.002075 | Commit Loss: 0.000842 | Perplexity: 1066.753402
2025-09-15 01:41:37,933 Stage: Train 0.5 | Epoch: 317 | Iter: 239400 | Total Loss: 0.002529 | Recon Loss: 0.002108 | Commit Loss: 0.000841 | Perplexity: 1064.406509
Trainning Epoch:  48%|████▊     | 318/665 [9:09:01<9:59:54, 103.73s/it] 2025-09-15 01:42:05,466 Stage: Train 0.5 | Epoch: 318 | Iter: 239600 | Total Loss: 0.002530 | Recon Loss: 0.002111 | Commit Loss: 0.000837 | Perplexity: 1064.361198
2025-09-15 01:42:33,007 Stage: Train 0.5 | Epoch: 318 | Iter: 239800 | Total Loss: 0.002508 | Recon Loss: 0.002087 | Commit Loss: 0.000842 | Perplexity: 1066.661060
2025-09-15 01:43:00,569 Stage: Train 0.5 | Epoch: 318 | Iter: 240000 | Total Loss: 0.002508 | Recon Loss: 0.002087 | Commit Loss: 0.000841 | Perplexity: 1066.430685
2025-09-15 01:43:00,569 Saving model at iteration 240000
2025-09-15 01:43:00,741 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_319_step_240000
2025-09-15 01:43:00,946 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_319_step_240000/pytorch_model.bin
2025-09-15 01:43:01,270 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_319_step_240000/optimizer.bin
2025-09-15 01:43:01,270 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_319_step_240000/scheduler.bin
2025-09-15 01:43:01,271 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_319_step_240000/random_states_0.pkl
2025-09-15 01:43:29,200 Stage: Train 0.5 | Epoch: 318 | Iter: 240200 | Total Loss: 0.002536 | Recon Loss: 0.002118 | Commit Loss: 0.000835 | Perplexity: 1063.670684
Trainning Epoch:  48%|████▊     | 319/665 [9:10:46<10:00:44, 104.18s/it]2025-09-15 01:43:57,062 Stage: Train 0.5 | Epoch: 319 | Iter: 240400 | Total Loss: 0.002585 | Recon Loss: 0.002166 | Commit Loss: 0.000839 | Perplexity: 1066.702036
2025-09-15 01:44:24,539 Stage: Train 0.5 | Epoch: 319 | Iter: 240600 | Total Loss: 0.002509 | Recon Loss: 0.002088 | Commit Loss: 0.000841 | Perplexity: 1067.250581
2025-09-15 01:44:52,161 Stage: Train 0.5 | Epoch: 319 | Iter: 240800 | Total Loss: 0.002513 | Recon Loss: 0.002090 | Commit Loss: 0.000846 | Perplexity: 1069.604850
Trainning Epoch:  48%|████▊     | 320/665 [9:12:30<9:57:58, 104.00s/it] 2025-09-15 01:45:19,726 Stage: Train 0.5 | Epoch: 320 | Iter: 241000 | Total Loss: 0.002453 | Recon Loss: 0.002041 | Commit Loss: 0.000825 | Perplexity: 1058.590331
2025-09-15 01:45:47,233 Stage: Train 0.5 | Epoch: 320 | Iter: 241200 | Total Loss: 0.002594 | Recon Loss: 0.002176 | Commit Loss: 0.000836 | Perplexity: 1067.005235
2025-09-15 01:46:14,636 Stage: Train 0.5 | Epoch: 320 | Iter: 241400 | Total Loss: 0.002552 | Recon Loss: 0.002136 | Commit Loss: 0.000832 | Perplexity: 1065.047910
2025-09-15 01:46:42,157 Stage: Train 0.5 | Epoch: 320 | Iter: 241600 | Total Loss: 0.002532 | Recon Loss: 0.002115 | Commit Loss: 0.000835 | Perplexity: 1066.209597
Trainning Epoch:  48%|████▊     | 321/665 [9:14:13<9:55:18, 103.83s/it]2025-09-15 01:47:09,508 Stage: Train 0.5 | Epoch: 321 | Iter: 241800 | Total Loss: 0.002493 | Recon Loss: 0.002072 | Commit Loss: 0.000843 | Perplexity: 1067.993119
2025-09-15 01:47:36,901 Stage: Train 0.5 | Epoch: 321 | Iter: 242000 | Total Loss: 0.002525 | Recon Loss: 0.002109 | Commit Loss: 0.000833 | Perplexity: 1064.777268
2025-09-15 01:48:04,343 Stage: Train 0.5 | Epoch: 321 | Iter: 242200 | Total Loss: 0.002580 | Recon Loss: 0.002162 | Commit Loss: 0.000835 | Perplexity: 1068.680219
2025-09-15 01:48:31,744 Stage: Train 0.5 | Epoch: 321 | Iter: 242400 | Total Loss: 0.002535 | Recon Loss: 0.002121 | Commit Loss: 0.000828 | Perplexity: 1062.302960
Trainning Epoch:  48%|████▊     | 322/665 [9:15:56<9:52:25, 103.63s/it]2025-09-15 01:48:59,098 Stage: Train 0.5 | Epoch: 322 | Iter: 242600 | Total Loss: 0.002558 | Recon Loss: 0.002136 | Commit Loss: 0.000844 | Perplexity: 1064.680953
2025-09-15 01:49:26,490 Stage: Train 0.5 | Epoch: 322 | Iter: 242800 | Total Loss: 0.002476 | Recon Loss: 0.002064 | Commit Loss: 0.000824 | Perplexity: 1067.681375
2025-09-15 01:49:53,994 Stage: Train 0.5 | Epoch: 322 | Iter: 243000 | Total Loss: 0.002491 | Recon Loss: 0.002072 | Commit Loss: 0.000839 | Perplexity: 1071.809191
2025-09-15 01:50:21,459 Stage: Train 0.5 | Epoch: 322 | Iter: 243200 | Total Loss: 0.002494 | Recon Loss: 0.002075 | Commit Loss: 0.000838 | Perplexity: 1065.458589
Trainning Epoch:  49%|████▊     | 323/665 [9:17:40<9:50:06, 103.53s/it]2025-09-15 01:50:48,893 Stage: Train 0.5 | Epoch: 323 | Iter: 243400 | Total Loss: 0.002553 | Recon Loss: 0.002136 | Commit Loss: 0.000833 | Perplexity: 1066.568816
2025-09-15 01:51:16,372 Stage: Train 0.5 | Epoch: 323 | Iter: 243600 | Total Loss: 0.002566 | Recon Loss: 0.002150 | Commit Loss: 0.000833 | Perplexity: 1065.500769
2025-09-15 01:51:43,859 Stage: Train 0.5 | Epoch: 323 | Iter: 243800 | Total Loss: 0.002526 | Recon Loss: 0.002107 | Commit Loss: 0.000838 | Perplexity: 1070.544604
Trainning Epoch:  49%|████▊     | 324/665 [9:19:23<9:48:06, 103.48s/it]2025-09-15 01:52:11,230 Stage: Train 0.5 | Epoch: 324 | Iter: 244000 | Total Loss: 0.002528 | Recon Loss: 0.002114 | Commit Loss: 0.000829 | Perplexity: 1063.724029
2025-09-15 01:52:38,739 Stage: Train 0.5 | Epoch: 324 | Iter: 244200 | Total Loss: 0.002514 | Recon Loss: 0.002097 | Commit Loss: 0.000834 | Perplexity: 1068.689109
2025-09-15 01:53:06,365 Stage: Train 0.5 | Epoch: 324 | Iter: 244400 | Total Loss: 0.002495 | Recon Loss: 0.002081 | Commit Loss: 0.000829 | Perplexity: 1063.300926
2025-09-15 01:53:34,211 Stage: Train 0.5 | Epoch: 324 | Iter: 244600 | Total Loss: 0.002532 | Recon Loss: 0.002112 | Commit Loss: 0.000841 | Perplexity: 1068.027643
Trainning Epoch:  49%|████▉     | 325/665 [9:21:07<9:47:19, 103.65s/it]2025-09-15 01:54:01,752 Stage: Train 0.5 | Epoch: 325 | Iter: 244800 | Total Loss: 0.002504 | Recon Loss: 0.002090 | Commit Loss: 0.000828 | Perplexity: 1067.222987
2025-09-15 01:54:29,582 Stage: Train 0.5 | Epoch: 325 | Iter: 245000 | Total Loss: 0.002529 | Recon Loss: 0.002111 | Commit Loss: 0.000836 | Perplexity: 1062.420493
2025-09-15 01:54:57,011 Stage: Train 0.5 | Epoch: 325 | Iter: 245200 | Total Loss: 0.002475 | Recon Loss: 0.002057 | Commit Loss: 0.000837 | Perplexity: 1069.308409
2025-09-15 01:55:24,434 Stage: Train 0.5 | Epoch: 325 | Iter: 245400 | Total Loss: 0.002582 | Recon Loss: 0.002164 | Commit Loss: 0.000837 | Perplexity: 1066.854854
Trainning Epoch:  49%|████▉     | 326/665 [9:22:51<9:45:36, 103.65s/it]2025-09-15 01:55:51,828 Stage: Train 0.5 | Epoch: 326 | Iter: 245600 | Total Loss: 0.002471 | Recon Loss: 0.002056 | Commit Loss: 0.000830 | Perplexity: 1064.369952
2025-09-15 01:56:19,293 Stage: Train 0.5 | Epoch: 326 | Iter: 245800 | Total Loss: 0.002520 | Recon Loss: 0.002104 | Commit Loss: 0.000832 | Perplexity: 1066.129778
2025-09-15 01:56:46,838 Stage: Train 0.5 | Epoch: 326 | Iter: 246000 | Total Loss: 0.002516 | Recon Loss: 0.002098 | Commit Loss: 0.000836 | Perplexity: 1069.921100
2025-09-15 01:57:14,282 Stage: Train 0.5 | Epoch: 326 | Iter: 246200 | Total Loss: 0.002593 | Recon Loss: 0.002174 | Commit Loss: 0.000837 | Perplexity: 1066.190819
Trainning Epoch:  49%|████▉     | 327/665 [9:24:34<9:43:28, 103.57s/it]2025-09-15 01:57:41,788 Stage: Train 0.5 | Epoch: 327 | Iter: 246400 | Total Loss: 0.002473 | Recon Loss: 0.002055 | Commit Loss: 0.000834 | Perplexity: 1071.256143
2025-09-15 01:58:09,185 Stage: Train 0.5 | Epoch: 327 | Iter: 246600 | Total Loss: 0.002560 | Recon Loss: 0.002143 | Commit Loss: 0.000834 | Perplexity: 1068.710924
2025-09-15 01:58:36,574 Stage: Train 0.5 | Epoch: 327 | Iter: 246800 | Total Loss: 0.002526 | Recon Loss: 0.002110 | Commit Loss: 0.000831 | Perplexity: 1066.895102
Trainning Epoch:  49%|████▉     | 328/665 [9:26:17<9:41:11, 103.48s/it]2025-09-15 01:59:03,934 Stage: Train 0.5 | Epoch: 328 | Iter: 247000 | Total Loss: 0.002438 | Recon Loss: 0.002024 | Commit Loss: 0.000828 | Perplexity: 1060.830588
2025-09-15 01:59:31,396 Stage: Train 0.5 | Epoch: 328 | Iter: 247200 | Total Loss: 0.002552 | Recon Loss: 0.002136 | Commit Loss: 0.000832 | Perplexity: 1065.674243
2025-09-15 01:59:58,808 Stage: Train 0.5 | Epoch: 328 | Iter: 247400 | Total Loss: 0.002480 | Recon Loss: 0.002065 | Commit Loss: 0.000830 | Perplexity: 1064.971052
2025-09-15 02:00:26,306 Stage: Train 0.5 | Epoch: 328 | Iter: 247600 | Total Loss: 0.002462 | Recon Loss: 0.002044 | Commit Loss: 0.000835 | Perplexity: 1069.832898
Trainning Epoch:  49%|████▉     | 329/665 [9:28:01<9:39:22, 103.46s/it]2025-09-15 02:00:53,810 Stage: Train 0.5 | Epoch: 329 | Iter: 247800 | Total Loss: 0.002526 | Recon Loss: 0.002107 | Commit Loss: 0.000837 | Perplexity: 1069.918619
2025-09-15 02:01:21,360 Stage: Train 0.5 | Epoch: 329 | Iter: 248000 | Total Loss: 0.002482 | Recon Loss: 0.002069 | Commit Loss: 0.000826 | Perplexity: 1062.613208
2025-09-15 02:01:49,070 Stage: Train 0.5 | Epoch: 329 | Iter: 248200 | Total Loss: 0.002521 | Recon Loss: 0.002105 | Commit Loss: 0.000834 | Perplexity: 1070.439334
2025-09-15 02:02:16,637 Stage: Train 0.5 | Epoch: 329 | Iter: 248400 | Total Loss: 0.002609 | Recon Loss: 0.002191 | Commit Loss: 0.000836 | Perplexity: 1068.366638
Trainning Epoch:  50%|████▉     | 330/665 [9:29:44<9:38:12, 103.56s/it]2025-09-15 02:02:44,044 Stage: Train 0.5 | Epoch: 330 | Iter: 248600 | Total Loss: 0.002470 | Recon Loss: 0.002051 | Commit Loss: 0.000839 | Perplexity: 1070.340885
2025-09-15 02:03:11,474 Stage: Train 0.5 | Epoch: 330 | Iter: 248800 | Total Loss: 0.002464 | Recon Loss: 0.002049 | Commit Loss: 0.000831 | Perplexity: 1067.307983
2025-09-15 02:03:39,032 Stage: Train 0.5 | Epoch: 330 | Iter: 249000 | Total Loss: 0.002505 | Recon Loss: 0.002092 | Commit Loss: 0.000827 | Perplexity: 1065.372155
2025-09-15 02:04:06,436 Stage: Train 0.5 | Epoch: 330 | Iter: 249200 | Total Loss: 0.002474 | Recon Loss: 0.002056 | Commit Loss: 0.000837 | Perplexity: 1069.384751
Trainning Epoch:  50%|████▉     | 331/665 [9:31:28<9:36:05, 103.49s/it]2025-09-15 02:04:33,761 Stage: Train 0.5 | Epoch: 331 | Iter: 249400 | Total Loss: 0.002538 | Recon Loss: 0.002122 | Commit Loss: 0.000832 | Perplexity: 1064.483759
2025-09-15 02:05:01,161 Stage: Train 0.5 | Epoch: 331 | Iter: 249600 | Total Loss: 0.002461 | Recon Loss: 0.002045 | Commit Loss: 0.000833 | Perplexity: 1067.560278
2025-09-15 02:05:28,536 Stage: Train 0.5 | Epoch: 331 | Iter: 249800 | Total Loss: 0.002499 | Recon Loss: 0.002085 | Commit Loss: 0.000828 | Perplexity: 1065.408518
Trainning Epoch:  50%|████▉     | 332/665 [9:33:11<9:33:38, 103.36s/it]2025-09-15 02:05:55,872 Stage: Train 0.5 | Epoch: 332 | Iter: 250000 | Total Loss: 0.002501 | Recon Loss: 0.002085 | Commit Loss: 0.000834 | Perplexity: 1066.821988
2025-09-15 02:06:23,298 Stage: Train 0.5 | Epoch: 332 | Iter: 250200 | Total Loss: 0.002488 | Recon Loss: 0.002071 | Commit Loss: 0.000834 | Perplexity: 1070.503768
2025-09-15 02:06:50,724 Stage: Train 0.5 | Epoch: 332 | Iter: 250400 | Total Loss: 0.002534 | Recon Loss: 0.002117 | Commit Loss: 0.000834 | Perplexity: 1064.439546
2025-09-15 02:07:18,239 Stage: Train 0.5 | Epoch: 332 | Iter: 250600 | Total Loss: 0.002504 | Recon Loss: 0.002089 | Commit Loss: 0.000829 | Perplexity: 1065.583155
Trainning Epoch:  50%|█████     | 333/665 [9:34:54<9:31:49, 103.34s/it]2025-09-15 02:07:45,621 Stage: Train 0.5 | Epoch: 333 | Iter: 250800 | Total Loss: 0.002492 | Recon Loss: 0.002079 | Commit Loss: 0.000826 | Perplexity: 1066.116019
2025-09-15 02:08:13,057 Stage: Train 0.5 | Epoch: 333 | Iter: 251000 | Total Loss: 0.002447 | Recon Loss: 0.002031 | Commit Loss: 0.000832 | Perplexity: 1072.218702
2025-09-15 02:08:40,503 Stage: Train 0.5 | Epoch: 333 | Iter: 251200 | Total Loss: 0.002462 | Recon Loss: 0.002046 | Commit Loss: 0.000833 | Perplexity: 1066.363181
2025-09-15 02:09:07,900 Stage: Train 0.5 | Epoch: 333 | Iter: 251400 | Total Loss: 0.002498 | Recon Loss: 0.002082 | Commit Loss: 0.000832 | Perplexity: 1067.635286
Trainning Epoch:  50%|█████     | 334/665 [9:36:37<9:29:50, 103.30s/it]2025-09-15 02:09:35,230 Stage: Train 0.5 | Epoch: 334 | Iter: 251600 | Total Loss: 0.002503 | Recon Loss: 0.002088 | Commit Loss: 0.000829 | Perplexity: 1068.592288
2025-09-15 02:10:02,602 Stage: Train 0.5 | Epoch: 334 | Iter: 251800 | Total Loss: 0.002481 | Recon Loss: 0.002065 | Commit Loss: 0.000832 | Perplexity: 1069.939816
2025-09-15 02:10:30,241 Stage: Train 0.5 | Epoch: 334 | Iter: 252000 | Total Loss: 0.002510 | Recon Loss: 0.002093 | Commit Loss: 0.000834 | Perplexity: 1063.910901
2025-09-15 02:10:57,676 Stage: Train 0.5 | Epoch: 334 | Iter: 252200 | Total Loss: 0.002453 | Recon Loss: 0.002038 | Commit Loss: 0.000831 | Perplexity: 1072.020431
Trainning Epoch:  50%|█████     | 335/665 [9:38:21<9:28:14, 103.32s/it]2025-09-15 02:11:25,080 Stage: Train 0.5 | Epoch: 335 | Iter: 252400 | Total Loss: 0.002543 | Recon Loss: 0.002128 | Commit Loss: 0.000830 | Perplexity: 1065.178598
2025-09-15 02:11:52,705 Stage: Train 0.5 | Epoch: 335 | Iter: 252600 | Total Loss: 0.002471 | Recon Loss: 0.002053 | Commit Loss: 0.000835 | Perplexity: 1068.253094
2025-09-15 02:12:20,366 Stage: Train 0.5 | Epoch: 335 | Iter: 252800 | Total Loss: 0.002450 | Recon Loss: 0.002036 | Commit Loss: 0.000828 | Perplexity: 1069.737530
2025-09-15 02:12:47,950 Stage: Train 0.5 | Epoch: 335 | Iter: 253000 | Total Loss: 0.002514 | Recon Loss: 0.002097 | Commit Loss: 0.000832 | Perplexity: 1068.769584
Trainning Epoch:  51%|█████     | 336/665 [9:40:05<9:27:21, 103.47s/it]2025-09-15 02:13:15,430 Stage: Train 0.5 | Epoch: 336 | Iter: 253200 | Total Loss: 0.002520 | Recon Loss: 0.002103 | Commit Loss: 0.000834 | Perplexity: 1068.217115
2025-09-15 02:13:42,952 Stage: Train 0.5 | Epoch: 336 | Iter: 253400 | Total Loss: 0.002431 | Recon Loss: 0.002016 | Commit Loss: 0.000830 | Perplexity: 1068.222500
2025-09-15 02:14:10,357 Stage: Train 0.5 | Epoch: 336 | Iter: 253600 | Total Loss: 0.002487 | Recon Loss: 0.002073 | Commit Loss: 0.000829 | Perplexity: 1068.456065
Trainning Epoch:  51%|█████     | 337/665 [9:41:48<9:25:27, 103.44s/it]2025-09-15 02:14:37,724 Stage: Train 0.5 | Epoch: 337 | Iter: 253800 | Total Loss: 0.002486 | Recon Loss: 0.002073 | Commit Loss: 0.000827 | Perplexity: 1068.060601
2025-09-15 02:15:05,302 Stage: Train 0.5 | Epoch: 337 | Iter: 254000 | Total Loss: 0.002483 | Recon Loss: 0.002069 | Commit Loss: 0.000828 | Perplexity: 1067.775655
2025-09-15 02:15:32,986 Stage: Train 0.5 | Epoch: 337 | Iter: 254200 | Total Loss: 0.002524 | Recon Loss: 0.002109 | Commit Loss: 0.000829 | Perplexity: 1063.970373
2025-09-15 02:16:00,611 Stage: Train 0.5 | Epoch: 337 | Iter: 254400 | Total Loss: 0.002432 | Recon Loss: 0.002016 | Commit Loss: 0.000832 | Perplexity: 1072.496316
Trainning Epoch:  51%|█████     | 338/665 [9:43:32<9:24:31, 103.58s/it]2025-09-15 02:16:28,154 Stage: Train 0.5 | Epoch: 338 | Iter: 254600 | Total Loss: 0.002507 | Recon Loss: 0.002091 | Commit Loss: 0.000832 | Perplexity: 1068.433542
2025-09-15 02:16:55,807 Stage: Train 0.5 | Epoch: 338 | Iter: 254800 | Total Loss: 0.002532 | Recon Loss: 0.002119 | Commit Loss: 0.000828 | Perplexity: 1070.193579
2025-09-15 02:17:23,393 Stage: Train 0.5 | Epoch: 338 | Iter: 255000 | Total Loss: 0.002467 | Recon Loss: 0.002053 | Commit Loss: 0.000829 | Perplexity: 1071.322892
2025-09-15 02:17:50,872 Stage: Train 0.5 | Epoch: 338 | Iter: 255200 | Total Loss: 0.002448 | Recon Loss: 0.002035 | Commit Loss: 0.000826 | Perplexity: 1068.254886
Trainning Epoch:  51%|█████     | 339/665 [9:45:16<9:23:04, 103.63s/it]2025-09-15 02:18:18,309 Stage: Train 0.5 | Epoch: 339 | Iter: 255400 | Total Loss: 0.002483 | Recon Loss: 0.002072 | Commit Loss: 0.000822 | Perplexity: 1064.311760
2025-09-15 02:18:45,741 Stage: Train 0.5 | Epoch: 339 | Iter: 255600 | Total Loss: 0.002514 | Recon Loss: 0.002100 | Commit Loss: 0.000829 | Perplexity: 1070.300411
2025-09-15 02:19:13,177 Stage: Train 0.5 | Epoch: 339 | Iter: 255800 | Total Loss: 0.002433 | Recon Loss: 0.002018 | Commit Loss: 0.000830 | Perplexity: 1069.632932
2025-09-15 02:19:40,621 Stage: Train 0.5 | Epoch: 339 | Iter: 256000 | Total Loss: 0.002486 | Recon Loss: 0.002068 | Commit Loss: 0.000836 | Perplexity: 1071.240587
Trainning Epoch:  51%|█████     | 340/665 [9:46:59<9:20:46, 103.53s/it]2025-09-15 02:20:08,036 Stage: Train 0.5 | Epoch: 340 | Iter: 256200 | Total Loss: 0.002475 | Recon Loss: 0.002066 | Commit Loss: 0.000819 | Perplexity: 1066.196544
2025-09-15 02:20:35,587 Stage: Train 0.5 | Epoch: 340 | Iter: 256400 | Total Loss: 0.002467 | Recon Loss: 0.002053 | Commit Loss: 0.000828 | Perplexity: 1069.652141
2025-09-15 02:21:03,174 Stage: Train 0.5 | Epoch: 340 | Iter: 256600 | Total Loss: 0.002455 | Recon Loss: 0.002037 | Commit Loss: 0.000837 | Perplexity: 1072.870099
Trainning Epoch:  51%|█████▏    | 341/665 [9:48:42<9:19:09, 103.55s/it]2025-09-15 02:21:30,626 Stage: Train 0.5 | Epoch: 341 | Iter: 256800 | Total Loss: 0.002529 | Recon Loss: 0.002114 | Commit Loss: 0.000831 | Perplexity: 1070.219431
2025-09-15 02:21:58,060 Stage: Train 0.5 | Epoch: 341 | Iter: 257000 | Total Loss: 0.002462 | Recon Loss: 0.002048 | Commit Loss: 0.000827 | Perplexity: 1067.040039
2025-09-15 02:22:25,535 Stage: Train 0.5 | Epoch: 341 | Iter: 257200 | Total Loss: 0.002450 | Recon Loss: 0.002036 | Commit Loss: 0.000828 | Perplexity: 1070.616821
2025-09-15 02:22:53,020 Stage: Train 0.5 | Epoch: 341 | Iter: 257400 | Total Loss: 0.002465 | Recon Loss: 0.002049 | Commit Loss: 0.000831 | Perplexity: 1069.096235
Trainning Epoch:  51%|█████▏    | 342/665 [9:50:26<9:17:12, 103.51s/it]2025-09-15 02:23:20,509 Stage: Train 0.5 | Epoch: 342 | Iter: 257600 | Total Loss: 0.002468 | Recon Loss: 0.002057 | Commit Loss: 0.000823 | Perplexity: 1069.889630
2025-09-15 02:23:48,026 Stage: Train 0.5 | Epoch: 342 | Iter: 257800 | Total Loss: 0.002486 | Recon Loss: 0.002072 | Commit Loss: 0.000827 | Perplexity: 1072.810189
2025-09-15 02:24:15,576 Stage: Train 0.5 | Epoch: 342 | Iter: 258000 | Total Loss: 0.002618 | Recon Loss: 0.002207 | Commit Loss: 0.000821 | Perplexity: 1062.861762
2025-09-15 02:24:43,160 Stage: Train 0.5 | Epoch: 342 | Iter: 258200 | Total Loss: 0.002446 | Recon Loss: 0.002029 | Commit Loss: 0.000834 | Perplexity: 1070.962203
Trainning Epoch:  52%|█████▏    | 343/665 [9:52:10<9:15:44, 103.55s/it]2025-09-15 02:25:10,589 Stage: Train 0.5 | Epoch: 343 | Iter: 258400 | Total Loss: 0.002512 | Recon Loss: 0.002099 | Commit Loss: 0.000825 | Perplexity: 1063.871364
2025-09-15 02:25:38,112 Stage: Train 0.5 | Epoch: 343 | Iter: 258600 | Total Loss: 0.002473 | Recon Loss: 0.002061 | Commit Loss: 0.000823 | Perplexity: 1069.935571
2025-09-15 02:26:05,671 Stage: Train 0.5 | Epoch: 343 | Iter: 258800 | Total Loss: 0.002440 | Recon Loss: 0.002028 | Commit Loss: 0.000824 | Perplexity: 1066.202534
2025-09-15 02:26:33,230 Stage: Train 0.5 | Epoch: 343 | Iter: 259000 | Total Loss: 0.002486 | Recon Loss: 0.002065 | Commit Loss: 0.000842 | Perplexity: 1074.906182
Trainning Epoch:  52%|█████▏    | 344/665 [9:53:53<9:14:04, 103.57s/it]2025-09-15 02:27:00,695 Stage: Train 0.5 | Epoch: 344 | Iter: 259200 | Total Loss: 0.002425 | Recon Loss: 0.002010 | Commit Loss: 0.000830 | Perplexity: 1071.420294
2025-09-15 02:27:28,255 Stage: Train 0.5 | Epoch: 344 | Iter: 259400 | Total Loss: 0.002495 | Recon Loss: 0.002080 | Commit Loss: 0.000831 | Perplexity: 1072.808304
2025-09-15 02:27:55,694 Stage: Train 0.5 | Epoch: 344 | Iter: 259600 | Total Loss: 0.002419 | Recon Loss: 0.002007 | Commit Loss: 0.000824 | Perplexity: 1067.980984
Trainning Epoch:  52%|█████▏    | 345/665 [9:55:37<9:12:06, 103.52s/it]2025-09-15 02:28:23,071 Stage: Train 0.5 | Epoch: 345 | Iter: 259800 | Total Loss: 0.002456 | Recon Loss: 0.002040 | Commit Loss: 0.000832 | Perplexity: 1070.564727
2025-09-15 02:28:50,509 Stage: Train 0.5 | Epoch: 345 | Iter: 260000 | Total Loss: 0.002507 | Recon Loss: 0.002095 | Commit Loss: 0.000825 | Perplexity: 1070.983841
2025-09-15 02:28:50,509 Saving model at iteration 260000
2025-09-15 02:28:50,894 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_346_step_260000
2025-09-15 02:28:51,103 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_346_step_260000/pytorch_model.bin
2025-09-15 02:28:51,428 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_346_step_260000/optimizer.bin
2025-09-15 02:28:51,429 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_346_step_260000/scheduler.bin
2025-09-15 02:28:51,429 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_346_step_260000/random_states_0.pkl
2025-09-15 02:29:19,270 Stage: Train 0.5 | Epoch: 345 | Iter: 260200 | Total Loss: 0.002473 | Recon Loss: 0.002056 | Commit Loss: 0.000833 | Perplexity: 1074.827112
2025-09-15 02:29:46,691 Stage: Train 0.5 | Epoch: 345 | Iter: 260400 | Total Loss: 0.002418 | Recon Loss: 0.002009 | Commit Loss: 0.000817 | Perplexity: 1067.628879
Trainning Epoch:  52%|█████▏    | 346/665 [9:57:22<9:12:43, 103.96s/it]2025-09-15 02:30:14,496 Stage: Train 0.5 | Epoch: 346 | Iter: 260600 | Total Loss: 0.002493 | Recon Loss: 0.002078 | Commit Loss: 0.000830 | Perplexity: 1067.834216
2025-09-15 02:30:41,882 Stage: Train 0.5 | Epoch: 346 | Iter: 260800 | Total Loss: 0.002441 | Recon Loss: 0.002031 | Commit Loss: 0.000821 | Perplexity: 1066.680801
2025-09-15 02:31:09,304 Stage: Train 0.5 | Epoch: 346 | Iter: 261000 | Total Loss: 0.002499 | Recon Loss: 0.002085 | Commit Loss: 0.000829 | Perplexity: 1072.953116
2025-09-15 02:31:36,732 Stage: Train 0.5 | Epoch: 346 | Iter: 261200 | Total Loss: 0.002456 | Recon Loss: 0.002042 | Commit Loss: 0.000828 | Perplexity: 1070.588940
Trainning Epoch:  52%|█████▏    | 347/665 [9:59:05<9:09:43, 103.72s/it]2025-09-15 02:32:04,094 Stage: Train 0.5 | Epoch: 347 | Iter: 261400 | Total Loss: 0.002538 | Recon Loss: 0.002127 | Commit Loss: 0.000822 | Perplexity: 1064.004722
2025-09-15 02:32:31,650 Stage: Train 0.5 | Epoch: 347 | Iter: 261600 | Total Loss: 0.002367 | Recon Loss: 0.001953 | Commit Loss: 0.000828 | Perplexity: 1072.309778
2025-09-15 02:32:59,051 Stage: Train 0.5 | Epoch: 347 | Iter: 261800 | Total Loss: 0.002476 | Recon Loss: 0.002064 | Commit Loss: 0.000824 | Perplexity: 1069.873433
2025-09-15 02:33:26,571 Stage: Train 0.5 | Epoch: 347 | Iter: 262000 | Total Loss: 0.002538 | Recon Loss: 0.002123 | Commit Loss: 0.000829 | Perplexity: 1073.470324
Trainning Epoch:  52%|█████▏    | 348/665 [10:00:48<9:07:29, 103.62s/it]2025-09-15 02:33:53,956 Stage: Train 0.5 | Epoch: 348 | Iter: 262200 | Total Loss: 0.002467 | Recon Loss: 0.002053 | Commit Loss: 0.000828 | Perplexity: 1071.109723
2025-09-15 02:34:21,370 Stage: Train 0.5 | Epoch: 348 | Iter: 262400 | Total Loss: 0.002497 | Recon Loss: 0.002087 | Commit Loss: 0.000821 | Perplexity: 1071.420858
2025-09-15 02:34:48,878 Stage: Train 0.5 | Epoch: 348 | Iter: 262600 | Total Loss: 0.002467 | Recon Loss: 0.002055 | Commit Loss: 0.000823 | Perplexity: 1072.101504
Trainning Epoch:  52%|█████▏    | 349/665 [10:02:32<9:06:33, 103.78s/it]2025-09-15 02:35:17,108 Stage: Train 0.5 | Epoch: 349 | Iter: 262800 | Total Loss: 0.002456 | Recon Loss: 0.002042 | Commit Loss: 0.000827 | Perplexity: 1069.291335
2025-09-15 02:35:45,081 Stage: Train 0.5 | Epoch: 349 | Iter: 263000 | Total Loss: 0.002448 | Recon Loss: 0.002037 | Commit Loss: 0.000823 | Perplexity: 1072.017170
2025-09-15 02:36:12,800 Stage: Train 0.5 | Epoch: 349 | Iter: 263200 | Total Loss: 0.002455 | Recon Loss: 0.002043 | Commit Loss: 0.000824 | Perplexity: 1071.167686
2025-09-15 02:36:40,245 Stage: Train 0.5 | Epoch: 349 | Iter: 263400 | Total Loss: 0.002465 | Recon Loss: 0.002054 | Commit Loss: 0.000822 | Perplexity: 1071.374314
Trainning Epoch:  53%|█████▎    | 350/665 [10:04:16<9:05:17, 103.86s/it]2025-09-15 02:37:07,616 Stage: Train 0.5 | Epoch: 350 | Iter: 263600 | Total Loss: 0.002477 | Recon Loss: 0.002063 | Commit Loss: 0.000828 | Perplexity: 1071.764173
2025-09-15 02:37:35,058 Stage: Train 0.5 | Epoch: 350 | Iter: 263800 | Total Loss: 0.002461 | Recon Loss: 0.002050 | Commit Loss: 0.000822 | Perplexity: 1074.027517
2025-09-15 02:38:02,479 Stage: Train 0.5 | Epoch: 350 | Iter: 264000 | Total Loss: 0.002539 | Recon Loss: 0.002126 | Commit Loss: 0.000825 | Perplexity: 1071.563604
2025-09-15 02:38:29,918 Stage: Train 0.5 | Epoch: 350 | Iter: 264200 | Total Loss: 0.002434 | Recon Loss: 0.002023 | Commit Loss: 0.000823 | Perplexity: 1071.549285
Trainning Epoch:  53%|█████▎    | 351/665 [10:06:00<9:02:34, 103.68s/it]2025-09-15 02:38:57,292 Stage: Train 0.5 | Epoch: 351 | Iter: 264400 | Total Loss: 0.002448 | Recon Loss: 0.002037 | Commit Loss: 0.000821 | Perplexity: 1066.551243
2025-09-15 02:39:24,821 Stage: Train 0.5 | Epoch: 351 | Iter: 264600 | Total Loss: 0.002462 | Recon Loss: 0.002050 | Commit Loss: 0.000824 | Perplexity: 1072.648426
2025-09-15 02:39:52,253 Stage: Train 0.5 | Epoch: 351 | Iter: 264800 | Total Loss: 0.002419 | Recon Loss: 0.002008 | Commit Loss: 0.000823 | Perplexity: 1071.557810
2025-09-15 02:40:19,675 Stage: Train 0.5 | Epoch: 351 | Iter: 265000 | Total Loss: 0.002444 | Recon Loss: 0.002030 | Commit Loss: 0.000828 | Perplexity: 1071.881107
Trainning Epoch:  53%|█████▎    | 352/665 [10:07:43<9:00:15, 103.57s/it]2025-09-15 02:40:47,049 Stage: Train 0.5 | Epoch: 352 | Iter: 265200 | Total Loss: 0.002498 | Recon Loss: 0.002088 | Commit Loss: 0.000821 | Perplexity: 1069.185700
2025-09-15 02:41:14,462 Stage: Train 0.5 | Epoch: 352 | Iter: 265400 | Total Loss: 0.002445 | Recon Loss: 0.002036 | Commit Loss: 0.000818 | Perplexity: 1066.258582
2025-09-15 02:41:41,868 Stage: Train 0.5 | Epoch: 352 | Iter: 265600 | Total Loss: 0.002449 | Recon Loss: 0.002035 | Commit Loss: 0.000828 | Perplexity: 1074.170626
2025-09-15 02:42:09,274 Stage: Train 0.5 | Epoch: 352 | Iter: 265800 | Total Loss: 0.002486 | Recon Loss: 0.002072 | Commit Loss: 0.000828 | Perplexity: 1073.994128
Trainning Epoch:  53%|█████▎    | 353/665 [10:09:26<8:57:54, 103.44s/it]2025-09-15 02:42:36,757 Stage: Train 0.5 | Epoch: 353 | Iter: 266000 | Total Loss: 0.002442 | Recon Loss: 0.002032 | Commit Loss: 0.000820 | Perplexity: 1072.482796
2025-09-15 02:43:04,245 Stage: Train 0.5 | Epoch: 353 | Iter: 266200 | Total Loss: 0.002425 | Recon Loss: 0.002015 | Commit Loss: 0.000821 | Perplexity: 1071.023773
2025-09-15 02:43:31,767 Stage: Train 0.5 | Epoch: 353 | Iter: 266400 | Total Loss: 0.002476 | Recon Loss: 0.002064 | Commit Loss: 0.000825 | Perplexity: 1075.431816
Trainning Epoch:  53%|█████▎    | 354/665 [10:11:10<8:56:26, 103.49s/it]2025-09-15 02:43:59,305 Stage: Train 0.5 | Epoch: 354 | Iter: 266600 | Total Loss: 0.002547 | Recon Loss: 0.002135 | Commit Loss: 0.000825 | Perplexity: 1075.179465
2025-09-15 02:44:26,724 Stage: Train 0.5 | Epoch: 354 | Iter: 266800 | Total Loss: 0.002392 | Recon Loss: 0.001985 | Commit Loss: 0.000814 | Perplexity: 1069.264328
2025-09-15 02:44:54,240 Stage: Train 0.5 | Epoch: 354 | Iter: 267000 | Total Loss: 0.002407 | Recon Loss: 0.001997 | Commit Loss: 0.000819 | Perplexity: 1075.378973
2025-09-15 02:45:21,731 Stage: Train 0.5 | Epoch: 354 | Iter: 267200 | Total Loss: 0.002471 | Recon Loss: 0.002059 | Commit Loss: 0.000824 | Perplexity: 1070.162098
Trainning Epoch:  53%|█████▎    | 355/665 [10:12:53<8:54:38, 103.48s/it]2025-09-15 02:45:49,213 Stage: Train 0.5 | Epoch: 355 | Iter: 267400 | Total Loss: 0.002509 | Recon Loss: 0.002096 | Commit Loss: 0.000826 | Perplexity: 1077.152752
2025-09-15 02:46:16,850 Stage: Train 0.5 | Epoch: 355 | Iter: 267600 | Total Loss: 0.002510 | Recon Loss: 0.002099 | Commit Loss: 0.000822 | Perplexity: 1078.130332
2025-09-15 02:46:44,336 Stage: Train 0.5 | Epoch: 355 | Iter: 267800 | Total Loss: 0.002456 | Recon Loss: 0.002048 | Commit Loss: 0.000816 | Perplexity: 1068.333474
2025-09-15 02:47:11,843 Stage: Train 0.5 | Epoch: 355 | Iter: 268000 | Total Loss: 0.002433 | Recon Loss: 0.002023 | Commit Loss: 0.000819 | Perplexity: 1069.859060
Trainning Epoch:  54%|█████▎    | 356/665 [10:14:37<8:53:08, 103.52s/it]2025-09-15 02:47:39,290 Stage: Train 0.5 | Epoch: 356 | Iter: 268200 | Total Loss: 0.002479 | Recon Loss: 0.002070 | Commit Loss: 0.000818 | Perplexity: 1071.428799
2025-09-15 02:48:06,797 Stage: Train 0.5 | Epoch: 356 | Iter: 268400 | Total Loss: 0.002437 | Recon Loss: 0.002026 | Commit Loss: 0.000821 | Perplexity: 1074.509647
2025-09-15 02:48:34,290 Stage: Train 0.5 | Epoch: 356 | Iter: 268600 | Total Loss: 0.002458 | Recon Loss: 0.002047 | Commit Loss: 0.000822 | Perplexity: 1076.484962
2025-09-15 02:49:01,758 Stage: Train 0.5 | Epoch: 356 | Iter: 268800 | Total Loss: 0.002494 | Recon Loss: 0.002086 | Commit Loss: 0.000817 | Perplexity: 1072.164507
Trainning Epoch:  54%|█████▎    | 357/665 [10:16:20<8:51:18, 103.50s/it]2025-09-15 02:49:29,272 Stage: Train 0.5 | Epoch: 357 | Iter: 269000 | Total Loss: 0.002381 | Recon Loss: 0.001971 | Commit Loss: 0.000819 | Perplexity: 1071.807448
2025-09-15 02:49:56,730 Stage: Train 0.5 | Epoch: 357 | Iter: 269200 | Total Loss: 0.002448 | Recon Loss: 0.002038 | Commit Loss: 0.000818 | Perplexity: 1073.819439
2025-09-15 02:50:24,196 Stage: Train 0.5 | Epoch: 357 | Iter: 269400 | Total Loss: 0.002411 | Recon Loss: 0.001997 | Commit Loss: 0.000827 | Perplexity: 1076.111899
Trainning Epoch:  54%|█████▍    | 358/665 [10:18:04<8:49:35, 103.50s/it]2025-09-15 02:50:51,693 Stage: Train 0.5 | Epoch: 358 | Iter: 269600 | Total Loss: 0.002434 | Recon Loss: 0.002022 | Commit Loss: 0.000825 | Perplexity: 1071.492021
2025-09-15 02:51:19,197 Stage: Train 0.5 | Epoch: 358 | Iter: 269800 | Total Loss: 0.002466 | Recon Loss: 0.002057 | Commit Loss: 0.000817 | Perplexity: 1071.839850
2025-09-15 02:51:46,632 Stage: Train 0.5 | Epoch: 358 | Iter: 270000 | Total Loss: 0.002460 | Recon Loss: 0.002051 | Commit Loss: 0.000817 | Perplexity: 1075.521857
2025-09-15 02:52:14,215 Stage: Train 0.5 | Epoch: 358 | Iter: 270200 | Total Loss: 0.002424 | Recon Loss: 0.002009 | Commit Loss: 0.000828 | Perplexity: 1079.571736
Trainning Epoch:  54%|█████▍    | 359/665 [10:19:47<8:48:23, 103.61s/it]2025-09-15 02:52:42,034 Stage: Train 0.5 | Epoch: 359 | Iter: 270400 | Total Loss: 0.002504 | Recon Loss: 0.002100 | Commit Loss: 0.000809 | Perplexity: 1069.556571
2025-09-15 02:53:09,831 Stage: Train 0.5 | Epoch: 359 | Iter: 270600 | Total Loss: 0.002390 | Recon Loss: 0.001982 | Commit Loss: 0.000816 | Perplexity: 1076.795563
2025-09-15 02:53:37,846 Stage: Train 0.5 | Epoch: 359 | Iter: 270800 | Total Loss: 0.002422 | Recon Loss: 0.002012 | Commit Loss: 0.000821 | Perplexity: 1077.463382
2025-09-15 02:54:05,674 Stage: Train 0.5 | Epoch: 359 | Iter: 271000 | Total Loss: 0.002444 | Recon Loss: 0.002033 | Commit Loss: 0.000822 | Perplexity: 1075.071560
Trainning Epoch:  54%|█████▍    | 360/665 [10:21:32<8:48:18, 103.93s/it]2025-09-15 02:54:33,212 Stage: Train 0.5 | Epoch: 360 | Iter: 271200 | Total Loss: 0.002451 | Recon Loss: 0.002038 | Commit Loss: 0.000826 | Perplexity: 1078.318757
2025-09-15 02:55:00,892 Stage: Train 0.5 | Epoch: 360 | Iter: 271400 | Total Loss: 0.002446 | Recon Loss: 0.002040 | Commit Loss: 0.000810 | Perplexity: 1070.636350
2025-09-15 02:55:28,537 Stage: Train 0.5 | Epoch: 360 | Iter: 271600 | Total Loss: 0.002433 | Recon Loss: 0.002023 | Commit Loss: 0.000821 | Perplexity: 1078.752287
2025-09-15 02:55:56,184 Stage: Train 0.5 | Epoch: 360 | Iter: 271800 | Total Loss: 0.002433 | Recon Loss: 0.002022 | Commit Loss: 0.000823 | Perplexity: 1074.229296
Trainning Epoch:  54%|█████▍    | 361/665 [10:23:16<8:46:54, 104.00s/it]2025-09-15 02:56:24,108 Stage: Train 0.5 | Epoch: 361 | Iter: 272000 | Total Loss: 0.002485 | Recon Loss: 0.002078 | Commit Loss: 0.000813 | Perplexity: 1070.814876
2025-09-15 02:56:51,807 Stage: Train 0.5 | Epoch: 361 | Iter: 272200 | Total Loss: 0.002414 | Recon Loss: 0.002004 | Commit Loss: 0.000821 | Perplexity: 1080.788353
2025-09-15 02:57:19,710 Stage: Train 0.5 | Epoch: 361 | Iter: 272400 | Total Loss: 0.002465 | Recon Loss: 0.002055 | Commit Loss: 0.000822 | Perplexity: 1076.677031
Trainning Epoch:  54%|█████▍    | 362/665 [10:25:01<8:45:54, 104.14s/it]2025-09-15 02:57:47,184 Stage: Train 0.5 | Epoch: 362 | Iter: 272600 | Total Loss: 0.002420 | Recon Loss: 0.002010 | Commit Loss: 0.000821 | Perplexity: 1076.794747
2025-09-15 02:58:14,741 Stage: Train 0.5 | Epoch: 362 | Iter: 272800 | Total Loss: 0.002427 | Recon Loss: 0.002019 | Commit Loss: 0.000816 | Perplexity: 1080.562606
2025-09-15 02:58:42,287 Stage: Train 0.5 | Epoch: 362 | Iter: 273000 | Total Loss: 0.002417 | Recon Loss: 0.002004 | Commit Loss: 0.000825 | Perplexity: 1079.806588
2025-09-15 02:59:09,829 Stage: Train 0.5 | Epoch: 362 | Iter: 273200 | Total Loss: 0.002399 | Recon Loss: 0.001992 | Commit Loss: 0.000812 | Perplexity: 1073.451186
Trainning Epoch:  55%|█████▍    | 363/665 [10:26:44<8:43:26, 104.00s/it]2025-09-15 02:59:37,416 Stage: Train 0.5 | Epoch: 363 | Iter: 273400 | Total Loss: 0.002428 | Recon Loss: 0.002017 | Commit Loss: 0.000821 | Perplexity: 1074.173981
2025-09-15 03:00:04,934 Stage: Train 0.5 | Epoch: 363 | Iter: 273600 | Total Loss: 0.002435 | Recon Loss: 0.002023 | Commit Loss: 0.000823 | Perplexity: 1078.764136
2025-09-15 03:00:32,486 Stage: Train 0.5 | Epoch: 363 | Iter: 273800 | Total Loss: 0.002544 | Recon Loss: 0.002137 | Commit Loss: 0.000814 | Perplexity: 1071.127596
2025-09-15 03:00:59,941 Stage: Train 0.5 | Epoch: 363 | Iter: 274000 | Total Loss: 0.002373 | Recon Loss: 0.001966 | Commit Loss: 0.000815 | Perplexity: 1074.630006
Trainning Epoch:  55%|█████▍    | 364/665 [10:28:28<8:41:07, 103.88s/it]2025-09-15 03:01:27,353 Stage: Train 0.5 | Epoch: 364 | Iter: 274200 | Total Loss: 0.002474 | Recon Loss: 0.002066 | Commit Loss: 0.000817 | Perplexity: 1076.492847
2025-09-15 03:01:54,813 Stage: Train 0.5 | Epoch: 364 | Iter: 274400 | Total Loss: 0.002412 | Recon Loss: 0.002002 | Commit Loss: 0.000819 | Perplexity: 1079.560480
2025-09-15 03:02:22,270 Stage: Train 0.5 | Epoch: 364 | Iter: 274600 | Total Loss: 0.002505 | Recon Loss: 0.002097 | Commit Loss: 0.000817 | Perplexity: 1073.489286
2025-09-15 03:02:49,724 Stage: Train 0.5 | Epoch: 364 | Iter: 274800 | Total Loss: 0.002424 | Recon Loss: 0.002017 | Commit Loss: 0.000813 | Perplexity: 1075.505573
Trainning Epoch:  55%|█████▍    | 365/665 [10:30:11<8:38:35, 103.72s/it]2025-09-15 03:03:17,266 Stage: Train 0.5 | Epoch: 365 | Iter: 275000 | Total Loss: 0.002372 | Recon Loss: 0.001964 | Commit Loss: 0.000816 | Perplexity: 1073.668566
2025-09-15 03:03:44,780 Stage: Train 0.5 | Epoch: 365 | Iter: 275200 | Total Loss: 0.002434 | Recon Loss: 0.002025 | Commit Loss: 0.000817 | Perplexity: 1076.228504
2025-09-15 03:04:12,214 Stage: Train 0.5 | Epoch: 365 | Iter: 275400 | Total Loss: 0.002442 | Recon Loss: 0.002035 | Commit Loss: 0.000815 | Perplexity: 1076.993042
Trainning Epoch:  55%|█████▌    | 366/665 [10:31:55<8:36:31, 103.65s/it]2025-09-15 03:04:39,654 Stage: Train 0.5 | Epoch: 366 | Iter: 275600 | Total Loss: 0.002422 | Recon Loss: 0.002015 | Commit Loss: 0.000813 | Perplexity: 1072.902605
2025-09-15 03:05:07,139 Stage: Train 0.5 | Epoch: 366 | Iter: 275800 | Total Loss: 0.002525 | Recon Loss: 0.002121 | Commit Loss: 0.000809 | Perplexity: 1074.287247
2025-09-15 03:05:34,583 Stage: Train 0.5 | Epoch: 366 | Iter: 276000 | Total Loss: 0.002442 | Recon Loss: 0.002035 | Commit Loss: 0.000813 | Perplexity: 1074.029337
2025-09-15 03:06:02,049 Stage: Train 0.5 | Epoch: 366 | Iter: 276200 | Total Loss: 0.002404 | Recon Loss: 0.001997 | Commit Loss: 0.000815 | Perplexity: 1076.841318
Trainning Epoch:  55%|█████▌    | 367/665 [10:33:38<8:34:23, 103.57s/it]2025-09-15 03:06:29,566 Stage: Train 0.5 | Epoch: 367 | Iter: 276400 | Total Loss: 0.002406 | Recon Loss: 0.001997 | Commit Loss: 0.000817 | Perplexity: 1076.219479
2025-09-15 03:06:57,056 Stage: Train 0.5 | Epoch: 367 | Iter: 276600 | Total Loss: 0.002465 | Recon Loss: 0.002056 | Commit Loss: 0.000818 | Perplexity: 1078.152067
2025-09-15 03:07:24,544 Stage: Train 0.5 | Epoch: 367 | Iter: 276800 | Total Loss: 0.002455 | Recon Loss: 0.002046 | Commit Loss: 0.000817 | Perplexity: 1080.221733
2025-09-15 03:07:51,989 Stage: Train 0.5 | Epoch: 367 | Iter: 277000 | Total Loss: 0.002388 | Recon Loss: 0.001983 | Commit Loss: 0.000809 | Perplexity: 1071.257997
Trainning Epoch:  55%|█████▌    | 368/665 [10:35:22<8:32:35, 103.55s/it]2025-09-15 03:08:19,429 Stage: Train 0.5 | Epoch: 368 | Iter: 277200 | Total Loss: 0.002504 | Recon Loss: 0.002094 | Commit Loss: 0.000819 | Perplexity: 1071.821949
2025-09-15 03:08:46,878 Stage: Train 0.5 | Epoch: 368 | Iter: 277400 | Total Loss: 0.002406 | Recon Loss: 0.001999 | Commit Loss: 0.000814 | Perplexity: 1079.345592
2025-09-15 03:09:14,334 Stage: Train 0.5 | Epoch: 368 | Iter: 277600 | Total Loss: 0.002415 | Recon Loss: 0.002009 | Commit Loss: 0.000814 | Perplexity: 1074.607538
2025-09-15 03:09:41,783 Stage: Train 0.5 | Epoch: 368 | Iter: 277800 | Total Loss: 0.002420 | Recon Loss: 0.002008 | Commit Loss: 0.000822 | Perplexity: 1081.417702
Trainning Epoch:  55%|█████▌    | 369/665 [10:37:05<8:30:29, 103.48s/it]2025-09-15 03:10:09,298 Stage: Train 0.5 | Epoch: 369 | Iter: 278000 | Total Loss: 0.002442 | Recon Loss: 0.002035 | Commit Loss: 0.000813 | Perplexity: 1072.223507
2025-09-15 03:10:36,732 Stage: Train 0.5 | Epoch: 369 | Iter: 278200 | Total Loss: 0.002456 | Recon Loss: 0.002048 | Commit Loss: 0.000815 | Perplexity: 1077.585139
2025-09-15 03:11:04,189 Stage: Train 0.5 | Epoch: 369 | Iter: 278400 | Total Loss: 0.002414 | Recon Loss: 0.002005 | Commit Loss: 0.000818 | Perplexity: 1077.416963
2025-09-15 03:11:31,629 Stage: Train 0.5 | Epoch: 369 | Iter: 278600 | Total Loss: 0.002481 | Recon Loss: 0.002076 | Commit Loss: 0.000811 | Perplexity: 1072.697376
Trainning Epoch:  56%|█████▌    | 370/665 [10:38:48<8:28:38, 103.45s/it]2025-09-15 03:11:59,026 Stage: Train 0.5 | Epoch: 370 | Iter: 278800 | Total Loss: 0.002436 | Recon Loss: 0.002031 | Commit Loss: 0.000810 | Perplexity: 1077.319705
2025-09-15 03:12:26,687 Stage: Train 0.5 | Epoch: 370 | Iter: 279000 | Total Loss: 0.002447 | Recon Loss: 0.002039 | Commit Loss: 0.000815 | Perplexity: 1076.007943
2025-09-15 03:12:54,294 Stage: Train 0.5 | Epoch: 370 | Iter: 279200 | Total Loss: 0.002386 | Recon Loss: 0.001976 | Commit Loss: 0.000819 | Perplexity: 1083.063856
Trainning Epoch:  56%|█████▌    | 371/665 [10:40:32<8:27:31, 103.58s/it]2025-09-15 03:13:21,904 Stage: Train 0.5 | Epoch: 371 | Iter: 279400 | Total Loss: 0.002518 | Recon Loss: 0.002113 | Commit Loss: 0.000811 | Perplexity: 1069.884633
2025-09-15 03:13:49,426 Stage: Train 0.5 | Epoch: 371 | Iter: 279600 | Total Loss: 0.002349 | Recon Loss: 0.001943 | Commit Loss: 0.000812 | Perplexity: 1079.218210
2025-09-15 03:14:16,925 Stage: Train 0.5 | Epoch: 371 | Iter: 279800 | Total Loss: 0.002415 | Recon Loss: 0.002008 | Commit Loss: 0.000812 | Perplexity: 1077.683454
2025-09-15 03:14:44,597 Stage: Train 0.5 | Epoch: 371 | Iter: 280000 | Total Loss: 0.002405 | Recon Loss: 0.001996 | Commit Loss: 0.000818 | Perplexity: 1077.499091
2025-09-15 03:14:44,597 Saving model at iteration 280000
2025-09-15 03:14:45,039 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_372_step_280000
2025-09-15 03:14:45,252 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_372_step_280000/pytorch_model.bin
2025-09-15 03:14:45,578 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_372_step_280000/optimizer.bin
2025-09-15 03:14:45,578 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_372_step_280000/scheduler.bin
2025-09-15 03:14:45,579 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_372_step_280000/random_states_0.pkl
Trainning Epoch:  56%|█████▌    | 372/665 [10:42:18<8:28:16, 104.08s/it]2025-09-15 03:15:13,619 Stage: Train 0.5 | Epoch: 372 | Iter: 280200 | Total Loss: 0.002427 | Recon Loss: 0.002021 | Commit Loss: 0.000810 | Perplexity: 1074.827971
2025-09-15 03:15:41,174 Stage: Train 0.5 | Epoch: 372 | Iter: 280400 | Total Loss: 0.002453 | Recon Loss: 0.002047 | Commit Loss: 0.000812 | Perplexity: 1072.550306
2025-09-15 03:16:08,673 Stage: Train 0.5 | Epoch: 372 | Iter: 280600 | Total Loss: 0.002379 | Recon Loss: 0.001973 | Commit Loss: 0.000811 | Perplexity: 1077.366208
2025-09-15 03:16:36,134 Stage: Train 0.5 | Epoch: 372 | Iter: 280800 | Total Loss: 0.002465 | Recon Loss: 0.002058 | Commit Loss: 0.000813 | Perplexity: 1073.426955
Trainning Epoch:  56%|█████▌    | 373/665 [10:44:01<8:25:41, 103.91s/it]2025-09-15 03:17:03,554 Stage: Train 0.5 | Epoch: 373 | Iter: 281000 | Total Loss: 0.002415 | Recon Loss: 0.002011 | Commit Loss: 0.000809 | Perplexity: 1074.907581
2025-09-15 03:17:30,963 Stage: Train 0.5 | Epoch: 373 | Iter: 281200 | Total Loss: 0.002427 | Recon Loss: 0.002018 | Commit Loss: 0.000817 | Perplexity: 1078.268873
2025-09-15 03:17:58,408 Stage: Train 0.5 | Epoch: 373 | Iter: 281400 | Total Loss: 0.002416 | Recon Loss: 0.002009 | Commit Loss: 0.000813 | Perplexity: 1074.438596
2025-09-15 03:18:25,917 Stage: Train 0.5 | Epoch: 373 | Iter: 281600 | Total Loss: 0.002426 | Recon Loss: 0.002022 | Commit Loss: 0.000808 | Perplexity: 1073.832375
Trainning Epoch:  56%|█████▌    | 374/665 [10:45:44<8:23:03, 103.72s/it]2025-09-15 03:18:53,298 Stage: Train 0.5 | Epoch: 374 | Iter: 281800 | Total Loss: 0.002374 | Recon Loss: 0.001969 | Commit Loss: 0.000810 | Perplexity: 1076.708076
2025-09-15 03:19:20,702 Stage: Train 0.5 | Epoch: 374 | Iter: 282000 | Total Loss: 0.002465 | Recon Loss: 0.002059 | Commit Loss: 0.000812 | Perplexity: 1077.696748
2025-09-15 03:19:48,116 Stage: Train 0.5 | Epoch: 374 | Iter: 282200 | Total Loss: 0.002416 | Recon Loss: 0.002008 | Commit Loss: 0.000815 | Perplexity: 1078.679020
Trainning Epoch:  56%|█████▋    | 375/665 [10:47:28<8:20:34, 103.57s/it]2025-09-15 03:20:15,511 Stage: Train 0.5 | Epoch: 375 | Iter: 282400 | Total Loss: 0.002394 | Recon Loss: 0.001989 | Commit Loss: 0.000810 | Perplexity: 1073.421943
2025-09-15 03:20:42,959 Stage: Train 0.5 | Epoch: 375 | Iter: 282600 | Total Loss: 0.002368 | Recon Loss: 0.001958 | Commit Loss: 0.000821 | Perplexity: 1081.582226
2025-09-15 03:21:10,360 Stage: Train 0.5 | Epoch: 375 | Iter: 282800 | Total Loss: 0.002479 | Recon Loss: 0.002074 | Commit Loss: 0.000810 | Perplexity: 1073.202518
2025-09-15 03:21:37,870 Stage: Train 0.5 | Epoch: 375 | Iter: 283000 | Total Loss: 0.002494 | Recon Loss: 0.002089 | Commit Loss: 0.000810 | Perplexity: 1073.401541
Trainning Epoch:  57%|█████▋    | 376/665 [10:49:11<8:18:24, 103.48s/it]2025-09-15 03:22:05,215 Stage: Train 0.5 | Epoch: 376 | Iter: 283200 | Total Loss: 0.002396 | Recon Loss: 0.001992 | Commit Loss: 0.000806 | Perplexity: 1073.247623
2025-09-15 03:22:32,624 Stage: Train 0.5 | Epoch: 376 | Iter: 283400 | Total Loss: 0.002362 | Recon Loss: 0.001958 | Commit Loss: 0.000808 | Perplexity: 1075.369259
2025-09-15 03:23:00,018 Stage: Train 0.5 | Epoch: 376 | Iter: 283600 | Total Loss: 0.002456 | Recon Loss: 0.002046 | Commit Loss: 0.000820 | Perplexity: 1078.827325
2025-09-15 03:23:27,426 Stage: Train 0.5 | Epoch: 376 | Iter: 283800 | Total Loss: 0.002394 | Recon Loss: 0.001989 | Commit Loss: 0.000810 | Perplexity: 1075.329850
Trainning Epoch:  57%|█████▋    | 377/665 [10:50:54<8:16:11, 103.37s/it]2025-09-15 03:23:54,770 Stage: Train 0.5 | Epoch: 377 | Iter: 284000 | Total Loss: 0.002391 | Recon Loss: 0.001983 | Commit Loss: 0.000815 | Perplexity: 1076.354615
2025-09-15 03:24:22,181 Stage: Train 0.5 | Epoch: 377 | Iter: 284200 | Total Loss: 0.002390 | Recon Loss: 0.001985 | Commit Loss: 0.000811 | Perplexity: 1077.281331
2025-09-15 03:24:49,626 Stage: Train 0.5 | Epoch: 377 | Iter: 284400 | Total Loss: 0.002400 | Recon Loss: 0.001996 | Commit Loss: 0.000807 | Perplexity: 1075.580495
2025-09-15 03:25:17,186 Stage: Train 0.5 | Epoch: 377 | Iter: 284600 | Total Loss: 0.002418 | Recon Loss: 0.002009 | Commit Loss: 0.000816 | Perplexity: 1078.240257
Trainning Epoch:  57%|█████▋    | 378/665 [10:52:37<8:14:23, 103.36s/it]2025-09-15 03:25:44,578 Stage: Train 0.5 | Epoch: 378 | Iter: 284800 | Total Loss: 0.002438 | Recon Loss: 0.002032 | Commit Loss: 0.000811 | Perplexity: 1076.107431
2025-09-15 03:26:12,036 Stage: Train 0.5 | Epoch: 378 | Iter: 285000 | Total Loss: 0.002363 | Recon Loss: 0.001955 | Commit Loss: 0.000816 | Perplexity: 1077.580971
2025-09-15 03:26:39,510 Stage: Train 0.5 | Epoch: 378 | Iter: 285200 | Total Loss: 0.002457 | Recon Loss: 0.002052 | Commit Loss: 0.000810 | Perplexity: 1074.353674
Trainning Epoch:  57%|█████▋    | 379/665 [10:54:21<8:12:35, 103.34s/it]2025-09-15 03:27:06,886 Stage: Train 0.5 | Epoch: 379 | Iter: 285400 | Total Loss: 0.002443 | Recon Loss: 0.002040 | Commit Loss: 0.000807 | Perplexity: 1072.180853
2025-09-15 03:27:34,323 Stage: Train 0.5 | Epoch: 379 | Iter: 285600 | Total Loss: 0.002399 | Recon Loss: 0.001996 | Commit Loss: 0.000806 | Perplexity: 1077.060487
2025-09-15 03:28:01,723 Stage: Train 0.5 | Epoch: 379 | Iter: 285800 | Total Loss: 0.002421 | Recon Loss: 0.002015 | Commit Loss: 0.000812 | Perplexity: 1076.031458
2025-09-15 03:28:29,165 Stage: Train 0.5 | Epoch: 379 | Iter: 286000 | Total Loss: 0.002410 | Recon Loss: 0.002001 | Commit Loss: 0.000817 | Perplexity: 1077.723410
Trainning Epoch:  57%|█████▋    | 380/665 [10:56:04<8:10:46, 103.32s/it]2025-09-15 03:28:56,593 Stage: Train 0.5 | Epoch: 380 | Iter: 286200 | Total Loss: 0.002384 | Recon Loss: 0.001980 | Commit Loss: 0.000808 | Perplexity: 1078.156850
2025-09-15 03:29:24,037 Stage: Train 0.5 | Epoch: 380 | Iter: 286400 | Total Loss: 0.002461 | Recon Loss: 0.002054 | Commit Loss: 0.000813 | Perplexity: 1076.169633
2025-09-15 03:29:51,456 Stage: Train 0.5 | Epoch: 380 | Iter: 286600 | Total Loss: 0.002354 | Recon Loss: 0.001950 | Commit Loss: 0.000809 | Perplexity: 1074.539704
2025-09-15 03:30:18,874 Stage: Train 0.5 | Epoch: 380 | Iter: 286800 | Total Loss: 0.002414 | Recon Loss: 0.002010 | Commit Loss: 0.000809 | Perplexity: 1075.438665
Trainning Epoch:  57%|█████▋    | 381/665 [10:57:47<8:08:54, 103.29s/it]2025-09-15 03:30:46,380 Stage: Train 0.5 | Epoch: 381 | Iter: 287000 | Total Loss: 0.002381 | Recon Loss: 0.001975 | Commit Loss: 0.000812 | Perplexity: 1073.972154
2025-09-15 03:31:13,878 Stage: Train 0.5 | Epoch: 381 | Iter: 287200 | Total Loss: 0.002401 | Recon Loss: 0.001999 | Commit Loss: 0.000803 | Perplexity: 1075.100560
2025-09-15 03:31:41,340 Stage: Train 0.5 | Epoch: 381 | Iter: 287400 | Total Loss: 0.002385 | Recon Loss: 0.001980 | Commit Loss: 0.000810 | Perplexity: 1079.633533
2025-09-15 03:32:08,877 Stage: Train 0.5 | Epoch: 381 | Iter: 287600 | Total Loss: 0.002352 | Recon Loss: 0.001943 | Commit Loss: 0.000817 | Perplexity: 1077.377044
Trainning Epoch:  57%|█████▋    | 382/665 [10:59:31<8:07:32, 103.37s/it]2025-09-15 03:32:36,187 Stage: Train 0.5 | Epoch: 382 | Iter: 287800 | Total Loss: 0.002449 | Recon Loss: 0.002041 | Commit Loss: 0.000816 | Perplexity: 1078.209297
2025-09-15 03:33:03,533 Stage: Train 0.5 | Epoch: 382 | Iter: 288000 | Total Loss: 0.002405 | Recon Loss: 0.002000 | Commit Loss: 0.000810 | Perplexity: 1076.704877
2025-09-15 03:33:30,916 Stage: Train 0.5 | Epoch: 382 | Iter: 288200 | Total Loss: 0.002382 | Recon Loss: 0.001979 | Commit Loss: 0.000807 | Perplexity: 1079.623914
Trainning Epoch:  58%|█████▊    | 383/665 [11:01:14<8:05:21, 103.27s/it]2025-09-15 03:33:58,306 Stage: Train 0.5 | Epoch: 383 | Iter: 288400 | Total Loss: 0.002405 | Recon Loss: 0.002001 | Commit Loss: 0.000809 | Perplexity: 1071.628314
2025-09-15 03:34:25,732 Stage: Train 0.5 | Epoch: 383 | Iter: 288600 | Total Loss: 0.002430 | Recon Loss: 0.002028 | Commit Loss: 0.000805 | Perplexity: 1074.860321
2025-09-15 03:34:53,146 Stage: Train 0.5 | Epoch: 383 | Iter: 288800 | Total Loss: 0.002447 | Recon Loss: 0.002040 | Commit Loss: 0.000814 | Perplexity: 1080.738271
2025-09-15 03:35:20,673 Stage: Train 0.5 | Epoch: 383 | Iter: 289000 | Total Loss: 0.002422 | Recon Loss: 0.002017 | Commit Loss: 0.000809 | Perplexity: 1076.441221
Trainning Epoch:  58%|█████▊    | 384/665 [11:02:57<8:03:41, 103.28s/it]2025-09-15 03:35:48,071 Stage: Train 0.5 | Epoch: 384 | Iter: 289200 | Total Loss: 0.002449 | Recon Loss: 0.002046 | Commit Loss: 0.000807 | Perplexity: 1068.417866
2025-09-15 03:36:15,613 Stage: Train 0.5 | Epoch: 384 | Iter: 289400 | Total Loss: 0.002367 | Recon Loss: 0.001963 | Commit Loss: 0.000808 | Perplexity: 1077.121944
2025-09-15 03:36:43,076 Stage: Train 0.5 | Epoch: 384 | Iter: 289600 | Total Loss: 0.002385 | Recon Loss: 0.001982 | Commit Loss: 0.000808 | Perplexity: 1080.004262
2025-09-15 03:37:10,554 Stage: Train 0.5 | Epoch: 384 | Iter: 289800 | Total Loss: 0.002458 | Recon Loss: 0.002055 | Commit Loss: 0.000806 | Perplexity: 1074.971596
Trainning Epoch:  58%|█████▊    | 385/665 [11:04:40<8:02:11, 103.33s/it]2025-09-15 03:37:37,956 Stage: Train 0.5 | Epoch: 385 | Iter: 290000 | Total Loss: 0.002374 | Recon Loss: 0.001968 | Commit Loss: 0.000812 | Perplexity: 1077.254142
2025-09-15 03:38:05,398 Stage: Train 0.5 | Epoch: 385 | Iter: 290200 | Total Loss: 0.002388 | Recon Loss: 0.001986 | Commit Loss: 0.000803 | Perplexity: 1074.048145
2025-09-15 03:38:32,808 Stage: Train 0.5 | Epoch: 385 | Iter: 290400 | Total Loss: 0.002402 | Recon Loss: 0.001996 | Commit Loss: 0.000813 | Perplexity: 1079.843465
2025-09-15 03:39:00,360 Stage: Train 0.5 | Epoch: 385 | Iter: 290600 | Total Loss: 0.002388 | Recon Loss: 0.001982 | Commit Loss: 0.000812 | Perplexity: 1075.813713
Trainning Epoch:  58%|█████▊    | 386/665 [11:06:24<8:00:32, 103.34s/it]2025-09-15 03:39:27,907 Stage: Train 0.5 | Epoch: 386 | Iter: 290800 | Total Loss: 0.002407 | Recon Loss: 0.001999 | Commit Loss: 0.000815 | Perplexity: 1076.051378
2025-09-15 03:39:55,567 Stage: Train 0.5 | Epoch: 386 | Iter: 291000 | Total Loss: 0.002416 | Recon Loss: 0.002013 | Commit Loss: 0.000805 | Perplexity: 1077.253912
2025-09-15 03:40:23,211 Stage: Train 0.5 | Epoch: 386 | Iter: 291200 | Total Loss: 0.002347 | Recon Loss: 0.001943 | Commit Loss: 0.000808 | Perplexity: 1075.026487
2025-09-15 03:40:50,797 Stage: Train 0.5 | Epoch: 386 | Iter: 291400 | Total Loss: 0.002417 | Recon Loss: 0.002012 | Commit Loss: 0.000810 | Perplexity: 1073.853096
Trainning Epoch:  58%|█████▊    | 387/665 [11:08:08<7:59:44, 103.54s/it]2025-09-15 03:41:18,474 Stage: Train 0.5 | Epoch: 387 | Iter: 291600 | Total Loss: 0.002447 | Recon Loss: 0.002046 | Commit Loss: 0.000802 | Perplexity: 1071.502515
2025-09-15 03:41:46,110 Stage: Train 0.5 | Epoch: 387 | Iter: 291800 | Total Loss: 0.002361 | Recon Loss: 0.001957 | Commit Loss: 0.000809 | Perplexity: 1078.020594
2025-09-15 03:42:13,772 Stage: Train 0.5 | Epoch: 387 | Iter: 292000 | Total Loss: 0.002428 | Recon Loss: 0.002021 | Commit Loss: 0.000815 | Perplexity: 1079.559422
Trainning Epoch:  58%|█████▊    | 388/665 [11:09:52<7:58:41, 103.69s/it]2025-09-15 03:42:41,283 Stage: Train 0.5 | Epoch: 388 | Iter: 292200 | Total Loss: 0.002326 | Recon Loss: 0.001924 | Commit Loss: 0.000804 | Perplexity: 1074.596357
2025-09-15 03:43:08,786 Stage: Train 0.5 | Epoch: 388 | Iter: 292400 | Total Loss: 0.002412 | Recon Loss: 0.002005 | Commit Loss: 0.000815 | Perplexity: 1077.401407
2025-09-15 03:43:36,236 Stage: Train 0.5 | Epoch: 388 | Iter: 292600 | Total Loss: 0.002374 | Recon Loss: 0.001967 | Commit Loss: 0.000813 | Perplexity: 1079.134758
2025-09-15 03:44:03,804 Stage: Train 0.5 | Epoch: 388 | Iter: 292800 | Total Loss: 0.002427 | Recon Loss: 0.002024 | Commit Loss: 0.000808 | Perplexity: 1075.166220
Trainning Epoch:  58%|█████▊    | 389/665 [11:11:36<7:56:58, 103.69s/it]2025-09-15 03:44:31,488 Stage: Train 0.5 | Epoch: 389 | Iter: 293000 | Total Loss: 0.002389 | Recon Loss: 0.001989 | Commit Loss: 0.000799 | Perplexity: 1072.624962
2025-09-15 03:44:59,024 Stage: Train 0.5 | Epoch: 389 | Iter: 293200 | Total Loss: 0.002336 | Recon Loss: 0.001932 | Commit Loss: 0.000808 | Perplexity: 1076.814839
2025-09-15 03:45:26,672 Stage: Train 0.5 | Epoch: 389 | Iter: 293400 | Total Loss: 0.002386 | Recon Loss: 0.001981 | Commit Loss: 0.000809 | Perplexity: 1074.585179
2025-09-15 03:45:54,196 Stage: Train 0.5 | Epoch: 389 | Iter: 293600 | Total Loss: 0.002400 | Recon Loss: 0.001995 | Commit Loss: 0.000809 | Perplexity: 1078.399566
Trainning Epoch:  59%|█████▊    | 390/665 [11:13:19<7:55:24, 103.72s/it]2025-09-15 03:46:21,762 Stage: Train 0.5 | Epoch: 390 | Iter: 293800 | Total Loss: 0.002389 | Recon Loss: 0.001980 | Commit Loss: 0.000818 | Perplexity: 1077.182295
2025-09-15 03:46:49,308 Stage: Train 0.5 | Epoch: 390 | Iter: 294000 | Total Loss: 0.002373 | Recon Loss: 0.001971 | Commit Loss: 0.000804 | Perplexity: 1075.475305
2025-09-15 03:47:16,721 Stage: Train 0.5 | Epoch: 390 | Iter: 294200 | Total Loss: 0.002396 | Recon Loss: 0.001993 | Commit Loss: 0.000805 | Perplexity: 1074.712425
2025-09-15 03:47:44,192 Stage: Train 0.5 | Epoch: 390 | Iter: 294400 | Total Loss: 0.002384 | Recon Loss: 0.001980 | Commit Loss: 0.000808 | Perplexity: 1078.732814
Trainning Epoch:  59%|█████▉    | 391/665 [11:15:03<7:53:19, 103.65s/it]2025-09-15 03:48:11,571 Stage: Train 0.5 | Epoch: 391 | Iter: 294600 | Total Loss: 0.002346 | Recon Loss: 0.001944 | Commit Loss: 0.000804 | Perplexity: 1075.566229
2025-09-15 03:48:39,086 Stage: Train 0.5 | Epoch: 391 | Iter: 294800 | Total Loss: 0.002411 | Recon Loss: 0.002007 | Commit Loss: 0.000808 | Perplexity: 1076.534503
2025-09-15 03:49:06,748 Stage: Train 0.5 | Epoch: 391 | Iter: 295000 | Total Loss: 0.002382 | Recon Loss: 0.001978 | Commit Loss: 0.000807 | Perplexity: 1074.874923
Trainning Epoch:  59%|█████▉    | 392/665 [11:16:47<7:51:39, 103.66s/it]2025-09-15 03:49:34,287 Stage: Train 0.5 | Epoch: 392 | Iter: 295200 | Total Loss: 0.002441 | Recon Loss: 0.002035 | Commit Loss: 0.000813 | Perplexity: 1074.728035
2025-09-15 03:50:01,805 Stage: Train 0.5 | Epoch: 392 | Iter: 295400 | Total Loss: 0.002402 | Recon Loss: 0.002000 | Commit Loss: 0.000805 | Perplexity: 1074.661687
2025-09-15 03:50:29,366 Stage: Train 0.5 | Epoch: 392 | Iter: 295600 | Total Loss: 0.002435 | Recon Loss: 0.002035 | Commit Loss: 0.000800 | Perplexity: 1072.831716
2025-09-15 03:50:56,888 Stage: Train 0.5 | Epoch: 392 | Iter: 295800 | Total Loss: 0.002350 | Recon Loss: 0.001943 | Commit Loss: 0.000815 | Perplexity: 1082.590554
Trainning Epoch:  59%|█████▉    | 393/665 [11:18:30<7:49:51, 103.65s/it]2025-09-15 03:51:24,365 Stage: Train 0.5 | Epoch: 393 | Iter: 296000 | Total Loss: 0.002403 | Recon Loss: 0.001998 | Commit Loss: 0.000809 | Perplexity: 1073.613157
2025-09-15 03:51:51,871 Stage: Train 0.5 | Epoch: 393 | Iter: 296200 | Total Loss: 0.002373 | Recon Loss: 0.001973 | Commit Loss: 0.000800 | Perplexity: 1076.574565
2025-09-15 03:52:19,527 Stage: Train 0.5 | Epoch: 393 | Iter: 296400 | Total Loss: 0.002364 | Recon Loss: 0.001961 | Commit Loss: 0.000806 | Perplexity: 1076.971017
2025-09-15 03:52:47,184 Stage: Train 0.5 | Epoch: 393 | Iter: 296600 | Total Loss: 0.002427 | Recon Loss: 0.002022 | Commit Loss: 0.000809 | Perplexity: 1077.607346
Trainning Epoch:  59%|█████▉    | 394/665 [11:20:14<7:48:24, 103.71s/it]2025-09-15 03:53:14,733 Stage: Train 0.5 | Epoch: 394 | Iter: 296800 | Total Loss: 0.002354 | Recon Loss: 0.001955 | Commit Loss: 0.000800 | Perplexity: 1072.581061
2025-09-15 03:53:42,225 Stage: Train 0.5 | Epoch: 394 | Iter: 297000 | Total Loss: 0.002349 | Recon Loss: 0.001947 | Commit Loss: 0.000804 | Perplexity: 1079.448316
2025-09-15 03:54:09,668 Stage: Train 0.5 | Epoch: 394 | Iter: 297200 | Total Loss: 0.002451 | Recon Loss: 0.002045 | Commit Loss: 0.000813 | Perplexity: 1076.410865
2025-09-15 03:54:37,199 Stage: Train 0.5 | Epoch: 394 | Iter: 297400 | Total Loss: 0.002406 | Recon Loss: 0.002004 | Commit Loss: 0.000803 | Perplexity: 1077.367560
Trainning Epoch:  59%|█████▉    | 395/665 [11:21:58<7:46:28, 103.66s/it]2025-09-15 03:55:04,850 Stage: Train 0.5 | Epoch: 395 | Iter: 297600 | Total Loss: 0.002385 | Recon Loss: 0.001982 | Commit Loss: 0.000805 | Perplexity: 1079.022050
2025-09-15 03:55:32,550 Stage: Train 0.5 | Epoch: 395 | Iter: 297800 | Total Loss: 0.002340 | Recon Loss: 0.001939 | Commit Loss: 0.000802 | Perplexity: 1072.332452
2025-09-15 03:56:00,290 Stage: Train 0.5 | Epoch: 395 | Iter: 298000 | Total Loss: 0.002382 | Recon Loss: 0.001980 | Commit Loss: 0.000804 | Perplexity: 1075.474188
Trainning Epoch:  60%|█████▉    | 396/665 [11:23:42<7:45:22, 103.80s/it]2025-09-15 03:56:27,787 Stage: Train 0.5 | Epoch: 396 | Iter: 298200 | Total Loss: 0.002410 | Recon Loss: 0.002006 | Commit Loss: 0.000807 | Perplexity: 1073.457745
2025-09-15 03:56:55,241 Stage: Train 0.5 | Epoch: 396 | Iter: 298400 | Total Loss: 0.002354 | Recon Loss: 0.001955 | Commit Loss: 0.000799 | Perplexity: 1077.809284
2025-09-15 03:57:22,631 Stage: Train 0.5 | Epoch: 396 | Iter: 298600 | Total Loss: 0.002455 | Recon Loss: 0.002052 | Commit Loss: 0.000807 | Perplexity: 1079.548416
2025-09-15 03:57:50,023 Stage: Train 0.5 | Epoch: 396 | Iter: 298800 | Total Loss: 0.002347 | Recon Loss: 0.001945 | Commit Loss: 0.000804 | Perplexity: 1076.546218
Trainning Epoch:  60%|█████▉    | 397/665 [11:25:25<7:42:47, 103.61s/it]2025-09-15 03:58:17,378 Stage: Train 0.5 | Epoch: 397 | Iter: 299000 | Total Loss: 0.002394 | Recon Loss: 0.001990 | Commit Loss: 0.000810 | Perplexity: 1074.241946
2025-09-15 03:58:44,759 Stage: Train 0.5 | Epoch: 397 | Iter: 299200 | Total Loss: 0.002373 | Recon Loss: 0.001973 | Commit Loss: 0.000801 | Perplexity: 1075.731722
2025-09-15 03:59:12,351 Stage: Train 0.5 | Epoch: 397 | Iter: 299400 | Total Loss: 0.002359 | Recon Loss: 0.001957 | Commit Loss: 0.000806 | Perplexity: 1075.254644
2025-09-15 03:59:39,840 Stage: Train 0.5 | Epoch: 397 | Iter: 299600 | Total Loss: 0.002387 | Recon Loss: 0.001983 | Commit Loss: 0.000808 | Perplexity: 1077.326660
Trainning Epoch:  60%|█████▉    | 398/665 [11:27:08<7:40:44, 103.54s/it]2025-09-15 04:00:07,400 Stage: Train 0.5 | Epoch: 398 | Iter: 299800 | Total Loss: 0.002426 | Recon Loss: 0.002022 | Commit Loss: 0.000807 | Perplexity: 1078.169153
2025-09-15 04:00:34,891 Stage: Train 0.5 | Epoch: 398 | Iter: 300000 | Total Loss: 0.002343 | Recon Loss: 0.001941 | Commit Loss: 0.000804 | Perplexity: 1076.951336
2025-09-15 04:00:34,891 Saving model at iteration 300000
2025-09-15 04:00:35,063 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_399_step_300000
2025-09-15 04:00:35,279 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_399_step_300000/pytorch_model.bin
2025-09-15 04:00:35,612 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_399_step_300000/optimizer.bin
2025-09-15 04:00:35,612 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_399_step_300000/scheduler.bin
2025-09-15 04:00:35,613 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_399_step_300000/random_states_0.pkl
2025-09-15 04:01:03,186 Stage: Train 0.5 | Epoch: 398 | Iter: 300200 | Total Loss: 0.002343 | Recon Loss: 0.001944 | Commit Loss: 0.000797 | Perplexity: 1070.268988
2025-09-15 04:01:30,707 Stage: Train 0.5 | Epoch: 398 | Iter: 300400 | Total Loss: 0.002400 | Recon Loss: 0.001996 | Commit Loss: 0.000809 | Perplexity: 1081.353826
Trainning Epoch:  60%|██████    | 399/665 [11:28:53<7:40:16, 103.82s/it]2025-09-15 04:01:58,194 Stage: Train 0.5 | Epoch: 399 | Iter: 300600 | Total Loss: 0.002361 | Recon Loss: 0.001958 | Commit Loss: 0.000806 | Perplexity: 1079.126359
2025-09-15 04:02:25,695 Stage: Train 0.5 | Epoch: 399 | Iter: 300800 | Total Loss: 0.002333 | Recon Loss: 0.001932 | Commit Loss: 0.000801 | Perplexity: 1074.864653
2025-09-15 04:02:53,186 Stage: Train 0.5 | Epoch: 399 | Iter: 301000 | Total Loss: 0.002364 | Recon Loss: 0.001958 | Commit Loss: 0.000813 | Perplexity: 1078.037475
2025-09-15 04:03:20,622 Stage: Train 0.5 | Epoch: 399 | Iter: 301200 | Total Loss: 0.002430 | Recon Loss: 0.002024 | Commit Loss: 0.000811 | Perplexity: 1073.546428
Trainning Epoch:  60%|██████    | 400/665 [11:30:36<7:38:04, 103.72s/it]2025-09-15 04:03:48,039 Stage: Train 0.5 | Epoch: 400 | Iter: 301400 | Total Loss: 0.002351 | Recon Loss: 0.001950 | Commit Loss: 0.000801 | Perplexity: 1076.835213
2025-09-15 04:04:15,572 Stage: Train 0.5 | Epoch: 400 | Iter: 301600 | Total Loss: 0.002336 | Recon Loss: 0.001935 | Commit Loss: 0.000803 | Perplexity: 1074.719862
2025-09-15 04:04:43,011 Stage: Train 0.5 | Epoch: 400 | Iter: 301800 | Total Loss: 0.002422 | Recon Loss: 0.002018 | Commit Loss: 0.000809 | Perplexity: 1078.458846
Trainning Epoch:  60%|██████    | 401/665 [11:32:20<7:35:51, 103.60s/it]2025-09-15 04:05:10,491 Stage: Train 0.5 | Epoch: 401 | Iter: 302000 | Total Loss: 0.002383 | Recon Loss: 0.001981 | Commit Loss: 0.000804 | Perplexity: 1075.714876
2025-09-15 04:05:38,115 Stage: Train 0.5 | Epoch: 401 | Iter: 302200 | Total Loss: 0.002322 | Recon Loss: 0.001923 | Commit Loss: 0.000797 | Perplexity: 1078.315356
2025-09-15 04:06:05,557 Stage: Train 0.5 | Epoch: 401 | Iter: 302400 | Total Loss: 0.002438 | Recon Loss: 0.002036 | Commit Loss: 0.000804 | Perplexity: 1076.069342
2025-09-15 04:06:32,969 Stage: Train 0.5 | Epoch: 401 | Iter: 302600 | Total Loss: 0.002453 | Recon Loss: 0.002050 | Commit Loss: 0.000806 | Perplexity: 1076.184236
Trainning Epoch:  60%|██████    | 402/665 [11:34:03<7:33:59, 103.57s/it]2025-09-15 04:07:00,370 Stage: Train 0.5 | Epoch: 402 | Iter: 302800 | Total Loss: 0.002368 | Recon Loss: 0.001963 | Commit Loss: 0.000809 | Perplexity: 1073.743909
2025-09-15 04:07:27,923 Stage: Train 0.5 | Epoch: 402 | Iter: 303000 | Total Loss: 0.002350 | Recon Loss: 0.001950 | Commit Loss: 0.000801 | Perplexity: 1076.486805
2025-09-15 04:07:55,433 Stage: Train 0.5 | Epoch: 402 | Iter: 303200 | Total Loss: 0.002375 | Recon Loss: 0.001973 | Commit Loss: 0.000803 | Perplexity: 1077.285151
2025-09-15 04:08:22,882 Stage: Train 0.5 | Epoch: 402 | Iter: 303400 | Total Loss: 0.002363 | Recon Loss: 0.001962 | Commit Loss: 0.000802 | Perplexity: 1075.423912
Trainning Epoch:  61%|██████    | 403/665 [11:35:46<7:32:08, 103.54s/it]2025-09-15 04:08:50,387 Stage: Train 0.5 | Epoch: 403 | Iter: 303600 | Total Loss: 0.002327 | Recon Loss: 0.001929 | Commit Loss: 0.000798 | Perplexity: 1070.660736
2025-09-15 04:09:17,882 Stage: Train 0.5 | Epoch: 403 | Iter: 303800 | Total Loss: 0.002395 | Recon Loss: 0.001992 | Commit Loss: 0.000805 | Perplexity: 1071.821080
2025-09-15 04:09:45,337 Stage: Train 0.5 | Epoch: 403 | Iter: 304000 | Total Loss: 0.002412 | Recon Loss: 0.002011 | Commit Loss: 0.000803 | Perplexity: 1074.812842
2025-09-15 04:10:12,815 Stage: Train 0.5 | Epoch: 403 | Iter: 304200 | Total Loss: 0.002394 | Recon Loss: 0.001989 | Commit Loss: 0.000810 | Perplexity: 1083.837549
Trainning Epoch:  61%|██████    | 404/665 [11:37:30<7:30:19, 103.52s/it]2025-09-15 04:10:40,376 Stage: Train 0.5 | Epoch: 404 | Iter: 304400 | Total Loss: 0.002366 | Recon Loss: 0.001965 | Commit Loss: 0.000801 | Perplexity: 1074.235700
2025-09-15 04:11:08,035 Stage: Train 0.5 | Epoch: 404 | Iter: 304600 | Total Loss: 0.002373 | Recon Loss: 0.001971 | Commit Loss: 0.000804 | Perplexity: 1080.043388
2025-09-15 04:11:35,729 Stage: Train 0.5 | Epoch: 404 | Iter: 304800 | Total Loss: 0.002454 | Recon Loss: 0.002056 | Commit Loss: 0.000797 | Perplexity: 1075.629937
Trainning Epoch:  61%|██████    | 405/665 [11:39:14<7:29:14, 103.67s/it]2025-09-15 04:12:03,251 Stage: Train 0.5 | Epoch: 405 | Iter: 305000 | Total Loss: 0.002331 | Recon Loss: 0.001927 | Commit Loss: 0.000806 | Perplexity: 1076.986481
2025-09-15 04:12:31,088 Stage: Train 0.5 | Epoch: 405 | Iter: 305200 | Total Loss: 0.002358 | Recon Loss: 0.001956 | Commit Loss: 0.000805 | Perplexity: 1078.791389
2025-09-15 04:12:58,690 Stage: Train 0.5 | Epoch: 405 | Iter: 305400 | Total Loss: 0.002391 | Recon Loss: 0.001991 | Commit Loss: 0.000800 | Perplexity: 1071.854099
2025-09-15 04:13:26,310 Stage: Train 0.5 | Epoch: 405 | Iter: 305600 | Total Loss: 0.002333 | Recon Loss: 0.001932 | Commit Loss: 0.000801 | Perplexity: 1075.671871
Trainning Epoch:  61%|██████    | 406/665 [11:40:58<7:28:04, 103.80s/it]2025-09-15 04:13:53,930 Stage: Train 0.5 | Epoch: 406 | Iter: 305800 | Total Loss: 0.002351 | Recon Loss: 0.001948 | Commit Loss: 0.000805 | Perplexity: 1078.322258
2025-09-15 04:14:21,632 Stage: Train 0.5 | Epoch: 406 | Iter: 306000 | Total Loss: 0.002421 | Recon Loss: 0.002023 | Commit Loss: 0.000796 | Perplexity: 1072.500318
2025-09-15 04:14:49,329 Stage: Train 0.5 | Epoch: 406 | Iter: 306200 | Total Loss: 0.002347 | Recon Loss: 0.001944 | Commit Loss: 0.000806 | Perplexity: 1079.159215
2025-09-15 04:15:16,926 Stage: Train 0.5 | Epoch: 406 | Iter: 306400 | Total Loss: 0.002407 | Recon Loss: 0.002006 | Commit Loss: 0.000802 | Perplexity: 1079.252635
Trainning Epoch:  61%|██████    | 407/665 [11:42:42<7:26:47, 103.91s/it]2025-09-15 04:15:44,406 Stage: Train 0.5 | Epoch: 407 | Iter: 306600 | Total Loss: 0.002418 | Recon Loss: 0.002020 | Commit Loss: 0.000795 | Perplexity: 1070.519367
2025-09-15 04:16:11,866 Stage: Train 0.5 | Epoch: 407 | Iter: 306800 | Total Loss: 0.002300 | Recon Loss: 0.001898 | Commit Loss: 0.000803 | Perplexity: 1079.004312
2025-09-15 04:16:39,300 Stage: Train 0.5 | Epoch: 407 | Iter: 307000 | Total Loss: 0.002389 | Recon Loss: 0.001987 | Commit Loss: 0.000802 | Perplexity: 1077.470042
2025-09-15 04:17:06,735 Stage: Train 0.5 | Epoch: 407 | Iter: 307200 | Total Loss: 0.002347 | Recon Loss: 0.001945 | Commit Loss: 0.000806 | Perplexity: 1077.485039
Trainning Epoch:  61%|██████▏   | 408/665 [11:44:26<7:24:16, 103.72s/it]2025-09-15 04:17:34,129 Stage: Train 0.5 | Epoch: 408 | Iter: 307400 | Total Loss: 0.002362 | Recon Loss: 0.001962 | Commit Loss: 0.000799 | Perplexity: 1074.292982
2025-09-15 04:18:01,676 Stage: Train 0.5 | Epoch: 408 | Iter: 307600 | Total Loss: 0.002347 | Recon Loss: 0.001946 | Commit Loss: 0.000801 | Perplexity: 1074.596253
2025-09-15 04:18:29,113 Stage: Train 0.5 | Epoch: 408 | Iter: 307800 | Total Loss: 0.002381 | Recon Loss: 0.001979 | Commit Loss: 0.000804 | Perplexity: 1077.404046
Trainning Epoch:  62%|██████▏   | 409/665 [11:46:09<7:22:05, 103.62s/it]2025-09-15 04:18:56,516 Stage: Train 0.5 | Epoch: 409 | Iter: 308000 | Total Loss: 0.002374 | Recon Loss: 0.001973 | Commit Loss: 0.000803 | Perplexity: 1077.829820
2025-09-15 04:19:23,943 Stage: Train 0.5 | Epoch: 409 | Iter: 308200 | Total Loss: 0.002341 | Recon Loss: 0.001943 | Commit Loss: 0.000797 | Perplexity: 1075.387752
2025-09-15 04:19:51,328 Stage: Train 0.5 | Epoch: 409 | Iter: 308400 | Total Loss: 0.002386 | Recon Loss: 0.001985 | Commit Loss: 0.000802 | Perplexity: 1076.610653
2025-09-15 04:20:18,719 Stage: Train 0.5 | Epoch: 409 | Iter: 308600 | Total Loss: 0.002419 | Recon Loss: 0.002020 | Commit Loss: 0.000798 | Perplexity: 1069.256548
Trainning Epoch:  62%|██████▏   | 410/665 [11:47:52<7:19:43, 103.47s/it]2025-09-15 04:20:46,048 Stage: Train 0.5 | Epoch: 410 | Iter: 308800 | Total Loss: 0.002373 | Recon Loss: 0.001971 | Commit Loss: 0.000804 | Perplexity: 1079.939915
2025-09-15 04:21:13,537 Stage: Train 0.5 | Epoch: 410 | Iter: 309000 | Total Loss: 0.002385 | Recon Loss: 0.001984 | Commit Loss: 0.000803 | Perplexity: 1077.571337
2025-09-15 04:21:40,913 Stage: Train 0.5 | Epoch: 410 | Iter: 309200 | Total Loss: 0.002323 | Recon Loss: 0.001920 | Commit Loss: 0.000806 | Perplexity: 1078.778917
2025-09-15 04:22:08,362 Stage: Train 0.5 | Epoch: 410 | Iter: 309400 | Total Loss: 0.002420 | Recon Loss: 0.002023 | Commit Loss: 0.000793 | Perplexity: 1072.942680
Trainning Epoch:  62%|██████▏   | 411/665 [11:49:35<7:17:43, 103.40s/it]2025-09-15 04:22:35,798 Stage: Train 0.5 | Epoch: 411 | Iter: 309600 | Total Loss: 0.002284 | Recon Loss: 0.001884 | Commit Loss: 0.000802 | Perplexity: 1079.027555
2025-09-15 04:23:03,284 Stage: Train 0.5 | Epoch: 411 | Iter: 309800 | Total Loss: 0.002351 | Recon Loss: 0.001952 | Commit Loss: 0.000798 | Perplexity: 1078.471542
2025-09-15 04:23:30,784 Stage: Train 0.5 | Epoch: 411 | Iter: 310000 | Total Loss: 0.002373 | Recon Loss: 0.001971 | Commit Loss: 0.000803 | Perplexity: 1077.407709
2025-09-15 04:23:58,270 Stage: Train 0.5 | Epoch: 411 | Iter: 310200 | Total Loss: 0.002296 | Recon Loss: 0.001895 | Commit Loss: 0.000802 | Perplexity: 1075.148372
Trainning Epoch:  62%|██████▏   | 412/665 [11:51:19<7:16:04, 103.42s/it]2025-09-15 04:24:25,723 Stage: Train 0.5 | Epoch: 412 | Iter: 310400 | Total Loss: 0.002368 | Recon Loss: 0.001967 | Commit Loss: 0.000800 | Perplexity: 1077.924160
2025-09-15 04:24:53,482 Stage: Train 0.5 | Epoch: 412 | Iter: 310600 | Total Loss: 0.002335 | Recon Loss: 0.001934 | Commit Loss: 0.000801 | Perplexity: 1077.140557
2025-09-15 04:25:21,144 Stage: Train 0.5 | Epoch: 412 | Iter: 310800 | Total Loss: 0.002390 | Recon Loss: 0.001988 | Commit Loss: 0.000804 | Perplexity: 1077.592404
Trainning Epoch:  62%|██████▏   | 413/665 [11:53:03<7:15:08, 103.60s/it]2025-09-15 04:25:48,732 Stage: Train 0.5 | Epoch: 413 | Iter: 311000 | Total Loss: 0.002338 | Recon Loss: 0.001940 | Commit Loss: 0.000796 | Perplexity: 1074.344680
2025-09-15 04:26:16,512 Stage: Train 0.5 | Epoch: 413 | Iter: 311200 | Total Loss: 0.002357 | Recon Loss: 0.001959 | Commit Loss: 0.000795 | Perplexity: 1076.724563
2025-09-15 04:26:44,146 Stage: Train 0.5 | Epoch: 413 | Iter: 311400 | Total Loss: 0.002345 | Recon Loss: 0.001942 | Commit Loss: 0.000806 | Perplexity: 1080.668369
2025-09-15 04:27:11,804 Stage: Train 0.5 | Epoch: 413 | Iter: 311600 | Total Loss: 0.002464 | Recon Loss: 0.002066 | Commit Loss: 0.000796 | Perplexity: 1075.342435
Trainning Epoch:  62%|██████▏   | 414/665 [11:54:47<7:14:08, 103.78s/it]2025-09-15 04:27:39,462 Stage: Train 0.5 | Epoch: 414 | Iter: 311800 | Total Loss: 0.002334 | Recon Loss: 0.001934 | Commit Loss: 0.000801 | Perplexity: 1075.534534
2025-09-15 04:28:07,426 Stage: Train 0.5 | Epoch: 414 | Iter: 312000 | Total Loss: 0.002335 | Recon Loss: 0.001937 | Commit Loss: 0.000797 | Perplexity: 1079.408433
2025-09-15 04:28:35,166 Stage: Train 0.5 | Epoch: 414 | Iter: 312200 | Total Loss: 0.002380 | Recon Loss: 0.001979 | Commit Loss: 0.000802 | Perplexity: 1074.993706
2025-09-15 04:29:02,808 Stage: Train 0.5 | Epoch: 414 | Iter: 312400 | Total Loss: 0.002349 | Recon Loss: 0.001951 | Commit Loss: 0.000796 | Perplexity: 1071.765214
Trainning Epoch:  62%|██████▏   | 415/665 [11:56:31<7:13:22, 104.01s/it]2025-09-15 04:29:30,361 Stage: Train 0.5 | Epoch: 415 | Iter: 312600 | Total Loss: 0.002353 | Recon Loss: 0.001954 | Commit Loss: 0.000799 | Perplexity: 1076.953598
2025-09-15 04:29:57,824 Stage: Train 0.5 | Epoch: 415 | Iter: 312800 | Total Loss: 0.002388 | Recon Loss: 0.001991 | Commit Loss: 0.000793 | Perplexity: 1074.882559
2025-09-15 04:30:25,293 Stage: Train 0.5 | Epoch: 415 | Iter: 313000 | Total Loss: 0.002304 | Recon Loss: 0.001905 | Commit Loss: 0.000798 | Perplexity: 1076.454909
2025-09-15 04:30:52,731 Stage: Train 0.5 | Epoch: 415 | Iter: 313200 | Total Loss: 0.002428 | Recon Loss: 0.002025 | Commit Loss: 0.000806 | Perplexity: 1078.832747
Trainning Epoch:  63%|██████▎   | 416/665 [11:58:15<7:10:46, 103.80s/it]2025-09-15 04:31:20,199 Stage: Train 0.5 | Epoch: 416 | Iter: 313400 | Total Loss: 0.002336 | Recon Loss: 0.001935 | Commit Loss: 0.000803 | Perplexity: 1078.368643
2025-09-15 04:31:47,702 Stage: Train 0.5 | Epoch: 416 | Iter: 313600 | Total Loss: 0.002321 | Recon Loss: 0.001922 | Commit Loss: 0.000798 | Perplexity: 1078.147062
2025-09-15 04:32:15,171 Stage: Train 0.5 | Epoch: 416 | Iter: 313800 | Total Loss: 0.002354 | Recon Loss: 0.001956 | Commit Loss: 0.000795 | Perplexity: 1076.471524
2025-09-15 04:32:42,605 Stage: Train 0.5 | Epoch: 416 | Iter: 314000 | Total Loss: 0.002384 | Recon Loss: 0.001982 | Commit Loss: 0.000803 | Perplexity: 1075.964429
Trainning Epoch:  63%|██████▎   | 417/665 [11:59:58<7:08:35, 103.69s/it]2025-09-15 04:33:10,034 Stage: Train 0.5 | Epoch: 417 | Iter: 314200 | Total Loss: 0.002371 | Recon Loss: 0.001974 | Commit Loss: 0.000793 | Perplexity: 1072.595429
2025-09-15 04:33:37,514 Stage: Train 0.5 | Epoch: 417 | Iter: 314400 | Total Loss: 0.002340 | Recon Loss: 0.001940 | Commit Loss: 0.000800 | Perplexity: 1076.193384
2025-09-15 04:34:05,007 Stage: Train 0.5 | Epoch: 417 | Iter: 314600 | Total Loss: 0.002324 | Recon Loss: 0.001923 | Commit Loss: 0.000802 | Perplexity: 1077.296453
Trainning Epoch:  63%|██████▎   | 418/665 [12:01:42<7:06:34, 103.62s/it]2025-09-15 04:34:32,474 Stage: Train 0.5 | Epoch: 418 | Iter: 314800 | Total Loss: 0.002381 | Recon Loss: 0.001980 | Commit Loss: 0.000801 | Perplexity: 1077.704280
2025-09-15 04:35:00,146 Stage: Train 0.5 | Epoch: 418 | Iter: 315000 | Total Loss: 0.002366 | Recon Loss: 0.001967 | Commit Loss: 0.000797 | Perplexity: 1074.047781
2025-09-15 04:35:27,718 Stage: Train 0.5 | Epoch: 418 | Iter: 315200 | Total Loss: 0.002341 | Recon Loss: 0.001942 | Commit Loss: 0.000798 | Perplexity: 1077.697210
2025-09-15 04:35:55,265 Stage: Train 0.5 | Epoch: 418 | Iter: 315400 | Total Loss: 0.002363 | Recon Loss: 0.001963 | Commit Loss: 0.000800 | Perplexity: 1075.211779
Trainning Epoch:  63%|██████▎   | 419/665 [12:03:25<7:05:02, 103.67s/it]2025-09-15 04:36:22,697 Stage: Train 0.5 | Epoch: 419 | Iter: 315600 | Total Loss: 0.002404 | Recon Loss: 0.002006 | Commit Loss: 0.000797 | Perplexity: 1077.229209
2025-09-15 04:36:50,145 Stage: Train 0.5 | Epoch: 419 | Iter: 315800 | Total Loss: 0.002306 | Recon Loss: 0.001906 | Commit Loss: 0.000799 | Perplexity: 1082.436992
2025-09-15 04:37:17,598 Stage: Train 0.5 | Epoch: 419 | Iter: 316000 | Total Loss: 0.002380 | Recon Loss: 0.001981 | Commit Loss: 0.000799 | Perplexity: 1075.200235
2025-09-15 04:37:45,031 Stage: Train 0.5 | Epoch: 419 | Iter: 316200 | Total Loss: 0.002339 | Recon Loss: 0.001941 | Commit Loss: 0.000795 | Perplexity: 1072.437781
Trainning Epoch:  63%|██████▎   | 420/665 [12:05:09<7:02:50, 103.55s/it]2025-09-15 04:38:12,555 Stage: Train 0.5 | Epoch: 420 | Iter: 316400 | Total Loss: 0.002318 | Recon Loss: 0.001921 | Commit Loss: 0.000794 | Perplexity: 1072.894240
2025-09-15 04:38:39,956 Stage: Train 0.5 | Epoch: 420 | Iter: 316600 | Total Loss: 0.002396 | Recon Loss: 0.001996 | Commit Loss: 0.000800 | Perplexity: 1078.695764
2025-09-15 04:39:07,518 Stage: Train 0.5 | Epoch: 420 | Iter: 316800 | Total Loss: 0.002312 | Recon Loss: 0.001910 | Commit Loss: 0.000803 | Perplexity: 1077.330707
2025-09-15 04:39:35,075 Stage: Train 0.5 | Epoch: 420 | Iter: 317000 | Total Loss: 0.002327 | Recon Loss: 0.001927 | Commit Loss: 0.000799 | Perplexity: 1081.446573
Trainning Epoch:  63%|██████▎   | 421/665 [12:06:52<7:01:10, 103.57s/it]2025-09-15 04:40:02,539 Stage: Train 0.5 | Epoch: 421 | Iter: 317200 | Total Loss: 0.002374 | Recon Loss: 0.001977 | Commit Loss: 0.000794 | Perplexity: 1071.357073
2025-09-15 04:40:30,016 Stage: Train 0.5 | Epoch: 421 | Iter: 317400 | Total Loss: 0.002330 | Recon Loss: 0.001933 | Commit Loss: 0.000794 | Perplexity: 1075.020938
2025-09-15 04:40:57,489 Stage: Train 0.5 | Epoch: 421 | Iter: 317600 | Total Loss: 0.002343 | Recon Loss: 0.001945 | Commit Loss: 0.000795 | Perplexity: 1078.257482
Trainning Epoch:  63%|██████▎   | 422/665 [12:08:36<6:59:16, 103.53s/it]2025-09-15 04:41:24,921 Stage: Train 0.5 | Epoch: 422 | Iter: 317800 | Total Loss: 0.002310 | Recon Loss: 0.001907 | Commit Loss: 0.000806 | Perplexity: 1081.255191
2025-09-15 04:41:52,501 Stage: Train 0.5 | Epoch: 422 | Iter: 318000 | Total Loss: 0.002405 | Recon Loss: 0.002010 | Commit Loss: 0.000791 | Perplexity: 1075.688138
2025-09-15 04:42:19,909 Stage: Train 0.5 | Epoch: 422 | Iter: 318200 | Total Loss: 0.002326 | Recon Loss: 0.001931 | Commit Loss: 0.000790 | Perplexity: 1071.446117
2025-09-15 04:42:47,306 Stage: Train 0.5 | Epoch: 422 | Iter: 318400 | Total Loss: 0.002344 | Recon Loss: 0.001940 | Commit Loss: 0.000809 | Perplexity: 1081.966342
Trainning Epoch:  64%|██████▎   | 423/665 [12:10:19<6:57:18, 103.46s/it]2025-09-15 04:43:14,666 Stage: Train 0.5 | Epoch: 423 | Iter: 318600 | Total Loss: 0.002444 | Recon Loss: 0.002044 | Commit Loss: 0.000799 | Perplexity: 1074.491121
2025-09-15 04:43:42,067 Stage: Train 0.5 | Epoch: 423 | Iter: 318800 | Total Loss: 0.002300 | Recon Loss: 0.001902 | Commit Loss: 0.000796 | Perplexity: 1076.395721
2025-09-15 04:44:09,474 Stage: Train 0.5 | Epoch: 423 | Iter: 319000 | Total Loss: 0.002336 | Recon Loss: 0.001938 | Commit Loss: 0.000796 | Perplexity: 1077.650265
2025-09-15 04:44:36,875 Stage: Train 0.5 | Epoch: 423 | Iter: 319200 | Total Loss: 0.002327 | Recon Loss: 0.001928 | Commit Loss: 0.000797 | Perplexity: 1073.826297
Trainning Epoch:  64%|██████▍   | 424/665 [12:12:02<6:55:09, 103.36s/it]2025-09-15 04:45:04,327 Stage: Train 0.5 | Epoch: 424 | Iter: 319400 | Total Loss: 0.002327 | Recon Loss: 0.001928 | Commit Loss: 0.000798 | Perplexity: 1076.089909
2025-09-15 04:45:31,731 Stage: Train 0.5 | Epoch: 424 | Iter: 319600 | Total Loss: 0.002348 | Recon Loss: 0.001949 | Commit Loss: 0.000797 | Perplexity: 1075.921292
2025-09-15 04:45:59,396 Stage: Train 0.5 | Epoch: 424 | Iter: 319800 | Total Loss: 0.002319 | Recon Loss: 0.001925 | Commit Loss: 0.000790 | Perplexity: 1073.657929
2025-09-15 04:46:27,050 Stage: Train 0.5 | Epoch: 424 | Iter: 320000 | Total Loss: 0.002344 | Recon Loss: 0.001940 | Commit Loss: 0.000807 | Perplexity: 1080.552256
2025-09-15 04:46:27,051 Saving model at iteration 320000
2025-09-15 04:46:27,225 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_425_step_320000
2025-09-15 04:46:27,430 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_425_step_320000/pytorch_model.bin
2025-09-15 04:46:27,763 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_425_step_320000/optimizer.bin
2025-09-15 04:46:27,764 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_425_step_320000/scheduler.bin
2025-09-15 04:46:27,764 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_425_step_320000/random_states_0.pkl
Trainning Epoch:  64%|██████▍   | 425/665 [12:13:47<6:55:16, 103.82s/it]2025-09-15 04:46:55,616 Stage: Train 0.5 | Epoch: 425 | Iter: 320200 | Total Loss: 0.002342 | Recon Loss: 0.001945 | Commit Loss: 0.000794 | Perplexity: 1074.767118
2025-09-15 04:47:23,126 Stage: Train 0.5 | Epoch: 425 | Iter: 320400 | Total Loss: 0.002291 | Recon Loss: 0.001892 | Commit Loss: 0.000800 | Perplexity: 1080.989653
2025-09-15 04:47:50,756 Stage: Train 0.5 | Epoch: 425 | Iter: 320600 | Total Loss: 0.002366 | Recon Loss: 0.001969 | Commit Loss: 0.000795 | Perplexity: 1076.536865
Trainning Epoch:  64%|██████▍   | 426/665 [12:15:31<6:53:30, 103.81s/it]2025-09-15 04:48:18,401 Stage: Train 0.5 | Epoch: 426 | Iter: 320800 | Total Loss: 0.002304 | Recon Loss: 0.001903 | Commit Loss: 0.000801 | Perplexity: 1075.786722
2025-09-15 04:48:46,032 Stage: Train 0.5 | Epoch: 426 | Iter: 321000 | Total Loss: 0.002323 | Recon Loss: 0.001928 | Commit Loss: 0.000790 | Perplexity: 1071.758635
2025-09-15 04:49:13,636 Stage: Train 0.5 | Epoch: 426 | Iter: 321200 | Total Loss: 0.002317 | Recon Loss: 0.001918 | Commit Loss: 0.000798 | Perplexity: 1080.211984
2025-09-15 04:49:41,243 Stage: Train 0.5 | Epoch: 426 | Iter: 321400 | Total Loss: 0.002404 | Recon Loss: 0.002003 | Commit Loss: 0.000801 | Perplexity: 1077.420579
Trainning Epoch:  64%|██████▍   | 427/665 [12:17:15<6:52:02, 103.87s/it]2025-09-15 04:50:08,860 Stage: Train 0.5 | Epoch: 427 | Iter: 321600 | Total Loss: 0.002332 | Recon Loss: 0.001932 | Commit Loss: 0.000800 | Perplexity: 1079.057461
2025-09-15 04:50:36,318 Stage: Train 0.5 | Epoch: 427 | Iter: 321800 | Total Loss: 0.002305 | Recon Loss: 0.001907 | Commit Loss: 0.000796 | Perplexity: 1079.079254
2025-09-15 04:51:03,789 Stage: Train 0.5 | Epoch: 427 | Iter: 322000 | Total Loss: 0.002327 | Recon Loss: 0.001932 | Commit Loss: 0.000791 | Perplexity: 1073.757339
2025-09-15 04:51:31,274 Stage: Train 0.5 | Epoch: 427 | Iter: 322200 | Total Loss: 0.002375 | Recon Loss: 0.001977 | Commit Loss: 0.000796 | Perplexity: 1073.515430
Trainning Epoch:  64%|██████▍   | 428/665 [12:18:58<6:49:42, 103.72s/it]2025-09-15 04:51:58,698 Stage: Train 0.5 | Epoch: 428 | Iter: 322400 | Total Loss: 0.002370 | Recon Loss: 0.001970 | Commit Loss: 0.000800 | Perplexity: 1077.742262
2025-09-15 04:52:26,183 Stage: Train 0.5 | Epoch: 428 | Iter: 322600 | Total Loss: 0.002345 | Recon Loss: 0.001946 | Commit Loss: 0.000798 | Perplexity: 1083.663235
2025-09-15 04:52:53,642 Stage: Train 0.5 | Epoch: 428 | Iter: 322800 | Total Loss: 0.002311 | Recon Loss: 0.001914 | Commit Loss: 0.000793 | Perplexity: 1075.933594
2025-09-15 04:53:21,204 Stage: Train 0.5 | Epoch: 428 | Iter: 323000 | Total Loss: 0.002279 | Recon Loss: 0.001882 | Commit Loss: 0.000794 | Perplexity: 1073.101880
Trainning Epoch:  65%|██████▍   | 429/665 [12:20:42<6:47:40, 103.65s/it]2025-09-15 04:53:48,599 Stage: Train 0.5 | Epoch: 429 | Iter: 323200 | Total Loss: 0.002349 | Recon Loss: 0.001954 | Commit Loss: 0.000790 | Perplexity: 1074.718796
2025-09-15 04:54:16,044 Stage: Train 0.5 | Epoch: 429 | Iter: 323400 | Total Loss: 0.002317 | Recon Loss: 0.001921 | Commit Loss: 0.000793 | Perplexity: 1075.757111
2025-09-15 04:54:43,526 Stage: Train 0.5 | Epoch: 429 | Iter: 323600 | Total Loss: 0.002286 | Recon Loss: 0.001887 | Commit Loss: 0.000799 | Perplexity: 1076.859480
Trainning Epoch:  65%|██████▍   | 430/665 [12:22:25<6:45:35, 103.56s/it]2025-09-15 04:55:10,947 Stage: Train 0.5 | Epoch: 430 | Iter: 323800 | Total Loss: 0.002379 | Recon Loss: 0.001976 | Commit Loss: 0.000807 | Perplexity: 1079.273307
2025-09-15 04:55:38,396 Stage: Train 0.5 | Epoch: 430 | Iter: 324000 | Total Loss: 0.002331 | Recon Loss: 0.001933 | Commit Loss: 0.000796 | Perplexity: 1077.702847
2025-09-15 04:56:05,859 Stage: Train 0.5 | Epoch: 430 | Iter: 324200 | Total Loss: 0.002361 | Recon Loss: 0.001962 | Commit Loss: 0.000798 | Perplexity: 1080.030528
2025-09-15 04:56:33,345 Stage: Train 0.5 | Epoch: 430 | Iter: 324400 | Total Loss: 0.002321 | Recon Loss: 0.001923 | Commit Loss: 0.000795 | Perplexity: 1078.005886
Trainning Epoch:  65%|██████▍   | 431/665 [12:24:09<6:43:46, 103.53s/it]2025-09-15 04:57:00,869 Stage: Train 0.5 | Epoch: 431 | Iter: 324600 | Total Loss: 0.002310 | Recon Loss: 0.001915 | Commit Loss: 0.000790 | Perplexity: 1073.879872
2025-09-15 04:57:28,480 Stage: Train 0.5 | Epoch: 431 | Iter: 324800 | Total Loss: 0.002369 | Recon Loss: 0.001974 | Commit Loss: 0.000789 | Perplexity: 1074.570288
2025-09-15 04:57:56,027 Stage: Train 0.5 | Epoch: 431 | Iter: 325000 | Total Loss: 0.002317 | Recon Loss: 0.001920 | Commit Loss: 0.000793 | Perplexity: 1073.634440
2025-09-15 04:58:23,629 Stage: Train 0.5 | Epoch: 431 | Iter: 325200 | Total Loss: 0.002321 | Recon Loss: 0.001922 | Commit Loss: 0.000797 | Perplexity: 1075.534020
Trainning Epoch:  65%|██████▍   | 432/665 [12:25:52<6:42:21, 103.61s/it]2025-09-15 04:58:51,156 Stage: Train 0.5 | Epoch: 432 | Iter: 325400 | Total Loss: 0.002340 | Recon Loss: 0.001941 | Commit Loss: 0.000799 | Perplexity: 1078.347355
2025-09-15 04:59:18,670 Stage: Train 0.5 | Epoch: 432 | Iter: 325600 | Total Loss: 0.002378 | Recon Loss: 0.001983 | Commit Loss: 0.000792 | Perplexity: 1076.851213
2025-09-15 04:59:46,139 Stage: Train 0.5 | Epoch: 432 | Iter: 325800 | Total Loss: 0.002318 | Recon Loss: 0.001920 | Commit Loss: 0.000796 | Perplexity: 1077.743882
2025-09-15 05:00:13,729 Stage: Train 0.5 | Epoch: 432 | Iter: 326000 | Total Loss: 0.002321 | Recon Loss: 0.001922 | Commit Loss: 0.000798 | Perplexity: 1076.302513
Trainning Epoch:  65%|██████▌   | 433/665 [12:27:36<6:40:34, 103.60s/it]2025-09-15 05:00:41,140 Stage: Train 0.5 | Epoch: 433 | Iter: 326200 | Total Loss: 0.002350 | Recon Loss: 0.001955 | Commit Loss: 0.000790 | Perplexity: 1076.599034
2025-09-15 05:01:08,626 Stage: Train 0.5 | Epoch: 433 | Iter: 326400 | Total Loss: 0.002337 | Recon Loss: 0.001937 | Commit Loss: 0.000801 | Perplexity: 1078.809412
2025-09-15 05:01:36,112 Stage: Train 0.5 | Epoch: 433 | Iter: 326600 | Total Loss: 0.002318 | Recon Loss: 0.001921 | Commit Loss: 0.000793 | Perplexity: 1075.801674
2025-09-15 05:02:03,634 Stage: Train 0.5 | Epoch: 433 | Iter: 326800 | Total Loss: 0.002284 | Recon Loss: 0.001887 | Commit Loss: 0.000794 | Perplexity: 1076.689856
Trainning Epoch:  65%|██████▌   | 434/665 [12:29:19<6:38:41, 103.55s/it]2025-09-15 05:02:31,146 Stage: Train 0.5 | Epoch: 434 | Iter: 327000 | Total Loss: 0.002353 | Recon Loss: 0.001954 | Commit Loss: 0.000796 | Perplexity: 1078.423006
2025-09-15 05:02:58,848 Stage: Train 0.5 | Epoch: 434 | Iter: 327200 | Total Loss: 0.002319 | Recon Loss: 0.001924 | Commit Loss: 0.000790 | Perplexity: 1078.052211
2025-09-15 05:03:26,433 Stage: Train 0.5 | Epoch: 434 | Iter: 327400 | Total Loss: 0.002398 | Recon Loss: 0.002001 | Commit Loss: 0.000796 | Perplexity: 1074.915237
Trainning Epoch:  65%|██████▌   | 435/665 [12:31:03<6:37:22, 103.66s/it]2025-09-15 05:03:53,943 Stage: Train 0.5 | Epoch: 435 | Iter: 327600 | Total Loss: 0.002283 | Recon Loss: 0.001888 | Commit Loss: 0.000790 | Perplexity: 1075.246406
2025-09-15 05:04:21,397 Stage: Train 0.5 | Epoch: 435 | Iter: 327800 | Total Loss: 0.002369 | Recon Loss: 0.001971 | Commit Loss: 0.000796 | Perplexity: 1079.514858
2025-09-15 05:04:49,019 Stage: Train 0.5 | Epoch: 435 | Iter: 328000 | Total Loss: 0.002297 | Recon Loss: 0.001901 | Commit Loss: 0.000792 | Perplexity: 1080.728209
2025-09-15 05:05:16,477 Stage: Train 0.5 | Epoch: 435 | Iter: 328200 | Total Loss: 0.002375 | Recon Loss: 0.001979 | Commit Loss: 0.000792 | Perplexity: 1075.058117
Trainning Epoch:  66%|██████▌   | 436/665 [12:32:47<6:35:24, 103.60s/it]2025-09-15 05:05:43,824 Stage: Train 0.5 | Epoch: 436 | Iter: 328400 | Total Loss: 0.002322 | Recon Loss: 0.001927 | Commit Loss: 0.000790 | Perplexity: 1069.477789
2025-09-15 05:06:11,236 Stage: Train 0.5 | Epoch: 436 | Iter: 328600 | Total Loss: 0.002316 | Recon Loss: 0.001922 | Commit Loss: 0.000788 | Perplexity: 1073.208556
2025-09-15 05:06:38,641 Stage: Train 0.5 | Epoch: 436 | Iter: 328800 | Total Loss: 0.002366 | Recon Loss: 0.001970 | Commit Loss: 0.000791 | Perplexity: 1077.151380
2025-09-15 05:07:06,093 Stage: Train 0.5 | Epoch: 436 | Iter: 329000 | Total Loss: 0.002321 | Recon Loss: 0.001921 | Commit Loss: 0.000798 | Perplexity: 1081.488549
Trainning Epoch:  66%|██████▌   | 437/665 [12:34:30<6:33:09, 103.46s/it]2025-09-15 05:07:33,352 Stage: Train 0.5 | Epoch: 437 | Iter: 329200 | Total Loss: 0.002371 | Recon Loss: 0.001974 | Commit Loss: 0.000794 | Perplexity: 1075.928959
2025-09-15 05:08:00,682 Stage: Train 0.5 | Epoch: 437 | Iter: 329400 | Total Loss: 0.002319 | Recon Loss: 0.001921 | Commit Loss: 0.000796 | Perplexity: 1076.644661
2025-09-15 05:08:28,019 Stage: Train 0.5 | Epoch: 437 | Iter: 329600 | Total Loss: 0.002310 | Recon Loss: 0.001914 | Commit Loss: 0.000792 | Perplexity: 1078.482379
2025-09-15 05:08:55,430 Stage: Train 0.5 | Epoch: 437 | Iter: 329800 | Total Loss: 0.002373 | Recon Loss: 0.001975 | Commit Loss: 0.000796 | Perplexity: 1078.568080
Trainning Epoch:  66%|██████▌   | 438/665 [12:36:13<6:30:49, 103.30s/it]2025-09-15 05:09:22,707 Stage: Train 0.5 | Epoch: 438 | Iter: 330000 | Total Loss: 0.002300 | Recon Loss: 0.001904 | Commit Loss: 0.000791 | Perplexity: 1077.923310
2025-09-15 05:09:50,058 Stage: Train 0.5 | Epoch: 438 | Iter: 330200 | Total Loss: 0.002391 | Recon Loss: 0.001994 | Commit Loss: 0.000793 | Perplexity: 1076.706462
2025-09-15 05:10:17,445 Stage: Train 0.5 | Epoch: 438 | Iter: 330400 | Total Loss: 0.002275 | Recon Loss: 0.001879 | Commit Loss: 0.000791 | Perplexity: 1075.322325
Trainning Epoch:  66%|██████▌   | 439/665 [12:37:56<6:28:57, 103.26s/it]2025-09-15 05:10:45,015 Stage: Train 0.5 | Epoch: 439 | Iter: 330600 | Total Loss: 0.002303 | Recon Loss: 0.001906 | Commit Loss: 0.000794 | Perplexity: 1074.442292
2025-09-15 05:11:12,490 Stage: Train 0.5 | Epoch: 439 | Iter: 330800 | Total Loss: 0.002304 | Recon Loss: 0.001907 | Commit Loss: 0.000793 | Perplexity: 1082.151366
2025-09-15 05:11:39,958 Stage: Train 0.5 | Epoch: 439 | Iter: 331000 | Total Loss: 0.002294 | Recon Loss: 0.001895 | Commit Loss: 0.000797 | Perplexity: 1076.934315
2025-09-15 05:12:07,411 Stage: Train 0.5 | Epoch: 439 | Iter: 331200 | Total Loss: 0.002320 | Recon Loss: 0.001922 | Commit Loss: 0.000796 | Perplexity: 1075.041695
Trainning Epoch:  66%|██████▌   | 440/665 [12:39:39<6:27:21, 103.29s/it]2025-09-15 05:12:34,827 Stage: Train 0.5 | Epoch: 440 | Iter: 331400 | Total Loss: 0.002322 | Recon Loss: 0.001925 | Commit Loss: 0.000794 | Perplexity: 1071.277233
2025-09-15 05:13:02,280 Stage: Train 0.5 | Epoch: 440 | Iter: 331600 | Total Loss: 0.002333 | Recon Loss: 0.001939 | Commit Loss: 0.000788 | Perplexity: 1073.966991
2025-09-15 05:13:29,784 Stage: Train 0.5 | Epoch: 440 | Iter: 331800 | Total Loss: 0.002296 | Recon Loss: 0.001900 | Commit Loss: 0.000791 | Perplexity: 1077.236582
2025-09-15 05:13:57,493 Stage: Train 0.5 | Epoch: 440 | Iter: 332000 | Total Loss: 0.002314 | Recon Loss: 0.001917 | Commit Loss: 0.000793 | Perplexity: 1079.480042
Trainning Epoch:  66%|██████▋   | 441/665 [12:41:23<6:26:00, 103.39s/it]2025-09-15 05:14:24,901 Stage: Train 0.5 | Epoch: 441 | Iter: 332200 | Total Loss: 0.002342 | Recon Loss: 0.001945 | Commit Loss: 0.000795 | Perplexity: 1076.425629
2025-09-15 05:14:52,401 Stage: Train 0.5 | Epoch: 441 | Iter: 332400 | Total Loss: 0.002388 | Recon Loss: 0.001993 | Commit Loss: 0.000790 | Perplexity: 1075.554363
2025-09-15 05:15:19,869 Stage: Train 0.5 | Epoch: 441 | Iter: 332600 | Total Loss: 0.002275 | Recon Loss: 0.001879 | Commit Loss: 0.000790 | Perplexity: 1076.834276
2025-09-15 05:15:47,453 Stage: Train 0.5 | Epoch: 441 | Iter: 332800 | Total Loss: 0.002291 | Recon Loss: 0.001896 | Commit Loss: 0.000792 | Perplexity: 1075.643906
Trainning Epoch:  66%|██████▋   | 442/665 [12:43:07<6:24:25, 103.43s/it]2025-09-15 05:16:14,933 Stage: Train 0.5 | Epoch: 442 | Iter: 333000 | Total Loss: 0.002326 | Recon Loss: 0.001933 | Commit Loss: 0.000787 | Perplexity: 1073.560721
2025-09-15 05:16:42,518 Stage: Train 0.5 | Epoch: 442 | Iter: 333200 | Total Loss: 0.002384 | Recon Loss: 0.001987 | Commit Loss: 0.000794 | Perplexity: 1079.424987
2025-09-15 05:17:10,282 Stage: Train 0.5 | Epoch: 442 | Iter: 333400 | Total Loss: 0.002289 | Recon Loss: 0.001891 | Commit Loss: 0.000794 | Perplexity: 1077.346891
Trainning Epoch:  67%|██████▋   | 443/665 [12:44:50<6:23:12, 103.57s/it]2025-09-15 05:17:37,766 Stage: Train 0.5 | Epoch: 443 | Iter: 333600 | Total Loss: 0.002325 | Recon Loss: 0.001927 | Commit Loss: 0.000796 | Perplexity: 1080.211615
2025-09-15 05:18:05,224 Stage: Train 0.5 | Epoch: 443 | Iter: 333800 | Total Loss: 0.002307 | Recon Loss: 0.001913 | Commit Loss: 0.000788 | Perplexity: 1078.118927
2025-09-15 05:18:32,787 Stage: Train 0.5 | Epoch: 443 | Iter: 334000 | Total Loss: 0.002374 | Recon Loss: 0.001975 | Commit Loss: 0.000798 | Perplexity: 1082.409074
2025-09-15 05:19:00,434 Stage: Train 0.5 | Epoch: 443 | Iter: 334200 | Total Loss: 0.002273 | Recon Loss: 0.001880 | Commit Loss: 0.000786 | Perplexity: 1071.960385
Trainning Epoch:  67%|██████▋   | 444/665 [12:46:34<6:21:37, 103.61s/it]2025-09-15 05:19:27,942 Stage: Train 0.5 | Epoch: 444 | Iter: 334400 | Total Loss: 0.002305 | Recon Loss: 0.001909 | Commit Loss: 0.000792 | Perplexity: 1075.206113
2025-09-15 05:19:55,487 Stage: Train 0.5 | Epoch: 444 | Iter: 334600 | Total Loss: 0.002330 | Recon Loss: 0.001934 | Commit Loss: 0.000792 | Perplexity: 1077.635598
2025-09-15 05:20:23,074 Stage: Train 0.5 | Epoch: 444 | Iter: 334800 | Total Loss: 0.002326 | Recon Loss: 0.001931 | Commit Loss: 0.000790 | Perplexity: 1076.785863
2025-09-15 05:20:50,760 Stage: Train 0.5 | Epoch: 444 | Iter: 335000 | Total Loss: 0.002351 | Recon Loss: 0.001953 | Commit Loss: 0.000796 | Perplexity: 1081.835876
Trainning Epoch:  67%|██████▋   | 445/665 [12:48:18<6:20:09, 103.68s/it]2025-09-15 05:21:18,219 Stage: Train 0.5 | Epoch: 445 | Iter: 335200 | Total Loss: 0.002291 | Recon Loss: 0.001896 | Commit Loss: 0.000791 | Perplexity: 1075.887288
2025-09-15 05:21:45,718 Stage: Train 0.5 | Epoch: 445 | Iter: 335400 | Total Loss: 0.002319 | Recon Loss: 0.001926 | Commit Loss: 0.000786 | Perplexity: 1076.224698
2025-09-15 05:22:13,269 Stage: Train 0.5 | Epoch: 445 | Iter: 335600 | Total Loss: 0.002341 | Recon Loss: 0.001943 | Commit Loss: 0.000796 | Perplexity: 1080.122057
2025-09-15 05:22:40,725 Stage: Train 0.5 | Epoch: 445 | Iter: 335800 | Total Loss: 0.002268 | Recon Loss: 0.001873 | Commit Loss: 0.000790 | Perplexity: 1077.333640
Trainning Epoch:  67%|██████▋   | 446/665 [12:50:01<6:18:11, 103.61s/it]2025-09-15 05:23:08,214 Stage: Train 0.5 | Epoch: 446 | Iter: 336000 | Total Loss: 0.002335 | Recon Loss: 0.001939 | Commit Loss: 0.000791 | Perplexity: 1078.199928
2025-09-15 05:23:35,727 Stage: Train 0.5 | Epoch: 446 | Iter: 336200 | Total Loss: 0.002293 | Recon Loss: 0.001900 | Commit Loss: 0.000786 | Perplexity: 1076.761450
2025-09-15 05:24:03,394 Stage: Train 0.5 | Epoch: 446 | Iter: 336400 | Total Loss: 0.002305 | Recon Loss: 0.001911 | Commit Loss: 0.000788 | Perplexity: 1077.337717
Trainning Epoch:  67%|██████▋   | 447/665 [12:51:45<6:16:36, 103.65s/it]2025-09-15 05:24:30,867 Stage: Train 0.5 | Epoch: 447 | Iter: 336600 | Total Loss: 0.002306 | Recon Loss: 0.001909 | Commit Loss: 0.000793 | Perplexity: 1077.610579
2025-09-15 05:24:58,398 Stage: Train 0.5 | Epoch: 447 | Iter: 336800 | Total Loss: 0.002253 | Recon Loss: 0.001858 | Commit Loss: 0.000788 | Perplexity: 1080.293478
2025-09-15 05:25:25,939 Stage: Train 0.5 | Epoch: 447 | Iter: 337000 | Total Loss: 0.002307 | Recon Loss: 0.001911 | Commit Loss: 0.000792 | Perplexity: 1078.762083
2025-09-15 05:25:53,508 Stage: Train 0.5 | Epoch: 447 | Iter: 337200 | Total Loss: 0.002335 | Recon Loss: 0.001941 | Commit Loss: 0.000789 | Perplexity: 1075.931240
Trainning Epoch:  67%|██████▋   | 448/665 [12:53:29<6:14:54, 103.66s/it]2025-09-15 05:26:21,011 Stage: Train 0.5 | Epoch: 448 | Iter: 337400 | Total Loss: 0.002339 | Recon Loss: 0.001944 | Commit Loss: 0.000789 | Perplexity: 1072.482850
2025-09-15 05:26:48,543 Stage: Train 0.5 | Epoch: 448 | Iter: 337600 | Total Loss: 0.002341 | Recon Loss: 0.001947 | Commit Loss: 0.000788 | Perplexity: 1079.912645
2025-09-15 05:27:16,090 Stage: Train 0.5 | Epoch: 448 | Iter: 337800 | Total Loss: 0.002297 | Recon Loss: 0.001903 | Commit Loss: 0.000788 | Perplexity: 1079.545862
2025-09-15 05:27:43,728 Stage: Train 0.5 | Epoch: 448 | Iter: 338000 | Total Loss: 0.002299 | Recon Loss: 0.001902 | Commit Loss: 0.000794 | Perplexity: 1078.279435
Trainning Epoch:  68%|██████▊   | 449/665 [12:55:13<6:13:15, 103.68s/it]2025-09-15 05:28:11,210 Stage: Train 0.5 | Epoch: 449 | Iter: 338200 | Total Loss: 0.002333 | Recon Loss: 0.001939 | Commit Loss: 0.000787 | Perplexity: 1073.024889
2025-09-15 05:28:38,734 Stage: Train 0.5 | Epoch: 449 | Iter: 338400 | Total Loss: 0.002277 | Recon Loss: 0.001883 | Commit Loss: 0.000788 | Perplexity: 1077.164818
2025-09-15 05:29:06,289 Stage: Train 0.5 | Epoch: 449 | Iter: 338600 | Total Loss: 0.002313 | Recon Loss: 0.001917 | Commit Loss: 0.000792 | Perplexity: 1079.687439
2025-09-15 05:29:33,802 Stage: Train 0.5 | Epoch: 449 | Iter: 338800 | Total Loss: 0.002316 | Recon Loss: 0.001921 | Commit Loss: 0.000791 | Perplexity: 1077.251954
Trainning Epoch:  68%|██████▊   | 450/665 [12:56:56<6:11:27, 103.66s/it]2025-09-15 05:30:01,243 Stage: Train 0.5 | Epoch: 450 | Iter: 339000 | Total Loss: 0.002294 | Recon Loss: 0.001899 | Commit Loss: 0.000789 | Perplexity: 1079.599382
2025-09-15 05:30:28,732 Stage: Train 0.5 | Epoch: 450 | Iter: 339200 | Total Loss: 0.002293 | Recon Loss: 0.001899 | Commit Loss: 0.000787 | Perplexity: 1075.422782
2025-09-15 05:30:56,424 Stage: Train 0.5 | Epoch: 450 | Iter: 339400 | Total Loss: 0.002277 | Recon Loss: 0.001882 | Commit Loss: 0.000790 | Perplexity: 1075.575132
2025-09-15 05:31:23,925 Stage: Train 0.5 | Epoch: 450 | Iter: 339600 | Total Loss: 0.002326 | Recon Loss: 0.001930 | Commit Loss: 0.000791 | Perplexity: 1078.394431
Trainning Epoch:  68%|██████▊   | 451/665 [12:58:40<6:09:41, 103.65s/it]2025-09-15 05:31:51,349 Stage: Train 0.5 | Epoch: 451 | Iter: 339800 | Total Loss: 0.002306 | Recon Loss: 0.001913 | Commit Loss: 0.000786 | Perplexity: 1077.129652
2025-09-15 05:32:18,828 Stage: Train 0.5 | Epoch: 451 | Iter: 340000 | Total Loss: 0.002302 | Recon Loss: 0.001905 | Commit Loss: 0.000794 | Perplexity: 1079.904872
2025-09-15 05:32:18,829 Saving model at iteration 340000
2025-09-15 05:32:19,216 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_452_step_340000
2025-09-15 05:32:19,419 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_452_step_340000/pytorch_model.bin
2025-09-15 05:32:19,741 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_452_step_340000/optimizer.bin
2025-09-15 05:32:19,741 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_452_step_340000/scheduler.bin
2025-09-15 05:32:19,742 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_452_step_340000/random_states_0.pkl
2025-09-15 05:32:47,588 Stage: Train 0.5 | Epoch: 451 | Iter: 340200 | Total Loss: 0.002312 | Recon Loss: 0.001919 | Commit Loss: 0.000787 | Perplexity: 1078.530283
Trainning Epoch:  68%|██████▊   | 452/665 [13:00:25<6:09:20, 104.04s/it]2025-09-15 05:33:15,279 Stage: Train 0.5 | Epoch: 452 | Iter: 340400 | Total Loss: 0.002298 | Recon Loss: 0.001904 | Commit Loss: 0.000786 | Perplexity: 1076.106748
2025-09-15 05:33:42,729 Stage: Train 0.5 | Epoch: 452 | Iter: 340600 | Total Loss: 0.002285 | Recon Loss: 0.001890 | Commit Loss: 0.000789 | Perplexity: 1078.444622
2025-09-15 05:34:10,158 Stage: Train 0.5 | Epoch: 452 | Iter: 340800 | Total Loss: 0.002297 | Recon Loss: 0.001903 | Commit Loss: 0.000788 | Perplexity: 1078.353250
2025-09-15 05:34:37,692 Stage: Train 0.5 | Epoch: 452 | Iter: 341000 | Total Loss: 0.002282 | Recon Loss: 0.001886 | Commit Loss: 0.000792 | Perplexity: 1080.797600
Trainning Epoch:  68%|██████▊   | 453/665 [13:02:08<6:06:53, 103.84s/it]2025-09-15 05:35:05,125 Stage: Train 0.5 | Epoch: 453 | Iter: 341200 | Total Loss: 0.002285 | Recon Loss: 0.001891 | Commit Loss: 0.000790 | Perplexity: 1078.082997
2025-09-15 05:35:32,610 Stage: Train 0.5 | Epoch: 453 | Iter: 341400 | Total Loss: 0.002337 | Recon Loss: 0.001942 | Commit Loss: 0.000790 | Perplexity: 1077.885892
2025-09-15 05:36:00,260 Stage: Train 0.5 | Epoch: 453 | Iter: 341600 | Total Loss: 0.002280 | Recon Loss: 0.001886 | Commit Loss: 0.000788 | Perplexity: 1075.110640
2025-09-15 05:36:27,902 Stage: Train 0.5 | Epoch: 453 | Iter: 341800 | Total Loss: 0.002296 | Recon Loss: 0.001901 | Commit Loss: 0.000789 | Perplexity: 1077.728863
Trainning Epoch:  68%|██████▊   | 454/665 [13:03:52<6:05:10, 103.84s/it]2025-09-15 05:36:55,456 Stage: Train 0.5 | Epoch: 454 | Iter: 342000 | Total Loss: 0.002288 | Recon Loss: 0.001895 | Commit Loss: 0.000786 | Perplexity: 1078.022695
2025-09-15 05:37:22,936 Stage: Train 0.5 | Epoch: 454 | Iter: 342200 | Total Loss: 0.002261 | Recon Loss: 0.001866 | Commit Loss: 0.000790 | Perplexity: 1079.412393
2025-09-15 05:37:50,415 Stage: Train 0.5 | Epoch: 454 | Iter: 342400 | Total Loss: 0.002336 | Recon Loss: 0.001938 | Commit Loss: 0.000795 | Perplexity: 1080.623459
2025-09-15 05:38:17,852 Stage: Train 0.5 | Epoch: 454 | Iter: 342600 | Total Loss: 0.002328 | Recon Loss: 0.001936 | Commit Loss: 0.000785 | Perplexity: 1075.433133
Trainning Epoch:  68%|██████▊   | 455/665 [13:05:35<6:02:59, 103.71s/it]2025-09-15 05:38:45,239 Stage: Train 0.5 | Epoch: 455 | Iter: 342800 | Total Loss: 0.002336 | Recon Loss: 0.001942 | Commit Loss: 0.000788 | Perplexity: 1076.880753
2025-09-15 05:39:12,737 Stage: Train 0.5 | Epoch: 455 | Iter: 343000 | Total Loss: 0.002265 | Recon Loss: 0.001874 | Commit Loss: 0.000782 | Perplexity: 1076.680419
2025-09-15 05:39:40,124 Stage: Train 0.5 | Epoch: 455 | Iter: 343200 | Total Loss: 0.002275 | Recon Loss: 0.001880 | Commit Loss: 0.000789 | Perplexity: 1075.777275
Trainning Epoch:  69%|██████▊   | 456/665 [13:07:19<6:00:45, 103.57s/it]2025-09-15 05:40:07,474 Stage: Train 0.5 | Epoch: 456 | Iter: 343400 | Total Loss: 0.002315 | Recon Loss: 0.001918 | Commit Loss: 0.000794 | Perplexity: 1080.449553
2025-09-15 05:40:34,892 Stage: Train 0.5 | Epoch: 456 | Iter: 343600 | Total Loss: 0.002294 | Recon Loss: 0.001901 | Commit Loss: 0.000786 | Perplexity: 1076.809997
2025-09-15 05:41:02,317 Stage: Train 0.5 | Epoch: 456 | Iter: 343800 | Total Loss: 0.002334 | Recon Loss: 0.001942 | Commit Loss: 0.000784 | Perplexity: 1073.710107
2025-09-15 05:41:29,698 Stage: Train 0.5 | Epoch: 456 | Iter: 344000 | Total Loss: 0.002288 | Recon Loss: 0.001891 | Commit Loss: 0.000794 | Perplexity: 1081.178845
Trainning Epoch:  69%|██████▊   | 457/665 [13:09:02<5:58:32, 103.43s/it]2025-09-15 05:41:56,990 Stage: Train 0.5 | Epoch: 457 | Iter: 344200 | Total Loss: 0.002308 | Recon Loss: 0.001916 | Commit Loss: 0.000785 | Perplexity: 1076.325489
2025-09-15 05:42:24,460 Stage: Train 0.5 | Epoch: 457 | Iter: 344400 | Total Loss: 0.002295 | Recon Loss: 0.001904 | Commit Loss: 0.000782 | Perplexity: 1075.552162
2025-09-15 05:42:52,110 Stage: Train 0.5 | Epoch: 457 | Iter: 344600 | Total Loss: 0.002309 | Recon Loss: 0.001916 | Commit Loss: 0.000786 | Perplexity: 1076.783188
2025-09-15 05:43:19,537 Stage: Train 0.5 | Epoch: 457 | Iter: 344800 | Total Loss: 0.002296 | Recon Loss: 0.001899 | Commit Loss: 0.000795 | Perplexity: 1079.428480
Trainning Epoch:  69%|██████▉   | 458/665 [13:10:45<5:56:54, 103.45s/it]2025-09-15 05:43:47,019 Stage: Train 0.5 | Epoch: 458 | Iter: 345000 | Total Loss: 0.002284 | Recon Loss: 0.001891 | Commit Loss: 0.000785 | Perplexity: 1074.806027
2025-09-15 05:44:14,648 Stage: Train 0.5 | Epoch: 458 | Iter: 345200 | Total Loss: 0.002266 | Recon Loss: 0.001873 | Commit Loss: 0.000785 | Perplexity: 1079.278096
2025-09-15 05:44:42,296 Stage: Train 0.5 | Epoch: 458 | Iter: 345400 | Total Loss: 0.002297 | Recon Loss: 0.001903 | Commit Loss: 0.000790 | Perplexity: 1079.512218
2025-09-15 05:45:09,866 Stage: Train 0.5 | Epoch: 458 | Iter: 345600 | Total Loss: 0.002284 | Recon Loss: 0.001890 | Commit Loss: 0.000788 | Perplexity: 1079.086100
Trainning Epoch:  69%|██████▉   | 459/665 [13:12:29<5:55:35, 103.57s/it]2025-09-15 05:45:37,501 Stage: Train 0.5 | Epoch: 459 | Iter: 345800 | Total Loss: 0.002303 | Recon Loss: 0.001914 | Commit Loss: 0.000776 | Perplexity: 1073.392221
2025-09-15 05:46:05,084 Stage: Train 0.5 | Epoch: 459 | Iter: 346000 | Total Loss: 0.002326 | Recon Loss: 0.001930 | Commit Loss: 0.000792 | Perplexity: 1080.793332
2025-09-15 05:46:32,525 Stage: Train 0.5 | Epoch: 459 | Iter: 346200 | Total Loss: 0.002358 | Recon Loss: 0.001963 | Commit Loss: 0.000791 | Perplexity: 1077.487830
Trainning Epoch:  69%|██████▉   | 460/665 [13:14:13<5:53:58, 103.60s/it]2025-09-15 05:46:59,978 Stage: Train 0.5 | Epoch: 460 | Iter: 346400 | Total Loss: 0.002285 | Recon Loss: 0.001892 | Commit Loss: 0.000786 | Perplexity: 1078.347940
2025-09-15 05:47:27,787 Stage: Train 0.5 | Epoch: 460 | Iter: 346600 | Total Loss: 0.002291 | Recon Loss: 0.001896 | Commit Loss: 0.000790 | Perplexity: 1081.754413
2025-09-15 05:47:55,387 Stage: Train 0.5 | Epoch: 460 | Iter: 346800 | Total Loss: 0.002284 | Recon Loss: 0.001893 | Commit Loss: 0.000783 | Perplexity: 1073.235382
2025-09-15 05:48:22,964 Stage: Train 0.5 | Epoch: 460 | Iter: 347000 | Total Loss: 0.002275 | Recon Loss: 0.001881 | Commit Loss: 0.000788 | Perplexity: 1080.149272
Trainning Epoch:  69%|██████▉   | 461/665 [13:15:57<5:52:39, 103.72s/it]2025-09-15 05:48:50,471 Stage: Train 0.5 | Epoch: 461 | Iter: 347200 | Total Loss: 0.002376 | Recon Loss: 0.001982 | Commit Loss: 0.000787 | Perplexity: 1077.552657
2025-09-15 05:49:17,985 Stage: Train 0.5 | Epoch: 461 | Iter: 347400 | Total Loss: 0.002276 | Recon Loss: 0.001884 | Commit Loss: 0.000784 | Perplexity: 1079.438626
2025-09-15 05:49:45,588 Stage: Train 0.5 | Epoch: 461 | Iter: 347600 | Total Loss: 0.002289 | Recon Loss: 0.001895 | Commit Loss: 0.000788 | Perplexity: 1081.004959
2025-09-15 05:50:13,244 Stage: Train 0.5 | Epoch: 461 | Iter: 347800 | Total Loss: 0.002344 | Recon Loss: 0.001950 | Commit Loss: 0.000788 | Perplexity: 1075.344255
Trainning Epoch:  69%|██████▉   | 462/665 [13:17:41<5:51:02, 103.76s/it]2025-09-15 05:50:40,835 Stage: Train 0.5 | Epoch: 462 | Iter: 348000 | Total Loss: 0.002280 | Recon Loss: 0.001887 | Commit Loss: 0.000785 | Perplexity: 1072.296085
2025-09-15 05:51:08,516 Stage: Train 0.5 | Epoch: 462 | Iter: 348200 | Total Loss: 0.002281 | Recon Loss: 0.001888 | Commit Loss: 0.000785 | Perplexity: 1080.603781
2025-09-15 05:51:36,127 Stage: Train 0.5 | Epoch: 462 | Iter: 348400 | Total Loss: 0.002282 | Recon Loss: 0.001891 | Commit Loss: 0.000784 | Perplexity: 1077.408894
2025-09-15 05:52:03,833 Stage: Train 0.5 | Epoch: 462 | Iter: 348600 | Total Loss: 0.002370 | Recon Loss: 0.001975 | Commit Loss: 0.000790 | Perplexity: 1078.567781
Trainning Epoch:  70%|██████▉   | 463/665 [13:19:25<5:49:38, 103.85s/it]2025-09-15 05:52:31,281 Stage: Train 0.5 | Epoch: 463 | Iter: 348800 | Total Loss: 0.002224 | Recon Loss: 0.001832 | Commit Loss: 0.000783 | Perplexity: 1080.138463
2025-09-15 05:52:58,849 Stage: Train 0.5 | Epoch: 463 | Iter: 349000 | Total Loss: 0.002365 | Recon Loss: 0.001973 | Commit Loss: 0.000785 | Perplexity: 1077.289864
2025-09-15 05:53:26,335 Stage: Train 0.5 | Epoch: 463 | Iter: 349200 | Total Loss: 0.002279 | Recon Loss: 0.001887 | Commit Loss: 0.000784 | Perplexity: 1080.261021
Trainning Epoch:  70%|██████▉   | 464/665 [13:21:08<5:47:33, 103.75s/it]2025-09-15 05:53:53,755 Stage: Train 0.5 | Epoch: 464 | Iter: 349400 | Total Loss: 0.002269 | Recon Loss: 0.001876 | Commit Loss: 0.000787 | Perplexity: 1078.691104
2025-09-15 05:54:21,234 Stage: Train 0.5 | Epoch: 464 | Iter: 349600 | Total Loss: 0.002328 | Recon Loss: 0.001939 | Commit Loss: 0.000779 | Perplexity: 1078.405766
2025-09-15 05:54:48,965 Stage: Train 0.5 | Epoch: 464 | Iter: 349800 | Total Loss: 0.002279 | Recon Loss: 0.001886 | Commit Loss: 0.000787 | Perplexity: 1086.153188
2025-09-15 05:55:16,547 Stage: Train 0.5 | Epoch: 464 | Iter: 350000 | Total Loss: 0.002273 | Recon Loss: 0.001884 | Commit Loss: 0.000778 | Perplexity: 1074.300691
Trainning Epoch:  70%|██████▉   | 465/665 [13:22:52<5:45:57, 103.79s/it]2025-09-15 05:55:44,155 Stage: Train 0.5 | Epoch: 465 | Iter: 350200 | Total Loss: 0.002298 | Recon Loss: 0.001903 | Commit Loss: 0.000790 | Perplexity: 1077.729850
2025-09-15 05:56:11,733 Stage: Train 0.5 | Epoch: 465 | Iter: 350400 | Total Loss: 0.002315 | Recon Loss: 0.001923 | Commit Loss: 0.000784 | Perplexity: 1082.889926
2025-09-15 05:56:39,306 Stage: Train 0.5 | Epoch: 465 | Iter: 350600 | Total Loss: 0.002290 | Recon Loss: 0.001899 | Commit Loss: 0.000783 | Perplexity: 1078.278855
2025-09-15 05:57:06,869 Stage: Train 0.5 | Epoch: 465 | Iter: 350800 | Total Loss: 0.002286 | Recon Loss: 0.001897 | Commit Loss: 0.000778 | Perplexity: 1076.994690
Trainning Epoch:  70%|███████   | 466/665 [13:24:36<5:44:12, 103.78s/it]2025-09-15 05:57:34,337 Stage: Train 0.5 | Epoch: 466 | Iter: 351000 | Total Loss: 0.002285 | Recon Loss: 0.001889 | Commit Loss: 0.000790 | Perplexity: 1080.232694
2025-09-15 05:58:01,846 Stage: Train 0.5 | Epoch: 466 | Iter: 351200 | Total Loss: 0.002290 | Recon Loss: 0.001901 | Commit Loss: 0.000778 | Perplexity: 1077.892305
2025-09-15 05:58:29,439 Stage: Train 0.5 | Epoch: 466 | Iter: 351400 | Total Loss: 0.002307 | Recon Loss: 0.001917 | Commit Loss: 0.000781 | Perplexity: 1080.390938
2025-09-15 05:58:56,962 Stage: Train 0.5 | Epoch: 466 | Iter: 351600 | Total Loss: 0.002248 | Recon Loss: 0.001853 | Commit Loss: 0.000789 | Perplexity: 1082.152424
Trainning Epoch:  70%|███████   | 467/665 [13:26:19<5:42:21, 103.75s/it]2025-09-15 05:59:24,549 Stage: Train 0.5 | Epoch: 467 | Iter: 351800 | Total Loss: 0.002342 | Recon Loss: 0.001949 | Commit Loss: 0.000787 | Perplexity: 1078.405360
2025-09-15 05:59:52,197 Stage: Train 0.5 | Epoch: 467 | Iter: 352000 | Total Loss: 0.002248 | Recon Loss: 0.001858 | Commit Loss: 0.000778 | Perplexity: 1079.477498
2025-09-15 06:00:19,717 Stage: Train 0.5 | Epoch: 467 | Iter: 352200 | Total Loss: 0.002300 | Recon Loss: 0.001908 | Commit Loss: 0.000784 | Perplexity: 1079.921036
2025-09-15 06:00:47,314 Stage: Train 0.5 | Epoch: 467 | Iter: 352400 | Total Loss: 0.002296 | Recon Loss: 0.001904 | Commit Loss: 0.000784 | Perplexity: 1081.703220
Trainning Epoch:  70%|███████   | 468/665 [13:28:03<5:40:44, 103.78s/it]2025-09-15 06:01:14,845 Stage: Train 0.5 | Epoch: 468 | Iter: 352600 | Total Loss: 0.002255 | Recon Loss: 0.001865 | Commit Loss: 0.000779 | Perplexity: 1080.447381
2025-09-15 06:01:42,368 Stage: Train 0.5 | Epoch: 468 | Iter: 352800 | Total Loss: 0.002326 | Recon Loss: 0.001937 | Commit Loss: 0.000779 | Perplexity: 1073.354398
2025-09-15 06:02:09,935 Stage: Train 0.5 | Epoch: 468 | Iter: 353000 | Total Loss: 0.002255 | Recon Loss: 0.001862 | Commit Loss: 0.000786 | Perplexity: 1080.279707
Trainning Epoch:  71%|███████   | 469/665 [13:29:47<5:38:57, 103.76s/it]2025-09-15 06:02:37,457 Stage: Train 0.5 | Epoch: 469 | Iter: 353200 | Total Loss: 0.002400 | Recon Loss: 0.002004 | Commit Loss: 0.000793 | Perplexity: 1082.446543
2025-09-15 06:03:05,096 Stage: Train 0.5 | Epoch: 469 | Iter: 353400 | Total Loss: 0.002229 | Recon Loss: 0.001840 | Commit Loss: 0.000778 | Perplexity: 1079.182328
2025-09-15 06:03:32,639 Stage: Train 0.5 | Epoch: 469 | Iter: 353600 | Total Loss: 0.002315 | Recon Loss: 0.001925 | Commit Loss: 0.000781 | Perplexity: 1085.003126
2025-09-15 06:04:00,130 Stage: Train 0.5 | Epoch: 469 | Iter: 353800 | Total Loss: 0.002274 | Recon Loss: 0.001880 | Commit Loss: 0.000787 | Perplexity: 1079.381260
Trainning Epoch:  71%|███████   | 470/665 [13:31:31<5:37:09, 103.74s/it]2025-09-15 06:04:27,607 Stage: Train 0.5 | Epoch: 470 | Iter: 354000 | Total Loss: 0.002283 | Recon Loss: 0.001891 | Commit Loss: 0.000783 | Perplexity: 1077.618849
2025-09-15 06:04:55,155 Stage: Train 0.5 | Epoch: 470 | Iter: 354200 | Total Loss: 0.002292 | Recon Loss: 0.001902 | Commit Loss: 0.000780 | Perplexity: 1078.326801
2025-09-15 06:05:22,689 Stage: Train 0.5 | Epoch: 470 | Iter: 354400 | Total Loss: 0.002249 | Recon Loss: 0.001857 | Commit Loss: 0.000784 | Perplexity: 1083.023745
2025-09-15 06:05:50,286 Stage: Train 0.5 | Epoch: 470 | Iter: 354600 | Total Loss: 0.002367 | Recon Loss: 0.001972 | Commit Loss: 0.000790 | Perplexity: 1083.172191
Trainning Epoch:  71%|███████   | 471/665 [13:33:14<5:35:20, 103.71s/it]2025-09-15 06:06:17,709 Stage: Train 0.5 | Epoch: 471 | Iter: 354800 | Total Loss: 0.002271 | Recon Loss: 0.001879 | Commit Loss: 0.000782 | Perplexity: 1077.656541
2025-09-15 06:06:45,403 Stage: Train 0.5 | Epoch: 471 | Iter: 355000 | Total Loss: 0.002245 | Recon Loss: 0.001857 | Commit Loss: 0.000777 | Perplexity: 1080.067578
2025-09-15 06:07:13,113 Stage: Train 0.5 | Epoch: 471 | Iter: 355200 | Total Loss: 0.002283 | Recon Loss: 0.001890 | Commit Loss: 0.000786 | Perplexity: 1080.711005
2025-09-15 06:07:40,744 Stage: Train 0.5 | Epoch: 471 | Iter: 355400 | Total Loss: 0.002325 | Recon Loss: 0.001930 | Commit Loss: 0.000790 | Perplexity: 1082.842586
Trainning Epoch:  71%|███████   | 472/665 [13:34:58<5:33:53, 103.80s/it]2025-09-15 06:08:08,265 Stage: Train 0.5 | Epoch: 472 | Iter: 355600 | Total Loss: 0.002257 | Recon Loss: 0.001866 | Commit Loss: 0.000782 | Perplexity: 1077.442364
2025-09-15 06:08:35,821 Stage: Train 0.5 | Epoch: 472 | Iter: 355800 | Total Loss: 0.002332 | Recon Loss: 0.001940 | Commit Loss: 0.000783 | Perplexity: 1080.774222
2025-09-15 06:09:03,371 Stage: Train 0.5 | Epoch: 472 | Iter: 356000 | Total Loss: 0.002254 | Recon Loss: 0.001864 | Commit Loss: 0.000781 | Perplexity: 1078.155357
Trainning Epoch:  71%|███████   | 473/665 [13:36:42<5:32:11, 103.81s/it]2025-09-15 06:09:30,998 Stage: Train 0.5 | Epoch: 473 | Iter: 356200 | Total Loss: 0.002272 | Recon Loss: 0.001878 | Commit Loss: 0.000789 | Perplexity: 1080.842175
2025-09-15 06:09:58,638 Stage: Train 0.5 | Epoch: 473 | Iter: 356400 | Total Loss: 0.002316 | Recon Loss: 0.001925 | Commit Loss: 0.000780 | Perplexity: 1076.752465
2025-09-15 06:10:26,108 Stage: Train 0.5 | Epoch: 473 | Iter: 356600 | Total Loss: 0.002267 | Recon Loss: 0.001875 | Commit Loss: 0.000783 | Perplexity: 1083.528482
2025-09-15 06:10:53,537 Stage: Train 0.5 | Epoch: 473 | Iter: 356800 | Total Loss: 0.002310 | Recon Loss: 0.001917 | Commit Loss: 0.000786 | Perplexity: 1081.859562
Trainning Epoch:  71%|███████▏  | 474/665 [13:38:26<5:30:09, 103.72s/it]2025-09-15 06:11:20,912 Stage: Train 0.5 | Epoch: 474 | Iter: 357000 | Total Loss: 0.002249 | Recon Loss: 0.001861 | Commit Loss: 0.000778 | Perplexity: 1076.524589
2025-09-15 06:11:48,332 Stage: Train 0.5 | Epoch: 474 | Iter: 357200 | Total Loss: 0.002313 | Recon Loss: 0.001922 | Commit Loss: 0.000782 | Perplexity: 1079.522581
2025-09-15 06:12:15,770 Stage: Train 0.5 | Epoch: 474 | Iter: 357400 | Total Loss: 0.002291 | Recon Loss: 0.001899 | Commit Loss: 0.000784 | Perplexity: 1086.130440
2025-09-15 06:12:43,221 Stage: Train 0.5 | Epoch: 474 | Iter: 357600 | Total Loss: 0.002369 | Recon Loss: 0.001981 | Commit Loss: 0.000777 | Perplexity: 1075.163091
Trainning Epoch:  71%|███████▏  | 475/665 [13:40:09<5:27:59, 103.57s/it]2025-09-15 06:13:10,596 Stage: Train 0.5 | Epoch: 475 | Iter: 357800 | Total Loss: 0.002243 | Recon Loss: 0.001854 | Commit Loss: 0.000777 | Perplexity: 1078.085943
2025-09-15 06:13:38,144 Stage: Train 0.5 | Epoch: 475 | Iter: 358000 | Total Loss: 0.002251 | Recon Loss: 0.001861 | Commit Loss: 0.000780 | Perplexity: 1080.919960
2025-09-15 06:14:05,632 Stage: Train 0.5 | Epoch: 475 | Iter: 358200 | Total Loss: 0.002271 | Recon Loss: 0.001882 | Commit Loss: 0.000779 | Perplexity: 1079.640222
2025-09-15 06:14:33,100 Stage: Train 0.5 | Epoch: 475 | Iter: 358400 | Total Loss: 0.002293 | Recon Loss: 0.001899 | Commit Loss: 0.000789 | Perplexity: 1081.077978
Trainning Epoch:  72%|███████▏  | 476/665 [13:41:52<5:26:07, 103.53s/it]2025-09-15 06:15:00,508 Stage: Train 0.5 | Epoch: 476 | Iter: 358600 | Total Loss: 0.002274 | Recon Loss: 0.001886 | Commit Loss: 0.000776 | Perplexity: 1079.586250
2025-09-15 06:15:27,962 Stage: Train 0.5 | Epoch: 476 | Iter: 358800 | Total Loss: 0.002280 | Recon Loss: 0.001889 | Commit Loss: 0.000780 | Perplexity: 1081.568188
2025-09-15 06:15:55,434 Stage: Train 0.5 | Epoch: 476 | Iter: 359000 | Total Loss: 0.002259 | Recon Loss: 0.001870 | Commit Loss: 0.000779 | Perplexity: 1076.457904
Trainning Epoch:  72%|███████▏  | 477/665 [13:43:36<5:24:14, 103.48s/it]2025-09-15 06:16:22,913 Stage: Train 0.5 | Epoch: 477 | Iter: 359200 | Total Loss: 0.002295 | Recon Loss: 0.001901 | Commit Loss: 0.000787 | Perplexity: 1082.117203
2025-09-15 06:16:50,610 Stage: Train 0.5 | Epoch: 477 | Iter: 359400 | Total Loss: 0.002293 | Recon Loss: 0.001904 | Commit Loss: 0.000777 | Perplexity: 1080.352214
2025-09-15 06:17:18,221 Stage: Train 0.5 | Epoch: 477 | Iter: 359600 | Total Loss: 0.002250 | Recon Loss: 0.001859 | Commit Loss: 0.000782 | Perplexity: 1084.181099
2025-09-15 06:17:45,766 Stage: Train 0.5 | Epoch: 477 | Iter: 359800 | Total Loss: 0.002266 | Recon Loss: 0.001876 | Commit Loss: 0.000778 | Perplexity: 1074.889111
Trainning Epoch:  72%|███████▏  | 478/665 [13:45:20<5:23:00, 103.64s/it]2025-09-15 06:18:13,344 Stage: Train 0.5 | Epoch: 478 | Iter: 360000 | Total Loss: 0.002293 | Recon Loss: 0.001900 | Commit Loss: 0.000786 | Perplexity: 1083.902162
2025-09-15 06:18:13,345 Saving model at iteration 360000
2025-09-15 06:18:13,516 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_479_step_360000
2025-09-15 06:18:13,724 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_479_step_360000/pytorch_model.bin
2025-09-15 06:18:14,058 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_479_step_360000/optimizer.bin
2025-09-15 06:18:14,058 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_479_step_360000/scheduler.bin
2025-09-15 06:18:14,059 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_479_step_360000/random_states_0.pkl
2025-09-15 06:18:41,802 Stage: Train 0.5 | Epoch: 478 | Iter: 360200 | Total Loss: 0.002283 | Recon Loss: 0.001893 | Commit Loss: 0.000779 | Perplexity: 1076.637391
2025-09-15 06:19:09,269 Stage: Train 0.5 | Epoch: 478 | Iter: 360400 | Total Loss: 0.002239 | Recon Loss: 0.001848 | Commit Loss: 0.000782 | Perplexity: 1081.039220
2025-09-15 06:19:36,687 Stage: Train 0.5 | Epoch: 478 | Iter: 360600 | Total Loss: 0.002262 | Recon Loss: 0.001873 | Commit Loss: 0.000778 | Perplexity: 1082.490252
Trainning Epoch:  72%|███████▏  | 479/665 [13:47:04<5:21:56, 103.85s/it]2025-09-15 06:20:04,163 Stage: Train 0.5 | Epoch: 479 | Iter: 360800 | Total Loss: 0.002322 | Recon Loss: 0.001932 | Commit Loss: 0.000781 | Perplexity: 1079.313478
2025-09-15 06:20:31,710 Stage: Train 0.5 | Epoch: 479 | Iter: 361000 | Total Loss: 0.002206 | Recon Loss: 0.001817 | Commit Loss: 0.000777 | Perplexity: 1077.543761
2025-09-15 06:20:59,257 Stage: Train 0.5 | Epoch: 479 | Iter: 361200 | Total Loss: 0.002295 | Recon Loss: 0.001902 | Commit Loss: 0.000785 | Perplexity: 1079.720864
2025-09-15 06:21:26,790 Stage: Train 0.5 | Epoch: 479 | Iter: 361400 | Total Loss: 0.002268 | Recon Loss: 0.001878 | Commit Loss: 0.000781 | Perplexity: 1082.584546
Trainning Epoch:  72%|███████▏  | 480/665 [13:48:48<5:19:59, 103.78s/it]2025-09-15 06:21:54,437 Stage: Train 0.5 | Epoch: 480 | Iter: 361600 | Total Loss: 0.002277 | Recon Loss: 0.001884 | Commit Loss: 0.000786 | Perplexity: 1078.865114
2025-09-15 06:22:21,967 Stage: Train 0.5 | Epoch: 480 | Iter: 361800 | Total Loss: 0.002279 | Recon Loss: 0.001890 | Commit Loss: 0.000778 | Perplexity: 1081.212728
2025-09-15 06:22:49,439 Stage: Train 0.5 | Epoch: 480 | Iter: 362000 | Total Loss: 0.002269 | Recon Loss: 0.001875 | Commit Loss: 0.000787 | Perplexity: 1081.995568
Trainning Epoch:  72%|███████▏  | 481/665 [13:50:31<5:18:09, 103.75s/it]2025-09-15 06:23:16,863 Stage: Train 0.5 | Epoch: 481 | Iter: 362200 | Total Loss: 0.002296 | Recon Loss: 0.001908 | Commit Loss: 0.000776 | Perplexity: 1076.380403
2025-09-15 06:23:44,351 Stage: Train 0.5 | Epoch: 481 | Iter: 362400 | Total Loss: 0.002308 | Recon Loss: 0.001921 | Commit Loss: 0.000774 | Perplexity: 1074.585460
2025-09-15 06:24:12,111 Stage: Train 0.5 | Epoch: 481 | Iter: 362600 | Total Loss: 0.002253 | Recon Loss: 0.001863 | Commit Loss: 0.000779 | Perplexity: 1079.650146
2025-09-15 06:24:39,553 Stage: Train 0.5 | Epoch: 481 | Iter: 362800 | Total Loss: 0.002292 | Recon Loss: 0.001900 | Commit Loss: 0.000785 | Perplexity: 1081.587672
Trainning Epoch:  72%|███████▏  | 482/665 [13:52:15<5:16:19, 103.71s/it]2025-09-15 06:25:07,050 Stage: Train 0.5 | Epoch: 482 | Iter: 363000 | Total Loss: 0.002258 | Recon Loss: 0.001867 | Commit Loss: 0.000782 | Perplexity: 1084.709425
2025-09-15 06:25:34,444 Stage: Train 0.5 | Epoch: 482 | Iter: 363200 | Total Loss: 0.002247 | Recon Loss: 0.001857 | Commit Loss: 0.000780 | Perplexity: 1080.047368
2025-09-15 06:26:01,889 Stage: Train 0.5 | Epoch: 482 | Iter: 363400 | Total Loss: 0.002265 | Recon Loss: 0.001876 | Commit Loss: 0.000777 | Perplexity: 1079.878283
2025-09-15 06:26:29,371 Stage: Train 0.5 | Epoch: 482 | Iter: 363600 | Total Loss: 0.002310 | Recon Loss: 0.001920 | Commit Loss: 0.000780 | Perplexity: 1080.185491
Trainning Epoch:  73%|███████▎  | 483/665 [13:53:58<5:14:18, 103.62s/it]2025-09-15 06:26:56,829 Stage: Train 0.5 | Epoch: 483 | Iter: 363800 | Total Loss: 0.002235 | Recon Loss: 0.001846 | Commit Loss: 0.000779 | Perplexity: 1078.735592
2025-09-15 06:27:24,323 Stage: Train 0.5 | Epoch: 483 | Iter: 364000 | Total Loss: 0.002280 | Recon Loss: 0.001891 | Commit Loss: 0.000777 | Perplexity: 1078.823288
2025-09-15 06:27:51,897 Stage: Train 0.5 | Epoch: 483 | Iter: 364200 | Total Loss: 0.002326 | Recon Loss: 0.001932 | Commit Loss: 0.000788 | Perplexity: 1083.730891
2025-09-15 06:28:19,612 Stage: Train 0.5 | Epoch: 483 | Iter: 364400 | Total Loss: 0.002259 | Recon Loss: 0.001871 | Commit Loss: 0.000775 | Perplexity: 1080.040492
Trainning Epoch:  73%|███████▎  | 484/665 [13:55:42<5:12:45, 103.67s/it]2025-09-15 06:28:47,415 Stage: Train 0.5 | Epoch: 484 | Iter: 364600 | Total Loss: 0.002269 | Recon Loss: 0.001880 | Commit Loss: 0.000779 | Perplexity: 1080.122511
2025-09-15 06:29:15,260 Stage: Train 0.5 | Epoch: 484 | Iter: 364800 | Total Loss: 0.002218 | Recon Loss: 0.001828 | Commit Loss: 0.000780 | Perplexity: 1080.094169
2025-09-15 06:29:42,781 Stage: Train 0.5 | Epoch: 484 | Iter: 365000 | Total Loss: 0.002300 | Recon Loss: 0.001910 | Commit Loss: 0.000780 | Perplexity: 1079.346322
2025-09-15 06:30:10,215 Stage: Train 0.5 | Epoch: 484 | Iter: 365200 | Total Loss: 0.002308 | Recon Loss: 0.001917 | Commit Loss: 0.000782 | Perplexity: 1081.268522
Trainning Epoch:  73%|███████▎  | 485/665 [13:57:26<5:11:24, 103.80s/it]2025-09-15 06:30:37,596 Stage: Train 0.5 | Epoch: 485 | Iter: 365400 | Total Loss: 0.002306 | Recon Loss: 0.001918 | Commit Loss: 0.000777 | Perplexity: 1077.073475
2025-09-15 06:31:05,178 Stage: Train 0.5 | Epoch: 485 | Iter: 365600 | Total Loss: 0.002253 | Recon Loss: 0.001863 | Commit Loss: 0.000779 | Perplexity: 1083.373886
2025-09-15 06:31:32,692 Stage: Train 0.5 | Epoch: 485 | Iter: 365800 | Total Loss: 0.002241 | Recon Loss: 0.001853 | Commit Loss: 0.000776 | Perplexity: 1079.252383
Trainning Epoch:  73%|███████▎  | 486/665 [13:59:10<5:09:33, 103.76s/it]2025-09-15 06:32:00,294 Stage: Train 0.5 | Epoch: 486 | Iter: 366000 | Total Loss: 0.002350 | Recon Loss: 0.001959 | Commit Loss: 0.000781 | Perplexity: 1077.201013
2025-09-15 06:32:27,777 Stage: Train 0.5 | Epoch: 486 | Iter: 366200 | Total Loss: 0.002207 | Recon Loss: 0.001817 | Commit Loss: 0.000779 | Perplexity: 1080.787437
2025-09-15 06:32:55,331 Stage: Train 0.5 | Epoch: 486 | Iter: 366400 | Total Loss: 0.002277 | Recon Loss: 0.001886 | Commit Loss: 0.000782 | Perplexity: 1080.408789
2025-09-15 06:33:22,901 Stage: Train 0.5 | Epoch: 486 | Iter: 366600 | Total Loss: 0.002286 | Recon Loss: 0.001896 | Commit Loss: 0.000779 | Perplexity: 1079.078186
Trainning Epoch:  73%|███████▎  | 487/665 [14:00:54<5:07:40, 103.71s/it]2025-09-15 06:33:50,404 Stage: Train 0.5 | Epoch: 487 | Iter: 366800 | Total Loss: 0.002244 | Recon Loss: 0.001855 | Commit Loss: 0.000778 | Perplexity: 1077.485446
2025-09-15 06:34:17,982 Stage: Train 0.5 | Epoch: 487 | Iter: 367000 | Total Loss: 0.002250 | Recon Loss: 0.001861 | Commit Loss: 0.000777 | Perplexity: 1081.684897
2025-09-15 06:34:45,624 Stage: Train 0.5 | Epoch: 487 | Iter: 367200 | Total Loss: 0.002283 | Recon Loss: 0.001892 | Commit Loss: 0.000782 | Perplexity: 1082.406939
2025-09-15 06:35:13,255 Stage: Train 0.5 | Epoch: 487 | Iter: 367400 | Total Loss: 0.002260 | Recon Loss: 0.001870 | Commit Loss: 0.000779 | Perplexity: 1080.368807
Trainning Epoch:  73%|███████▎  | 488/665 [14:02:38<5:06:15, 103.81s/it]2025-09-15 06:35:40,916 Stage: Train 0.5 | Epoch: 488 | Iter: 367600 | Total Loss: 0.002260 | Recon Loss: 0.001872 | Commit Loss: 0.000776 | Perplexity: 1075.107286
2025-09-15 06:36:08,476 Stage: Train 0.5 | Epoch: 488 | Iter: 367800 | Total Loss: 0.002207 | Recon Loss: 0.001820 | Commit Loss: 0.000773 | Perplexity: 1078.875358
2025-09-15 06:36:36,002 Stage: Train 0.5 | Epoch: 488 | Iter: 368000 | Total Loss: 0.002301 | Recon Loss: 0.001910 | Commit Loss: 0.000783 | Perplexity: 1083.060284
2025-09-15 06:37:03,530 Stage: Train 0.5 | Epoch: 488 | Iter: 368200 | Total Loss: 0.002238 | Recon Loss: 0.001846 | Commit Loss: 0.000783 | Perplexity: 1082.280748
Trainning Epoch:  74%|███████▎  | 489/665 [14:04:21<5:04:23, 103.77s/it]2025-09-15 06:37:31,060 Stage: Train 0.5 | Epoch: 489 | Iter: 368400 | Total Loss: 0.002251 | Recon Loss: 0.001861 | Commit Loss: 0.000780 | Perplexity: 1078.872452
2025-09-15 06:37:58,629 Stage: Train 0.5 | Epoch: 489 | Iter: 368600 | Total Loss: 0.002244 | Recon Loss: 0.001855 | Commit Loss: 0.000777 | Perplexity: 1078.250378
2025-09-15 06:38:26,073 Stage: Train 0.5 | Epoch: 489 | Iter: 368800 | Total Loss: 0.002280 | Recon Loss: 0.001888 | Commit Loss: 0.000783 | Perplexity: 1085.407158
Trainning Epoch:  74%|███████▎  | 490/665 [14:06:05<5:02:35, 103.75s/it]2025-09-15 06:38:53,636 Stage: Train 0.5 | Epoch: 490 | Iter: 369000 | Total Loss: 0.002211 | Recon Loss: 0.001823 | Commit Loss: 0.000776 | Perplexity: 1076.463395
2025-09-15 06:39:21,090 Stage: Train 0.5 | Epoch: 490 | Iter: 369200 | Total Loss: 0.002271 | Recon Loss: 0.001880 | Commit Loss: 0.000783 | Perplexity: 1084.976417
2025-09-15 06:39:48,490 Stage: Train 0.5 | Epoch: 490 | Iter: 369400 | Total Loss: 0.002224 | Recon Loss: 0.001838 | Commit Loss: 0.000773 | Perplexity: 1076.004896
2025-09-15 06:40:15,892 Stage: Train 0.5 | Epoch: 490 | Iter: 369600 | Total Loss: 0.002331 | Recon Loss: 0.001940 | Commit Loss: 0.000783 | Perplexity: 1079.940652
Trainning Epoch:  74%|███████▍  | 491/665 [14:07:48<5:00:22, 103.58s/it]2025-09-15 06:40:43,283 Stage: Train 0.5 | Epoch: 491 | Iter: 369800 | Total Loss: 0.002217 | Recon Loss: 0.001830 | Commit Loss: 0.000772 | Perplexity: 1076.017614
2025-09-15 06:41:10,683 Stage: Train 0.5 | Epoch: 491 | Iter: 370000 | Total Loss: 0.002244 | Recon Loss: 0.001856 | Commit Loss: 0.000775 | Perplexity: 1079.317710
2025-09-15 06:41:38,041 Stage: Train 0.5 | Epoch: 491 | Iter: 370200 | Total Loss: 0.002240 | Recon Loss: 0.001846 | Commit Loss: 0.000790 | Perplexity: 1086.224265
2025-09-15 06:42:05,389 Stage: Train 0.5 | Epoch: 491 | Iter: 370400 | Total Loss: 0.002233 | Recon Loss: 0.001846 | Commit Loss: 0.000775 | Perplexity: 1079.440027
Trainning Epoch:  74%|███████▍  | 492/665 [14:09:31<4:58:15, 103.44s/it]2025-09-15 06:42:32,831 Stage: Train 0.5 | Epoch: 492 | Iter: 370600 | Total Loss: 0.002275 | Recon Loss: 0.001883 | Commit Loss: 0.000783 | Perplexity: 1079.590651
2025-09-15 06:43:00,467 Stage: Train 0.5 | Epoch: 492 | Iter: 370800 | Total Loss: 0.002277 | Recon Loss: 0.001888 | Commit Loss: 0.000778 | Perplexity: 1080.288481
2025-09-15 06:43:28,211 Stage: Train 0.5 | Epoch: 492 | Iter: 371000 | Total Loss: 0.002239 | Recon Loss: 0.001851 | Commit Loss: 0.000776 | Perplexity: 1080.315347
2025-09-15 06:43:55,740 Stage: Train 0.5 | Epoch: 492 | Iter: 371200 | Total Loss: 0.002206 | Recon Loss: 0.001816 | Commit Loss: 0.000780 | Perplexity: 1084.495468
Trainning Epoch:  74%|███████▍  | 493/665 [14:11:15<4:56:52, 103.56s/it]2025-09-15 06:44:23,147 Stage: Train 0.5 | Epoch: 493 | Iter: 371400 | Total Loss: 0.002225 | Recon Loss: 0.001840 | Commit Loss: 0.000769 | Perplexity: 1074.113306
2025-09-15 06:44:50,534 Stage: Train 0.5 | Epoch: 493 | Iter: 371600 | Total Loss: 0.002287 | Recon Loss: 0.001899 | Commit Loss: 0.000776 | Perplexity: 1078.640655
2025-09-15 06:45:18,202 Stage: Train 0.5 | Epoch: 493 | Iter: 371800 | Total Loss: 0.002247 | Recon Loss: 0.001855 | Commit Loss: 0.000784 | Perplexity: 1083.638066
Trainning Epoch:  74%|███████▍  | 494/665 [14:12:59<4:55:13, 103.59s/it]2025-09-15 06:45:45,825 Stage: Train 0.5 | Epoch: 494 | Iter: 372000 | Total Loss: 0.002265 | Recon Loss: 0.001875 | Commit Loss: 0.000780 | Perplexity: 1080.999262
2025-09-15 06:46:13,414 Stage: Train 0.5 | Epoch: 494 | Iter: 372200 | Total Loss: 0.002308 | Recon Loss: 0.001923 | Commit Loss: 0.000771 | Perplexity: 1079.106490
2025-09-15 06:46:40,890 Stage: Train 0.5 | Epoch: 494 | Iter: 372400 | Total Loss: 0.002269 | Recon Loss: 0.001878 | Commit Loss: 0.000781 | Perplexity: 1080.635650
2025-09-15 06:47:08,368 Stage: Train 0.5 | Epoch: 494 | Iter: 372600 | Total Loss: 0.002258 | Recon Loss: 0.001869 | Commit Loss: 0.000778 | Perplexity: 1078.115316
Trainning Epoch:  74%|███████▍  | 495/665 [14:14:42<4:53:26, 103.57s/it]2025-09-15 06:47:35,776 Stage: Train 0.5 | Epoch: 495 | Iter: 372800 | Total Loss: 0.002262 | Recon Loss: 0.001875 | Commit Loss: 0.000774 | Perplexity: 1078.672123
2025-09-15 06:48:03,179 Stage: Train 0.5 | Epoch: 495 | Iter: 373000 | Total Loss: 0.002266 | Recon Loss: 0.001878 | Commit Loss: 0.000777 | Perplexity: 1081.865177
2025-09-15 06:48:30,591 Stage: Train 0.5 | Epoch: 495 | Iter: 373200 | Total Loss: 0.002270 | Recon Loss: 0.001882 | Commit Loss: 0.000774 | Perplexity: 1078.706381
2025-09-15 06:48:58,167 Stage: Train 0.5 | Epoch: 495 | Iter: 373400 | Total Loss: 0.002284 | Recon Loss: 0.001894 | Commit Loss: 0.000780 | Perplexity: 1081.973304
Trainning Epoch:  75%|███████▍  | 496/665 [14:16:26<4:51:32, 103.51s/it]2025-09-15 06:49:25,596 Stage: Train 0.5 | Epoch: 496 | Iter: 373600 | Total Loss: 0.002203 | Recon Loss: 0.001815 | Commit Loss: 0.000775 | Perplexity: 1076.176204
2025-09-15 06:49:53,106 Stage: Train 0.5 | Epoch: 496 | Iter: 373800 | Total Loss: 0.002237 | Recon Loss: 0.001848 | Commit Loss: 0.000778 | Perplexity: 1082.654458
2025-09-15 06:50:20,607 Stage: Train 0.5 | Epoch: 496 | Iter: 374000 | Total Loss: 0.002239 | Recon Loss: 0.001849 | Commit Loss: 0.000780 | Perplexity: 1083.544480
2025-09-15 06:50:48,086 Stage: Train 0.5 | Epoch: 496 | Iter: 374200 | Total Loss: 0.002256 | Recon Loss: 0.001868 | Commit Loss: 0.000775 | Perplexity: 1075.523637
Trainning Epoch:  75%|███████▍  | 497/665 [14:18:09<4:49:46, 103.49s/it]2025-09-15 06:51:15,522 Stage: Train 0.5 | Epoch: 497 | Iter: 374400 | Total Loss: 0.002243 | Recon Loss: 0.001858 | Commit Loss: 0.000770 | Perplexity: 1076.845461
2025-09-15 06:51:43,016 Stage: Train 0.5 | Epoch: 497 | Iter: 374600 | Total Loss: 0.002251 | Recon Loss: 0.001861 | Commit Loss: 0.000780 | Perplexity: 1083.608204
2025-09-15 06:52:10,582 Stage: Train 0.5 | Epoch: 497 | Iter: 374800 | Total Loss: 0.002223 | Recon Loss: 0.001834 | Commit Loss: 0.000779 | Perplexity: 1079.753654
Trainning Epoch:  75%|███████▍  | 498/665 [14:19:53<4:48:09, 103.53s/it]2025-09-15 06:52:38,100 Stage: Train 0.5 | Epoch: 498 | Iter: 375000 | Total Loss: 0.002231 | Recon Loss: 0.001844 | Commit Loss: 0.000775 | Perplexity: 1076.412068
2025-09-15 06:53:05,578 Stage: Train 0.5 | Epoch: 498 | Iter: 375200 | Total Loss: 0.002267 | Recon Loss: 0.001879 | Commit Loss: 0.000777 | Perplexity: 1079.971066
2025-09-15 06:53:33,110 Stage: Train 0.5 | Epoch: 498 | Iter: 375400 | Total Loss: 0.002279 | Recon Loss: 0.001891 | Commit Loss: 0.000777 | Perplexity: 1080.484260
2025-09-15 06:54:00,581 Stage: Train 0.5 | Epoch: 498 | Iter: 375600 | Total Loss: 0.002225 | Recon Loss: 0.001837 | Commit Loss: 0.000777 | Perplexity: 1081.341546
Trainning Epoch:  75%|███████▌  | 499/665 [14:21:36<4:46:18, 103.49s/it]2025-09-15 06:54:27,943 Stage: Train 0.5 | Epoch: 499 | Iter: 375800 | Total Loss: 0.002266 | Recon Loss: 0.001878 | Commit Loss: 0.000776 | Perplexity: 1074.952205
2025-09-15 06:54:55,333 Stage: Train 0.5 | Epoch: 499 | Iter: 376000 | Total Loss: 0.002208 | Recon Loss: 0.001822 | Commit Loss: 0.000770 | Perplexity: 1079.331714
2025-09-15 06:55:22,732 Stage: Train 0.5 | Epoch: 499 | Iter: 376200 | Total Loss: 0.002315 | Recon Loss: 0.001925 | Commit Loss: 0.000780 | Perplexity: 1077.386030
2025-09-15 06:55:50,220 Stage: Train 0.5 | Epoch: 499 | Iter: 376400 | Total Loss: 0.002208 | Recon Loss: 0.001819 | Commit Loss: 0.000777 | Perplexity: 1082.957691
Trainning Epoch:  75%|███████▌  | 500/665 [14:23:19<4:44:21, 103.40s/it]2025-09-15 06:56:17,578 Stage: Train 0.5 | Epoch: 500 | Iter: 376600 | Total Loss: 0.002244 | Recon Loss: 0.001859 | Commit Loss: 0.000770 | Perplexity: 1071.989111
2025-09-15 06:56:45,087 Stage: Train 0.5 | Epoch: 500 | Iter: 376800 | Total Loss: 0.002225 | Recon Loss: 0.001837 | Commit Loss: 0.000776 | Perplexity: 1079.577817
2025-09-15 06:57:12,777 Stage: Train 0.5 | Epoch: 500 | Iter: 377000 | Total Loss: 0.002237 | Recon Loss: 0.001848 | Commit Loss: 0.000779 | Perplexity: 1083.613264
2025-09-15 06:57:40,524 Stage: Train 0.5 | Epoch: 500 | Iter: 377200 | Total Loss: 0.002265 | Recon Loss: 0.001876 | Commit Loss: 0.000778 | Perplexity: 1078.968600
Trainning Epoch:  75%|███████▌  | 501/665 [14:25:03<4:43:01, 103.55s/it]2025-09-15 06:58:07,927 Stage: Train 0.5 | Epoch: 501 | Iter: 377400 | Total Loss: 0.002243 | Recon Loss: 0.001854 | Commit Loss: 0.000776 | Perplexity: 1082.241105
2025-09-15 06:58:35,363 Stage: Train 0.5 | Epoch: 501 | Iter: 377600 | Total Loss: 0.002220 | Recon Loss: 0.001831 | Commit Loss: 0.000778 | Perplexity: 1080.741826
2025-09-15 06:59:02,775 Stage: Train 0.5 | Epoch: 501 | Iter: 377800 | Total Loss: 0.002251 | Recon Loss: 0.001866 | Commit Loss: 0.000772 | Perplexity: 1074.941934
2025-09-15 06:59:30,329 Stage: Train 0.5 | Epoch: 501 | Iter: 378000 | Total Loss: 0.002253 | Recon Loss: 0.001863 | Commit Loss: 0.000780 | Perplexity: 1084.141284
Trainning Epoch:  75%|███████▌  | 502/665 [14:26:47<4:41:08, 103.49s/it]2025-09-15 06:59:57,744 Stage: Train 0.5 | Epoch: 502 | Iter: 378200 | Total Loss: 0.002235 | Recon Loss: 0.001848 | Commit Loss: 0.000773 | Perplexity: 1077.637375
2025-09-15 07:00:25,298 Stage: Train 0.5 | Epoch: 502 | Iter: 378400 | Total Loss: 0.002241 | Recon Loss: 0.001853 | Commit Loss: 0.000776 | Perplexity: 1077.865728
2025-09-15 07:00:52,832 Stage: Train 0.5 | Epoch: 502 | Iter: 378600 | Total Loss: 0.002220 | Recon Loss: 0.001831 | Commit Loss: 0.000778 | Perplexity: 1081.173793
Trainning Epoch:  76%|███████▌  | 503/665 [14:28:30<4:39:28, 103.51s/it]2025-09-15 07:01:20,303 Stage: Train 0.5 | Epoch: 503 | Iter: 378800 | Total Loss: 0.002239 | Recon Loss: 0.001852 | Commit Loss: 0.000774 | Perplexity: 1079.150243
2025-09-15 07:01:47,800 Stage: Train 0.5 | Epoch: 503 | Iter: 379000 | Total Loss: 0.002245 | Recon Loss: 0.001859 | Commit Loss: 0.000772 | Perplexity: 1081.889849
2025-09-15 07:02:15,364 Stage: Train 0.5 | Epoch: 503 | Iter: 379200 | Total Loss: 0.002223 | Recon Loss: 0.001835 | Commit Loss: 0.000778 | Perplexity: 1077.509550
2025-09-15 07:02:43,061 Stage: Train 0.5 | Epoch: 503 | Iter: 379400 | Total Loss: 0.002229 | Recon Loss: 0.001842 | Commit Loss: 0.000774 | Perplexity: 1080.620204
Trainning Epoch:  76%|███████▌  | 504/665 [14:30:14<4:37:58, 103.59s/it]2025-09-15 07:03:10,563 Stage: Train 0.5 | Epoch: 504 | Iter: 379600 | Total Loss: 0.002303 | Recon Loss: 0.001914 | Commit Loss: 0.000776 | Perplexity: 1080.939971
2025-09-15 07:03:38,068 Stage: Train 0.5 | Epoch: 504 | Iter: 379800 | Total Loss: 0.002177 | Recon Loss: 0.001793 | Commit Loss: 0.000768 | Perplexity: 1077.336014
2025-09-15 07:04:05,560 Stage: Train 0.5 | Epoch: 504 | Iter: 380000 | Total Loss: 0.002235 | Recon Loss: 0.001847 | Commit Loss: 0.000776 | Perplexity: 1078.777389
2025-09-15 07:04:05,560 Saving model at iteration 380000
2025-09-15 07:04:05,727 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_505_step_380000
2025-09-15 07:04:05,931 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_505_step_380000/pytorch_model.bin
2025-09-15 07:04:06,256 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_505_step_380000/optimizer.bin
2025-09-15 07:04:06,256 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_505_step_380000/scheduler.bin
2025-09-15 07:04:06,257 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_505_step_380000/random_states_0.pkl
2025-09-15 07:04:33,896 Stage: Train 0.5 | Epoch: 504 | Iter: 380200 | Total Loss: 0.002249 | Recon Loss: 0.001859 | Commit Loss: 0.000781 | Perplexity: 1082.244463
Trainning Epoch:  76%|███████▌  | 505/665 [14:31:58<4:36:51, 103.82s/it]2025-09-15 07:05:01,377 Stage: Train 0.5 | Epoch: 505 | Iter: 380400 | Total Loss: 0.002280 | Recon Loss: 0.001891 | Commit Loss: 0.000779 | Perplexity: 1077.595958
2025-09-15 07:05:28,899 Stage: Train 0.5 | Epoch: 505 | Iter: 380600 | Total Loss: 0.002194 | Recon Loss: 0.001809 | Commit Loss: 0.000769 | Perplexity: 1077.696467
2025-09-15 07:05:56,407 Stage: Train 0.5 | Epoch: 505 | Iter: 380800 | Total Loss: 0.002215 | Recon Loss: 0.001829 | Commit Loss: 0.000772 | Perplexity: 1082.049853
2025-09-15 07:06:23,920 Stage: Train 0.5 | Epoch: 505 | Iter: 381000 | Total Loss: 0.002269 | Recon Loss: 0.001878 | Commit Loss: 0.000782 | Perplexity: 1083.684982
Trainning Epoch:  76%|███████▌  | 506/665 [14:33:42<4:34:54, 103.74s/it]2025-09-15 07:06:51,485 Stage: Train 0.5 | Epoch: 506 | Iter: 381200 | Total Loss: 0.002217 | Recon Loss: 0.001833 | Commit Loss: 0.000768 | Perplexity: 1074.135427
2025-09-15 07:07:18,987 Stage: Train 0.5 | Epoch: 506 | Iter: 381400 | Total Loss: 0.002262 | Recon Loss: 0.001874 | Commit Loss: 0.000777 | Perplexity: 1082.217171
2025-09-15 07:07:46,575 Stage: Train 0.5 | Epoch: 506 | Iter: 381600 | Total Loss: 0.002257 | Recon Loss: 0.001870 | Commit Loss: 0.000776 | Perplexity: 1078.932176
Trainning Epoch:  76%|███████▌  | 507/665 [14:35:26<4:33:07, 103.72s/it]2025-09-15 07:08:14,007 Stage: Train 0.5 | Epoch: 507 | Iter: 381800 | Total Loss: 0.002217 | Recon Loss: 0.001829 | Commit Loss: 0.000777 | Perplexity: 1079.973655
2025-09-15 07:08:41,373 Stage: Train 0.5 | Epoch: 507 | Iter: 382000 | Total Loss: 0.002242 | Recon Loss: 0.001857 | Commit Loss: 0.000771 | Perplexity: 1076.773019
2025-09-15 07:09:08,843 Stage: Train 0.5 | Epoch: 507 | Iter: 382200 | Total Loss: 0.002259 | Recon Loss: 0.001872 | Commit Loss: 0.000773 | Perplexity: 1080.733903
2025-09-15 07:09:36,273 Stage: Train 0.5 | Epoch: 507 | Iter: 382400 | Total Loss: 0.002269 | Recon Loss: 0.001879 | Commit Loss: 0.000780 | Perplexity: 1083.413610
Trainning Epoch:  76%|███████▋  | 508/665 [14:37:09<4:31:00, 103.57s/it]2025-09-15 07:10:03,727 Stage: Train 0.5 | Epoch: 508 | Iter: 382600 | Total Loss: 0.002248 | Recon Loss: 0.001861 | Commit Loss: 0.000774 | Perplexity: 1080.819240
2025-09-15 07:10:31,304 Stage: Train 0.5 | Epoch: 508 | Iter: 382800 | Total Loss: 0.002258 | Recon Loss: 0.001872 | Commit Loss: 0.000772 | Perplexity: 1077.876476
2025-09-15 07:10:58,940 Stage: Train 0.5 | Epoch: 508 | Iter: 383000 | Total Loss: 0.002233 | Recon Loss: 0.001848 | Commit Loss: 0.000771 | Perplexity: 1077.144566
2025-09-15 07:11:26,534 Stage: Train 0.5 | Epoch: 508 | Iter: 383200 | Total Loss: 0.002264 | Recon Loss: 0.001873 | Commit Loss: 0.000781 | Perplexity: 1079.837784
Trainning Epoch:  77%|███████▋  | 509/665 [14:38:53<4:29:30, 103.66s/it]2025-09-15 07:11:54,031 Stage: Train 0.5 | Epoch: 509 | Iter: 383400 | Total Loss: 0.002262 | Recon Loss: 0.001874 | Commit Loss: 0.000777 | Perplexity: 1081.671609
2025-09-15 07:12:21,538 Stage: Train 0.5 | Epoch: 509 | Iter: 383600 | Total Loss: 0.002239 | Recon Loss: 0.001850 | Commit Loss: 0.000777 | Perplexity: 1080.694830
2025-09-15 07:12:49,110 Stage: Train 0.5 | Epoch: 509 | Iter: 383800 | Total Loss: 0.002234 | Recon Loss: 0.001846 | Commit Loss: 0.000776 | Perplexity: 1082.626703
2025-09-15 07:13:16,787 Stage: Train 0.5 | Epoch: 509 | Iter: 384000 | Total Loss: 0.002207 | Recon Loss: 0.001820 | Commit Loss: 0.000774 | Perplexity: 1079.298471
Trainning Epoch:  77%|███████▋  | 510/665 [14:40:37<4:27:58, 103.73s/it]2025-09-15 07:13:44,440 Stage: Train 0.5 | Epoch: 510 | Iter: 384200 | Total Loss: 0.002253 | Recon Loss: 0.001866 | Commit Loss: 0.000773 | Perplexity: 1077.669971
2025-09-15 07:14:11,933 Stage: Train 0.5 | Epoch: 510 | Iter: 384400 | Total Loss: 0.002248 | Recon Loss: 0.001862 | Commit Loss: 0.000772 | Perplexity: 1077.649738
2025-09-15 07:14:39,577 Stage: Train 0.5 | Epoch: 510 | Iter: 384600 | Total Loss: 0.002235 | Recon Loss: 0.001849 | Commit Loss: 0.000771 | Perplexity: 1083.177344
Trainning Epoch:  77%|███████▋  | 511/665 [14:42:20<4:26:10, 103.71s/it]2025-09-15 07:15:07,002 Stage: Train 0.5 | Epoch: 511 | Iter: 384800 | Total Loss: 0.002289 | Recon Loss: 0.001903 | Commit Loss: 0.000773 | Perplexity: 1072.577425
2025-09-15 07:15:34,476 Stage: Train 0.5 | Epoch: 511 | Iter: 385000 | Total Loss: 0.002214 | Recon Loss: 0.001829 | Commit Loss: 0.000770 | Perplexity: 1078.207127
2025-09-15 07:16:01,977 Stage: Train 0.5 | Epoch: 511 | Iter: 385200 | Total Loss: 0.002252 | Recon Loss: 0.001867 | Commit Loss: 0.000771 | Perplexity: 1077.224620
2025-09-15 07:16:29,664 Stage: Train 0.5 | Epoch: 511 | Iter: 385400 | Total Loss: 0.002226 | Recon Loss: 0.001840 | Commit Loss: 0.000773 | Perplexity: 1080.302598
Trainning Epoch:  77%|███████▋  | 512/665 [14:44:04<4:24:26, 103.70s/it]2025-09-15 07:16:57,162 Stage: Train 0.5 | Epoch: 512 | Iter: 385600 | Total Loss: 0.002252 | Recon Loss: 0.001864 | Commit Loss: 0.000776 | Perplexity: 1079.718547
2025-09-15 07:17:24,813 Stage: Train 0.5 | Epoch: 512 | Iter: 385800 | Total Loss: 0.002205 | Recon Loss: 0.001817 | Commit Loss: 0.000777 | Perplexity: 1083.935291
2025-09-15 07:17:52,428 Stage: Train 0.5 | Epoch: 512 | Iter: 386000 | Total Loss: 0.002305 | Recon Loss: 0.001919 | Commit Loss: 0.000772 | Perplexity: 1075.310700
2025-09-15 07:18:19,951 Stage: Train 0.5 | Epoch: 512 | Iter: 386200 | Total Loss: 0.002280 | Recon Loss: 0.001893 | Commit Loss: 0.000773 | Perplexity: 1079.942587
Trainning Epoch:  77%|███████▋  | 513/665 [14:45:48<4:22:49, 103.74s/it]2025-09-15 07:18:47,477 Stage: Train 0.5 | Epoch: 513 | Iter: 386400 | Total Loss: 0.002211 | Recon Loss: 0.001825 | Commit Loss: 0.000772 | Perplexity: 1077.212438
2025-09-15 07:19:14,990 Stage: Train 0.5 | Epoch: 513 | Iter: 386600 | Total Loss: 0.002233 | Recon Loss: 0.001846 | Commit Loss: 0.000773 | Perplexity: 1081.852459
2025-09-15 07:19:42,460 Stage: Train 0.5 | Epoch: 513 | Iter: 386800 | Total Loss: 0.002284 | Recon Loss: 0.001899 | Commit Loss: 0.000770 | Perplexity: 1076.096103
2025-09-15 07:20:09,941 Stage: Train 0.5 | Epoch: 513 | Iter: 387000 | Total Loss: 0.002226 | Recon Loss: 0.001838 | Commit Loss: 0.000776 | Perplexity: 1081.378896
Trainning Epoch:  77%|███████▋  | 514/665 [14:47:31<4:20:53, 103.67s/it]2025-09-15 07:20:37,375 Stage: Train 0.5 | Epoch: 514 | Iter: 387200 | Total Loss: 0.002227 | Recon Loss: 0.001839 | Commit Loss: 0.000775 | Perplexity: 1081.792328
2025-09-15 07:21:04,881 Stage: Train 0.5 | Epoch: 514 | Iter: 387400 | Total Loss: 0.002251 | Recon Loss: 0.001867 | Commit Loss: 0.000768 | Perplexity: 1078.107241
2025-09-15 07:21:32,477 Stage: Train 0.5 | Epoch: 514 | Iter: 387600 | Total Loss: 0.002195 | Recon Loss: 0.001810 | Commit Loss: 0.000768 | Perplexity: 1079.595453
Trainning Epoch:  77%|███████▋  | 515/665 [14:49:15<4:19:05, 103.63s/it]2025-09-15 07:21:59,938 Stage: Train 0.5 | Epoch: 515 | Iter: 387800 | Total Loss: 0.002236 | Recon Loss: 0.001848 | Commit Loss: 0.000776 | Perplexity: 1081.671527
2025-09-15 07:22:27,420 Stage: Train 0.5 | Epoch: 515 | Iter: 388000 | Total Loss: 0.002224 | Recon Loss: 0.001840 | Commit Loss: 0.000768 | Perplexity: 1079.456072
2025-09-15 07:22:54,899 Stage: Train 0.5 | Epoch: 515 | Iter: 388200 | Total Loss: 0.002239 | Recon Loss: 0.001854 | Commit Loss: 0.000770 | Perplexity: 1079.276681
2025-09-15 07:23:22,423 Stage: Train 0.5 | Epoch: 515 | Iter: 388400 | Total Loss: 0.002190 | Recon Loss: 0.001801 | Commit Loss: 0.000776 | Perplexity: 1081.634372
Trainning Epoch:  78%|███████▊  | 516/665 [14:50:58<4:17:13, 103.58s/it]2025-09-15 07:23:49,843 Stage: Train 0.5 | Epoch: 516 | Iter: 388600 | Total Loss: 0.002237 | Recon Loss: 0.001851 | Commit Loss: 0.000773 | Perplexity: 1079.785751
2025-09-15 07:24:17,379 Stage: Train 0.5 | Epoch: 516 | Iter: 388800 | Total Loss: 0.002229 | Recon Loss: 0.001845 | Commit Loss: 0.000768 | Perplexity: 1078.244051
2025-09-15 07:24:45,048 Stage: Train 0.5 | Epoch: 516 | Iter: 389000 | Total Loss: 0.002234 | Recon Loss: 0.001846 | Commit Loss: 0.000776 | Perplexity: 1082.504492
2025-09-15 07:25:12,591 Stage: Train 0.5 | Epoch: 516 | Iter: 389200 | Total Loss: 0.002259 | Recon Loss: 0.001870 | Commit Loss: 0.000777 | Perplexity: 1080.736717
Trainning Epoch:  78%|███████▊  | 517/665 [14:52:42<4:15:36, 103.63s/it]2025-09-15 07:25:40,074 Stage: Train 0.5 | Epoch: 517 | Iter: 389400 | Total Loss: 0.002278 | Recon Loss: 0.001895 | Commit Loss: 0.000767 | Perplexity: 1077.787811
2025-09-15 07:26:07,631 Stage: Train 0.5 | Epoch: 517 | Iter: 389600 | Total Loss: 0.002182 | Recon Loss: 0.001796 | Commit Loss: 0.000771 | Perplexity: 1080.061285
2025-09-15 07:26:35,139 Stage: Train 0.5 | Epoch: 517 | Iter: 389800 | Total Loss: 0.002249 | Recon Loss: 0.001862 | Commit Loss: 0.000773 | Perplexity: 1083.779831
2025-09-15 07:27:02,759 Stage: Train 0.5 | Epoch: 517 | Iter: 390000 | Total Loss: 0.002224 | Recon Loss: 0.001840 | Commit Loss: 0.000769 | Perplexity: 1078.590059
Trainning Epoch:  78%|███████▊  | 518/665 [14:54:26<4:13:56, 103.65s/it]2025-09-15 07:27:30,244 Stage: Train 0.5 | Epoch: 518 | Iter: 390200 | Total Loss: 0.002217 | Recon Loss: 0.001831 | Commit Loss: 0.000772 | Perplexity: 1079.442739
2025-09-15 07:27:57,743 Stage: Train 0.5 | Epoch: 518 | Iter: 390400 | Total Loss: 0.002220 | Recon Loss: 0.001835 | Commit Loss: 0.000771 | Perplexity: 1081.530369
2025-09-15 07:28:25,281 Stage: Train 0.5 | Epoch: 518 | Iter: 390600 | Total Loss: 0.002204 | Recon Loss: 0.001817 | Commit Loss: 0.000774 | Perplexity: 1081.765714
2025-09-15 07:28:52,676 Stage: Train 0.5 | Epoch: 518 | Iter: 390800 | Total Loss: 0.002212 | Recon Loss: 0.001825 | Commit Loss: 0.000775 | Perplexity: 1079.245078
Trainning Epoch:  78%|███████▊  | 519/665 [14:56:09<4:12:04, 103.59s/it]2025-09-15 07:29:20,021 Stage: Train 0.5 | Epoch: 519 | Iter: 391000 | Total Loss: 0.002258 | Recon Loss: 0.001873 | Commit Loss: 0.000770 | Perplexity: 1077.057577
2025-09-15 07:29:47,412 Stage: Train 0.5 | Epoch: 519 | Iter: 391200 | Total Loss: 0.002181 | Recon Loss: 0.001794 | Commit Loss: 0.000775 | Perplexity: 1085.761418
2025-09-15 07:30:14,829 Stage: Train 0.5 | Epoch: 519 | Iter: 391400 | Total Loss: 0.002265 | Recon Loss: 0.001879 | Commit Loss: 0.000771 | Perplexity: 1081.989426
Trainning Epoch:  78%|███████▊  | 520/665 [14:57:52<4:10:01, 103.46s/it]2025-09-15 07:30:42,238 Stage: Train 0.5 | Epoch: 520 | Iter: 391600 | Total Loss: 0.002222 | Recon Loss: 0.001839 | Commit Loss: 0.000768 | Perplexity: 1074.067797
2025-09-15 07:31:09,652 Stage: Train 0.5 | Epoch: 520 | Iter: 391800 | Total Loss: 0.002257 | Recon Loss: 0.001868 | Commit Loss: 0.000777 | Perplexity: 1084.014783
2025-09-15 07:31:37,163 Stage: Train 0.5 | Epoch: 520 | Iter: 392000 | Total Loss: 0.002208 | Recon Loss: 0.001825 | Commit Loss: 0.000767 | Perplexity: 1077.534496
2025-09-15 07:32:04,571 Stage: Train 0.5 | Epoch: 520 | Iter: 392200 | Total Loss: 0.002252 | Recon Loss: 0.001865 | Commit Loss: 0.000774 | Perplexity: 1081.297215
Trainning Epoch:  78%|███████▊  | 521/665 [14:59:36<4:08:10, 103.41s/it]2025-09-15 07:32:31,977 Stage: Train 0.5 | Epoch: 521 | Iter: 392400 | Total Loss: 0.002214 | Recon Loss: 0.001830 | Commit Loss: 0.000768 | Perplexity: 1070.692184
2025-09-15 07:32:59,568 Stage: Train 0.5 | Epoch: 521 | Iter: 392600 | Total Loss: 0.002222 | Recon Loss: 0.001834 | Commit Loss: 0.000777 | Perplexity: 1082.349143
2025-09-15 07:33:27,033 Stage: Train 0.5 | Epoch: 521 | Iter: 392800 | Total Loss: 0.002219 | Recon Loss: 0.001835 | Commit Loss: 0.000768 | Perplexity: 1081.235716
2025-09-15 07:33:54,495 Stage: Train 0.5 | Epoch: 521 | Iter: 393000 | Total Loss: 0.002219 | Recon Loss: 0.001833 | Commit Loss: 0.000772 | Perplexity: 1081.951622
Trainning Epoch:  78%|███████▊  | 522/665 [15:01:19<4:06:31, 103.44s/it]2025-09-15 07:34:22,031 Stage: Train 0.5 | Epoch: 522 | Iter: 393200 | Total Loss: 0.002227 | Recon Loss: 0.001844 | Commit Loss: 0.000767 | Perplexity: 1077.862273
2025-09-15 07:34:49,722 Stage: Train 0.5 | Epoch: 522 | Iter: 393400 | Total Loss: 0.002260 | Recon Loss: 0.001872 | Commit Loss: 0.000775 | Perplexity: 1082.867872
2025-09-15 07:35:17,214 Stage: Train 0.5 | Epoch: 522 | Iter: 393600 | Total Loss: 0.002243 | Recon Loss: 0.001857 | Commit Loss: 0.000772 | Perplexity: 1080.923627
2025-09-15 07:35:44,611 Stage: Train 0.5 | Epoch: 522 | Iter: 393800 | Total Loss: 0.002187 | Recon Loss: 0.001804 | Commit Loss: 0.000767 | Perplexity: 1079.653579
Trainning Epoch:  79%|███████▊  | 523/665 [15:03:03<4:04:56, 103.50s/it]2025-09-15 07:36:11,979 Stage: Train 0.5 | Epoch: 523 | Iter: 394000 | Total Loss: 0.002228 | Recon Loss: 0.001844 | Commit Loss: 0.000767 | Perplexity: 1076.661022
2025-09-15 07:36:39,408 Stage: Train 0.5 | Epoch: 523 | Iter: 394200 | Total Loss: 0.002239 | Recon Loss: 0.001854 | Commit Loss: 0.000771 | Perplexity: 1081.253274
2025-09-15 07:37:06,829 Stage: Train 0.5 | Epoch: 523 | Iter: 394400 | Total Loss: 0.002222 | Recon Loss: 0.001837 | Commit Loss: 0.000769 | Perplexity: 1080.507379
Trainning Epoch:  79%|███████▉  | 524/665 [15:04:46<4:03:03, 103.43s/it]2025-09-15 07:37:34,289 Stage: Train 0.5 | Epoch: 524 | Iter: 394600 | Total Loss: 0.002255 | Recon Loss: 0.001870 | Commit Loss: 0.000771 | Perplexity: 1080.158985
2025-09-15 07:38:01,756 Stage: Train 0.5 | Epoch: 524 | Iter: 394800 | Total Loss: 0.002197 | Recon Loss: 0.001812 | Commit Loss: 0.000770 | Perplexity: 1083.779155
2025-09-15 07:38:29,327 Stage: Train 0.5 | Epoch: 524 | Iter: 395000 | Total Loss: 0.002256 | Recon Loss: 0.001872 | Commit Loss: 0.000768 | Perplexity: 1080.224776
2025-09-15 07:38:56,842 Stage: Train 0.5 | Epoch: 524 | Iter: 395200 | Total Loss: 0.002259 | Recon Loss: 0.001872 | Commit Loss: 0.000775 | Perplexity: 1081.352338
Trainning Epoch:  79%|███████▉  | 525/665 [15:06:30<4:01:26, 103.48s/it]2025-09-15 07:39:24,338 Stage: Train 0.5 | Epoch: 525 | Iter: 395400 | Total Loss: 0.002191 | Recon Loss: 0.001806 | Commit Loss: 0.000770 | Perplexity: 1081.977636
2025-09-15 07:39:51,815 Stage: Train 0.5 | Epoch: 525 | Iter: 395600 | Total Loss: 0.002218 | Recon Loss: 0.001832 | Commit Loss: 0.000772 | Perplexity: 1087.061719
2025-09-15 07:40:19,507 Stage: Train 0.5 | Epoch: 525 | Iter: 395800 | Total Loss: 0.002206 | Recon Loss: 0.001823 | Commit Loss: 0.000765 | Perplexity: 1080.509164
2025-09-15 07:40:46,887 Stage: Train 0.5 | Epoch: 525 | Iter: 396000 | Total Loss: 0.002211 | Recon Loss: 0.001828 | Commit Loss: 0.000767 | Perplexity: 1076.791250
Trainning Epoch:  79%|███████▉  | 526/665 [15:08:13<3:59:44, 103.48s/it]2025-09-15 07:41:14,249 Stage: Train 0.5 | Epoch: 526 | Iter: 396200 | Total Loss: 0.002241 | Recon Loss: 0.001857 | Commit Loss: 0.000768 | Perplexity: 1076.237952
2025-09-15 07:41:41,753 Stage: Train 0.5 | Epoch: 526 | Iter: 396400 | Total Loss: 0.002243 | Recon Loss: 0.001857 | Commit Loss: 0.000772 | Perplexity: 1084.108413
2025-09-15 07:42:09,162 Stage: Train 0.5 | Epoch: 526 | Iter: 396600 | Total Loss: 0.002188 | Recon Loss: 0.001803 | Commit Loss: 0.000770 | Perplexity: 1080.230019
2025-09-15 07:42:36,552 Stage: Train 0.5 | Epoch: 526 | Iter: 396800 | Total Loss: 0.002235 | Recon Loss: 0.001851 | Commit Loss: 0.000767 | Perplexity: 1078.358656
Trainning Epoch:  79%|███████▉  | 527/665 [15:09:56<3:57:49, 103.40s/it]2025-09-15 07:43:03,901 Stage: Train 0.5 | Epoch: 527 | Iter: 397000 | Total Loss: 0.002227 | Recon Loss: 0.001843 | Commit Loss: 0.000767 | Perplexity: 1082.973886
2025-09-15 07:43:31,379 Stage: Train 0.5 | Epoch: 527 | Iter: 397200 | Total Loss: 0.002204 | Recon Loss: 0.001819 | Commit Loss: 0.000771 | Perplexity: 1082.551949
2025-09-15 07:43:58,715 Stage: Train 0.5 | Epoch: 527 | Iter: 397400 | Total Loss: 0.002223 | Recon Loss: 0.001840 | Commit Loss: 0.000766 | Perplexity: 1077.215577
Trainning Epoch:  79%|███████▉  | 528/665 [15:11:39<3:55:52, 103.30s/it]2025-09-15 07:44:25,995 Stage: Train 0.5 | Epoch: 528 | Iter: 397600 | Total Loss: 0.002279 | Recon Loss: 0.001894 | Commit Loss: 0.000769 | Perplexity: 1079.507918
2025-09-15 07:44:53,304 Stage: Train 0.5 | Epoch: 528 | Iter: 397800 | Total Loss: 0.002240 | Recon Loss: 0.001856 | Commit Loss: 0.000768 | Perplexity: 1081.528190
2025-09-15 07:45:20,720 Stage: Train 0.5 | Epoch: 528 | Iter: 398000 | Total Loss: 0.002207 | Recon Loss: 0.001822 | Commit Loss: 0.000771 | Perplexity: 1082.765411
2025-09-15 07:45:48,051 Stage: Train 0.5 | Epoch: 528 | Iter: 398200 | Total Loss: 0.002179 | Recon Loss: 0.001795 | Commit Loss: 0.000767 | Perplexity: 1081.267866
Trainning Epoch:  80%|███████▉  | 529/665 [15:13:22<3:53:53, 103.18s/it]2025-09-15 07:46:15,322 Stage: Train 0.5 | Epoch: 529 | Iter: 398400 | Total Loss: 0.002206 | Recon Loss: 0.001819 | Commit Loss: 0.000774 | Perplexity: 1079.496530
2025-09-15 07:46:42,629 Stage: Train 0.5 | Epoch: 529 | Iter: 398600 | Total Loss: 0.002207 | Recon Loss: 0.001826 | Commit Loss: 0.000762 | Perplexity: 1079.323896
2025-09-15 07:47:09,942 Stage: Train 0.5 | Epoch: 529 | Iter: 398800 | Total Loss: 0.002252 | Recon Loss: 0.001866 | Commit Loss: 0.000772 | Perplexity: 1080.381884
2025-09-15 07:47:37,255 Stage: Train 0.5 | Epoch: 529 | Iter: 399000 | Total Loss: 0.002185 | Recon Loss: 0.001802 | Commit Loss: 0.000767 | Perplexity: 1083.092786
Trainning Epoch:  80%|███████▉  | 530/665 [15:15:05<3:51:53, 103.07s/it]2025-09-15 07:48:04,682 Stage: Train 0.5 | Epoch: 530 | Iter: 399200 | Total Loss: 0.002212 | Recon Loss: 0.001828 | Commit Loss: 0.000768 | Perplexity: 1081.569522
2025-09-15 07:48:32,192 Stage: Train 0.5 | Epoch: 530 | Iter: 399400 | Total Loss: 0.002205 | Recon Loss: 0.001821 | Commit Loss: 0.000768 | Perplexity: 1076.937924
2025-09-15 07:48:59,606 Stage: Train 0.5 | Epoch: 530 | Iter: 399600 | Total Loss: 0.002197 | Recon Loss: 0.001812 | Commit Loss: 0.000770 | Perplexity: 1081.962759
2025-09-15 07:49:27,130 Stage: Train 0.5 | Epoch: 530 | Iter: 399800 | Total Loss: 0.002176 | Recon Loss: 0.001791 | Commit Loss: 0.000769 | Perplexity: 1082.568122
Trainning Epoch:  80%|███████▉  | 531/665 [15:16:49<3:50:26, 103.18s/it]2025-09-15 07:49:54,485 Stage: Train 0.5 | Epoch: 531 | Iter: 400000 | Total Loss: 0.002242 | Recon Loss: 0.001857 | Commit Loss: 0.000770 | Perplexity: 1081.343012
2025-09-15 07:49:54,485 Saving model at iteration 400000
2025-09-15 07:49:54,651 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_532_step_400000
2025-09-15 07:49:54,857 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_532_step_400000/pytorch_model.bin
2025-09-15 07:49:55,184 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_532_step_400000/optimizer.bin
2025-09-15 07:49:55,185 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_532_step_400000/scheduler.bin
2025-09-15 07:49:55,185 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_532_step_400000/random_states_0.pkl
2025-09-15 07:50:22,853 Stage: Train 0.5 | Epoch: 531 | Iter: 400200 | Total Loss: 0.002244 | Recon Loss: 0.001858 | Commit Loss: 0.000773 | Perplexity: 1080.511358
2025-09-15 07:50:50,242 Stage: Train 0.5 | Epoch: 531 | Iter: 400400 | Total Loss: 0.002182 | Recon Loss: 0.001799 | Commit Loss: 0.000765 | Perplexity: 1082.349291
Trainning Epoch:  80%|████████  | 532/665 [15:18:33<3:49:20, 103.46s/it]2025-09-15 07:51:17,634 Stage: Train 0.5 | Epoch: 532 | Iter: 400600 | Total Loss: 0.002214 | Recon Loss: 0.001832 | Commit Loss: 0.000764 | Perplexity: 1074.940009
2025-09-15 07:51:45,032 Stage: Train 0.5 | Epoch: 532 | Iter: 400800 | Total Loss: 0.002267 | Recon Loss: 0.001882 | Commit Loss: 0.000769 | Perplexity: 1080.523553
2025-09-15 07:52:12,425 Stage: Train 0.5 | Epoch: 532 | Iter: 401000 | Total Loss: 0.002203 | Recon Loss: 0.001820 | Commit Loss: 0.000766 | Perplexity: 1079.439611
2025-09-15 07:52:39,826 Stage: Train 0.5 | Epoch: 532 | Iter: 401200 | Total Loss: 0.002237 | Recon Loss: 0.001856 | Commit Loss: 0.000762 | Perplexity: 1079.259732
Trainning Epoch:  80%|████████  | 533/665 [15:20:16<3:47:22, 103.36s/it]2025-09-15 07:53:07,180 Stage: Train 0.5 | Epoch: 533 | Iter: 401400 | Total Loss: 0.002222 | Recon Loss: 0.001837 | Commit Loss: 0.000769 | Perplexity: 1082.405590
2025-09-15 07:53:34,685 Stage: Train 0.5 | Epoch: 533 | Iter: 401600 | Total Loss: 0.002227 | Recon Loss: 0.001844 | Commit Loss: 0.000766 | Perplexity: 1083.296230
2025-09-15 07:54:02,078 Stage: Train 0.5 | Epoch: 533 | Iter: 401800 | Total Loss: 0.002247 | Recon Loss: 0.001861 | Commit Loss: 0.000772 | Perplexity: 1082.017143
2025-09-15 07:54:29,609 Stage: Train 0.5 | Epoch: 533 | Iter: 402000 | Total Loss: 0.002181 | Recon Loss: 0.001798 | Commit Loss: 0.000765 | Perplexity: 1075.378144
Trainning Epoch:  80%|████████  | 534/665 [15:21:59<3:45:42, 103.38s/it]2025-09-15 07:54:57,125 Stage: Train 0.5 | Epoch: 534 | Iter: 402200 | Total Loss: 0.002257 | Recon Loss: 0.001873 | Commit Loss: 0.000768 | Perplexity: 1078.980642
2025-09-15 07:55:24,846 Stage: Train 0.5 | Epoch: 534 | Iter: 402400 | Total Loss: 0.002172 | Recon Loss: 0.001789 | Commit Loss: 0.000765 | Perplexity: 1082.983539
2025-09-15 07:55:52,470 Stage: Train 0.5 | Epoch: 534 | Iter: 402600 | Total Loss: 0.002159 | Recon Loss: 0.001778 | Commit Loss: 0.000761 | Perplexity: 1078.305004
2025-09-15 07:56:20,121 Stage: Train 0.5 | Epoch: 534 | Iter: 402800 | Total Loss: 0.002254 | Recon Loss: 0.001868 | Commit Loss: 0.000771 | Perplexity: 1081.571474
Trainning Epoch:  80%|████████  | 535/665 [15:23:43<3:44:25, 103.58s/it]2025-09-15 07:56:47,821 Stage: Train 0.5 | Epoch: 535 | Iter: 403000 | Total Loss: 0.002255 | Recon Loss: 0.001869 | Commit Loss: 0.000774 | Perplexity: 1080.264377
2025-09-15 07:57:15,398 Stage: Train 0.5 | Epoch: 535 | Iter: 403200 | Total Loss: 0.002193 | Recon Loss: 0.001813 | Commit Loss: 0.000760 | Perplexity: 1077.386775
2025-09-15 07:57:42,923 Stage: Train 0.5 | Epoch: 535 | Iter: 403400 | Total Loss: 0.002185 | Recon Loss: 0.001801 | Commit Loss: 0.000767 | Perplexity: 1078.882000
2025-09-15 07:58:10,468 Stage: Train 0.5 | Epoch: 535 | Iter: 403600 | Total Loss: 0.002209 | Recon Loss: 0.001822 | Commit Loss: 0.000772 | Perplexity: 1084.341304
Trainning Epoch:  81%|████████  | 536/665 [15:25:27<3:42:52, 103.66s/it]2025-09-15 07:58:37,874 Stage: Train 0.5 | Epoch: 536 | Iter: 403800 | Total Loss: 0.002188 | Recon Loss: 0.001805 | Commit Loss: 0.000764 | Perplexity: 1077.246020
2025-09-15 07:59:05,269 Stage: Train 0.5 | Epoch: 536 | Iter: 404000 | Total Loss: 0.002233 | Recon Loss: 0.001849 | Commit Loss: 0.000768 | Perplexity: 1082.678843
2025-09-15 07:59:32,669 Stage: Train 0.5 | Epoch: 536 | Iter: 404200 | Total Loss: 0.002224 | Recon Loss: 0.001839 | Commit Loss: 0.000769 | Perplexity: 1081.583723
Trainning Epoch:  81%|████████  | 537/665 [15:27:10<3:40:49, 103.51s/it]2025-09-15 08:00:00,023 Stage: Train 0.5 | Epoch: 537 | Iter: 404400 | Total Loss: 0.002206 | Recon Loss: 0.001823 | Commit Loss: 0.000766 | Perplexity: 1077.449018
2025-09-15 08:00:27,527 Stage: Train 0.5 | Epoch: 537 | Iter: 404600 | Total Loss: 0.002203 | Recon Loss: 0.001818 | Commit Loss: 0.000769 | Perplexity: 1081.617790
2025-09-15 08:00:54,935 Stage: Train 0.5 | Epoch: 537 | Iter: 404800 | Total Loss: 0.002231 | Recon Loss: 0.001848 | Commit Loss: 0.000766 | Perplexity: 1079.466047
2025-09-15 08:01:22,378 Stage: Train 0.5 | Epoch: 537 | Iter: 405000 | Total Loss: 0.002213 | Recon Loss: 0.001830 | Commit Loss: 0.000767 | Perplexity: 1083.793993
Trainning Epoch:  81%|████████  | 538/665 [15:28:54<3:38:57, 103.44s/it]2025-09-15 08:01:49,763 Stage: Train 0.5 | Epoch: 538 | Iter: 405200 | Total Loss: 0.002243 | Recon Loss: 0.001862 | Commit Loss: 0.000762 | Perplexity: 1075.503938
2025-09-15 08:02:17,171 Stage: Train 0.5 | Epoch: 538 | Iter: 405400 | Total Loss: 0.002237 | Recon Loss: 0.001854 | Commit Loss: 0.000764 | Perplexity: 1080.745830
2025-09-15 08:02:44,583 Stage: Train 0.5 | Epoch: 538 | Iter: 405600 | Total Loss: 0.002178 | Recon Loss: 0.001795 | Commit Loss: 0.000765 | Perplexity: 1077.575630
2025-09-15 08:03:12,005 Stage: Train 0.5 | Epoch: 538 | Iter: 405800 | Total Loss: 0.002211 | Recon Loss: 0.001827 | Commit Loss: 0.000769 | Perplexity: 1081.782055
Trainning Epoch:  81%|████████  | 539/665 [15:30:37<3:37:03, 103.36s/it]2025-09-15 08:03:39,473 Stage: Train 0.5 | Epoch: 539 | Iter: 406000 | Total Loss: 0.002244 | Recon Loss: 0.001858 | Commit Loss: 0.000772 | Perplexity: 1083.750110
2025-09-15 08:04:06,902 Stage: Train 0.5 | Epoch: 539 | Iter: 406200 | Total Loss: 0.002181 | Recon Loss: 0.001798 | Commit Loss: 0.000767 | Perplexity: 1082.640882
2025-09-15 08:04:34,299 Stage: Train 0.5 | Epoch: 539 | Iter: 406400 | Total Loss: 0.002151 | Recon Loss: 0.001768 | Commit Loss: 0.000766 | Perplexity: 1082.781481
2025-09-15 08:05:01,732 Stage: Train 0.5 | Epoch: 539 | Iter: 406600 | Total Loss: 0.002216 | Recon Loss: 0.001834 | Commit Loss: 0.000764 | Perplexity: 1077.581140
Trainning Epoch:  81%|████████  | 540/665 [15:32:20<3:35:18, 103.35s/it]2025-09-15 08:05:29,193 Stage: Train 0.5 | Epoch: 540 | Iter: 406800 | Total Loss: 0.002192 | Recon Loss: 0.001811 | Commit Loss: 0.000763 | Perplexity: 1080.998410
2025-09-15 08:05:56,743 Stage: Train 0.5 | Epoch: 540 | Iter: 407000 | Total Loss: 0.002169 | Recon Loss: 0.001786 | Commit Loss: 0.000767 | Perplexity: 1078.398976
2025-09-15 08:06:24,192 Stage: Train 0.5 | Epoch: 540 | Iter: 407200 | Total Loss: 0.002239 | Recon Loss: 0.001852 | Commit Loss: 0.000773 | Perplexity: 1083.616589
Trainning Epoch:  81%|████████▏ | 541/665 [15:34:03<3:33:36, 103.36s/it]2025-09-15 08:06:51,538 Stage: Train 0.5 | Epoch: 541 | Iter: 407400 | Total Loss: 0.002183 | Recon Loss: 0.001799 | Commit Loss: 0.000767 | Perplexity: 1080.192895
2025-09-15 08:07:19,045 Stage: Train 0.5 | Epoch: 541 | Iter: 407600 | Total Loss: 0.002229 | Recon Loss: 0.001846 | Commit Loss: 0.000765 | Perplexity: 1080.116334
2025-09-15 08:07:46,449 Stage: Train 0.5 | Epoch: 541 | Iter: 407800 | Total Loss: 0.002166 | Recon Loss: 0.001783 | Commit Loss: 0.000766 | Perplexity: 1083.695398
2025-09-15 08:08:13,858 Stage: Train 0.5 | Epoch: 541 | Iter: 408000 | Total Loss: 0.002238 | Recon Loss: 0.001855 | Commit Loss: 0.000766 | Perplexity: 1078.551827
Trainning Epoch:  82%|████████▏ | 542/665 [15:35:47<3:31:50, 103.33s/it]2025-09-15 08:08:41,278 Stage: Train 0.5 | Epoch: 542 | Iter: 408200 | Total Loss: 0.002169 | Recon Loss: 0.001787 | Commit Loss: 0.000765 | Perplexity: 1079.129618
2025-09-15 08:09:08,749 Stage: Train 0.5 | Epoch: 542 | Iter: 408400 | Total Loss: 0.002202 | Recon Loss: 0.001820 | Commit Loss: 0.000763 | Perplexity: 1076.041219
2025-09-15 08:09:36,245 Stage: Train 0.5 | Epoch: 542 | Iter: 408600 | Total Loss: 0.002209 | Recon Loss: 0.001824 | Commit Loss: 0.000769 | Perplexity: 1081.378830
2025-09-15 08:10:03,879 Stage: Train 0.5 | Epoch: 542 | Iter: 408800 | Total Loss: 0.002215 | Recon Loss: 0.001834 | Commit Loss: 0.000763 | Perplexity: 1078.717207
Trainning Epoch:  82%|████████▏ | 543/665 [15:37:30<3:30:17, 103.42s/it]2025-09-15 08:10:31,742 Stage: Train 0.5 | Epoch: 543 | Iter: 409000 | Total Loss: 0.002212 | Recon Loss: 0.001826 | Commit Loss: 0.000773 | Perplexity: 1084.367853
2025-09-15 08:10:59,768 Stage: Train 0.5 | Epoch: 543 | Iter: 409200 | Total Loss: 0.002184 | Recon Loss: 0.001803 | Commit Loss: 0.000762 | Perplexity: 1078.918828
2025-09-15 08:11:27,333 Stage: Train 0.5 | Epoch: 543 | Iter: 409400 | Total Loss: 0.002251 | Recon Loss: 0.001870 | Commit Loss: 0.000762 | Perplexity: 1076.779241
2025-09-15 08:11:54,860 Stage: Train 0.5 | Epoch: 543 | Iter: 409600 | Total Loss: 0.002198 | Recon Loss: 0.001814 | Commit Loss: 0.000769 | Perplexity: 1082.158128
Trainning Epoch:  82%|████████▏ | 544/665 [15:39:15<3:29:11, 103.73s/it]2025-09-15 08:12:22,304 Stage: Train 0.5 | Epoch: 544 | Iter: 409800 | Total Loss: 0.002249 | Recon Loss: 0.001867 | Commit Loss: 0.000764 | Perplexity: 1076.136204
2025-09-15 08:12:49,787 Stage: Train 0.5 | Epoch: 544 | Iter: 410000 | Total Loss: 0.002183 | Recon Loss: 0.001800 | Commit Loss: 0.000766 | Perplexity: 1085.100311
2025-09-15 08:13:17,233 Stage: Train 0.5 | Epoch: 544 | Iter: 410200 | Total Loss: 0.002198 | Recon Loss: 0.001815 | Commit Loss: 0.000766 | Perplexity: 1083.672312
Trainning Epoch:  82%|████████▏ | 545/665 [15:40:58<3:27:13, 103.61s/it]2025-09-15 08:13:44,587 Stage: Train 0.5 | Epoch: 545 | Iter: 410400 | Total Loss: 0.002192 | Recon Loss: 0.001810 | Commit Loss: 0.000765 | Perplexity: 1077.913654
2025-09-15 08:14:12,111 Stage: Train 0.5 | Epoch: 545 | Iter: 410600 | Total Loss: 0.002217 | Recon Loss: 0.001835 | Commit Loss: 0.000765 | Perplexity: 1081.178802
2025-09-15 08:14:39,518 Stage: Train 0.5 | Epoch: 545 | Iter: 410800 | Total Loss: 0.002166 | Recon Loss: 0.001784 | Commit Loss: 0.000764 | Perplexity: 1081.363677
2025-09-15 08:15:06,934 Stage: Train 0.5 | Epoch: 545 | Iter: 411000 | Total Loss: 0.002214 | Recon Loss: 0.001829 | Commit Loss: 0.000770 | Perplexity: 1080.276963
Trainning Epoch:  82%|████████▏ | 546/665 [15:42:41<3:25:17, 103.51s/it]2025-09-15 08:15:34,302 Stage: Train 0.5 | Epoch: 546 | Iter: 411200 | Total Loss: 0.002186 | Recon Loss: 0.001804 | Commit Loss: 0.000764 | Perplexity: 1077.944674
2025-09-15 08:16:01,708 Stage: Train 0.5 | Epoch: 546 | Iter: 411400 | Total Loss: 0.002214 | Recon Loss: 0.001834 | Commit Loss: 0.000761 | Perplexity: 1083.577432
2025-09-15 08:16:29,176 Stage: Train 0.5 | Epoch: 546 | Iter: 411600 | Total Loss: 0.002176 | Recon Loss: 0.001794 | Commit Loss: 0.000764 | Perplexity: 1077.247799
2025-09-15 08:16:56,658 Stage: Train 0.5 | Epoch: 546 | Iter: 411800 | Total Loss: 0.002203 | Recon Loss: 0.001820 | Commit Loss: 0.000766 | Perplexity: 1085.306472
Trainning Epoch:  82%|████████▏ | 547/665 [15:44:25<3:23:27, 103.45s/it]2025-09-15 08:17:24,206 Stage: Train 0.5 | Epoch: 547 | Iter: 412000 | Total Loss: 0.002186 | Recon Loss: 0.001803 | Commit Loss: 0.000766 | Perplexity: 1081.474417
2025-09-15 08:17:51,657 Stage: Train 0.5 | Epoch: 547 | Iter: 412200 | Total Loss: 0.002199 | Recon Loss: 0.001820 | Commit Loss: 0.000759 | Perplexity: 1080.876008
2025-09-15 08:18:19,104 Stage: Train 0.5 | Epoch: 547 | Iter: 412400 | Total Loss: 0.002238 | Recon Loss: 0.001853 | Commit Loss: 0.000770 | Perplexity: 1081.228608
2025-09-15 08:18:46,590 Stage: Train 0.5 | Epoch: 547 | Iter: 412600 | Total Loss: 0.002171 | Recon Loss: 0.001789 | Commit Loss: 0.000763 | Perplexity: 1083.907025
Trainning Epoch:  82%|████████▏ | 548/665 [15:46:08<3:21:43, 103.45s/it]2025-09-15 08:19:13,941 Stage: Train 0.5 | Epoch: 548 | Iter: 412800 | Total Loss: 0.002192 | Recon Loss: 0.001812 | Commit Loss: 0.000762 | Perplexity: 1081.046879
2025-09-15 08:19:41,365 Stage: Train 0.5 | Epoch: 548 | Iter: 413000 | Total Loss: 0.002221 | Recon Loss: 0.001840 | Commit Loss: 0.000764 | Perplexity: 1082.293742
2025-09-15 08:20:08,771 Stage: Train 0.5 | Epoch: 548 | Iter: 413200 | Total Loss: 0.002162 | Recon Loss: 0.001780 | Commit Loss: 0.000766 | Perplexity: 1079.993081
Trainning Epoch:  83%|████████▎ | 549/665 [15:47:51<3:19:54, 103.40s/it]2025-09-15 08:20:36,258 Stage: Train 0.5 | Epoch: 549 | Iter: 413400 | Total Loss: 0.002179 | Recon Loss: 0.001796 | Commit Loss: 0.000767 | Perplexity: 1081.503770
2025-09-15 08:21:03,767 Stage: Train 0.5 | Epoch: 549 | Iter: 413600 | Total Loss: 0.002190 | Recon Loss: 0.001811 | Commit Loss: 0.000758 | Perplexity: 1078.118776
2025-09-15 08:21:31,485 Stage: Train 0.5 | Epoch: 549 | Iter: 413800 | Total Loss: 0.002193 | Recon Loss: 0.001813 | Commit Loss: 0.000761 | Perplexity: 1083.326239
2025-09-15 08:21:59,062 Stage: Train 0.5 | Epoch: 549 | Iter: 414000 | Total Loss: 0.002186 | Recon Loss: 0.001800 | Commit Loss: 0.000771 | Perplexity: 1086.546399
Trainning Epoch:  83%|████████▎ | 550/665 [15:49:35<3:18:25, 103.53s/it]2025-09-15 08:22:26,608 Stage: Train 0.5 | Epoch: 550 | Iter: 414200 | Total Loss: 0.002195 | Recon Loss: 0.001812 | Commit Loss: 0.000766 | Perplexity: 1082.482958
2025-09-15 08:22:54,171 Stage: Train 0.5 | Epoch: 550 | Iter: 414400 | Total Loss: 0.002199 | Recon Loss: 0.001819 | Commit Loss: 0.000761 | Perplexity: 1081.913347
2025-09-15 08:23:21,769 Stage: Train 0.5 | Epoch: 550 | Iter: 414600 | Total Loss: 0.002161 | Recon Loss: 0.001777 | Commit Loss: 0.000767 | Perplexity: 1083.159253
2025-09-15 08:23:49,328 Stage: Train 0.5 | Epoch: 550 | Iter: 414800 | Total Loss: 0.002205 | Recon Loss: 0.001823 | Commit Loss: 0.000766 | Perplexity: 1079.453287
Trainning Epoch:  83%|████████▎ | 551/665 [15:51:19<3:16:55, 103.64s/it]2025-09-15 08:24:16,960 Stage: Train 0.5 | Epoch: 551 | Iter: 415000 | Total Loss: 0.002168 | Recon Loss: 0.001787 | Commit Loss: 0.000763 | Perplexity: 1082.120245
2025-09-15 08:24:44,507 Stage: Train 0.5 | Epoch: 551 | Iter: 415200 | Total Loss: 0.002206 | Recon Loss: 0.001825 | Commit Loss: 0.000763 | Perplexity: 1082.070119
2025-09-15 08:25:11,946 Stage: Train 0.5 | Epoch: 551 | Iter: 415400 | Total Loss: 0.002208 | Recon Loss: 0.001826 | Commit Loss: 0.000764 | Perplexity: 1083.244055
2025-09-15 08:25:39,367 Stage: Train 0.5 | Epoch: 551 | Iter: 415600 | Total Loss: 0.002168 | Recon Loss: 0.001786 | Commit Loss: 0.000765 | Perplexity: 1082.422882
Trainning Epoch:  83%|████████▎ | 552/665 [15:53:03<3:15:03, 103.57s/it]2025-09-15 08:26:06,741 Stage: Train 0.5 | Epoch: 552 | Iter: 415800 | Total Loss: 0.002172 | Recon Loss: 0.001792 | Commit Loss: 0.000761 | Perplexity: 1081.285244
2025-09-15 08:26:34,162 Stage: Train 0.5 | Epoch: 552 | Iter: 416000 | Total Loss: 0.002194 | Recon Loss: 0.001811 | Commit Loss: 0.000766 | Perplexity: 1080.903409
2025-09-15 08:27:01,594 Stage: Train 0.5 | Epoch: 552 | Iter: 416200 | Total Loss: 0.002196 | Recon Loss: 0.001815 | Commit Loss: 0.000761 | Perplexity: 1082.711710
2025-09-15 08:27:29,106 Stage: Train 0.5 | Epoch: 552 | Iter: 416400 | Total Loss: 0.002197 | Recon Loss: 0.001814 | Commit Loss: 0.000765 | Perplexity: 1080.915503
Trainning Epoch:  83%|████████▎ | 553/665 [15:54:46<3:13:10, 103.49s/it]2025-09-15 08:27:56,487 Stage: Train 0.5 | Epoch: 553 | Iter: 416600 | Total Loss: 0.002201 | Recon Loss: 0.001820 | Commit Loss: 0.000762 | Perplexity: 1081.919605
2025-09-15 08:28:23,931 Stage: Train 0.5 | Epoch: 553 | Iter: 416800 | Total Loss: 0.002151 | Recon Loss: 0.001767 | Commit Loss: 0.000768 | Perplexity: 1084.776795
2025-09-15 08:28:51,340 Stage: Train 0.5 | Epoch: 553 | Iter: 417000 | Total Loss: 0.002202 | Recon Loss: 0.001820 | Commit Loss: 0.000763 | Perplexity: 1080.226732
Trainning Epoch:  83%|████████▎ | 554/665 [15:56:29<3:11:17, 103.40s/it]2025-09-15 08:29:18,699 Stage: Train 0.5 | Epoch: 554 | Iter: 417200 | Total Loss: 0.002127 | Recon Loss: 0.001745 | Commit Loss: 0.000764 | Perplexity: 1083.811113
2025-09-15 08:29:46,094 Stage: Train 0.5 | Epoch: 554 | Iter: 417400 | Total Loss: 0.002217 | Recon Loss: 0.001833 | Commit Loss: 0.000767 | Perplexity: 1083.317820
2025-09-15 08:30:13,479 Stage: Train 0.5 | Epoch: 554 | Iter: 417600 | Total Loss: 0.002194 | Recon Loss: 0.001811 | Commit Loss: 0.000765 | Perplexity: 1078.055266
2025-09-15 08:30:40,869 Stage: Train 0.5 | Epoch: 554 | Iter: 417800 | Total Loss: 0.002174 | Recon Loss: 0.001793 | Commit Loss: 0.000763 | Perplexity: 1081.923594
Trainning Epoch:  83%|████████▎ | 555/665 [15:58:12<3:09:27, 103.34s/it]2025-09-15 08:31:08,438 Stage: Train 0.5 | Epoch: 555 | Iter: 418000 | Total Loss: 0.002205 | Recon Loss: 0.001824 | Commit Loss: 0.000762 | Perplexity: 1079.300408
2025-09-15 08:31:35,912 Stage: Train 0.5 | Epoch: 555 | Iter: 418200 | Total Loss: 0.002168 | Recon Loss: 0.001788 | Commit Loss: 0.000760 | Perplexity: 1080.683144
2025-09-15 08:32:03,420 Stage: Train 0.5 | Epoch: 555 | Iter: 418400 | Total Loss: 0.002149 | Recon Loss: 0.001768 | Commit Loss: 0.000760 | Perplexity: 1082.554479
2025-09-15 08:32:31,043 Stage: Train 0.5 | Epoch: 555 | Iter: 418600 | Total Loss: 0.002203 | Recon Loss: 0.001818 | Commit Loss: 0.000771 | Perplexity: 1086.686537
Trainning Epoch:  84%|████████▎ | 556/665 [15:59:56<3:07:55, 103.44s/it]2025-09-15 08:32:58,521 Stage: Train 0.5 | Epoch: 556 | Iter: 418800 | Total Loss: 0.002181 | Recon Loss: 0.001801 | Commit Loss: 0.000759 | Perplexity: 1077.956801
2025-09-15 08:33:26,037 Stage: Train 0.5 | Epoch: 556 | Iter: 419000 | Total Loss: 0.002273 | Recon Loss: 0.001892 | Commit Loss: 0.000763 | Perplexity: 1084.412809
2025-09-15 08:33:53,604 Stage: Train 0.5 | Epoch: 556 | Iter: 419200 | Total Loss: 0.002117 | Recon Loss: 0.001733 | Commit Loss: 0.000767 | Perplexity: 1086.290761
2025-09-15 08:34:21,313 Stage: Train 0.5 | Epoch: 556 | Iter: 419400 | Total Loss: 0.002166 | Recon Loss: 0.001784 | Commit Loss: 0.000764 | Perplexity: 1081.687677
Trainning Epoch:  84%|████████▍ | 557/665 [16:01:40<3:06:22, 103.55s/it]2025-09-15 08:34:48,829 Stage: Train 0.5 | Epoch: 557 | Iter: 419600 | Total Loss: 0.002160 | Recon Loss: 0.001779 | Commit Loss: 0.000762 | Perplexity: 1078.478222
2025-09-15 08:35:16,378 Stage: Train 0.5 | Epoch: 557 | Iter: 419800 | Total Loss: 0.002232 | Recon Loss: 0.001854 | Commit Loss: 0.000757 | Perplexity: 1080.425659
2025-09-15 08:35:43,856 Stage: Train 0.5 | Epoch: 557 | Iter: 420000 | Total Loss: 0.002187 | Recon Loss: 0.001806 | Commit Loss: 0.000762 | Perplexity: 1083.549979
2025-09-15 08:35:43,856 Saving model at iteration 420000
2025-09-15 08:35:44,021 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_558_step_420000
2025-09-15 08:35:44,230 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_558_step_420000/pytorch_model.bin
2025-09-15 08:35:44,558 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_558_step_420000/optimizer.bin
2025-09-15 08:35:44,558 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_558_step_420000/scheduler.bin
2025-09-15 08:35:44,559 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_558_step_420000/random_states_0.pkl
Trainning Epoch:  84%|████████▍ | 558/665 [16:03:24<3:05:10, 103.83s/it]2025-09-15 08:36:12,225 Stage: Train 0.5 | Epoch: 558 | Iter: 420200 | Total Loss: 0.002203 | Recon Loss: 0.001819 | Commit Loss: 0.000769 | Perplexity: 1084.434413
2025-09-15 08:36:39,837 Stage: Train 0.5 | Epoch: 558 | Iter: 420400 | Total Loss: 0.002181 | Recon Loss: 0.001799 | Commit Loss: 0.000762 | Perplexity: 1082.107271
2025-09-15 08:37:07,652 Stage: Train 0.5 | Epoch: 558 | Iter: 420600 | Total Loss: 0.002199 | Recon Loss: 0.001821 | Commit Loss: 0.000756 | Perplexity: 1080.346041
2025-09-15 08:37:35,352 Stage: Train 0.5 | Epoch: 558 | Iter: 420800 | Total Loss: 0.002175 | Recon Loss: 0.001792 | Commit Loss: 0.000766 | Perplexity: 1086.255809
Trainning Epoch:  84%|████████▍ | 559/665 [16:05:08<3:03:37, 103.94s/it]2025-09-15 08:38:02,946 Stage: Train 0.5 | Epoch: 559 | Iter: 421000 | Total Loss: 0.002232 | Recon Loss: 0.001851 | Commit Loss: 0.000761 | Perplexity: 1080.384609
2025-09-15 08:38:30,472 Stage: Train 0.5 | Epoch: 559 | Iter: 421200 | Total Loss: 0.002151 | Recon Loss: 0.001770 | Commit Loss: 0.000762 | Perplexity: 1087.698411
2025-09-15 08:38:57,995 Stage: Train 0.5 | Epoch: 559 | Iter: 421400 | Total Loss: 0.002236 | Recon Loss: 0.001854 | Commit Loss: 0.000765 | Perplexity: 1082.109614
2025-09-15 08:39:25,653 Stage: Train 0.5 | Epoch: 559 | Iter: 421600 | Total Loss: 0.002183 | Recon Loss: 0.001804 | Commit Loss: 0.000758 | Perplexity: 1080.265772
Trainning Epoch:  84%|████████▍ | 560/665 [16:06:52<3:01:50, 103.91s/it]2025-09-15 08:39:53,733 Stage: Train 0.5 | Epoch: 560 | Iter: 421800 | Total Loss: 0.002157 | Recon Loss: 0.001778 | Commit Loss: 0.000758 | Perplexity: 1081.541376
2025-09-15 08:40:21,284 Stage: Train 0.5 | Epoch: 560 | Iter: 422000 | Total Loss: 0.002188 | Recon Loss: 0.001805 | Commit Loss: 0.000766 | Perplexity: 1084.666236
2025-09-15 08:40:48,738 Stage: Train 0.5 | Epoch: 560 | Iter: 422200 | Total Loss: 0.002195 | Recon Loss: 0.001813 | Commit Loss: 0.000764 | Perplexity: 1084.037037
2025-09-15 08:41:16,201 Stage: Train 0.5 | Epoch: 560 | Iter: 422400 | Total Loss: 0.002192 | Recon Loss: 0.001813 | Commit Loss: 0.000760 | Perplexity: 1078.107428
Trainning Epoch:  84%|████████▍ | 561/665 [16:08:36<3:00:09, 103.94s/it]2025-09-15 08:41:43,595 Stage: Train 0.5 | Epoch: 561 | Iter: 422600 | Total Loss: 0.002167 | Recon Loss: 0.001785 | Commit Loss: 0.000764 | Perplexity: 1078.751177
2025-09-15 08:42:11,037 Stage: Train 0.5 | Epoch: 561 | Iter: 422800 | Total Loss: 0.002161 | Recon Loss: 0.001783 | Commit Loss: 0.000758 | Perplexity: 1081.710480
2025-09-15 08:42:38,584 Stage: Train 0.5 | Epoch: 561 | Iter: 423000 | Total Loss: 0.002162 | Recon Loss: 0.001780 | Commit Loss: 0.000764 | Perplexity: 1089.341226
Trainning Epoch:  85%|████████▍ | 562/665 [16:10:20<2:58:06, 103.75s/it]2025-09-15 08:43:05,999 Stage: Train 0.5 | Epoch: 562 | Iter: 423200 | Total Loss: 0.002182 | Recon Loss: 0.001802 | Commit Loss: 0.000760 | Perplexity: 1078.318253
2025-09-15 08:43:34,153 Stage: Train 0.5 | Epoch: 562 | Iter: 423400 | Total Loss: 0.002151 | Recon Loss: 0.001771 | Commit Loss: 0.000759 | Perplexity: 1084.760896
2025-09-15 08:44:01,778 Stage: Train 0.5 | Epoch: 562 | Iter: 423600 | Total Loss: 0.002170 | Recon Loss: 0.001788 | Commit Loss: 0.000763 | Perplexity: 1082.282993
2025-09-15 08:44:29,314 Stage: Train 0.5 | Epoch: 562 | Iter: 423800 | Total Loss: 0.002170 | Recon Loss: 0.001790 | Commit Loss: 0.000760 | Perplexity: 1080.384049
Trainning Epoch:  85%|████████▍ | 563/665 [16:12:04<2:56:42, 103.95s/it]2025-09-15 08:44:56,821 Stage: Train 0.5 | Epoch: 563 | Iter: 424000 | Total Loss: 0.002237 | Recon Loss: 0.001855 | Commit Loss: 0.000763 | Perplexity: 1083.694864
2025-09-15 08:45:24,329 Stage: Train 0.5 | Epoch: 563 | Iter: 424200 | Total Loss: 0.002182 | Recon Loss: 0.001800 | Commit Loss: 0.000763 | Perplexity: 1082.767024
2025-09-15 08:45:52,049 Stage: Train 0.5 | Epoch: 563 | Iter: 424400 | Total Loss: 0.002181 | Recon Loss: 0.001800 | Commit Loss: 0.000763 | Perplexity: 1082.987343
2025-09-15 08:46:20,050 Stage: Train 0.5 | Epoch: 563 | Iter: 424600 | Total Loss: 0.002219 | Recon Loss: 0.001837 | Commit Loss: 0.000765 | Perplexity: 1086.290114
Trainning Epoch:  85%|████████▍ | 564/665 [16:13:48<2:55:13, 104.09s/it]2025-09-15 08:46:47,724 Stage: Train 0.5 | Epoch: 564 | Iter: 424800 | Total Loss: 0.002163 | Recon Loss: 0.001783 | Commit Loss: 0.000760 | Perplexity: 1082.062318
2025-09-15 08:47:15,146 Stage: Train 0.5 | Epoch: 564 | Iter: 425000 | Total Loss: 0.002224 | Recon Loss: 0.001845 | Commit Loss: 0.000758 | Perplexity: 1082.949779
2025-09-15 08:47:42,634 Stage: Train 0.5 | Epoch: 564 | Iter: 425200 | Total Loss: 0.002133 | Recon Loss: 0.001752 | Commit Loss: 0.000761 | Perplexity: 1081.978722
2025-09-15 08:48:10,238 Stage: Train 0.5 | Epoch: 564 | Iter: 425400 | Total Loss: 0.002167 | Recon Loss: 0.001784 | Commit Loss: 0.000765 | Perplexity: 1082.623248
Trainning Epoch:  85%|████████▍ | 565/665 [16:15:32<2:53:14, 103.95s/it]2025-09-15 08:48:37,807 Stage: Train 0.5 | Epoch: 565 | Iter: 425600 | Total Loss: 0.002190 | Recon Loss: 0.001811 | Commit Loss: 0.000759 | Perplexity: 1079.911822
2025-09-15 08:49:05,322 Stage: Train 0.5 | Epoch: 565 | Iter: 425800 | Total Loss: 0.002160 | Recon Loss: 0.001783 | Commit Loss: 0.000754 | Perplexity: 1079.807082
2025-09-15 08:49:32,996 Stage: Train 0.5 | Epoch: 565 | Iter: 426000 | Total Loss: 0.002245 | Recon Loss: 0.001861 | Commit Loss: 0.000769 | Perplexity: 1085.597222
Trainning Epoch:  85%|████████▌ | 566/665 [16:17:16<2:51:23, 103.88s/it]2025-09-15 08:50:00,433 Stage: Train 0.5 | Epoch: 566 | Iter: 426200 | Total Loss: 0.002175 | Recon Loss: 0.001797 | Commit Loss: 0.000757 | Perplexity: 1082.754529
2025-09-15 08:50:27,878 Stage: Train 0.5 | Epoch: 566 | Iter: 426400 | Total Loss: 0.002133 | Recon Loss: 0.001756 | Commit Loss: 0.000755 | Perplexity: 1084.203354
2025-09-15 08:50:55,569 Stage: Train 0.5 | Epoch: 566 | Iter: 426600 | Total Loss: 0.002282 | Recon Loss: 0.001901 | Commit Loss: 0.000762 | Perplexity: 1080.363872
2025-09-15 08:51:23,235 Stage: Train 0.5 | Epoch: 566 | Iter: 426800 | Total Loss: 0.002170 | Recon Loss: 0.001790 | Commit Loss: 0.000760 | Perplexity: 1082.676026
Trainning Epoch:  85%|████████▌ | 567/665 [16:18:59<2:49:37, 103.85s/it]2025-09-15 08:51:50,677 Stage: Train 0.5 | Epoch: 567 | Iter: 427000 | Total Loss: 0.002160 | Recon Loss: 0.001780 | Commit Loss: 0.000761 | Perplexity: 1081.889084
2025-09-15 08:52:18,292 Stage: Train 0.5 | Epoch: 567 | Iter: 427200 | Total Loss: 0.002125 | Recon Loss: 0.001747 | Commit Loss: 0.000757 | Perplexity: 1082.353304
2025-09-15 08:52:45,802 Stage: Train 0.5 | Epoch: 567 | Iter: 427400 | Total Loss: 0.002191 | Recon Loss: 0.001811 | Commit Loss: 0.000759 | Perplexity: 1079.951721
2025-09-15 08:53:13,429 Stage: Train 0.5 | Epoch: 567 | Iter: 427600 | Total Loss: 0.002159 | Recon Loss: 0.001775 | Commit Loss: 0.000768 | Perplexity: 1085.637685
Trainning Epoch:  85%|████████▌ | 568/665 [16:20:43<2:47:51, 103.83s/it]2025-09-15 08:53:40,947 Stage: Train 0.5 | Epoch: 568 | Iter: 427800 | Total Loss: 0.002191 | Recon Loss: 0.001811 | Commit Loss: 0.000759 | Perplexity: 1081.885387
2025-09-15 08:54:08,376 Stage: Train 0.5 | Epoch: 568 | Iter: 428000 | Total Loss: 0.002172 | Recon Loss: 0.001790 | Commit Loss: 0.000765 | Perplexity: 1084.513770
2025-09-15 08:54:35,808 Stage: Train 0.5 | Epoch: 568 | Iter: 428200 | Total Loss: 0.002211 | Recon Loss: 0.001833 | Commit Loss: 0.000756 | Perplexity: 1081.626854
2025-09-15 08:55:03,205 Stage: Train 0.5 | Epoch: 568 | Iter: 428400 | Total Loss: 0.002162 | Recon Loss: 0.001780 | Commit Loss: 0.000762 | Perplexity: 1085.262564
Trainning Epoch:  86%|████████▌ | 569/665 [16:22:27<2:45:50, 103.65s/it]2025-09-15 08:55:30,572 Stage: Train 0.5 | Epoch: 569 | Iter: 428600 | Total Loss: 0.002162 | Recon Loss: 0.001785 | Commit Loss: 0.000755 | Perplexity: 1081.648339
2025-09-15 08:55:58,023 Stage: Train 0.5 | Epoch: 569 | Iter: 428800 | Total Loss: 0.002142 | Recon Loss: 0.001761 | Commit Loss: 0.000762 | Perplexity: 1082.872904
2025-09-15 08:56:25,545 Stage: Train 0.5 | Epoch: 569 | Iter: 429000 | Total Loss: 0.002203 | Recon Loss: 0.001818 | Commit Loss: 0.000769 | Perplexity: 1087.155731
2025-09-15 08:56:52,959 Stage: Train 0.5 | Epoch: 569 | Iter: 429200 | Total Loss: 0.002207 | Recon Loss: 0.001829 | Commit Loss: 0.000757 | Perplexity: 1080.087704
Trainning Epoch:  86%|████████▌ | 570/665 [16:24:10<2:43:58, 103.56s/it]2025-09-15 08:57:20,373 Stage: Train 0.5 | Epoch: 570 | Iter: 429400 | Total Loss: 0.002204 | Recon Loss: 0.001826 | Commit Loss: 0.000757 | Perplexity: 1079.349521
2025-09-15 08:57:47,828 Stage: Train 0.5 | Epoch: 570 | Iter: 429600 | Total Loss: 0.002148 | Recon Loss: 0.001769 | Commit Loss: 0.000756 | Perplexity: 1084.242234
2025-09-15 08:58:15,278 Stage: Train 0.5 | Epoch: 570 | Iter: 429800 | Total Loss: 0.002215 | Recon Loss: 0.001832 | Commit Loss: 0.000766 | Perplexity: 1083.984359
Trainning Epoch:  86%|████████▌ | 571/665 [16:25:53<2:42:06, 103.48s/it]2025-09-15 08:58:42,673 Stage: Train 0.5 | Epoch: 571 | Iter: 430000 | Total Loss: 0.002233 | Recon Loss: 0.001856 | Commit Loss: 0.000753 | Perplexity: 1078.676304
2025-09-15 08:59:10,124 Stage: Train 0.5 | Epoch: 571 | Iter: 430200 | Total Loss: 0.002138 | Recon Loss: 0.001763 | Commit Loss: 0.000751 | Perplexity: 1077.886762
2025-09-15 08:59:37,590 Stage: Train 0.5 | Epoch: 571 | Iter: 430400 | Total Loss: 0.002219 | Recon Loss: 0.001838 | Commit Loss: 0.000761 | Perplexity: 1083.810372
2025-09-15 09:00:05,228 Stage: Train 0.5 | Epoch: 571 | Iter: 430600 | Total Loss: 0.002178 | Recon Loss: 0.001797 | Commit Loss: 0.000763 | Perplexity: 1086.677758
Trainning Epoch:  86%|████████▌ | 572/665 [16:27:37<2:40:24, 103.49s/it]2025-09-15 09:00:32,644 Stage: Train 0.5 | Epoch: 572 | Iter: 430800 | Total Loss: 0.002195 | Recon Loss: 0.001816 | Commit Loss: 0.000757 | Perplexity: 1084.126219
2025-09-15 09:01:00,230 Stage: Train 0.5 | Epoch: 572 | Iter: 431000 | Total Loss: 0.002169 | Recon Loss: 0.001790 | Commit Loss: 0.000758 | Perplexity: 1085.008228
2025-09-15 09:01:27,659 Stage: Train 0.5 | Epoch: 572 | Iter: 431200 | Total Loss: 0.002133 | Recon Loss: 0.001758 | Commit Loss: 0.000751 | Perplexity: 1078.920064
2025-09-15 09:01:55,184 Stage: Train 0.5 | Epoch: 572 | Iter: 431400 | Total Loss: 0.002188 | Recon Loss: 0.001808 | Commit Loss: 0.000759 | Perplexity: 1079.680021
Trainning Epoch:  86%|████████▌ | 573/665 [16:29:20<2:38:45, 103.53s/it]2025-09-15 09:02:22,903 Stage: Train 0.5 | Epoch: 573 | Iter: 431600 | Total Loss: 0.002175 | Recon Loss: 0.001795 | Commit Loss: 0.000760 | Perplexity: 1082.523640
2025-09-15 09:02:50,408 Stage: Train 0.5 | Epoch: 573 | Iter: 431800 | Total Loss: 0.002189 | Recon Loss: 0.001809 | Commit Loss: 0.000760 | Perplexity: 1085.061902
2025-09-15 09:03:17,968 Stage: Train 0.5 | Epoch: 573 | Iter: 432000 | Total Loss: 0.002193 | Recon Loss: 0.001816 | Commit Loss: 0.000755 | Perplexity: 1083.050341
2025-09-15 09:03:45,421 Stage: Train 0.5 | Epoch: 573 | Iter: 432200 | Total Loss: 0.002191 | Recon Loss: 0.001813 | Commit Loss: 0.000757 | Perplexity: 1082.885046
Trainning Epoch:  86%|████████▋ | 574/665 [16:31:04<2:37:04, 103.57s/it]2025-09-15 09:04:12,830 Stage: Train 0.5 | Epoch: 574 | Iter: 432400 | Total Loss: 0.002128 | Recon Loss: 0.001752 | Commit Loss: 0.000752 | Perplexity: 1080.765202
2025-09-15 09:04:40,372 Stage: Train 0.5 | Epoch: 574 | Iter: 432600 | Total Loss: 0.002215 | Recon Loss: 0.001837 | Commit Loss: 0.000757 | Perplexity: 1082.335072
2025-09-15 09:05:07,861 Stage: Train 0.5 | Epoch: 574 | Iter: 432800 | Total Loss: 0.002154 | Recon Loss: 0.001775 | Commit Loss: 0.000760 | Perplexity: 1083.820230
Trainning Epoch:  86%|████████▋ | 575/665 [16:32:47<2:35:20, 103.56s/it]2025-09-15 09:05:35,390 Stage: Train 0.5 | Epoch: 575 | Iter: 433000 | Total Loss: 0.002195 | Recon Loss: 0.001814 | Commit Loss: 0.000761 | Perplexity: 1080.306471
2025-09-15 09:06:02,882 Stage: Train 0.5 | Epoch: 575 | Iter: 433200 | Total Loss: 0.002135 | Recon Loss: 0.001756 | Commit Loss: 0.000758 | Perplexity: 1080.014749
2025-09-15 09:06:30,400 Stage: Train 0.5 | Epoch: 575 | Iter: 433400 | Total Loss: 0.002172 | Recon Loss: 0.001794 | Commit Loss: 0.000755 | Perplexity: 1081.897383
2025-09-15 09:06:57,813 Stage: Train 0.5 | Epoch: 575 | Iter: 433600 | Total Loss: 0.002141 | Recon Loss: 0.001763 | Commit Loss: 0.000755 | Perplexity: 1085.908690
Trainning Epoch:  87%|████████▋ | 576/665 [16:34:31<2:33:31, 103.50s/it]2025-09-15 09:07:25,167 Stage: Train 0.5 | Epoch: 576 | Iter: 433800 | Total Loss: 0.002186 | Recon Loss: 0.001804 | Commit Loss: 0.000763 | Perplexity: 1084.355218
2025-09-15 09:07:52,849 Stage: Train 0.5 | Epoch: 576 | Iter: 434000 | Total Loss: 0.002241 | Recon Loss: 0.001862 | Commit Loss: 0.000759 | Perplexity: 1083.517440
2025-09-15 09:08:20,460 Stage: Train 0.5 | Epoch: 576 | Iter: 434200 | Total Loss: 0.002167 | Recon Loss: 0.001791 | Commit Loss: 0.000753 | Perplexity: 1080.175724
2025-09-15 09:08:48,035 Stage: Train 0.5 | Epoch: 576 | Iter: 434400 | Total Loss: 0.002158 | Recon Loss: 0.001779 | Commit Loss: 0.000758 | Perplexity: 1083.120176
Trainning Epoch:  87%|████████▋ | 577/665 [16:36:15<2:32:01, 103.65s/it]2025-09-15 09:09:15,762 Stage: Train 0.5 | Epoch: 577 | Iter: 434600 | Total Loss: 0.002143 | Recon Loss: 0.001767 | Commit Loss: 0.000753 | Perplexity: 1081.537861
2025-09-15 09:09:43,193 Stage: Train 0.5 | Epoch: 577 | Iter: 434800 | Total Loss: 0.002157 | Recon Loss: 0.001780 | Commit Loss: 0.000756 | Perplexity: 1081.594929
2025-09-15 09:10:10,752 Stage: Train 0.5 | Epoch: 577 | Iter: 435000 | Total Loss: 0.002157 | Recon Loss: 0.001776 | Commit Loss: 0.000762 | Perplexity: 1086.754037
2025-09-15 09:10:38,262 Stage: Train 0.5 | Epoch: 577 | Iter: 435200 | Total Loss: 0.002153 | Recon Loss: 0.001774 | Commit Loss: 0.000759 | Perplexity: 1080.939549
Trainning Epoch:  87%|████████▋ | 578/665 [16:37:58<2:30:16, 103.63s/it]2025-09-15 09:11:05,740 Stage: Train 0.5 | Epoch: 578 | Iter: 435400 | Total Loss: 0.002164 | Recon Loss: 0.001788 | Commit Loss: 0.000753 | Perplexity: 1078.850998
2025-09-15 09:11:33,378 Stage: Train 0.5 | Epoch: 578 | Iter: 435600 | Total Loss: 0.002157 | Recon Loss: 0.001778 | Commit Loss: 0.000758 | Perplexity: 1083.850127
2025-09-15 09:12:00,976 Stage: Train 0.5 | Epoch: 578 | Iter: 435800 | Total Loss: 0.002160 | Recon Loss: 0.001781 | Commit Loss: 0.000758 | Perplexity: 1082.053078
Trainning Epoch:  87%|████████▋ | 579/665 [16:39:42<2:28:34, 103.65s/it]2025-09-15 09:12:28,377 Stage: Train 0.5 | Epoch: 579 | Iter: 436000 | Total Loss: 0.002164 | Recon Loss: 0.001782 | Commit Loss: 0.000764 | Perplexity: 1083.052843
2025-09-15 09:12:55,773 Stage: Train 0.5 | Epoch: 579 | Iter: 436200 | Total Loss: 0.002142 | Recon Loss: 0.001765 | Commit Loss: 0.000753 | Perplexity: 1081.511035
2025-09-15 09:13:23,358 Stage: Train 0.5 | Epoch: 579 | Iter: 436400 | Total Loss: 0.002185 | Recon Loss: 0.001809 | Commit Loss: 0.000752 | Perplexity: 1080.393606
2025-09-15 09:13:50,880 Stage: Train 0.5 | Epoch: 579 | Iter: 436600 | Total Loss: 0.002148 | Recon Loss: 0.001766 | Commit Loss: 0.000764 | Perplexity: 1087.445373
Trainning Epoch:  87%|████████▋ | 580/665 [16:41:26<2:26:45, 103.60s/it]2025-09-15 09:14:18,296 Stage: Train 0.5 | Epoch: 580 | Iter: 436800 | Total Loss: 0.002176 | Recon Loss: 0.001792 | Commit Loss: 0.000769 | Perplexity: 1078.387885
2025-09-15 09:14:46,135 Stage: Train 0.5 | Epoch: 580 | Iter: 437000 | Total Loss: 0.002152 | Recon Loss: 0.001774 | Commit Loss: 0.000755 | Perplexity: 1083.347021
2025-09-15 09:15:13,645 Stage: Train 0.5 | Epoch: 580 | Iter: 437200 | Total Loss: 0.002190 | Recon Loss: 0.001811 | Commit Loss: 0.000758 | Perplexity: 1083.119036
2025-09-15 09:15:41,182 Stage: Train 0.5 | Epoch: 580 | Iter: 437400 | Total Loss: 0.002168 | Recon Loss: 0.001788 | Commit Loss: 0.000759 | Perplexity: 1085.440172
Trainning Epoch:  87%|████████▋ | 581/665 [16:43:10<2:25:16, 103.77s/it]2025-09-15 09:16:09,127 Stage: Train 0.5 | Epoch: 581 | Iter: 437600 | Total Loss: 0.002153 | Recon Loss: 0.001773 | Commit Loss: 0.000760 | Perplexity: 1082.412113
2025-09-15 09:16:36,639 Stage: Train 0.5 | Epoch: 581 | Iter: 437800 | Total Loss: 0.002185 | Recon Loss: 0.001806 | Commit Loss: 0.000757 | Perplexity: 1082.633136
2025-09-15 09:17:04,214 Stage: Train 0.5 | Epoch: 581 | Iter: 438000 | Total Loss: 0.002144 | Recon Loss: 0.001768 | Commit Loss: 0.000753 | Perplexity: 1080.106517
2025-09-15 09:17:31,768 Stage: Train 0.5 | Epoch: 581 | Iter: 438200 | Total Loss: 0.002170 | Recon Loss: 0.001790 | Commit Loss: 0.000760 | Perplexity: 1085.390162
Trainning Epoch:  88%|████████▊ | 582/665 [16:44:54<2:23:33, 103.78s/it]2025-09-15 09:17:59,157 Stage: Train 0.5 | Epoch: 582 | Iter: 438400 | Total Loss: 0.002176 | Recon Loss: 0.001797 | Commit Loss: 0.000759 | Perplexity: 1084.607172
2025-09-15 09:18:26,586 Stage: Train 0.5 | Epoch: 582 | Iter: 438600 | Total Loss: 0.002207 | Recon Loss: 0.001826 | Commit Loss: 0.000761 | Perplexity: 1084.102918
2025-09-15 09:18:54,042 Stage: Train 0.5 | Epoch: 582 | Iter: 438800 | Total Loss: 0.002155 | Recon Loss: 0.001775 | Commit Loss: 0.000759 | Perplexity: 1082.728970
Trainning Epoch:  88%|████████▊ | 583/665 [16:46:37<2:21:33, 103.58s/it]2025-09-15 09:19:21,301 Stage: Train 0.5 | Epoch: 583 | Iter: 439000 | Total Loss: 0.002160 | Recon Loss: 0.001785 | Commit Loss: 0.000751 | Perplexity: 1079.853598
2025-09-15 09:19:48,619 Stage: Train 0.5 | Epoch: 583 | Iter: 439200 | Total Loss: 0.002147 | Recon Loss: 0.001771 | Commit Loss: 0.000752 | Perplexity: 1083.363826
2025-09-15 09:20:16,107 Stage: Train 0.5 | Epoch: 583 | Iter: 439400 | Total Loss: 0.002203 | Recon Loss: 0.001825 | Commit Loss: 0.000756 | Perplexity: 1082.422616
2025-09-15 09:20:43,452 Stage: Train 0.5 | Epoch: 583 | Iter: 439600 | Total Loss: 0.002207 | Recon Loss: 0.001829 | Commit Loss: 0.000756 | Perplexity: 1083.539232
Trainning Epoch:  88%|████████▊ | 584/665 [16:48:20<2:19:37, 103.43s/it]2025-09-15 09:21:10,845 Stage: Train 0.5 | Epoch: 584 | Iter: 439800 | Total Loss: 0.002116 | Recon Loss: 0.001737 | Commit Loss: 0.000759 | Perplexity: 1079.942062
2025-09-15 09:21:38,310 Stage: Train 0.5 | Epoch: 584 | Iter: 440000 | Total Loss: 0.002201 | Recon Loss: 0.001824 | Commit Loss: 0.000753 | Perplexity: 1083.250835
2025-09-15 09:21:38,310 Saving model at iteration 440000
2025-09-15 09:21:38,745 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_585_step_440000
2025-09-15 09:21:38,957 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_585_step_440000/pytorch_model.bin
2025-09-15 09:21:39,286 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_585_step_440000/optimizer.bin
2025-09-15 09:21:39,286 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_585_step_440000/scheduler.bin
2025-09-15 09:21:39,287 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_585_step_440000/random_states_0.pkl
2025-09-15 09:22:07,228 Stage: Train 0.5 | Epoch: 584 | Iter: 440200 | Total Loss: 0.002121 | Recon Loss: 0.001744 | Commit Loss: 0.000753 | Perplexity: 1083.508652
2025-09-15 09:22:35,110 Stage: Train 0.5 | Epoch: 584 | Iter: 440400 | Total Loss: 0.002184 | Recon Loss: 0.001802 | Commit Loss: 0.000762 | Perplexity: 1084.012942
Trainning Epoch:  88%|████████▊ | 585/665 [16:50:05<2:18:45, 104.07s/it]2025-09-15 09:23:02,973 Stage: Train 0.5 | Epoch: 585 | Iter: 440600 | Total Loss: 0.002192 | Recon Loss: 0.001812 | Commit Loss: 0.000761 | Perplexity: 1083.453583
2025-09-15 09:23:30,603 Stage: Train 0.5 | Epoch: 585 | Iter: 440800 | Total Loss: 0.002138 | Recon Loss: 0.001761 | Commit Loss: 0.000753 | Perplexity: 1082.894335
2025-09-15 09:23:58,250 Stage: Train 0.5 | Epoch: 585 | Iter: 441000 | Total Loss: 0.002161 | Recon Loss: 0.001785 | Commit Loss: 0.000752 | Perplexity: 1080.408163
2025-09-15 09:24:25,737 Stage: Train 0.5 | Epoch: 585 | Iter: 441200 | Total Loss: 0.002165 | Recon Loss: 0.001786 | Commit Loss: 0.000757 | Perplexity: 1086.288063
Trainning Epoch:  88%|████████▊ | 586/665 [16:51:49<2:16:56, 104.01s/it]2025-09-15 09:24:53,215 Stage: Train 0.5 | Epoch: 586 | Iter: 441400 | Total Loss: 0.002207 | Recon Loss: 0.001828 | Commit Loss: 0.000758 | Perplexity: 1084.578142
2025-09-15 09:25:20,997 Stage: Train 0.5 | Epoch: 586 | Iter: 441600 | Total Loss: 0.002114 | Recon Loss: 0.001738 | Commit Loss: 0.000752 | Perplexity: 1083.519703
2025-09-15 09:25:48,535 Stage: Train 0.5 | Epoch: 586 | Iter: 441800 | Total Loss: 0.002147 | Recon Loss: 0.001769 | Commit Loss: 0.000756 | Perplexity: 1084.407576
2025-09-15 09:26:16,011 Stage: Train 0.5 | Epoch: 586 | Iter: 442000 | Total Loss: 0.002250 | Recon Loss: 0.001872 | Commit Loss: 0.000757 | Perplexity: 1079.099937
Trainning Epoch:  88%|████████▊ | 587/665 [16:53:33<2:15:08, 103.95s/it]2025-09-15 09:26:43,569 Stage: Train 0.5 | Epoch: 587 | Iter: 442200 | Total Loss: 0.002148 | Recon Loss: 0.001772 | Commit Loss: 0.000752 | Perplexity: 1082.543616
2025-09-15 09:27:11,245 Stage: Train 0.5 | Epoch: 587 | Iter: 442400 | Total Loss: 0.002165 | Recon Loss: 0.001786 | Commit Loss: 0.000757 | Perplexity: 1084.656033
2025-09-15 09:27:38,847 Stage: Train 0.5 | Epoch: 587 | Iter: 442600 | Total Loss: 0.002196 | Recon Loss: 0.001817 | Commit Loss: 0.000759 | Perplexity: 1082.209111
Trainning Epoch:  88%|████████▊ | 588/665 [16:55:17<2:13:23, 103.94s/it]2025-09-15 09:28:06,380 Stage: Train 0.5 | Epoch: 588 | Iter: 442800 | Total Loss: 0.002161 | Recon Loss: 0.001785 | Commit Loss: 0.000751 | Perplexity: 1078.311622
2025-09-15 09:28:34,108 Stage: Train 0.5 | Epoch: 588 | Iter: 443000 | Total Loss: 0.002146 | Recon Loss: 0.001770 | Commit Loss: 0.000752 | Perplexity: 1081.948169
2025-09-15 09:29:01,709 Stage: Train 0.5 | Epoch: 588 | Iter: 443200 | Total Loss: 0.002149 | Recon Loss: 0.001768 | Commit Loss: 0.000762 | Perplexity: 1084.605652
2025-09-15 09:29:29,306 Stage: Train 0.5 | Epoch: 588 | Iter: 443400 | Total Loss: 0.002139 | Recon Loss: 0.001762 | Commit Loss: 0.000754 | Perplexity: 1084.999663
Trainning Epoch:  89%|████████▊ | 589/665 [16:57:01<2:11:42, 103.97s/it]2025-09-15 09:29:56,934 Stage: Train 0.5 | Epoch: 589 | Iter: 443600 | Total Loss: 0.002193 | Recon Loss: 0.001817 | Commit Loss: 0.000752 | Perplexity: 1080.104370
2025-09-15 09:30:24,614 Stage: Train 0.5 | Epoch: 589 | Iter: 443800 | Total Loss: 0.002174 | Recon Loss: 0.001797 | Commit Loss: 0.000754 | Perplexity: 1083.115940
2025-09-15 09:30:52,470 Stage: Train 0.5 | Epoch: 589 | Iter: 444000 | Total Loss: 0.002161 | Recon Loss: 0.001784 | Commit Loss: 0.000754 | Perplexity: 1081.687422
2025-09-15 09:31:20,496 Stage: Train 0.5 | Epoch: 589 | Iter: 444200 | Total Loss: 0.002110 | Recon Loss: 0.001731 | Commit Loss: 0.000758 | Perplexity: 1086.598380
Trainning Epoch:  89%|████████▊ | 590/665 [16:58:46<2:10:14, 104.19s/it]2025-09-15 09:31:48,212 Stage: Train 0.5 | Epoch: 590 | Iter: 444400 | Total Loss: 0.002152 | Recon Loss: 0.001775 | Commit Loss: 0.000754 | Perplexity: 1079.828502
2025-09-15 09:32:15,928 Stage: Train 0.5 | Epoch: 590 | Iter: 444600 | Total Loss: 0.002145 | Recon Loss: 0.001767 | Commit Loss: 0.000756 | Perplexity: 1083.730990
2025-09-15 09:32:43,443 Stage: Train 0.5 | Epoch: 590 | Iter: 444800 | Total Loss: 0.002169 | Recon Loss: 0.001794 | Commit Loss: 0.000750 | Perplexity: 1082.870022
2025-09-15 09:33:11,115 Stage: Train 0.5 | Epoch: 590 | Iter: 445000 | Total Loss: 0.002140 | Recon Loss: 0.001761 | Commit Loss: 0.000759 | Perplexity: 1085.441888
Trainning Epoch:  89%|████████▉ | 591/665 [17:00:30<2:08:30, 104.19s/it]2025-09-15 09:33:38,857 Stage: Train 0.5 | Epoch: 591 | Iter: 445200 | Total Loss: 0.002139 | Recon Loss: 0.001764 | Commit Loss: 0.000749 | Perplexity: 1080.138797
2025-09-15 09:34:06,362 Stage: Train 0.5 | Epoch: 591 | Iter: 445400 | Total Loss: 0.002214 | Recon Loss: 0.001835 | Commit Loss: 0.000759 | Perplexity: 1085.999220
2025-09-15 09:34:33,825 Stage: Train 0.5 | Epoch: 591 | Iter: 445600 | Total Loss: 0.002123 | Recon Loss: 0.001748 | Commit Loss: 0.000750 | Perplexity: 1080.335978
Trainning Epoch:  89%|████████▉ | 592/665 [17:02:14<2:06:33, 104.02s/it]2025-09-15 09:35:01,272 Stage: Train 0.5 | Epoch: 592 | Iter: 445800 | Total Loss: 0.002190 | Recon Loss: 0.001812 | Commit Loss: 0.000756 | Perplexity: 1079.874869
2025-09-15 09:35:28,823 Stage: Train 0.5 | Epoch: 592 | Iter: 446000 | Total Loss: 0.002153 | Recon Loss: 0.001777 | Commit Loss: 0.000752 | Perplexity: 1084.240621
2025-09-15 09:35:56,250 Stage: Train 0.5 | Epoch: 592 | Iter: 446200 | Total Loss: 0.002175 | Recon Loss: 0.001798 | Commit Loss: 0.000755 | Perplexity: 1084.589913
2025-09-15 09:36:23,695 Stage: Train 0.5 | Epoch: 592 | Iter: 446400 | Total Loss: 0.002172 | Recon Loss: 0.001795 | Commit Loss: 0.000755 | Perplexity: 1080.236312
Trainning Epoch:  89%|████████▉ | 593/665 [17:03:57<2:04:36, 103.84s/it]2025-09-15 09:36:51,141 Stage: Train 0.5 | Epoch: 593 | Iter: 446600 | Total Loss: 0.002187 | Recon Loss: 0.001812 | Commit Loss: 0.000750 | Perplexity: 1080.112895
2025-09-15 09:37:18,666 Stage: Train 0.5 | Epoch: 593 | Iter: 446800 | Total Loss: 0.002131 | Recon Loss: 0.001755 | Commit Loss: 0.000753 | Perplexity: 1084.904170
2025-09-15 09:37:46,332 Stage: Train 0.5 | Epoch: 593 | Iter: 447000 | Total Loss: 0.002125 | Recon Loss: 0.001747 | Commit Loss: 0.000756 | Perplexity: 1083.305649
2025-09-15 09:38:13,895 Stage: Train 0.5 | Epoch: 593 | Iter: 447200 | Total Loss: 0.002180 | Recon Loss: 0.001805 | Commit Loss: 0.000751 | Perplexity: 1079.381827
Trainning Epoch:  89%|████████▉ | 594/665 [17:05:41<2:02:52, 103.84s/it]2025-09-15 09:38:41,595 Stage: Train 0.5 | Epoch: 594 | Iter: 447400 | Total Loss: 0.002202 | Recon Loss: 0.001825 | Commit Loss: 0.000754 | Perplexity: 1083.479081
2025-09-15 09:39:09,185 Stage: Train 0.5 | Epoch: 594 | Iter: 447600 | Total Loss: 0.002130 | Recon Loss: 0.001754 | Commit Loss: 0.000751 | Perplexity: 1083.939136
2025-09-15 09:39:36,611 Stage: Train 0.5 | Epoch: 594 | Iter: 447800 | Total Loss: 0.002194 | Recon Loss: 0.001815 | Commit Loss: 0.000758 | Perplexity: 1083.566398
2025-09-15 09:40:04,045 Stage: Train 0.5 | Epoch: 594 | Iter: 448000 | Total Loss: 0.002108 | Recon Loss: 0.001732 | Commit Loss: 0.000750 | Perplexity: 1077.622694
Trainning Epoch:  89%|████████▉ | 595/665 [17:07:24<2:01:03, 103.76s/it]2025-09-15 09:40:31,425 Stage: Train 0.5 | Epoch: 595 | Iter: 448200 | Total Loss: 0.002205 | Recon Loss: 0.001826 | Commit Loss: 0.000758 | Perplexity: 1086.140390
2025-09-15 09:40:58,847 Stage: Train 0.5 | Epoch: 595 | Iter: 448400 | Total Loss: 0.002142 | Recon Loss: 0.001765 | Commit Loss: 0.000754 | Perplexity: 1082.033392
2025-09-15 09:41:26,276 Stage: Train 0.5 | Epoch: 595 | Iter: 448600 | Total Loss: 0.002103 | Recon Loss: 0.001728 | Commit Loss: 0.000749 | Perplexity: 1083.810205
Trainning Epoch:  90%|████████▉ | 596/665 [17:09:08<1:59:09, 103.61s/it]2025-09-15 09:41:53,733 Stage: Train 0.5 | Epoch: 596 | Iter: 448800 | Total Loss: 0.002143 | Recon Loss: 0.001767 | Commit Loss: 0.000752 | Perplexity: 1079.673066
2025-09-15 09:42:21,479 Stage: Train 0.5 | Epoch: 596 | Iter: 449000 | Total Loss: 0.002155 | Recon Loss: 0.001777 | Commit Loss: 0.000756 | Perplexity: 1085.949979
2025-09-15 09:42:48,904 Stage: Train 0.5 | Epoch: 596 | Iter: 449200 | Total Loss: 0.002130 | Recon Loss: 0.001752 | Commit Loss: 0.000756 | Perplexity: 1083.234658
2025-09-15 09:43:16,281 Stage: Train 0.5 | Epoch: 596 | Iter: 449400 | Total Loss: 0.002145 | Recon Loss: 0.001768 | Commit Loss: 0.000754 | Perplexity: 1081.400503
Trainning Epoch:  90%|████████▉ | 597/665 [17:10:51<1:57:22, 103.57s/it]2025-09-15 09:43:43,627 Stage: Train 0.5 | Epoch: 597 | Iter: 449600 | Total Loss: 0.002109 | Recon Loss: 0.001736 | Commit Loss: 0.000746 | Perplexity: 1079.240166
2025-09-15 09:44:11,035 Stage: Train 0.5 | Epoch: 597 | Iter: 449800 | Total Loss: 0.002171 | Recon Loss: 0.001789 | Commit Loss: 0.000765 | Perplexity: 1084.175442
2025-09-15 09:44:38,482 Stage: Train 0.5 | Epoch: 597 | Iter: 450000 | Total Loss: 0.002122 | Recon Loss: 0.001745 | Commit Loss: 0.000755 | Perplexity: 1085.267756
2025-09-15 09:45:05,919 Stage: Train 0.5 | Epoch: 597 | Iter: 450200 | Total Loss: 0.002167 | Recon Loss: 0.001787 | Commit Loss: 0.000760 | Perplexity: 1084.442071
Trainning Epoch:  90%|████████▉ | 598/665 [17:12:34<1:55:32, 103.47s/it]2025-09-15 09:45:33,336 Stage: Train 0.5 | Epoch: 598 | Iter: 450400 | Total Loss: 0.002138 | Recon Loss: 0.001762 | Commit Loss: 0.000752 | Perplexity: 1078.308015
2025-09-15 09:46:00,917 Stage: Train 0.5 | Epoch: 598 | Iter: 450600 | Total Loss: 0.002110 | Recon Loss: 0.001734 | Commit Loss: 0.000751 | Perplexity: 1083.475831
2025-09-15 09:46:28,654 Stage: Train 0.5 | Epoch: 598 | Iter: 450800 | Total Loss: 0.002196 | Recon Loss: 0.001819 | Commit Loss: 0.000755 | Perplexity: 1085.723625
2025-09-15 09:46:56,323 Stage: Train 0.5 | Epoch: 598 | Iter: 451000 | Total Loss: 0.002287 | Recon Loss: 0.001908 | Commit Loss: 0.000757 | Perplexity: 1079.741924
Trainning Epoch:  90%|█████████ | 599/665 [17:14:18<1:53:58, 103.62s/it]2025-09-15 09:47:23,800 Stage: Train 0.5 | Epoch: 599 | Iter: 451200 | Total Loss: 0.002129 | Recon Loss: 0.001753 | Commit Loss: 0.000751 | Perplexity: 1079.731789
2025-09-15 09:47:51,652 Stage: Train 0.5 | Epoch: 599 | Iter: 451400 | Total Loss: 0.002128 | Recon Loss: 0.001750 | Commit Loss: 0.000755 | Perplexity: 1084.604673
2025-09-15 09:48:19,214 Stage: Train 0.5 | Epoch: 599 | Iter: 451600 | Total Loss: 0.002134 | Recon Loss: 0.001760 | Commit Loss: 0.000749 | Perplexity: 1080.158167
2025-09-15 09:48:46,636 Stage: Train 0.5 | Epoch: 599 | Iter: 451800 | Total Loss: 0.002180 | Recon Loss: 0.001802 | Commit Loss: 0.000756 | Perplexity: 1081.027008
Trainning Epoch:  90%|█████████ | 600/665 [17:16:02<1:52:20, 103.70s/it]2025-09-15 09:49:14,209 Stage: Train 0.5 | Epoch: 600 | Iter: 452000 | Total Loss: 0.002135 | Recon Loss: 0.001759 | Commit Loss: 0.000752 | Perplexity: 1083.336587
2025-09-15 09:49:41,731 Stage: Train 0.5 | Epoch: 600 | Iter: 452200 | Total Loss: 0.002108 | Recon Loss: 0.001733 | Commit Loss: 0.000751 | Perplexity: 1084.564852
2025-09-15 09:50:09,202 Stage: Train 0.5 | Epoch: 600 | Iter: 452400 | Total Loss: 0.002158 | Recon Loss: 0.001781 | Commit Loss: 0.000754 | Perplexity: 1080.993100
Trainning Epoch:  90%|█████████ | 601/665 [17:17:46<1:50:33, 103.65s/it]2025-09-15 09:50:36,615 Stage: Train 0.5 | Epoch: 601 | Iter: 452600 | Total Loss: 0.002133 | Recon Loss: 0.001756 | Commit Loss: 0.000754 | Perplexity: 1083.865640
2025-09-15 09:51:04,047 Stage: Train 0.5 | Epoch: 601 | Iter: 452800 | Total Loss: 0.002203 | Recon Loss: 0.001829 | Commit Loss: 0.000750 | Perplexity: 1078.729005
2025-09-15 09:51:31,508 Stage: Train 0.5 | Epoch: 601 | Iter: 453000 | Total Loss: 0.002144 | Recon Loss: 0.001765 | Commit Loss: 0.000760 | Perplexity: 1087.069011
2025-09-15 09:51:58,943 Stage: Train 0.5 | Epoch: 601 | Iter: 453200 | Total Loss: 0.002121 | Recon Loss: 0.001746 | Commit Loss: 0.000751 | Perplexity: 1083.603867
Trainning Epoch:  91%|█████████ | 602/665 [17:19:29<1:48:43, 103.54s/it]2025-09-15 09:52:26,428 Stage: Train 0.5 | Epoch: 602 | Iter: 453400 | Total Loss: 0.002149 | Recon Loss: 0.001775 | Commit Loss: 0.000748 | Perplexity: 1081.039500
2025-09-15 09:52:53,910 Stage: Train 0.5 | Epoch: 602 | Iter: 453600 | Total Loss: 0.002145 | Recon Loss: 0.001768 | Commit Loss: 0.000754 | Perplexity: 1084.713842
2025-09-15 09:53:21,488 Stage: Train 0.5 | Epoch: 602 | Iter: 453800 | Total Loss: 0.002130 | Recon Loss: 0.001756 | Commit Loss: 0.000749 | Perplexity: 1082.066659
2025-09-15 09:53:49,237 Stage: Train 0.5 | Epoch: 602 | Iter: 454000 | Total Loss: 0.002183 | Recon Loss: 0.001806 | Commit Loss: 0.000754 | Perplexity: 1081.178582
Trainning Epoch:  91%|█████████ | 603/665 [17:21:13<1:47:06, 103.66s/it]2025-09-15 09:54:16,859 Stage: Train 0.5 | Epoch: 603 | Iter: 454200 | Total Loss: 0.002105 | Recon Loss: 0.001731 | Commit Loss: 0.000750 | Perplexity: 1082.296217
2025-09-15 09:54:44,507 Stage: Train 0.5 | Epoch: 603 | Iter: 454400 | Total Loss: 0.002179 | Recon Loss: 0.001803 | Commit Loss: 0.000754 | Perplexity: 1082.565422
2025-09-15 09:55:12,089 Stage: Train 0.5 | Epoch: 603 | Iter: 454600 | Total Loss: 0.002162 | Recon Loss: 0.001789 | Commit Loss: 0.000746 | Perplexity: 1078.465904
2025-09-15 09:55:39,674 Stage: Train 0.5 | Epoch: 603 | Iter: 454800 | Total Loss: 0.002164 | Recon Loss: 0.001785 | Commit Loss: 0.000757 | Perplexity: 1086.618088
Trainning Epoch:  91%|█████████ | 604/665 [17:22:57<1:45:27, 103.73s/it]2025-09-15 09:56:07,273 Stage: Train 0.5 | Epoch: 604 | Iter: 455000 | Total Loss: 0.002128 | Recon Loss: 0.001748 | Commit Loss: 0.000760 | Perplexity: 1082.770269
2025-09-15 09:56:34,850 Stage: Train 0.5 | Epoch: 604 | Iter: 455200 | Total Loss: 0.002147 | Recon Loss: 0.001772 | Commit Loss: 0.000751 | Perplexity: 1083.006646
2025-09-15 09:57:02,390 Stage: Train 0.5 | Epoch: 604 | Iter: 455400 | Total Loss: 0.002157 | Recon Loss: 0.001781 | Commit Loss: 0.000752 | Perplexity: 1084.180810
Trainning Epoch:  91%|█████████ | 605/665 [17:24:41<1:43:45, 103.75s/it]2025-09-15 09:57:29,911 Stage: Train 0.5 | Epoch: 605 | Iter: 455600 | Total Loss: 0.002151 | Recon Loss: 0.001773 | Commit Loss: 0.000755 | Perplexity: 1084.351111
2025-09-15 09:57:57,485 Stage: Train 0.5 | Epoch: 605 | Iter: 455800 | Total Loss: 0.002146 | Recon Loss: 0.001772 | Commit Loss: 0.000748 | Perplexity: 1084.024872
2025-09-15 09:58:25,026 Stage: Train 0.5 | Epoch: 605 | Iter: 456000 | Total Loss: 0.002171 | Recon Loss: 0.001794 | Commit Loss: 0.000753 | Perplexity: 1085.335324
2025-09-15 09:58:52,560 Stage: Train 0.5 | Epoch: 605 | Iter: 456200 | Total Loss: 0.002165 | Recon Loss: 0.001793 | Commit Loss: 0.000745 | Perplexity: 1078.394904
Trainning Epoch:  91%|█████████ | 606/665 [17:26:24<1:42:00, 103.73s/it]2025-09-15 09:59:20,165 Stage: Train 0.5 | Epoch: 606 | Iter: 456400 | Total Loss: 0.002111 | Recon Loss: 0.001734 | Commit Loss: 0.000754 | Perplexity: 1084.767033
2025-09-15 09:59:47,708 Stage: Train 0.5 | Epoch: 606 | Iter: 456600 | Total Loss: 0.002181 | Recon Loss: 0.001803 | Commit Loss: 0.000755 | Perplexity: 1087.460437
2025-09-15 10:00:15,295 Stage: Train 0.5 | Epoch: 606 | Iter: 456800 | Total Loss: 0.002214 | Recon Loss: 0.001839 | Commit Loss: 0.000749 | Perplexity: 1081.916770
2025-09-15 10:00:42,815 Stage: Train 0.5 | Epoch: 606 | Iter: 457000 | Total Loss: 0.002131 | Recon Loss: 0.001759 | Commit Loss: 0.000746 | Perplexity: 1081.932971
Trainning Epoch:  91%|█████████▏| 607/665 [17:28:08<1:40:16, 103.74s/it]2025-09-15 10:01:10,286 Stage: Train 0.5 | Epoch: 607 | Iter: 457200 | Total Loss: 0.002121 | Recon Loss: 0.001746 | Commit Loss: 0.000750 | Perplexity: 1081.140634
2025-09-15 10:01:37,786 Stage: Train 0.5 | Epoch: 607 | Iter: 457400 | Total Loss: 0.002133 | Recon Loss: 0.001760 | Commit Loss: 0.000747 | Perplexity: 1083.017653
2025-09-15 10:02:05,329 Stage: Train 0.5 | Epoch: 607 | Iter: 457600 | Total Loss: 0.002151 | Recon Loss: 0.001775 | Commit Loss: 0.000751 | Perplexity: 1084.959093
2025-09-15 10:02:32,885 Stage: Train 0.5 | Epoch: 607 | Iter: 457800 | Total Loss: 0.002141 | Recon Loss: 0.001765 | Commit Loss: 0.000752 | Perplexity: 1084.535090
Trainning Epoch:  91%|█████████▏| 608/665 [17:29:52<1:38:30, 103.70s/it]2025-09-15 10:03:00,465 Stage: Train 0.5 | Epoch: 608 | Iter: 458000 | Total Loss: 0.002104 | Recon Loss: 0.001731 | Commit Loss: 0.000747 | Perplexity: 1079.856472
2025-09-15 10:03:28,039 Stage: Train 0.5 | Epoch: 608 | Iter: 458200 | Total Loss: 0.002149 | Recon Loss: 0.001775 | Commit Loss: 0.000750 | Perplexity: 1082.655914
2025-09-15 10:03:55,632 Stage: Train 0.5 | Epoch: 608 | Iter: 458400 | Total Loss: 0.002167 | Recon Loss: 0.001791 | Commit Loss: 0.000751 | Perplexity: 1083.775048
Trainning Epoch:  92%|█████████▏| 609/665 [17:31:36<1:36:50, 103.75s/it]2025-09-15 10:04:23,172 Stage: Train 0.5 | Epoch: 609 | Iter: 458600 | Total Loss: 0.002146 | Recon Loss: 0.001767 | Commit Loss: 0.000757 | Perplexity: 1084.486259
2025-09-15 10:04:50,775 Stage: Train 0.5 | Epoch: 609 | Iter: 458800 | Total Loss: 0.002109 | Recon Loss: 0.001736 | Commit Loss: 0.000746 | Perplexity: 1083.081711
2025-09-15 10:05:18,356 Stage: Train 0.5 | Epoch: 609 | Iter: 459000 | Total Loss: 0.002123 | Recon Loss: 0.001749 | Commit Loss: 0.000748 | Perplexity: 1080.492805
2025-09-15 10:05:46,118 Stage: Train 0.5 | Epoch: 609 | Iter: 459200 | Total Loss: 0.002135 | Recon Loss: 0.001759 | Commit Loss: 0.000751 | Perplexity: 1086.335992
Trainning Epoch:  92%|█████████▏| 610/665 [17:33:20<1:35:09, 103.81s/it]2025-09-15 10:06:13,711 Stage: Train 0.5 | Epoch: 610 | Iter: 459400 | Total Loss: 0.002165 | Recon Loss: 0.001791 | Commit Loss: 0.000748 | Perplexity: 1081.299948
2025-09-15 10:06:41,263 Stage: Train 0.5 | Epoch: 610 | Iter: 459600 | Total Loss: 0.002113 | Recon Loss: 0.001739 | Commit Loss: 0.000748 | Perplexity: 1081.769438
2025-09-15 10:07:08,760 Stage: Train 0.5 | Epoch: 610 | Iter: 459800 | Total Loss: 0.002137 | Recon Loss: 0.001762 | Commit Loss: 0.000750 | Perplexity: 1083.449736
2025-09-15 10:07:36,281 Stage: Train 0.5 | Epoch: 610 | Iter: 460000 | Total Loss: 0.002181 | Recon Loss: 0.001802 | Commit Loss: 0.000757 | Perplexity: 1084.986574
2025-09-15 10:07:36,281 Saving model at iteration 460000
2025-09-15 10:07:36,765 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_611_step_460000
2025-09-15 10:07:36,971 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_611_step_460000/pytorch_model.bin
2025-09-15 10:07:37,303 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_611_step_460000/optimizer.bin
2025-09-15 10:07:37,303 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_611_step_460000/scheduler.bin
2025-09-15 10:07:37,304 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_611_step_460000/random_states_0.pkl
Trainning Epoch:  92%|█████████▏| 611/665 [17:35:05<1:33:49, 104.24s/it]2025-09-15 10:08:05,270 Stage: Train 0.5 | Epoch: 611 | Iter: 460200 | Total Loss: 0.002093 | Recon Loss: 0.001719 | Commit Loss: 0.000747 | Perplexity: 1079.351578
2025-09-15 10:08:33,124 Stage: Train 0.5 | Epoch: 611 | Iter: 460400 | Total Loss: 0.002134 | Recon Loss: 0.001758 | Commit Loss: 0.000752 | Perplexity: 1085.264099
2025-09-15 10:09:00,717 Stage: Train 0.5 | Epoch: 611 | Iter: 460600 | Total Loss: 0.002139 | Recon Loss: 0.001762 | Commit Loss: 0.000752 | Perplexity: 1084.239077
2025-09-15 10:09:28,168 Stage: Train 0.5 | Epoch: 611 | Iter: 460800 | Total Loss: 0.002180 | Recon Loss: 0.001803 | Commit Loss: 0.000754 | Perplexity: 1084.874220
Trainning Epoch:  92%|█████████▏| 612/665 [17:36:49<1:31:58, 104.12s/it]2025-09-15 10:09:55,538 Stage: Train 0.5 | Epoch: 612 | Iter: 461000 | Total Loss: 0.002147 | Recon Loss: 0.001769 | Commit Loss: 0.000756 | Perplexity: 1082.417345
2025-09-15 10:10:22,961 Stage: Train 0.5 | Epoch: 612 | Iter: 461200 | Total Loss: 0.002115 | Recon Loss: 0.001737 | Commit Loss: 0.000756 | Perplexity: 1084.204184
2025-09-15 10:10:50,364 Stage: Train 0.5 | Epoch: 612 | Iter: 461400 | Total Loss: 0.002189 | Recon Loss: 0.001814 | Commit Loss: 0.000749 | Perplexity: 1078.807500
Trainning Epoch:  92%|█████████▏| 613/665 [17:38:32<1:30:06, 103.97s/it]2025-09-15 10:11:18,222 Stage: Train 0.5 | Epoch: 613 | Iter: 461600 | Total Loss: 0.002096 | Recon Loss: 0.001718 | Commit Loss: 0.000755 | Perplexity: 1082.761463
2025-09-15 10:11:45,822 Stage: Train 0.5 | Epoch: 613 | Iter: 461800 | Total Loss: 0.002120 | Recon Loss: 0.001746 | Commit Loss: 0.000749 | Perplexity: 1082.067742
2025-09-15 10:12:13,371 Stage: Train 0.5 | Epoch: 613 | Iter: 462000 | Total Loss: 0.002150 | Recon Loss: 0.001774 | Commit Loss: 0.000751 | Perplexity: 1083.163873
2025-09-15 10:12:40,902 Stage: Train 0.5 | Epoch: 613 | Iter: 462200 | Total Loss: 0.002123 | Recon Loss: 0.001749 | Commit Loss: 0.000748 | Perplexity: 1079.994660
Trainning Epoch:  92%|█████████▏| 614/665 [17:40:16<1:28:18, 103.90s/it]2025-09-15 10:13:08,369 Stage: Train 0.5 | Epoch: 614 | Iter: 462400 | Total Loss: 0.002130 | Recon Loss: 0.001754 | Commit Loss: 0.000753 | Perplexity: 1084.441182
2025-09-15 10:13:35,838 Stage: Train 0.5 | Epoch: 614 | Iter: 462600 | Total Loss: 0.002111 | Recon Loss: 0.001736 | Commit Loss: 0.000751 | Perplexity: 1086.134809
2025-09-15 10:14:03,237 Stage: Train 0.5 | Epoch: 614 | Iter: 462800 | Total Loss: 0.002116 | Recon Loss: 0.001742 | Commit Loss: 0.000749 | Perplexity: 1080.763505
2025-09-15 10:14:30,755 Stage: Train 0.5 | Epoch: 614 | Iter: 463000 | Total Loss: 0.002118 | Recon Loss: 0.001742 | Commit Loss: 0.000751 | Perplexity: 1083.301229
Trainning Epoch:  92%|█████████▏| 615/665 [17:41:59<1:26:26, 103.73s/it]2025-09-15 10:14:58,125 Stage: Train 0.5 | Epoch: 615 | Iter: 463200 | Total Loss: 0.002168 | Recon Loss: 0.001791 | Commit Loss: 0.000755 | Perplexity: 1085.301379
2025-09-15 10:15:25,529 Stage: Train 0.5 | Epoch: 615 | Iter: 463400 | Total Loss: 0.002128 | Recon Loss: 0.001754 | Commit Loss: 0.000748 | Perplexity: 1082.233131
2025-09-15 10:15:53,016 Stage: Train 0.5 | Epoch: 615 | Iter: 463600 | Total Loss: 0.002122 | Recon Loss: 0.001746 | Commit Loss: 0.000752 | Perplexity: 1083.011445
2025-09-15 10:16:20,575 Stage: Train 0.5 | Epoch: 615 | Iter: 463800 | Total Loss: 0.002129 | Recon Loss: 0.001754 | Commit Loss: 0.000750 | Perplexity: 1084.122694
Trainning Epoch:  93%|█████████▎| 616/665 [17:43:43<1:24:38, 103.63s/it]2025-09-15 10:16:48,122 Stage: Train 0.5 | Epoch: 616 | Iter: 464000 | Total Loss: 0.002135 | Recon Loss: 0.001760 | Commit Loss: 0.000750 | Perplexity: 1084.677020
2025-09-15 10:17:15,668 Stage: Train 0.5 | Epoch: 616 | Iter: 464200 | Total Loss: 0.002127 | Recon Loss: 0.001752 | Commit Loss: 0.000750 | Perplexity: 1086.340448
2025-09-15 10:17:43,210 Stage: Train 0.5 | Epoch: 616 | Iter: 464400 | Total Loss: 0.002138 | Recon Loss: 0.001764 | Commit Loss: 0.000749 | Perplexity: 1081.394362
2025-09-15 10:18:11,019 Stage: Train 0.5 | Epoch: 616 | Iter: 464600 | Total Loss: 0.002134 | Recon Loss: 0.001758 | Commit Loss: 0.000753 | Perplexity: 1083.233586
Trainning Epoch:  93%|█████████▎| 617/665 [17:45:27<1:22:59, 103.73s/it]2025-09-15 10:18:38,578 Stage: Train 0.5 | Epoch: 617 | Iter: 464800 | Total Loss: 0.002119 | Recon Loss: 0.001744 | Commit Loss: 0.000750 | Perplexity: 1084.245848
2025-09-15 10:19:06,111 Stage: Train 0.5 | Epoch: 617 | Iter: 465000 | Total Loss: 0.002219 | Recon Loss: 0.001842 | Commit Loss: 0.000754 | Perplexity: 1084.238551
2025-09-15 10:19:33,597 Stage: Train 0.5 | Epoch: 617 | Iter: 465200 | Total Loss: 0.002084 | Recon Loss: 0.001709 | Commit Loss: 0.000750 | Perplexity: 1084.403208
Trainning Epoch:  93%|█████████▎| 618/665 [17:47:10<1:21:13, 103.70s/it]2025-09-15 10:20:01,086 Stage: Train 0.5 | Epoch: 618 | Iter: 465400 | Total Loss: 0.002126 | Recon Loss: 0.001753 | Commit Loss: 0.000744 | Perplexity: 1077.199357
2025-09-15 10:20:28,619 Stage: Train 0.5 | Epoch: 618 | Iter: 465600 | Total Loss: 0.002112 | Recon Loss: 0.001738 | Commit Loss: 0.000747 | Perplexity: 1087.031667
2025-09-15 10:20:56,122 Stage: Train 0.5 | Epoch: 618 | Iter: 465800 | Total Loss: 0.002153 | Recon Loss: 0.001777 | Commit Loss: 0.000752 | Perplexity: 1082.475424
2025-09-15 10:21:23,746 Stage: Train 0.5 | Epoch: 618 | Iter: 466000 | Total Loss: 0.002139 | Recon Loss: 0.001763 | Commit Loss: 0.000752 | Perplexity: 1083.352165
Trainning Epoch:  93%|█████████▎| 619/665 [17:48:54<1:19:31, 103.73s/it]2025-09-15 10:21:51,301 Stage: Train 0.5 | Epoch: 619 | Iter: 466200 | Total Loss: 0.002138 | Recon Loss: 0.001764 | Commit Loss: 0.000748 | Perplexity: 1082.234464
2025-09-15 10:22:18,861 Stage: Train 0.5 | Epoch: 619 | Iter: 466400 | Total Loss: 0.002155 | Recon Loss: 0.001784 | Commit Loss: 0.000742 | Perplexity: 1081.305131
2025-09-15 10:22:46,343 Stage: Train 0.5 | Epoch: 619 | Iter: 466600 | Total Loss: 0.002116 | Recon Loss: 0.001744 | Commit Loss: 0.000744 | Perplexity: 1080.604965
2025-09-15 10:23:13,843 Stage: Train 0.5 | Epoch: 619 | Iter: 466800 | Total Loss: 0.002183 | Recon Loss: 0.001804 | Commit Loss: 0.000759 | Perplexity: 1089.183246
Trainning Epoch:  93%|█████████▎| 620/665 [17:50:38<1:17:44, 103.67s/it]2025-09-15 10:23:41,329 Stage: Train 0.5 | Epoch: 620 | Iter: 467000 | Total Loss: 0.002123 | Recon Loss: 0.001750 | Commit Loss: 0.000746 | Perplexity: 1086.545223
2025-09-15 10:24:08,809 Stage: Train 0.5 | Epoch: 620 | Iter: 467200 | Total Loss: 0.002112 | Recon Loss: 0.001737 | Commit Loss: 0.000749 | Perplexity: 1080.688843
2025-09-15 10:24:36,332 Stage: Train 0.5 | Epoch: 620 | Iter: 467400 | Total Loss: 0.002164 | Recon Loss: 0.001791 | Commit Loss: 0.000746 | Perplexity: 1085.053835
2025-09-15 10:25:03,971 Stage: Train 0.5 | Epoch: 620 | Iter: 467600 | Total Loss: 0.002135 | Recon Loss: 0.001760 | Commit Loss: 0.000750 | Perplexity: 1081.106704
Trainning Epoch:  93%|█████████▎| 621/665 [17:52:21<1:16:01, 103.68s/it]2025-09-15 10:25:31,588 Stage: Train 0.5 | Epoch: 621 | Iter: 467800 | Total Loss: 0.002141 | Recon Loss: 0.001768 | Commit Loss: 0.000746 | Perplexity: 1083.335760
2025-09-15 10:25:59,223 Stage: Train 0.5 | Epoch: 621 | Iter: 468000 | Total Loss: 0.002113 | Recon Loss: 0.001741 | Commit Loss: 0.000743 | Perplexity: 1082.073896
2025-09-15 10:26:26,862 Stage: Train 0.5 | Epoch: 621 | Iter: 468200 | Total Loss: 0.002143 | Recon Loss: 0.001765 | Commit Loss: 0.000756 | Perplexity: 1089.611996
Trainning Epoch:  94%|█████████▎| 622/665 [17:54:05<1:14:23, 103.79s/it]2025-09-15 10:26:54,505 Stage: Train 0.5 | Epoch: 622 | Iter: 468400 | Total Loss: 0.002148 | Recon Loss: 0.001773 | Commit Loss: 0.000750 | Perplexity: 1079.513977
2025-09-15 10:27:21,966 Stage: Train 0.5 | Epoch: 622 | Iter: 468600 | Total Loss: 0.002150 | Recon Loss: 0.001778 | Commit Loss: 0.000744 | Perplexity: 1082.784691
2025-09-15 10:27:49,361 Stage: Train 0.5 | Epoch: 622 | Iter: 468800 | Total Loss: 0.002085 | Recon Loss: 0.001713 | Commit Loss: 0.000744 | Perplexity: 1086.149478
2025-09-15 10:28:16,947 Stage: Train 0.5 | Epoch: 622 | Iter: 469000 | Total Loss: 0.002138 | Recon Loss: 0.001762 | Commit Loss: 0.000753 | Perplexity: 1085.353283
Trainning Epoch:  94%|█████████▎| 623/665 [17:55:49<1:12:35, 103.70s/it]2025-09-15 10:28:44,465 Stage: Train 0.5 | Epoch: 623 | Iter: 469200 | Total Loss: 0.002147 | Recon Loss: 0.001772 | Commit Loss: 0.000752 | Perplexity: 1079.127965
2025-09-15 10:29:12,054 Stage: Train 0.5 | Epoch: 623 | Iter: 469400 | Total Loss: 0.002115 | Recon Loss: 0.001742 | Commit Loss: 0.000747 | Perplexity: 1087.928324
2025-09-15 10:29:39,852 Stage: Train 0.5 | Epoch: 623 | Iter: 469600 | Total Loss: 0.002128 | Recon Loss: 0.001754 | Commit Loss: 0.000748 | Perplexity: 1082.228959
2025-09-15 10:30:07,276 Stage: Train 0.5 | Epoch: 623 | Iter: 469800 | Total Loss: 0.002127 | Recon Loss: 0.001754 | Commit Loss: 0.000746 | Perplexity: 1084.268253
Trainning Epoch:  94%|█████████▍| 624/665 [17:57:33<1:10:52, 103.73s/it]2025-09-15 10:30:34,638 Stage: Train 0.5 | Epoch: 624 | Iter: 470000 | Total Loss: 0.002110 | Recon Loss: 0.001738 | Commit Loss: 0.000744 | Perplexity: 1083.062992
2025-09-15 10:31:02,060 Stage: Train 0.5 | Epoch: 624 | Iter: 470200 | Total Loss: 0.002117 | Recon Loss: 0.001743 | Commit Loss: 0.000749 | Perplexity: 1084.680311
2025-09-15 10:31:29,578 Stage: Train 0.5 | Epoch: 624 | Iter: 470400 | Total Loss: 0.002107 | Recon Loss: 0.001734 | Commit Loss: 0.000746 | Perplexity: 1081.917478
2025-09-15 10:31:57,231 Stage: Train 0.5 | Epoch: 624 | Iter: 470600 | Total Loss: 0.002152 | Recon Loss: 0.001775 | Commit Loss: 0.000754 | Perplexity: 1083.669814
Trainning Epoch:  94%|█████████▍| 625/665 [17:59:16<1:09:06, 103.67s/it]2025-09-15 10:32:24,795 Stage: Train 0.5 | Epoch: 625 | Iter: 470800 | Total Loss: 0.002101 | Recon Loss: 0.001729 | Commit Loss: 0.000745 | Perplexity: 1080.487114
2025-09-15 10:32:52,634 Stage: Train 0.5 | Epoch: 625 | Iter: 471000 | Total Loss: 0.002151 | Recon Loss: 0.001778 | Commit Loss: 0.000747 | Perplexity: 1084.104230
2025-09-15 10:33:20,264 Stage: Train 0.5 | Epoch: 625 | Iter: 471200 | Total Loss: 0.002124 | Recon Loss: 0.001749 | Commit Loss: 0.000750 | Perplexity: 1086.167719
Trainning Epoch:  94%|█████████▍| 626/665 [18:01:00<1:07:29, 103.82s/it]2025-09-15 10:33:47,830 Stage: Train 0.5 | Epoch: 626 | Iter: 471400 | Total Loss: 0.002129 | Recon Loss: 0.001755 | Commit Loss: 0.000748 | Perplexity: 1079.061533
2025-09-15 10:34:15,377 Stage: Train 0.5 | Epoch: 626 | Iter: 471600 | Total Loss: 0.002083 | Recon Loss: 0.001713 | Commit Loss: 0.000740 | Perplexity: 1082.762558
2025-09-15 10:34:42,933 Stage: Train 0.5 | Epoch: 626 | Iter: 471800 | Total Loss: 0.002114 | Recon Loss: 0.001737 | Commit Loss: 0.000754 | Perplexity: 1084.492166
2025-09-15 10:35:10,774 Stage: Train 0.5 | Epoch: 626 | Iter: 472000 | Total Loss: 0.002148 | Recon Loss: 0.001772 | Commit Loss: 0.000751 | Perplexity: 1086.458592
Trainning Epoch:  94%|█████████▍| 627/665 [18:02:44<1:05:46, 103.86s/it]2025-09-15 10:35:38,211 Stage: Train 0.5 | Epoch: 627 | Iter: 472200 | Total Loss: 0.002136 | Recon Loss: 0.001762 | Commit Loss: 0.000749 | Perplexity: 1081.646035
2025-09-15 10:36:05,664 Stage: Train 0.5 | Epoch: 627 | Iter: 472400 | Total Loss: 0.002107 | Recon Loss: 0.001735 | Commit Loss: 0.000744 | Perplexity: 1081.546074
2025-09-15 10:36:33,058 Stage: Train 0.5 | Epoch: 627 | Iter: 472600 | Total Loss: 0.002132 | Recon Loss: 0.001755 | Commit Loss: 0.000754 | Perplexity: 1087.360981
2025-09-15 10:37:00,744 Stage: Train 0.5 | Epoch: 627 | Iter: 472800 | Total Loss: 0.002102 | Recon Loss: 0.001730 | Commit Loss: 0.000744 | Perplexity: 1086.286905
Trainning Epoch:  94%|█████████▍| 628/665 [18:04:28<1:03:59, 103.76s/it]2025-09-15 10:37:28,277 Stage: Train 0.5 | Epoch: 628 | Iter: 473000 | Total Loss: 0.002144 | Recon Loss: 0.001771 | Commit Loss: 0.000746 | Perplexity: 1083.467745
2025-09-15 10:37:55,807 Stage: Train 0.5 | Epoch: 628 | Iter: 473200 | Total Loss: 0.002105 | Recon Loss: 0.001732 | Commit Loss: 0.000746 | Perplexity: 1084.313865
2025-09-15 10:38:23,429 Stage: Train 0.5 | Epoch: 628 | Iter: 473400 | Total Loss: 0.002171 | Recon Loss: 0.001796 | Commit Loss: 0.000750 | Perplexity: 1087.917444
2025-09-15 10:38:50,965 Stage: Train 0.5 | Epoch: 628 | Iter: 473600 | Total Loss: 0.002158 | Recon Loss: 0.001786 | Commit Loss: 0.000746 | Perplexity: 1078.794369
Trainning Epoch:  95%|█████████▍| 629/665 [18:06:12<1:02:15, 103.75s/it]2025-09-15 10:39:18,500 Stage: Train 0.5 | Epoch: 629 | Iter: 473800 | Total Loss: 0.002095 | Recon Loss: 0.001722 | Commit Loss: 0.000747 | Perplexity: 1083.051932
2025-09-15 10:39:46,048 Stage: Train 0.5 | Epoch: 629 | Iter: 474000 | Total Loss: 0.002116 | Recon Loss: 0.001743 | Commit Loss: 0.000745 | Perplexity: 1082.472184
2025-09-15 10:40:13,614 Stage: Train 0.5 | Epoch: 629 | Iter: 474200 | Total Loss: 0.002141 | Recon Loss: 0.001765 | Commit Loss: 0.000752 | Perplexity: 1087.932743
Trainning Epoch:  95%|█████████▍| 630/665 [18:07:55<1:00:31, 103.75s/it]2025-09-15 10:40:41,142 Stage: Train 0.5 | Epoch: 630 | Iter: 474400 | Total Loss: 0.002111 | Recon Loss: 0.001739 | Commit Loss: 0.000744 | Perplexity: 1080.741132
2025-09-15 10:41:08,680 Stage: Train 0.5 | Epoch: 630 | Iter: 474600 | Total Loss: 0.002117 | Recon Loss: 0.001745 | Commit Loss: 0.000743 | Perplexity: 1083.716395
2025-09-15 10:41:36,197 Stage: Train 0.5 | Epoch: 630 | Iter: 474800 | Total Loss: 0.002102 | Recon Loss: 0.001728 | Commit Loss: 0.000748 | Perplexity: 1085.435310
2025-09-15 10:42:03,800 Stage: Train 0.5 | Epoch: 630 | Iter: 475000 | Total Loss: 0.002116 | Recon Loss: 0.001743 | Commit Loss: 0.000745 | Perplexity: 1080.345849
Trainning Epoch:  95%|█████████▍| 631/665 [18:09:39<58:46, 103.71s/it]  2025-09-15 10:42:31,205 Stage: Train 0.5 | Epoch: 631 | Iter: 475200 | Total Loss: 0.002152 | Recon Loss: 0.001777 | Commit Loss: 0.000749 | Perplexity: 1084.633779
2025-09-15 10:42:58,658 Stage: Train 0.5 | Epoch: 631 | Iter: 475400 | Total Loss: 0.002097 | Recon Loss: 0.001723 | Commit Loss: 0.000748 | Perplexity: 1087.351112
2025-09-15 10:43:26,117 Stage: Train 0.5 | Epoch: 631 | Iter: 475600 | Total Loss: 0.002097 | Recon Loss: 0.001725 | Commit Loss: 0.000745 | Perplexity: 1083.365072
2025-09-15 10:43:53,580 Stage: Train 0.5 | Epoch: 631 | Iter: 475800 | Total Loss: 0.002141 | Recon Loss: 0.001767 | Commit Loss: 0.000747 | Perplexity: 1079.697746
Trainning Epoch:  95%|█████████▌| 632/665 [18:11:22<56:59, 103.61s/it]2025-09-15 10:44:21,110 Stage: Train 0.5 | Epoch: 632 | Iter: 476000 | Total Loss: 0.002144 | Recon Loss: 0.001770 | Commit Loss: 0.000747 | Perplexity: 1084.645173
2025-09-15 10:44:48,601 Stage: Train 0.5 | Epoch: 632 | Iter: 476200 | Total Loss: 0.002103 | Recon Loss: 0.001734 | Commit Loss: 0.000739 | Perplexity: 1082.047751
2025-09-15 10:45:16,258 Stage: Train 0.5 | Epoch: 632 | Iter: 476400 | Total Loss: 0.002079 | Recon Loss: 0.001706 | Commit Loss: 0.000746 | Perplexity: 1087.160037
2025-09-15 10:45:43,863 Stage: Train 0.5 | Epoch: 632 | Iter: 476600 | Total Loss: 0.002136 | Recon Loss: 0.001761 | Commit Loss: 0.000750 | Perplexity: 1085.361325
Trainning Epoch:  95%|█████████▌| 633/665 [18:13:06<55:17, 103.67s/it]2025-09-15 10:46:11,296 Stage: Train 0.5 | Epoch: 633 | Iter: 476800 | Total Loss: 0.002120 | Recon Loss: 0.001747 | Commit Loss: 0.000747 | Perplexity: 1082.408210
2025-09-15 10:46:38,743 Stage: Train 0.5 | Epoch: 633 | Iter: 477000 | Total Loss: 0.002105 | Recon Loss: 0.001732 | Commit Loss: 0.000745 | Perplexity: 1082.150772
2025-09-15 10:47:06,205 Stage: Train 0.5 | Epoch: 633 | Iter: 477200 | Total Loss: 0.002116 | Recon Loss: 0.001745 | Commit Loss: 0.000743 | Perplexity: 1082.894036
2025-09-15 10:47:33,662 Stage: Train 0.5 | Epoch: 633 | Iter: 477400 | Total Loss: 0.002147 | Recon Loss: 0.001771 | Commit Loss: 0.000752 | Perplexity: 1085.079683
Trainning Epoch:  95%|█████████▌| 634/665 [18:14:49<53:30, 103.57s/it]2025-09-15 10:48:01,087 Stage: Train 0.5 | Epoch: 634 | Iter: 477600 | Total Loss: 0.002129 | Recon Loss: 0.001756 | Commit Loss: 0.000747 | Perplexity: 1086.794856
2025-09-15 10:48:28,533 Stage: Train 0.5 | Epoch: 634 | Iter: 477800 | Total Loss: 0.002093 | Recon Loss: 0.001722 | Commit Loss: 0.000742 | Perplexity: 1082.697901
2025-09-15 10:48:56,039 Stage: Train 0.5 | Epoch: 634 | Iter: 478000 | Total Loss: 0.002147 | Recon Loss: 0.001773 | Commit Loss: 0.000747 | Perplexity: 1084.535673
Trainning Epoch:  95%|█████████▌| 635/665 [18:16:33<51:44, 103.49s/it]2025-09-15 10:49:23,369 Stage: Train 0.5 | Epoch: 635 | Iter: 478200 | Total Loss: 0.002167 | Recon Loss: 0.001792 | Commit Loss: 0.000749 | Perplexity: 1081.838650
2025-09-15 10:49:50,702 Stage: Train 0.5 | Epoch: 635 | Iter: 478400 | Total Loss: 0.002107 | Recon Loss: 0.001735 | Commit Loss: 0.000743 | Perplexity: 1083.136156
2025-09-15 10:50:18,042 Stage: Train 0.5 | Epoch: 635 | Iter: 478600 | Total Loss: 0.002089 | Recon Loss: 0.001715 | Commit Loss: 0.000748 | Perplexity: 1085.669961
2025-09-15 10:50:45,376 Stage: Train 0.5 | Epoch: 635 | Iter: 478800 | Total Loss: 0.002113 | Recon Loss: 0.001740 | Commit Loss: 0.000745 | Perplexity: 1082.490816
Trainning Epoch:  96%|█████████▌| 636/665 [18:18:16<49:56, 103.31s/it]2025-09-15 10:51:12,715 Stage: Train 0.5 | Epoch: 636 | Iter: 479000 | Total Loss: 0.002121 | Recon Loss: 0.001750 | Commit Loss: 0.000741 | Perplexity: 1081.373145
2025-09-15 10:51:40,086 Stage: Train 0.5 | Epoch: 636 | Iter: 479200 | Total Loss: 0.002145 | Recon Loss: 0.001774 | Commit Loss: 0.000743 | Perplexity: 1084.531488
2025-09-15 10:52:07,545 Stage: Train 0.5 | Epoch: 636 | Iter: 479400 | Total Loss: 0.002111 | Recon Loss: 0.001737 | Commit Loss: 0.000750 | Perplexity: 1086.478729
2025-09-15 10:52:34,885 Stage: Train 0.5 | Epoch: 636 | Iter: 479600 | Total Loss: 0.002123 | Recon Loss: 0.001748 | Commit Loss: 0.000751 | Perplexity: 1087.785491
Trainning Epoch:  96%|█████████▌| 637/665 [18:19:59<48:10, 103.24s/it]2025-09-15 10:53:02,175 Stage: Train 0.5 | Epoch: 637 | Iter: 479800 | Total Loss: 0.002108 | Recon Loss: 0.001740 | Commit Loss: 0.000737 | Perplexity: 1081.003073
2025-09-15 10:53:29,533 Stage: Train 0.5 | Epoch: 637 | Iter: 480000 | Total Loss: 0.002120 | Recon Loss: 0.001748 | Commit Loss: 0.000744 | Perplexity: 1082.903143
2025-09-15 10:53:29,533 Saving model at iteration 480000
2025-09-15 10:53:29,820 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_638_step_480000
2025-09-15 10:53:30,031 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_638_step_480000/pytorch_model.bin
2025-09-15 10:53:30,358 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_638_step_480000/optimizer.bin
2025-09-15 10:53:30,358 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_638_step_480000/scheduler.bin
2025-09-15 10:53:30,359 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_638_step_480000/random_states_0.pkl
2025-09-15 10:53:58,190 Stage: Train 0.5 | Epoch: 637 | Iter: 480200 | Total Loss: 0.002126 | Recon Loss: 0.001751 | Commit Loss: 0.000749 | Perplexity: 1084.846897
2025-09-15 10:54:25,604 Stage: Train 0.5 | Epoch: 637 | Iter: 480400 | Total Loss: 0.002127 | Recon Loss: 0.001753 | Commit Loss: 0.000747 | Perplexity: 1085.656565
Trainning Epoch:  96%|█████████▌| 638/665 [18:21:43<46:39, 103.69s/it]2025-09-15 10:54:53,384 Stage: Train 0.5 | Epoch: 638 | Iter: 480600 | Total Loss: 0.002098 | Recon Loss: 0.001725 | Commit Loss: 0.000746 | Perplexity: 1086.206320
2025-09-15 10:55:20,775 Stage: Train 0.5 | Epoch: 638 | Iter: 480800 | Total Loss: 0.002129 | Recon Loss: 0.001758 | Commit Loss: 0.000741 | Perplexity: 1081.484898
2025-09-15 10:55:48,192 Stage: Train 0.5 | Epoch: 638 | Iter: 481000 | Total Loss: 0.002076 | Recon Loss: 0.001704 | Commit Loss: 0.000746 | Perplexity: 1086.146508
Trainning Epoch:  96%|█████████▌| 639/665 [18:23:27<44:51, 103.52s/it]2025-09-15 10:56:15,545 Stage: Train 0.5 | Epoch: 639 | Iter: 481200 | Total Loss: 0.002107 | Recon Loss: 0.001733 | Commit Loss: 0.000747 | Perplexity: 1082.871087
2025-09-15 10:56:43,162 Stage: Train 0.5 | Epoch: 639 | Iter: 481400 | Total Loss: 0.002100 | Recon Loss: 0.001728 | Commit Loss: 0.000744 | Perplexity: 1089.646528
2025-09-15 10:57:10,750 Stage: Train 0.5 | Epoch: 639 | Iter: 481600 | Total Loss: 0.002112 | Recon Loss: 0.001743 | Commit Loss: 0.000740 | Perplexity: 1080.028696
2025-09-15 10:57:38,283 Stage: Train 0.5 | Epoch: 639 | Iter: 481800 | Total Loss: 0.002138 | Recon Loss: 0.001763 | Commit Loss: 0.000750 | Perplexity: 1085.674924
Trainning Epoch:  96%|█████████▌| 640/665 [18:25:10<43:09, 103.58s/it]2025-09-15 10:58:05,758 Stage: Train 0.5 | Epoch: 640 | Iter: 482000 | Total Loss: 0.002135 | Recon Loss: 0.001761 | Commit Loss: 0.000748 | Perplexity: 1081.904120
2025-09-15 10:58:33,286 Stage: Train 0.5 | Epoch: 640 | Iter: 482200 | Total Loss: 0.002129 | Recon Loss: 0.001757 | Commit Loss: 0.000746 | Perplexity: 1081.881316
2025-09-15 10:59:00,822 Stage: Train 0.5 | Epoch: 640 | Iter: 482400 | Total Loss: 0.002080 | Recon Loss: 0.001709 | Commit Loss: 0.000742 | Perplexity: 1084.789008
2025-09-15 10:59:28,341 Stage: Train 0.5 | Epoch: 640 | Iter: 482600 | Total Loss: 0.002118 | Recon Loss: 0.001744 | Commit Loss: 0.000748 | Perplexity: 1086.263326
Trainning Epoch:  96%|█████████▋| 641/665 [18:26:54<41:26, 103.58s/it]2025-09-15 10:59:55,881 Stage: Train 0.5 | Epoch: 641 | Iter: 482800 | Total Loss: 0.002133 | Recon Loss: 0.001760 | Commit Loss: 0.000746 | Perplexity: 1081.481684
2025-09-15 11:00:23,603 Stage: Train 0.5 | Epoch: 641 | Iter: 483000 | Total Loss: 0.002076 | Recon Loss: 0.001704 | Commit Loss: 0.000743 | Perplexity: 1086.429417
2025-09-15 11:00:51,265 Stage: Train 0.5 | Epoch: 641 | Iter: 483200 | Total Loss: 0.002087 | Recon Loss: 0.001713 | Commit Loss: 0.000747 | Perplexity: 1086.665533
2025-09-15 11:01:18,900 Stage: Train 0.5 | Epoch: 641 | Iter: 483400 | Total Loss: 0.002151 | Recon Loss: 0.001779 | Commit Loss: 0.000744 | Perplexity: 1081.670709
Trainning Epoch:  97%|█████████▋| 642/665 [18:28:38<39:45, 103.73s/it]2025-09-15 11:01:46,438 Stage: Train 0.5 | Epoch: 642 | Iter: 483600 | Total Loss: 0.002117 | Recon Loss: 0.001746 | Commit Loss: 0.000742 | Perplexity: 1082.551817
2025-09-15 11:02:14,016 Stage: Train 0.5 | Epoch: 642 | Iter: 483800 | Total Loss: 0.002170 | Recon Loss: 0.001799 | Commit Loss: 0.000742 | Perplexity: 1083.752447
2025-09-15 11:02:41,534 Stage: Train 0.5 | Epoch: 642 | Iter: 484000 | Total Loss: 0.002105 | Recon Loss: 0.001733 | Commit Loss: 0.000744 | Perplexity: 1086.016965
Trainning Epoch:  97%|█████████▋| 643/665 [18:30:22<38:03, 103.78s/it]2025-09-15 11:03:09,187 Stage: Train 0.5 | Epoch: 643 | Iter: 484200 | Total Loss: 0.002103 | Recon Loss: 0.001729 | Commit Loss: 0.000748 | Perplexity: 1083.088281
2025-09-15 11:03:36,647 Stage: Train 0.5 | Epoch: 643 | Iter: 484400 | Total Loss: 0.002062 | Recon Loss: 0.001693 | Commit Loss: 0.000739 | Perplexity: 1084.304092
2025-09-15 11:04:04,222 Stage: Train 0.5 | Epoch: 643 | Iter: 484600 | Total Loss: 0.002154 | Recon Loss: 0.001781 | Commit Loss: 0.000747 | Perplexity: 1085.369754
2025-09-15 11:04:31,664 Stage: Train 0.5 | Epoch: 643 | Iter: 484800 | Total Loss: 0.002075 | Recon Loss: 0.001704 | Commit Loss: 0.000741 | Perplexity: 1080.963035
Trainning Epoch:  97%|█████████▋| 644/665 [18:32:05<36:17, 103.67s/it]2025-09-15 11:04:59,035 Stage: Train 0.5 | Epoch: 644 | Iter: 485000 | Total Loss: 0.002151 | Recon Loss: 0.001778 | Commit Loss: 0.000747 | Perplexity: 1082.596483
2025-09-15 11:05:26,433 Stage: Train 0.5 | Epoch: 644 | Iter: 485200 | Total Loss: 0.002112 | Recon Loss: 0.001740 | Commit Loss: 0.000745 | Perplexity: 1086.605710
2025-09-15 11:05:53,827 Stage: Train 0.5 | Epoch: 644 | Iter: 485400 | Total Loss: 0.002104 | Recon Loss: 0.001732 | Commit Loss: 0.000744 | Perplexity: 1084.646665
2025-09-15 11:06:21,233 Stage: Train 0.5 | Epoch: 644 | Iter: 485600 | Total Loss: 0.002133 | Recon Loss: 0.001759 | Commit Loss: 0.000748 | Perplexity: 1084.696605
Trainning Epoch:  97%|█████████▋| 645/665 [18:33:48<34:30, 103.52s/it]2025-09-15 11:06:48,666 Stage: Train 0.5 | Epoch: 645 | Iter: 485800 | Total Loss: 0.002113 | Recon Loss: 0.001744 | Commit Loss: 0.000737 | Perplexity: 1080.120146
2025-09-15 11:07:16,183 Stage: Train 0.5 | Epoch: 645 | Iter: 486000 | Total Loss: 0.002123 | Recon Loss: 0.001753 | Commit Loss: 0.000738 | Perplexity: 1081.826395
2025-09-15 11:07:43,683 Stage: Train 0.5 | Epoch: 645 | Iter: 486200 | Total Loss: 0.002122 | Recon Loss: 0.001748 | Commit Loss: 0.000747 | Perplexity: 1087.380163
2025-09-15 11:08:11,390 Stage: Train 0.5 | Epoch: 645 | Iter: 486400 | Total Loss: 0.002085 | Recon Loss: 0.001710 | Commit Loss: 0.000750 | Perplexity: 1085.891872
Trainning Epoch:  97%|█████████▋| 646/665 [18:35:32<32:47, 103.57s/it]2025-09-15 11:08:38,884 Stage: Train 0.5 | Epoch: 646 | Iter: 486600 | Total Loss: 0.002100 | Recon Loss: 0.001729 | Commit Loss: 0.000742 | Perplexity: 1081.461650
2025-09-15 11:09:06,427 Stage: Train 0.5 | Epoch: 646 | Iter: 486800 | Total Loss: 0.002079 | Recon Loss: 0.001708 | Commit Loss: 0.000742 | Perplexity: 1084.585910
2025-09-15 11:09:33,960 Stage: Train 0.5 | Epoch: 646 | Iter: 487000 | Total Loss: 0.002105 | Recon Loss: 0.001733 | Commit Loss: 0.000744 | Perplexity: 1087.694422
Trainning Epoch:  97%|█████████▋| 647/665 [18:37:16<31:04, 103.59s/it]2025-09-15 11:10:01,470 Stage: Train 0.5 | Epoch: 647 | Iter: 487200 | Total Loss: 0.002130 | Recon Loss: 0.001755 | Commit Loss: 0.000749 | Perplexity: 1080.943564
2025-09-15 11:10:28,920 Stage: Train 0.5 | Epoch: 647 | Iter: 487400 | Total Loss: 0.002076 | Recon Loss: 0.001705 | Commit Loss: 0.000743 | Perplexity: 1086.362947
2025-09-15 11:10:56,489 Stage: Train 0.5 | Epoch: 647 | Iter: 487600 | Total Loss: 0.002156 | Recon Loss: 0.001785 | Commit Loss: 0.000743 | Perplexity: 1087.680777
2025-09-15 11:11:23,973 Stage: Train 0.5 | Epoch: 647 | Iter: 487800 | Total Loss: 0.002083 | Recon Loss: 0.001711 | Commit Loss: 0.000744 | Perplexity: 1081.707429
Trainning Epoch:  97%|█████████▋| 648/665 [18:38:59<29:20, 103.56s/it]2025-09-15 11:11:51,376 Stage: Train 0.5 | Epoch: 648 | Iter: 488000 | Total Loss: 0.002123 | Recon Loss: 0.001750 | Commit Loss: 0.000746 | Perplexity: 1084.530786
2025-09-15 11:12:18,863 Stage: Train 0.5 | Epoch: 648 | Iter: 488200 | Total Loss: 0.002092 | Recon Loss: 0.001721 | Commit Loss: 0.000742 | Perplexity: 1086.574106
2025-09-15 11:12:46,338 Stage: Train 0.5 | Epoch: 648 | Iter: 488400 | Total Loss: 0.002070 | Recon Loss: 0.001701 | Commit Loss: 0.000740 | Perplexity: 1080.124839
2025-09-15 11:13:13,817 Stage: Train 0.5 | Epoch: 648 | Iter: 488600 | Total Loss: 0.002097 | Recon Loss: 0.001724 | Commit Loss: 0.000746 | Perplexity: 1085.964242
Trainning Epoch:  98%|█████████▊| 649/665 [18:40:43<27:36, 103.51s/it]2025-09-15 11:13:41,246 Stage: Train 0.5 | Epoch: 649 | Iter: 488800 | Total Loss: 0.002127 | Recon Loss: 0.001754 | Commit Loss: 0.000746 | Perplexity: 1084.174678
2025-09-15 11:14:08,863 Stage: Train 0.5 | Epoch: 649 | Iter: 489000 | Total Loss: 0.002087 | Recon Loss: 0.001717 | Commit Loss: 0.000740 | Perplexity: 1083.722180
2025-09-15 11:14:36,339 Stage: Train 0.5 | Epoch: 649 | Iter: 489200 | Total Loss: 0.002137 | Recon Loss: 0.001765 | Commit Loss: 0.000744 | Perplexity: 1086.417079
2025-09-15 11:15:03,825 Stage: Train 0.5 | Epoch: 649 | Iter: 489400 | Total Loss: 0.002065 | Recon Loss: 0.001694 | Commit Loss: 0.000742 | Perplexity: 1085.314311
Trainning Epoch:  98%|█████████▊| 650/665 [18:42:26<25:52, 103.52s/it]2025-09-15 11:15:31,231 Stage: Train 0.5 | Epoch: 650 | Iter: 489600 | Total Loss: 0.002154 | Recon Loss: 0.001782 | Commit Loss: 0.000743 | Perplexity: 1084.200989
2025-09-15 11:15:58,631 Stage: Train 0.5 | Epoch: 650 | Iter: 489800 | Total Loss: 0.002096 | Recon Loss: 0.001725 | Commit Loss: 0.000742 | Perplexity: 1084.136326
2025-09-15 11:16:26,028 Stage: Train 0.5 | Epoch: 650 | Iter: 490000 | Total Loss: 0.002083 | Recon Loss: 0.001711 | Commit Loss: 0.000745 | Perplexity: 1086.972738
2025-09-15 11:16:53,529 Stage: Train 0.5 | Epoch: 650 | Iter: 490200 | Total Loss: 0.002117 | Recon Loss: 0.001744 | Commit Loss: 0.000746 | Perplexity: 1082.898088
Trainning Epoch:  98%|█████████▊| 651/665 [18:44:09<24:08, 103.44s/it]2025-09-15 11:17:20,938 Stage: Train 0.5 | Epoch: 651 | Iter: 490400 | Total Loss: 0.002090 | Recon Loss: 0.001722 | Commit Loss: 0.000736 | Perplexity: 1081.834991
2025-09-15 11:17:48,484 Stage: Train 0.5 | Epoch: 651 | Iter: 490600 | Total Loss: 0.002123 | Recon Loss: 0.001752 | Commit Loss: 0.000742 | Perplexity: 1085.598533
2025-09-15 11:18:15,888 Stage: Train 0.5 | Epoch: 651 | Iter: 490800 | Total Loss: 0.002125 | Recon Loss: 0.001753 | Commit Loss: 0.000745 | Perplexity: 1084.897964
Trainning Epoch:  98%|█████████▊| 652/665 [18:45:53<22:24, 103.41s/it]2025-09-15 11:18:43,269 Stage: Train 0.5 | Epoch: 652 | Iter: 491000 | Total Loss: 0.002107 | Recon Loss: 0.001732 | Commit Loss: 0.000750 | Perplexity: 1084.712121
2025-09-15 11:19:10,697 Stage: Train 0.5 | Epoch: 652 | Iter: 491200 | Total Loss: 0.002123 | Recon Loss: 0.001754 | Commit Loss: 0.000738 | Perplexity: 1084.066129
2025-09-15 11:19:38,114 Stage: Train 0.5 | Epoch: 652 | Iter: 491400 | Total Loss: 0.002077 | Recon Loss: 0.001707 | Commit Loss: 0.000739 | Perplexity: 1082.816195
2025-09-15 11:20:05,490 Stage: Train 0.5 | Epoch: 652 | Iter: 491600 | Total Loss: 0.002120 | Recon Loss: 0.001746 | Commit Loss: 0.000746 | Perplexity: 1084.264575
Trainning Epoch:  98%|█████████▊| 653/665 [18:47:36<20:39, 103.32s/it]2025-09-15 11:20:32,845 Stage: Train 0.5 | Epoch: 653 | Iter: 491800 | Total Loss: 0.002118 | Recon Loss: 0.001746 | Commit Loss: 0.000744 | Perplexity: 1085.377320
2025-09-15 11:21:00,296 Stage: Train 0.5 | Epoch: 653 | Iter: 492000 | Total Loss: 0.002117 | Recon Loss: 0.001745 | Commit Loss: 0.000743 | Perplexity: 1086.168596
2025-09-15 11:21:27,771 Stage: Train 0.5 | Epoch: 653 | Iter: 492200 | Total Loss: 0.002095 | Recon Loss: 0.001725 | Commit Loss: 0.000742 | Perplexity: 1084.991167
2025-09-15 11:21:55,173 Stage: Train 0.5 | Epoch: 653 | Iter: 492400 | Total Loss: 0.002091 | Recon Loss: 0.001722 | Commit Loss: 0.000739 | Perplexity: 1083.326971
Trainning Epoch:  98%|█████████▊| 654/665 [18:49:19<18:56, 103.31s/it]2025-09-15 11:22:22,570 Stage: Train 0.5 | Epoch: 654 | Iter: 492600 | Total Loss: 0.002112 | Recon Loss: 0.001741 | Commit Loss: 0.000743 | Perplexity: 1083.166044
2025-09-15 11:22:49,986 Stage: Train 0.5 | Epoch: 654 | Iter: 492800 | Total Loss: 0.002105 | Recon Loss: 0.001735 | Commit Loss: 0.000739 | Perplexity: 1083.798586
2025-09-15 11:23:17,472 Stage: Train 0.5 | Epoch: 654 | Iter: 493000 | Total Loss: 0.002103 | Recon Loss: 0.001729 | Commit Loss: 0.000748 | Perplexity: 1089.933600
2025-09-15 11:23:44,878 Stage: Train 0.5 | Epoch: 654 | Iter: 493200 | Total Loss: 0.002103 | Recon Loss: 0.001734 | Commit Loss: 0.000738 | Perplexity: 1082.688370
Trainning Epoch:  98%|█████████▊| 655/665 [18:51:02<17:12, 103.29s/it]2025-09-15 11:24:12,352 Stage: Train 0.5 | Epoch: 655 | Iter: 493400 | Total Loss: 0.002099 | Recon Loss: 0.001729 | Commit Loss: 0.000740 | Perplexity: 1083.660513
2025-09-15 11:24:39,773 Stage: Train 0.5 | Epoch: 655 | Iter: 493600 | Total Loss: 0.002140 | Recon Loss: 0.001767 | Commit Loss: 0.000746 | Perplexity: 1086.444469
2025-09-15 11:25:07,189 Stage: Train 0.5 | Epoch: 655 | Iter: 493800 | Total Loss: 0.002061 | Recon Loss: 0.001692 | Commit Loss: 0.000738 | Perplexity: 1079.788604
Trainning Epoch:  99%|█████████▊| 656/665 [18:52:46<15:29, 103.32s/it]2025-09-15 11:25:34,671 Stage: Train 0.5 | Epoch: 656 | Iter: 494000 | Total Loss: 0.002112 | Recon Loss: 0.001741 | Commit Loss: 0.000741 | Perplexity: 1084.634425
2025-09-15 11:26:02,214 Stage: Train 0.5 | Epoch: 656 | Iter: 494200 | Total Loss: 0.002087 | Recon Loss: 0.001718 | Commit Loss: 0.000739 | Perplexity: 1084.567220
2025-09-15 11:26:29,605 Stage: Train 0.5 | Epoch: 656 | Iter: 494400 | Total Loss: 0.002100 | Recon Loss: 0.001730 | Commit Loss: 0.000740 | Perplexity: 1086.038406
2025-09-15 11:26:57,042 Stage: Train 0.5 | Epoch: 656 | Iter: 494600 | Total Loss: 0.002071 | Recon Loss: 0.001699 | Commit Loss: 0.000744 | Perplexity: 1086.914400
Trainning Epoch:  99%|█████████▉| 657/665 [18:54:29<13:46, 103.32s/it]2025-09-15 11:27:24,396 Stage: Train 0.5 | Epoch: 657 | Iter: 494800 | Total Loss: 0.002092 | Recon Loss: 0.001721 | Commit Loss: 0.000741 | Perplexity: 1079.242989
2025-09-15 11:27:51,915 Stage: Train 0.5 | Epoch: 657 | Iter: 495000 | Total Loss: 0.002091 | Recon Loss: 0.001722 | Commit Loss: 0.000738 | Perplexity: 1081.745961
2025-09-15 11:28:19,442 Stage: Train 0.5 | Epoch: 657 | Iter: 495200 | Total Loss: 0.002114 | Recon Loss: 0.001744 | Commit Loss: 0.000739 | Perplexity: 1086.984673
2025-09-15 11:28:46,984 Stage: Train 0.5 | Epoch: 657 | Iter: 495400 | Total Loss: 0.002089 | Recon Loss: 0.001714 | Commit Loss: 0.000750 | Perplexity: 1089.127130
Trainning Epoch:  99%|█████████▉| 658/665 [18:56:13<12:03, 103.38s/it]2025-09-15 11:29:14,410 Stage: Train 0.5 | Epoch: 658 | Iter: 495600 | Total Loss: 0.002080 | Recon Loss: 0.001710 | Commit Loss: 0.000739 | Perplexity: 1081.203237
2025-09-15 11:29:41,835 Stage: Train 0.5 | Epoch: 658 | Iter: 495800 | Total Loss: 0.002121 | Recon Loss: 0.001752 | Commit Loss: 0.000737 | Perplexity: 1081.079681
2025-09-15 11:30:09,286 Stage: Train 0.5 | Epoch: 658 | Iter: 496000 | Total Loss: 0.002139 | Recon Loss: 0.001767 | Commit Loss: 0.000744 | Perplexity: 1088.144406
2025-09-15 11:30:36,752 Stage: Train 0.5 | Epoch: 658 | Iter: 496200 | Total Loss: 0.002077 | Recon Loss: 0.001704 | Commit Loss: 0.000744 | Perplexity: 1090.885710
Trainning Epoch:  99%|█████████▉| 659/665 [18:57:56<10:20, 103.36s/it]2025-09-15 11:31:04,317 Stage: Train 0.5 | Epoch: 659 | Iter: 496400 | Total Loss: 0.002113 | Recon Loss: 0.001743 | Commit Loss: 0.000739 | Perplexity: 1085.069525
2025-09-15 11:31:31,799 Stage: Train 0.5 | Epoch: 659 | Iter: 496600 | Total Loss: 0.002106 | Recon Loss: 0.001735 | Commit Loss: 0.000742 | Perplexity: 1083.500095
2025-09-15 11:31:59,289 Stage: Train 0.5 | Epoch: 659 | Iter: 496800 | Total Loss: 0.002071 | Recon Loss: 0.001702 | Commit Loss: 0.000739 | Perplexity: 1085.544723
Trainning Epoch:  99%|█████████▉| 660/665 [18:59:40<08:37, 103.46s/it]2025-09-15 11:32:26,851 Stage: Train 0.5 | Epoch: 660 | Iter: 497000 | Total Loss: 0.002122 | Recon Loss: 0.001749 | Commit Loss: 0.000745 | Perplexity: 1084.616303
2025-09-15 11:32:54,459 Stage: Train 0.5 | Epoch: 660 | Iter: 497200 | Total Loss: 0.002126 | Recon Loss: 0.001757 | Commit Loss: 0.000738 | Perplexity: 1082.847399
2025-09-15 11:33:22,013 Stage: Train 0.5 | Epoch: 660 | Iter: 497400 | Total Loss: 0.002045 | Recon Loss: 0.001673 | Commit Loss: 0.000743 | Perplexity: 1089.428211
2025-09-15 11:33:49,521 Stage: Train 0.5 | Epoch: 660 | Iter: 497600 | Total Loss: 0.002108 | Recon Loss: 0.001737 | Commit Loss: 0.000741 | Perplexity: 1085.499613
Trainning Epoch:  99%|█████████▉| 661/665 [19:01:23<06:54, 103.53s/it]2025-09-15 11:34:17,005 Stage: Train 0.5 | Epoch: 661 | Iter: 497800 | Total Loss: 0.002089 | Recon Loss: 0.001721 | Commit Loss: 0.000737 | Perplexity: 1081.158766
2025-09-15 11:34:44,591 Stage: Train 0.5 | Epoch: 661 | Iter: 498000 | Total Loss: 0.002089 | Recon Loss: 0.001720 | Commit Loss: 0.000737 | Perplexity: 1082.444123
2025-09-15 11:35:12,077 Stage: Train 0.5 | Epoch: 661 | Iter: 498200 | Total Loss: 0.002067 | Recon Loss: 0.001696 | Commit Loss: 0.000743 | Perplexity: 1087.457446
2025-09-15 11:35:39,566 Stage: Train 0.5 | Epoch: 661 | Iter: 498400 | Total Loss: 0.002077 | Recon Loss: 0.001706 | Commit Loss: 0.000742 | Perplexity: 1086.239137
Trainning Epoch: 100%|█████████▉| 662/665 [19:03:07<05:10, 103.53s/it]2025-09-15 11:36:07,012 Stage: Train 0.5 | Epoch: 662 | Iter: 498600 | Total Loss: 0.002113 | Recon Loss: 0.001744 | Commit Loss: 0.000739 | Perplexity: 1085.437965
2025-09-15 11:36:34,457 Stage: Train 0.5 | Epoch: 662 | Iter: 498800 | Total Loss: 0.002090 | Recon Loss: 0.001722 | Commit Loss: 0.000737 | Perplexity: 1080.753276
2025-09-15 11:37:01,958 Stage: Train 0.5 | Epoch: 662 | Iter: 499000 | Total Loss: 0.002056 | Recon Loss: 0.001688 | Commit Loss: 0.000736 | Perplexity: 1083.323397
2025-09-15 11:37:29,338 Stage: Train 0.5 | Epoch: 662 | Iter: 499200 | Total Loss: 0.002129 | Recon Loss: 0.001756 | Commit Loss: 0.000744 | Perplexity: 1083.437606
Trainning Epoch: 100%|█████████▉| 663/665 [19:04:50<03:26, 103.46s/it]2025-09-15 11:37:56,788 Stage: Train 0.5 | Epoch: 663 | Iter: 499400 | Total Loss: 0.002078 | Recon Loss: 0.001710 | Commit Loss: 0.000737 | Perplexity: 1079.675325
2025-09-15 11:38:24,211 Stage: Train 0.5 | Epoch: 663 | Iter: 499600 | Total Loss: 0.002085 | Recon Loss: 0.001715 | Commit Loss: 0.000741 | Perplexity: 1086.885936
2025-09-15 11:38:51,659 Stage: Train 0.5 | Epoch: 663 | Iter: 499800 | Total Loss: 0.002108 | Recon Loss: 0.001735 | Commit Loss: 0.000746 | Perplexity: 1087.071333
Trainning Epoch: 100%|█████████▉| 664/665 [19:06:33<01:43, 103.41s/it]2025-09-15 11:39:19,024 Stage: Train 0.5 | Epoch: 664 | Iter: 500000 | Total Loss: 0.002066 | Recon Loss: 0.001694 | Commit Loss: 0.000744 | Perplexity: 1084.509991
2025-09-15 11:39:19,024 Saving model at iteration 500000
2025-09-15 11:39:19,187 Saving current state to vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_665_step_500000
2025-09-15 11:39:19,389 Model weights saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_665_step_500000/pytorch_model.bin
2025-09-15 11:39:19,710 Optimizer state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_665_step_500000/optimizer.bin
2025-09-15 11:39:19,710 Scheduler state saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_665_step_500000/scheduler.bin
2025-09-15 11:39:19,711 Random states saved in vqvae_experiment/h36m_j3d_f64s1_cb4096x2048/models/checkpoint_epoch_665_step_500000/random_states_0.pkl
Trainning Epoch: 100%|█████████▉| 664/665 [19:06:35<01:43, 103.61s/it]
2025-09-15 11:39:19,713 Training finished
