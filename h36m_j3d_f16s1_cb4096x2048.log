[2025-09-14 16:27:45,734] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-09-14 16:27:46,920 
python train_vqvae.py --data_mode joint3d --load_data_file /data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl --num_frames 16 --sample_stride 1 --project_dir vqvae_experiment/h36m_j3d_f16s1_cb4096x2048 --not_find_unused_parameters --nb_code 4096 --codebook_dim 2048
2025-09-14 16:28:06,094 Data loaded with 97196 samples and 1559752 frames
2025-09-14 16:28:06,480 Number of trainable parameters: 38.884867 M
2025-09-14 16:28:06,480 Args: Namespace(data_root='./', num_frames=16, batch_size=32, max_epoch=1000000000.0, total_iter=500000, world_size=1, rank=0, save_interval=20000, warm_up_iter=5000, print_iter=200, learning_rate=0.0002, lr_schedule=[300000], gamma=0.05, weight_decay=0.0001, resume_pth='', device='cuda', project_config='', allow_tf32=False, project_dir='vqvae_experiment/h36m_j3d_f16s1_cb4096x2048', seed=6666, commit_ratio=0.5, nb_code=4096, codebook_dim=2048, load_data_file='/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl', data_mode='joint3d', not_find_unused_parameters=True, sample_stride=1)
Trainning Epoch:   0%|          | 0/165 [00:00<?, ?it/s]2025-09-14 16:28:16,600 current_lr 0.000008 at iteration 200
2025-09-14 16:28:16,642 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.083901 | Recon Loss: 0.081454 | Commit Loss: 0.004895 | Perplexity: 619.571019
2025-09-14 16:28:25,368 current_lr 0.000016 at iteration 400
2025-09-14 16:28:25,412 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.051869 | Recon Loss: 0.044609 | Commit Loss: 0.014520 | Perplexity: 586.999073
2025-09-14 16:28:34,121 current_lr 0.000024 at iteration 600
2025-09-14 16:28:34,164 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.046512 | Recon Loss: 0.034680 | Commit Loss: 0.023664 | Perplexity: 675.115469
2025-09-14 16:28:42,885 current_lr 0.000032 at iteration 800
2025-09-14 16:28:42,926 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.043442 | Recon Loss: 0.030128 | Commit Loss: 0.026629 | Perplexity: 783.238233
2025-09-14 16:28:51,666 current_lr 0.000040 at iteration 1000
2025-09-14 16:28:51,711 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.041028 | Recon Loss: 0.027400 | Commit Loss: 0.027257 | Perplexity: 818.111630
2025-09-14 16:29:00,422 current_lr 0.000048 at iteration 1200
2025-09-14 16:29:00,468 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.039515 | Recon Loss: 0.026737 | Commit Loss: 0.025556 | Perplexity: 827.970729
2025-09-14 16:29:09,205 current_lr 0.000056 at iteration 1400
2025-09-14 16:29:09,246 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.035135 | Recon Loss: 0.023625 | Commit Loss: 0.023020 | Perplexity: 828.541120
2025-09-14 16:29:17,993 current_lr 0.000064 at iteration 1600
2025-09-14 16:29:18,036 Stage: Warm Up | Epoch: 0 | Iter: 1600 | Total Loss: 0.031675 | Recon Loss: 0.022077 | Commit Loss: 0.019196 | Perplexity: 829.487930
2025-09-14 16:29:26,757 current_lr 0.000072 at iteration 1800
2025-09-14 16:29:26,800 Stage: Warm Up | Epoch: 0 | Iter: 1800 | Total Loss: 0.028721 | Recon Loss: 0.021083 | Commit Loss: 0.015276 | Perplexity: 835.664200
2025-09-14 16:29:35,533 current_lr 0.000080 at iteration 2000
2025-09-14 16:29:35,576 Stage: Warm Up | Epoch: 0 | Iter: 2000 | Total Loss: 0.025453 | Recon Loss: 0.018781 | Commit Loss: 0.013344 | Perplexity: 846.855702
2025-09-14 16:29:44,317 current_lr 0.000088 at iteration 2200
2025-09-14 16:29:44,360 Stage: Warm Up | Epoch: 0 | Iter: 2200 | Total Loss: 0.023521 | Recon Loss: 0.018193 | Commit Loss: 0.010657 | Perplexity: 852.116725
2025-09-14 16:29:53,078 current_lr 0.000096 at iteration 2400
2025-09-14 16:29:53,124 Stage: Warm Up | Epoch: 0 | Iter: 2400 | Total Loss: 0.021500 | Recon Loss: 0.017104 | Commit Loss: 0.008793 | Perplexity: 854.105653
2025-09-14 16:30:01,873 current_lr 0.000104 at iteration 2600
2025-09-14 16:30:01,914 Stage: Warm Up | Epoch: 0 | Iter: 2600 | Total Loss: 0.020502 | Recon Loss: 0.016973 | Commit Loss: 0.007058 | Perplexity: 854.030261
2025-09-14 16:30:10,642 current_lr 0.000112 at iteration 2800
2025-09-14 16:30:10,688 Stage: Warm Up | Epoch: 0 | Iter: 2800 | Total Loss: 0.018472 | Recon Loss: 0.015532 | Commit Loss: 0.005879 | Perplexity: 868.333223
2025-09-14 16:30:19,427 current_lr 0.000120 at iteration 3000
2025-09-14 16:30:19,473 Stage: Warm Up | Epoch: 0 | Iter: 3000 | Total Loss: 0.017497 | Recon Loss: 0.015138 | Commit Loss: 0.004717 | Perplexity: 871.099747
Trainning Epoch:   1%|          | 1/165 [02:15<6:09:16, 135.10s/it]2025-09-14 16:30:28,621 current_lr 0.000128 at iteration 3200
2025-09-14 16:30:28,664 Stage: Warm Up | Epoch: 1 | Iter: 3200 | Total Loss: 0.016083 | Recon Loss: 0.014159 | Commit Loss: 0.003846 | Perplexity: 880.994776
2025-09-14 16:30:37,390 current_lr 0.000136 at iteration 3400
2025-09-14 16:30:37,432 Stage: Warm Up | Epoch: 1 | Iter: 3400 | Total Loss: 0.015091 | Recon Loss: 0.013516 | Commit Loss: 0.003150 | Perplexity: 890.600195
2025-09-14 16:30:46,142 current_lr 0.000144 at iteration 3600
2025-09-14 16:30:46,185 Stage: Warm Up | Epoch: 1 | Iter: 3600 | Total Loss: 0.014489 | Recon Loss: 0.013163 | Commit Loss: 0.002652 | Perplexity: 893.328589
2025-09-14 16:30:54,884 current_lr 0.000152 at iteration 3800
2025-09-14 16:30:54,926 Stage: Warm Up | Epoch: 1 | Iter: 3800 | Total Loss: 0.014642 | Recon Loss: 0.013516 | Commit Loss: 0.002252 | Perplexity: 894.797068
2025-09-14 16:31:03,626 current_lr 0.000160 at iteration 4000
2025-09-14 16:31:03,672 Stage: Warm Up | Epoch: 1 | Iter: 4000 | Total Loss: 0.012911 | Recon Loss: 0.011904 | Commit Loss: 0.002013 | Perplexity: 900.566972
2025-09-14 16:31:12,350 current_lr 0.000168 at iteration 4200
2025-09-14 16:31:12,397 Stage: Warm Up | Epoch: 1 | Iter: 4200 | Total Loss: 0.013626 | Recon Loss: 0.012768 | Commit Loss: 0.001715 | Perplexity: 898.510626
2025-09-14 16:31:21,111 current_lr 0.000176 at iteration 4400
2025-09-14 16:31:21,153 Stage: Warm Up | Epoch: 1 | Iter: 4400 | Total Loss: 0.012658 | Recon Loss: 0.011849 | Commit Loss: 0.001618 | Perplexity: 911.577798
2025-09-14 16:31:29,846 current_lr 0.000184 at iteration 4600
2025-09-14 16:31:29,893 Stage: Warm Up | Epoch: 1 | Iter: 4600 | Total Loss: 0.012491 | Recon Loss: 0.011774 | Commit Loss: 0.001434 | Perplexity: 909.493225
2025-09-14 16:31:38,592 current_lr 0.000192 at iteration 4800
2025-09-14 16:31:38,634 Stage: Warm Up | Epoch: 1 | Iter: 4800 | Total Loss: 0.012379 | Recon Loss: 0.011727 | Commit Loss: 0.001305 | Perplexity: 911.571731
2025-09-14 16:31:47,330 current_lr 0.000200 at iteration 5000
2025-09-14 16:31:47,372 Stage: Warm Up | Epoch: 1 | Iter: 5000 | Total Loss: 0.011738 | Recon Loss: 0.011141 | Commit Loss: 0.001194 | Perplexity: 917.131824
2025-09-14 16:31:56,118 Stage: Train 0.5 | Epoch: 1 | Iter: 5200 | Total Loss: 0.011783 | Recon Loss: 0.011230 | Commit Loss: 0.001106 | Perplexity: 928.319784
2025-09-14 16:32:04,876 Stage: Train 0.5 | Epoch: 1 | Iter: 5400 | Total Loss: 0.011269 | Recon Loss: 0.010711 | Commit Loss: 0.001116 | Perplexity: 933.533602
2025-09-14 16:32:13,619 Stage: Train 0.5 | Epoch: 1 | Iter: 5600 | Total Loss: 0.010895 | Recon Loss: 0.010383 | Commit Loss: 0.001024 | Perplexity: 940.450820
2025-09-14 16:32:22,347 Stage: Train 0.5 | Epoch: 1 | Iter: 5800 | Total Loss: 0.011690 | Recon Loss: 0.011166 | Commit Loss: 0.001048 | Perplexity: 938.203171
2025-09-14 16:32:31,080 Stage: Train 0.5 | Epoch: 1 | Iter: 6000 | Total Loss: 0.010364 | Recon Loss: 0.009847 | Commit Loss: 0.001033 | Perplexity: 943.145318
Trainning Epoch:   1%|          | 2/165 [04:27<6:03:21, 133.75s/it]2025-09-14 16:32:39,793 Stage: Train 0.5 | Epoch: 2 | Iter: 6200 | Total Loss: 0.010149 | Recon Loss: 0.009586 | Commit Loss: 0.001125 | Perplexity: 938.296489
2025-09-14 16:32:48,538 Stage: Train 0.5 | Epoch: 2 | Iter: 6400 | Total Loss: 0.010134 | Recon Loss: 0.009598 | Commit Loss: 0.001072 | Perplexity: 942.287448
2025-09-14 16:32:57,234 Stage: Train 0.5 | Epoch: 2 | Iter: 6600 | Total Loss: 0.009533 | Recon Loss: 0.009000 | Commit Loss: 0.001065 | Perplexity: 936.668643
2025-09-14 16:33:05,921 Stage: Train 0.5 | Epoch: 2 | Iter: 6800 | Total Loss: 0.009348 | Recon Loss: 0.008806 | Commit Loss: 0.001083 | Perplexity: 924.379196
2025-09-14 16:33:14,656 Stage: Train 0.5 | Epoch: 2 | Iter: 7000 | Total Loss: 0.009393 | Recon Loss: 0.008863 | Commit Loss: 0.001061 | Perplexity: 926.776725
2025-09-14 16:33:23,346 Stage: Train 0.5 | Epoch: 2 | Iter: 7200 | Total Loss: 0.009240 | Recon Loss: 0.008703 | Commit Loss: 0.001075 | Perplexity: 932.738534
2025-09-14 16:33:32,022 Stage: Train 0.5 | Epoch: 2 | Iter: 7400 | Total Loss: 0.008918 | Recon Loss: 0.008358 | Commit Loss: 0.001120 | Perplexity: 932.752222
2025-09-14 16:33:40,735 Stage: Train 0.5 | Epoch: 2 | Iter: 7600 | Total Loss: 0.008630 | Recon Loss: 0.008079 | Commit Loss: 0.001101 | Perplexity: 930.586026
2025-09-14 16:33:49,413 Stage: Train 0.5 | Epoch: 2 | Iter: 7800 | Total Loss: 0.008666 | Recon Loss: 0.008076 | Commit Loss: 0.001181 | Perplexity: 924.068699
2025-09-14 16:33:58,125 Stage: Train 0.5 | Epoch: 2 | Iter: 8000 | Total Loss: 0.008846 | Recon Loss: 0.008256 | Commit Loss: 0.001180 | Perplexity: 933.052143
2025-09-14 16:34:06,811 Stage: Train 0.5 | Epoch: 2 | Iter: 8200 | Total Loss: 0.008616 | Recon Loss: 0.008066 | Commit Loss: 0.001099 | Perplexity: 932.419019
2025-09-14 16:34:15,541 Stage: Train 0.5 | Epoch: 2 | Iter: 8400 | Total Loss: 0.008313 | Recon Loss: 0.007728 | Commit Loss: 0.001170 | Perplexity: 935.055697
2025-09-14 16:34:24,248 Stage: Train 0.5 | Epoch: 2 | Iter: 8600 | Total Loss: 0.008474 | Recon Loss: 0.007878 | Commit Loss: 0.001191 | Perplexity: 931.991776
2025-09-14 16:34:32,960 Stage: Train 0.5 | Epoch: 2 | Iter: 8800 | Total Loss: 0.008068 | Recon Loss: 0.007469 | Commit Loss: 0.001199 | Perplexity: 929.443323
2025-09-14 16:34:41,640 Stage: Train 0.5 | Epoch: 2 | Iter: 9000 | Total Loss: 0.008046 | Recon Loss: 0.007426 | Commit Loss: 0.001241 | Perplexity: 926.978197
Trainning Epoch:   2%|▏         | 3/165 [06:40<5:59:13, 133.04s/it]2025-09-14 16:34:50,349 Stage: Train 0.5 | Epoch: 3 | Iter: 9200 | Total Loss: 0.008284 | Recon Loss: 0.007669 | Commit Loss: 0.001230 | Perplexity: 925.955565
2025-09-14 16:34:59,102 Stage: Train 0.5 | Epoch: 3 | Iter: 9400 | Total Loss: 0.007622 | Recon Loss: 0.007011 | Commit Loss: 0.001222 | Perplexity: 932.844492
2025-09-14 16:35:07,884 Stage: Train 0.5 | Epoch: 3 | Iter: 9600 | Total Loss: 0.007741 | Recon Loss: 0.007125 | Commit Loss: 0.001231 | Perplexity: 928.673547
2025-09-14 16:35:16,674 Stage: Train 0.5 | Epoch: 3 | Iter: 9800 | Total Loss: 0.007608 | Recon Loss: 0.006985 | Commit Loss: 0.001245 | Perplexity: 930.888752
2025-09-14 16:35:25,575 Stage: Train 0.5 | Epoch: 3 | Iter: 10000 | Total Loss: 0.007551 | Recon Loss: 0.006937 | Commit Loss: 0.001229 | Perplexity: 932.690728
2025-09-14 16:35:34,384 Stage: Train 0.5 | Epoch: 3 | Iter: 10200 | Total Loss: 0.007231 | Recon Loss: 0.006608 | Commit Loss: 0.001247 | Perplexity: 931.786248
2025-09-14 16:35:43,210 Stage: Train 0.5 | Epoch: 3 | Iter: 10400 | Total Loss: 0.007707 | Recon Loss: 0.007109 | Commit Loss: 0.001197 | Perplexity: 925.969409
2025-09-14 16:35:51,909 Stage: Train 0.5 | Epoch: 3 | Iter: 10600 | Total Loss: 0.007141 | Recon Loss: 0.006517 | Commit Loss: 0.001249 | Perplexity: 926.463648
2025-09-14 16:36:00,583 Stage: Train 0.5 | Epoch: 3 | Iter: 10800 | Total Loss: 0.007511 | Recon Loss: 0.006896 | Commit Loss: 0.001229 | Perplexity: 925.315816
2025-09-14 16:36:09,286 Stage: Train 0.5 | Epoch: 3 | Iter: 11000 | Total Loss: 0.007524 | Recon Loss: 0.006924 | Commit Loss: 0.001200 | Perplexity: 927.948597
2025-09-14 16:36:17,990 Stage: Train 0.5 | Epoch: 3 | Iter: 11200 | Total Loss: 0.007141 | Recon Loss: 0.006541 | Commit Loss: 0.001199 | Perplexity: 927.582186
2025-09-14 16:36:26,681 Stage: Train 0.5 | Epoch: 3 | Iter: 11400 | Total Loss: 0.007271 | Recon Loss: 0.006664 | Commit Loss: 0.001216 | Perplexity: 928.347586
2025-09-14 16:36:35,388 Stage: Train 0.5 | Epoch: 3 | Iter: 11600 | Total Loss: 0.007183 | Recon Loss: 0.006602 | Commit Loss: 0.001163 | Perplexity: 930.189875
2025-09-14 16:36:44,089 Stage: Train 0.5 | Epoch: 3 | Iter: 11800 | Total Loss: 0.007004 | Recon Loss: 0.006395 | Commit Loss: 0.001218 | Perplexity: 931.099113
2025-09-14 16:36:52,763 Stage: Train 0.5 | Epoch: 3 | Iter: 12000 | Total Loss: 0.006953 | Recon Loss: 0.006344 | Commit Loss: 0.001218 | Perplexity: 931.833800
Trainning Epoch:   2%|▏         | 4/165 [08:52<5:56:43, 132.94s/it]2025-09-14 16:37:01,458 Stage: Train 0.5 | Epoch: 4 | Iter: 12200 | Total Loss: 0.006930 | Recon Loss: 0.006369 | Commit Loss: 0.001122 | Perplexity: 926.019527
2025-09-14 16:37:10,151 Stage: Train 0.5 | Epoch: 4 | Iter: 12400 | Total Loss: 0.006804 | Recon Loss: 0.006207 | Commit Loss: 0.001192 | Perplexity: 935.488661
2025-09-14 16:37:18,848 Stage: Train 0.5 | Epoch: 4 | Iter: 12600 | Total Loss: 0.006930 | Recon Loss: 0.006344 | Commit Loss: 0.001172 | Perplexity: 935.332380
2025-09-14 16:37:27,553 Stage: Train 0.5 | Epoch: 4 | Iter: 12800 | Total Loss: 0.006581 | Recon Loss: 0.005988 | Commit Loss: 0.001187 | Perplexity: 938.543950
2025-09-14 16:37:36,275 Stage: Train 0.5 | Epoch: 4 | Iter: 13000 | Total Loss: 0.006729 | Recon Loss: 0.006151 | Commit Loss: 0.001156 | Perplexity: 936.735636
2025-09-14 16:37:44,968 Stage: Train 0.5 | Epoch: 4 | Iter: 13200 | Total Loss: 0.006640 | Recon Loss: 0.006033 | Commit Loss: 0.001215 | Perplexity: 938.534339
2025-09-14 16:37:53,690 Stage: Train 0.5 | Epoch: 4 | Iter: 13400 | Total Loss: 0.006515 | Recon Loss: 0.005919 | Commit Loss: 0.001193 | Perplexity: 935.263126
2025-09-14 16:38:02,405 Stage: Train 0.5 | Epoch: 4 | Iter: 13600 | Total Loss: 0.006339 | Recon Loss: 0.005749 | Commit Loss: 0.001180 | Perplexity: 935.898374
2025-09-14 16:38:11,096 Stage: Train 0.5 | Epoch: 4 | Iter: 13800 | Total Loss: 0.006785 | Recon Loss: 0.006196 | Commit Loss: 0.001177 | Perplexity: 938.494944
2025-09-14 16:38:19,800 Stage: Train 0.5 | Epoch: 4 | Iter: 14000 | Total Loss: 0.006499 | Recon Loss: 0.005906 | Commit Loss: 0.001186 | Perplexity: 940.396333
2025-09-14 16:38:28,486 Stage: Train 0.5 | Epoch: 4 | Iter: 14200 | Total Loss: 0.006447 | Recon Loss: 0.005850 | Commit Loss: 0.001192 | Perplexity: 937.157259
2025-09-14 16:38:37,196 Stage: Train 0.5 | Epoch: 4 | Iter: 14400 | Total Loss: 0.006244 | Recon Loss: 0.005654 | Commit Loss: 0.001180 | Perplexity: 935.758827
2025-09-14 16:38:45,880 Stage: Train 0.5 | Epoch: 4 | Iter: 14600 | Total Loss: 0.006374 | Recon Loss: 0.005801 | Commit Loss: 0.001148 | Perplexity: 936.426557
2025-09-14 16:38:54,573 Stage: Train 0.5 | Epoch: 4 | Iter: 14800 | Total Loss: 0.006170 | Recon Loss: 0.005600 | Commit Loss: 0.001139 | Perplexity: 935.584343
2025-09-14 16:39:03,302 Stage: Train 0.5 | Epoch: 4 | Iter: 15000 | Total Loss: 0.006183 | Recon Loss: 0.005597 | Commit Loss: 0.001172 | Perplexity: 939.098616
Trainning Epoch:   3%|▎         | 5/165 [11:05<5:53:45, 132.66s/it]2025-09-14 16:39:11,981 Stage: Train 0.5 | Epoch: 5 | Iter: 15200 | Total Loss: 0.006121 | Recon Loss: 0.005537 | Commit Loss: 0.001167 | Perplexity: 936.721150
2025-09-14 16:39:20,705 Stage: Train 0.5 | Epoch: 5 | Iter: 15400 | Total Loss: 0.006183 | Recon Loss: 0.005608 | Commit Loss: 0.001150 | Perplexity: 941.460050
2025-09-14 16:39:29,432 Stage: Train 0.5 | Epoch: 5 | Iter: 15600 | Total Loss: 0.006104 | Recon Loss: 0.005528 | Commit Loss: 0.001153 | Perplexity: 940.999590
2025-09-14 16:39:38,175 Stage: Train 0.5 | Epoch: 5 | Iter: 15800 | Total Loss: 0.006125 | Recon Loss: 0.005546 | Commit Loss: 0.001158 | Perplexity: 937.310836
2025-09-14 16:39:46,991 Stage: Train 0.5 | Epoch: 5 | Iter: 16000 | Total Loss: 0.006179 | Recon Loss: 0.005607 | Commit Loss: 0.001144 | Perplexity: 938.719988
2025-09-14 16:39:55,699 Stage: Train 0.5 | Epoch: 5 | Iter: 16200 | Total Loss: 0.005846 | Recon Loss: 0.005281 | Commit Loss: 0.001130 | Perplexity: 944.266609
2025-09-14 16:40:04,472 Stage: Train 0.5 | Epoch: 5 | Iter: 16400 | Total Loss: 0.006047 | Recon Loss: 0.005455 | Commit Loss: 0.001183 | Perplexity: 941.111448
2025-09-14 16:40:13,190 Stage: Train 0.5 | Epoch: 5 | Iter: 16600 | Total Loss: 0.005907 | Recon Loss: 0.005320 | Commit Loss: 0.001174 | Perplexity: 939.503459
2025-09-14 16:40:21,945 Stage: Train 0.5 | Epoch: 5 | Iter: 16800 | Total Loss: 0.005891 | Recon Loss: 0.005315 | Commit Loss: 0.001152 | Perplexity: 941.963968
2025-09-14 16:40:30,653 Stage: Train 0.5 | Epoch: 5 | Iter: 17000 | Total Loss: 0.005937 | Recon Loss: 0.005359 | Commit Loss: 0.001156 | Perplexity: 941.942972
2025-09-14 16:40:39,371 Stage: Train 0.5 | Epoch: 5 | Iter: 17200 | Total Loss: 0.005894 | Recon Loss: 0.005330 | Commit Loss: 0.001128 | Perplexity: 940.906892
2025-09-14 16:40:48,088 Stage: Train 0.5 | Epoch: 5 | Iter: 17400 | Total Loss: 0.005742 | Recon Loss: 0.005179 | Commit Loss: 0.001125 | Perplexity: 948.950118
2025-09-14 16:40:56,831 Stage: Train 0.5 | Epoch: 5 | Iter: 17600 | Total Loss: 0.005774 | Recon Loss: 0.005214 | Commit Loss: 0.001119 | Perplexity: 947.446455
2025-09-14 16:41:05,573 Stage: Train 0.5 | Epoch: 5 | Iter: 17800 | Total Loss: 0.005903 | Recon Loss: 0.005343 | Commit Loss: 0.001119 | Perplexity: 950.539993
2025-09-14 16:41:14,265 Stage: Train 0.5 | Epoch: 5 | Iter: 18000 | Total Loss: 0.005770 | Recon Loss: 0.005219 | Commit Loss: 0.001102 | Perplexity: 954.269986
2025-09-14 16:41:23,017 Stage: Train 0.5 | Epoch: 5 | Iter: 18200 | Total Loss: 0.005769 | Recon Loss: 0.005200 | Commit Loss: 0.001137 | Perplexity: 954.662879
Trainning Epoch:   4%|▎         | 6/165 [13:17<5:51:34, 132.67s/it]2025-09-14 16:41:31,728 Stage: Train 0.5 | Epoch: 6 | Iter: 18400 | Total Loss: 0.005841 | Recon Loss: 0.005292 | Commit Loss: 0.001099 | Perplexity: 946.011736
2025-09-14 16:41:40,465 Stage: Train 0.5 | Epoch: 6 | Iter: 18600 | Total Loss: 0.005619 | Recon Loss: 0.005059 | Commit Loss: 0.001121 | Perplexity: 951.998994
2025-09-14 16:41:49,197 Stage: Train 0.5 | Epoch: 6 | Iter: 18800 | Total Loss: 0.005764 | Recon Loss: 0.005205 | Commit Loss: 0.001117 | Perplexity: 949.593372
2025-09-14 16:41:57,928 Stage: Train 0.5 | Epoch: 6 | Iter: 19000 | Total Loss: 0.005472 | Recon Loss: 0.004908 | Commit Loss: 0.001129 | Perplexity: 948.117862
2025-09-14 16:42:06,645 Stage: Train 0.5 | Epoch: 6 | Iter: 19200 | Total Loss: 0.005700 | Recon Loss: 0.005133 | Commit Loss: 0.001134 | Perplexity: 955.471362
2025-09-14 16:42:15,372 Stage: Train 0.5 | Epoch: 6 | Iter: 19400 | Total Loss: 0.005517 | Recon Loss: 0.004972 | Commit Loss: 0.001091 | Perplexity: 950.071053
2025-09-14 16:42:24,099 Stage: Train 0.5 | Epoch: 6 | Iter: 19600 | Total Loss: 0.005705 | Recon Loss: 0.005148 | Commit Loss: 0.001113 | Perplexity: 952.791983
2025-09-14 16:42:32,790 Stage: Train 0.5 | Epoch: 6 | Iter: 19800 | Total Loss: 0.005652 | Recon Loss: 0.005106 | Commit Loss: 0.001091 | Perplexity: 954.320995
2025-09-14 16:42:41,486 Stage: Train 0.5 | Epoch: 6 | Iter: 20000 | Total Loss: 0.005343 | Recon Loss: 0.004785 | Commit Loss: 0.001116 | Perplexity: 957.932679
2025-09-14 16:42:41,487 Saving model at iteration 20000
2025-09-14 16:42:41,646 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_7_step_20000
2025-09-14 16:42:41,886 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_7_step_20000/pytorch_model.bin
2025-09-14 16:42:42,253 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_7_step_20000/optimizer.bin
2025-09-14 16:42:42,253 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_7_step_20000/scheduler.bin
2025-09-14 16:42:42,254 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_7_step_20000/random_states_0.pkl
2025-09-14 16:42:50,944 Stage: Train 0.5 | Epoch: 6 | Iter: 20200 | Total Loss: 0.005459 | Recon Loss: 0.004907 | Commit Loss: 0.001105 | Perplexity: 948.287807
2025-09-14 16:42:59,603 Stage: Train 0.5 | Epoch: 6 | Iter: 20400 | Total Loss: 0.005427 | Recon Loss: 0.004881 | Commit Loss: 0.001094 | Perplexity: 955.258579
2025-09-14 16:43:08,261 Stage: Train 0.5 | Epoch: 6 | Iter: 20600 | Total Loss: 0.005454 | Recon Loss: 0.004912 | Commit Loss: 0.001085 | Perplexity: 953.745921
2025-09-14 16:43:16,912 Stage: Train 0.5 | Epoch: 6 | Iter: 20800 | Total Loss: 0.005387 | Recon Loss: 0.004851 | Commit Loss: 0.001071 | Perplexity: 955.554658
2025-09-14 16:43:25,553 Stage: Train 0.5 | Epoch: 6 | Iter: 21000 | Total Loss: 0.005466 | Recon Loss: 0.004923 | Commit Loss: 0.001085 | Perplexity: 954.697955
2025-09-14 16:43:34,257 Stage: Train 0.5 | Epoch: 6 | Iter: 21200 | Total Loss: 0.005356 | Recon Loss: 0.004808 | Commit Loss: 0.001095 | Perplexity: 954.086087
Trainning Epoch:   4%|▍         | 7/165 [15:31<5:49:55, 132.88s/it]2025-09-14 16:43:43,387 Stage: Train 0.5 | Epoch: 7 | Iter: 21400 | Total Loss: 0.005318 | Recon Loss: 0.004783 | Commit Loss: 0.001070 | Perplexity: 954.088364
2025-09-14 16:43:52,068 Stage: Train 0.5 | Epoch: 7 | Iter: 21600 | Total Loss: 0.005305 | Recon Loss: 0.004774 | Commit Loss: 0.001062 | Perplexity: 955.132639
2025-09-14 16:44:00,757 Stage: Train 0.5 | Epoch: 7 | Iter: 21800 | Total Loss: 0.005431 | Recon Loss: 0.004886 | Commit Loss: 0.001090 | Perplexity: 952.273095
2025-09-14 16:44:09,428 Stage: Train 0.5 | Epoch: 7 | Iter: 22000 | Total Loss: 0.005315 | Recon Loss: 0.004763 | Commit Loss: 0.001105 | Perplexity: 957.023777
2025-09-14 16:44:18,116 Stage: Train 0.5 | Epoch: 7 | Iter: 22200 | Total Loss: 0.005314 | Recon Loss: 0.004781 | Commit Loss: 0.001065 | Perplexity: 959.569236
2025-09-14 16:44:26,834 Stage: Train 0.5 | Epoch: 7 | Iter: 22400 | Total Loss: 0.005331 | Recon Loss: 0.004786 | Commit Loss: 0.001090 | Perplexity: 962.790692
2025-09-14 16:44:35,534 Stage: Train 0.5 | Epoch: 7 | Iter: 22600 | Total Loss: 0.005417 | Recon Loss: 0.004875 | Commit Loss: 0.001084 | Perplexity: 964.598752
2025-09-14 16:44:44,237 Stage: Train 0.5 | Epoch: 7 | Iter: 22800 | Total Loss: 0.005207 | Recon Loss: 0.004675 | Commit Loss: 0.001064 | Perplexity: 961.536446
2025-09-14 16:44:52,937 Stage: Train 0.5 | Epoch: 7 | Iter: 23000 | Total Loss: 0.005239 | Recon Loss: 0.004691 | Commit Loss: 0.001095 | Perplexity: 962.134338
2025-09-14 16:45:01,609 Stage: Train 0.5 | Epoch: 7 | Iter: 23200 | Total Loss: 0.005230 | Recon Loss: 0.004715 | Commit Loss: 0.001031 | Perplexity: 960.834803
2025-09-14 16:45:10,308 Stage: Train 0.5 | Epoch: 7 | Iter: 23400 | Total Loss: 0.005058 | Recon Loss: 0.004535 | Commit Loss: 0.001044 | Perplexity: 963.309493
2025-09-14 16:45:19,016 Stage: Train 0.5 | Epoch: 7 | Iter: 23600 | Total Loss: 0.005196 | Recon Loss: 0.004663 | Commit Loss: 0.001065 | Perplexity: 961.264379
2025-09-14 16:45:27,712 Stage: Train 0.5 | Epoch: 7 | Iter: 23800 | Total Loss: 0.005045 | Recon Loss: 0.004513 | Commit Loss: 0.001064 | Perplexity: 965.250361
2025-09-14 16:45:36,413 Stage: Train 0.5 | Epoch: 7 | Iter: 24000 | Total Loss: 0.005084 | Recon Loss: 0.004575 | Commit Loss: 0.001019 | Perplexity: 959.097049
2025-09-14 16:45:45,140 Stage: Train 0.5 | Epoch: 7 | Iter: 24200 | Total Loss: 0.005073 | Recon Loss: 0.004539 | Commit Loss: 0.001068 | Perplexity: 962.683079
Trainning Epoch:   5%|▍         | 8/165 [17:43<5:47:04, 132.64s/it]2025-09-14 16:45:53,841 Stage: Train 0.5 | Epoch: 8 | Iter: 24400 | Total Loss: 0.005226 | Recon Loss: 0.004709 | Commit Loss: 0.001033 | Perplexity: 964.487016
2025-09-14 16:46:02,521 Stage: Train 0.5 | Epoch: 8 | Iter: 24600 | Total Loss: 0.005055 | Recon Loss: 0.004533 | Commit Loss: 0.001044 | Perplexity: 965.713929
2025-09-14 16:46:11,245 Stage: Train 0.5 | Epoch: 8 | Iter: 24800 | Total Loss: 0.005161 | Recon Loss: 0.004651 | Commit Loss: 0.001018 | Perplexity: 965.037948
2025-09-14 16:46:19,965 Stage: Train 0.5 | Epoch: 8 | Iter: 25000 | Total Loss: 0.004957 | Recon Loss: 0.004443 | Commit Loss: 0.001029 | Perplexity: 968.200346
2025-09-14 16:46:28,647 Stage: Train 0.5 | Epoch: 8 | Iter: 25200 | Total Loss: 0.005013 | Recon Loss: 0.004507 | Commit Loss: 0.001011 | Perplexity: 966.080930
2025-09-14 16:46:37,348 Stage: Train 0.5 | Epoch: 8 | Iter: 25400 | Total Loss: 0.004865 | Recon Loss: 0.004340 | Commit Loss: 0.001052 | Perplexity: 964.914095
2025-09-14 16:46:46,057 Stage: Train 0.5 | Epoch: 8 | Iter: 25600 | Total Loss: 0.004894 | Recon Loss: 0.004379 | Commit Loss: 0.001030 | Perplexity: 967.908415
2025-09-14 16:46:54,761 Stage: Train 0.5 | Epoch: 8 | Iter: 25800 | Total Loss: 0.005008 | Recon Loss: 0.004484 | Commit Loss: 0.001048 | Perplexity: 966.506367
2025-09-14 16:47:03,465 Stage: Train 0.5 | Epoch: 8 | Iter: 26000 | Total Loss: 0.004834 | Recon Loss: 0.004333 | Commit Loss: 0.001003 | Perplexity: 962.216137
2025-09-14 16:47:12,146 Stage: Train 0.5 | Epoch: 8 | Iter: 26200 | Total Loss: 0.004916 | Recon Loss: 0.004402 | Commit Loss: 0.001027 | Perplexity: 966.910557
2025-09-14 16:47:20,856 Stage: Train 0.5 | Epoch: 8 | Iter: 26400 | Total Loss: 0.005030 | Recon Loss: 0.004517 | Commit Loss: 0.001025 | Perplexity: 970.155791
2025-09-14 16:47:29,553 Stage: Train 0.5 | Epoch: 8 | Iter: 26600 | Total Loss: 0.004958 | Recon Loss: 0.004453 | Commit Loss: 0.001009 | Perplexity: 964.232181
2025-09-14 16:47:38,277 Stage: Train 0.5 | Epoch: 8 | Iter: 26800 | Total Loss: 0.004832 | Recon Loss: 0.004325 | Commit Loss: 0.001014 | Perplexity: 968.440845
2025-09-14 16:47:46,994 Stage: Train 0.5 | Epoch: 8 | Iter: 27000 | Total Loss: 0.004921 | Recon Loss: 0.004425 | Commit Loss: 0.000991 | Perplexity: 972.599749
2025-09-14 16:47:55,692 Stage: Train 0.5 | Epoch: 8 | Iter: 27200 | Total Loss: 0.004769 | Recon Loss: 0.004258 | Commit Loss: 0.001022 | Perplexity: 968.236017
Trainning Epoch:   5%|▌         | 9/165 [19:55<5:44:31, 132.51s/it]2025-09-14 16:48:04,436 Stage: Train 0.5 | Epoch: 9 | Iter: 27400 | Total Loss: 0.004872 | Recon Loss: 0.004367 | Commit Loss: 0.001010 | Perplexity: 962.692602
2025-09-14 16:48:13,169 Stage: Train 0.5 | Epoch: 9 | Iter: 27600 | Total Loss: 0.004832 | Recon Loss: 0.004323 | Commit Loss: 0.001019 | Perplexity: 965.452876
2025-09-14 16:48:21,877 Stage: Train 0.5 | Epoch: 9 | Iter: 27800 | Total Loss: 0.004628 | Recon Loss: 0.004119 | Commit Loss: 0.001019 | Perplexity: 974.123070
2025-09-14 16:48:30,597 Stage: Train 0.5 | Epoch: 9 | Iter: 28000 | Total Loss: 0.004715 | Recon Loss: 0.004200 | Commit Loss: 0.001030 | Perplexity: 972.620774
2025-09-14 16:48:39,284 Stage: Train 0.5 | Epoch: 9 | Iter: 28200 | Total Loss: 0.004809 | Recon Loss: 0.004305 | Commit Loss: 0.001008 | Perplexity: 969.823335
2025-09-14 16:48:47,997 Stage: Train 0.5 | Epoch: 9 | Iter: 28400 | Total Loss: 0.004794 | Recon Loss: 0.004297 | Commit Loss: 0.000996 | Perplexity: 973.672110
2025-09-14 16:48:56,718 Stage: Train 0.5 | Epoch: 9 | Iter: 28600 | Total Loss: 0.004806 | Recon Loss: 0.004286 | Commit Loss: 0.001040 | Perplexity: 973.079121
2025-09-14 16:49:05,450 Stage: Train 0.5 | Epoch: 9 | Iter: 28800 | Total Loss: 0.004683 | Recon Loss: 0.004176 | Commit Loss: 0.001016 | Perplexity: 976.398791
2025-09-14 16:49:14,164 Stage: Train 0.5 | Epoch: 9 | Iter: 29000 | Total Loss: 0.004779 | Recon Loss: 0.004289 | Commit Loss: 0.000979 | Perplexity: 966.967020
2025-09-14 16:49:22,869 Stage: Train 0.5 | Epoch: 9 | Iter: 29200 | Total Loss: 0.004567 | Recon Loss: 0.004055 | Commit Loss: 0.001025 | Perplexity: 974.040826
2025-09-14 16:49:31,580 Stage: Train 0.5 | Epoch: 9 | Iter: 29400 | Total Loss: 0.004876 | Recon Loss: 0.004390 | Commit Loss: 0.000972 | Perplexity: 975.264277
2025-09-14 16:49:40,272 Stage: Train 0.5 | Epoch: 9 | Iter: 29600 | Total Loss: 0.004742 | Recon Loss: 0.004250 | Commit Loss: 0.000985 | Perplexity: 975.639700
2025-09-14 16:49:48,945 Stage: Train 0.5 | Epoch: 9 | Iter: 29800 | Total Loss: 0.004773 | Recon Loss: 0.004309 | Commit Loss: 0.000928 | Perplexity: 973.836783
2025-09-14 16:49:57,658 Stage: Train 0.5 | Epoch: 9 | Iter: 30000 | Total Loss: 0.004540 | Recon Loss: 0.004050 | Commit Loss: 0.000979 | Perplexity: 978.578647
2025-09-14 16:50:06,337 Stage: Train 0.5 | Epoch: 9 | Iter: 30200 | Total Loss: 0.004820 | Recon Loss: 0.004352 | Commit Loss: 0.000936 | Perplexity: 971.175441
Trainning Epoch:   6%|▌         | 10/165 [22:07<5:42:08, 132.44s/it]2025-09-14 16:50:15,049 Stage: Train 0.5 | Epoch: 10 | Iter: 30400 | Total Loss: 0.004658 | Recon Loss: 0.004171 | Commit Loss: 0.000973 | Perplexity: 968.732458
2025-09-14 16:50:23,748 Stage: Train 0.5 | Epoch: 10 | Iter: 30600 | Total Loss: 0.004565 | Recon Loss: 0.004087 | Commit Loss: 0.000955 | Perplexity: 972.622672
2025-09-14 16:50:32,428 Stage: Train 0.5 | Epoch: 10 | Iter: 30800 | Total Loss: 0.004725 | Recon Loss: 0.004243 | Commit Loss: 0.000965 | Perplexity: 971.256240
2025-09-14 16:50:41,128 Stage: Train 0.5 | Epoch: 10 | Iter: 31000 | Total Loss: 0.004765 | Recon Loss: 0.004294 | Commit Loss: 0.000943 | Perplexity: 961.907133
2025-09-14 16:50:49,804 Stage: Train 0.5 | Epoch: 10 | Iter: 31200 | Total Loss: 0.004386 | Recon Loss: 0.003916 | Commit Loss: 0.000939 | Perplexity: 969.633954
2025-09-14 16:50:58,510 Stage: Train 0.5 | Epoch: 10 | Iter: 31400 | Total Loss: 0.004551 | Recon Loss: 0.004072 | Commit Loss: 0.000958 | Perplexity: 968.665261
2025-09-14 16:51:07,218 Stage: Train 0.5 | Epoch: 10 | Iter: 31600 | Total Loss: 0.004526 | Recon Loss: 0.004059 | Commit Loss: 0.000934 | Perplexity: 969.411664
2025-09-14 16:51:15,918 Stage: Train 0.5 | Epoch: 10 | Iter: 31800 | Total Loss: 0.004597 | Recon Loss: 0.004123 | Commit Loss: 0.000948 | Perplexity: 968.488893
2025-09-14 16:51:24,622 Stage: Train 0.5 | Epoch: 10 | Iter: 32000 | Total Loss: 0.004342 | Recon Loss: 0.003867 | Commit Loss: 0.000950 | Perplexity: 972.278769
2025-09-14 16:51:33,317 Stage: Train 0.5 | Epoch: 10 | Iter: 32200 | Total Loss: 0.004613 | Recon Loss: 0.004138 | Commit Loss: 0.000951 | Perplexity: 970.693808
2025-09-14 16:51:42,057 Stage: Train 0.5 | Epoch: 10 | Iter: 32400 | Total Loss: 0.004375 | Recon Loss: 0.003904 | Commit Loss: 0.000942 | Perplexity: 973.679066
2025-09-14 16:51:50,745 Stage: Train 0.5 | Epoch: 10 | Iter: 32600 | Total Loss: 0.004458 | Recon Loss: 0.003990 | Commit Loss: 0.000935 | Perplexity: 969.499679
2025-09-14 16:51:59,445 Stage: Train 0.5 | Epoch: 10 | Iter: 32800 | Total Loss: 0.004659 | Recon Loss: 0.004182 | Commit Loss: 0.000954 | Perplexity: 968.557102
2025-09-14 16:52:08,148 Stage: Train 0.5 | Epoch: 10 | Iter: 33000 | Total Loss: 0.004401 | Recon Loss: 0.003937 | Commit Loss: 0.000928 | Perplexity: 970.692366
2025-09-14 16:52:16,859 Stage: Train 0.5 | Epoch: 10 | Iter: 33200 | Total Loss: 0.004466 | Recon Loss: 0.004006 | Commit Loss: 0.000919 | Perplexity: 968.759719
2025-09-14 16:52:25,572 Stage: Train 0.5 | Epoch: 10 | Iter: 33400 | Total Loss: 0.004521 | Recon Loss: 0.004047 | Commit Loss: 0.000949 | Perplexity: 969.654533
Trainning Epoch:   7%|▋         | 11/165 [24:19<5:39:43, 132.36s/it]2025-09-14 16:52:34,285 Stage: Train 0.5 | Epoch: 11 | Iter: 33600 | Total Loss: 0.004542 | Recon Loss: 0.004072 | Commit Loss: 0.000938 | Perplexity: 968.415911
2025-09-14 16:52:43,016 Stage: Train 0.5 | Epoch: 11 | Iter: 33800 | Total Loss: 0.004428 | Recon Loss: 0.003964 | Commit Loss: 0.000927 | Perplexity: 974.881071
2025-09-14 16:52:51,747 Stage: Train 0.5 | Epoch: 11 | Iter: 34000 | Total Loss: 0.004498 | Recon Loss: 0.004037 | Commit Loss: 0.000921 | Perplexity: 967.121174
2025-09-14 16:53:00,461 Stage: Train 0.5 | Epoch: 11 | Iter: 34200 | Total Loss: 0.004327 | Recon Loss: 0.003869 | Commit Loss: 0.000917 | Perplexity: 974.399959
2025-09-14 16:53:09,189 Stage: Train 0.5 | Epoch: 11 | Iter: 34400 | Total Loss: 0.004563 | Recon Loss: 0.004096 | Commit Loss: 0.000934 | Perplexity: 973.812345
2025-09-14 16:53:17,906 Stage: Train 0.5 | Epoch: 11 | Iter: 34600 | Total Loss: 0.004382 | Recon Loss: 0.003918 | Commit Loss: 0.000928 | Perplexity: 973.258737
2025-09-14 16:53:26,622 Stage: Train 0.5 | Epoch: 11 | Iter: 34800 | Total Loss: 0.004331 | Recon Loss: 0.003873 | Commit Loss: 0.000916 | Perplexity: 974.376544
2025-09-14 16:53:35,316 Stage: Train 0.5 | Epoch: 11 | Iter: 35000 | Total Loss: 0.004369 | Recon Loss: 0.003914 | Commit Loss: 0.000911 | Perplexity: 977.950236
2025-09-14 16:53:44,044 Stage: Train 0.5 | Epoch: 11 | Iter: 35200 | Total Loss: 0.004389 | Recon Loss: 0.003928 | Commit Loss: 0.000921 | Perplexity: 973.296500
2025-09-14 16:53:52,785 Stage: Train 0.5 | Epoch: 11 | Iter: 35400 | Total Loss: 0.004217 | Recon Loss: 0.003760 | Commit Loss: 0.000914 | Perplexity: 974.443627
2025-09-14 16:54:01,496 Stage: Train 0.5 | Epoch: 11 | Iter: 35600 | Total Loss: 0.004669 | Recon Loss: 0.004203 | Commit Loss: 0.000933 | Perplexity: 967.103782
2025-09-14 16:54:10,197 Stage: Train 0.5 | Epoch: 11 | Iter: 35800 | Total Loss: 0.004336 | Recon Loss: 0.003873 | Commit Loss: 0.000926 | Perplexity: 968.873338
2025-09-14 16:54:18,888 Stage: Train 0.5 | Epoch: 11 | Iter: 36000 | Total Loss: 0.004232 | Recon Loss: 0.003769 | Commit Loss: 0.000925 | Perplexity: 973.600766
2025-09-14 16:54:27,612 Stage: Train 0.5 | Epoch: 11 | Iter: 36200 | Total Loss: 0.004262 | Recon Loss: 0.003799 | Commit Loss: 0.000925 | Perplexity: 971.039713
2025-09-14 16:54:36,316 Stage: Train 0.5 | Epoch: 11 | Iter: 36400 | Total Loss: 0.004391 | Recon Loss: 0.003922 | Commit Loss: 0.000937 | Perplexity: 971.286162
Trainning Epoch:   7%|▋         | 12/165 [26:32<5:37:32, 132.37s/it]2025-09-14 16:54:45,010 Stage: Train 0.5 | Epoch: 12 | Iter: 36600 | Total Loss: 0.004206 | Recon Loss: 0.003749 | Commit Loss: 0.000914 | Perplexity: 970.338602
2025-09-14 16:54:53,688 Stage: Train 0.5 | Epoch: 12 | Iter: 36800 | Total Loss: 0.004281 | Recon Loss: 0.003824 | Commit Loss: 0.000913 | Perplexity: 970.522803
2025-09-14 16:55:02,377 Stage: Train 0.5 | Epoch: 12 | Iter: 37000 | Total Loss: 0.004306 | Recon Loss: 0.003849 | Commit Loss: 0.000915 | Perplexity: 967.394022
2025-09-14 16:55:11,073 Stage: Train 0.5 | Epoch: 12 | Iter: 37200 | Total Loss: 0.004341 | Recon Loss: 0.003860 | Commit Loss: 0.000961 | Perplexity: 973.933271
2025-09-14 16:55:19,778 Stage: Train 0.5 | Epoch: 12 | Iter: 37400 | Total Loss: 0.004071 | Recon Loss: 0.003616 | Commit Loss: 0.000911 | Perplexity: 966.379233
2025-09-14 16:55:28,504 Stage: Train 0.5 | Epoch: 12 | Iter: 37600 | Total Loss: 0.004336 | Recon Loss: 0.003870 | Commit Loss: 0.000932 | Perplexity: 969.686007
2025-09-14 16:55:37,230 Stage: Train 0.5 | Epoch: 12 | Iter: 37800 | Total Loss: 0.004255 | Recon Loss: 0.003800 | Commit Loss: 0.000911 | Perplexity: 967.794933
2025-09-14 16:55:45,957 Stage: Train 0.5 | Epoch: 12 | Iter: 38000 | Total Loss: 0.004236 | Recon Loss: 0.003773 | Commit Loss: 0.000925 | Perplexity: 972.033044
2025-09-14 16:55:54,660 Stage: Train 0.5 | Epoch: 12 | Iter: 38200 | Total Loss: 0.004185 | Recon Loss: 0.003730 | Commit Loss: 0.000908 | Perplexity: 966.292411
2025-09-14 16:56:03,404 Stage: Train 0.5 | Epoch: 12 | Iter: 38400 | Total Loss: 0.004253 | Recon Loss: 0.003793 | Commit Loss: 0.000919 | Perplexity: 971.542505
2025-09-14 16:56:12,124 Stage: Train 0.5 | Epoch: 12 | Iter: 38600 | Total Loss: 0.004182 | Recon Loss: 0.003729 | Commit Loss: 0.000907 | Perplexity: 968.207203
2025-09-14 16:56:20,838 Stage: Train 0.5 | Epoch: 12 | Iter: 38800 | Total Loss: 0.004336 | Recon Loss: 0.003863 | Commit Loss: 0.000946 | Perplexity: 966.725573
2025-09-14 16:56:29,589 Stage: Train 0.5 | Epoch: 12 | Iter: 39000 | Total Loss: 0.004343 | Recon Loss: 0.003879 | Commit Loss: 0.000927 | Perplexity: 964.987074
2025-09-14 16:56:38,311 Stage: Train 0.5 | Epoch: 12 | Iter: 39200 | Total Loss: 0.004211 | Recon Loss: 0.003771 | Commit Loss: 0.000881 | Perplexity: 968.494488
2025-09-14 16:56:47,032 Stage: Train 0.5 | Epoch: 12 | Iter: 39400 | Total Loss: 0.004185 | Recon Loss: 0.003733 | Commit Loss: 0.000904 | Perplexity: 968.843023
Trainning Epoch:   8%|▊         | 13/165 [28:44<5:35:21, 132.38s/it]2025-09-14 16:56:55,777 Stage: Train 0.5 | Epoch: 13 | Iter: 39600 | Total Loss: 0.004177 | Recon Loss: 0.003724 | Commit Loss: 0.000907 | Perplexity: 968.570269
2025-09-14 16:57:04,532 Stage: Train 0.5 | Epoch: 13 | Iter: 39800 | Total Loss: 0.004158 | Recon Loss: 0.003707 | Commit Loss: 0.000902 | Perplexity: 970.194113
2025-09-14 16:57:13,280 Stage: Train 0.5 | Epoch: 13 | Iter: 40000 | Total Loss: 0.004162 | Recon Loss: 0.003713 | Commit Loss: 0.000898 | Perplexity: 968.342956
2025-09-14 16:57:13,281 Saving model at iteration 40000
2025-09-14 16:57:13,435 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_14_step_40000
2025-09-14 16:57:13,671 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_14_step_40000/pytorch_model.bin
2025-09-14 16:57:14,037 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_14_step_40000/optimizer.bin
2025-09-14 16:57:14,037 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_14_step_40000/scheduler.bin
2025-09-14 16:57:14,038 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_14_step_40000/random_states_0.pkl
2025-09-14 16:57:22,790 Stage: Train 0.5 | Epoch: 13 | Iter: 40200 | Total Loss: 0.004389 | Recon Loss: 0.003932 | Commit Loss: 0.000914 | Perplexity: 970.412289
2025-09-14 16:57:31,546 Stage: Train 0.5 | Epoch: 13 | Iter: 40400 | Total Loss: 0.004243 | Recon Loss: 0.003797 | Commit Loss: 0.000893 | Perplexity: 969.113508
2025-09-14 16:57:40,292 Stage: Train 0.5 | Epoch: 13 | Iter: 40600 | Total Loss: 0.004046 | Recon Loss: 0.003598 | Commit Loss: 0.000896 | Perplexity: 967.727243
2025-09-14 16:57:49,017 Stage: Train 0.5 | Epoch: 13 | Iter: 40800 | Total Loss: 0.004159 | Recon Loss: 0.003712 | Commit Loss: 0.000895 | Perplexity: 974.173926
2025-09-14 16:57:57,730 Stage: Train 0.5 | Epoch: 13 | Iter: 41000 | Total Loss: 0.004230 | Recon Loss: 0.003772 | Commit Loss: 0.000916 | Perplexity: 967.384109
2025-09-14 16:58:06,472 Stage: Train 0.5 | Epoch: 13 | Iter: 41200 | Total Loss: 0.004161 | Recon Loss: 0.003707 | Commit Loss: 0.000908 | Perplexity: 972.376928
2025-09-14 16:58:15,221 Stage: Train 0.5 | Epoch: 13 | Iter: 41400 | Total Loss: 0.004104 | Recon Loss: 0.003650 | Commit Loss: 0.000909 | Perplexity: 968.842808
2025-09-14 16:58:23,966 Stage: Train 0.5 | Epoch: 13 | Iter: 41600 | Total Loss: 0.004041 | Recon Loss: 0.003586 | Commit Loss: 0.000911 | Perplexity: 968.181180
2025-09-14 16:58:32,703 Stage: Train 0.5 | Epoch: 13 | Iter: 41800 | Total Loss: 0.004062 | Recon Loss: 0.003604 | Commit Loss: 0.000917 | Perplexity: 969.714297
2025-09-14 16:58:41,460 Stage: Train 0.5 | Epoch: 13 | Iter: 42000 | Total Loss: 0.004141 | Recon Loss: 0.003681 | Commit Loss: 0.000919 | Perplexity: 970.722228
2025-09-14 16:58:50,254 Stage: Train 0.5 | Epoch: 13 | Iter: 42200 | Total Loss: 0.004240 | Recon Loss: 0.003785 | Commit Loss: 0.000910 | Perplexity: 968.923024
2025-09-14 16:58:59,048 Stage: Train 0.5 | Epoch: 13 | Iter: 42400 | Total Loss: 0.004186 | Recon Loss: 0.003740 | Commit Loss: 0.000893 | Perplexity: 963.807486
Trainning Epoch:   8%|▊         | 14/165 [30:58<5:34:09, 132.78s/it]2025-09-14 16:59:07,856 Stage: Train 0.5 | Epoch: 14 | Iter: 42600 | Total Loss: 0.004206 | Recon Loss: 0.003714 | Commit Loss: 0.000984 | Perplexity: 964.424538
2025-09-14 16:59:16,616 Stage: Train 0.5 | Epoch: 14 | Iter: 42800 | Total Loss: 0.003976 | Recon Loss: 0.003528 | Commit Loss: 0.000896 | Perplexity: 974.694142
2025-09-14 16:59:25,383 Stage: Train 0.5 | Epoch: 14 | Iter: 43000 | Total Loss: 0.004052 | Recon Loss: 0.003605 | Commit Loss: 0.000894 | Perplexity: 970.582229
2025-09-14 16:59:34,168 Stage: Train 0.5 | Epoch: 14 | Iter: 43200 | Total Loss: 0.004006 | Recon Loss: 0.003555 | Commit Loss: 0.000903 | Perplexity: 971.285022
2025-09-14 16:59:42,944 Stage: Train 0.5 | Epoch: 14 | Iter: 43400 | Total Loss: 0.004320 | Recon Loss: 0.003876 | Commit Loss: 0.000889 | Perplexity: 970.119214
2025-09-14 16:59:51,710 Stage: Train 0.5 | Epoch: 14 | Iter: 43600 | Total Loss: 0.004123 | Recon Loss: 0.003673 | Commit Loss: 0.000899 | Perplexity: 973.870683
2025-09-14 17:00:00,500 Stage: Train 0.5 | Epoch: 14 | Iter: 43800 | Total Loss: 0.004075 | Recon Loss: 0.003628 | Commit Loss: 0.000895 | Perplexity: 970.341502
2025-09-14 17:00:09,257 Stage: Train 0.5 | Epoch: 14 | Iter: 44000 | Total Loss: 0.004073 | Recon Loss: 0.003614 | Commit Loss: 0.000919 | Perplexity: 974.866305
2025-09-14 17:00:18,005 Stage: Train 0.5 | Epoch: 14 | Iter: 44200 | Total Loss: 0.004044 | Recon Loss: 0.003591 | Commit Loss: 0.000907 | Perplexity: 968.087064
2025-09-14 17:00:26,756 Stage: Train 0.5 | Epoch: 14 | Iter: 44400 | Total Loss: 0.003994 | Recon Loss: 0.003553 | Commit Loss: 0.000882 | Perplexity: 974.059070
2025-09-14 17:00:35,524 Stage: Train 0.5 | Epoch: 14 | Iter: 44600 | Total Loss: 0.004083 | Recon Loss: 0.003637 | Commit Loss: 0.000892 | Perplexity: 977.227179
2025-09-14 17:00:44,287 Stage: Train 0.5 | Epoch: 14 | Iter: 44800 | Total Loss: 0.004078 | Recon Loss: 0.003635 | Commit Loss: 0.000885 | Perplexity: 978.151100
2025-09-14 17:00:53,035 Stage: Train 0.5 | Epoch: 14 | Iter: 45000 | Total Loss: 0.004012 | Recon Loss: 0.003570 | Commit Loss: 0.000884 | Perplexity: 975.715042
2025-09-14 17:01:01,780 Stage: Train 0.5 | Epoch: 14 | Iter: 45200 | Total Loss: 0.003928 | Recon Loss: 0.003480 | Commit Loss: 0.000896 | Perplexity: 982.577611
2025-09-14 17:01:10,523 Stage: Train 0.5 | Epoch: 14 | Iter: 45400 | Total Loss: 0.004046 | Recon Loss: 0.003600 | Commit Loss: 0.000893 | Perplexity: 980.686378
Trainning Epoch:   9%|▉         | 15/165 [33:11<5:32:12, 132.88s/it]2025-09-14 17:01:19,306 Stage: Train 0.5 | Epoch: 15 | Iter: 45600 | Total Loss: 0.004040 | Recon Loss: 0.003595 | Commit Loss: 0.000889 | Perplexity: 975.357897
2025-09-14 17:01:28,176 Stage: Train 0.5 | Epoch: 15 | Iter: 45800 | Total Loss: 0.003980 | Recon Loss: 0.003548 | Commit Loss: 0.000865 | Perplexity: 981.104748
2025-09-14 17:01:36,928 Stage: Train 0.5 | Epoch: 15 | Iter: 46000 | Total Loss: 0.003869 | Recon Loss: 0.003427 | Commit Loss: 0.000884 | Perplexity: 977.911254
2025-09-14 17:01:45,687 Stage: Train 0.5 | Epoch: 15 | Iter: 46200 | Total Loss: 0.003916 | Recon Loss: 0.003465 | Commit Loss: 0.000902 | Perplexity: 978.245395
2025-09-14 17:01:54,459 Stage: Train 0.5 | Epoch: 15 | Iter: 46400 | Total Loss: 0.004033 | Recon Loss: 0.003589 | Commit Loss: 0.000889 | Perplexity: 977.827342
2025-09-14 17:02:03,247 Stage: Train 0.5 | Epoch: 15 | Iter: 46600 | Total Loss: 0.003892 | Recon Loss: 0.003439 | Commit Loss: 0.000906 | Perplexity: 977.313854
2025-09-14 17:02:12,021 Stage: Train 0.5 | Epoch: 15 | Iter: 46800 | Total Loss: 0.004061 | Recon Loss: 0.003608 | Commit Loss: 0.000905 | Perplexity: 979.262696
2025-09-14 17:02:20,792 Stage: Train 0.5 | Epoch: 15 | Iter: 47000 | Total Loss: 0.003913 | Recon Loss: 0.003472 | Commit Loss: 0.000884 | Perplexity: 978.261482
2025-09-14 17:02:29,585 Stage: Train 0.5 | Epoch: 15 | Iter: 47200 | Total Loss: 0.004161 | Recon Loss: 0.003666 | Commit Loss: 0.000989 | Perplexity: 969.144073
2025-09-14 17:02:38,365 Stage: Train 0.5 | Epoch: 15 | Iter: 47400 | Total Loss: 0.003958 | Recon Loss: 0.003522 | Commit Loss: 0.000872 | Perplexity: 972.439805
2025-09-14 17:02:47,152 Stage: Train 0.5 | Epoch: 15 | Iter: 47600 | Total Loss: 0.003995 | Recon Loss: 0.003555 | Commit Loss: 0.000879 | Perplexity: 972.492978
2025-09-14 17:02:55,929 Stage: Train 0.5 | Epoch: 15 | Iter: 47800 | Total Loss: 0.003890 | Recon Loss: 0.003451 | Commit Loss: 0.000878 | Perplexity: 967.805927
2025-09-14 17:03:04,683 Stage: Train 0.5 | Epoch: 15 | Iter: 48000 | Total Loss: 0.003905 | Recon Loss: 0.003455 | Commit Loss: 0.000900 | Perplexity: 977.433318
2025-09-14 17:03:13,452 Stage: Train 0.5 | Epoch: 15 | Iter: 48200 | Total Loss: 0.003863 | Recon Loss: 0.003417 | Commit Loss: 0.000892 | Perplexity: 974.229346
2025-09-14 17:03:22,215 Stage: Train 0.5 | Epoch: 15 | Iter: 48400 | Total Loss: 0.003903 | Recon Loss: 0.003444 | Commit Loss: 0.000918 | Perplexity: 971.606251
2025-09-14 17:03:30,963 Stage: Train 0.5 | Epoch: 15 | Iter: 48600 | Total Loss: 0.003897 | Recon Loss: 0.003449 | Commit Loss: 0.000897 | Perplexity: 972.282363
Trainning Epoch:  10%|▉         | 16/165 [35:24<5:30:19, 133.02s/it]2025-09-14 17:03:39,740 Stage: Train 0.5 | Epoch: 16 | Iter: 48800 | Total Loss: 0.004065 | Recon Loss: 0.003612 | Commit Loss: 0.000907 | Perplexity: 969.542098
2025-09-14 17:03:48,528 Stage: Train 0.5 | Epoch: 16 | Iter: 49000 | Total Loss: 0.003867 | Recon Loss: 0.003421 | Commit Loss: 0.000892 | Perplexity: 974.532553
2025-09-14 17:03:57,322 Stage: Train 0.5 | Epoch: 16 | Iter: 49200 | Total Loss: 0.003900 | Recon Loss: 0.003452 | Commit Loss: 0.000896 | Perplexity: 974.904011
2025-09-14 17:04:06,115 Stage: Train 0.5 | Epoch: 16 | Iter: 49400 | Total Loss: 0.003846 | Recon Loss: 0.003402 | Commit Loss: 0.000887 | Perplexity: 971.754170
2025-09-14 17:04:14,904 Stage: Train 0.5 | Epoch: 16 | Iter: 49600 | Total Loss: 0.003869 | Recon Loss: 0.003417 | Commit Loss: 0.000905 | Perplexity: 972.949178
2025-09-14 17:04:23,687 Stage: Train 0.5 | Epoch: 16 | Iter: 49800 | Total Loss: 0.003998 | Recon Loss: 0.003552 | Commit Loss: 0.000892 | Perplexity: 972.100845
2025-09-14 17:04:32,479 Stage: Train 0.5 | Epoch: 16 | Iter: 50000 | Total Loss: 0.003937 | Recon Loss: 0.003482 | Commit Loss: 0.000910 | Perplexity: 974.817589
2025-09-14 17:04:41,247 Stage: Train 0.5 | Epoch: 16 | Iter: 50200 | Total Loss: 0.003923 | Recon Loss: 0.003474 | Commit Loss: 0.000897 | Perplexity: 971.383161
2025-09-14 17:04:49,998 Stage: Train 0.5 | Epoch: 16 | Iter: 50400 | Total Loss: 0.003797 | Recon Loss: 0.003350 | Commit Loss: 0.000896 | Perplexity: 971.027088
2025-09-14 17:04:58,781 Stage: Train 0.5 | Epoch: 16 | Iter: 50600 | Total Loss: 0.004056 | Recon Loss: 0.003607 | Commit Loss: 0.000898 | Perplexity: 964.356857
2025-09-14 17:05:07,542 Stage: Train 0.5 | Epoch: 16 | Iter: 50800 | Total Loss: 0.003841 | Recon Loss: 0.003397 | Commit Loss: 0.000888 | Perplexity: 971.504749
2025-09-14 17:05:16,300 Stage: Train 0.5 | Epoch: 16 | Iter: 51000 | Total Loss: 0.003810 | Recon Loss: 0.003363 | Commit Loss: 0.000895 | Perplexity: 974.913539
2025-09-14 17:05:25,076 Stage: Train 0.5 | Epoch: 16 | Iter: 51200 | Total Loss: 0.003847 | Recon Loss: 0.003390 | Commit Loss: 0.000912 | Perplexity: 973.064862
2025-09-14 17:05:33,889 Stage: Train 0.5 | Epoch: 16 | Iter: 51400 | Total Loss: 0.003838 | Recon Loss: 0.003392 | Commit Loss: 0.000892 | Perplexity: 964.531029
2025-09-14 17:05:42,644 Stage: Train 0.5 | Epoch: 16 | Iter: 51600 | Total Loss: 0.003889 | Recon Loss: 0.003441 | Commit Loss: 0.000896 | Perplexity: 970.816406
Trainning Epoch:  10%|█         | 17/165 [37:38<5:28:21, 133.12s/it]2025-09-14 17:05:51,412 Stage: Train 0.5 | Epoch: 17 | Iter: 51800 | Total Loss: 0.003903 | Recon Loss: 0.003447 | Commit Loss: 0.000912 | Perplexity: 971.309792
2025-09-14 17:06:00,167 Stage: Train 0.5 | Epoch: 17 | Iter: 52000 | Total Loss: 0.003801 | Recon Loss: 0.003363 | Commit Loss: 0.000876 | Perplexity: 970.748156
2025-09-14 17:06:08,927 Stage: Train 0.5 | Epoch: 17 | Iter: 52200 | Total Loss: 0.003871 | Recon Loss: 0.003420 | Commit Loss: 0.000903 | Perplexity: 972.578878
2025-09-14 17:06:17,711 Stage: Train 0.5 | Epoch: 17 | Iter: 52400 | Total Loss: 0.003992 | Recon Loss: 0.003540 | Commit Loss: 0.000904 | Perplexity: 970.368481
2025-09-14 17:06:26,460 Stage: Train 0.5 | Epoch: 17 | Iter: 52600 | Total Loss: 0.003969 | Recon Loss: 0.003529 | Commit Loss: 0.000879 | Perplexity: 963.819893
2025-09-14 17:06:35,231 Stage: Train 0.5 | Epoch: 17 | Iter: 52800 | Total Loss: 0.003814 | Recon Loss: 0.003371 | Commit Loss: 0.000886 | Perplexity: 968.648797
2025-09-14 17:06:44,006 Stage: Train 0.5 | Epoch: 17 | Iter: 53000 | Total Loss: 0.003934 | Recon Loss: 0.003492 | Commit Loss: 0.000883 | Perplexity: 965.450179
2025-09-14 17:06:52,901 Stage: Train 0.5 | Epoch: 17 | Iter: 53200 | Total Loss: 0.003895 | Recon Loss: 0.003457 | Commit Loss: 0.000876 | Perplexity: 966.271609
2025-09-14 17:07:01,818 Stage: Train 0.5 | Epoch: 17 | Iter: 53400 | Total Loss: 0.003864 | Recon Loss: 0.003421 | Commit Loss: 0.000886 | Perplexity: 969.450415
2025-09-14 17:07:10,697 Stage: Train 0.5 | Epoch: 17 | Iter: 53600 | Total Loss: 0.003848 | Recon Loss: 0.003417 | Commit Loss: 0.000862 | Perplexity: 965.143685
2025-09-14 17:07:19,580 Stage: Train 0.5 | Epoch: 17 | Iter: 53800 | Total Loss: 0.003755 | Recon Loss: 0.003306 | Commit Loss: 0.000898 | Perplexity: 973.998763
2025-09-14 17:07:28,455 Stage: Train 0.5 | Epoch: 17 | Iter: 54000 | Total Loss: 0.003903 | Recon Loss: 0.003451 | Commit Loss: 0.000904 | Perplexity: 971.102240
2025-09-14 17:07:37,298 Stage: Train 0.5 | Epoch: 17 | Iter: 54200 | Total Loss: 0.003861 | Recon Loss: 0.003411 | Commit Loss: 0.000900 | Perplexity: 967.677782
2025-09-14 17:07:45,999 Stage: Train 0.5 | Epoch: 17 | Iter: 54400 | Total Loss: 0.003886 | Recon Loss: 0.003445 | Commit Loss: 0.000882 | Perplexity: 965.295357
2025-09-14 17:07:54,690 Stage: Train 0.5 | Epoch: 17 | Iter: 54600 | Total Loss: 0.003725 | Recon Loss: 0.003270 | Commit Loss: 0.000911 | Perplexity: 971.889703
Trainning Epoch:  11%|█         | 18/165 [39:51<5:26:32, 133.28s/it]2025-09-14 17:08:03,397 Stage: Train 0.5 | Epoch: 18 | Iter: 54800 | Total Loss: 0.003891 | Recon Loss: 0.003444 | Commit Loss: 0.000894 | Perplexity: 962.919163
2025-09-14 17:08:12,101 Stage: Train 0.5 | Epoch: 18 | Iter: 55000 | Total Loss: 0.003819 | Recon Loss: 0.003379 | Commit Loss: 0.000880 | Perplexity: 965.327217
2025-09-14 17:08:20,828 Stage: Train 0.5 | Epoch: 18 | Iter: 55200 | Total Loss: 0.003788 | Recon Loss: 0.003334 | Commit Loss: 0.000908 | Perplexity: 970.236979
2025-09-14 17:08:29,511 Stage: Train 0.5 | Epoch: 18 | Iter: 55400 | Total Loss: 0.003696 | Recon Loss: 0.003243 | Commit Loss: 0.000906 | Perplexity: 975.145094
2025-09-14 17:08:38,228 Stage: Train 0.5 | Epoch: 18 | Iter: 55600 | Total Loss: 0.003714 | Recon Loss: 0.003266 | Commit Loss: 0.000897 | Perplexity: 967.229461
2025-09-14 17:08:46,937 Stage: Train 0.5 | Epoch: 18 | Iter: 55800 | Total Loss: 0.003862 | Recon Loss: 0.003408 | Commit Loss: 0.000908 | Perplexity: 965.345548
2025-09-14 17:08:55,644 Stage: Train 0.5 | Epoch: 18 | Iter: 56000 | Total Loss: 0.003732 | Recon Loss: 0.003278 | Commit Loss: 0.000909 | Perplexity: 973.262274
2025-09-14 17:09:04,350 Stage: Train 0.5 | Epoch: 18 | Iter: 56200 | Total Loss: 0.003851 | Recon Loss: 0.003401 | Commit Loss: 0.000899 | Perplexity: 966.820789
2025-09-14 17:09:13,040 Stage: Train 0.5 | Epoch: 18 | Iter: 56400 | Total Loss: 0.003724 | Recon Loss: 0.003276 | Commit Loss: 0.000896 | Perplexity: 973.113280
2025-09-14 17:09:21,774 Stage: Train 0.5 | Epoch: 18 | Iter: 56600 | Total Loss: 0.003842 | Recon Loss: 0.003394 | Commit Loss: 0.000895 | Perplexity: 966.374077
2025-09-14 17:09:30,470 Stage: Train 0.5 | Epoch: 18 | Iter: 56800 | Total Loss: 0.003757 | Recon Loss: 0.003307 | Commit Loss: 0.000900 | Perplexity: 967.945434
2025-09-14 17:09:39,166 Stage: Train 0.5 | Epoch: 18 | Iter: 57000 | Total Loss: 0.003695 | Recon Loss: 0.003243 | Commit Loss: 0.000904 | Perplexity: 968.725286
2025-09-14 17:09:47,907 Stage: Train 0.5 | Epoch: 18 | Iter: 57200 | Total Loss: 0.003735 | Recon Loss: 0.003276 | Commit Loss: 0.000917 | Perplexity: 973.069438
2025-09-14 17:09:56,596 Stage: Train 0.5 | Epoch: 18 | Iter: 57400 | Total Loss: 0.003752 | Recon Loss: 0.003305 | Commit Loss: 0.000893 | Perplexity: 974.299952
2025-09-14 17:10:05,313 Stage: Train 0.5 | Epoch: 18 | Iter: 57600 | Total Loss: 0.003813 | Recon Loss: 0.003364 | Commit Loss: 0.000899 | Perplexity: 968.295882
Trainning Epoch:  12%|█▏        | 19/165 [42:04<5:23:36, 132.99s/it]2025-09-14 17:10:14,045 Stage: Train 0.5 | Epoch: 19 | Iter: 57800 | Total Loss: 0.003770 | Recon Loss: 0.003341 | Commit Loss: 0.000859 | Perplexity: 964.985177
2025-09-14 17:10:22,747 Stage: Train 0.5 | Epoch: 19 | Iter: 58000 | Total Loss: 0.003634 | Recon Loss: 0.003193 | Commit Loss: 0.000884 | Perplexity: 971.281730
2025-09-14 17:10:31,441 Stage: Train 0.5 | Epoch: 19 | Iter: 58200 | Total Loss: 0.003750 | Recon Loss: 0.003311 | Commit Loss: 0.000878 | Perplexity: 967.564857
2025-09-14 17:10:40,130 Stage: Train 0.5 | Epoch: 19 | Iter: 58400 | Total Loss: 0.003766 | Recon Loss: 0.003325 | Commit Loss: 0.000881 | Perplexity: 965.602578
2025-09-14 17:10:48,853 Stage: Train 0.5 | Epoch: 19 | Iter: 58600 | Total Loss: 0.003771 | Recon Loss: 0.003327 | Commit Loss: 0.000889 | Perplexity: 973.683676
2025-09-14 17:10:57,564 Stage: Train 0.5 | Epoch: 19 | Iter: 58800 | Total Loss: 0.003661 | Recon Loss: 0.003224 | Commit Loss: 0.000875 | Perplexity: 970.323587
2025-09-14 17:11:06,283 Stage: Train 0.5 | Epoch: 19 | Iter: 59000 | Total Loss: 0.003710 | Recon Loss: 0.003273 | Commit Loss: 0.000873 | Perplexity: 965.584544
2025-09-14 17:11:15,011 Stage: Train 0.5 | Epoch: 19 | Iter: 59200 | Total Loss: 0.003731 | Recon Loss: 0.003295 | Commit Loss: 0.000871 | Perplexity: 965.918071
2025-09-14 17:11:23,748 Stage: Train 0.5 | Epoch: 19 | Iter: 59400 | Total Loss: 0.003705 | Recon Loss: 0.003272 | Commit Loss: 0.000866 | Perplexity: 969.701634
2025-09-14 17:11:32,418 Stage: Train 0.5 | Epoch: 19 | Iter: 59600 | Total Loss: 0.003675 | Recon Loss: 0.003238 | Commit Loss: 0.000874 | Perplexity: 960.970639
2025-09-14 17:11:41,105 Stage: Train 0.5 | Epoch: 19 | Iter: 59800 | Total Loss: 0.003753 | Recon Loss: 0.003283 | Commit Loss: 0.000940 | Perplexity: 962.017343
2025-09-14 17:11:49,804 Stage: Train 0.5 | Epoch: 19 | Iter: 60000 | Total Loss: 0.003773 | Recon Loss: 0.003308 | Commit Loss: 0.000931 | Perplexity: 963.315752
2025-09-14 17:11:49,804 Saving model at iteration 60000
2025-09-14 17:11:49,998 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_20_step_60000
2025-09-14 17:11:50,227 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_20_step_60000/pytorch_model.bin
2025-09-14 17:11:50,590 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_20_step_60000/optimizer.bin
2025-09-14 17:11:50,591 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_20_step_60000/scheduler.bin
2025-09-14 17:11:50,592 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_20_step_60000/random_states_0.pkl
2025-09-14 17:11:59,705 Stage: Train 0.5 | Epoch: 19 | Iter: 60200 | Total Loss: 0.003714 | Recon Loss: 0.003272 | Commit Loss: 0.000884 | Perplexity: 965.164884
2025-09-14 17:12:08,443 Stage: Train 0.5 | Epoch: 19 | Iter: 60400 | Total Loss: 0.003605 | Recon Loss: 0.003164 | Commit Loss: 0.000881 | Perplexity: 964.762628
2025-09-14 17:12:17,185 Stage: Train 0.5 | Epoch: 19 | Iter: 60600 | Total Loss: 0.003647 | Recon Loss: 0.003209 | Commit Loss: 0.000876 | Perplexity: 957.832100
Trainning Epoch:  12%|█▏        | 20/165 [44:18<5:22:04, 133.27s/it]2025-09-14 17:12:26,324 Stage: Train 0.5 | Epoch: 20 | Iter: 60800 | Total Loss: 0.003665 | Recon Loss: 0.003229 | Commit Loss: 0.000872 | Perplexity: 961.924585
2025-09-14 17:12:35,126 Stage: Train 0.5 | Epoch: 20 | Iter: 61000 | Total Loss: 0.003602 | Recon Loss: 0.003167 | Commit Loss: 0.000869 | Perplexity: 956.809388
2025-09-14 17:12:43,862 Stage: Train 0.5 | Epoch: 20 | Iter: 61200 | Total Loss: 0.003668 | Recon Loss: 0.003235 | Commit Loss: 0.000864 | Perplexity: 965.256213
2025-09-14 17:12:52,586 Stage: Train 0.5 | Epoch: 20 | Iter: 61400 | Total Loss: 0.003599 | Recon Loss: 0.003159 | Commit Loss: 0.000881 | Perplexity: 966.266620
2025-09-14 17:13:01,302 Stage: Train 0.5 | Epoch: 20 | Iter: 61600 | Total Loss: 0.003707 | Recon Loss: 0.003268 | Commit Loss: 0.000878 | Perplexity: 961.765722
2025-09-14 17:13:10,018 Stage: Train 0.5 | Epoch: 20 | Iter: 61800 | Total Loss: 0.003720 | Recon Loss: 0.003280 | Commit Loss: 0.000879 | Perplexity: 958.376752
2025-09-14 17:13:18,740 Stage: Train 0.5 | Epoch: 20 | Iter: 62000 | Total Loss: 0.003503 | Recon Loss: 0.003062 | Commit Loss: 0.000882 | Perplexity: 961.692716
2025-09-14 17:13:27,442 Stage: Train 0.5 | Epoch: 20 | Iter: 62200 | Total Loss: 0.003666 | Recon Loss: 0.003226 | Commit Loss: 0.000879 | Perplexity: 959.133049
2025-09-14 17:13:36,180 Stage: Train 0.5 | Epoch: 20 | Iter: 62400 | Total Loss: 0.003656 | Recon Loss: 0.003213 | Commit Loss: 0.000886 | Perplexity: 954.395531
2025-09-14 17:13:44,889 Stage: Train 0.5 | Epoch: 20 | Iter: 62600 | Total Loss: 0.003771 | Recon Loss: 0.003322 | Commit Loss: 0.000897 | Perplexity: 960.666803
2025-09-14 17:13:53,592 Stage: Train 0.5 | Epoch: 20 | Iter: 62800 | Total Loss: 0.003545 | Recon Loss: 0.003106 | Commit Loss: 0.000878 | Perplexity: 957.808969
2025-09-14 17:14:02,316 Stage: Train 0.5 | Epoch: 20 | Iter: 63000 | Total Loss: 0.003638 | Recon Loss: 0.003198 | Commit Loss: 0.000880 | Perplexity: 956.647943
2025-09-14 17:14:11,028 Stage: Train 0.5 | Epoch: 20 | Iter: 63200 | Total Loss: 0.003666 | Recon Loss: 0.003220 | Commit Loss: 0.000893 | Perplexity: 960.762136
2025-09-14 17:14:19,692 Stage: Train 0.5 | Epoch: 20 | Iter: 63400 | Total Loss: 0.003569 | Recon Loss: 0.003129 | Commit Loss: 0.000880 | Perplexity: 957.575572
2025-09-14 17:14:28,392 Stage: Train 0.5 | Epoch: 20 | Iter: 63600 | Total Loss: 0.003671 | Recon Loss: 0.003227 | Commit Loss: 0.000888 | Perplexity: 963.130154
Trainning Epoch:  13%|█▎        | 21/165 [46:30<5:19:15, 133.03s/it]2025-09-14 17:14:37,102 Stage: Train 0.5 | Epoch: 21 | Iter: 63800 | Total Loss: 0.003630 | Recon Loss: 0.003183 | Commit Loss: 0.000894 | Perplexity: 958.310241
2025-09-14 17:14:45,796 Stage: Train 0.5 | Epoch: 21 | Iter: 64000 | Total Loss: 0.003986 | Recon Loss: 0.003460 | Commit Loss: 0.001052 | Perplexity: 946.248991
2025-09-14 17:14:54,484 Stage: Train 0.5 | Epoch: 21 | Iter: 64200 | Total Loss: 0.003525 | Recon Loss: 0.003088 | Commit Loss: 0.000874 | Perplexity: 959.899900
2025-09-14 17:15:03,193 Stage: Train 0.5 | Epoch: 21 | Iter: 64400 | Total Loss: 0.003649 | Recon Loss: 0.003208 | Commit Loss: 0.000881 | Perplexity: 961.496702
2025-09-14 17:15:11,895 Stage: Train 0.5 | Epoch: 21 | Iter: 64600 | Total Loss: 0.003609 | Recon Loss: 0.003163 | Commit Loss: 0.000893 | Perplexity: 963.257828
2025-09-14 17:15:20,588 Stage: Train 0.5 | Epoch: 21 | Iter: 64800 | Total Loss: 0.003661 | Recon Loss: 0.003215 | Commit Loss: 0.000893 | Perplexity: 958.547945
2025-09-14 17:15:29,329 Stage: Train 0.5 | Epoch: 21 | Iter: 65000 | Total Loss: 0.003580 | Recon Loss: 0.003139 | Commit Loss: 0.000881 | Perplexity: 959.976147
2025-09-14 17:15:38,053 Stage: Train 0.5 | Epoch: 21 | Iter: 65200 | Total Loss: 0.003607 | Recon Loss: 0.003164 | Commit Loss: 0.000885 | Perplexity: 957.708772
2025-09-14 17:15:46,784 Stage: Train 0.5 | Epoch: 21 | Iter: 65400 | Total Loss: 0.003703 | Recon Loss: 0.003268 | Commit Loss: 0.000871 | Perplexity: 960.818507
2025-09-14 17:15:55,547 Stage: Train 0.5 | Epoch: 21 | Iter: 65600 | Total Loss: 0.003551 | Recon Loss: 0.003115 | Commit Loss: 0.000871 | Perplexity: 959.861681
2025-09-14 17:16:04,267 Stage: Train 0.5 | Epoch: 21 | Iter: 65800 | Total Loss: 0.003643 | Recon Loss: 0.003195 | Commit Loss: 0.000895 | Perplexity: 956.969328
2025-09-14 17:16:12,980 Stage: Train 0.5 | Epoch: 21 | Iter: 66000 | Total Loss: 0.003571 | Recon Loss: 0.003136 | Commit Loss: 0.000870 | Perplexity: 956.849252
2025-09-14 17:16:21,719 Stage: Train 0.5 | Epoch: 21 | Iter: 66200 | Total Loss: 0.003593 | Recon Loss: 0.003153 | Commit Loss: 0.000879 | Perplexity: 962.852352
2025-09-14 17:16:30,456 Stage: Train 0.5 | Epoch: 21 | Iter: 66400 | Total Loss: 0.003720 | Recon Loss: 0.003281 | Commit Loss: 0.000878 | Perplexity: 955.437265
2025-09-14 17:16:39,192 Stage: Train 0.5 | Epoch: 21 | Iter: 66600 | Total Loss: 0.003576 | Recon Loss: 0.003139 | Commit Loss: 0.000874 | Perplexity: 955.105056
2025-09-14 17:16:47,925 Stage: Train 0.5 | Epoch: 21 | Iter: 66800 | Total Loss: 0.003728 | Recon Loss: 0.003258 | Commit Loss: 0.000941 | Perplexity: 956.118964
Trainning Epoch:  13%|█▎        | 22/165 [48:43<5:16:39, 132.86s/it]2025-09-14 17:16:56,696 Stage: Train 0.5 | Epoch: 22 | Iter: 67000 | Total Loss: 0.003651 | Recon Loss: 0.003206 | Commit Loss: 0.000889 | Perplexity: 954.487370
2025-09-14 17:17:05,414 Stage: Train 0.5 | Epoch: 22 | Iter: 67200 | Total Loss: 0.003466 | Recon Loss: 0.003029 | Commit Loss: 0.000875 | Perplexity: 959.169378
2025-09-14 17:17:14,115 Stage: Train 0.5 | Epoch: 22 | Iter: 67400 | Total Loss: 0.003647 | Recon Loss: 0.003205 | Commit Loss: 0.000884 | Perplexity: 959.954755
2025-09-14 17:17:22,873 Stage: Train 0.5 | Epoch: 22 | Iter: 67600 | Total Loss: 0.003612 | Recon Loss: 0.003171 | Commit Loss: 0.000882 | Perplexity: 963.585789
2025-09-14 17:17:31,616 Stage: Train 0.5 | Epoch: 22 | Iter: 67800 | Total Loss: 0.003561 | Recon Loss: 0.003126 | Commit Loss: 0.000870 | Perplexity: 955.074196
2025-09-14 17:17:40,336 Stage: Train 0.5 | Epoch: 22 | Iter: 68000 | Total Loss: 0.003536 | Recon Loss: 0.003091 | Commit Loss: 0.000890 | Perplexity: 963.823900
2025-09-14 17:17:49,036 Stage: Train 0.5 | Epoch: 22 | Iter: 68200 | Total Loss: 0.003526 | Recon Loss: 0.003084 | Commit Loss: 0.000883 | Perplexity: 958.359432
2025-09-14 17:17:57,746 Stage: Train 0.5 | Epoch: 22 | Iter: 68400 | Total Loss: 0.003654 | Recon Loss: 0.003210 | Commit Loss: 0.000888 | Perplexity: 953.899818
2025-09-14 17:18:06,470 Stage: Train 0.5 | Epoch: 22 | Iter: 68600 | Total Loss: 0.003503 | Recon Loss: 0.003062 | Commit Loss: 0.000883 | Perplexity: 958.823657
2025-09-14 17:18:15,194 Stage: Train 0.5 | Epoch: 22 | Iter: 68800 | Total Loss: 0.003525 | Recon Loss: 0.003077 | Commit Loss: 0.000895 | Perplexity: 963.771812
2025-09-14 17:18:23,901 Stage: Train 0.5 | Epoch: 22 | Iter: 69000 | Total Loss: 0.003570 | Recon Loss: 0.003129 | Commit Loss: 0.000883 | Perplexity: 960.089416
2025-09-14 17:18:32,613 Stage: Train 0.5 | Epoch: 22 | Iter: 69200 | Total Loss: 0.003454 | Recon Loss: 0.003008 | Commit Loss: 0.000892 | Perplexity: 962.728513
2025-09-14 17:18:41,328 Stage: Train 0.5 | Epoch: 22 | Iter: 69400 | Total Loss: 0.003565 | Recon Loss: 0.003118 | Commit Loss: 0.000893 | Perplexity: 954.876594
2025-09-14 17:18:50,031 Stage: Train 0.5 | Epoch: 22 | Iter: 69600 | Total Loss: 0.003488 | Recon Loss: 0.003045 | Commit Loss: 0.000887 | Perplexity: 953.667980
2025-09-14 17:18:58,685 Stage: Train 0.5 | Epoch: 22 | Iter: 69800 | Total Loss: 0.003627 | Recon Loss: 0.003171 | Commit Loss: 0.000913 | Perplexity: 960.181091
Trainning Epoch:  14%|█▍        | 23/165 [50:55<5:14:06, 132.72s/it]2025-09-14 17:19:07,390 Stage: Train 0.5 | Epoch: 23 | Iter: 70000 | Total Loss: 0.003521 | Recon Loss: 0.003078 | Commit Loss: 0.000887 | Perplexity: 955.465770
2025-09-14 17:19:16,070 Stage: Train 0.5 | Epoch: 23 | Iter: 70200 | Total Loss: 0.003561 | Recon Loss: 0.003120 | Commit Loss: 0.000882 | Perplexity: 957.004132
2025-09-14 17:19:24,775 Stage: Train 0.5 | Epoch: 23 | Iter: 70400 | Total Loss: 0.003532 | Recon Loss: 0.003081 | Commit Loss: 0.000903 | Perplexity: 959.400054
2025-09-14 17:19:33,480 Stage: Train 0.5 | Epoch: 23 | Iter: 70600 | Total Loss: 0.003580 | Recon Loss: 0.003138 | Commit Loss: 0.000882 | Perplexity: 954.390792
2025-09-14 17:19:42,186 Stage: Train 0.5 | Epoch: 23 | Iter: 70800 | Total Loss: 0.003606 | Recon Loss: 0.003160 | Commit Loss: 0.000893 | Perplexity: 956.769667
2025-09-14 17:19:50,904 Stage: Train 0.5 | Epoch: 23 | Iter: 71000 | Total Loss: 0.003451 | Recon Loss: 0.003019 | Commit Loss: 0.000864 | Perplexity: 956.082254
2025-09-14 17:19:59,597 Stage: Train 0.5 | Epoch: 23 | Iter: 71200 | Total Loss: 0.003611 | Recon Loss: 0.003160 | Commit Loss: 0.000902 | Perplexity: 964.525005
2025-09-14 17:20:08,290 Stage: Train 0.5 | Epoch: 23 | Iter: 71400 | Total Loss: 0.003647 | Recon Loss: 0.003199 | Commit Loss: 0.000896 | Perplexity: 962.111980
2025-09-14 17:20:16,998 Stage: Train 0.5 | Epoch: 23 | Iter: 71600 | Total Loss: 0.003488 | Recon Loss: 0.003048 | Commit Loss: 0.000880 | Perplexity: 962.603666
2025-09-14 17:20:25,714 Stage: Train 0.5 | Epoch: 23 | Iter: 71800 | Total Loss: 0.003553 | Recon Loss: 0.003104 | Commit Loss: 0.000898 | Perplexity: 956.807228
2025-09-14 17:20:34,461 Stage: Train 0.5 | Epoch: 23 | Iter: 72000 | Total Loss: 0.003596 | Recon Loss: 0.003144 | Commit Loss: 0.000904 | Perplexity: 962.026425
2025-09-14 17:20:43,211 Stage: Train 0.5 | Epoch: 23 | Iter: 72200 | Total Loss: 0.003464 | Recon Loss: 0.003016 | Commit Loss: 0.000896 | Perplexity: 963.215182
2025-09-14 17:20:51,938 Stage: Train 0.5 | Epoch: 23 | Iter: 72400 | Total Loss: 0.003554 | Recon Loss: 0.003107 | Commit Loss: 0.000894 | Perplexity: 961.020268
2025-09-14 17:21:00,641 Stage: Train 0.5 | Epoch: 23 | Iter: 72600 | Total Loss: 0.003472 | Recon Loss: 0.003023 | Commit Loss: 0.000900 | Perplexity: 957.916821
2025-09-14 17:21:09,382 Stage: Train 0.5 | Epoch: 23 | Iter: 72800 | Total Loss: 0.003609 | Recon Loss: 0.003172 | Commit Loss: 0.000873 | Perplexity: 954.870879
Trainning Epoch:  15%|█▍        | 24/165 [53:07<5:11:38, 132.62s/it]2025-09-14 17:21:18,108 Stage: Train 0.5 | Epoch: 24 | Iter: 73000 | Total Loss: 0.003503 | Recon Loss: 0.003062 | Commit Loss: 0.000883 | Perplexity: 958.087562
2025-09-14 17:21:26,850 Stage: Train 0.5 | Epoch: 24 | Iter: 73200 | Total Loss: 0.003571 | Recon Loss: 0.003118 | Commit Loss: 0.000906 | Perplexity: 956.229211
2025-09-14 17:21:35,580 Stage: Train 0.5 | Epoch: 24 | Iter: 73400 | Total Loss: 0.003444 | Recon Loss: 0.002991 | Commit Loss: 0.000907 | Perplexity: 961.175516
2025-09-14 17:21:44,292 Stage: Train 0.5 | Epoch: 24 | Iter: 73600 | Total Loss: 0.003474 | Recon Loss: 0.003025 | Commit Loss: 0.000898 | Perplexity: 961.265648
2025-09-14 17:21:53,001 Stage: Train 0.5 | Epoch: 24 | Iter: 73800 | Total Loss: 0.003693 | Recon Loss: 0.003218 | Commit Loss: 0.000949 | Perplexity: 952.933984
2025-09-14 17:22:01,742 Stage: Train 0.5 | Epoch: 24 | Iter: 74000 | Total Loss: 0.003505 | Recon Loss: 0.003062 | Commit Loss: 0.000884 | Perplexity: 962.546638
2025-09-14 17:22:10,436 Stage: Train 0.5 | Epoch: 24 | Iter: 74200 | Total Loss: 0.003486 | Recon Loss: 0.003045 | Commit Loss: 0.000882 | Perplexity: 956.251137
2025-09-14 17:22:19,137 Stage: Train 0.5 | Epoch: 24 | Iter: 74400 | Total Loss: 0.003439 | Recon Loss: 0.002990 | Commit Loss: 0.000897 | Perplexity: 962.738919
2025-09-14 17:22:27,844 Stage: Train 0.5 | Epoch: 24 | Iter: 74600 | Total Loss: 0.003684 | Recon Loss: 0.003237 | Commit Loss: 0.000894 | Perplexity: 955.503440
2025-09-14 17:22:36,546 Stage: Train 0.5 | Epoch: 24 | Iter: 74800 | Total Loss: 0.003447 | Recon Loss: 0.003003 | Commit Loss: 0.000888 | Perplexity: 960.551980
2025-09-14 17:22:45,278 Stage: Train 0.5 | Epoch: 24 | Iter: 75000 | Total Loss: 0.003636 | Recon Loss: 0.003179 | Commit Loss: 0.000913 | Perplexity: 954.928600
2025-09-14 17:22:53,969 Stage: Train 0.5 | Epoch: 24 | Iter: 75200 | Total Loss: 0.003442 | Recon Loss: 0.002994 | Commit Loss: 0.000897 | Perplexity: 956.410991
2025-09-14 17:23:02,695 Stage: Train 0.5 | Epoch: 24 | Iter: 75400 | Total Loss: 0.003438 | Recon Loss: 0.002989 | Commit Loss: 0.000899 | Perplexity: 962.935210
2025-09-14 17:23:11,412 Stage: Train 0.5 | Epoch: 24 | Iter: 75600 | Total Loss: 0.003551 | Recon Loss: 0.003102 | Commit Loss: 0.000898 | Perplexity: 950.681641
2025-09-14 17:23:20,106 Stage: Train 0.5 | Epoch: 24 | Iter: 75800 | Total Loss: 0.003508 | Recon Loss: 0.003050 | Commit Loss: 0.000916 | Perplexity: 959.918835
Trainning Epoch:  15%|█▌        | 25/165 [55:20<5:09:16, 132.55s/it]2025-09-14 17:23:28,830 Stage: Train 0.5 | Epoch: 25 | Iter: 76000 | Total Loss: 0.003491 | Recon Loss: 0.003041 | Commit Loss: 0.000900 | Perplexity: 957.328559
2025-09-14 17:23:37,532 Stage: Train 0.5 | Epoch: 25 | Iter: 76200 | Total Loss: 0.003490 | Recon Loss: 0.003045 | Commit Loss: 0.000891 | Perplexity: 958.573681
2025-09-14 17:23:46,231 Stage: Train 0.5 | Epoch: 25 | Iter: 76400 | Total Loss: 0.003482 | Recon Loss: 0.003037 | Commit Loss: 0.000890 | Perplexity: 956.569041
2025-09-14 17:23:54,928 Stage: Train 0.5 | Epoch: 25 | Iter: 76600 | Total Loss: 0.003394 | Recon Loss: 0.002942 | Commit Loss: 0.000905 | Perplexity: 961.074828
2025-09-14 17:24:03,628 Stage: Train 0.5 | Epoch: 25 | Iter: 76800 | Total Loss: 0.003375 | Recon Loss: 0.002921 | Commit Loss: 0.000908 | Perplexity: 957.851695
2025-09-14 17:24:12,348 Stage: Train 0.5 | Epoch: 25 | Iter: 77000 | Total Loss: 0.003556 | Recon Loss: 0.003107 | Commit Loss: 0.000897 | Perplexity: 957.023839
2025-09-14 17:24:21,078 Stage: Train 0.5 | Epoch: 25 | Iter: 77200 | Total Loss: 0.003419 | Recon Loss: 0.002976 | Commit Loss: 0.000886 | Perplexity: 957.722868
2025-09-14 17:24:29,771 Stage: Train 0.5 | Epoch: 25 | Iter: 77400 | Total Loss: 0.003399 | Recon Loss: 0.002942 | Commit Loss: 0.000913 | Perplexity: 962.404228
2025-09-14 17:24:38,474 Stage: Train 0.5 | Epoch: 25 | Iter: 77600 | Total Loss: 0.003443 | Recon Loss: 0.002996 | Commit Loss: 0.000893 | Perplexity: 956.552482
2025-09-14 17:24:47,188 Stage: Train 0.5 | Epoch: 25 | Iter: 77800 | Total Loss: 0.003461 | Recon Loss: 0.003013 | Commit Loss: 0.000897 | Perplexity: 957.807382
2025-09-14 17:24:55,893 Stage: Train 0.5 | Epoch: 25 | Iter: 78000 | Total Loss: 0.003455 | Recon Loss: 0.003003 | Commit Loss: 0.000904 | Perplexity: 962.990494
2025-09-14 17:25:04,606 Stage: Train 0.5 | Epoch: 25 | Iter: 78200 | Total Loss: 0.003439 | Recon Loss: 0.002987 | Commit Loss: 0.000903 | Perplexity: 960.636064
2025-09-14 17:25:13,357 Stage: Train 0.5 | Epoch: 25 | Iter: 78400 | Total Loss: 0.003458 | Recon Loss: 0.003006 | Commit Loss: 0.000903 | Perplexity: 959.268644
2025-09-14 17:25:22,104 Stage: Train 0.5 | Epoch: 25 | Iter: 78600 | Total Loss: 0.003504 | Recon Loss: 0.003051 | Commit Loss: 0.000905 | Perplexity: 958.398005
2025-09-14 17:25:30,821 Stage: Train 0.5 | Epoch: 25 | Iter: 78800 | Total Loss: 0.003621 | Recon Loss: 0.003179 | Commit Loss: 0.000883 | Perplexity: 951.122487
Trainning Epoch:  16%|█▌        | 26/165 [57:32<5:06:56, 132.49s/it]2025-09-14 17:25:39,547 Stage: Train 0.5 | Epoch: 26 | Iter: 79000 | Total Loss: 0.003419 | Recon Loss: 0.002964 | Commit Loss: 0.000910 | Perplexity: 959.813508
2025-09-14 17:25:48,251 Stage: Train 0.5 | Epoch: 26 | Iter: 79200 | Total Loss: 0.003496 | Recon Loss: 0.003046 | Commit Loss: 0.000900 | Perplexity: 955.187049
2025-09-14 17:25:56,991 Stage: Train 0.5 | Epoch: 26 | Iter: 79400 | Total Loss: 0.003368 | Recon Loss: 0.002923 | Commit Loss: 0.000889 | Perplexity: 959.826412
2025-09-14 17:26:05,684 Stage: Train 0.5 | Epoch: 26 | Iter: 79600 | Total Loss: 0.003643 | Recon Loss: 0.003160 | Commit Loss: 0.000965 | Perplexity: 951.133595
2025-09-14 17:26:14,353 Stage: Train 0.5 | Epoch: 26 | Iter: 79800 | Total Loss: 0.003331 | Recon Loss: 0.002885 | Commit Loss: 0.000893 | Perplexity: 959.992731
2025-09-14 17:26:23,070 Stage: Train 0.5 | Epoch: 26 | Iter: 80000 | Total Loss: 0.003360 | Recon Loss: 0.002914 | Commit Loss: 0.000893 | Perplexity: 957.843851
2025-09-14 17:26:23,070 Saving model at iteration 80000
2025-09-14 17:26:23,567 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_27_step_80000
2025-09-14 17:26:23,799 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_27_step_80000/pytorch_model.bin
2025-09-14 17:26:24,170 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_27_step_80000/optimizer.bin
2025-09-14 17:26:24,170 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_27_step_80000/scheduler.bin
2025-09-14 17:26:24,171 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_27_step_80000/random_states_0.pkl
2025-09-14 17:26:33,314 Stage: Train 0.5 | Epoch: 26 | Iter: 80200 | Total Loss: 0.003425 | Recon Loss: 0.002973 | Commit Loss: 0.000904 | Perplexity: 964.049783
2025-09-14 17:26:42,038 Stage: Train 0.5 | Epoch: 26 | Iter: 80400 | Total Loss: 0.003466 | Recon Loss: 0.003015 | Commit Loss: 0.000901 | Perplexity: 954.155215
2025-09-14 17:26:50,782 Stage: Train 0.5 | Epoch: 26 | Iter: 80600 | Total Loss: 0.003484 | Recon Loss: 0.003028 | Commit Loss: 0.000911 | Perplexity: 961.918702
2025-09-14 17:26:59,505 Stage: Train 0.5 | Epoch: 26 | Iter: 80800 | Total Loss: 0.003418 | Recon Loss: 0.002960 | Commit Loss: 0.000916 | Perplexity: 959.872801
2025-09-14 17:27:08,222 Stage: Train 0.5 | Epoch: 26 | Iter: 81000 | Total Loss: 0.003458 | Recon Loss: 0.003004 | Commit Loss: 0.000908 | Perplexity: 955.665159
2025-09-14 17:27:16,974 Stage: Train 0.5 | Epoch: 26 | Iter: 81200 | Total Loss: 0.003396 | Recon Loss: 0.002936 | Commit Loss: 0.000920 | Perplexity: 962.493120
2025-09-14 17:27:25,696 Stage: Train 0.5 | Epoch: 26 | Iter: 81400 | Total Loss: 0.003593 | Recon Loss: 0.003156 | Commit Loss: 0.000875 | Perplexity: 950.304475
2025-09-14 17:27:34,421 Stage: Train 0.5 | Epoch: 26 | Iter: 81600 | Total Loss: 0.003377 | Recon Loss: 0.002930 | Commit Loss: 0.000894 | Perplexity: 959.286800
2025-09-14 17:27:43,131 Stage: Train 0.5 | Epoch: 26 | Iter: 81800 | Total Loss: 0.003463 | Recon Loss: 0.003012 | Commit Loss: 0.000902 | Perplexity: 960.931648
2025-09-14 17:27:51,837 Stage: Train 0.5 | Epoch: 26 | Iter: 82000 | Total Loss: 0.003439 | Recon Loss: 0.002992 | Commit Loss: 0.000894 | Perplexity: 958.817733
Trainning Epoch:  16%|█▋        | 27/165 [59:46<5:05:44, 132.93s/it]2025-09-14 17:28:00,597 Stage: Train 0.5 | Epoch: 27 | Iter: 82200 | Total Loss: 0.003410 | Recon Loss: 0.002962 | Commit Loss: 0.000896 | Perplexity: 950.968955
2025-09-14 17:28:09,291 Stage: Train 0.5 | Epoch: 27 | Iter: 82400 | Total Loss: 0.003407 | Recon Loss: 0.002960 | Commit Loss: 0.000893 | Perplexity: 954.350847
2025-09-14 17:28:17,962 Stage: Train 0.5 | Epoch: 27 | Iter: 82600 | Total Loss: 0.003404 | Recon Loss: 0.002952 | Commit Loss: 0.000903 | Perplexity: 960.641018
2025-09-14 17:28:26,670 Stage: Train 0.5 | Epoch: 27 | Iter: 82800 | Total Loss: 0.003376 | Recon Loss: 0.002927 | Commit Loss: 0.000898 | Perplexity: 963.420832
2025-09-14 17:28:35,395 Stage: Train 0.5 | Epoch: 27 | Iter: 83000 | Total Loss: 0.003395 | Recon Loss: 0.002945 | Commit Loss: 0.000900 | Perplexity: 959.559664
2025-09-14 17:28:44,092 Stage: Train 0.5 | Epoch: 27 | Iter: 83200 | Total Loss: 0.003498 | Recon Loss: 0.003036 | Commit Loss: 0.000925 | Perplexity: 953.146381
2025-09-14 17:28:52,833 Stage: Train 0.5 | Epoch: 27 | Iter: 83400 | Total Loss: 0.003418 | Recon Loss: 0.002971 | Commit Loss: 0.000895 | Perplexity: 959.649705
2025-09-14 17:29:01,516 Stage: Train 0.5 | Epoch: 27 | Iter: 83600 | Total Loss: 0.003460 | Recon Loss: 0.003006 | Commit Loss: 0.000907 | Perplexity: 961.063657
2025-09-14 17:29:10,214 Stage: Train 0.5 | Epoch: 27 | Iter: 83800 | Total Loss: 0.003415 | Recon Loss: 0.002954 | Commit Loss: 0.000921 | Perplexity: 956.783887
2025-09-14 17:29:18,900 Stage: Train 0.5 | Epoch: 27 | Iter: 84000 | Total Loss: 0.003361 | Recon Loss: 0.002912 | Commit Loss: 0.000898 | Perplexity: 959.867493
2025-09-14 17:29:27,646 Stage: Train 0.5 | Epoch: 27 | Iter: 84200 | Total Loss: 0.003469 | Recon Loss: 0.003017 | Commit Loss: 0.000905 | Perplexity: 959.499845
2025-09-14 17:29:36,369 Stage: Train 0.5 | Epoch: 27 | Iter: 84400 | Total Loss: 0.003305 | Recon Loss: 0.002855 | Commit Loss: 0.000899 | Perplexity: 961.419429
2025-09-14 17:29:45,093 Stage: Train 0.5 | Epoch: 27 | Iter: 84600 | Total Loss: 0.003375 | Recon Loss: 0.002917 | Commit Loss: 0.000917 | Perplexity: 962.377244
2025-09-14 17:29:53,792 Stage: Train 0.5 | Epoch: 27 | Iter: 84800 | Total Loss: 0.003361 | Recon Loss: 0.002903 | Commit Loss: 0.000916 | Perplexity: 960.197086
2025-09-14 17:30:02,480 Stage: Train 0.5 | Epoch: 27 | Iter: 85000 | Total Loss: 0.003477 | Recon Loss: 0.003024 | Commit Loss: 0.000907 | Perplexity: 954.787405
Trainning Epoch:  17%|█▋        | 28/165 [1:01:58<5:03:05, 132.74s/it]2025-09-14 17:30:11,244 Stage: Train 0.5 | Epoch: 28 | Iter: 85200 | Total Loss: 0.003528 | Recon Loss: 0.003074 | Commit Loss: 0.000909 | Perplexity: 953.031671
2025-09-14 17:30:20,008 Stage: Train 0.5 | Epoch: 28 | Iter: 85400 | Total Loss: 0.003459 | Recon Loss: 0.003005 | Commit Loss: 0.000908 | Perplexity: 949.263410
2025-09-14 17:30:28,775 Stage: Train 0.5 | Epoch: 28 | Iter: 85600 | Total Loss: 0.003347 | Recon Loss: 0.002892 | Commit Loss: 0.000910 | Perplexity: 960.682110
2025-09-14 17:30:37,553 Stage: Train 0.5 | Epoch: 28 | Iter: 85800 | Total Loss: 0.003327 | Recon Loss: 0.002874 | Commit Loss: 0.000906 | Perplexity: 961.988167
2025-09-14 17:30:46,315 Stage: Train 0.5 | Epoch: 28 | Iter: 86000 | Total Loss: 0.003474 | Recon Loss: 0.002998 | Commit Loss: 0.000952 | Perplexity: 949.870019
2025-09-14 17:30:55,082 Stage: Train 0.5 | Epoch: 28 | Iter: 86200 | Total Loss: 0.003658 | Recon Loss: 0.003160 | Commit Loss: 0.000996 | Perplexity: 955.253235
2025-09-14 17:31:03,855 Stage: Train 0.5 | Epoch: 28 | Iter: 86400 | Total Loss: 0.003353 | Recon Loss: 0.002907 | Commit Loss: 0.000891 | Perplexity: 959.776244
2025-09-14 17:31:12,635 Stage: Train 0.5 | Epoch: 28 | Iter: 86600 | Total Loss: 0.003330 | Recon Loss: 0.002881 | Commit Loss: 0.000898 | Perplexity: 955.128529
2025-09-14 17:31:21,388 Stage: Train 0.5 | Epoch: 28 | Iter: 86800 | Total Loss: 0.003393 | Recon Loss: 0.002930 | Commit Loss: 0.000925 | Perplexity: 960.887362
2025-09-14 17:31:30,127 Stage: Train 0.5 | Epoch: 28 | Iter: 87000 | Total Loss: 0.003302 | Recon Loss: 0.002842 | Commit Loss: 0.000921 | Perplexity: 957.936467
2025-09-14 17:31:38,869 Stage: Train 0.5 | Epoch: 28 | Iter: 87200 | Total Loss: 0.003430 | Recon Loss: 0.002971 | Commit Loss: 0.000918 | Perplexity: 957.035101
2025-09-14 17:31:47,637 Stage: Train 0.5 | Epoch: 28 | Iter: 87400 | Total Loss: 0.003383 | Recon Loss: 0.002920 | Commit Loss: 0.000928 | Perplexity: 959.503155
2025-09-14 17:31:56,375 Stage: Train 0.5 | Epoch: 28 | Iter: 87600 | Total Loss: 0.003382 | Recon Loss: 0.002933 | Commit Loss: 0.000899 | Perplexity: 959.557780
2025-09-14 17:32:05,140 Stage: Train 0.5 | Epoch: 28 | Iter: 87800 | Total Loss: 0.003404 | Recon Loss: 0.002949 | Commit Loss: 0.000911 | Perplexity: 958.703563
2025-09-14 17:32:13,867 Stage: Train 0.5 | Epoch: 28 | Iter: 88000 | Total Loss: 0.003372 | Recon Loss: 0.002913 | Commit Loss: 0.000919 | Perplexity: 957.536369
Trainning Epoch:  18%|█▊        | 29/165 [1:04:11<5:01:05, 132.83s/it]2025-09-14 17:32:22,602 Stage: Train 0.5 | Epoch: 29 | Iter: 88200 | Total Loss: 0.003393 | Recon Loss: 0.002933 | Commit Loss: 0.000920 | Perplexity: 959.917242
2025-09-14 17:32:31,352 Stage: Train 0.5 | Epoch: 29 | Iter: 88400 | Total Loss: 0.003339 | Recon Loss: 0.002888 | Commit Loss: 0.000901 | Perplexity: 956.457728
2025-09-14 17:32:40,098 Stage: Train 0.5 | Epoch: 29 | Iter: 88600 | Total Loss: 0.003367 | Recon Loss: 0.002920 | Commit Loss: 0.000894 | Perplexity: 955.627285
2025-09-14 17:32:48,869 Stage: Train 0.5 | Epoch: 29 | Iter: 88800 | Total Loss: 0.003384 | Recon Loss: 0.002932 | Commit Loss: 0.000904 | Perplexity: 962.175789
2025-09-14 17:32:57,662 Stage: Train 0.5 | Epoch: 29 | Iter: 89000 | Total Loss: 0.003358 | Recon Loss: 0.002902 | Commit Loss: 0.000913 | Perplexity: 957.977924
2025-09-14 17:33:06,388 Stage: Train 0.5 | Epoch: 29 | Iter: 89200 | Total Loss: 0.003320 | Recon Loss: 0.002860 | Commit Loss: 0.000920 | Perplexity: 960.891680
2025-09-14 17:33:15,181 Stage: Train 0.5 | Epoch: 29 | Iter: 89400 | Total Loss: 0.003474 | Recon Loss: 0.003014 | Commit Loss: 0.000920 | Perplexity: 958.296828
2025-09-14 17:33:23,963 Stage: Train 0.5 | Epoch: 29 | Iter: 89600 | Total Loss: 0.003379 | Recon Loss: 0.002924 | Commit Loss: 0.000909 | Perplexity: 954.552094
2025-09-14 17:33:32,739 Stage: Train 0.5 | Epoch: 29 | Iter: 89800 | Total Loss: 0.003384 | Recon Loss: 0.002929 | Commit Loss: 0.000910 | Perplexity: 958.837446
2025-09-14 17:33:41,479 Stage: Train 0.5 | Epoch: 29 | Iter: 90000 | Total Loss: 0.003319 | Recon Loss: 0.002866 | Commit Loss: 0.000906 | Perplexity: 953.716354
2025-09-14 17:33:50,241 Stage: Train 0.5 | Epoch: 29 | Iter: 90200 | Total Loss: 0.003432 | Recon Loss: 0.002977 | Commit Loss: 0.000909 | Perplexity: 955.641353
2025-09-14 17:33:59,011 Stage: Train 0.5 | Epoch: 29 | Iter: 90400 | Total Loss: 0.003313 | Recon Loss: 0.002846 | Commit Loss: 0.000935 | Perplexity: 959.011856
2025-09-14 17:34:07,762 Stage: Train 0.5 | Epoch: 29 | Iter: 90600 | Total Loss: 0.003404 | Recon Loss: 0.002947 | Commit Loss: 0.000913 | Perplexity: 957.616622
2025-09-14 17:34:16,540 Stage: Train 0.5 | Epoch: 29 | Iter: 90800 | Total Loss: 0.003398 | Recon Loss: 0.002940 | Commit Loss: 0.000915 | Perplexity: 959.704879
2025-09-14 17:34:25,349 Stage: Train 0.5 | Epoch: 29 | Iter: 91000 | Total Loss: 0.003311 | Recon Loss: 0.002858 | Commit Loss: 0.000905 | Perplexity: 950.747257
Trainning Epoch:  18%|█▊        | 30/165 [1:06:24<4:59:05, 132.93s/it]2025-09-14 17:34:34,116 Stage: Train 0.5 | Epoch: 30 | Iter: 91200 | Total Loss: 0.003278 | Recon Loss: 0.002820 | Commit Loss: 0.000917 | Perplexity: 953.207157
2025-09-14 17:34:42,846 Stage: Train 0.5 | Epoch: 30 | Iter: 91400 | Total Loss: 0.003325 | Recon Loss: 0.002867 | Commit Loss: 0.000915 | Perplexity: 956.415749
2025-09-14 17:34:51,588 Stage: Train 0.5 | Epoch: 30 | Iter: 91600 | Total Loss: 0.003321 | Recon Loss: 0.002862 | Commit Loss: 0.000919 | Perplexity: 954.095919
2025-09-14 17:35:00,333 Stage: Train 0.5 | Epoch: 30 | Iter: 91800 | Total Loss: 0.003334 | Recon Loss: 0.002877 | Commit Loss: 0.000914 | Perplexity: 956.137672
2025-09-14 17:35:09,085 Stage: Train 0.5 | Epoch: 30 | Iter: 92000 | Total Loss: 0.003345 | Recon Loss: 0.002886 | Commit Loss: 0.000918 | Perplexity: 959.286217
2025-09-14 17:35:17,825 Stage: Train 0.5 | Epoch: 30 | Iter: 92200 | Total Loss: 0.003305 | Recon Loss: 0.002851 | Commit Loss: 0.000907 | Perplexity: 960.698940
2025-09-14 17:35:26,554 Stage: Train 0.5 | Epoch: 30 | Iter: 92400 | Total Loss: 0.003506 | Recon Loss: 0.003042 | Commit Loss: 0.000928 | Perplexity: 958.371770
2025-09-14 17:35:35,294 Stage: Train 0.5 | Epoch: 30 | Iter: 92600 | Total Loss: 0.003324 | Recon Loss: 0.002865 | Commit Loss: 0.000917 | Perplexity: 956.637792
2025-09-14 17:35:44,045 Stage: Train 0.5 | Epoch: 30 | Iter: 92800 | Total Loss: 0.003342 | Recon Loss: 0.002884 | Commit Loss: 0.000915 | Perplexity: 958.587698
2025-09-14 17:35:52,806 Stage: Train 0.5 | Epoch: 30 | Iter: 93000 | Total Loss: 0.003327 | Recon Loss: 0.002869 | Commit Loss: 0.000915 | Perplexity: 964.570452
2025-09-14 17:36:01,558 Stage: Train 0.5 | Epoch: 30 | Iter: 93200 | Total Loss: 0.003320 | Recon Loss: 0.002865 | Commit Loss: 0.000911 | Perplexity: 958.030525
2025-09-14 17:36:10,305 Stage: Train 0.5 | Epoch: 30 | Iter: 93400 | Total Loss: 0.003369 | Recon Loss: 0.002912 | Commit Loss: 0.000914 | Perplexity: 954.344002
2025-09-14 17:36:19,040 Stage: Train 0.5 | Epoch: 30 | Iter: 93600 | Total Loss: 0.003348 | Recon Loss: 0.002893 | Commit Loss: 0.000911 | Perplexity: 953.970784
2025-09-14 17:36:27,772 Stage: Train 0.5 | Epoch: 30 | Iter: 93800 | Total Loss: 0.003339 | Recon Loss: 0.002884 | Commit Loss: 0.000909 | Perplexity: 959.876254
2025-09-14 17:36:36,528 Stage: Train 0.5 | Epoch: 30 | Iter: 94000 | Total Loss: 0.003428 | Recon Loss: 0.002965 | Commit Loss: 0.000925 | Perplexity: 954.887188
Trainning Epoch:  19%|█▉        | 31/165 [1:08:37<4:56:49, 132.91s/it]2025-09-14 17:36:45,295 Stage: Train 0.5 | Epoch: 31 | Iter: 94200 | Total Loss: 0.003453 | Recon Loss: 0.003002 | Commit Loss: 0.000901 | Perplexity: 949.419973
2025-09-14 17:36:53,991 Stage: Train 0.5 | Epoch: 31 | Iter: 94400 | Total Loss: 0.003313 | Recon Loss: 0.002858 | Commit Loss: 0.000911 | Perplexity: 950.723219
2025-09-14 17:37:02,780 Stage: Train 0.5 | Epoch: 31 | Iter: 94600 | Total Loss: 0.003286 | Recon Loss: 0.002834 | Commit Loss: 0.000903 | Perplexity: 953.293308
2025-09-14 17:37:11,549 Stage: Train 0.5 | Epoch: 31 | Iter: 94800 | Total Loss: 0.003323 | Recon Loss: 0.002867 | Commit Loss: 0.000912 | Perplexity: 956.331303
2025-09-14 17:37:20,271 Stage: Train 0.5 | Epoch: 31 | Iter: 95000 | Total Loss: 0.003299 | Recon Loss: 0.002835 | Commit Loss: 0.000927 | Perplexity: 956.051064
2025-09-14 17:37:29,012 Stage: Train 0.5 | Epoch: 31 | Iter: 95200 | Total Loss: 0.003353 | Recon Loss: 0.002893 | Commit Loss: 0.000919 | Perplexity: 962.721379
2025-09-14 17:37:37,732 Stage: Train 0.5 | Epoch: 31 | Iter: 95400 | Total Loss: 0.003302 | Recon Loss: 0.002854 | Commit Loss: 0.000897 | Perplexity: 952.037274
2025-09-14 17:37:46,468 Stage: Train 0.5 | Epoch: 31 | Iter: 95600 | Total Loss: 0.003273 | Recon Loss: 0.002816 | Commit Loss: 0.000913 | Perplexity: 954.971163
2025-09-14 17:37:55,189 Stage: Train 0.5 | Epoch: 31 | Iter: 95800 | Total Loss: 0.003325 | Recon Loss: 0.002870 | Commit Loss: 0.000909 | Perplexity: 955.746381
2025-09-14 17:38:03,928 Stage: Train 0.5 | Epoch: 31 | Iter: 96000 | Total Loss: 0.003420 | Recon Loss: 0.002926 | Commit Loss: 0.000988 | Perplexity: 960.724717
2025-09-14 17:38:12,671 Stage: Train 0.5 | Epoch: 31 | Iter: 96200 | Total Loss: 0.003276 | Recon Loss: 0.002817 | Commit Loss: 0.000920 | Perplexity: 955.192097
2025-09-14 17:38:21,518 Stage: Train 0.5 | Epoch: 31 | Iter: 96400 | Total Loss: 0.003320 | Recon Loss: 0.002862 | Commit Loss: 0.000916 | Perplexity: 954.489216
2025-09-14 17:38:30,274 Stage: Train 0.5 | Epoch: 31 | Iter: 96600 | Total Loss: 0.003269 | Recon Loss: 0.002810 | Commit Loss: 0.000919 | Perplexity: 956.949990
2025-09-14 17:38:38,980 Stage: Train 0.5 | Epoch: 31 | Iter: 96800 | Total Loss: 0.003310 | Recon Loss: 0.002846 | Commit Loss: 0.000929 | Perplexity: 963.204889
2025-09-14 17:38:47,688 Stage: Train 0.5 | Epoch: 31 | Iter: 97000 | Total Loss: 0.003311 | Recon Loss: 0.002847 | Commit Loss: 0.000928 | Perplexity: 956.159870
2025-09-14 17:38:56,388 Stage: Train 0.5 | Epoch: 31 | Iter: 97200 | Total Loss: 0.003422 | Recon Loss: 0.002961 | Commit Loss: 0.000921 | Perplexity: 954.978620
Trainning Epoch:  19%|█▉        | 32/165 [1:10:50<4:54:30, 132.86s/it]2025-09-14 17:39:05,102 Stage: Train 0.5 | Epoch: 32 | Iter: 97400 | Total Loss: 0.003223 | Recon Loss: 0.002764 | Commit Loss: 0.000918 | Perplexity: 955.179719
2025-09-14 17:39:13,790 Stage: Train 0.5 | Epoch: 32 | Iter: 97600 | Total Loss: 0.003358 | Recon Loss: 0.002897 | Commit Loss: 0.000922 | Perplexity: 954.896197
2025-09-14 17:39:22,527 Stage: Train 0.5 | Epoch: 32 | Iter: 97800 | Total Loss: 0.003245 | Recon Loss: 0.002790 | Commit Loss: 0.000910 | Perplexity: 954.899857
2025-09-14 17:39:31,383 Stage: Train 0.5 | Epoch: 32 | Iter: 98000 | Total Loss: 0.003359 | Recon Loss: 0.002899 | Commit Loss: 0.000919 | Perplexity: 953.687602
2025-09-14 17:39:40,164 Stage: Train 0.5 | Epoch: 32 | Iter: 98200 | Total Loss: 0.003305 | Recon Loss: 0.002845 | Commit Loss: 0.000921 | Perplexity: 958.347134
2025-09-14 17:39:48,880 Stage: Train 0.5 | Epoch: 32 | Iter: 98400 | Total Loss: 0.003310 | Recon Loss: 0.002846 | Commit Loss: 0.000929 | Perplexity: 955.509661
2025-09-14 17:39:57,641 Stage: Train 0.5 | Epoch: 32 | Iter: 98600 | Total Loss: 0.003322 | Recon Loss: 0.002862 | Commit Loss: 0.000919 | Perplexity: 960.761946
2025-09-14 17:40:06,364 Stage: Train 0.5 | Epoch: 32 | Iter: 98800 | Total Loss: 0.003321 | Recon Loss: 0.002859 | Commit Loss: 0.000923 | Perplexity: 959.785161
2025-09-14 17:40:15,069 Stage: Train 0.5 | Epoch: 32 | Iter: 99000 | Total Loss: 0.003244 | Recon Loss: 0.002791 | Commit Loss: 0.000906 | Perplexity: 955.554194
2025-09-14 17:40:23,779 Stage: Train 0.5 | Epoch: 32 | Iter: 99200 | Total Loss: 0.003318 | Recon Loss: 0.002848 | Commit Loss: 0.000941 | Perplexity: 956.388325
2025-09-14 17:40:32,532 Stage: Train 0.5 | Epoch: 32 | Iter: 99400 | Total Loss: 0.003269 | Recon Loss: 0.002822 | Commit Loss: 0.000896 | Perplexity: 948.986405
2025-09-14 17:40:41,246 Stage: Train 0.5 | Epoch: 32 | Iter: 99600 | Total Loss: 0.003272 | Recon Loss: 0.002809 | Commit Loss: 0.000927 | Perplexity: 957.163396
2025-09-14 17:40:49,973 Stage: Train 0.5 | Epoch: 32 | Iter: 99800 | Total Loss: 0.003405 | Recon Loss: 0.002935 | Commit Loss: 0.000940 | Perplexity: 958.599011
2025-09-14 17:40:58,694 Stage: Train 0.5 | Epoch: 32 | Iter: 100000 | Total Loss: 0.003275 | Recon Loss: 0.002821 | Commit Loss: 0.000909 | Perplexity: 955.892740
2025-09-14 17:40:58,695 Saving model at iteration 100000
2025-09-14 17:40:58,852 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_33_step_100000
2025-09-14 17:40:59,092 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_33_step_100000/pytorch_model.bin
2025-09-14 17:40:59,465 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_33_step_100000/optimizer.bin
2025-09-14 17:40:59,465 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_33_step_100000/scheduler.bin
2025-09-14 17:40:59,466 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_33_step_100000/random_states_0.pkl
2025-09-14 17:41:08,188 Stage: Train 0.5 | Epoch: 32 | Iter: 100200 | Total Loss: 0.003236 | Recon Loss: 0.002781 | Commit Loss: 0.000911 | Perplexity: 959.100905
Trainning Epoch:  20%|██        | 33/165 [1:13:04<4:52:41, 133.04s/it]2025-09-14 17:41:16,936 Stage: Train 0.5 | Epoch: 33 | Iter: 100400 | Total Loss: 0.003306 | Recon Loss: 0.002851 | Commit Loss: 0.000910 | Perplexity: 953.189391
2025-09-14 17:41:25,634 Stage: Train 0.5 | Epoch: 33 | Iter: 100600 | Total Loss: 0.003255 | Recon Loss: 0.002800 | Commit Loss: 0.000909 | Perplexity: 954.980706
2025-09-14 17:41:34,352 Stage: Train 0.5 | Epoch: 33 | Iter: 100800 | Total Loss: 0.003297 | Recon Loss: 0.002840 | Commit Loss: 0.000913 | Perplexity: 953.511781
2025-09-14 17:41:43,063 Stage: Train 0.5 | Epoch: 33 | Iter: 101000 | Total Loss: 0.003247 | Recon Loss: 0.002787 | Commit Loss: 0.000920 | Perplexity: 957.811059
2025-09-14 17:41:51,791 Stage: Train 0.5 | Epoch: 33 | Iter: 101200 | Total Loss: 0.003221 | Recon Loss: 0.002764 | Commit Loss: 0.000914 | Perplexity: 955.720056
2025-09-14 17:42:00,486 Stage: Train 0.5 | Epoch: 33 | Iter: 101400 | Total Loss: 0.003318 | Recon Loss: 0.002859 | Commit Loss: 0.000917 | Perplexity: 958.582667
2025-09-14 17:42:09,228 Stage: Train 0.5 | Epoch: 33 | Iter: 101600 | Total Loss: 0.003218 | Recon Loss: 0.002759 | Commit Loss: 0.000918 | Perplexity: 956.734433
2025-09-14 17:42:17,956 Stage: Train 0.5 | Epoch: 33 | Iter: 101800 | Total Loss: 0.003347 | Recon Loss: 0.002889 | Commit Loss: 0.000917 | Perplexity: 956.199633
2025-09-14 17:42:26,702 Stage: Train 0.5 | Epoch: 33 | Iter: 102000 | Total Loss: 0.003303 | Recon Loss: 0.002846 | Commit Loss: 0.000913 | Perplexity: 958.092917
2025-09-14 17:42:35,537 Stage: Train 0.5 | Epoch: 33 | Iter: 102200 | Total Loss: 0.003246 | Recon Loss: 0.002784 | Commit Loss: 0.000923 | Perplexity: 961.473022
2025-09-14 17:42:44,280 Stage: Train 0.5 | Epoch: 33 | Iter: 102400 | Total Loss: 0.003283 | Recon Loss: 0.002820 | Commit Loss: 0.000927 | Perplexity: 954.966505
2025-09-14 17:42:53,002 Stage: Train 0.5 | Epoch: 33 | Iter: 102600 | Total Loss: 0.003341 | Recon Loss: 0.002872 | Commit Loss: 0.000939 | Perplexity: 953.156170
2025-09-14 17:43:01,719 Stage: Train 0.5 | Epoch: 33 | Iter: 102800 | Total Loss: 0.003324 | Recon Loss: 0.002860 | Commit Loss: 0.000927 | Perplexity: 952.673011
2025-09-14 17:43:10,476 Stage: Train 0.5 | Epoch: 33 | Iter: 103000 | Total Loss: 0.003251 | Recon Loss: 0.002793 | Commit Loss: 0.000916 | Perplexity: 949.209195
2025-09-14 17:43:19,253 Stage: Train 0.5 | Epoch: 33 | Iter: 103200 | Total Loss: 0.003239 | Recon Loss: 0.002776 | Commit Loss: 0.000924 | Perplexity: 955.608725
Trainning Epoch:  21%|██        | 34/165 [1:15:16<4:50:15, 132.95s/it]2025-09-14 17:43:28,000 Stage: Train 0.5 | Epoch: 34 | Iter: 103400 | Total Loss: 0.003228 | Recon Loss: 0.002770 | Commit Loss: 0.000916 | Perplexity: 953.913740
2025-09-14 17:43:36,742 Stage: Train 0.5 | Epoch: 34 | Iter: 103600 | Total Loss: 0.003211 | Recon Loss: 0.002748 | Commit Loss: 0.000925 | Perplexity: 958.084022
2025-09-14 17:43:45,541 Stage: Train 0.5 | Epoch: 34 | Iter: 103800 | Total Loss: 0.003342 | Recon Loss: 0.002881 | Commit Loss: 0.000922 | Perplexity: 956.079897
2025-09-14 17:43:54,245 Stage: Train 0.5 | Epoch: 34 | Iter: 104000 | Total Loss: 0.003224 | Recon Loss: 0.002765 | Commit Loss: 0.000917 | Perplexity: 959.326016
2025-09-14 17:44:02,997 Stage: Train 0.5 | Epoch: 34 | Iter: 104200 | Total Loss: 0.003258 | Recon Loss: 0.002801 | Commit Loss: 0.000916 | Perplexity: 950.428920
2025-09-14 17:44:11,714 Stage: Train 0.5 | Epoch: 34 | Iter: 104400 | Total Loss: 0.003248 | Recon Loss: 0.002779 | Commit Loss: 0.000939 | Perplexity: 963.133495
2025-09-14 17:44:20,406 Stage: Train 0.5 | Epoch: 34 | Iter: 104600 | Total Loss: 0.003279 | Recon Loss: 0.002816 | Commit Loss: 0.000925 | Perplexity: 956.554915
2025-09-14 17:44:29,154 Stage: Train 0.5 | Epoch: 34 | Iter: 104800 | Total Loss: 0.003340 | Recon Loss: 0.002880 | Commit Loss: 0.000920 | Perplexity: 958.016901
2025-09-14 17:44:37,897 Stage: Train 0.5 | Epoch: 34 | Iter: 105000 | Total Loss: 0.003230 | Recon Loss: 0.002770 | Commit Loss: 0.000918 | Perplexity: 956.342736
2025-09-14 17:44:46,574 Stage: Train 0.5 | Epoch: 34 | Iter: 105200 | Total Loss: 0.003253 | Recon Loss: 0.002796 | Commit Loss: 0.000914 | Perplexity: 949.007876
2025-09-14 17:44:55,239 Stage: Train 0.5 | Epoch: 34 | Iter: 105400 | Total Loss: 0.003259 | Recon Loss: 0.002803 | Commit Loss: 0.000913 | Perplexity: 952.308814
2025-09-14 17:45:03,905 Stage: Train 0.5 | Epoch: 34 | Iter: 105600 | Total Loss: 0.003276 | Recon Loss: 0.002811 | Commit Loss: 0.000930 | Perplexity: 958.872870
2025-09-14 17:45:12,594 Stage: Train 0.5 | Epoch: 34 | Iter: 105800 | Total Loss: 0.003191 | Recon Loss: 0.002726 | Commit Loss: 0.000929 | Perplexity: 959.952574
2025-09-14 17:45:21,304 Stage: Train 0.5 | Epoch: 34 | Iter: 106000 | Total Loss: 0.003305 | Recon Loss: 0.002845 | Commit Loss: 0.000920 | Perplexity: 953.337496
2025-09-14 17:45:30,048 Stage: Train 0.5 | Epoch: 34 | Iter: 106200 | Total Loss: 0.003306 | Recon Loss: 0.002835 | Commit Loss: 0.000942 | Perplexity: 958.679462
Trainning Epoch:  21%|██        | 35/165 [1:17:29<4:47:44, 132.80s/it]2025-09-14 17:45:38,804 Stage: Train 0.5 | Epoch: 35 | Iter: 106400 | Total Loss: 0.003253 | Recon Loss: 0.002790 | Commit Loss: 0.000928 | Perplexity: 952.219668
2025-09-14 17:45:47,553 Stage: Train 0.5 | Epoch: 35 | Iter: 106600 | Total Loss: 0.003250 | Recon Loss: 0.002790 | Commit Loss: 0.000918 | Perplexity: 956.682332
2025-09-14 17:45:56,278 Stage: Train 0.5 | Epoch: 35 | Iter: 106800 | Total Loss: 0.003269 | Recon Loss: 0.002811 | Commit Loss: 0.000916 | Perplexity: 958.006636
2025-09-14 17:46:05,008 Stage: Train 0.5 | Epoch: 35 | Iter: 107000 | Total Loss: 0.003262 | Recon Loss: 0.002795 | Commit Loss: 0.000935 | Perplexity: 959.683372
2025-09-14 17:46:13,723 Stage: Train 0.5 | Epoch: 35 | Iter: 107200 | Total Loss: 0.003246 | Recon Loss: 0.002784 | Commit Loss: 0.000924 | Perplexity: 955.425727
2025-09-14 17:46:22,447 Stage: Train 0.5 | Epoch: 35 | Iter: 107400 | Total Loss: 0.003191 | Recon Loss: 0.002735 | Commit Loss: 0.000912 | Perplexity: 956.139107
2025-09-14 17:46:31,181 Stage: Train 0.5 | Epoch: 35 | Iter: 107600 | Total Loss: 0.003204 | Recon Loss: 0.002749 | Commit Loss: 0.000908 | Perplexity: 952.379825
2025-09-14 17:46:39,895 Stage: Train 0.5 | Epoch: 35 | Iter: 107800 | Total Loss: 0.003199 | Recon Loss: 0.002737 | Commit Loss: 0.000924 | Perplexity: 955.984038
2025-09-14 17:46:48,632 Stage: Train 0.5 | Epoch: 35 | Iter: 108000 | Total Loss: 0.003223 | Recon Loss: 0.002757 | Commit Loss: 0.000930 | Perplexity: 959.785251
2025-09-14 17:46:57,374 Stage: Train 0.5 | Epoch: 35 | Iter: 108200 | Total Loss: 0.003215 | Recon Loss: 0.002758 | Commit Loss: 0.000915 | Perplexity: 953.902379
2025-09-14 17:47:06,140 Stage: Train 0.5 | Epoch: 35 | Iter: 108400 | Total Loss: 0.003186 | Recon Loss: 0.002721 | Commit Loss: 0.000931 | Perplexity: 957.318461
2025-09-14 17:47:14,875 Stage: Train 0.5 | Epoch: 35 | Iter: 108600 | Total Loss: 0.003310 | Recon Loss: 0.002842 | Commit Loss: 0.000937 | Perplexity: 956.726260
2025-09-14 17:47:23,598 Stage: Train 0.5 | Epoch: 35 | Iter: 108800 | Total Loss: 0.003256 | Recon Loss: 0.002789 | Commit Loss: 0.000933 | Perplexity: 957.436740
2025-09-14 17:47:32,328 Stage: Train 0.5 | Epoch: 35 | Iter: 109000 | Total Loss: 0.003224 | Recon Loss: 0.002752 | Commit Loss: 0.000944 | Perplexity: 959.461591
2025-09-14 17:47:41,082 Stage: Train 0.5 | Epoch: 35 | Iter: 109200 | Total Loss: 0.003220 | Recon Loss: 0.002759 | Commit Loss: 0.000922 | Perplexity: 957.341691
Trainning Epoch:  22%|██▏       | 36/165 [1:19:41<4:45:27, 132.77s/it]2025-09-14 17:47:49,833 Stage: Train 0.5 | Epoch: 36 | Iter: 109400 | Total Loss: 0.003320 | Recon Loss: 0.002859 | Commit Loss: 0.000924 | Perplexity: 951.429416
2025-09-14 17:47:58,555 Stage: Train 0.5 | Epoch: 36 | Iter: 109600 | Total Loss: 0.003183 | Recon Loss: 0.002722 | Commit Loss: 0.000922 | Perplexity: 951.321822
2025-09-14 17:48:07,313 Stage: Train 0.5 | Epoch: 36 | Iter: 109800 | Total Loss: 0.003211 | Recon Loss: 0.002750 | Commit Loss: 0.000923 | Perplexity: 955.008488
2025-09-14 17:48:16,030 Stage: Train 0.5 | Epoch: 36 | Iter: 110000 | Total Loss: 0.003216 | Recon Loss: 0.002757 | Commit Loss: 0.000919 | Perplexity: 957.264231
2025-09-14 17:48:24,732 Stage: Train 0.5 | Epoch: 36 | Iter: 110200 | Total Loss: 0.003183 | Recon Loss: 0.002720 | Commit Loss: 0.000927 | Perplexity: 958.932872
2025-09-14 17:48:33,488 Stage: Train 0.5 | Epoch: 36 | Iter: 110400 | Total Loss: 0.003204 | Recon Loss: 0.002740 | Commit Loss: 0.000929 | Perplexity: 958.322554
2025-09-14 17:48:42,216 Stage: Train 0.5 | Epoch: 36 | Iter: 110600 | Total Loss: 0.003239 | Recon Loss: 0.002780 | Commit Loss: 0.000918 | Perplexity: 951.303700
2025-09-14 17:48:50,968 Stage: Train 0.5 | Epoch: 36 | Iter: 110800 | Total Loss: 0.003208 | Recon Loss: 0.002746 | Commit Loss: 0.000923 | Perplexity: 954.742829
2025-09-14 17:48:59,682 Stage: Train 0.5 | Epoch: 36 | Iter: 111000 | Total Loss: 0.003225 | Recon Loss: 0.002757 | Commit Loss: 0.000936 | Perplexity: 954.644743
2025-09-14 17:49:08,404 Stage: Train 0.5 | Epoch: 36 | Iter: 111200 | Total Loss: 0.003275 | Recon Loss: 0.002805 | Commit Loss: 0.000940 | Perplexity: 960.649794
2025-09-14 17:49:17,101 Stage: Train 0.5 | Epoch: 36 | Iter: 111400 | Total Loss: 0.003239 | Recon Loss: 0.002774 | Commit Loss: 0.000929 | Perplexity: 958.016446
2025-09-14 17:49:25,814 Stage: Train 0.5 | Epoch: 36 | Iter: 111600 | Total Loss: 0.003241 | Recon Loss: 0.002776 | Commit Loss: 0.000929 | Perplexity: 954.367195
2025-09-14 17:49:34,521 Stage: Train 0.5 | Epoch: 36 | Iter: 111800 | Total Loss: 0.003213 | Recon Loss: 0.002746 | Commit Loss: 0.000935 | Perplexity: 960.688254
2025-09-14 17:49:43,293 Stage: Train 0.5 | Epoch: 36 | Iter: 112000 | Total Loss: 0.003186 | Recon Loss: 0.002726 | Commit Loss: 0.000920 | Perplexity: 954.951604
2025-09-14 17:49:51,989 Stage: Train 0.5 | Epoch: 36 | Iter: 112200 | Total Loss: 0.003215 | Recon Loss: 0.002748 | Commit Loss: 0.000935 | Perplexity: 956.894265
2025-09-14 17:50:00,709 Stage: Train 0.5 | Epoch: 36 | Iter: 112400 | Total Loss: 0.003222 | Recon Loss: 0.002759 | Commit Loss: 0.000926 | Perplexity: 954.497075
Trainning Epoch:  22%|██▏       | 37/165 [1:21:54<4:43:05, 132.70s/it]2025-09-14 17:50:09,397 Stage: Train 0.5 | Epoch: 37 | Iter: 112600 | Total Loss: 0.003239 | Recon Loss: 0.002784 | Commit Loss: 0.000911 | Perplexity: 954.261078
2025-09-14 17:50:18,104 Stage: Train 0.5 | Epoch: 37 | Iter: 112800 | Total Loss: 0.003239 | Recon Loss: 0.002776 | Commit Loss: 0.000925 | Perplexity: 952.419518
2025-09-14 17:50:26,813 Stage: Train 0.5 | Epoch: 37 | Iter: 113000 | Total Loss: 0.003218 | Recon Loss: 0.002758 | Commit Loss: 0.000921 | Perplexity: 954.147477
2025-09-14 17:50:35,530 Stage: Train 0.5 | Epoch: 37 | Iter: 113200 | Total Loss: 0.003173 | Recon Loss: 0.002712 | Commit Loss: 0.000923 | Perplexity: 959.266433
2025-09-14 17:50:44,285 Stage: Train 0.5 | Epoch: 37 | Iter: 113400 | Total Loss: 0.003162 | Recon Loss: 0.002700 | Commit Loss: 0.000923 | Perplexity: 951.496107
2025-09-14 17:50:53,051 Stage: Train 0.5 | Epoch: 37 | Iter: 113600 | Total Loss: 0.003196 | Recon Loss: 0.002735 | Commit Loss: 0.000921 | Perplexity: 956.427155
2025-09-14 17:51:01,746 Stage: Train 0.5 | Epoch: 37 | Iter: 113800 | Total Loss: 0.003205 | Recon Loss: 0.002742 | Commit Loss: 0.000926 | Perplexity: 954.525272
2025-09-14 17:51:10,462 Stage: Train 0.5 | Epoch: 37 | Iter: 114000 | Total Loss: 0.003196 | Recon Loss: 0.002742 | Commit Loss: 0.000908 | Perplexity: 949.705882
2025-09-14 17:51:19,180 Stage: Train 0.5 | Epoch: 37 | Iter: 114200 | Total Loss: 0.003153 | Recon Loss: 0.002688 | Commit Loss: 0.000930 | Perplexity: 958.695585
2025-09-14 17:51:27,941 Stage: Train 0.5 | Epoch: 37 | Iter: 114400 | Total Loss: 0.003150 | Recon Loss: 0.002688 | Commit Loss: 0.000925 | Perplexity: 955.250424
2025-09-14 17:51:36,659 Stage: Train 0.5 | Epoch: 37 | Iter: 114600 | Total Loss: 0.003291 | Recon Loss: 0.002829 | Commit Loss: 0.000925 | Perplexity: 956.840906
2025-09-14 17:51:45,414 Stage: Train 0.5 | Epoch: 37 | Iter: 114800 | Total Loss: 0.003224 | Recon Loss: 0.002762 | Commit Loss: 0.000923 | Perplexity: 951.573859
2025-09-14 17:51:54,135 Stage: Train 0.5 | Epoch: 37 | Iter: 115000 | Total Loss: 0.003161 | Recon Loss: 0.002688 | Commit Loss: 0.000945 | Perplexity: 951.498500
2025-09-14 17:52:02,848 Stage: Train 0.5 | Epoch: 37 | Iter: 115200 | Total Loss: 0.003283 | Recon Loss: 0.002811 | Commit Loss: 0.000944 | Perplexity: 952.974452
2025-09-14 17:52:11,581 Stage: Train 0.5 | Epoch: 37 | Iter: 115400 | Total Loss: 0.003128 | Recon Loss: 0.002668 | Commit Loss: 0.000919 | Perplexity: 955.009323
Trainning Epoch:  23%|██▎       | 38/165 [1:24:07<4:40:46, 132.65s/it]2025-09-14 17:52:20,330 Stage: Train 0.5 | Epoch: 38 | Iter: 115600 | Total Loss: 0.003174 | Recon Loss: 0.002709 | Commit Loss: 0.000930 | Perplexity: 954.417113
2025-09-14 17:52:29,058 Stage: Train 0.5 | Epoch: 38 | Iter: 115800 | Total Loss: 0.003345 | Recon Loss: 0.002888 | Commit Loss: 0.000914 | Perplexity: 953.949804
2025-09-14 17:52:37,767 Stage: Train 0.5 | Epoch: 38 | Iter: 116000 | Total Loss: 0.003117 | Recon Loss: 0.002652 | Commit Loss: 0.000931 | Perplexity: 961.523690
2025-09-14 17:52:46,472 Stage: Train 0.5 | Epoch: 38 | Iter: 116200 | Total Loss: 0.003278 | Recon Loss: 0.002819 | Commit Loss: 0.000919 | Perplexity: 951.918140
2025-09-14 17:52:55,202 Stage: Train 0.5 | Epoch: 38 | Iter: 116400 | Total Loss: 0.003182 | Recon Loss: 0.002717 | Commit Loss: 0.000930 | Perplexity: 954.496138
2025-09-14 17:53:03,956 Stage: Train 0.5 | Epoch: 38 | Iter: 116600 | Total Loss: 0.003266 | Recon Loss: 0.002805 | Commit Loss: 0.000921 | Perplexity: 953.990242
2025-09-14 17:53:12,707 Stage: Train 0.5 | Epoch: 38 | Iter: 116800 | Total Loss: 0.003127 | Recon Loss: 0.002660 | Commit Loss: 0.000934 | Perplexity: 954.043004
2025-09-14 17:53:21,404 Stage: Train 0.5 | Epoch: 38 | Iter: 117000 | Total Loss: 0.003242 | Recon Loss: 0.002772 | Commit Loss: 0.000940 | Perplexity: 955.030850
2025-09-14 17:53:30,105 Stage: Train 0.5 | Epoch: 38 | Iter: 117200 | Total Loss: 0.003296 | Recon Loss: 0.002798 | Commit Loss: 0.000997 | Perplexity: 952.771186
2025-09-14 17:53:38,832 Stage: Train 0.5 | Epoch: 38 | Iter: 117400 | Total Loss: 0.003173 | Recon Loss: 0.002712 | Commit Loss: 0.000921 | Perplexity: 957.367176
2025-09-14 17:53:47,557 Stage: Train 0.5 | Epoch: 38 | Iter: 117600 | Total Loss: 0.003152 | Recon Loss: 0.002687 | Commit Loss: 0.000930 | Perplexity: 958.174732
2025-09-14 17:53:56,332 Stage: Train 0.5 | Epoch: 38 | Iter: 117800 | Total Loss: 0.003145 | Recon Loss: 0.002680 | Commit Loss: 0.000929 | Perplexity: 958.036905
2025-09-14 17:54:05,048 Stage: Train 0.5 | Epoch: 38 | Iter: 118000 | Total Loss: 0.003151 | Recon Loss: 0.002690 | Commit Loss: 0.000922 | Perplexity: 956.451749
2025-09-14 17:54:13,752 Stage: Train 0.5 | Epoch: 38 | Iter: 118200 | Total Loss: 0.003196 | Recon Loss: 0.002734 | Commit Loss: 0.000925 | Perplexity: 954.610389
2025-09-14 17:54:22,481 Stage: Train 0.5 | Epoch: 38 | Iter: 118400 | Total Loss: 0.003165 | Recon Loss: 0.002704 | Commit Loss: 0.000922 | Perplexity: 954.041754
Trainning Epoch:  24%|██▎       | 39/165 [1:26:19<4:38:29, 132.61s/it]2025-09-14 17:54:31,182 Stage: Train 0.5 | Epoch: 39 | Iter: 118600 | Total Loss: 0.003142 | Recon Loss: 0.002683 | Commit Loss: 0.000917 | Perplexity: 948.071713
2025-09-14 17:54:39,894 Stage: Train 0.5 | Epoch: 39 | Iter: 118800 | Total Loss: 0.003248 | Recon Loss: 0.002781 | Commit Loss: 0.000934 | Perplexity: 956.850575
2025-09-14 17:54:48,622 Stage: Train 0.5 | Epoch: 39 | Iter: 119000 | Total Loss: 0.003141 | Recon Loss: 0.002679 | Commit Loss: 0.000924 | Perplexity: 950.986317
2025-09-14 17:54:57,332 Stage: Train 0.5 | Epoch: 39 | Iter: 119200 | Total Loss: 0.003163 | Recon Loss: 0.002692 | Commit Loss: 0.000942 | Perplexity: 957.405338
2025-09-14 17:55:06,064 Stage: Train 0.5 | Epoch: 39 | Iter: 119400 | Total Loss: 0.003200 | Recon Loss: 0.002736 | Commit Loss: 0.000928 | Perplexity: 951.313863
2025-09-14 17:55:14,753 Stage: Train 0.5 | Epoch: 39 | Iter: 119600 | Total Loss: 0.003119 | Recon Loss: 0.002654 | Commit Loss: 0.000930 | Perplexity: 956.236225
2025-09-14 17:55:23,444 Stage: Train 0.5 | Epoch: 39 | Iter: 119800 | Total Loss: 0.003143 | Recon Loss: 0.002681 | Commit Loss: 0.000923 | Perplexity: 951.997774
2025-09-14 17:55:32,164 Stage: Train 0.5 | Epoch: 39 | Iter: 120000 | Total Loss: 0.003202 | Recon Loss: 0.002737 | Commit Loss: 0.000930 | Perplexity: 950.013059
2025-09-14 17:55:32,164 Saving model at iteration 120000
2025-09-14 17:55:32,318 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_40_step_120000
2025-09-14 17:55:32,547 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_40_step_120000/pytorch_model.bin
2025-09-14 17:55:32,910 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_40_step_120000/optimizer.bin
2025-09-14 17:55:32,910 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_40_step_120000/scheduler.bin
2025-09-14 17:55:32,911 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_40_step_120000/random_states_0.pkl
2025-09-14 17:55:41,870 Stage: Train 0.5 | Epoch: 39 | Iter: 120200 | Total Loss: 0.003133 | Recon Loss: 0.002677 | Commit Loss: 0.000913 | Perplexity: 951.718183
2025-09-14 17:55:50,604 Stage: Train 0.5 | Epoch: 39 | Iter: 120400 | Total Loss: 0.003227 | Recon Loss: 0.002766 | Commit Loss: 0.000923 | Perplexity: 955.707603
2025-09-14 17:55:59,361 Stage: Train 0.5 | Epoch: 39 | Iter: 120600 | Total Loss: 0.003126 | Recon Loss: 0.002668 | Commit Loss: 0.000916 | Perplexity: 950.139646
2025-09-14 17:56:08,108 Stage: Train 0.5 | Epoch: 39 | Iter: 120800 | Total Loss: 0.003172 | Recon Loss: 0.002708 | Commit Loss: 0.000928 | Perplexity: 957.215964
2025-09-14 17:56:16,834 Stage: Train 0.5 | Epoch: 39 | Iter: 121000 | Total Loss: 0.003162 | Recon Loss: 0.002691 | Commit Loss: 0.000942 | Perplexity: 957.276503
2025-09-14 17:56:25,559 Stage: Train 0.5 | Epoch: 39 | Iter: 121200 | Total Loss: 0.003148 | Recon Loss: 0.002684 | Commit Loss: 0.000927 | Perplexity: 956.419669
2025-09-14 17:56:34,298 Stage: Train 0.5 | Epoch: 39 | Iter: 121400 | Total Loss: 0.003168 | Recon Loss: 0.002703 | Commit Loss: 0.000930 | Perplexity: 951.181247
Trainning Epoch:  24%|██▍       | 40/165 [1:28:33<4:36:53, 132.91s/it]2025-09-14 17:56:43,111 Stage: Train 0.5 | Epoch: 40 | Iter: 121600 | Total Loss: 0.003139 | Recon Loss: 0.002671 | Commit Loss: 0.000935 | Perplexity: 954.932145
2025-09-14 17:56:51,836 Stage: Train 0.5 | Epoch: 40 | Iter: 121800 | Total Loss: 0.003133 | Recon Loss: 0.002673 | Commit Loss: 0.000919 | Perplexity: 956.269970
2025-09-14 17:57:00,521 Stage: Train 0.5 | Epoch: 40 | Iter: 122000 | Total Loss: 0.003157 | Recon Loss: 0.002692 | Commit Loss: 0.000930 | Perplexity: 955.754175
2025-09-14 17:57:09,268 Stage: Train 0.5 | Epoch: 40 | Iter: 122200 | Total Loss: 0.003149 | Recon Loss: 0.002680 | Commit Loss: 0.000937 | Perplexity: 954.150608
2025-09-14 17:57:18,012 Stage: Train 0.5 | Epoch: 40 | Iter: 122400 | Total Loss: 0.003155 | Recon Loss: 0.002687 | Commit Loss: 0.000936 | Perplexity: 960.354975
2025-09-14 17:57:26,725 Stage: Train 0.5 | Epoch: 40 | Iter: 122600 | Total Loss: 0.003117 | Recon Loss: 0.002651 | Commit Loss: 0.000933 | Perplexity: 956.722013
2025-09-14 17:57:35,440 Stage: Train 0.5 | Epoch: 40 | Iter: 122800 | Total Loss: 0.003211 | Recon Loss: 0.002751 | Commit Loss: 0.000920 | Perplexity: 951.650742
2025-09-14 17:57:44,133 Stage: Train 0.5 | Epoch: 40 | Iter: 123000 | Total Loss: 0.003194 | Recon Loss: 0.002728 | Commit Loss: 0.000932 | Perplexity: 955.555789
2025-09-14 17:57:52,842 Stage: Train 0.5 | Epoch: 40 | Iter: 123200 | Total Loss: 0.003135 | Recon Loss: 0.002669 | Commit Loss: 0.000933 | Perplexity: 954.917056
2025-09-14 17:58:01,538 Stage: Train 0.5 | Epoch: 40 | Iter: 123400 | Total Loss: 0.003149 | Recon Loss: 0.002682 | Commit Loss: 0.000933 | Perplexity: 955.437930
2025-09-14 17:58:10,260 Stage: Train 0.5 | Epoch: 40 | Iter: 123600 | Total Loss: 0.003149 | Recon Loss: 0.002688 | Commit Loss: 0.000923 | Perplexity: 960.082404
2025-09-14 17:58:18,973 Stage: Train 0.5 | Epoch: 40 | Iter: 123800 | Total Loss: 0.003174 | Recon Loss: 0.002707 | Commit Loss: 0.000934 | Perplexity: 958.573456
2025-09-14 17:58:27,665 Stage: Train 0.5 | Epoch: 40 | Iter: 124000 | Total Loss: 0.003101 | Recon Loss: 0.002640 | Commit Loss: 0.000923 | Perplexity: 953.147225
2025-09-14 17:58:36,358 Stage: Train 0.5 | Epoch: 40 | Iter: 124200 | Total Loss: 0.003124 | Recon Loss: 0.002661 | Commit Loss: 0.000926 | Perplexity: 954.070855
2025-09-14 17:58:45,063 Stage: Train 0.5 | Epoch: 40 | Iter: 124400 | Total Loss: 0.003156 | Recon Loss: 0.002695 | Commit Loss: 0.000922 | Perplexity: 956.509594
Trainning Epoch:  25%|██▍       | 41/165 [1:30:45<4:34:18, 132.73s/it]2025-09-14 17:58:53,774 Stage: Train 0.5 | Epoch: 41 | Iter: 124600 | Total Loss: 0.003187 | Recon Loss: 0.002720 | Commit Loss: 0.000935 | Perplexity: 954.766546
2025-09-14 17:59:02,472 Stage: Train 0.5 | Epoch: 41 | Iter: 124800 | Total Loss: 0.003061 | Recon Loss: 0.002604 | Commit Loss: 0.000915 | Perplexity: 951.566136
2025-09-14 17:59:11,188 Stage: Train 0.5 | Epoch: 41 | Iter: 125000 | Total Loss: 0.003182 | Recon Loss: 0.002716 | Commit Loss: 0.000932 | Perplexity: 951.581774
2025-09-14 17:59:19,886 Stage: Train 0.5 | Epoch: 41 | Iter: 125200 | Total Loss: 0.003164 | Recon Loss: 0.002701 | Commit Loss: 0.000925 | Perplexity: 957.264649
2025-09-14 17:59:28,592 Stage: Train 0.5 | Epoch: 41 | Iter: 125400 | Total Loss: 0.003169 | Recon Loss: 0.002703 | Commit Loss: 0.000933 | Perplexity: 956.864237
2025-09-14 17:59:37,300 Stage: Train 0.5 | Epoch: 41 | Iter: 125600 | Total Loss: 0.003137 | Recon Loss: 0.002672 | Commit Loss: 0.000930 | Perplexity: 953.850614
2025-09-14 17:59:46,032 Stage: Train 0.5 | Epoch: 41 | Iter: 125800 | Total Loss: 0.003080 | Recon Loss: 0.002618 | Commit Loss: 0.000923 | Perplexity: 957.923689
2025-09-14 17:59:54,770 Stage: Train 0.5 | Epoch: 41 | Iter: 126000 | Total Loss: 0.003193 | Recon Loss: 0.002728 | Commit Loss: 0.000930 | Perplexity: 948.215981
2025-09-14 18:00:03,517 Stage: Train 0.5 | Epoch: 41 | Iter: 126200 | Total Loss: 0.003177 | Recon Loss: 0.002716 | Commit Loss: 0.000923 | Perplexity: 951.396705
2025-09-14 18:00:12,260 Stage: Train 0.5 | Epoch: 41 | Iter: 126400 | Total Loss: 0.003112 | Recon Loss: 0.002652 | Commit Loss: 0.000920 | Perplexity: 956.224821
2025-09-14 18:00:20,988 Stage: Train 0.5 | Epoch: 41 | Iter: 126600 | Total Loss: 0.003186 | Recon Loss: 0.002723 | Commit Loss: 0.000925 | Perplexity: 953.761015
2025-09-14 18:00:29,744 Stage: Train 0.5 | Epoch: 41 | Iter: 126800 | Total Loss: 0.003079 | Recon Loss: 0.002625 | Commit Loss: 0.000909 | Perplexity: 955.679510
2025-09-14 18:00:38,490 Stage: Train 0.5 | Epoch: 41 | Iter: 127000 | Total Loss: 0.003084 | Recon Loss: 0.002617 | Commit Loss: 0.000933 | Perplexity: 957.591812
2025-09-14 18:00:47,231 Stage: Train 0.5 | Epoch: 41 | Iter: 127200 | Total Loss: 0.003129 | Recon Loss: 0.002655 | Commit Loss: 0.000948 | Perplexity: 957.790074
2025-09-14 18:00:55,956 Stage: Train 0.5 | Epoch: 41 | Iter: 127400 | Total Loss: 0.003149 | Recon Loss: 0.002674 | Commit Loss: 0.000950 | Perplexity: 958.259742
Trainning Epoch:  25%|██▌       | 42/165 [1:32:58<4:31:59, 132.68s/it]2025-09-14 18:01:04,676 Stage: Train 0.5 | Epoch: 42 | Iter: 127600 | Total Loss: 0.003104 | Recon Loss: 0.002640 | Commit Loss: 0.000927 | Perplexity: 949.643973
2025-09-14 18:01:13,408 Stage: Train 0.5 | Epoch: 42 | Iter: 127800 | Total Loss: 0.003160 | Recon Loss: 0.002695 | Commit Loss: 0.000930 | Perplexity: 947.927069
2025-09-14 18:01:22,136 Stage: Train 0.5 | Epoch: 42 | Iter: 128000 | Total Loss: 0.003054 | Recon Loss: 0.002594 | Commit Loss: 0.000921 | Perplexity: 953.695843
2025-09-14 18:01:30,831 Stage: Train 0.5 | Epoch: 42 | Iter: 128200 | Total Loss: 0.003121 | Recon Loss: 0.002657 | Commit Loss: 0.000929 | Perplexity: 952.527361
2025-09-14 18:01:39,440 Stage: Train 0.5 | Epoch: 42 | Iter: 128400 | Total Loss: 0.003132 | Recon Loss: 0.002667 | Commit Loss: 0.000931 | Perplexity: 953.567392
2025-09-14 18:01:48,064 Stage: Train 0.5 | Epoch: 42 | Iter: 128600 | Total Loss: 0.003092 | Recon Loss: 0.002631 | Commit Loss: 0.000922 | Perplexity: 955.739618
2025-09-14 18:01:56,625 Stage: Train 0.5 | Epoch: 42 | Iter: 128800 | Total Loss: 0.003095 | Recon Loss: 0.002629 | Commit Loss: 0.000933 | Perplexity: 955.977954
2025-09-14 18:02:05,293 Stage: Train 0.5 | Epoch: 42 | Iter: 129000 | Total Loss: 0.003164 | Recon Loss: 0.002695 | Commit Loss: 0.000937 | Perplexity: 958.430367
2025-09-14 18:02:14,051 Stage: Train 0.5 | Epoch: 42 | Iter: 129200 | Total Loss: 0.003069 | Recon Loss: 0.002604 | Commit Loss: 0.000929 | Perplexity: 953.742637
2025-09-14 18:02:22,828 Stage: Train 0.5 | Epoch: 42 | Iter: 129400 | Total Loss: 0.003171 | Recon Loss: 0.002699 | Commit Loss: 0.000944 | Perplexity: 955.715602
2025-09-14 18:02:31,608 Stage: Train 0.5 | Epoch: 42 | Iter: 129600 | Total Loss: 0.003097 | Recon Loss: 0.002630 | Commit Loss: 0.000934 | Perplexity: 954.083112
2025-09-14 18:02:40,356 Stage: Train 0.5 | Epoch: 42 | Iter: 129800 | Total Loss: 0.003101 | Recon Loss: 0.002638 | Commit Loss: 0.000926 | Perplexity: 957.626054
2025-09-14 18:02:49,158 Stage: Train 0.5 | Epoch: 42 | Iter: 130000 | Total Loss: 0.003093 | Recon Loss: 0.002623 | Commit Loss: 0.000939 | Perplexity: 951.049456
2025-09-14 18:02:57,955 Stage: Train 0.5 | Epoch: 42 | Iter: 130200 | Total Loss: 0.003138 | Recon Loss: 0.002673 | Commit Loss: 0.000930 | Perplexity: 953.164430
2025-09-14 18:03:06,692 Stage: Train 0.5 | Epoch: 42 | Iter: 130400 | Total Loss: 0.003119 | Recon Loss: 0.002657 | Commit Loss: 0.000923 | Perplexity: 948.535617
2025-09-14 18:03:15,469 Stage: Train 0.5 | Epoch: 42 | Iter: 130600 | Total Loss: 0.003176 | Recon Loss: 0.002710 | Commit Loss: 0.000930 | Perplexity: 953.789417
Trainning Epoch:  26%|██▌       | 43/165 [1:35:10<4:29:39, 132.62s/it]2025-09-14 18:03:24,280 Stage: Train 0.5 | Epoch: 43 | Iter: 130800 | Total Loss: 0.003062 | Recon Loss: 0.002596 | Commit Loss: 0.000932 | Perplexity: 955.027333
2025-09-14 18:03:33,039 Stage: Train 0.5 | Epoch: 43 | Iter: 131000 | Total Loss: 0.003183 | Recon Loss: 0.002716 | Commit Loss: 0.000933 | Perplexity: 955.289824
2025-09-14 18:03:41,800 Stage: Train 0.5 | Epoch: 43 | Iter: 131200 | Total Loss: 0.003147 | Recon Loss: 0.002678 | Commit Loss: 0.000938 | Perplexity: 952.200102
2025-09-14 18:03:50,592 Stage: Train 0.5 | Epoch: 43 | Iter: 131400 | Total Loss: 0.003038 | Recon Loss: 0.002574 | Commit Loss: 0.000928 | Perplexity: 952.027085
2025-09-14 18:03:59,352 Stage: Train 0.5 | Epoch: 43 | Iter: 131600 | Total Loss: 0.003087 | Recon Loss: 0.002619 | Commit Loss: 0.000936 | Perplexity: 953.570464
2025-09-14 18:04:08,087 Stage: Train 0.5 | Epoch: 43 | Iter: 131800 | Total Loss: 0.003148 | Recon Loss: 0.002681 | Commit Loss: 0.000933 | Perplexity: 958.610833
2025-09-14 18:04:16,857 Stage: Train 0.5 | Epoch: 43 | Iter: 132000 | Total Loss: 0.003069 | Recon Loss: 0.002600 | Commit Loss: 0.000939 | Perplexity: 959.527581
2025-09-14 18:04:25,636 Stage: Train 0.5 | Epoch: 43 | Iter: 132200 | Total Loss: 0.003128 | Recon Loss: 0.002666 | Commit Loss: 0.000924 | Perplexity: 955.998735
2025-09-14 18:04:34,431 Stage: Train 0.5 | Epoch: 43 | Iter: 132400 | Total Loss: 0.003011 | Recon Loss: 0.002547 | Commit Loss: 0.000929 | Perplexity: 952.121888
2025-09-14 18:04:43,215 Stage: Train 0.5 | Epoch: 43 | Iter: 132600 | Total Loss: 0.003139 | Recon Loss: 0.002677 | Commit Loss: 0.000923 | Perplexity: 952.699017
2025-09-14 18:04:51,968 Stage: Train 0.5 | Epoch: 43 | Iter: 132800 | Total Loss: 0.003094 | Recon Loss: 0.002622 | Commit Loss: 0.000943 | Perplexity: 951.852718
2025-09-14 18:05:00,727 Stage: Train 0.5 | Epoch: 43 | Iter: 133000 | Total Loss: 0.003121 | Recon Loss: 0.002652 | Commit Loss: 0.000937 | Perplexity: 951.693217
2025-09-14 18:05:09,468 Stage: Train 0.5 | Epoch: 43 | Iter: 133200 | Total Loss: 0.003117 | Recon Loss: 0.002652 | Commit Loss: 0.000929 | Perplexity: 953.859314
2025-09-14 18:05:18,237 Stage: Train 0.5 | Epoch: 43 | Iter: 133400 | Total Loss: 0.003091 | Recon Loss: 0.002626 | Commit Loss: 0.000931 | Perplexity: 958.727401
2025-09-14 18:05:27,021 Stage: Train 0.5 | Epoch: 43 | Iter: 133600 | Total Loss: 0.003053 | Recon Loss: 0.002594 | Commit Loss: 0.000918 | Perplexity: 955.889643
Trainning Epoch:  27%|██▋       | 44/165 [1:37:23<4:27:47, 132.79s/it]2025-09-14 18:05:35,776 Stage: Train 0.5 | Epoch: 44 | Iter: 133800 | Total Loss: 0.003089 | Recon Loss: 0.002625 | Commit Loss: 0.000927 | Perplexity: 952.677046
2025-09-14 18:05:44,572 Stage: Train 0.5 | Epoch: 44 | Iter: 134000 | Total Loss: 0.003048 | Recon Loss: 0.002583 | Commit Loss: 0.000930 | Perplexity: 957.783986
2025-09-14 18:05:53,321 Stage: Train 0.5 | Epoch: 44 | Iter: 134200 | Total Loss: 0.003105 | Recon Loss: 0.002643 | Commit Loss: 0.000923 | Perplexity: 952.528766
2025-09-14 18:06:02,072 Stage: Train 0.5 | Epoch: 44 | Iter: 134400 | Total Loss: 0.003075 | Recon Loss: 0.002603 | Commit Loss: 0.000945 | Perplexity: 958.435732
2025-09-14 18:06:10,845 Stage: Train 0.5 | Epoch: 44 | Iter: 134600 | Total Loss: 0.003078 | Recon Loss: 0.002613 | Commit Loss: 0.000928 | Perplexity: 953.234573
2025-09-14 18:06:19,575 Stage: Train 0.5 | Epoch: 44 | Iter: 134800 | Total Loss: 0.003129 | Recon Loss: 0.002658 | Commit Loss: 0.000942 | Perplexity: 952.405559
2025-09-14 18:06:28,325 Stage: Train 0.5 | Epoch: 44 | Iter: 135000 | Total Loss: 0.003068 | Recon Loss: 0.002606 | Commit Loss: 0.000924 | Perplexity: 952.648937
2025-09-14 18:06:37,068 Stage: Train 0.5 | Epoch: 44 | Iter: 135200 | Total Loss: 0.003170 | Recon Loss: 0.002700 | Commit Loss: 0.000939 | Perplexity: 953.511587
2025-09-14 18:06:45,822 Stage: Train 0.5 | Epoch: 44 | Iter: 135400 | Total Loss: 0.003051 | Recon Loss: 0.002592 | Commit Loss: 0.000918 | Perplexity: 952.224606
2025-09-14 18:06:54,571 Stage: Train 0.5 | Epoch: 44 | Iter: 135600 | Total Loss: 0.003067 | Recon Loss: 0.002600 | Commit Loss: 0.000935 | Perplexity: 955.032045
2025-09-14 18:07:03,329 Stage: Train 0.5 | Epoch: 44 | Iter: 135800 | Total Loss: 0.003091 | Recon Loss: 0.002630 | Commit Loss: 0.000924 | Perplexity: 956.292060
2025-09-14 18:07:12,083 Stage: Train 0.5 | Epoch: 44 | Iter: 136000 | Total Loss: 0.003058 | Recon Loss: 0.002591 | Commit Loss: 0.000933 | Perplexity: 951.929407
2025-09-14 18:07:20,800 Stage: Train 0.5 | Epoch: 44 | Iter: 136200 | Total Loss: 0.003047 | Recon Loss: 0.002583 | Commit Loss: 0.000928 | Perplexity: 954.850468
2025-09-14 18:07:29,524 Stage: Train 0.5 | Epoch: 44 | Iter: 136400 | Total Loss: 0.003071 | Recon Loss: 0.002603 | Commit Loss: 0.000936 | Perplexity: 955.483642
2025-09-14 18:07:38,242 Stage: Train 0.5 | Epoch: 44 | Iter: 136600 | Total Loss: 0.003053 | Recon Loss: 0.002582 | Commit Loss: 0.000942 | Perplexity: 959.794946
Trainning Epoch:  27%|██▋       | 45/165 [1:39:36<4:25:38, 132.82s/it]2025-09-14 18:07:46,981 Stage: Train 0.5 | Epoch: 45 | Iter: 136800 | Total Loss: 0.003091 | Recon Loss: 0.002626 | Commit Loss: 0.000930 | Perplexity: 952.120953
2025-09-14 18:07:55,721 Stage: Train 0.5 | Epoch: 45 | Iter: 137000 | Total Loss: 0.003155 | Recon Loss: 0.002692 | Commit Loss: 0.000925 | Perplexity: 954.025795
2025-09-14 18:08:04,489 Stage: Train 0.5 | Epoch: 45 | Iter: 137200 | Total Loss: 0.003041 | Recon Loss: 0.002575 | Commit Loss: 0.000933 | Perplexity: 954.325790
2025-09-14 18:08:13,255 Stage: Train 0.5 | Epoch: 45 | Iter: 137400 | Total Loss: 0.003146 | Recon Loss: 0.002682 | Commit Loss: 0.000927 | Perplexity: 945.120375
2025-09-14 18:08:21,989 Stage: Train 0.5 | Epoch: 45 | Iter: 137600 | Total Loss: 0.003075 | Recon Loss: 0.002609 | Commit Loss: 0.000933 | Perplexity: 951.750292
2025-09-14 18:08:30,694 Stage: Train 0.5 | Epoch: 45 | Iter: 137800 | Total Loss: 0.003041 | Recon Loss: 0.002580 | Commit Loss: 0.000922 | Perplexity: 954.325588
2025-09-14 18:08:39,435 Stage: Train 0.5 | Epoch: 45 | Iter: 138000 | Total Loss: 0.003047 | Recon Loss: 0.002583 | Commit Loss: 0.000926 | Perplexity: 952.060392
2025-09-14 18:08:48,212 Stage: Train 0.5 | Epoch: 45 | Iter: 138200 | Total Loss: 0.003063 | Recon Loss: 0.002602 | Commit Loss: 0.000922 | Perplexity: 950.096924
2025-09-14 18:08:56,984 Stage: Train 0.5 | Epoch: 45 | Iter: 138400 | Total Loss: 0.003109 | Recon Loss: 0.002641 | Commit Loss: 0.000936 | Perplexity: 956.031702
2025-09-14 18:09:05,748 Stage: Train 0.5 | Epoch: 45 | Iter: 138600 | Total Loss: 0.003040 | Recon Loss: 0.002576 | Commit Loss: 0.000929 | Perplexity: 954.401834
2025-09-14 18:09:14,536 Stage: Train 0.5 | Epoch: 45 | Iter: 138800 | Total Loss: 0.003085 | Recon Loss: 0.002619 | Commit Loss: 0.000930 | Perplexity: 950.162715
2025-09-14 18:09:23,318 Stage: Train 0.5 | Epoch: 45 | Iter: 139000 | Total Loss: 0.003020 | Recon Loss: 0.002550 | Commit Loss: 0.000940 | Perplexity: 955.574717
2025-09-14 18:09:32,084 Stage: Train 0.5 | Epoch: 45 | Iter: 139200 | Total Loss: 0.003124 | Recon Loss: 0.002655 | Commit Loss: 0.000939 | Perplexity: 954.701586
2025-09-14 18:09:40,867 Stage: Train 0.5 | Epoch: 45 | Iter: 139400 | Total Loss: 0.003061 | Recon Loss: 0.002599 | Commit Loss: 0.000925 | Perplexity: 951.567920
2025-09-14 18:09:49,601 Stage: Train 0.5 | Epoch: 45 | Iter: 139600 | Total Loss: 0.003123 | Recon Loss: 0.002659 | Commit Loss: 0.000929 | Perplexity: 956.119567
Trainning Epoch:  28%|██▊       | 46/165 [1:41:49<4:23:31, 132.87s/it]2025-09-14 18:09:58,325 Stage: Train 0.5 | Epoch: 46 | Iter: 139800 | Total Loss: 0.003049 | Recon Loss: 0.002584 | Commit Loss: 0.000930 | Perplexity: 950.679298
2025-09-14 18:10:07,034 Stage: Train 0.5 | Epoch: 46 | Iter: 140000 | Total Loss: 0.003034 | Recon Loss: 0.002574 | Commit Loss: 0.000919 | Perplexity: 949.388803
2025-09-14 18:10:07,034 Saving model at iteration 140000
2025-09-14 18:10:07,274 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_47_step_140000
2025-09-14 18:10:07,509 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_47_step_140000/pytorch_model.bin
2025-09-14 18:10:07,880 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_47_step_140000/optimizer.bin
2025-09-14 18:10:07,881 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_47_step_140000/scheduler.bin
2025-09-14 18:10:07,881 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_47_step_140000/random_states_0.pkl
2025-09-14 18:10:16,976 Stage: Train 0.5 | Epoch: 46 | Iter: 140200 | Total Loss: 0.003124 | Recon Loss: 0.002660 | Commit Loss: 0.000929 | Perplexity: 956.868128
2025-09-14 18:10:25,688 Stage: Train 0.5 | Epoch: 46 | Iter: 140400 | Total Loss: 0.003053 | Recon Loss: 0.002587 | Commit Loss: 0.000931 | Perplexity: 953.166304
2025-09-14 18:10:34,395 Stage: Train 0.5 | Epoch: 46 | Iter: 140600 | Total Loss: 0.003025 | Recon Loss: 0.002564 | Commit Loss: 0.000922 | Perplexity: 953.513788
2025-09-14 18:10:43,093 Stage: Train 0.5 | Epoch: 46 | Iter: 140800 | Total Loss: 0.003082 | Recon Loss: 0.002623 | Commit Loss: 0.000918 | Perplexity: 947.958664
2025-09-14 18:10:51,788 Stage: Train 0.5 | Epoch: 46 | Iter: 141000 | Total Loss: 0.002994 | Recon Loss: 0.002525 | Commit Loss: 0.000939 | Perplexity: 953.825620
2025-09-14 18:11:00,506 Stage: Train 0.5 | Epoch: 46 | Iter: 141200 | Total Loss: 0.003135 | Recon Loss: 0.002663 | Commit Loss: 0.000944 | Perplexity: 957.926407
2025-09-14 18:11:09,206 Stage: Train 0.5 | Epoch: 46 | Iter: 141400 | Total Loss: 0.003163 | Recon Loss: 0.002698 | Commit Loss: 0.000930 | Perplexity: 951.512269
2025-09-14 18:11:17,917 Stage: Train 0.5 | Epoch: 46 | Iter: 141600 | Total Loss: 0.003028 | Recon Loss: 0.002567 | Commit Loss: 0.000922 | Perplexity: 955.750557
2025-09-14 18:11:26,624 Stage: Train 0.5 | Epoch: 46 | Iter: 141800 | Total Loss: 0.003056 | Recon Loss: 0.002590 | Commit Loss: 0.000931 | Perplexity: 953.411863
2025-09-14 18:11:35,328 Stage: Train 0.5 | Epoch: 46 | Iter: 142000 | Total Loss: 0.003040 | Recon Loss: 0.002568 | Commit Loss: 0.000943 | Perplexity: 956.694884
2025-09-14 18:11:44,066 Stage: Train 0.5 | Epoch: 46 | Iter: 142200 | Total Loss: 0.003043 | Recon Loss: 0.002580 | Commit Loss: 0.000927 | Perplexity: 954.217201
2025-09-14 18:11:52,741 Stage: Train 0.5 | Epoch: 46 | Iter: 142400 | Total Loss: 0.003138 | Recon Loss: 0.002669 | Commit Loss: 0.000939 | Perplexity: 955.159402
2025-09-14 18:12:01,455 Stage: Train 0.5 | Epoch: 46 | Iter: 142600 | Total Loss: 0.003038 | Recon Loss: 0.002573 | Commit Loss: 0.000930 | Perplexity: 950.872533
Trainning Epoch:  28%|██▊       | 47/165 [1:44:03<4:21:50, 133.14s/it]2025-09-14 18:12:10,416 Stage: Train 0.5 | Epoch: 47 | Iter: 142800 | Total Loss: 0.002971 | Recon Loss: 0.002514 | Commit Loss: 0.000914 | Perplexity: 947.344408
2025-09-14 18:12:19,123 Stage: Train 0.5 | Epoch: 47 | Iter: 143000 | Total Loss: 0.003013 | Recon Loss: 0.002549 | Commit Loss: 0.000929 | Perplexity: 959.839819
2025-09-14 18:12:27,853 Stage: Train 0.5 | Epoch: 47 | Iter: 143200 | Total Loss: 0.003046 | Recon Loss: 0.002578 | Commit Loss: 0.000935 | Perplexity: 954.002568
2025-09-14 18:12:36,608 Stage: Train 0.5 | Epoch: 47 | Iter: 143400 | Total Loss: 0.003135 | Recon Loss: 0.002666 | Commit Loss: 0.000937 | Perplexity: 949.878549
2025-09-14 18:12:45,321 Stage: Train 0.5 | Epoch: 47 | Iter: 143600 | Total Loss: 0.003009 | Recon Loss: 0.002547 | Commit Loss: 0.000925 | Perplexity: 951.745165
2025-09-14 18:12:54,003 Stage: Train 0.5 | Epoch: 47 | Iter: 143800 | Total Loss: 0.003020 | Recon Loss: 0.002555 | Commit Loss: 0.000930 | Perplexity: 954.850384
2025-09-14 18:13:02,705 Stage: Train 0.5 | Epoch: 47 | Iter: 144000 | Total Loss: 0.002987 | Recon Loss: 0.002523 | Commit Loss: 0.000928 | Perplexity: 953.282662
2025-09-14 18:13:11,424 Stage: Train 0.5 | Epoch: 47 | Iter: 144200 | Total Loss: 0.003159 | Recon Loss: 0.002691 | Commit Loss: 0.000936 | Perplexity: 949.367843
2025-09-14 18:13:20,157 Stage: Train 0.5 | Epoch: 47 | Iter: 144400 | Total Loss: 0.003064 | Recon Loss: 0.002606 | Commit Loss: 0.000917 | Perplexity: 951.297086
2025-09-14 18:13:28,862 Stage: Train 0.5 | Epoch: 47 | Iter: 144600 | Total Loss: 0.003073 | Recon Loss: 0.002615 | Commit Loss: 0.000916 | Perplexity: 951.095010
2025-09-14 18:13:37,556 Stage: Train 0.5 | Epoch: 47 | Iter: 144800 | Total Loss: 0.003087 | Recon Loss: 0.002616 | Commit Loss: 0.000941 | Perplexity: 951.222456
2025-09-14 18:13:46,286 Stage: Train 0.5 | Epoch: 47 | Iter: 145000 | Total Loss: 0.003036 | Recon Loss: 0.002570 | Commit Loss: 0.000932 | Perplexity: 950.634316
2025-09-14 18:13:55,024 Stage: Train 0.5 | Epoch: 47 | Iter: 145200 | Total Loss: 0.003022 | Recon Loss: 0.002560 | Commit Loss: 0.000926 | Perplexity: 955.034195
2025-09-14 18:14:03,723 Stage: Train 0.5 | Epoch: 47 | Iter: 145400 | Total Loss: 0.002985 | Recon Loss: 0.002521 | Commit Loss: 0.000928 | Perplexity: 957.205611
2025-09-14 18:14:12,460 Stage: Train 0.5 | Epoch: 47 | Iter: 145600 | Total Loss: 0.002984 | Recon Loss: 0.002522 | Commit Loss: 0.000924 | Perplexity: 953.520584
2025-09-14 18:14:21,198 Stage: Train 0.5 | Epoch: 47 | Iter: 145800 | Total Loss: 0.003114 | Recon Loss: 0.002649 | Commit Loss: 0.000930 | Perplexity: 953.287314
Trainning Epoch:  29%|██▉       | 48/165 [1:46:15<4:19:12, 132.93s/it]2025-09-14 18:14:29,917 Stage: Train 0.5 | Epoch: 48 | Iter: 146000 | Total Loss: 0.002979 | Recon Loss: 0.002522 | Commit Loss: 0.000914 | Perplexity: 948.278809
2025-09-14 18:14:38,632 Stage: Train 0.5 | Epoch: 48 | Iter: 146200 | Total Loss: 0.003070 | Recon Loss: 0.002599 | Commit Loss: 0.000942 | Perplexity: 956.000029
2025-09-14 18:14:47,379 Stage: Train 0.5 | Epoch: 48 | Iter: 146400 | Total Loss: 0.003060 | Recon Loss: 0.002597 | Commit Loss: 0.000926 | Perplexity: 950.415062
2025-09-14 18:14:56,124 Stage: Train 0.5 | Epoch: 48 | Iter: 146600 | Total Loss: 0.002972 | Recon Loss: 0.002515 | Commit Loss: 0.000916 | Perplexity: 950.016812
2025-09-14 18:15:04,836 Stage: Train 0.5 | Epoch: 48 | Iter: 146800 | Total Loss: 0.003030 | Recon Loss: 0.002571 | Commit Loss: 0.000918 | Perplexity: 951.698021
2025-09-14 18:15:13,576 Stage: Train 0.5 | Epoch: 48 | Iter: 147000 | Total Loss: 0.003045 | Recon Loss: 0.002581 | Commit Loss: 0.000929 | Perplexity: 950.607009
2025-09-14 18:15:22,301 Stage: Train 0.5 | Epoch: 48 | Iter: 147200 | Total Loss: 0.002961 | Recon Loss: 0.002491 | Commit Loss: 0.000940 | Perplexity: 958.442370
2025-09-14 18:15:31,028 Stage: Train 0.5 | Epoch: 48 | Iter: 147400 | Total Loss: 0.002988 | Recon Loss: 0.002525 | Commit Loss: 0.000925 | Perplexity: 952.091603
2025-09-14 18:15:39,755 Stage: Train 0.5 | Epoch: 48 | Iter: 147600 | Total Loss: 0.003036 | Recon Loss: 0.002571 | Commit Loss: 0.000929 | Perplexity: 950.447100
2025-09-14 18:15:48,484 Stage: Train 0.5 | Epoch: 48 | Iter: 147800 | Total Loss: 0.003045 | Recon Loss: 0.002581 | Commit Loss: 0.000928 | Perplexity: 950.362091
2025-09-14 18:15:57,235 Stage: Train 0.5 | Epoch: 48 | Iter: 148000 | Total Loss: 0.002996 | Recon Loss: 0.002525 | Commit Loss: 0.000942 | Perplexity: 956.651300
2025-09-14 18:16:05,963 Stage: Train 0.5 | Epoch: 48 | Iter: 148200 | Total Loss: 0.003012 | Recon Loss: 0.002545 | Commit Loss: 0.000934 | Perplexity: 955.874966
2025-09-14 18:16:14,700 Stage: Train 0.5 | Epoch: 48 | Iter: 148400 | Total Loss: 0.003021 | Recon Loss: 0.002557 | Commit Loss: 0.000930 | Perplexity: 952.331432
2025-09-14 18:16:23,389 Stage: Train 0.5 | Epoch: 48 | Iter: 148600 | Total Loss: 0.003008 | Recon Loss: 0.002542 | Commit Loss: 0.000932 | Perplexity: 954.057466
2025-09-14 18:16:32,124 Stage: Train 0.5 | Epoch: 48 | Iter: 148800 | Total Loss: 0.003031 | Recon Loss: 0.002566 | Commit Loss: 0.000929 | Perplexity: 955.596902
Trainning Epoch:  30%|██▉       | 49/165 [1:48:28<4:16:47, 132.82s/it]2025-09-14 18:16:40,845 Stage: Train 0.5 | Epoch: 49 | Iter: 149000 | Total Loss: 0.003032 | Recon Loss: 0.002567 | Commit Loss: 0.000929 | Perplexity: 951.563861
2025-09-14 18:16:49,568 Stage: Train 0.5 | Epoch: 49 | Iter: 149200 | Total Loss: 0.003044 | Recon Loss: 0.002578 | Commit Loss: 0.000932 | Perplexity: 950.495928
2025-09-14 18:16:58,283 Stage: Train 0.5 | Epoch: 49 | Iter: 149400 | Total Loss: 0.003036 | Recon Loss: 0.002574 | Commit Loss: 0.000923 | Perplexity: 951.002181
2025-09-14 18:17:06,971 Stage: Train 0.5 | Epoch: 49 | Iter: 149600 | Total Loss: 0.003049 | Recon Loss: 0.002593 | Commit Loss: 0.000910 | Perplexity: 945.541506
2025-09-14 18:17:15,690 Stage: Train 0.5 | Epoch: 49 | Iter: 149800 | Total Loss: 0.002984 | Recon Loss: 0.002522 | Commit Loss: 0.000925 | Perplexity: 950.975930
2025-09-14 18:17:24,389 Stage: Train 0.5 | Epoch: 49 | Iter: 150000 | Total Loss: 0.002992 | Recon Loss: 0.002522 | Commit Loss: 0.000939 | Perplexity: 951.093244
2025-09-14 18:17:33,092 Stage: Train 0.5 | Epoch: 49 | Iter: 150200 | Total Loss: 0.003003 | Recon Loss: 0.002539 | Commit Loss: 0.000927 | Perplexity: 953.076955
2025-09-14 18:17:41,797 Stage: Train 0.5 | Epoch: 49 | Iter: 150400 | Total Loss: 0.003084 | Recon Loss: 0.002615 | Commit Loss: 0.000938 | Perplexity: 957.304955
2025-09-14 18:17:50,490 Stage: Train 0.5 | Epoch: 49 | Iter: 150600 | Total Loss: 0.002972 | Recon Loss: 0.002508 | Commit Loss: 0.000927 | Perplexity: 954.720053
2025-09-14 18:17:59,175 Stage: Train 0.5 | Epoch: 49 | Iter: 150800 | Total Loss: 0.003043 | Recon Loss: 0.002582 | Commit Loss: 0.000923 | Perplexity: 949.004099
2025-09-14 18:18:07,872 Stage: Train 0.5 | Epoch: 49 | Iter: 151000 | Total Loss: 0.003021 | Recon Loss: 0.002555 | Commit Loss: 0.000931 | Perplexity: 955.014340
2025-09-14 18:18:16,580 Stage: Train 0.5 | Epoch: 49 | Iter: 151200 | Total Loss: 0.003055 | Recon Loss: 0.002592 | Commit Loss: 0.000926 | Perplexity: 953.442249
2025-09-14 18:18:25,260 Stage: Train 0.5 | Epoch: 49 | Iter: 151400 | Total Loss: 0.003024 | Recon Loss: 0.002552 | Commit Loss: 0.000943 | Perplexity: 955.384643
2025-09-14 18:18:33,967 Stage: Train 0.5 | Epoch: 49 | Iter: 151600 | Total Loss: 0.002950 | Recon Loss: 0.002487 | Commit Loss: 0.000927 | Perplexity: 953.651724
2025-09-14 18:18:42,674 Stage: Train 0.5 | Epoch: 49 | Iter: 151800 | Total Loss: 0.003081 | Recon Loss: 0.002615 | Commit Loss: 0.000933 | Perplexity: 952.830604
Trainning Epoch:  30%|███       | 50/165 [1:50:40<4:14:12, 132.63s/it]2025-09-14 18:18:51,366 Stage: Train 0.5 | Epoch: 50 | Iter: 152000 | Total Loss: 0.002963 | Recon Loss: 0.002494 | Commit Loss: 0.000939 | Perplexity: 946.031310
2025-09-14 18:19:00,070 Stage: Train 0.5 | Epoch: 50 | Iter: 152200 | Total Loss: 0.002942 | Recon Loss: 0.002476 | Commit Loss: 0.000932 | Perplexity: 958.393708
2025-09-14 18:19:08,783 Stage: Train 0.5 | Epoch: 50 | Iter: 152400 | Total Loss: 0.003015 | Recon Loss: 0.002548 | Commit Loss: 0.000935 | Perplexity: 953.345180
2025-09-14 18:19:17,515 Stage: Train 0.5 | Epoch: 50 | Iter: 152600 | Total Loss: 0.002985 | Recon Loss: 0.002523 | Commit Loss: 0.000925 | Perplexity: 945.963755
2025-09-14 18:19:26,212 Stage: Train 0.5 | Epoch: 50 | Iter: 152800 | Total Loss: 0.003146 | Recon Loss: 0.002677 | Commit Loss: 0.000937 | Perplexity: 951.434464
2025-09-14 18:19:34,926 Stage: Train 0.5 | Epoch: 50 | Iter: 153000 | Total Loss: 0.002988 | Recon Loss: 0.002523 | Commit Loss: 0.000931 | Perplexity: 958.230893
2025-09-14 18:19:43,697 Stage: Train 0.5 | Epoch: 50 | Iter: 153200 | Total Loss: 0.003014 | Recon Loss: 0.002548 | Commit Loss: 0.000932 | Perplexity: 951.517526
2025-09-14 18:19:52,413 Stage: Train 0.5 | Epoch: 50 | Iter: 153400 | Total Loss: 0.002962 | Recon Loss: 0.002495 | Commit Loss: 0.000934 | Perplexity: 956.786714
2025-09-14 18:20:01,134 Stage: Train 0.5 | Epoch: 50 | Iter: 153600 | Total Loss: 0.002964 | Recon Loss: 0.002500 | Commit Loss: 0.000929 | Perplexity: 954.375589
2025-09-14 18:20:09,868 Stage: Train 0.5 | Epoch: 50 | Iter: 153800 | Total Loss: 0.003067 | Recon Loss: 0.002598 | Commit Loss: 0.000937 | Perplexity: 953.927379
2025-09-14 18:20:18,613 Stage: Train 0.5 | Epoch: 50 | Iter: 154000 | Total Loss: 0.002989 | Recon Loss: 0.002526 | Commit Loss: 0.000926 | Perplexity: 950.988365
2025-09-14 18:20:27,356 Stage: Train 0.5 | Epoch: 50 | Iter: 154200 | Total Loss: 0.002982 | Recon Loss: 0.002523 | Commit Loss: 0.000918 | Perplexity: 949.581156
2025-09-14 18:20:36,091 Stage: Train 0.5 | Epoch: 50 | Iter: 154400 | Total Loss: 0.002971 | Recon Loss: 0.002506 | Commit Loss: 0.000931 | Perplexity: 954.305553
2025-09-14 18:20:44,819 Stage: Train 0.5 | Epoch: 50 | Iter: 154600 | Total Loss: 0.003003 | Recon Loss: 0.002533 | Commit Loss: 0.000939 | Perplexity: 952.712901
2025-09-14 18:20:53,549 Stage: Train 0.5 | Epoch: 50 | Iter: 154800 | Total Loss: 0.003016 | Recon Loss: 0.002552 | Commit Loss: 0.000927 | Perplexity: 947.141978
Trainning Epoch:  31%|███       | 51/165 [1:52:53<4:11:57, 132.61s/it]2025-09-14 18:21:02,288 Stage: Train 0.5 | Epoch: 51 | Iter: 155000 | Total Loss: 0.002963 | Recon Loss: 0.002500 | Commit Loss: 0.000926 | Perplexity: 954.128147
2025-09-14 18:21:11,018 Stage: Train 0.5 | Epoch: 51 | Iter: 155200 | Total Loss: 0.002991 | Recon Loss: 0.002530 | Commit Loss: 0.000923 | Perplexity: 951.149512
2025-09-14 18:21:19,712 Stage: Train 0.5 | Epoch: 51 | Iter: 155400 | Total Loss: 0.003044 | Recon Loss: 0.002582 | Commit Loss: 0.000924 | Perplexity: 951.541750
2025-09-14 18:21:28,420 Stage: Train 0.5 | Epoch: 51 | Iter: 155600 | Total Loss: 0.002933 | Recon Loss: 0.002471 | Commit Loss: 0.000924 | Perplexity: 953.593016
2025-09-14 18:21:37,102 Stage: Train 0.5 | Epoch: 51 | Iter: 155800 | Total Loss: 0.003011 | Recon Loss: 0.002553 | Commit Loss: 0.000917 | Perplexity: 955.520022
2025-09-14 18:21:45,787 Stage: Train 0.5 | Epoch: 51 | Iter: 156000 | Total Loss: 0.002941 | Recon Loss: 0.002483 | Commit Loss: 0.000916 | Perplexity: 947.092614
2025-09-14 18:21:54,477 Stage: Train 0.5 | Epoch: 51 | Iter: 156200 | Total Loss: 0.002987 | Recon Loss: 0.002523 | Commit Loss: 0.000928 | Perplexity: 954.007631
2025-09-14 18:22:03,188 Stage: Train 0.5 | Epoch: 51 | Iter: 156400 | Total Loss: 0.003070 | Recon Loss: 0.002600 | Commit Loss: 0.000940 | Perplexity: 951.083342
2025-09-14 18:22:11,924 Stage: Train 0.5 | Epoch: 51 | Iter: 156600 | Total Loss: 0.002966 | Recon Loss: 0.002506 | Commit Loss: 0.000919 | Perplexity: 953.254962
2025-09-14 18:22:20,634 Stage: Train 0.5 | Epoch: 51 | Iter: 156800 | Total Loss: 0.003022 | Recon Loss: 0.002556 | Commit Loss: 0.000931 | Perplexity: 957.357037
2025-09-14 18:22:29,333 Stage: Train 0.5 | Epoch: 51 | Iter: 157000 | Total Loss: 0.002968 | Recon Loss: 0.002505 | Commit Loss: 0.000924 | Perplexity: 953.924923
2025-09-14 18:22:38,009 Stage: Train 0.5 | Epoch: 51 | Iter: 157200 | Total Loss: 0.003017 | Recon Loss: 0.002552 | Commit Loss: 0.000931 | Perplexity: 951.950781
2025-09-14 18:22:46,740 Stage: Train 0.5 | Epoch: 51 | Iter: 157400 | Total Loss: 0.002991 | Recon Loss: 0.002529 | Commit Loss: 0.000926 | Perplexity: 951.226817
2025-09-14 18:22:55,475 Stage: Train 0.5 | Epoch: 51 | Iter: 157600 | Total Loss: 0.002959 | Recon Loss: 0.002493 | Commit Loss: 0.000932 | Perplexity: 956.639327
2025-09-14 18:23:04,235 Stage: Train 0.5 | Epoch: 51 | Iter: 157800 | Total Loss: 0.002909 | Recon Loss: 0.002454 | Commit Loss: 0.000910 | Perplexity: 950.207657
Trainning Epoch:  32%|███▏      | 52/165 [1:55:05<4:09:35, 132.52s/it]2025-09-14 18:23:12,948 Stage: Train 0.5 | Epoch: 52 | Iter: 158000 | Total Loss: 0.003001 | Recon Loss: 0.002541 | Commit Loss: 0.000920 | Perplexity: 950.356951
2025-09-14 18:23:21,686 Stage: Train 0.5 | Epoch: 52 | Iter: 158200 | Total Loss: 0.002979 | Recon Loss: 0.002516 | Commit Loss: 0.000926 | Perplexity: 952.464292
2025-09-14 18:23:30,404 Stage: Train 0.5 | Epoch: 52 | Iter: 158400 | Total Loss: 0.002994 | Recon Loss: 0.002529 | Commit Loss: 0.000928 | Perplexity: 957.315930
2025-09-14 18:23:39,134 Stage: Train 0.5 | Epoch: 52 | Iter: 158600 | Total Loss: 0.002967 | Recon Loss: 0.002508 | Commit Loss: 0.000918 | Perplexity: 952.285637
2025-09-14 18:23:47,887 Stage: Train 0.5 | Epoch: 52 | Iter: 158800 | Total Loss: 0.003009 | Recon Loss: 0.002545 | Commit Loss: 0.000929 | Perplexity: 956.428112
2025-09-14 18:23:56,624 Stage: Train 0.5 | Epoch: 52 | Iter: 159000 | Total Loss: 0.002899 | Recon Loss: 0.002436 | Commit Loss: 0.000926 | Perplexity: 955.038199
2025-09-14 18:24:05,372 Stage: Train 0.5 | Epoch: 52 | Iter: 159200 | Total Loss: 0.002991 | Recon Loss: 0.002531 | Commit Loss: 0.000919 | Perplexity: 950.534171
2025-09-14 18:24:14,113 Stage: Train 0.5 | Epoch: 52 | Iter: 159400 | Total Loss: 0.002953 | Recon Loss: 0.002490 | Commit Loss: 0.000925 | Perplexity: 956.306087
2025-09-14 18:24:22,834 Stage: Train 0.5 | Epoch: 52 | Iter: 159600 | Total Loss: 0.003018 | Recon Loss: 0.002549 | Commit Loss: 0.000938 | Perplexity: 956.415013
2025-09-14 18:24:31,546 Stage: Train 0.5 | Epoch: 52 | Iter: 159800 | Total Loss: 0.002940 | Recon Loss: 0.002475 | Commit Loss: 0.000930 | Perplexity: 954.820294
2025-09-14 18:24:40,256 Stage: Train 0.5 | Epoch: 52 | Iter: 160000 | Total Loss: 0.002950 | Recon Loss: 0.002486 | Commit Loss: 0.000927 | Perplexity: 955.585501
2025-09-14 18:24:40,256 Saving model at iteration 160000
2025-09-14 18:24:40,523 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_53_step_160000
2025-09-14 18:24:40,756 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_53_step_160000/pytorch_model.bin
2025-09-14 18:24:41,126 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_53_step_160000/optimizer.bin
2025-09-14 18:24:41,127 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_53_step_160000/scheduler.bin
2025-09-14 18:24:41,128 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_53_step_160000/random_states_0.pkl
2025-09-14 18:24:50,332 Stage: Train 0.5 | Epoch: 52 | Iter: 160200 | Total Loss: 0.003063 | Recon Loss: 0.002593 | Commit Loss: 0.000939 | Perplexity: 954.148195
2025-09-14 18:24:59,052 Stage: Train 0.5 | Epoch: 52 | Iter: 160400 | Total Loss: 0.002949 | Recon Loss: 0.002484 | Commit Loss: 0.000930 | Perplexity: 954.150882
2025-09-14 18:25:07,777 Stage: Train 0.5 | Epoch: 52 | Iter: 160600 | Total Loss: 0.003007 | Recon Loss: 0.002546 | Commit Loss: 0.000921 | Perplexity: 946.562222
2025-09-14 18:25:16,512 Stage: Train 0.5 | Epoch: 52 | Iter: 160800 | Total Loss: 0.002990 | Recon Loss: 0.002521 | Commit Loss: 0.000938 | Perplexity: 953.510337
2025-09-14 18:25:25,236 Stage: Train 0.5 | Epoch: 52 | Iter: 161000 | Total Loss: 0.002932 | Recon Loss: 0.002465 | Commit Loss: 0.000934 | Perplexity: 953.907076
Trainning Epoch:  32%|███▏      | 53/165 [1:57:19<4:08:10, 132.95s/it]2025-09-14 18:25:33,975 Stage: Train 0.5 | Epoch: 53 | Iter: 161200 | Total Loss: 0.002902 | Recon Loss: 0.002436 | Commit Loss: 0.000933 | Perplexity: 953.475794
2025-09-14 18:25:42,701 Stage: Train 0.5 | Epoch: 53 | Iter: 161400 | Total Loss: 0.003023 | Recon Loss: 0.002562 | Commit Loss: 0.000923 | Perplexity: 954.406297
2025-09-14 18:25:51,432 Stage: Train 0.5 | Epoch: 53 | Iter: 161600 | Total Loss: 0.002956 | Recon Loss: 0.002490 | Commit Loss: 0.000932 | Perplexity: 955.805255
2025-09-14 18:26:00,161 Stage: Train 0.5 | Epoch: 53 | Iter: 161800 | Total Loss: 0.002915 | Recon Loss: 0.002453 | Commit Loss: 0.000922 | Perplexity: 955.735789
2025-09-14 18:26:08,890 Stage: Train 0.5 | Epoch: 53 | Iter: 162000 | Total Loss: 0.002954 | Recon Loss: 0.002493 | Commit Loss: 0.000921 | Perplexity: 952.825135
2025-09-14 18:26:17,599 Stage: Train 0.5 | Epoch: 53 | Iter: 162200 | Total Loss: 0.002919 | Recon Loss: 0.002454 | Commit Loss: 0.000929 | Perplexity: 954.167213
2025-09-14 18:26:26,346 Stage: Train 0.5 | Epoch: 53 | Iter: 162400 | Total Loss: 0.003030 | Recon Loss: 0.002570 | Commit Loss: 0.000920 | Perplexity: 950.806501
2025-09-14 18:26:35,082 Stage: Train 0.5 | Epoch: 53 | Iter: 162600 | Total Loss: 0.002887 | Recon Loss: 0.002427 | Commit Loss: 0.000921 | Perplexity: 953.584186
2025-09-14 18:26:43,809 Stage: Train 0.5 | Epoch: 53 | Iter: 162800 | Total Loss: 0.003006 | Recon Loss: 0.002539 | Commit Loss: 0.000934 | Perplexity: 954.974938
2025-09-14 18:26:52,526 Stage: Train 0.5 | Epoch: 53 | Iter: 163000 | Total Loss: 0.002931 | Recon Loss: 0.002471 | Commit Loss: 0.000920 | Perplexity: 955.096721
2025-09-14 18:27:01,232 Stage: Train 0.5 | Epoch: 53 | Iter: 163200 | Total Loss: 0.002987 | Recon Loss: 0.002522 | Commit Loss: 0.000931 | Perplexity: 953.678617
2025-09-14 18:27:09,927 Stage: Train 0.5 | Epoch: 53 | Iter: 163400 | Total Loss: 0.003045 | Recon Loss: 0.002580 | Commit Loss: 0.000930 | Perplexity: 952.140955
2025-09-14 18:27:18,624 Stage: Train 0.5 | Epoch: 53 | Iter: 163600 | Total Loss: 0.002954 | Recon Loss: 0.002494 | Commit Loss: 0.000922 | Perplexity: 955.504505
2025-09-14 18:27:27,352 Stage: Train 0.5 | Epoch: 53 | Iter: 163800 | Total Loss: 0.003009 | Recon Loss: 0.002555 | Commit Loss: 0.000909 | Perplexity: 942.340174
2025-09-14 18:27:36,068 Stage: Train 0.5 | Epoch: 53 | Iter: 164000 | Total Loss: 0.002955 | Recon Loss: 0.002491 | Commit Loss: 0.000929 | Perplexity: 951.064499
Trainning Epoch:  33%|███▎      | 54/165 [1:59:31<4:05:42, 132.81s/it]2025-09-14 18:27:44,766 Stage: Train 0.5 | Epoch: 54 | Iter: 164200 | Total Loss: 0.003010 | Recon Loss: 0.002549 | Commit Loss: 0.000922 | Perplexity: 947.080940
2025-09-14 18:27:53,471 Stage: Train 0.5 | Epoch: 54 | Iter: 164400 | Total Loss: 0.002926 | Recon Loss: 0.002466 | Commit Loss: 0.000920 | Perplexity: 950.100358
2025-09-14 18:28:02,169 Stage: Train 0.5 | Epoch: 54 | Iter: 164600 | Total Loss: 0.002959 | Recon Loss: 0.002502 | Commit Loss: 0.000913 | Perplexity: 949.714695
2025-09-14 18:28:10,872 Stage: Train 0.5 | Epoch: 54 | Iter: 164800 | Total Loss: 0.002903 | Recon Loss: 0.002443 | Commit Loss: 0.000920 | Perplexity: 955.113213
2025-09-14 18:28:19,556 Stage: Train 0.5 | Epoch: 54 | Iter: 165000 | Total Loss: 0.002951 | Recon Loss: 0.002491 | Commit Loss: 0.000921 | Perplexity: 952.209904
2025-09-14 18:28:28,244 Stage: Train 0.5 | Epoch: 54 | Iter: 165200 | Total Loss: 0.003007 | Recon Loss: 0.002545 | Commit Loss: 0.000925 | Perplexity: 952.929253
2025-09-14 18:28:36,943 Stage: Train 0.5 | Epoch: 54 | Iter: 165400 | Total Loss: 0.003005 | Recon Loss: 0.002545 | Commit Loss: 0.000919 | Perplexity: 947.844687
2025-09-14 18:28:45,645 Stage: Train 0.5 | Epoch: 54 | Iter: 165600 | Total Loss: 0.002974 | Recon Loss: 0.002512 | Commit Loss: 0.000924 | Perplexity: 954.523881
2025-09-14 18:28:54,342 Stage: Train 0.5 | Epoch: 54 | Iter: 165800 | Total Loss: 0.002936 | Recon Loss: 0.002473 | Commit Loss: 0.000924 | Perplexity: 950.270346
2025-09-14 18:29:03,065 Stage: Train 0.5 | Epoch: 54 | Iter: 166000 | Total Loss: 0.002967 | Recon Loss: 0.002510 | Commit Loss: 0.000914 | Perplexity: 954.879738
2025-09-14 18:29:11,769 Stage: Train 0.5 | Epoch: 54 | Iter: 166200 | Total Loss: 0.002933 | Recon Loss: 0.002470 | Commit Loss: 0.000925 | Perplexity: 955.074799
2025-09-14 18:29:20,470 Stage: Train 0.5 | Epoch: 54 | Iter: 166400 | Total Loss: 0.002981 | Recon Loss: 0.002521 | Commit Loss: 0.000919 | Perplexity: 952.143826
2025-09-14 18:29:29,188 Stage: Train 0.5 | Epoch: 54 | Iter: 166600 | Total Loss: 0.002998 | Recon Loss: 0.002537 | Commit Loss: 0.000923 | Perplexity: 952.115457
2025-09-14 18:29:37,920 Stage: Train 0.5 | Epoch: 54 | Iter: 166800 | Total Loss: 0.002904 | Recon Loss: 0.002444 | Commit Loss: 0.000920 | Perplexity: 952.491286
2025-09-14 18:29:46,645 Stage: Train 0.5 | Epoch: 54 | Iter: 167000 | Total Loss: 0.002960 | Recon Loss: 0.002491 | Commit Loss: 0.000939 | Perplexity: 954.930802
Trainning Epoch:  33%|███▎      | 55/165 [2:01:44<4:03:09, 132.63s/it]2025-09-14 18:29:55,342 Stage: Train 0.5 | Epoch: 55 | Iter: 167200 | Total Loss: 0.002973 | Recon Loss: 0.002511 | Commit Loss: 0.000925 | Perplexity: 953.059454
2025-09-14 18:30:04,060 Stage: Train 0.5 | Epoch: 55 | Iter: 167400 | Total Loss: 0.003035 | Recon Loss: 0.002573 | Commit Loss: 0.000923 | Perplexity: 956.018981
2025-09-14 18:30:12,784 Stage: Train 0.5 | Epoch: 55 | Iter: 167600 | Total Loss: 0.002917 | Recon Loss: 0.002458 | Commit Loss: 0.000919 | Perplexity: 949.581644
2025-09-14 18:30:21,505 Stage: Train 0.5 | Epoch: 55 | Iter: 167800 | Total Loss: 0.002983 | Recon Loss: 0.002519 | Commit Loss: 0.000928 | Perplexity: 954.997117
2025-09-14 18:30:30,260 Stage: Train 0.5 | Epoch: 55 | Iter: 168000 | Total Loss: 0.002955 | Recon Loss: 0.002496 | Commit Loss: 0.000916 | Perplexity: 953.329357
2025-09-14 18:30:39,005 Stage: Train 0.5 | Epoch: 55 | Iter: 168200 | Total Loss: 0.002976 | Recon Loss: 0.002512 | Commit Loss: 0.000928 | Perplexity: 952.470310
2025-09-14 18:30:47,796 Stage: Train 0.5 | Epoch: 55 | Iter: 168400 | Total Loss: 0.002914 | Recon Loss: 0.002450 | Commit Loss: 0.000927 | Perplexity: 951.268514
2025-09-14 18:30:56,565 Stage: Train 0.5 | Epoch: 55 | Iter: 168600 | Total Loss: 0.002965 | Recon Loss: 0.002496 | Commit Loss: 0.000938 | Perplexity: 957.819001
2025-09-14 18:31:05,300 Stage: Train 0.5 | Epoch: 55 | Iter: 168800 | Total Loss: 0.002965 | Recon Loss: 0.002501 | Commit Loss: 0.000927 | Perplexity: 951.165445
2025-09-14 18:31:14,047 Stage: Train 0.5 | Epoch: 55 | Iter: 169000 | Total Loss: 0.002914 | Recon Loss: 0.002455 | Commit Loss: 0.000918 | Perplexity: 948.336275
2025-09-14 18:31:22,770 Stage: Train 0.5 | Epoch: 55 | Iter: 169200 | Total Loss: 0.002880 | Recon Loss: 0.002415 | Commit Loss: 0.000928 | Perplexity: 954.350158
2025-09-14 18:31:31,498 Stage: Train 0.5 | Epoch: 55 | Iter: 169400 | Total Loss: 0.002965 | Recon Loss: 0.002506 | Commit Loss: 0.000918 | Perplexity: 955.451756
2025-09-14 18:31:40,225 Stage: Train 0.5 | Epoch: 55 | Iter: 169600 | Total Loss: 0.002908 | Recon Loss: 0.002451 | Commit Loss: 0.000916 | Perplexity: 948.492364
2025-09-14 18:31:48,928 Stage: Train 0.5 | Epoch: 55 | Iter: 169800 | Total Loss: 0.002947 | Recon Loss: 0.002487 | Commit Loss: 0.000920 | Perplexity: 953.417355
2025-09-14 18:31:57,663 Stage: Train 0.5 | Epoch: 55 | Iter: 170000 | Total Loss: 0.002916 | Recon Loss: 0.002450 | Commit Loss: 0.000932 | Perplexity: 957.054150
Trainning Epoch:  34%|███▍      | 56/165 [2:03:56<4:00:58, 132.64s/it]2025-09-14 18:32:06,355 Stage: Train 0.5 | Epoch: 56 | Iter: 170200 | Total Loss: 0.002967 | Recon Loss: 0.002500 | Commit Loss: 0.000933 | Perplexity: 948.187257
2025-09-14 18:32:15,058 Stage: Train 0.5 | Epoch: 56 | Iter: 170400 | Total Loss: 0.002926 | Recon Loss: 0.002465 | Commit Loss: 0.000921 | Perplexity: 956.696134
2025-09-14 18:32:23,741 Stage: Train 0.5 | Epoch: 56 | Iter: 170600 | Total Loss: 0.002977 | Recon Loss: 0.002514 | Commit Loss: 0.000926 | Perplexity: 952.460515
2025-09-14 18:32:32,419 Stage: Train 0.5 | Epoch: 56 | Iter: 170800 | Total Loss: 0.002872 | Recon Loss: 0.002416 | Commit Loss: 0.000912 | Perplexity: 951.717383
2025-09-14 18:32:41,111 Stage: Train 0.5 | Epoch: 56 | Iter: 171000 | Total Loss: 0.002910 | Recon Loss: 0.002443 | Commit Loss: 0.000933 | Perplexity: 957.272699
2025-09-14 18:32:49,811 Stage: Train 0.5 | Epoch: 56 | Iter: 171200 | Total Loss: 0.002890 | Recon Loss: 0.002426 | Commit Loss: 0.000929 | Perplexity: 953.550502
2025-09-14 18:32:58,526 Stage: Train 0.5 | Epoch: 56 | Iter: 171400 | Total Loss: 0.002951 | Recon Loss: 0.002488 | Commit Loss: 0.000927 | Perplexity: 953.046559
2025-09-14 18:33:07,276 Stage: Train 0.5 | Epoch: 56 | Iter: 171600 | Total Loss: 0.002884 | Recon Loss: 0.002417 | Commit Loss: 0.000933 | Perplexity: 953.000739
2025-09-14 18:33:16,052 Stage: Train 0.5 | Epoch: 56 | Iter: 171800 | Total Loss: 0.002895 | Recon Loss: 0.002432 | Commit Loss: 0.000927 | Perplexity: 958.740181
2025-09-14 18:33:24,819 Stage: Train 0.5 | Epoch: 56 | Iter: 172000 | Total Loss: 0.002975 | Recon Loss: 0.002515 | Commit Loss: 0.000922 | Perplexity: 954.417283
2025-09-14 18:33:33,580 Stage: Train 0.5 | Epoch: 56 | Iter: 172200 | Total Loss: 0.002878 | Recon Loss: 0.002412 | Commit Loss: 0.000931 | Perplexity: 954.504406
2025-09-14 18:33:42,319 Stage: Train 0.5 | Epoch: 56 | Iter: 172400 | Total Loss: 0.002917 | Recon Loss: 0.002451 | Commit Loss: 0.000931 | Perplexity: 958.680617
2025-09-14 18:33:51,084 Stage: Train 0.5 | Epoch: 56 | Iter: 172600 | Total Loss: 0.002925 | Recon Loss: 0.002466 | Commit Loss: 0.000918 | Perplexity: 953.227703
2025-09-14 18:33:59,840 Stage: Train 0.5 | Epoch: 56 | Iter: 172800 | Total Loss: 0.002929 | Recon Loss: 0.002468 | Commit Loss: 0.000923 | Perplexity: 951.288966
2025-09-14 18:34:08,611 Stage: Train 0.5 | Epoch: 56 | Iter: 173000 | Total Loss: 0.002884 | Recon Loss: 0.002426 | Commit Loss: 0.000915 | Perplexity: 949.640881
Trainning Epoch:  35%|███▍      | 57/165 [2:06:09<3:58:45, 132.64s/it]2025-09-14 18:34:17,372 Stage: Train 0.5 | Epoch: 57 | Iter: 173200 | Total Loss: 0.002918 | Recon Loss: 0.002456 | Commit Loss: 0.000925 | Perplexity: 952.757096
2025-09-14 18:34:26,141 Stage: Train 0.5 | Epoch: 57 | Iter: 173400 | Total Loss: 0.002863 | Recon Loss: 0.002398 | Commit Loss: 0.000930 | Perplexity: 960.394860
2025-09-14 18:34:34,887 Stage: Train 0.5 | Epoch: 57 | Iter: 173600 | Total Loss: 0.002892 | Recon Loss: 0.002430 | Commit Loss: 0.000924 | Perplexity: 951.947979
2025-09-14 18:34:43,676 Stage: Train 0.5 | Epoch: 57 | Iter: 173800 | Total Loss: 0.003011 | Recon Loss: 0.002546 | Commit Loss: 0.000930 | Perplexity: 953.911859
2025-09-14 18:34:52,465 Stage: Train 0.5 | Epoch: 57 | Iter: 174000 | Total Loss: 0.002872 | Recon Loss: 0.002411 | Commit Loss: 0.000921 | Perplexity: 951.692430
2025-09-14 18:35:01,254 Stage: Train 0.5 | Epoch: 57 | Iter: 174200 | Total Loss: 0.002996 | Recon Loss: 0.002544 | Commit Loss: 0.000903 | Perplexity: 949.319249
2025-09-14 18:35:10,056 Stage: Train 0.5 | Epoch: 57 | Iter: 174400 | Total Loss: 0.002936 | Recon Loss: 0.002476 | Commit Loss: 0.000919 | Perplexity: 955.727894
2025-09-14 18:35:18,837 Stage: Train 0.5 | Epoch: 57 | Iter: 174600 | Total Loss: 0.002909 | Recon Loss: 0.002440 | Commit Loss: 0.000937 | Perplexity: 957.302324
2025-09-14 18:35:27,585 Stage: Train 0.5 | Epoch: 57 | Iter: 174800 | Total Loss: 0.002924 | Recon Loss: 0.002461 | Commit Loss: 0.000926 | Perplexity: 955.175842
2025-09-14 18:35:36,327 Stage: Train 0.5 | Epoch: 57 | Iter: 175000 | Total Loss: 0.002967 | Recon Loss: 0.002503 | Commit Loss: 0.000928 | Perplexity: 949.921267
2025-09-14 18:35:45,117 Stage: Train 0.5 | Epoch: 57 | Iter: 175200 | Total Loss: 0.002913 | Recon Loss: 0.002450 | Commit Loss: 0.000927 | Perplexity: 955.939391
2025-09-14 18:35:53,888 Stage: Train 0.5 | Epoch: 57 | Iter: 175400 | Total Loss: 0.002912 | Recon Loss: 0.002453 | Commit Loss: 0.000918 | Perplexity: 950.369273
2025-09-14 18:36:02,636 Stage: Train 0.5 | Epoch: 57 | Iter: 175600 | Total Loss: 0.002945 | Recon Loss: 0.002482 | Commit Loss: 0.000926 | Perplexity: 950.639022
2025-09-14 18:36:11,404 Stage: Train 0.5 | Epoch: 57 | Iter: 175800 | Total Loss: 0.002925 | Recon Loss: 0.002460 | Commit Loss: 0.000931 | Perplexity: 955.431226
2025-09-14 18:36:20,180 Stage: Train 0.5 | Epoch: 57 | Iter: 176000 | Total Loss: 0.002910 | Recon Loss: 0.002452 | Commit Loss: 0.000918 | Perplexity: 950.805707
2025-09-14 18:36:28,958 Stage: Train 0.5 | Epoch: 57 | Iter: 176200 | Total Loss: 0.002911 | Recon Loss: 0.002447 | Commit Loss: 0.000928 | Perplexity: 950.389757
Trainning Epoch:  35%|███▌      | 58/165 [2:08:22<3:56:53, 132.83s/it]2025-09-14 18:36:37,732 Stage: Train 0.5 | Epoch: 58 | Iter: 176400 | Total Loss: 0.002907 | Recon Loss: 0.002448 | Commit Loss: 0.000918 | Perplexity: 951.637600
2025-09-14 18:36:46,493 Stage: Train 0.5 | Epoch: 58 | Iter: 176600 | Total Loss: 0.002906 | Recon Loss: 0.002447 | Commit Loss: 0.000919 | Perplexity: 953.532000
2025-09-14 18:36:55,257 Stage: Train 0.5 | Epoch: 58 | Iter: 176800 | Total Loss: 0.002862 | Recon Loss: 0.002404 | Commit Loss: 0.000916 | Perplexity: 956.732193
2025-09-14 18:37:04,016 Stage: Train 0.5 | Epoch: 58 | Iter: 177000 | Total Loss: 0.002924 | Recon Loss: 0.002460 | Commit Loss: 0.000929 | Perplexity: 948.538070
2025-09-14 18:37:12,754 Stage: Train 0.5 | Epoch: 58 | Iter: 177200 | Total Loss: 0.003057 | Recon Loss: 0.002595 | Commit Loss: 0.000924 | Perplexity: 953.829140
2025-09-14 18:37:21,520 Stage: Train 0.5 | Epoch: 58 | Iter: 177400 | Total Loss: 0.002953 | Recon Loss: 0.002494 | Commit Loss: 0.000919 | Perplexity: 949.062461
2025-09-14 18:37:30,279 Stage: Train 0.5 | Epoch: 58 | Iter: 177600 | Total Loss: 0.002870 | Recon Loss: 0.002408 | Commit Loss: 0.000925 | Perplexity: 956.509437
2025-09-14 18:37:39,021 Stage: Train 0.5 | Epoch: 58 | Iter: 177800 | Total Loss: 0.002844 | Recon Loss: 0.002380 | Commit Loss: 0.000928 | Perplexity: 954.671241
2025-09-14 18:37:47,801 Stage: Train 0.5 | Epoch: 58 | Iter: 178000 | Total Loss: 0.002939 | Recon Loss: 0.002473 | Commit Loss: 0.000932 | Perplexity: 954.057665
2025-09-14 18:37:56,560 Stage: Train 0.5 | Epoch: 58 | Iter: 178200 | Total Loss: 0.002952 | Recon Loss: 0.002487 | Commit Loss: 0.000930 | Perplexity: 954.834637
2025-09-14 18:38:05,320 Stage: Train 0.5 | Epoch: 58 | Iter: 178400 | Total Loss: 0.002942 | Recon Loss: 0.002478 | Commit Loss: 0.000926 | Perplexity: 952.721447
2025-09-14 18:38:14,063 Stage: Train 0.5 | Epoch: 58 | Iter: 178600 | Total Loss: 0.002881 | Recon Loss: 0.002421 | Commit Loss: 0.000921 | Perplexity: 954.352873
2025-09-14 18:38:22,797 Stage: Train 0.5 | Epoch: 58 | Iter: 178800 | Total Loss: 0.002909 | Recon Loss: 0.002447 | Commit Loss: 0.000925 | Perplexity: 950.706809
2025-09-14 18:38:31,528 Stage: Train 0.5 | Epoch: 58 | Iter: 179000 | Total Loss: 0.002913 | Recon Loss: 0.002451 | Commit Loss: 0.000924 | Perplexity: 952.614797
2025-09-14 18:38:40,299 Stage: Train 0.5 | Epoch: 58 | Iter: 179200 | Total Loss: 0.002893 | Recon Loss: 0.002429 | Commit Loss: 0.000929 | Perplexity: 954.500950
Trainning Epoch:  36%|███▌      | 59/165 [2:10:35<3:54:45, 132.88s/it]2025-09-14 18:38:49,073 Stage: Train 0.5 | Epoch: 59 | Iter: 179400 | Total Loss: 0.002904 | Recon Loss: 0.002445 | Commit Loss: 0.000918 | Perplexity: 952.967560
2025-09-14 18:38:57,871 Stage: Train 0.5 | Epoch: 59 | Iter: 179600 | Total Loss: 0.002885 | Recon Loss: 0.002422 | Commit Loss: 0.000926 | Perplexity: 952.929316
2025-09-14 18:39:06,650 Stage: Train 0.5 | Epoch: 59 | Iter: 179800 | Total Loss: 0.002825 | Recon Loss: 0.002360 | Commit Loss: 0.000930 | Perplexity: 956.333405
2025-09-14 18:39:15,384 Stage: Train 0.5 | Epoch: 59 | Iter: 180000 | Total Loss: 0.002912 | Recon Loss: 0.002441 | Commit Loss: 0.000942 | Perplexity: 954.394318
2025-09-14 18:39:15,385 Saving model at iteration 180000
2025-09-14 18:39:15,541 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_60_step_180000
2025-09-14 18:39:15,784 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_60_step_180000/pytorch_model.bin
2025-09-14 18:39:16,166 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_60_step_180000/optimizer.bin
2025-09-14 18:39:16,166 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_60_step_180000/scheduler.bin
2025-09-14 18:39:16,167 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_60_step_180000/random_states_0.pkl
2025-09-14 18:39:25,007 Stage: Train 0.5 | Epoch: 59 | Iter: 180200 | Total Loss: 0.002934 | Recon Loss: 0.002472 | Commit Loss: 0.000923 | Perplexity: 949.676979
2025-09-14 18:39:33,785 Stage: Train 0.5 | Epoch: 59 | Iter: 180400 | Total Loss: 0.002882 | Recon Loss: 0.002419 | Commit Loss: 0.000926 | Perplexity: 950.749097
2025-09-14 18:39:42,561 Stage: Train 0.5 | Epoch: 59 | Iter: 180600 | Total Loss: 0.002888 | Recon Loss: 0.002423 | Commit Loss: 0.000931 | Perplexity: 952.941318
2025-09-14 18:39:51,310 Stage: Train 0.5 | Epoch: 59 | Iter: 180800 | Total Loss: 0.002882 | Recon Loss: 0.002417 | Commit Loss: 0.000929 | Perplexity: 951.068696
2025-09-14 18:40:00,057 Stage: Train 0.5 | Epoch: 59 | Iter: 181000 | Total Loss: 0.002957 | Recon Loss: 0.002490 | Commit Loss: 0.000934 | Perplexity: 952.425115
2025-09-14 18:40:08,798 Stage: Train 0.5 | Epoch: 59 | Iter: 181200 | Total Loss: 0.002855 | Recon Loss: 0.002393 | Commit Loss: 0.000924 | Perplexity: 951.691480
2025-09-14 18:40:17,580 Stage: Train 0.5 | Epoch: 59 | Iter: 181400 | Total Loss: 0.002957 | Recon Loss: 0.002501 | Commit Loss: 0.000912 | Perplexity: 951.182484
2025-09-14 18:40:26,380 Stage: Train 0.5 | Epoch: 59 | Iter: 181600 | Total Loss: 0.002881 | Recon Loss: 0.002422 | Commit Loss: 0.000918 | Perplexity: 952.045598
2025-09-14 18:40:35,159 Stage: Train 0.5 | Epoch: 59 | Iter: 181800 | Total Loss: 0.002888 | Recon Loss: 0.002421 | Commit Loss: 0.000933 | Perplexity: 954.818077
2025-09-14 18:40:43,942 Stage: Train 0.5 | Epoch: 59 | Iter: 182000 | Total Loss: 0.002926 | Recon Loss: 0.002466 | Commit Loss: 0.000921 | Perplexity: 952.510843
2025-09-14 18:40:52,705 Stage: Train 0.5 | Epoch: 59 | Iter: 182200 | Total Loss: 0.002888 | Recon Loss: 0.002423 | Commit Loss: 0.000928 | Perplexity: 951.982031
Trainning Epoch:  36%|███▋      | 60/165 [2:12:49<3:53:10, 133.24s/it]2025-09-14 18:41:01,504 Stage: Train 0.5 | Epoch: 60 | Iter: 182400 | Total Loss: 0.002881 | Recon Loss: 0.002415 | Commit Loss: 0.000932 | Perplexity: 954.633212
2025-09-14 18:41:10,276 Stage: Train 0.5 | Epoch: 60 | Iter: 182600 | Total Loss: 0.002902 | Recon Loss: 0.002439 | Commit Loss: 0.000926 | Perplexity: 951.719724
2025-09-14 18:41:19,136 Stage: Train 0.5 | Epoch: 60 | Iter: 182800 | Total Loss: 0.002890 | Recon Loss: 0.002434 | Commit Loss: 0.000913 | Perplexity: 950.521515
2025-09-14 18:41:27,854 Stage: Train 0.5 | Epoch: 60 | Iter: 183000 | Total Loss: 0.002812 | Recon Loss: 0.002352 | Commit Loss: 0.000920 | Perplexity: 954.285562
2025-09-14 18:41:36,597 Stage: Train 0.5 | Epoch: 60 | Iter: 183200 | Total Loss: 0.002915 | Recon Loss: 0.002450 | Commit Loss: 0.000930 | Perplexity: 953.379957
2025-09-14 18:41:45,341 Stage: Train 0.5 | Epoch: 60 | Iter: 183400 | Total Loss: 0.002860 | Recon Loss: 0.002401 | Commit Loss: 0.000919 | Perplexity: 955.629957
2025-09-14 18:41:54,080 Stage: Train 0.5 | Epoch: 60 | Iter: 183600 | Total Loss: 0.002890 | Recon Loss: 0.002429 | Commit Loss: 0.000922 | Perplexity: 948.337253
2025-09-14 18:42:02,824 Stage: Train 0.5 | Epoch: 60 | Iter: 183800 | Total Loss: 0.002963 | Recon Loss: 0.002497 | Commit Loss: 0.000932 | Perplexity: 951.137916
2025-09-14 18:42:11,577 Stage: Train 0.5 | Epoch: 60 | Iter: 184000 | Total Loss: 0.002884 | Recon Loss: 0.002424 | Commit Loss: 0.000920 | Perplexity: 950.346853
2025-09-14 18:42:20,322 Stage: Train 0.5 | Epoch: 60 | Iter: 184200 | Total Loss: 0.002828 | Recon Loss: 0.002363 | Commit Loss: 0.000931 | Perplexity: 955.163168
2025-09-14 18:42:29,056 Stage: Train 0.5 | Epoch: 60 | Iter: 184400 | Total Loss: 0.002865 | Recon Loss: 0.002397 | Commit Loss: 0.000936 | Perplexity: 956.556209
2025-09-14 18:42:37,770 Stage: Train 0.5 | Epoch: 60 | Iter: 184600 | Total Loss: 0.002888 | Recon Loss: 0.002423 | Commit Loss: 0.000930 | Perplexity: 948.034278
2025-09-14 18:42:46,481 Stage: Train 0.5 | Epoch: 60 | Iter: 184800 | Total Loss: 0.002884 | Recon Loss: 0.002425 | Commit Loss: 0.000918 | Perplexity: 955.486662
2025-09-14 18:42:55,212 Stage: Train 0.5 | Epoch: 60 | Iter: 185000 | Total Loss: 0.002888 | Recon Loss: 0.002423 | Commit Loss: 0.000930 | Perplexity: 952.417538
2025-09-14 18:43:03,928 Stage: Train 0.5 | Epoch: 60 | Iter: 185200 | Total Loss: 0.002984 | Recon Loss: 0.002518 | Commit Loss: 0.000934 | Perplexity: 953.997174
Trainning Epoch:  37%|███▋      | 61/165 [2:15:02<3:50:45, 133.13s/it]2025-09-14 18:43:12,648 Stage: Train 0.5 | Epoch: 61 | Iter: 185400 | Total Loss: 0.002879 | Recon Loss: 0.002423 | Commit Loss: 0.000911 | Perplexity: 947.349757
2025-09-14 18:43:21,360 Stage: Train 0.5 | Epoch: 61 | Iter: 185600 | Total Loss: 0.002871 | Recon Loss: 0.002412 | Commit Loss: 0.000918 | Perplexity: 950.376109
2025-09-14 18:43:30,070 Stage: Train 0.5 | Epoch: 61 | Iter: 185800 | Total Loss: 0.002871 | Recon Loss: 0.002407 | Commit Loss: 0.000928 | Perplexity: 951.022129
2025-09-14 18:43:38,795 Stage: Train 0.5 | Epoch: 61 | Iter: 186000 | Total Loss: 0.002907 | Recon Loss: 0.002442 | Commit Loss: 0.000930 | Perplexity: 953.114962
2025-09-14 18:43:47,512 Stage: Train 0.5 | Epoch: 61 | Iter: 186200 | Total Loss: 0.002865 | Recon Loss: 0.002403 | Commit Loss: 0.000924 | Perplexity: 953.452529
2025-09-14 18:43:56,204 Stage: Train 0.5 | Epoch: 61 | Iter: 186400 | Total Loss: 0.002876 | Recon Loss: 0.002415 | Commit Loss: 0.000921 | Perplexity: 952.835747
2025-09-14 18:44:04,916 Stage: Train 0.5 | Epoch: 61 | Iter: 186600 | Total Loss: 0.002884 | Recon Loss: 0.002426 | Commit Loss: 0.000916 | Perplexity: 946.044613
2025-09-14 18:44:13,590 Stage: Train 0.5 | Epoch: 61 | Iter: 186800 | Total Loss: 0.002866 | Recon Loss: 0.002408 | Commit Loss: 0.000915 | Perplexity: 948.456288
2025-09-14 18:44:22,296 Stage: Train 0.5 | Epoch: 61 | Iter: 187000 | Total Loss: 0.002846 | Recon Loss: 0.002381 | Commit Loss: 0.000930 | Perplexity: 951.897885
2025-09-14 18:44:31,044 Stage: Train 0.5 | Epoch: 61 | Iter: 187200 | Total Loss: 0.002977 | Recon Loss: 0.002511 | Commit Loss: 0.000932 | Perplexity: 953.224034
2025-09-14 18:44:39,742 Stage: Train 0.5 | Epoch: 61 | Iter: 187400 | Total Loss: 0.002936 | Recon Loss: 0.002481 | Commit Loss: 0.000909 | Perplexity: 951.487133
2025-09-14 18:44:48,464 Stage: Train 0.5 | Epoch: 61 | Iter: 187600 | Total Loss: 0.002896 | Recon Loss: 0.002437 | Commit Loss: 0.000918 | Perplexity: 953.231888
2025-09-14 18:44:57,200 Stage: Train 0.5 | Epoch: 61 | Iter: 187800 | Total Loss: 0.002891 | Recon Loss: 0.002427 | Commit Loss: 0.000928 | Perplexity: 951.252524
2025-09-14 18:45:05,924 Stage: Train 0.5 | Epoch: 61 | Iter: 188000 | Total Loss: 0.002917 | Recon Loss: 0.002458 | Commit Loss: 0.000920 | Perplexity: 952.977268
2025-09-14 18:45:14,646 Stage: Train 0.5 | Epoch: 61 | Iter: 188200 | Total Loss: 0.002860 | Recon Loss: 0.002396 | Commit Loss: 0.000927 | Perplexity: 953.221216
Trainning Epoch:  38%|███▊      | 62/165 [2:17:14<3:48:08, 132.90s/it]2025-09-14 18:45:23,384 Stage: Train 0.5 | Epoch: 62 | Iter: 188400 | Total Loss: 0.002866 | Recon Loss: 0.002405 | Commit Loss: 0.000922 | Perplexity: 951.659291
2025-09-14 18:45:32,113 Stage: Train 0.5 | Epoch: 62 | Iter: 188600 | Total Loss: 0.002844 | Recon Loss: 0.002383 | Commit Loss: 0.000923 | Perplexity: 960.177913
2025-09-14 18:45:40,877 Stage: Train 0.5 | Epoch: 62 | Iter: 188800 | Total Loss: 0.002961 | Recon Loss: 0.002509 | Commit Loss: 0.000905 | Perplexity: 945.603676
2025-09-14 18:45:49,618 Stage: Train 0.5 | Epoch: 62 | Iter: 189000 | Total Loss: 0.002861 | Recon Loss: 0.002400 | Commit Loss: 0.000922 | Perplexity: 950.533817
2025-09-14 18:45:58,346 Stage: Train 0.5 | Epoch: 62 | Iter: 189200 | Total Loss: 0.002899 | Recon Loss: 0.002433 | Commit Loss: 0.000932 | Perplexity: 951.926168
2025-09-14 18:46:07,074 Stage: Train 0.5 | Epoch: 62 | Iter: 189400 | Total Loss: 0.002954 | Recon Loss: 0.002494 | Commit Loss: 0.000920 | Perplexity: 955.320586
2025-09-14 18:46:15,830 Stage: Train 0.5 | Epoch: 62 | Iter: 189600 | Total Loss: 0.002871 | Recon Loss: 0.002414 | Commit Loss: 0.000915 | Perplexity: 947.706339
2025-09-14 18:46:24,556 Stage: Train 0.5 | Epoch: 62 | Iter: 189800 | Total Loss: 0.002811 | Recon Loss: 0.002351 | Commit Loss: 0.000921 | Perplexity: 957.452425
2025-09-14 18:46:33,278 Stage: Train 0.5 | Epoch: 62 | Iter: 190000 | Total Loss: 0.002928 | Recon Loss: 0.002466 | Commit Loss: 0.000923 | Perplexity: 953.049403
2025-09-14 18:46:42,028 Stage: Train 0.5 | Epoch: 62 | Iter: 190200 | Total Loss: 0.002883 | Recon Loss: 0.002427 | Commit Loss: 0.000912 | Perplexity: 952.054370
2025-09-14 18:46:50,746 Stage: Train 0.5 | Epoch: 62 | Iter: 190400 | Total Loss: 0.002842 | Recon Loss: 0.002383 | Commit Loss: 0.000916 | Perplexity: 951.705389
2025-09-14 18:46:59,440 Stage: Train 0.5 | Epoch: 62 | Iter: 190600 | Total Loss: 0.002835 | Recon Loss: 0.002372 | Commit Loss: 0.000926 | Perplexity: 955.326705
2025-09-14 18:47:08,172 Stage: Train 0.5 | Epoch: 62 | Iter: 190800 | Total Loss: 0.002818 | Recon Loss: 0.002363 | Commit Loss: 0.000910 | Perplexity: 952.129412
2025-09-14 18:47:16,880 Stage: Train 0.5 | Epoch: 62 | Iter: 191000 | Total Loss: 0.002904 | Recon Loss: 0.002444 | Commit Loss: 0.000920 | Perplexity: 952.484099
2025-09-14 18:47:25,580 Stage: Train 0.5 | Epoch: 62 | Iter: 191200 | Total Loss: 0.002859 | Recon Loss: 0.002396 | Commit Loss: 0.000926 | Perplexity: 951.425384
Trainning Epoch:  38%|███▊      | 63/165 [2:19:27<3:45:45, 132.80s/it]2025-09-14 18:47:34,292 Stage: Train 0.5 | Epoch: 63 | Iter: 191400 | Total Loss: 0.002879 | Recon Loss: 0.002426 | Commit Loss: 0.000906 | Perplexity: 953.442934
2025-09-14 18:47:43,004 Stage: Train 0.5 | Epoch: 63 | Iter: 191600 | Total Loss: 0.002873 | Recon Loss: 0.002416 | Commit Loss: 0.000914 | Perplexity: 951.368758
2025-09-14 18:47:51,712 Stage: Train 0.5 | Epoch: 63 | Iter: 191800 | Total Loss: 0.002815 | Recon Loss: 0.002356 | Commit Loss: 0.000919 | Perplexity: 958.007587
2025-09-14 18:48:00,436 Stage: Train 0.5 | Epoch: 63 | Iter: 192000 | Total Loss: 0.002989 | Recon Loss: 0.002529 | Commit Loss: 0.000919 | Perplexity: 953.112131
2025-09-14 18:48:09,157 Stage: Train 0.5 | Epoch: 63 | Iter: 192200 | Total Loss: 0.002821 | Recon Loss: 0.002360 | Commit Loss: 0.000922 | Perplexity: 948.537436
2025-09-14 18:48:17,853 Stage: Train 0.5 | Epoch: 63 | Iter: 192400 | Total Loss: 0.002891 | Recon Loss: 0.002429 | Commit Loss: 0.000922 | Perplexity: 954.122668
2025-09-14 18:48:26,569 Stage: Train 0.5 | Epoch: 63 | Iter: 192600 | Total Loss: 0.002817 | Recon Loss: 0.002360 | Commit Loss: 0.000913 | Perplexity: 950.419694
2025-09-14 18:48:35,264 Stage: Train 0.5 | Epoch: 63 | Iter: 192800 | Total Loss: 0.002885 | Recon Loss: 0.002429 | Commit Loss: 0.000912 | Perplexity: 951.538129
2025-09-14 18:48:43,947 Stage: Train 0.5 | Epoch: 63 | Iter: 193000 | Total Loss: 0.002896 | Recon Loss: 0.002441 | Commit Loss: 0.000909 | Perplexity: 952.478531
2025-09-14 18:48:52,590 Stage: Train 0.5 | Epoch: 63 | Iter: 193200 | Total Loss: 0.002842 | Recon Loss: 0.002384 | Commit Loss: 0.000916 | Perplexity: 958.508738
2025-09-14 18:49:01,332 Stage: Train 0.5 | Epoch: 63 | Iter: 193400 | Total Loss: 0.002915 | Recon Loss: 0.002460 | Commit Loss: 0.000910 | Perplexity: 949.679417
2025-09-14 18:49:10,032 Stage: Train 0.5 | Epoch: 63 | Iter: 193600 | Total Loss: 0.002786 | Recon Loss: 0.002327 | Commit Loss: 0.000918 | Perplexity: 952.554228
2025-09-14 18:49:18,732 Stage: Train 0.5 | Epoch: 63 | Iter: 193800 | Total Loss: 0.002847 | Recon Loss: 0.002387 | Commit Loss: 0.000921 | Perplexity: 958.070975
2025-09-14 18:49:27,443 Stage: Train 0.5 | Epoch: 63 | Iter: 194000 | Total Loss: 0.002856 | Recon Loss: 0.002395 | Commit Loss: 0.000921 | Perplexity: 950.739302
2025-09-14 18:49:36,152 Stage: Train 0.5 | Epoch: 63 | Iter: 194200 | Total Loss: 0.002927 | Recon Loss: 0.002462 | Commit Loss: 0.000930 | Perplexity: 955.130426
2025-09-14 18:49:44,820 Stage: Train 0.5 | Epoch: 63 | Iter: 194400 | Total Loss: 0.002838 | Recon Loss: 0.002381 | Commit Loss: 0.000914 | Perplexity: 951.034105
Trainning Epoch:  39%|███▉      | 64/165 [2:21:39<3:43:14, 132.62s/it]2025-09-14 18:49:53,537 Stage: Train 0.5 | Epoch: 64 | Iter: 194600 | Total Loss: 0.002888 | Recon Loss: 0.002432 | Commit Loss: 0.000912 | Perplexity: 952.163367
2025-09-14 18:50:02,250 Stage: Train 0.5 | Epoch: 64 | Iter: 194800 | Total Loss: 0.002841 | Recon Loss: 0.002380 | Commit Loss: 0.000920 | Perplexity: 954.103602
2025-09-14 18:50:10,958 Stage: Train 0.5 | Epoch: 64 | Iter: 195000 | Total Loss: 0.002835 | Recon Loss: 0.002378 | Commit Loss: 0.000914 | Perplexity: 951.918910
2025-09-14 18:50:19,641 Stage: Train 0.5 | Epoch: 64 | Iter: 195200 | Total Loss: 0.002862 | Recon Loss: 0.002405 | Commit Loss: 0.000913 | Perplexity: 952.206420
2025-09-14 18:50:28,356 Stage: Train 0.5 | Epoch: 64 | Iter: 195400 | Total Loss: 0.002845 | Recon Loss: 0.002385 | Commit Loss: 0.000919 | Perplexity: 956.344378
2025-09-14 18:50:37,083 Stage: Train 0.5 | Epoch: 64 | Iter: 195600 | Total Loss: 0.002838 | Recon Loss: 0.002378 | Commit Loss: 0.000919 | Perplexity: 953.744092
2025-09-14 18:50:45,832 Stage: Train 0.5 | Epoch: 64 | Iter: 195800 | Total Loss: 0.002860 | Recon Loss: 0.002400 | Commit Loss: 0.000921 | Perplexity: 951.023308
2025-09-14 18:50:54,528 Stage: Train 0.5 | Epoch: 64 | Iter: 196000 | Total Loss: 0.002839 | Recon Loss: 0.002382 | Commit Loss: 0.000915 | Perplexity: 953.967622
2025-09-14 18:51:03,244 Stage: Train 0.5 | Epoch: 64 | Iter: 196200 | Total Loss: 0.002893 | Recon Loss: 0.002433 | Commit Loss: 0.000921 | Perplexity: 954.968623
2025-09-14 18:51:11,918 Stage: Train 0.5 | Epoch: 64 | Iter: 196400 | Total Loss: 0.002839 | Recon Loss: 0.002385 | Commit Loss: 0.000908 | Perplexity: 949.315934
2025-09-14 18:51:20,618 Stage: Train 0.5 | Epoch: 64 | Iter: 196600 | Total Loss: 0.002832 | Recon Loss: 0.002372 | Commit Loss: 0.000919 | Perplexity: 957.315399
2025-09-14 18:51:29,317 Stage: Train 0.5 | Epoch: 64 | Iter: 196800 | Total Loss: 0.002889 | Recon Loss: 0.002429 | Commit Loss: 0.000920 | Perplexity: 954.170515
2025-09-14 18:51:38,028 Stage: Train 0.5 | Epoch: 64 | Iter: 197000 | Total Loss: 0.002864 | Recon Loss: 0.002407 | Commit Loss: 0.000914 | Perplexity: 948.013141
2025-09-14 18:51:46,749 Stage: Train 0.5 | Epoch: 64 | Iter: 197200 | Total Loss: 0.002862 | Recon Loss: 0.002406 | Commit Loss: 0.000913 | Perplexity: 948.718934
2025-09-14 18:51:55,438 Stage: Train 0.5 | Epoch: 64 | Iter: 197400 | Total Loss: 0.002835 | Recon Loss: 0.002379 | Commit Loss: 0.000912 | Perplexity: 953.284986
Trainning Epoch:  39%|███▉      | 65/165 [2:23:51<3:40:51, 132.51s/it]2025-09-14 18:52:04,128 Stage: Train 0.5 | Epoch: 65 | Iter: 197600 | Total Loss: 0.002797 | Recon Loss: 0.002343 | Commit Loss: 0.000908 | Perplexity: 944.388909
2025-09-14 18:52:12,822 Stage: Train 0.5 | Epoch: 65 | Iter: 197800 | Total Loss: 0.002832 | Recon Loss: 0.002376 | Commit Loss: 0.000911 | Perplexity: 955.043041
2025-09-14 18:52:21,521 Stage: Train 0.5 | Epoch: 65 | Iter: 198000 | Total Loss: 0.002847 | Recon Loss: 0.002390 | Commit Loss: 0.000913 | Perplexity: 952.046606
2025-09-14 18:52:30,214 Stage: Train 0.5 | Epoch: 65 | Iter: 198200 | Total Loss: 0.002860 | Recon Loss: 0.002403 | Commit Loss: 0.000914 | Perplexity: 950.946607
2025-09-14 18:52:38,924 Stage: Train 0.5 | Epoch: 65 | Iter: 198400 | Total Loss: 0.002867 | Recon Loss: 0.002413 | Commit Loss: 0.000907 | Perplexity: 956.472578
2025-09-14 18:52:47,664 Stage: Train 0.5 | Epoch: 65 | Iter: 198600 | Total Loss: 0.002838 | Recon Loss: 0.002378 | Commit Loss: 0.000919 | Perplexity: 955.559658
2025-09-14 18:52:56,358 Stage: Train 0.5 | Epoch: 65 | Iter: 198800 | Total Loss: 0.002850 | Recon Loss: 0.002396 | Commit Loss: 0.000909 | Perplexity: 951.818081
2025-09-14 18:53:05,082 Stage: Train 0.5 | Epoch: 65 | Iter: 199000 | Total Loss: 0.002861 | Recon Loss: 0.002395 | Commit Loss: 0.000931 | Perplexity: 960.130557
2025-09-14 18:53:13,794 Stage: Train 0.5 | Epoch: 65 | Iter: 199200 | Total Loss: 0.002858 | Recon Loss: 0.002400 | Commit Loss: 0.000916 | Perplexity: 952.458105
2025-09-14 18:53:22,506 Stage: Train 0.5 | Epoch: 65 | Iter: 199400 | Total Loss: 0.002881 | Recon Loss: 0.002429 | Commit Loss: 0.000903 | Perplexity: 948.588976
2025-09-14 18:53:31,256 Stage: Train 0.5 | Epoch: 65 | Iter: 199600 | Total Loss: 0.002783 | Recon Loss: 0.002323 | Commit Loss: 0.000919 | Perplexity: 954.759144
2025-09-14 18:53:39,995 Stage: Train 0.5 | Epoch: 65 | Iter: 199800 | Total Loss: 0.002833 | Recon Loss: 0.002375 | Commit Loss: 0.000916 | Perplexity: 955.164192
2025-09-14 18:53:48,757 Stage: Train 0.5 | Epoch: 65 | Iter: 200000 | Total Loss: 0.002921 | Recon Loss: 0.002454 | Commit Loss: 0.000935 | Perplexity: 954.880571
2025-09-14 18:53:48,757 Saving model at iteration 200000
2025-09-14 18:53:48,994 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_66_step_200000
2025-09-14 18:53:49,228 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_66_step_200000/pytorch_model.bin
2025-09-14 18:53:49,589 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_66_step_200000/optimizer.bin
2025-09-14 18:53:49,590 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_66_step_200000/scheduler.bin
2025-09-14 18:53:49,590 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_66_step_200000/random_states_0.pkl
2025-09-14 18:53:58,717 Stage: Train 0.5 | Epoch: 65 | Iter: 200200 | Total Loss: 0.002837 | Recon Loss: 0.002377 | Commit Loss: 0.000919 | Perplexity: 952.802546
2025-09-14 18:54:07,471 Stage: Train 0.5 | Epoch: 65 | Iter: 200400 | Total Loss: 0.002838 | Recon Loss: 0.002380 | Commit Loss: 0.000916 | Perplexity: 949.862564
Trainning Epoch:  40%|████      | 66/165 [2:26:05<3:39:14, 132.88s/it]2025-09-14 18:54:16,237 Stage: Train 0.5 | Epoch: 66 | Iter: 200600 | Total Loss: 0.002910 | Recon Loss: 0.002451 | Commit Loss: 0.000918 | Perplexity: 949.346911
2025-09-14 18:54:24,989 Stage: Train 0.5 | Epoch: 66 | Iter: 200800 | Total Loss: 0.002840 | Recon Loss: 0.002383 | Commit Loss: 0.000914 | Perplexity: 952.668893
2025-09-14 18:54:33,734 Stage: Train 0.5 | Epoch: 66 | Iter: 201000 | Total Loss: 0.002800 | Recon Loss: 0.002346 | Commit Loss: 0.000906 | Perplexity: 951.804710
2025-09-14 18:54:42,472 Stage: Train 0.5 | Epoch: 66 | Iter: 201200 | Total Loss: 0.002818 | Recon Loss: 0.002360 | Commit Loss: 0.000916 | Perplexity: 960.031519
2025-09-14 18:54:51,190 Stage: Train 0.5 | Epoch: 66 | Iter: 201400 | Total Loss: 0.002826 | Recon Loss: 0.002368 | Commit Loss: 0.000917 | Perplexity: 953.288177
2025-09-14 18:54:59,956 Stage: Train 0.5 | Epoch: 66 | Iter: 201600 | Total Loss: 0.002852 | Recon Loss: 0.002396 | Commit Loss: 0.000912 | Perplexity: 956.113196
2025-09-14 18:55:08,664 Stage: Train 0.5 | Epoch: 66 | Iter: 201800 | Total Loss: 0.002802 | Recon Loss: 0.002349 | Commit Loss: 0.000907 | Perplexity: 950.244597
2025-09-14 18:55:17,388 Stage: Train 0.5 | Epoch: 66 | Iter: 202000 | Total Loss: 0.002823 | Recon Loss: 0.002366 | Commit Loss: 0.000913 | Perplexity: 950.903918
2025-09-14 18:55:26,102 Stage: Train 0.5 | Epoch: 66 | Iter: 202200 | Total Loss: 0.002823 | Recon Loss: 0.002365 | Commit Loss: 0.000915 | Perplexity: 948.079660
2025-09-14 18:55:34,903 Stage: Train 0.5 | Epoch: 66 | Iter: 202400 | Total Loss: 0.002839 | Recon Loss: 0.002379 | Commit Loss: 0.000921 | Perplexity: 955.000258
2025-09-14 18:55:43,618 Stage: Train 0.5 | Epoch: 66 | Iter: 202600 | Total Loss: 0.002882 | Recon Loss: 0.002422 | Commit Loss: 0.000921 | Perplexity: 952.186496
2025-09-14 18:55:52,309 Stage: Train 0.5 | Epoch: 66 | Iter: 202800 | Total Loss: 0.002846 | Recon Loss: 0.002385 | Commit Loss: 0.000923 | Perplexity: 955.650707
2025-09-14 18:56:01,014 Stage: Train 0.5 | Epoch: 66 | Iter: 203000 | Total Loss: 0.002801 | Recon Loss: 0.002341 | Commit Loss: 0.000918 | Perplexity: 951.887719
2025-09-14 18:56:09,734 Stage: Train 0.5 | Epoch: 66 | Iter: 203200 | Total Loss: 0.002817 | Recon Loss: 0.002363 | Commit Loss: 0.000907 | Perplexity: 950.061831
2025-09-14 18:56:18,481 Stage: Train 0.5 | Epoch: 66 | Iter: 203400 | Total Loss: 0.002829 | Recon Loss: 0.002372 | Commit Loss: 0.000912 | Perplexity: 948.663207
Trainning Epoch:  41%|████      | 67/165 [2:28:18<3:36:56, 132.82s/it]2025-09-14 18:56:27,253 Stage: Train 0.5 | Epoch: 67 | Iter: 203600 | Total Loss: 0.002827 | Recon Loss: 0.002370 | Commit Loss: 0.000913 | Perplexity: 948.771767
2025-09-14 18:56:35,982 Stage: Train 0.5 | Epoch: 67 | Iter: 203800 | Total Loss: 0.002777 | Recon Loss: 0.002317 | Commit Loss: 0.000920 | Perplexity: 955.036901
2025-09-14 18:56:44,689 Stage: Train 0.5 | Epoch: 67 | Iter: 204000 | Total Loss: 0.002828 | Recon Loss: 0.002374 | Commit Loss: 0.000908 | Perplexity: 948.676666
2025-09-14 18:56:53,412 Stage: Train 0.5 | Epoch: 67 | Iter: 204200 | Total Loss: 0.002819 | Recon Loss: 0.002362 | Commit Loss: 0.000915 | Perplexity: 957.279822
2025-09-14 18:57:02,140 Stage: Train 0.5 | Epoch: 67 | Iter: 204400 | Total Loss: 0.002814 | Recon Loss: 0.002357 | Commit Loss: 0.000914 | Perplexity: 951.949596
2025-09-14 18:57:10,878 Stage: Train 0.5 | Epoch: 67 | Iter: 204600 | Total Loss: 0.002806 | Recon Loss: 0.002348 | Commit Loss: 0.000916 | Perplexity: 955.870274
2025-09-14 18:57:19,616 Stage: Train 0.5 | Epoch: 67 | Iter: 204800 | Total Loss: 0.002799 | Recon Loss: 0.002339 | Commit Loss: 0.000919 | Perplexity: 949.544570
2025-09-14 18:57:28,346 Stage: Train 0.5 | Epoch: 67 | Iter: 205000 | Total Loss: 0.002823 | Recon Loss: 0.002364 | Commit Loss: 0.000917 | Perplexity: 954.955858
2025-09-14 18:57:37,085 Stage: Train 0.5 | Epoch: 67 | Iter: 205200 | Total Loss: 0.002831 | Recon Loss: 0.002374 | Commit Loss: 0.000913 | Perplexity: 953.221535
2025-09-14 18:57:45,822 Stage: Train 0.5 | Epoch: 67 | Iter: 205400 | Total Loss: 0.002803 | Recon Loss: 0.002347 | Commit Loss: 0.000912 | Perplexity: 952.066466
2025-09-14 18:57:54,589 Stage: Train 0.5 | Epoch: 67 | Iter: 205600 | Total Loss: 0.002855 | Recon Loss: 0.002396 | Commit Loss: 0.000918 | Perplexity: 955.907489
2025-09-14 18:58:03,332 Stage: Train 0.5 | Epoch: 67 | Iter: 205800 | Total Loss: 0.002772 | Recon Loss: 0.002320 | Commit Loss: 0.000905 | Perplexity: 949.261481
2025-09-14 18:58:12,052 Stage: Train 0.5 | Epoch: 67 | Iter: 206000 | Total Loss: 0.002861 | Recon Loss: 0.002403 | Commit Loss: 0.000917 | Perplexity: 951.204113
2025-09-14 18:58:20,757 Stage: Train 0.5 | Epoch: 67 | Iter: 206200 | Total Loss: 0.002833 | Recon Loss: 0.002376 | Commit Loss: 0.000913 | Perplexity: 953.679752
2025-09-14 18:58:29,460 Stage: Train 0.5 | Epoch: 67 | Iter: 206400 | Total Loss: 0.002789 | Recon Loss: 0.002332 | Commit Loss: 0.000914 | Perplexity: 949.938512
Trainning Epoch:  41%|████      | 68/165 [2:30:30<3:34:36, 132.75s/it]2025-09-14 18:58:38,169 Stage: Train 0.5 | Epoch: 68 | Iter: 206600 | Total Loss: 0.002828 | Recon Loss: 0.002377 | Commit Loss: 0.000902 | Perplexity: 948.066342
2025-09-14 18:58:46,914 Stage: Train 0.5 | Epoch: 68 | Iter: 206800 | Total Loss: 0.002783 | Recon Loss: 0.002332 | Commit Loss: 0.000902 | Perplexity: 951.674776
2025-09-14 18:58:55,579 Stage: Train 0.5 | Epoch: 68 | Iter: 207000 | Total Loss: 0.002862 | Recon Loss: 0.002406 | Commit Loss: 0.000912 | Perplexity: 954.992181
2025-09-14 18:59:04,270 Stage: Train 0.5 | Epoch: 68 | Iter: 207200 | Total Loss: 0.002785 | Recon Loss: 0.002330 | Commit Loss: 0.000910 | Perplexity: 956.412294
2025-09-14 18:59:12,980 Stage: Train 0.5 | Epoch: 68 | Iter: 207400 | Total Loss: 0.002831 | Recon Loss: 0.002374 | Commit Loss: 0.000913 | Perplexity: 950.677879
2025-09-14 18:59:21,699 Stage: Train 0.5 | Epoch: 68 | Iter: 207600 | Total Loss: 0.002876 | Recon Loss: 0.002418 | Commit Loss: 0.000917 | Perplexity: 955.930381
2025-09-14 18:59:30,395 Stage: Train 0.5 | Epoch: 68 | Iter: 207800 | Total Loss: 0.002826 | Recon Loss: 0.002371 | Commit Loss: 0.000911 | Perplexity: 953.206472
2025-09-14 18:59:39,092 Stage: Train 0.5 | Epoch: 68 | Iter: 208000 | Total Loss: 0.002775 | Recon Loss: 0.002323 | Commit Loss: 0.000905 | Perplexity: 955.249221
2025-09-14 18:59:47,799 Stage: Train 0.5 | Epoch: 68 | Iter: 208200 | Total Loss: 0.002814 | Recon Loss: 0.002354 | Commit Loss: 0.000919 | Perplexity: 957.886466
2025-09-14 18:59:56,494 Stage: Train 0.5 | Epoch: 68 | Iter: 208400 | Total Loss: 0.002800 | Recon Loss: 0.002336 | Commit Loss: 0.000927 | Perplexity: 956.255676
2025-09-14 19:00:05,194 Stage: Train 0.5 | Epoch: 68 | Iter: 208600 | Total Loss: 0.002822 | Recon Loss: 0.002358 | Commit Loss: 0.000928 | Perplexity: 953.994339
2025-09-14 19:00:13,879 Stage: Train 0.5 | Epoch: 68 | Iter: 208800 | Total Loss: 0.002812 | Recon Loss: 0.002356 | Commit Loss: 0.000911 | Perplexity: 953.160970
2025-09-14 19:00:22,537 Stage: Train 0.5 | Epoch: 68 | Iter: 209000 | Total Loss: 0.002825 | Recon Loss: 0.002365 | Commit Loss: 0.000918 | Perplexity: 956.263488
2025-09-14 19:00:31,264 Stage: Train 0.5 | Epoch: 68 | Iter: 209200 | Total Loss: 0.002846 | Recon Loss: 0.002385 | Commit Loss: 0.000923 | Perplexity: 952.962576
2025-09-14 19:00:39,943 Stage: Train 0.5 | Epoch: 68 | Iter: 209400 | Total Loss: 0.002780 | Recon Loss: 0.002324 | Commit Loss: 0.000913 | Perplexity: 952.652366
2025-09-14 19:00:48,672 Stage: Train 0.5 | Epoch: 68 | Iter: 209600 | Total Loss: 0.002809 | Recon Loss: 0.002353 | Commit Loss: 0.000912 | Perplexity: 954.400343
Trainning Epoch:  42%|████▏     | 69/165 [2:32:43<3:32:07, 132.58s/it]2025-09-14 19:00:57,383 Stage: Train 0.5 | Epoch: 69 | Iter: 209800 | Total Loss: 0.002786 | Recon Loss: 0.002324 | Commit Loss: 0.000924 | Perplexity: 953.241316
2025-09-14 19:01:06,094 Stage: Train 0.5 | Epoch: 69 | Iter: 210000 | Total Loss: 0.002799 | Recon Loss: 0.002345 | Commit Loss: 0.000910 | Perplexity: 952.545947
2025-09-14 19:01:14,792 Stage: Train 0.5 | Epoch: 69 | Iter: 210200 | Total Loss: 0.002797 | Recon Loss: 0.002336 | Commit Loss: 0.000921 | Perplexity: 959.859061
2025-09-14 19:01:23,500 Stage: Train 0.5 | Epoch: 69 | Iter: 210400 | Total Loss: 0.002812 | Recon Loss: 0.002351 | Commit Loss: 0.000923 | Perplexity: 956.182715
2025-09-14 19:01:32,216 Stage: Train 0.5 | Epoch: 69 | Iter: 210600 | Total Loss: 0.002808 | Recon Loss: 0.002352 | Commit Loss: 0.000912 | Perplexity: 950.350386
2025-09-14 19:01:40,908 Stage: Train 0.5 | Epoch: 69 | Iter: 210800 | Total Loss: 0.002815 | Recon Loss: 0.002357 | Commit Loss: 0.000917 | Perplexity: 956.679114
2025-09-14 19:01:49,615 Stage: Train 0.5 | Epoch: 69 | Iter: 211000 | Total Loss: 0.002783 | Recon Loss: 0.002327 | Commit Loss: 0.000913 | Perplexity: 953.121132
2025-09-14 19:01:58,310 Stage: Train 0.5 | Epoch: 69 | Iter: 211200 | Total Loss: 0.002776 | Recon Loss: 0.002322 | Commit Loss: 0.000907 | Perplexity: 959.105913
2025-09-14 19:02:07,012 Stage: Train 0.5 | Epoch: 69 | Iter: 211400 | Total Loss: 0.002793 | Recon Loss: 0.002337 | Commit Loss: 0.000911 | Perplexity: 954.881316
2025-09-14 19:02:15,713 Stage: Train 0.5 | Epoch: 69 | Iter: 211600 | Total Loss: 0.002840 | Recon Loss: 0.002384 | Commit Loss: 0.000914 | Perplexity: 949.420160
2025-09-14 19:02:24,456 Stage: Train 0.5 | Epoch: 69 | Iter: 211800 | Total Loss: 0.002795 | Recon Loss: 0.002338 | Commit Loss: 0.000916 | Perplexity: 953.452868
2025-09-14 19:02:33,164 Stage: Train 0.5 | Epoch: 69 | Iter: 212000 | Total Loss: 0.002811 | Recon Loss: 0.002353 | Commit Loss: 0.000916 | Perplexity: 956.562121
2025-09-14 19:02:41,894 Stage: Train 0.5 | Epoch: 69 | Iter: 212200 | Total Loss: 0.002826 | Recon Loss: 0.002372 | Commit Loss: 0.000908 | Perplexity: 952.806158
2025-09-14 19:02:50,614 Stage: Train 0.5 | Epoch: 69 | Iter: 212400 | Total Loss: 0.002752 | Recon Loss: 0.002292 | Commit Loss: 0.000921 | Perplexity: 954.629435
2025-09-14 19:02:59,337 Stage: Train 0.5 | Epoch: 69 | Iter: 212600 | Total Loss: 0.002879 | Recon Loss: 0.002426 | Commit Loss: 0.000904 | Perplexity: 947.301750
Trainning Epoch:  42%|████▏     | 70/165 [2:34:55<3:29:47, 132.50s/it]2025-09-14 19:03:08,056 Stage: Train 0.5 | Epoch: 70 | Iter: 212800 | Total Loss: 0.002780 | Recon Loss: 0.002319 | Commit Loss: 0.000922 | Perplexity: 953.315146
2025-09-14 19:03:16,781 Stage: Train 0.5 | Epoch: 70 | Iter: 213000 | Total Loss: 0.002881 | Recon Loss: 0.002426 | Commit Loss: 0.000910 | Perplexity: 951.009202
2025-09-14 19:03:25,464 Stage: Train 0.5 | Epoch: 70 | Iter: 213200 | Total Loss: 0.002761 | Recon Loss: 0.002303 | Commit Loss: 0.000916 | Perplexity: 957.498566
2025-09-14 19:03:34,177 Stage: Train 0.5 | Epoch: 70 | Iter: 213400 | Total Loss: 0.002793 | Recon Loss: 0.002339 | Commit Loss: 0.000907 | Perplexity: 951.348101
2025-09-14 19:03:42,887 Stage: Train 0.5 | Epoch: 70 | Iter: 213600 | Total Loss: 0.002733 | Recon Loss: 0.002280 | Commit Loss: 0.000906 | Perplexity: 949.723147
2025-09-14 19:03:51,597 Stage: Train 0.5 | Epoch: 70 | Iter: 213800 | Total Loss: 0.002797 | Recon Loss: 0.002335 | Commit Loss: 0.000924 | Perplexity: 958.283739
2025-09-14 19:04:00,280 Stage: Train 0.5 | Epoch: 70 | Iter: 214000 | Total Loss: 0.002811 | Recon Loss: 0.002350 | Commit Loss: 0.000923 | Perplexity: 954.409930
2025-09-14 19:04:08,996 Stage: Train 0.5 | Epoch: 70 | Iter: 214200 | Total Loss: 0.002751 | Recon Loss: 0.002293 | Commit Loss: 0.000916 | Perplexity: 958.817921
2025-09-14 19:04:17,683 Stage: Train 0.5 | Epoch: 70 | Iter: 214400 | Total Loss: 0.002775 | Recon Loss: 0.002323 | Commit Loss: 0.000904 | Perplexity: 954.152740
2025-09-14 19:04:26,392 Stage: Train 0.5 | Epoch: 70 | Iter: 214600 | Total Loss: 0.002824 | Recon Loss: 0.002360 | Commit Loss: 0.000928 | Perplexity: 959.053708
2025-09-14 19:04:35,164 Stage: Train 0.5 | Epoch: 70 | Iter: 214800 | Total Loss: 0.002750 | Recon Loss: 0.002294 | Commit Loss: 0.000912 | Perplexity: 955.255413
2025-09-14 19:04:43,934 Stage: Train 0.5 | Epoch: 70 | Iter: 215000 | Total Loss: 0.002797 | Recon Loss: 0.002340 | Commit Loss: 0.000913 | Perplexity: 954.753777
2025-09-14 19:04:52,699 Stage: Train 0.5 | Epoch: 70 | Iter: 215200 | Total Loss: 0.002785 | Recon Loss: 0.002326 | Commit Loss: 0.000917 | Perplexity: 954.421495
2025-09-14 19:05:01,480 Stage: Train 0.5 | Epoch: 70 | Iter: 215400 | Total Loss: 0.002771 | Recon Loss: 0.002316 | Commit Loss: 0.000910 | Perplexity: 957.433265
2025-09-14 19:05:10,254 Stage: Train 0.5 | Epoch: 70 | Iter: 215600 | Total Loss: 0.002792 | Recon Loss: 0.002336 | Commit Loss: 0.000913 | Perplexity: 950.292522
Trainning Epoch:  43%|████▎     | 71/165 [2:37:08<3:27:37, 132.53s/it]2025-09-14 19:05:19,033 Stage: Train 0.5 | Epoch: 71 | Iter: 215800 | Total Loss: 0.002814 | Recon Loss: 0.002361 | Commit Loss: 0.000906 | Perplexity: 952.050217
2025-09-14 19:05:27,780 Stage: Train 0.5 | Epoch: 71 | Iter: 216000 | Total Loss: 0.002773 | Recon Loss: 0.002318 | Commit Loss: 0.000910 | Perplexity: 955.068719
2025-09-14 19:05:36,547 Stage: Train 0.5 | Epoch: 71 | Iter: 216200 | Total Loss: 0.002773 | Recon Loss: 0.002318 | Commit Loss: 0.000911 | Perplexity: 954.626342
2025-09-14 19:05:45,330 Stage: Train 0.5 | Epoch: 71 | Iter: 216400 | Total Loss: 0.002804 | Recon Loss: 0.002344 | Commit Loss: 0.000920 | Perplexity: 956.460733
2025-09-14 19:05:54,108 Stage: Train 0.5 | Epoch: 71 | Iter: 216600 | Total Loss: 0.002766 | Recon Loss: 0.002309 | Commit Loss: 0.000914 | Perplexity: 958.619568
2025-09-14 19:06:02,873 Stage: Train 0.5 | Epoch: 71 | Iter: 216800 | Total Loss: 0.002755 | Recon Loss: 0.002303 | Commit Loss: 0.000903 | Perplexity: 953.372410
2025-09-14 19:06:11,649 Stage: Train 0.5 | Epoch: 71 | Iter: 217000 | Total Loss: 0.002776 | Recon Loss: 0.002326 | Commit Loss: 0.000901 | Perplexity: 954.345354
2025-09-14 19:06:20,412 Stage: Train 0.5 | Epoch: 71 | Iter: 217200 | Total Loss: 0.002772 | Recon Loss: 0.002316 | Commit Loss: 0.000912 | Perplexity: 959.104484
2025-09-14 19:06:29,172 Stage: Train 0.5 | Epoch: 71 | Iter: 217400 | Total Loss: 0.002777 | Recon Loss: 0.002323 | Commit Loss: 0.000908 | Perplexity: 956.245814
2025-09-14 19:06:37,913 Stage: Train 0.5 | Epoch: 71 | Iter: 217600 | Total Loss: 0.002785 | Recon Loss: 0.002328 | Commit Loss: 0.000914 | Perplexity: 954.006640
2025-09-14 19:06:46,634 Stage: Train 0.5 | Epoch: 71 | Iter: 217800 | Total Loss: 0.002786 | Recon Loss: 0.002331 | Commit Loss: 0.000910 | Perplexity: 955.119387
2025-09-14 19:06:55,334 Stage: Train 0.5 | Epoch: 71 | Iter: 218000 | Total Loss: 0.002795 | Recon Loss: 0.002337 | Commit Loss: 0.000914 | Perplexity: 953.801165
2025-09-14 19:07:04,042 Stage: Train 0.5 | Epoch: 71 | Iter: 218200 | Total Loss: 0.002732 | Recon Loss: 0.002276 | Commit Loss: 0.000914 | Perplexity: 955.321711
2025-09-14 19:07:12,764 Stage: Train 0.5 | Epoch: 71 | Iter: 218400 | Total Loss: 0.002747 | Recon Loss: 0.002289 | Commit Loss: 0.000917 | Perplexity: 960.625320
2025-09-14 19:07:21,472 Stage: Train 0.5 | Epoch: 71 | Iter: 218600 | Total Loss: 0.002744 | Recon Loss: 0.002290 | Commit Loss: 0.000908 | Perplexity: 953.955548
Trainning Epoch:  44%|████▎     | 72/165 [2:39:20<3:25:34, 132.63s/it]2025-09-14 19:07:30,237 Stage: Train 0.5 | Epoch: 72 | Iter: 218800 | Total Loss: 0.002855 | Recon Loss: 0.002396 | Commit Loss: 0.000917 | Perplexity: 950.942787
2025-09-14 19:07:39,023 Stage: Train 0.5 | Epoch: 72 | Iter: 219000 | Total Loss: 0.002814 | Recon Loss: 0.002364 | Commit Loss: 0.000900 | Perplexity: 951.111549
2025-09-14 19:07:47,819 Stage: Train 0.5 | Epoch: 72 | Iter: 219200 | Total Loss: 0.002792 | Recon Loss: 0.002336 | Commit Loss: 0.000911 | Perplexity: 959.495505
2025-09-14 19:07:56,604 Stage: Train 0.5 | Epoch: 72 | Iter: 219400 | Total Loss: 0.002741 | Recon Loss: 0.002285 | Commit Loss: 0.000912 | Perplexity: 957.200059
2025-09-14 19:08:05,380 Stage: Train 0.5 | Epoch: 72 | Iter: 219600 | Total Loss: 0.002771 | Recon Loss: 0.002315 | Commit Loss: 0.000911 | Perplexity: 958.547983
2025-09-14 19:08:14,120 Stage: Train 0.5 | Epoch: 72 | Iter: 219800 | Total Loss: 0.002771 | Recon Loss: 0.002313 | Commit Loss: 0.000917 | Perplexity: 957.596156
2025-09-14 19:08:22,873 Stage: Train 0.5 | Epoch: 72 | Iter: 220000 | Total Loss: 0.002785 | Recon Loss: 0.002328 | Commit Loss: 0.000915 | Perplexity: 956.290044
2025-09-14 19:08:22,873 Saving model at iteration 220000
2025-09-14 19:08:23,030 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_73_step_220000
2025-09-14 19:08:23,268 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_73_step_220000/pytorch_model.bin
2025-09-14 19:08:23,636 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_73_step_220000/optimizer.bin
2025-09-14 19:08:23,636 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_73_step_220000/scheduler.bin
2025-09-14 19:08:23,637 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_73_step_220000/random_states_0.pkl
2025-09-14 19:08:32,348 Stage: Train 0.5 | Epoch: 72 | Iter: 220200 | Total Loss: 0.002769 | Recon Loss: 0.002313 | Commit Loss: 0.000912 | Perplexity: 957.143046
2025-09-14 19:08:41,110 Stage: Train 0.5 | Epoch: 72 | Iter: 220400 | Total Loss: 0.002772 | Recon Loss: 0.002314 | Commit Loss: 0.000915 | Perplexity: 959.177008
2025-09-14 19:08:49,871 Stage: Train 0.5 | Epoch: 72 | Iter: 220600 | Total Loss: 0.002755 | Recon Loss: 0.002300 | Commit Loss: 0.000909 | Perplexity: 956.029688
2025-09-14 19:08:58,605 Stage: Train 0.5 | Epoch: 72 | Iter: 220800 | Total Loss: 0.002802 | Recon Loss: 0.002345 | Commit Loss: 0.000914 | Perplexity: 956.729750
2025-09-14 19:09:07,355 Stage: Train 0.5 | Epoch: 72 | Iter: 221000 | Total Loss: 0.002784 | Recon Loss: 0.002331 | Commit Loss: 0.000906 | Perplexity: 955.558639
2025-09-14 19:09:16,130 Stage: Train 0.5 | Epoch: 72 | Iter: 221200 | Total Loss: 0.002741 | Recon Loss: 0.002286 | Commit Loss: 0.000910 | Perplexity: 956.616064
2025-09-14 19:09:24,916 Stage: Train 0.5 | Epoch: 72 | Iter: 221400 | Total Loss: 0.002776 | Recon Loss: 0.002321 | Commit Loss: 0.000909 | Perplexity: 955.715081
2025-09-14 19:09:33,712 Stage: Train 0.5 | Epoch: 72 | Iter: 221600 | Total Loss: 0.002766 | Recon Loss: 0.002313 | Commit Loss: 0.000907 | Perplexity: 956.934490
Trainning Epoch:  44%|████▍     | 73/165 [2:41:35<3:24:07, 133.12s/it]2025-09-14 19:09:42,835 Stage: Train 0.5 | Epoch: 73 | Iter: 221800 | Total Loss: 0.002801 | Recon Loss: 0.002345 | Commit Loss: 0.000913 | Perplexity: 957.270513
2025-09-14 19:09:51,611 Stage: Train 0.5 | Epoch: 73 | Iter: 222000 | Total Loss: 0.002727 | Recon Loss: 0.002274 | Commit Loss: 0.000905 | Perplexity: 947.809109
2025-09-14 19:10:00,395 Stage: Train 0.5 | Epoch: 73 | Iter: 222200 | Total Loss: 0.002747 | Recon Loss: 0.002294 | Commit Loss: 0.000906 | Perplexity: 955.653290
2025-09-14 19:10:09,157 Stage: Train 0.5 | Epoch: 73 | Iter: 222400 | Total Loss: 0.002797 | Recon Loss: 0.002348 | Commit Loss: 0.000898 | Perplexity: 953.073915
2025-09-14 19:10:17,921 Stage: Train 0.5 | Epoch: 73 | Iter: 222600 | Total Loss: 0.002770 | Recon Loss: 0.002315 | Commit Loss: 0.000910 | Perplexity: 956.156529
2025-09-14 19:10:26,657 Stage: Train 0.5 | Epoch: 73 | Iter: 222800 | Total Loss: 0.002736 | Recon Loss: 0.002281 | Commit Loss: 0.000910 | Perplexity: 956.371745
2025-09-14 19:10:35,368 Stage: Train 0.5 | Epoch: 73 | Iter: 223000 | Total Loss: 0.002731 | Recon Loss: 0.002279 | Commit Loss: 0.000904 | Perplexity: 954.820854
2025-09-14 19:10:44,093 Stage: Train 0.5 | Epoch: 73 | Iter: 223200 | Total Loss: 0.002767 | Recon Loss: 0.002310 | Commit Loss: 0.000913 | Perplexity: 960.797794
2025-09-14 19:10:52,836 Stage: Train 0.5 | Epoch: 73 | Iter: 223400 | Total Loss: 0.002763 | Recon Loss: 0.002312 | Commit Loss: 0.000902 | Perplexity: 955.561405
2025-09-14 19:11:01,596 Stage: Train 0.5 | Epoch: 73 | Iter: 223600 | Total Loss: 0.002736 | Recon Loss: 0.002278 | Commit Loss: 0.000916 | Perplexity: 957.044041
2025-09-14 19:11:10,379 Stage: Train 0.5 | Epoch: 73 | Iter: 223800 | Total Loss: 0.002758 | Recon Loss: 0.002300 | Commit Loss: 0.000916 | Perplexity: 954.407163
2025-09-14 19:11:19,127 Stage: Train 0.5 | Epoch: 73 | Iter: 224000 | Total Loss: 0.002764 | Recon Loss: 0.002305 | Commit Loss: 0.000919 | Perplexity: 957.991822
2025-09-14 19:11:27,922 Stage: Train 0.5 | Epoch: 73 | Iter: 224200 | Total Loss: 0.002727 | Recon Loss: 0.002273 | Commit Loss: 0.000907 | Perplexity: 957.036891
2025-09-14 19:11:36,690 Stage: Train 0.5 | Epoch: 73 | Iter: 224400 | Total Loss: 0.002785 | Recon Loss: 0.002335 | Commit Loss: 0.000900 | Perplexity: 956.178920
2025-09-14 19:11:45,448 Stage: Train 0.5 | Epoch: 73 | Iter: 224600 | Total Loss: 0.002741 | Recon Loss: 0.002286 | Commit Loss: 0.000910 | Perplexity: 951.128100
2025-09-14 19:11:54,248 Stage: Train 0.5 | Epoch: 73 | Iter: 224800 | Total Loss: 0.002740 | Recon Loss: 0.002288 | Commit Loss: 0.000904 | Perplexity: 954.123394
Trainning Epoch:  45%|████▍     | 74/165 [2:43:48<3:21:53, 133.11s/it]2025-09-14 19:12:03,031 Stage: Train 0.5 | Epoch: 74 | Iter: 225000 | Total Loss: 0.002742 | Recon Loss: 0.002290 | Commit Loss: 0.000905 | Perplexity: 959.383471
2025-09-14 19:12:11,802 Stage: Train 0.5 | Epoch: 74 | Iter: 225200 | Total Loss: 0.002732 | Recon Loss: 0.002279 | Commit Loss: 0.000906 | Perplexity: 958.263559
2025-09-14 19:12:20,562 Stage: Train 0.5 | Epoch: 74 | Iter: 225400 | Total Loss: 0.002747 | Recon Loss: 0.002296 | Commit Loss: 0.000903 | Perplexity: 956.981449
2025-09-14 19:12:29,350 Stage: Train 0.5 | Epoch: 74 | Iter: 225600 | Total Loss: 0.002707 | Recon Loss: 0.002257 | Commit Loss: 0.000901 | Perplexity: 954.478880
2025-09-14 19:12:38,162 Stage: Train 0.5 | Epoch: 74 | Iter: 225800 | Total Loss: 0.002772 | Recon Loss: 0.002319 | Commit Loss: 0.000906 | Perplexity: 953.811974
2025-09-14 19:12:46,846 Stage: Train 0.5 | Epoch: 74 | Iter: 226000 | Total Loss: 0.002801 | Recon Loss: 0.002348 | Commit Loss: 0.000905 | Perplexity: 952.929855
2025-09-14 19:12:55,511 Stage: Train 0.5 | Epoch: 74 | Iter: 226200 | Total Loss: 0.002782 | Recon Loss: 0.002327 | Commit Loss: 0.000910 | Perplexity: 958.134149
2025-09-14 19:13:04,223 Stage: Train 0.5 | Epoch: 74 | Iter: 226400 | Total Loss: 0.002790 | Recon Loss: 0.002333 | Commit Loss: 0.000914 | Perplexity: 960.138140
2025-09-14 19:13:12,913 Stage: Train 0.5 | Epoch: 74 | Iter: 226600 | Total Loss: 0.002762 | Recon Loss: 0.002309 | Commit Loss: 0.000907 | Perplexity: 955.684545
2025-09-14 19:13:21,649 Stage: Train 0.5 | Epoch: 74 | Iter: 226800 | Total Loss: 0.002785 | Recon Loss: 0.002331 | Commit Loss: 0.000907 | Perplexity: 950.744065
2025-09-14 19:13:30,374 Stage: Train 0.5 | Epoch: 74 | Iter: 227000 | Total Loss: 0.002752 | Recon Loss: 0.002293 | Commit Loss: 0.000917 | Perplexity: 954.118924
2025-09-14 19:13:39,070 Stage: Train 0.5 | Epoch: 74 | Iter: 227200 | Total Loss: 0.002790 | Recon Loss: 0.002339 | Commit Loss: 0.000903 | Perplexity: 960.929809
2025-09-14 19:13:47,779 Stage: Train 0.5 | Epoch: 74 | Iter: 227400 | Total Loss: 0.002687 | Recon Loss: 0.002236 | Commit Loss: 0.000901 | Perplexity: 955.072453
2025-09-14 19:13:56,508 Stage: Train 0.5 | Epoch: 74 | Iter: 227600 | Total Loss: 0.002735 | Recon Loss: 0.002280 | Commit Loss: 0.000910 | Perplexity: 952.708889
2025-09-14 19:14:05,234 Stage: Train 0.5 | Epoch: 74 | Iter: 227800 | Total Loss: 0.002724 | Recon Loss: 0.002267 | Commit Loss: 0.000914 | Perplexity: 955.851631
Trainning Epoch:  45%|████▌     | 75/165 [2:46:00<3:19:27, 132.97s/it]2025-09-14 19:14:13,964 Stage: Train 0.5 | Epoch: 75 | Iter: 228000 | Total Loss: 0.002742 | Recon Loss: 0.002287 | Commit Loss: 0.000910 | Perplexity: 953.665389
2025-09-14 19:14:22,797 Stage: Train 0.5 | Epoch: 75 | Iter: 228200 | Total Loss: 0.002722 | Recon Loss: 0.002272 | Commit Loss: 0.000899 | Perplexity: 955.240237
2025-09-14 19:14:31,523 Stage: Train 0.5 | Epoch: 75 | Iter: 228400 | Total Loss: 0.002767 | Recon Loss: 0.002314 | Commit Loss: 0.000906 | Perplexity: 955.842173
2025-09-14 19:14:40,218 Stage: Train 0.5 | Epoch: 75 | Iter: 228600 | Total Loss: 0.002725 | Recon Loss: 0.002270 | Commit Loss: 0.000910 | Perplexity: 958.790989
2025-09-14 19:14:48,900 Stage: Train 0.5 | Epoch: 75 | Iter: 228800 | Total Loss: 0.002748 | Recon Loss: 0.002294 | Commit Loss: 0.000909 | Perplexity: 957.497614
2025-09-14 19:14:57,552 Stage: Train 0.5 | Epoch: 75 | Iter: 229000 | Total Loss: 0.002732 | Recon Loss: 0.002280 | Commit Loss: 0.000903 | Perplexity: 953.846682
2025-09-14 19:15:06,212 Stage: Train 0.5 | Epoch: 75 | Iter: 229200 | Total Loss: 0.002745 | Recon Loss: 0.002286 | Commit Loss: 0.000918 | Perplexity: 962.301122
2025-09-14 19:15:14,916 Stage: Train 0.5 | Epoch: 75 | Iter: 229400 | Total Loss: 0.002791 | Recon Loss: 0.002337 | Commit Loss: 0.000909 | Perplexity: 958.737885
2025-09-14 19:15:23,596 Stage: Train 0.5 | Epoch: 75 | Iter: 229600 | Total Loss: 0.002711 | Recon Loss: 0.002261 | Commit Loss: 0.000900 | Perplexity: 955.708919
2025-09-14 19:15:32,290 Stage: Train 0.5 | Epoch: 75 | Iter: 229800 | Total Loss: 0.002827 | Recon Loss: 0.002375 | Commit Loss: 0.000904 | Perplexity: 954.989717
2025-09-14 19:15:40,970 Stage: Train 0.5 | Epoch: 75 | Iter: 230000 | Total Loss: 0.002717 | Recon Loss: 0.002265 | Commit Loss: 0.000905 | Perplexity: 955.499164
2025-09-14 19:15:49,671 Stage: Train 0.5 | Epoch: 75 | Iter: 230200 | Total Loss: 0.002743 | Recon Loss: 0.002288 | Commit Loss: 0.000910 | Perplexity: 961.337600
2025-09-14 19:15:58,384 Stage: Train 0.5 | Epoch: 75 | Iter: 230400 | Total Loss: 0.002765 | Recon Loss: 0.002313 | Commit Loss: 0.000904 | Perplexity: 951.604123
2025-09-14 19:16:07,097 Stage: Train 0.5 | Epoch: 75 | Iter: 230600 | Total Loss: 0.002763 | Recon Loss: 0.002313 | Commit Loss: 0.000899 | Perplexity: 949.539214
2025-09-14 19:16:15,808 Stage: Train 0.5 | Epoch: 75 | Iter: 230800 | Total Loss: 0.002720 | Recon Loss: 0.002262 | Commit Loss: 0.000916 | Perplexity: 958.863291
Trainning Epoch:  46%|████▌     | 76/165 [2:48:13<3:16:54, 132.74s/it]2025-09-14 19:16:24,526 Stage: Train 0.5 | Epoch: 76 | Iter: 231000 | Total Loss: 0.002721 | Recon Loss: 0.002268 | Commit Loss: 0.000906 | Perplexity: 952.979848
2025-09-14 19:16:33,246 Stage: Train 0.5 | Epoch: 76 | Iter: 231200 | Total Loss: 0.002679 | Recon Loss: 0.002228 | Commit Loss: 0.000903 | Perplexity: 952.063637
2025-09-14 19:16:41,959 Stage: Train 0.5 | Epoch: 76 | Iter: 231400 | Total Loss: 0.002692 | Recon Loss: 0.002240 | Commit Loss: 0.000903 | Perplexity: 954.996024
2025-09-14 19:16:50,672 Stage: Train 0.5 | Epoch: 76 | Iter: 231600 | Total Loss: 0.002739 | Recon Loss: 0.002288 | Commit Loss: 0.000901 | Perplexity: 957.942247
2025-09-14 19:16:59,387 Stage: Train 0.5 | Epoch: 76 | Iter: 231800 | Total Loss: 0.002715 | Recon Loss: 0.002258 | Commit Loss: 0.000914 | Perplexity: 959.097330
2025-09-14 19:17:08,096 Stage: Train 0.5 | Epoch: 76 | Iter: 232000 | Total Loss: 0.002751 | Recon Loss: 0.002299 | Commit Loss: 0.000905 | Perplexity: 953.152887
2025-09-14 19:17:16,815 Stage: Train 0.5 | Epoch: 76 | Iter: 232200 | Total Loss: 0.002764 | Recon Loss: 0.002311 | Commit Loss: 0.000906 | Perplexity: 956.615435
2025-09-14 19:17:25,535 Stage: Train 0.5 | Epoch: 76 | Iter: 232400 | Total Loss: 0.002724 | Recon Loss: 0.002274 | Commit Loss: 0.000899 | Perplexity: 958.478294
2025-09-14 19:17:34,263 Stage: Train 0.5 | Epoch: 76 | Iter: 232600 | Total Loss: 0.002736 | Recon Loss: 0.002282 | Commit Loss: 0.000909 | Perplexity: 960.047749
2025-09-14 19:17:42,993 Stage: Train 0.5 | Epoch: 76 | Iter: 232800 | Total Loss: 0.002782 | Recon Loss: 0.002340 | Commit Loss: 0.000882 | Perplexity: 949.242242
2025-09-14 19:17:51,681 Stage: Train 0.5 | Epoch: 76 | Iter: 233000 | Total Loss: 0.002693 | Recon Loss: 0.002240 | Commit Loss: 0.000906 | Perplexity: 957.999020
2025-09-14 19:18:00,388 Stage: Train 0.5 | Epoch: 76 | Iter: 233200 | Total Loss: 0.002730 | Recon Loss: 0.002274 | Commit Loss: 0.000911 | Perplexity: 956.220634
2025-09-14 19:18:09,113 Stage: Train 0.5 | Epoch: 76 | Iter: 233400 | Total Loss: 0.002752 | Recon Loss: 0.002300 | Commit Loss: 0.000904 | Perplexity: 955.678617
2025-09-14 19:18:17,832 Stage: Train 0.5 | Epoch: 76 | Iter: 233600 | Total Loss: 0.002753 | Recon Loss: 0.002301 | Commit Loss: 0.000904 | Perplexity: 951.198647
2025-09-14 19:18:26,567 Stage: Train 0.5 | Epoch: 76 | Iter: 233800 | Total Loss: 0.002717 | Recon Loss: 0.002267 | Commit Loss: 0.000900 | Perplexity: 954.890287
Trainning Epoch:  47%|████▋     | 77/165 [2:50:25<3:14:32, 132.65s/it]2025-09-14 19:18:35,280 Stage: Train 0.5 | Epoch: 77 | Iter: 234000 | Total Loss: 0.002766 | Recon Loss: 0.002314 | Commit Loss: 0.000905 | Perplexity: 953.066183
2025-09-14 19:18:43,996 Stage: Train 0.5 | Epoch: 77 | Iter: 234200 | Total Loss: 0.002729 | Recon Loss: 0.002282 | Commit Loss: 0.000893 | Perplexity: 951.771764
2025-09-14 19:18:52,692 Stage: Train 0.5 | Epoch: 77 | Iter: 234400 | Total Loss: 0.002749 | Recon Loss: 0.002294 | Commit Loss: 0.000911 | Perplexity: 956.384704
2025-09-14 19:19:01,392 Stage: Train 0.5 | Epoch: 77 | Iter: 234600 | Total Loss: 0.002711 | Recon Loss: 0.002263 | Commit Loss: 0.000894 | Perplexity: 955.789755
2025-09-14 19:19:10,118 Stage: Train 0.5 | Epoch: 77 | Iter: 234800 | Total Loss: 0.002737 | Recon Loss: 0.002284 | Commit Loss: 0.000907 | Perplexity: 956.669244
2025-09-14 19:19:18,836 Stage: Train 0.5 | Epoch: 77 | Iter: 235000 | Total Loss: 0.002643 | Recon Loss: 0.002201 | Commit Loss: 0.000884 | Perplexity: 955.580736
2025-09-14 19:19:27,525 Stage: Train 0.5 | Epoch: 77 | Iter: 235200 | Total Loss: 0.002737 | Recon Loss: 0.002287 | Commit Loss: 0.000900 | Perplexity: 952.908940
2025-09-14 19:19:36,219 Stage: Train 0.5 | Epoch: 77 | Iter: 235400 | Total Loss: 0.002709 | Recon Loss: 0.002259 | Commit Loss: 0.000901 | Perplexity: 958.930818
2025-09-14 19:19:44,931 Stage: Train 0.5 | Epoch: 77 | Iter: 235600 | Total Loss: 0.002775 | Recon Loss: 0.002324 | Commit Loss: 0.000903 | Perplexity: 952.932218
2025-09-14 19:19:53,608 Stage: Train 0.5 | Epoch: 77 | Iter: 235800 | Total Loss: 0.002693 | Recon Loss: 0.002243 | Commit Loss: 0.000900 | Perplexity: 953.120546
2025-09-14 19:20:02,304 Stage: Train 0.5 | Epoch: 77 | Iter: 236000 | Total Loss: 0.002710 | Recon Loss: 0.002255 | Commit Loss: 0.000910 | Perplexity: 958.454847
2025-09-14 19:20:11,004 Stage: Train 0.5 | Epoch: 77 | Iter: 236200 | Total Loss: 0.002733 | Recon Loss: 0.002283 | Commit Loss: 0.000900 | Perplexity: 952.991032
2025-09-14 19:20:19,707 Stage: Train 0.5 | Epoch: 77 | Iter: 236400 | Total Loss: 0.002724 | Recon Loss: 0.002271 | Commit Loss: 0.000905 | Perplexity: 962.310912
2025-09-14 19:20:28,401 Stage: Train 0.5 | Epoch: 77 | Iter: 236600 | Total Loss: 0.002756 | Recon Loss: 0.002304 | Commit Loss: 0.000904 | Perplexity: 960.318256
2025-09-14 19:20:37,115 Stage: Train 0.5 | Epoch: 77 | Iter: 236800 | Total Loss: 0.002708 | Recon Loss: 0.002253 | Commit Loss: 0.000910 | Perplexity: 954.272976
Trainning Epoch:  47%|████▋     | 78/165 [2:52:37<3:12:09, 132.52s/it]2025-09-14 19:20:45,863 Stage: Train 0.5 | Epoch: 78 | Iter: 237000 | Total Loss: 0.002754 | Recon Loss: 0.002301 | Commit Loss: 0.000907 | Perplexity: 952.123517
2025-09-14 19:20:54,556 Stage: Train 0.5 | Epoch: 78 | Iter: 237200 | Total Loss: 0.002681 | Recon Loss: 0.002233 | Commit Loss: 0.000895 | Perplexity: 959.336175
2025-09-14 19:21:03,251 Stage: Train 0.5 | Epoch: 78 | Iter: 237400 | Total Loss: 0.002675 | Recon Loss: 0.002227 | Commit Loss: 0.000895 | Perplexity: 952.351650
2025-09-14 19:21:11,950 Stage: Train 0.5 | Epoch: 78 | Iter: 237600 | Total Loss: 0.002755 | Recon Loss: 0.002304 | Commit Loss: 0.000903 | Perplexity: 952.385377
2025-09-14 19:21:20,674 Stage: Train 0.5 | Epoch: 78 | Iter: 237800 | Total Loss: 0.002761 | Recon Loss: 0.002310 | Commit Loss: 0.000903 | Perplexity: 953.738693
2025-09-14 19:21:29,355 Stage: Train 0.5 | Epoch: 78 | Iter: 238000 | Total Loss: 0.002701 | Recon Loss: 0.002252 | Commit Loss: 0.000898 | Perplexity: 954.551638
2025-09-14 19:21:38,044 Stage: Train 0.5 | Epoch: 78 | Iter: 238200 | Total Loss: 0.002698 | Recon Loss: 0.002247 | Commit Loss: 0.000900 | Perplexity: 958.534666
2025-09-14 19:21:46,743 Stage: Train 0.5 | Epoch: 78 | Iter: 238400 | Total Loss: 0.002726 | Recon Loss: 0.002277 | Commit Loss: 0.000897 | Perplexity: 956.390572
2025-09-14 19:21:55,458 Stage: Train 0.5 | Epoch: 78 | Iter: 238600 | Total Loss: 0.002688 | Recon Loss: 0.002240 | Commit Loss: 0.000895 | Perplexity: 961.706335
2025-09-14 19:22:04,200 Stage: Train 0.5 | Epoch: 78 | Iter: 238800 | Total Loss: 0.002806 | Recon Loss: 0.002354 | Commit Loss: 0.000904 | Perplexity: 956.702191
2025-09-14 19:22:12,988 Stage: Train 0.5 | Epoch: 78 | Iter: 239000 | Total Loss: 0.002713 | Recon Loss: 0.002262 | Commit Loss: 0.000903 | Perplexity: 955.423666
2025-09-14 19:22:21,709 Stage: Train 0.5 | Epoch: 78 | Iter: 239200 | Total Loss: 0.002739 | Recon Loss: 0.002285 | Commit Loss: 0.000909 | Perplexity: 961.847182
2025-09-14 19:22:30,408 Stage: Train 0.5 | Epoch: 78 | Iter: 239400 | Total Loss: 0.002727 | Recon Loss: 0.002279 | Commit Loss: 0.000896 | Perplexity: 953.677773
2025-09-14 19:22:39,109 Stage: Train 0.5 | Epoch: 78 | Iter: 239600 | Total Loss: 0.002772 | Recon Loss: 0.002314 | Commit Loss: 0.000916 | Perplexity: 958.775818
2025-09-14 19:22:47,853 Stage: Train 0.5 | Epoch: 78 | Iter: 239800 | Total Loss: 0.002702 | Recon Loss: 0.002256 | Commit Loss: 0.000893 | Perplexity: 952.348297
2025-09-14 19:22:56,578 Stage: Train 0.5 | Epoch: 78 | Iter: 240000 | Total Loss: 0.002765 | Recon Loss: 0.002315 | Commit Loss: 0.000899 | Perplexity: 955.021041
2025-09-14 19:22:56,578 Saving model at iteration 240000
2025-09-14 19:22:56,737 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_79_step_240000
2025-09-14 19:22:56,971 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_79_step_240000/pytorch_model.bin
2025-09-14 19:22:57,333 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_79_step_240000/optimizer.bin
2025-09-14 19:22:57,333 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_79_step_240000/scheduler.bin
2025-09-14 19:22:57,334 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_79_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 79/165 [2:54:50<3:10:13, 132.71s/it]2025-09-14 19:23:06,059 Stage: Train 0.5 | Epoch: 79 | Iter: 240200 | Total Loss: 0.002678 | Recon Loss: 0.002229 | Commit Loss: 0.000899 | Perplexity: 956.838469
2025-09-14 19:23:14,777 Stage: Train 0.5 | Epoch: 79 | Iter: 240400 | Total Loss: 0.002718 | Recon Loss: 0.002271 | Commit Loss: 0.000895 | Perplexity: 957.116630
2025-09-14 19:23:23,467 Stage: Train 0.5 | Epoch: 79 | Iter: 240600 | Total Loss: 0.002719 | Recon Loss: 0.002270 | Commit Loss: 0.000898 | Perplexity: 950.251071
2025-09-14 19:23:32,216 Stage: Train 0.5 | Epoch: 79 | Iter: 240800 | Total Loss: 0.002710 | Recon Loss: 0.002259 | Commit Loss: 0.000903 | Perplexity: 957.598689
2025-09-14 19:23:40,894 Stage: Train 0.5 | Epoch: 79 | Iter: 241000 | Total Loss: 0.002740 | Recon Loss: 0.002292 | Commit Loss: 0.000896 | Perplexity: 960.140734
2025-09-14 19:23:49,591 Stage: Train 0.5 | Epoch: 79 | Iter: 241200 | Total Loss: 0.002677 | Recon Loss: 0.002229 | Commit Loss: 0.000898 | Perplexity: 952.565270
2025-09-14 19:23:58,296 Stage: Train 0.5 | Epoch: 79 | Iter: 241400 | Total Loss: 0.002704 | Recon Loss: 0.002254 | Commit Loss: 0.000900 | Perplexity: 956.632613
2025-09-14 19:24:06,999 Stage: Train 0.5 | Epoch: 79 | Iter: 241600 | Total Loss: 0.002720 | Recon Loss: 0.002266 | Commit Loss: 0.000909 | Perplexity: 960.833850
2025-09-14 19:24:15,716 Stage: Train 0.5 | Epoch: 79 | Iter: 241800 | Total Loss: 0.002742 | Recon Loss: 0.002295 | Commit Loss: 0.000893 | Perplexity: 951.085599
2025-09-14 19:24:24,420 Stage: Train 0.5 | Epoch: 79 | Iter: 242000 | Total Loss: 0.002718 | Recon Loss: 0.002265 | Commit Loss: 0.000906 | Perplexity: 959.626107
2025-09-14 19:24:33,142 Stage: Train 0.5 | Epoch: 79 | Iter: 242200 | Total Loss: 0.002679 | Recon Loss: 0.002231 | Commit Loss: 0.000896 | Perplexity: 957.746674
2025-09-14 19:24:41,840 Stage: Train 0.5 | Epoch: 79 | Iter: 242400 | Total Loss: 0.002710 | Recon Loss: 0.002257 | Commit Loss: 0.000904 | Perplexity: 959.077039
2025-09-14 19:24:50,533 Stage: Train 0.5 | Epoch: 79 | Iter: 242600 | Total Loss: 0.002731 | Recon Loss: 0.002274 | Commit Loss: 0.000913 | Perplexity: 959.137336
2025-09-14 19:24:59,244 Stage: Train 0.5 | Epoch: 79 | Iter: 242800 | Total Loss: 0.002690 | Recon Loss: 0.002247 | Commit Loss: 0.000886 | Perplexity: 956.117649
2025-09-14 19:25:07,992 Stage: Train 0.5 | Epoch: 79 | Iter: 243000 | Total Loss: 0.002731 | Recon Loss: 0.002287 | Commit Loss: 0.000888 | Perplexity: 951.471909
Trainning Epoch:  48%|████▊     | 80/165 [2:57:03<3:07:50, 132.59s/it]2025-09-14 19:25:16,756 Stage: Train 0.5 | Epoch: 80 | Iter: 243200 | Total Loss: 0.002709 | Recon Loss: 0.002259 | Commit Loss: 0.000900 | Perplexity: 953.336433
2025-09-14 19:25:25,476 Stage: Train 0.5 | Epoch: 80 | Iter: 243400 | Total Loss: 0.002727 | Recon Loss: 0.002278 | Commit Loss: 0.000899 | Perplexity: 959.706976
2025-09-14 19:25:34,175 Stage: Train 0.5 | Epoch: 80 | Iter: 243600 | Total Loss: 0.002728 | Recon Loss: 0.002281 | Commit Loss: 0.000893 | Perplexity: 951.838803
2025-09-14 19:25:42,992 Stage: Train 0.5 | Epoch: 80 | Iter: 243800 | Total Loss: 0.002742 | Recon Loss: 0.002300 | Commit Loss: 0.000886 | Perplexity: 955.571685
2025-09-14 19:25:51,720 Stage: Train 0.5 | Epoch: 80 | Iter: 244000 | Total Loss: 0.002689 | Recon Loss: 0.002245 | Commit Loss: 0.000889 | Perplexity: 955.024423
2025-09-14 19:26:00,433 Stage: Train 0.5 | Epoch: 80 | Iter: 244200 | Total Loss: 0.002735 | Recon Loss: 0.002283 | Commit Loss: 0.000905 | Perplexity: 963.462547
2025-09-14 19:26:09,159 Stage: Train 0.5 | Epoch: 80 | Iter: 244400 | Total Loss: 0.002673 | Recon Loss: 0.002226 | Commit Loss: 0.000893 | Perplexity: 950.665210
2025-09-14 19:26:17,926 Stage: Train 0.5 | Epoch: 80 | Iter: 244600 | Total Loss: 0.002738 | Recon Loss: 0.002289 | Commit Loss: 0.000897 | Perplexity: 953.941121
2025-09-14 19:26:26,667 Stage: Train 0.5 | Epoch: 80 | Iter: 244800 | Total Loss: 0.002760 | Recon Loss: 0.002316 | Commit Loss: 0.000887 | Perplexity: 955.895475
2025-09-14 19:26:35,377 Stage: Train 0.5 | Epoch: 80 | Iter: 245000 | Total Loss: 0.002684 | Recon Loss: 0.002239 | Commit Loss: 0.000890 | Perplexity: 957.307713
2025-09-14 19:26:44,160 Stage: Train 0.5 | Epoch: 80 | Iter: 245200 | Total Loss: 0.002701 | Recon Loss: 0.002248 | Commit Loss: 0.000907 | Perplexity: 965.405055
2025-09-14 19:26:52,908 Stage: Train 0.5 | Epoch: 80 | Iter: 245400 | Total Loss: 0.002693 | Recon Loss: 0.002246 | Commit Loss: 0.000893 | Perplexity: 957.169854
2025-09-14 19:27:01,632 Stage: Train 0.5 | Epoch: 80 | Iter: 245600 | Total Loss: 0.002742 | Recon Loss: 0.002293 | Commit Loss: 0.000897 | Perplexity: 955.612722
2025-09-14 19:27:10,354 Stage: Train 0.5 | Epoch: 80 | Iter: 245800 | Total Loss: 0.002669 | Recon Loss: 0.002220 | Commit Loss: 0.000898 | Perplexity: 958.604362
2025-09-14 19:27:19,089 Stage: Train 0.5 | Epoch: 80 | Iter: 246000 | Total Loss: 0.002778 | Recon Loss: 0.002333 | Commit Loss: 0.000889 | Perplexity: 947.512697
Trainning Epoch:  49%|████▉     | 81/165 [2:59:15<3:05:40, 132.63s/it]2025-09-14 19:27:27,794 Stage: Train 0.5 | Epoch: 81 | Iter: 246200 | Total Loss: 0.002638 | Recon Loss: 0.002189 | Commit Loss: 0.000899 | Perplexity: 955.143059
2025-09-14 19:27:36,484 Stage: Train 0.5 | Epoch: 81 | Iter: 246400 | Total Loss: 0.002720 | Recon Loss: 0.002271 | Commit Loss: 0.000897 | Perplexity: 959.131879
2025-09-14 19:27:45,188 Stage: Train 0.5 | Epoch: 81 | Iter: 246600 | Total Loss: 0.002642 | Recon Loss: 0.002192 | Commit Loss: 0.000900 | Perplexity: 962.558014
2025-09-14 19:27:53,880 Stage: Train 0.5 | Epoch: 81 | Iter: 246800 | Total Loss: 0.002712 | Recon Loss: 0.002263 | Commit Loss: 0.000898 | Perplexity: 961.197730
2025-09-14 19:28:02,578 Stage: Train 0.5 | Epoch: 81 | Iter: 247000 | Total Loss: 0.002681 | Recon Loss: 0.002237 | Commit Loss: 0.000890 | Perplexity: 955.112371
2025-09-14 19:28:11,283 Stage: Train 0.5 | Epoch: 81 | Iter: 247200 | Total Loss: 0.002727 | Recon Loss: 0.002280 | Commit Loss: 0.000894 | Perplexity: 956.085810
2025-09-14 19:28:19,978 Stage: Train 0.5 | Epoch: 81 | Iter: 247400 | Total Loss: 0.002752 | Recon Loss: 0.002297 | Commit Loss: 0.000909 | Perplexity: 958.728400
2025-09-14 19:28:28,686 Stage: Train 0.5 | Epoch: 81 | Iter: 247600 | Total Loss: 0.002762 | Recon Loss: 0.002312 | Commit Loss: 0.000900 | Perplexity: 956.962729
2025-09-14 19:28:37,387 Stage: Train 0.5 | Epoch: 81 | Iter: 247800 | Total Loss: 0.002665 | Recon Loss: 0.002219 | Commit Loss: 0.000891 | Perplexity: 961.250652
2025-09-14 19:28:46,096 Stage: Train 0.5 | Epoch: 81 | Iter: 248000 | Total Loss: 0.002698 | Recon Loss: 0.002249 | Commit Loss: 0.000898 | Perplexity: 956.884717
2025-09-14 19:28:54,826 Stage: Train 0.5 | Epoch: 81 | Iter: 248200 | Total Loss: 0.002726 | Recon Loss: 0.002280 | Commit Loss: 0.000893 | Perplexity: 956.347412
2025-09-14 19:29:03,513 Stage: Train 0.5 | Epoch: 81 | Iter: 248400 | Total Loss: 0.002731 | Recon Loss: 0.002282 | Commit Loss: 0.000899 | Perplexity: 958.202136
2025-09-14 19:29:12,207 Stage: Train 0.5 | Epoch: 81 | Iter: 248600 | Total Loss: 0.002665 | Recon Loss: 0.002222 | Commit Loss: 0.000886 | Perplexity: 957.836411
2025-09-14 19:29:20,899 Stage: Train 0.5 | Epoch: 81 | Iter: 248800 | Total Loss: 0.002682 | Recon Loss: 0.002234 | Commit Loss: 0.000896 | Perplexity: 961.755056
2025-09-14 19:29:29,605 Stage: Train 0.5 | Epoch: 81 | Iter: 249000 | Total Loss: 0.002695 | Recon Loss: 0.002250 | Commit Loss: 0.000890 | Perplexity: 958.344233
Trainning Epoch:  50%|████▉     | 82/165 [3:01:28<3:03:17, 132.50s/it]2025-09-14 19:29:38,324 Stage: Train 0.5 | Epoch: 82 | Iter: 249200 | Total Loss: 0.002680 | Recon Loss: 0.002232 | Commit Loss: 0.000895 | Perplexity: 956.695123
2025-09-14 19:29:47,056 Stage: Train 0.5 | Epoch: 82 | Iter: 249400 | Total Loss: 0.002675 | Recon Loss: 0.002229 | Commit Loss: 0.000892 | Perplexity: 960.844299
2025-09-14 19:29:55,788 Stage: Train 0.5 | Epoch: 82 | Iter: 249600 | Total Loss: 0.002655 | Recon Loss: 0.002207 | Commit Loss: 0.000897 | Perplexity: 960.032514
2025-09-14 19:30:04,486 Stage: Train 0.5 | Epoch: 82 | Iter: 249800 | Total Loss: 0.002679 | Recon Loss: 0.002232 | Commit Loss: 0.000893 | Perplexity: 958.011808
2025-09-14 19:30:13,192 Stage: Train 0.5 | Epoch: 82 | Iter: 250000 | Total Loss: 0.002729 | Recon Loss: 0.002282 | Commit Loss: 0.000893 | Perplexity: 965.158762
2025-09-14 19:30:21,930 Stage: Train 0.5 | Epoch: 82 | Iter: 250200 | Total Loss: 0.002688 | Recon Loss: 0.002242 | Commit Loss: 0.000893 | Perplexity: 955.818143
2025-09-14 19:30:30,665 Stage: Train 0.5 | Epoch: 82 | Iter: 250400 | Total Loss: 0.002662 | Recon Loss: 0.002216 | Commit Loss: 0.000893 | Perplexity: 963.367167
2025-09-14 19:30:39,390 Stage: Train 0.5 | Epoch: 82 | Iter: 250600 | Total Loss: 0.002678 | Recon Loss: 0.002234 | Commit Loss: 0.000888 | Perplexity: 955.621517
2025-09-14 19:30:48,114 Stage: Train 0.5 | Epoch: 82 | Iter: 250800 | Total Loss: 0.002670 | Recon Loss: 0.002227 | Commit Loss: 0.000886 | Perplexity: 956.651944
2025-09-14 19:30:56,832 Stage: Train 0.5 | Epoch: 82 | Iter: 251000 | Total Loss: 0.002747 | Recon Loss: 0.002305 | Commit Loss: 0.000883 | Perplexity: 954.199908
2025-09-14 19:31:05,584 Stage: Train 0.5 | Epoch: 82 | Iter: 251200 | Total Loss: 0.002697 | Recon Loss: 0.002254 | Commit Loss: 0.000887 | Perplexity: 957.135727
2025-09-14 19:31:14,308 Stage: Train 0.5 | Epoch: 82 | Iter: 251400 | Total Loss: 0.002690 | Recon Loss: 0.002243 | Commit Loss: 0.000894 | Perplexity: 961.960887
2025-09-14 19:31:23,031 Stage: Train 0.5 | Epoch: 82 | Iter: 251600 | Total Loss: 0.002716 | Recon Loss: 0.002266 | Commit Loss: 0.000900 | Perplexity: 963.265722
2025-09-14 19:31:31,742 Stage: Train 0.5 | Epoch: 82 | Iter: 251800 | Total Loss: 0.002730 | Recon Loss: 0.002286 | Commit Loss: 0.000888 | Perplexity: 955.595472
2025-09-14 19:31:40,476 Stage: Train 0.5 | Epoch: 82 | Iter: 252000 | Total Loss: 0.002661 | Recon Loss: 0.002217 | Commit Loss: 0.000888 | Perplexity: 955.582100
Trainning Epoch:  50%|█████     | 83/165 [3:03:40<3:01:05, 132.51s/it]2025-09-14 19:31:49,208 Stage: Train 0.5 | Epoch: 83 | Iter: 252200 | Total Loss: 0.002759 | Recon Loss: 0.002317 | Commit Loss: 0.000884 | Perplexity: 955.895581
2025-09-14 19:31:57,956 Stage: Train 0.5 | Epoch: 83 | Iter: 252400 | Total Loss: 0.002654 | Recon Loss: 0.002211 | Commit Loss: 0.000886 | Perplexity: 960.826310
2025-09-14 19:32:06,708 Stage: Train 0.5 | Epoch: 83 | Iter: 252600 | Total Loss: 0.002690 | Recon Loss: 0.002245 | Commit Loss: 0.000891 | Perplexity: 957.326678
2025-09-14 19:32:15,428 Stage: Train 0.5 | Epoch: 83 | Iter: 252800 | Total Loss: 0.002679 | Recon Loss: 0.002236 | Commit Loss: 0.000886 | Perplexity: 960.946947
2025-09-14 19:32:24,147 Stage: Train 0.5 | Epoch: 83 | Iter: 253000 | Total Loss: 0.002680 | Recon Loss: 0.002236 | Commit Loss: 0.000887 | Perplexity: 955.794996
2025-09-14 19:32:32,828 Stage: Train 0.5 | Epoch: 83 | Iter: 253200 | Total Loss: 0.002610 | Recon Loss: 0.002163 | Commit Loss: 0.000895 | Perplexity: 960.849594
2025-09-14 19:32:41,523 Stage: Train 0.5 | Epoch: 83 | Iter: 253400 | Total Loss: 0.002675 | Recon Loss: 0.002230 | Commit Loss: 0.000890 | Perplexity: 960.425408
2025-09-14 19:32:50,256 Stage: Train 0.5 | Epoch: 83 | Iter: 253600 | Total Loss: 0.002669 | Recon Loss: 0.002226 | Commit Loss: 0.000886 | Perplexity: 957.097796
2025-09-14 19:32:58,979 Stage: Train 0.5 | Epoch: 83 | Iter: 253800 | Total Loss: 0.002702 | Recon Loss: 0.002259 | Commit Loss: 0.000886 | Perplexity: 959.253920
2025-09-14 19:33:07,711 Stage: Train 0.5 | Epoch: 83 | Iter: 254000 | Total Loss: 0.002688 | Recon Loss: 0.002243 | Commit Loss: 0.000890 | Perplexity: 965.111592
2025-09-14 19:33:16,412 Stage: Train 0.5 | Epoch: 83 | Iter: 254200 | Total Loss: 0.002665 | Recon Loss: 0.002221 | Commit Loss: 0.000889 | Perplexity: 959.792849
2025-09-14 19:33:25,128 Stage: Train 0.5 | Epoch: 83 | Iter: 254400 | Total Loss: 0.002706 | Recon Loss: 0.002257 | Commit Loss: 0.000899 | Perplexity: 959.248732
2025-09-14 19:33:33,839 Stage: Train 0.5 | Epoch: 83 | Iter: 254600 | Total Loss: 0.002662 | Recon Loss: 0.002213 | Commit Loss: 0.000897 | Perplexity: 962.186688
2025-09-14 19:33:42,526 Stage: Train 0.5 | Epoch: 83 | Iter: 254800 | Total Loss: 0.002675 | Recon Loss: 0.002233 | Commit Loss: 0.000884 | Perplexity: 952.895309
2025-09-14 19:33:51,230 Stage: Train 0.5 | Epoch: 83 | Iter: 255000 | Total Loss: 0.002648 | Recon Loss: 0.002207 | Commit Loss: 0.000881 | Perplexity: 954.092253
Trainning Epoch:  51%|█████     | 84/165 [3:05:53<2:58:50, 132.47s/it]2025-09-14 19:33:59,944 Stage: Train 0.5 | Epoch: 84 | Iter: 255200 | Total Loss: 0.002712 | Recon Loss: 0.002269 | Commit Loss: 0.000886 | Perplexity: 957.623948
2025-09-14 19:34:08,680 Stage: Train 0.5 | Epoch: 84 | Iter: 255400 | Total Loss: 0.002637 | Recon Loss: 0.002197 | Commit Loss: 0.000880 | Perplexity: 956.733410
2025-09-14 19:34:17,404 Stage: Train 0.5 | Epoch: 84 | Iter: 255600 | Total Loss: 0.002670 | Recon Loss: 0.002227 | Commit Loss: 0.000886 | Perplexity: 960.631443
2025-09-14 19:34:26,143 Stage: Train 0.5 | Epoch: 84 | Iter: 255800 | Total Loss: 0.002654 | Recon Loss: 0.002214 | Commit Loss: 0.000879 | Perplexity: 958.967599
2025-09-14 19:34:34,870 Stage: Train 0.5 | Epoch: 84 | Iter: 256000 | Total Loss: 0.002685 | Recon Loss: 0.002240 | Commit Loss: 0.000888 | Perplexity: 960.990620
2025-09-14 19:34:43,580 Stage: Train 0.5 | Epoch: 84 | Iter: 256200 | Total Loss: 0.002623 | Recon Loss: 0.002177 | Commit Loss: 0.000893 | Perplexity: 957.318763
2025-09-14 19:34:52,288 Stage: Train 0.5 | Epoch: 84 | Iter: 256400 | Total Loss: 0.002671 | Recon Loss: 0.002225 | Commit Loss: 0.000892 | Perplexity: 954.683943
2025-09-14 19:35:01,012 Stage: Train 0.5 | Epoch: 84 | Iter: 256600 | Total Loss: 0.002745 | Recon Loss: 0.002305 | Commit Loss: 0.000882 | Perplexity: 955.413867
2025-09-14 19:35:09,717 Stage: Train 0.5 | Epoch: 84 | Iter: 256800 | Total Loss: 0.002621 | Recon Loss: 0.002178 | Commit Loss: 0.000887 | Perplexity: 961.442203
2025-09-14 19:35:18,420 Stage: Train 0.5 | Epoch: 84 | Iter: 257000 | Total Loss: 0.002668 | Recon Loss: 0.002221 | Commit Loss: 0.000894 | Perplexity: 958.093283
2025-09-14 19:35:27,222 Stage: Train 0.5 | Epoch: 84 | Iter: 257200 | Total Loss: 0.002673 | Recon Loss: 0.002232 | Commit Loss: 0.000882 | Perplexity: 958.147748
2025-09-14 19:35:35,989 Stage: Train 0.5 | Epoch: 84 | Iter: 257400 | Total Loss: 0.002677 | Recon Loss: 0.002230 | Commit Loss: 0.000894 | Perplexity: 957.762564
2025-09-14 19:35:44,711 Stage: Train 0.5 | Epoch: 84 | Iter: 257600 | Total Loss: 0.002639 | Recon Loss: 0.002196 | Commit Loss: 0.000887 | Perplexity: 960.139487
2025-09-14 19:35:53,425 Stage: Train 0.5 | Epoch: 84 | Iter: 257800 | Total Loss: 0.002650 | Recon Loss: 0.002202 | Commit Loss: 0.000895 | Perplexity: 959.157782
2025-09-14 19:36:02,198 Stage: Train 0.5 | Epoch: 84 | Iter: 258000 | Total Loss: 0.002731 | Recon Loss: 0.002284 | Commit Loss: 0.000894 | Perplexity: 960.126665
2025-09-14 19:36:10,980 Stage: Train 0.5 | Epoch: 84 | Iter: 258200 | Total Loss: 0.002636 | Recon Loss: 0.002194 | Commit Loss: 0.000885 | Perplexity: 962.443025
Trainning Epoch:  52%|█████▏    | 85/165 [3:08:05<2:56:43, 132.55s/it]2025-09-14 19:36:19,781 Stage: Train 0.5 | Epoch: 85 | Iter: 258400 | Total Loss: 0.002579 | Recon Loss: 0.002140 | Commit Loss: 0.000879 | Perplexity: 954.411265
2025-09-14 19:36:28,576 Stage: Train 0.5 | Epoch: 85 | Iter: 258600 | Total Loss: 0.002703 | Recon Loss: 0.002260 | Commit Loss: 0.000888 | Perplexity: 959.822208
2025-09-14 19:36:37,350 Stage: Train 0.5 | Epoch: 85 | Iter: 258800 | Total Loss: 0.002624 | Recon Loss: 0.002180 | Commit Loss: 0.000887 | Perplexity: 959.331555
2025-09-14 19:36:46,099 Stage: Train 0.5 | Epoch: 85 | Iter: 259000 | Total Loss: 0.002670 | Recon Loss: 0.002226 | Commit Loss: 0.000887 | Perplexity: 956.655401
2025-09-14 19:36:54,832 Stage: Train 0.5 | Epoch: 85 | Iter: 259200 | Total Loss: 0.002632 | Recon Loss: 0.002187 | Commit Loss: 0.000891 | Perplexity: 957.403008
2025-09-14 19:37:03,554 Stage: Train 0.5 | Epoch: 85 | Iter: 259400 | Total Loss: 0.002666 | Recon Loss: 0.002222 | Commit Loss: 0.000889 | Perplexity: 962.221977
2025-09-14 19:37:12,274 Stage: Train 0.5 | Epoch: 85 | Iter: 259600 | Total Loss: 0.002654 | Recon Loss: 0.002209 | Commit Loss: 0.000890 | Perplexity: 964.346027
2025-09-14 19:37:20,999 Stage: Train 0.5 | Epoch: 85 | Iter: 259800 | Total Loss: 0.002705 | Recon Loss: 0.002260 | Commit Loss: 0.000890 | Perplexity: 957.129835
2025-09-14 19:37:29,737 Stage: Train 0.5 | Epoch: 85 | Iter: 260000 | Total Loss: 0.002651 | Recon Loss: 0.002208 | Commit Loss: 0.000887 | Perplexity: 961.391997
2025-09-14 19:37:29,737 Saving model at iteration 260000
2025-09-14 19:37:30,145 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_86_step_260000
2025-09-14 19:37:30,385 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_86_step_260000/pytorch_model.bin
2025-09-14 19:37:30,760 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_86_step_260000/optimizer.bin
2025-09-14 19:37:30,761 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_86_step_260000/scheduler.bin
2025-09-14 19:37:30,761 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_86_step_260000/random_states_0.pkl
2025-09-14 19:37:39,830 Stage: Train 0.5 | Epoch: 85 | Iter: 260200 | Total Loss: 0.002641 | Recon Loss: 0.002198 | Commit Loss: 0.000886 | Perplexity: 961.082252
2025-09-14 19:37:48,572 Stage: Train 0.5 | Epoch: 85 | Iter: 260400 | Total Loss: 0.002655 | Recon Loss: 0.002209 | Commit Loss: 0.000893 | Perplexity: 964.997765
2025-09-14 19:37:57,343 Stage: Train 0.5 | Epoch: 85 | Iter: 260600 | Total Loss: 0.002654 | Recon Loss: 0.002207 | Commit Loss: 0.000894 | Perplexity: 965.565851
2025-09-14 19:38:06,111 Stage: Train 0.5 | Epoch: 85 | Iter: 260800 | Total Loss: 0.002670 | Recon Loss: 0.002224 | Commit Loss: 0.000893 | Perplexity: 956.626531
2025-09-14 19:38:14,858 Stage: Train 0.5 | Epoch: 85 | Iter: 261000 | Total Loss: 0.002682 | Recon Loss: 0.002235 | Commit Loss: 0.000892 | Perplexity: 962.222368
2025-09-14 19:38:23,643 Stage: Train 0.5 | Epoch: 85 | Iter: 261200 | Total Loss: 0.002637 | Recon Loss: 0.002197 | Commit Loss: 0.000879 | Perplexity: 957.786395
Trainning Epoch:  52%|█████▏    | 86/165 [3:10:20<2:55:13, 133.08s/it]2025-09-14 19:38:32,420 Stage: Train 0.5 | Epoch: 86 | Iter: 261400 | Total Loss: 0.002638 | Recon Loss: 0.002196 | Commit Loss: 0.000884 | Perplexity: 960.087280
2025-09-14 19:38:41,199 Stage: Train 0.5 | Epoch: 86 | Iter: 261600 | Total Loss: 0.002629 | Recon Loss: 0.002189 | Commit Loss: 0.000880 | Perplexity: 957.470419
2025-09-14 19:38:49,969 Stage: Train 0.5 | Epoch: 86 | Iter: 261800 | Total Loss: 0.002643 | Recon Loss: 0.002200 | Commit Loss: 0.000887 | Perplexity: 961.171304
2025-09-14 19:38:58,732 Stage: Train 0.5 | Epoch: 86 | Iter: 262000 | Total Loss: 0.002683 | Recon Loss: 0.002237 | Commit Loss: 0.000892 | Perplexity: 962.752631
2025-09-14 19:39:07,524 Stage: Train 0.5 | Epoch: 86 | Iter: 262200 | Total Loss: 0.002637 | Recon Loss: 0.002198 | Commit Loss: 0.000879 | Perplexity: 958.056172
2025-09-14 19:39:16,299 Stage: Train 0.5 | Epoch: 86 | Iter: 262400 | Total Loss: 0.002681 | Recon Loss: 0.002236 | Commit Loss: 0.000890 | Perplexity: 960.347655
2025-09-14 19:39:25,091 Stage: Train 0.5 | Epoch: 86 | Iter: 262600 | Total Loss: 0.002642 | Recon Loss: 0.002197 | Commit Loss: 0.000889 | Perplexity: 961.098582
2025-09-14 19:39:33,855 Stage: Train 0.5 | Epoch: 86 | Iter: 262800 | Total Loss: 0.002639 | Recon Loss: 0.002194 | Commit Loss: 0.000889 | Perplexity: 958.406310
2025-09-14 19:39:42,603 Stage: Train 0.5 | Epoch: 86 | Iter: 263000 | Total Loss: 0.002674 | Recon Loss: 0.002228 | Commit Loss: 0.000893 | Perplexity: 964.883578
2025-09-14 19:39:51,351 Stage: Train 0.5 | Epoch: 86 | Iter: 263200 | Total Loss: 0.002738 | Recon Loss: 0.002297 | Commit Loss: 0.000881 | Perplexity: 959.020504
2025-09-14 19:40:00,089 Stage: Train 0.5 | Epoch: 86 | Iter: 263400 | Total Loss: 0.002681 | Recon Loss: 0.002239 | Commit Loss: 0.000884 | Perplexity: 963.273897
2025-09-14 19:40:08,850 Stage: Train 0.5 | Epoch: 86 | Iter: 263600 | Total Loss: 0.002655 | Recon Loss: 0.002209 | Commit Loss: 0.000892 | Perplexity: 959.732636
2025-09-14 19:40:17,597 Stage: Train 0.5 | Epoch: 86 | Iter: 263800 | Total Loss: 0.002686 | Recon Loss: 0.002241 | Commit Loss: 0.000891 | Perplexity: 957.665703
2025-09-14 19:40:26,320 Stage: Train 0.5 | Epoch: 86 | Iter: 264000 | Total Loss: 0.002614 | Recon Loss: 0.002175 | Commit Loss: 0.000877 | Perplexity: 957.348980
2025-09-14 19:40:35,084 Stage: Train 0.5 | Epoch: 86 | Iter: 264200 | Total Loss: 0.002643 | Recon Loss: 0.002201 | Commit Loss: 0.000883 | Perplexity: 963.138920
Trainning Epoch:  53%|█████▎    | 87/165 [3:12:33<2:53:00, 133.09s/it]2025-09-14 19:40:43,840 Stage: Train 0.5 | Epoch: 87 | Iter: 264400 | Total Loss: 0.002630 | Recon Loss: 0.002187 | Commit Loss: 0.000886 | Perplexity: 958.030264
2025-09-14 19:40:52,586 Stage: Train 0.5 | Epoch: 87 | Iter: 264600 | Total Loss: 0.002621 | Recon Loss: 0.002181 | Commit Loss: 0.000880 | Perplexity: 959.117461
2025-09-14 19:41:01,358 Stage: Train 0.5 | Epoch: 87 | Iter: 264800 | Total Loss: 0.002688 | Recon Loss: 0.002244 | Commit Loss: 0.000888 | Perplexity: 961.846609
2025-09-14 19:41:10,140 Stage: Train 0.5 | Epoch: 87 | Iter: 265000 | Total Loss: 0.002616 | Recon Loss: 0.002172 | Commit Loss: 0.000887 | Perplexity: 962.870616
2025-09-14 19:41:18,883 Stage: Train 0.5 | Epoch: 87 | Iter: 265200 | Total Loss: 0.002674 | Recon Loss: 0.002236 | Commit Loss: 0.000877 | Perplexity: 957.838397
2025-09-14 19:41:27,604 Stage: Train 0.5 | Epoch: 87 | Iter: 265400 | Total Loss: 0.002637 | Recon Loss: 0.002196 | Commit Loss: 0.000882 | Perplexity: 958.375293
2025-09-14 19:41:36,364 Stage: Train 0.5 | Epoch: 87 | Iter: 265600 | Total Loss: 0.002613 | Recon Loss: 0.002169 | Commit Loss: 0.000888 | Perplexity: 964.060272
2025-09-14 19:41:45,112 Stage: Train 0.5 | Epoch: 87 | Iter: 265800 | Total Loss: 0.002614 | Recon Loss: 0.002169 | Commit Loss: 0.000891 | Perplexity: 964.304617
2025-09-14 19:41:53,862 Stage: Train 0.5 | Epoch: 87 | Iter: 266000 | Total Loss: 0.002650 | Recon Loss: 0.002208 | Commit Loss: 0.000884 | Perplexity: 957.967175
2025-09-14 19:42:02,616 Stage: Train 0.5 | Epoch: 87 | Iter: 266200 | Total Loss: 0.002671 | Recon Loss: 0.002229 | Commit Loss: 0.000884 | Perplexity: 958.822869
2025-09-14 19:42:11,348 Stage: Train 0.5 | Epoch: 87 | Iter: 266400 | Total Loss: 0.002681 | Recon Loss: 0.002240 | Commit Loss: 0.000882 | Perplexity: 961.268508
2025-09-14 19:42:20,094 Stage: Train 0.5 | Epoch: 87 | Iter: 266600 | Total Loss: 0.002659 | Recon Loss: 0.002216 | Commit Loss: 0.000886 | Perplexity: 960.197182
2025-09-14 19:42:28,877 Stage: Train 0.5 | Epoch: 87 | Iter: 266800 | Total Loss: 0.002604 | Recon Loss: 0.002166 | Commit Loss: 0.000876 | Perplexity: 959.857831
2025-09-14 19:42:37,675 Stage: Train 0.5 | Epoch: 87 | Iter: 267000 | Total Loss: 0.002649 | Recon Loss: 0.002206 | Commit Loss: 0.000886 | Perplexity: 963.074909
2025-09-14 19:42:46,460 Stage: Train 0.5 | Epoch: 87 | Iter: 267200 | Total Loss: 0.002624 | Recon Loss: 0.002183 | Commit Loss: 0.000882 | Perplexity: 959.689517
Trainning Epoch:  53%|█████▎    | 88/165 [3:14:46<2:50:46, 133.07s/it]2025-09-14 19:42:55,225 Stage: Train 0.5 | Epoch: 88 | Iter: 267400 | Total Loss: 0.002634 | Recon Loss: 0.002197 | Commit Loss: 0.000873 | Perplexity: 955.903161
2025-09-14 19:43:03,951 Stage: Train 0.5 | Epoch: 88 | Iter: 267600 | Total Loss: 0.002645 | Recon Loss: 0.002202 | Commit Loss: 0.000886 | Perplexity: 962.450364
2025-09-14 19:43:12,710 Stage: Train 0.5 | Epoch: 88 | Iter: 267800 | Total Loss: 0.002639 | Recon Loss: 0.002201 | Commit Loss: 0.000877 | Perplexity: 954.143601
2025-09-14 19:43:21,481 Stage: Train 0.5 | Epoch: 88 | Iter: 268000 | Total Loss: 0.002684 | Recon Loss: 0.002248 | Commit Loss: 0.000873 | Perplexity: 960.181668
2025-09-14 19:43:30,265 Stage: Train 0.5 | Epoch: 88 | Iter: 268200 | Total Loss: 0.002606 | Recon Loss: 0.002166 | Commit Loss: 0.000880 | Perplexity: 965.626045
2025-09-14 19:43:39,040 Stage: Train 0.5 | Epoch: 88 | Iter: 268400 | Total Loss: 0.002594 | Recon Loss: 0.002152 | Commit Loss: 0.000883 | Perplexity: 958.735717
2025-09-14 19:43:47,805 Stage: Train 0.5 | Epoch: 88 | Iter: 268600 | Total Loss: 0.002643 | Recon Loss: 0.002200 | Commit Loss: 0.000887 | Perplexity: 963.482141
2025-09-14 19:43:56,566 Stage: Train 0.5 | Epoch: 88 | Iter: 268800 | Total Loss: 0.002673 | Recon Loss: 0.002228 | Commit Loss: 0.000891 | Perplexity: 963.893694
2025-09-14 19:44:05,416 Stage: Train 0.5 | Epoch: 88 | Iter: 269000 | Total Loss: 0.002643 | Recon Loss: 0.002206 | Commit Loss: 0.000875 | Perplexity: 955.126451
2025-09-14 19:44:14,243 Stage: Train 0.5 | Epoch: 88 | Iter: 269200 | Total Loss: 0.002647 | Recon Loss: 0.002209 | Commit Loss: 0.000877 | Perplexity: 957.216924
2025-09-14 19:44:22,952 Stage: Train 0.5 | Epoch: 88 | Iter: 269400 | Total Loss: 0.002644 | Recon Loss: 0.002204 | Commit Loss: 0.000879 | Perplexity: 957.731285
2025-09-14 19:44:31,648 Stage: Train 0.5 | Epoch: 88 | Iter: 269600 | Total Loss: 0.002623 | Recon Loss: 0.002182 | Commit Loss: 0.000881 | Perplexity: 958.545380
2025-09-14 19:44:40,308 Stage: Train 0.5 | Epoch: 88 | Iter: 269800 | Total Loss: 0.002629 | Recon Loss: 0.002180 | Commit Loss: 0.000898 | Perplexity: 961.650600
2025-09-14 19:44:48,986 Stage: Train 0.5 | Epoch: 88 | Iter: 270000 | Total Loss: 0.002651 | Recon Loss: 0.002211 | Commit Loss: 0.000880 | Perplexity: 962.887487
2025-09-14 19:44:57,695 Stage: Train 0.5 | Epoch: 88 | Iter: 270200 | Total Loss: 0.002622 | Recon Loss: 0.002179 | Commit Loss: 0.000886 | Perplexity: 962.957426
Trainning Epoch:  54%|█████▍    | 89/165 [3:16:59<2:48:28, 133.01s/it]2025-09-14 19:45:06,404 Stage: Train 0.5 | Epoch: 89 | Iter: 270400 | Total Loss: 0.002603 | Recon Loss: 0.002157 | Commit Loss: 0.000893 | Perplexity: 960.919371
2025-09-14 19:45:15,084 Stage: Train 0.5 | Epoch: 89 | Iter: 270600 | Total Loss: 0.002657 | Recon Loss: 0.002219 | Commit Loss: 0.000876 | Perplexity: 957.314915
2025-09-14 19:45:23,752 Stage: Train 0.5 | Epoch: 89 | Iter: 270800 | Total Loss: 0.002622 | Recon Loss: 0.002183 | Commit Loss: 0.000879 | Perplexity: 961.198334
2025-09-14 19:45:32,444 Stage: Train 0.5 | Epoch: 89 | Iter: 271000 | Total Loss: 0.002679 | Recon Loss: 0.002244 | Commit Loss: 0.000870 | Perplexity: 956.793439
2025-09-14 19:45:41,142 Stage: Train 0.5 | Epoch: 89 | Iter: 271200 | Total Loss: 0.002644 | Recon Loss: 0.002200 | Commit Loss: 0.000888 | Perplexity: 963.553085
2025-09-14 19:45:49,841 Stage: Train 0.5 | Epoch: 89 | Iter: 271400 | Total Loss: 0.002657 | Recon Loss: 0.002216 | Commit Loss: 0.000883 | Perplexity: 962.104855
2025-09-14 19:45:58,527 Stage: Train 0.5 | Epoch: 89 | Iter: 271600 | Total Loss: 0.002606 | Recon Loss: 0.002166 | Commit Loss: 0.000881 | Perplexity: 962.648762
2025-09-14 19:46:07,204 Stage: Train 0.5 | Epoch: 89 | Iter: 271800 | Total Loss: 0.002609 | Recon Loss: 0.002169 | Commit Loss: 0.000880 | Perplexity: 959.377422
2025-09-14 19:46:15,903 Stage: Train 0.5 | Epoch: 89 | Iter: 272000 | Total Loss: 0.002680 | Recon Loss: 0.002244 | Commit Loss: 0.000872 | Perplexity: 956.269359
2025-09-14 19:46:24,601 Stage: Train 0.5 | Epoch: 89 | Iter: 272200 | Total Loss: 0.002596 | Recon Loss: 0.002154 | Commit Loss: 0.000884 | Perplexity: 963.630299
2025-09-14 19:46:33,292 Stage: Train 0.5 | Epoch: 89 | Iter: 272400 | Total Loss: 0.002650 | Recon Loss: 0.002209 | Commit Loss: 0.000882 | Perplexity: 956.813509
2025-09-14 19:46:41,988 Stage: Train 0.5 | Epoch: 89 | Iter: 272600 | Total Loss: 0.002616 | Recon Loss: 0.002176 | Commit Loss: 0.000880 | Perplexity: 959.376714
2025-09-14 19:46:50,653 Stage: Train 0.5 | Epoch: 89 | Iter: 272800 | Total Loss: 0.002660 | Recon Loss: 0.002221 | Commit Loss: 0.000878 | Perplexity: 960.305988
2025-09-14 19:46:59,334 Stage: Train 0.5 | Epoch: 89 | Iter: 273000 | Total Loss: 0.002631 | Recon Loss: 0.002195 | Commit Loss: 0.000871 | Perplexity: 959.688456
2025-09-14 19:47:08,045 Stage: Train 0.5 | Epoch: 89 | Iter: 273200 | Total Loss: 0.002649 | Recon Loss: 0.002207 | Commit Loss: 0.000884 | Perplexity: 964.062607
2025-09-14 19:47:16,740 Stage: Train 0.5 | Epoch: 89 | Iter: 273400 | Total Loss: 0.002635 | Recon Loss: 0.002191 | Commit Loss: 0.000887 | Perplexity: 959.270287
Trainning Epoch:  55%|█████▍    | 90/165 [3:19:11<2:45:52, 132.70s/it]2025-09-14 19:47:25,435 Stage: Train 0.5 | Epoch: 90 | Iter: 273600 | Total Loss: 0.002675 | Recon Loss: 0.002235 | Commit Loss: 0.000879 | Perplexity: 960.524226
2025-09-14 19:47:34,134 Stage: Train 0.5 | Epoch: 90 | Iter: 273800 | Total Loss: 0.002620 | Recon Loss: 0.002176 | Commit Loss: 0.000888 | Perplexity: 960.197505
2025-09-14 19:47:42,836 Stage: Train 0.5 | Epoch: 90 | Iter: 274000 | Total Loss: 0.002597 | Recon Loss: 0.002160 | Commit Loss: 0.000874 | Perplexity: 961.748180
2025-09-14 19:47:51,502 Stage: Train 0.5 | Epoch: 90 | Iter: 274200 | Total Loss: 0.002670 | Recon Loss: 0.002231 | Commit Loss: 0.000877 | Perplexity: 959.480757
2025-09-14 19:48:00,193 Stage: Train 0.5 | Epoch: 90 | Iter: 274400 | Total Loss: 0.002640 | Recon Loss: 0.002205 | Commit Loss: 0.000870 | Perplexity: 959.150759
2025-09-14 19:48:08,902 Stage: Train 0.5 | Epoch: 90 | Iter: 274600 | Total Loss: 0.002613 | Recon Loss: 0.002172 | Commit Loss: 0.000882 | Perplexity: 964.700763
2025-09-14 19:48:17,595 Stage: Train 0.5 | Epoch: 90 | Iter: 274800 | Total Loss: 0.002618 | Recon Loss: 0.002177 | Commit Loss: 0.000882 | Perplexity: 960.872513
2025-09-14 19:48:26,305 Stage: Train 0.5 | Epoch: 90 | Iter: 275000 | Total Loss: 0.002593 | Recon Loss: 0.002153 | Commit Loss: 0.000882 | Perplexity: 961.486979
2025-09-14 19:48:35,018 Stage: Train 0.5 | Epoch: 90 | Iter: 275200 | Total Loss: 0.002655 | Recon Loss: 0.002218 | Commit Loss: 0.000875 | Perplexity: 958.551119
2025-09-14 19:48:43,727 Stage: Train 0.5 | Epoch: 90 | Iter: 275400 | Total Loss: 0.002620 | Recon Loss: 0.002185 | Commit Loss: 0.000870 | Perplexity: 961.187271
2025-09-14 19:48:52,437 Stage: Train 0.5 | Epoch: 90 | Iter: 275600 | Total Loss: 0.002579 | Recon Loss: 0.002140 | Commit Loss: 0.000878 | Perplexity: 961.761671
2025-09-14 19:49:01,148 Stage: Train 0.5 | Epoch: 90 | Iter: 275800 | Total Loss: 0.002635 | Recon Loss: 0.002194 | Commit Loss: 0.000882 | Perplexity: 964.147634
2025-09-14 19:49:09,835 Stage: Train 0.5 | Epoch: 90 | Iter: 276000 | Total Loss: 0.002611 | Recon Loss: 0.002178 | Commit Loss: 0.000867 | Perplexity: 957.163605
2025-09-14 19:49:18,542 Stage: Train 0.5 | Epoch: 90 | Iter: 276200 | Total Loss: 0.002599 | Recon Loss: 0.002161 | Commit Loss: 0.000877 | Perplexity: 958.429314
2025-09-14 19:49:27,240 Stage: Train 0.5 | Epoch: 90 | Iter: 276400 | Total Loss: 0.002588 | Recon Loss: 0.002150 | Commit Loss: 0.000876 | Perplexity: 959.778169
Trainning Epoch:  55%|█████▌    | 91/165 [3:21:23<2:43:27, 132.54s/it]2025-09-14 19:49:35,929 Stage: Train 0.5 | Epoch: 91 | Iter: 276600 | Total Loss: 0.002617 | Recon Loss: 0.002177 | Commit Loss: 0.000879 | Perplexity: 960.343084
2025-09-14 19:49:44,620 Stage: Train 0.5 | Epoch: 91 | Iter: 276800 | Total Loss: 0.002608 | Recon Loss: 0.002167 | Commit Loss: 0.000883 | Perplexity: 961.131687
2025-09-14 19:49:53,330 Stage: Train 0.5 | Epoch: 91 | Iter: 277000 | Total Loss: 0.002590 | Recon Loss: 0.002151 | Commit Loss: 0.000877 | Perplexity: 953.877731
2025-09-14 19:50:02,040 Stage: Train 0.5 | Epoch: 91 | Iter: 277200 | Total Loss: 0.002672 | Recon Loss: 0.002235 | Commit Loss: 0.000872 | Perplexity: 961.975518
2025-09-14 19:50:10,743 Stage: Train 0.5 | Epoch: 91 | Iter: 277400 | Total Loss: 0.002643 | Recon Loss: 0.002204 | Commit Loss: 0.000878 | Perplexity: 963.039784
2025-09-14 19:50:19,421 Stage: Train 0.5 | Epoch: 91 | Iter: 277600 | Total Loss: 0.002613 | Recon Loss: 0.002178 | Commit Loss: 0.000869 | Perplexity: 956.176254
2025-09-14 19:50:28,110 Stage: Train 0.5 | Epoch: 91 | Iter: 277800 | Total Loss: 0.002627 | Recon Loss: 0.002188 | Commit Loss: 0.000877 | Perplexity: 962.690581
2025-09-14 19:50:36,811 Stage: Train 0.5 | Epoch: 91 | Iter: 278000 | Total Loss: 0.002626 | Recon Loss: 0.002189 | Commit Loss: 0.000873 | Perplexity: 962.952729
2025-09-14 19:50:45,513 Stage: Train 0.5 | Epoch: 91 | Iter: 278200 | Total Loss: 0.002678 | Recon Loss: 0.002234 | Commit Loss: 0.000887 | Perplexity: 966.755193
2025-09-14 19:50:54,210 Stage: Train 0.5 | Epoch: 91 | Iter: 278400 | Total Loss: 0.002644 | Recon Loss: 0.002208 | Commit Loss: 0.000872 | Perplexity: 958.938928
2025-09-14 19:51:02,883 Stage: Train 0.5 | Epoch: 91 | Iter: 278600 | Total Loss: 0.002599 | Recon Loss: 0.002163 | Commit Loss: 0.000874 | Perplexity: 958.976043
2025-09-14 19:51:11,572 Stage: Train 0.5 | Epoch: 91 | Iter: 278800 | Total Loss: 0.002647 | Recon Loss: 0.002208 | Commit Loss: 0.000878 | Perplexity: 954.438914
2025-09-14 19:51:20,306 Stage: Train 0.5 | Epoch: 91 | Iter: 279000 | Total Loss: 0.002633 | Recon Loss: 0.002198 | Commit Loss: 0.000868 | Perplexity: 958.240282
2025-09-14 19:51:29,035 Stage: Train 0.5 | Epoch: 91 | Iter: 279200 | Total Loss: 0.002601 | Recon Loss: 0.002164 | Commit Loss: 0.000875 | Perplexity: 960.073815
2025-09-14 19:51:37,778 Stage: Train 0.5 | Epoch: 91 | Iter: 279400 | Total Loss: 0.002572 | Recon Loss: 0.002136 | Commit Loss: 0.000872 | Perplexity: 960.565865
Trainning Epoch:  56%|█████▌    | 92/165 [3:23:35<2:41:08, 132.44s/it]2025-09-14 19:51:46,541 Stage: Train 0.5 | Epoch: 92 | Iter: 279600 | Total Loss: 0.002582 | Recon Loss: 0.002142 | Commit Loss: 0.000880 | Perplexity: 959.169919
2025-09-14 19:51:55,305 Stage: Train 0.5 | Epoch: 92 | Iter: 279800 | Total Loss: 0.002586 | Recon Loss: 0.002150 | Commit Loss: 0.000872 | Perplexity: 958.212358
2025-09-14 19:52:04,047 Stage: Train 0.5 | Epoch: 92 | Iter: 280000 | Total Loss: 0.002594 | Recon Loss: 0.002158 | Commit Loss: 0.000873 | Perplexity: 966.017941
2025-09-14 19:52:04,047 Saving model at iteration 280000
2025-09-14 19:52:04,204 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_93_step_280000
2025-09-14 19:52:04,437 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_93_step_280000/pytorch_model.bin
2025-09-14 19:52:04,808 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_93_step_280000/optimizer.bin
2025-09-14 19:52:04,808 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_93_step_280000/scheduler.bin
2025-09-14 19:52:04,809 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_93_step_280000/random_states_0.pkl
2025-09-14 19:52:13,534 Stage: Train 0.5 | Epoch: 92 | Iter: 280200 | Total Loss: 0.002610 | Recon Loss: 0.002170 | Commit Loss: 0.000880 | Perplexity: 959.648902
2025-09-14 19:52:22,240 Stage: Train 0.5 | Epoch: 92 | Iter: 280400 | Total Loss: 0.002597 | Recon Loss: 0.002156 | Commit Loss: 0.000882 | Perplexity: 964.112419
2025-09-14 19:52:30,968 Stage: Train 0.5 | Epoch: 92 | Iter: 280600 | Total Loss: 0.002638 | Recon Loss: 0.002201 | Commit Loss: 0.000874 | Perplexity: 958.330483
2025-09-14 19:52:39,690 Stage: Train 0.5 | Epoch: 92 | Iter: 280800 | Total Loss: 0.002639 | Recon Loss: 0.002199 | Commit Loss: 0.000879 | Perplexity: 960.670581
2025-09-14 19:52:48,384 Stage: Train 0.5 | Epoch: 92 | Iter: 281000 | Total Loss: 0.002606 | Recon Loss: 0.002172 | Commit Loss: 0.000869 | Perplexity: 959.734820
2025-09-14 19:52:57,076 Stage: Train 0.5 | Epoch: 92 | Iter: 281200 | Total Loss: 0.002595 | Recon Loss: 0.002156 | Commit Loss: 0.000879 | Perplexity: 964.157420
2025-09-14 19:53:05,792 Stage: Train 0.5 | Epoch: 92 | Iter: 281400 | Total Loss: 0.002539 | Recon Loss: 0.002105 | Commit Loss: 0.000867 | Perplexity: 960.308896
2025-09-14 19:53:14,532 Stage: Train 0.5 | Epoch: 92 | Iter: 281600 | Total Loss: 0.002626 | Recon Loss: 0.002187 | Commit Loss: 0.000878 | Perplexity: 962.213018
2025-09-14 19:53:23,212 Stage: Train 0.5 | Epoch: 92 | Iter: 281800 | Total Loss: 0.002572 | Recon Loss: 0.002141 | Commit Loss: 0.000863 | Perplexity: 958.041384
2025-09-14 19:53:31,925 Stage: Train 0.5 | Epoch: 92 | Iter: 282000 | Total Loss: 0.002570 | Recon Loss: 0.002134 | Commit Loss: 0.000872 | Perplexity: 961.907718
2025-09-14 19:53:40,616 Stage: Train 0.5 | Epoch: 92 | Iter: 282200 | Total Loss: 0.002622 | Recon Loss: 0.002181 | Commit Loss: 0.000882 | Perplexity: 959.948781
2025-09-14 19:53:49,326 Stage: Train 0.5 | Epoch: 92 | Iter: 282400 | Total Loss: 0.002621 | Recon Loss: 0.002186 | Commit Loss: 0.000871 | Perplexity: 961.707439
Trainning Epoch:  56%|█████▋    | 93/165 [3:25:48<2:39:12, 132.67s/it]2025-09-14 19:53:58,070 Stage: Train 0.5 | Epoch: 93 | Iter: 282600 | Total Loss: 0.002654 | Recon Loss: 0.002219 | Commit Loss: 0.000869 | Perplexity: 955.455395
2025-09-14 19:54:06,808 Stage: Train 0.5 | Epoch: 93 | Iter: 282800 | Total Loss: 0.002592 | Recon Loss: 0.002159 | Commit Loss: 0.000866 | Perplexity: 963.232195
2025-09-14 19:54:15,474 Stage: Train 0.5 | Epoch: 93 | Iter: 283000 | Total Loss: 0.002589 | Recon Loss: 0.002150 | Commit Loss: 0.000877 | Perplexity: 965.562647
2025-09-14 19:54:24,176 Stage: Train 0.5 | Epoch: 93 | Iter: 283200 | Total Loss: 0.002573 | Recon Loss: 0.002134 | Commit Loss: 0.000877 | Perplexity: 962.400111
2025-09-14 19:54:32,888 Stage: Train 0.5 | Epoch: 93 | Iter: 283400 | Total Loss: 0.002563 | Recon Loss: 0.002124 | Commit Loss: 0.000878 | Perplexity: 965.609085
2025-09-14 19:54:41,565 Stage: Train 0.5 | Epoch: 93 | Iter: 283600 | Total Loss: 0.002602 | Recon Loss: 0.002167 | Commit Loss: 0.000870 | Perplexity: 960.438716
2025-09-14 19:54:50,258 Stage: Train 0.5 | Epoch: 93 | Iter: 283800 | Total Loss: 0.002668 | Recon Loss: 0.002233 | Commit Loss: 0.000870 | Perplexity: 962.118439
2025-09-14 19:54:58,967 Stage: Train 0.5 | Epoch: 93 | Iter: 284000 | Total Loss: 0.002616 | Recon Loss: 0.002183 | Commit Loss: 0.000868 | Perplexity: 957.805654
2025-09-14 19:55:07,662 Stage: Train 0.5 | Epoch: 93 | Iter: 284200 | Total Loss: 0.002612 | Recon Loss: 0.002172 | Commit Loss: 0.000880 | Perplexity: 963.492870
2025-09-14 19:55:16,350 Stage: Train 0.5 | Epoch: 93 | Iter: 284400 | Total Loss: 0.002605 | Recon Loss: 0.002165 | Commit Loss: 0.000879 | Perplexity: 960.752523
2025-09-14 19:55:25,046 Stage: Train 0.5 | Epoch: 93 | Iter: 284600 | Total Loss: 0.002544 | Recon Loss: 0.002108 | Commit Loss: 0.000874 | Perplexity: 962.542715
2025-09-14 19:55:33,734 Stage: Train 0.5 | Epoch: 93 | Iter: 284800 | Total Loss: 0.002574 | Recon Loss: 0.002139 | Commit Loss: 0.000870 | Perplexity: 960.313481
2025-09-14 19:55:42,412 Stage: Train 0.5 | Epoch: 93 | Iter: 285000 | Total Loss: 0.002619 | Recon Loss: 0.002186 | Commit Loss: 0.000868 | Perplexity: 955.236392
2025-09-14 19:55:51,129 Stage: Train 0.5 | Epoch: 93 | Iter: 285200 | Total Loss: 0.002599 | Recon Loss: 0.002164 | Commit Loss: 0.000870 | Perplexity: 957.345900
2025-09-14 19:55:59,854 Stage: Train 0.5 | Epoch: 93 | Iter: 285400 | Total Loss: 0.002602 | Recon Loss: 0.002164 | Commit Loss: 0.000877 | Perplexity: 962.309913
Trainning Epoch:  57%|█████▋    | 94/165 [3:28:00<2:36:47, 132.51s/it]2025-09-14 19:56:08,521 Stage: Train 0.5 | Epoch: 94 | Iter: 285600 | Total Loss: 0.002665 | Recon Loss: 0.002224 | Commit Loss: 0.000883 | Perplexity: 962.850425
2025-09-14 19:56:17,268 Stage: Train 0.5 | Epoch: 94 | Iter: 285800 | Total Loss: 0.002595 | Recon Loss: 0.002165 | Commit Loss: 0.000860 | Perplexity: 961.633050
2025-09-14 19:56:26,005 Stage: Train 0.5 | Epoch: 94 | Iter: 286000 | Total Loss: 0.002573 | Recon Loss: 0.002141 | Commit Loss: 0.000863 | Perplexity: 958.144993
2025-09-14 19:56:34,709 Stage: Train 0.5 | Epoch: 94 | Iter: 286200 | Total Loss: 0.002611 | Recon Loss: 0.002180 | Commit Loss: 0.000863 | Perplexity: 960.965227
2025-09-14 19:56:43,439 Stage: Train 0.5 | Epoch: 94 | Iter: 286400 | Total Loss: 0.002645 | Recon Loss: 0.002212 | Commit Loss: 0.000867 | Perplexity: 964.791993
2025-09-14 19:56:52,151 Stage: Train 0.5 | Epoch: 94 | Iter: 286600 | Total Loss: 0.002597 | Recon Loss: 0.002164 | Commit Loss: 0.000865 | Perplexity: 962.038591
2025-09-14 19:57:00,879 Stage: Train 0.5 | Epoch: 94 | Iter: 286800 | Total Loss: 0.002640 | Recon Loss: 0.002203 | Commit Loss: 0.000872 | Perplexity: 966.566522
2025-09-14 19:57:09,624 Stage: Train 0.5 | Epoch: 94 | Iter: 287000 | Total Loss: 0.002570 | Recon Loss: 0.002137 | Commit Loss: 0.000864 | Perplexity: 961.556108
2025-09-14 19:57:18,344 Stage: Train 0.5 | Epoch: 94 | Iter: 287200 | Total Loss: 0.002545 | Recon Loss: 0.002110 | Commit Loss: 0.000872 | Perplexity: 963.832956
2025-09-14 19:57:27,080 Stage: Train 0.5 | Epoch: 94 | Iter: 287400 | Total Loss: 0.002619 | Recon Loss: 0.002183 | Commit Loss: 0.000871 | Perplexity: 959.470385
2025-09-14 19:57:35,772 Stage: Train 0.5 | Epoch: 94 | Iter: 287600 | Total Loss: 0.002586 | Recon Loss: 0.002146 | Commit Loss: 0.000880 | Perplexity: 963.167010
2025-09-14 19:57:44,525 Stage: Train 0.5 | Epoch: 94 | Iter: 287800 | Total Loss: 0.002599 | Recon Loss: 0.002165 | Commit Loss: 0.000868 | Perplexity: 960.213844
2025-09-14 19:57:53,232 Stage: Train 0.5 | Epoch: 94 | Iter: 288000 | Total Loss: 0.002651 | Recon Loss: 0.002216 | Commit Loss: 0.000870 | Perplexity: 961.159239
2025-09-14 19:58:01,952 Stage: Train 0.5 | Epoch: 94 | Iter: 288200 | Total Loss: 0.002588 | Recon Loss: 0.002152 | Commit Loss: 0.000873 | Perplexity: 962.700970
2025-09-14 19:58:10,684 Stage: Train 0.5 | Epoch: 94 | Iter: 288400 | Total Loss: 0.002613 | Recon Loss: 0.002176 | Commit Loss: 0.000873 | Perplexity: 961.896766
2025-09-14 19:58:19,394 Stage: Train 0.5 | Epoch: 94 | Iter: 288600 | Total Loss: 0.002606 | Recon Loss: 0.002172 | Commit Loss: 0.000867 | Perplexity: 959.413665
Trainning Epoch:  58%|█████▊    | 95/165 [3:30:13<2:34:35, 132.51s/it]2025-09-14 19:58:28,197 Stage: Train 0.5 | Epoch: 95 | Iter: 288800 | Total Loss: 0.002532 | Recon Loss: 0.002098 | Commit Loss: 0.000868 | Perplexity: 956.695054
2025-09-14 19:58:36,967 Stage: Train 0.5 | Epoch: 95 | Iter: 289000 | Total Loss: 0.002607 | Recon Loss: 0.002174 | Commit Loss: 0.000867 | Perplexity: 963.895412
2025-09-14 19:58:45,691 Stage: Train 0.5 | Epoch: 95 | Iter: 289200 | Total Loss: 0.002580 | Recon Loss: 0.002144 | Commit Loss: 0.000873 | Perplexity: 961.361625
2025-09-14 19:58:54,429 Stage: Train 0.5 | Epoch: 95 | Iter: 289400 | Total Loss: 0.002574 | Recon Loss: 0.002142 | Commit Loss: 0.000864 | Perplexity: 962.813524
2025-09-14 19:59:03,153 Stage: Train 0.5 | Epoch: 95 | Iter: 289600 | Total Loss: 0.002613 | Recon Loss: 0.002177 | Commit Loss: 0.000871 | Perplexity: 963.291287
2025-09-14 19:59:11,902 Stage: Train 0.5 | Epoch: 95 | Iter: 289800 | Total Loss: 0.002540 | Recon Loss: 0.002106 | Commit Loss: 0.000868 | Perplexity: 956.070245
2025-09-14 19:59:20,608 Stage: Train 0.5 | Epoch: 95 | Iter: 290000 | Total Loss: 0.002592 | Recon Loss: 0.002158 | Commit Loss: 0.000868 | Perplexity: 957.650523
2025-09-14 19:59:29,308 Stage: Train 0.5 | Epoch: 95 | Iter: 290200 | Total Loss: 0.002638 | Recon Loss: 0.002201 | Commit Loss: 0.000873 | Perplexity: 967.002134
2025-09-14 19:59:38,048 Stage: Train 0.5 | Epoch: 95 | Iter: 290400 | Total Loss: 0.002562 | Recon Loss: 0.002128 | Commit Loss: 0.000869 | Perplexity: 965.714415
2025-09-14 19:59:46,783 Stage: Train 0.5 | Epoch: 95 | Iter: 290600 | Total Loss: 0.002594 | Recon Loss: 0.002156 | Commit Loss: 0.000876 | Perplexity: 961.723789
2025-09-14 19:59:55,516 Stage: Train 0.5 | Epoch: 95 | Iter: 290800 | Total Loss: 0.002626 | Recon Loss: 0.002191 | Commit Loss: 0.000870 | Perplexity: 960.078721
2025-09-14 20:00:04,230 Stage: Train 0.5 | Epoch: 95 | Iter: 291000 | Total Loss: 0.002593 | Recon Loss: 0.002158 | Commit Loss: 0.000870 | Perplexity: 961.836373
2025-09-14 20:00:12,954 Stage: Train 0.5 | Epoch: 95 | Iter: 291200 | Total Loss: 0.002566 | Recon Loss: 0.002132 | Commit Loss: 0.000868 | Perplexity: 963.418017
2025-09-14 20:00:21,684 Stage: Train 0.5 | Epoch: 95 | Iter: 291400 | Total Loss: 0.002616 | Recon Loss: 0.002179 | Commit Loss: 0.000872 | Perplexity: 963.499647
2025-09-14 20:00:30,420 Stage: Train 0.5 | Epoch: 95 | Iter: 291600 | Total Loss: 0.002570 | Recon Loss: 0.002140 | Commit Loss: 0.000860 | Perplexity: 962.836428
Trainning Epoch:  58%|█████▊    | 96/165 [3:32:26<2:32:27, 132.57s/it]2025-09-14 20:00:39,153 Stage: Train 0.5 | Epoch: 96 | Iter: 291800 | Total Loss: 0.002544 | Recon Loss: 0.002114 | Commit Loss: 0.000859 | Perplexity: 955.318552
2025-09-14 20:00:47,867 Stage: Train 0.5 | Epoch: 96 | Iter: 292000 | Total Loss: 0.002584 | Recon Loss: 0.002151 | Commit Loss: 0.000866 | Perplexity: 960.345524
2025-09-14 20:00:56,604 Stage: Train 0.5 | Epoch: 96 | Iter: 292200 | Total Loss: 0.002588 | Recon Loss: 0.002150 | Commit Loss: 0.000877 | Perplexity: 964.719011
2025-09-14 20:01:05,317 Stage: Train 0.5 | Epoch: 96 | Iter: 292400 | Total Loss: 0.002602 | Recon Loss: 0.002166 | Commit Loss: 0.000873 | Perplexity: 964.985918
2025-09-14 20:01:14,048 Stage: Train 0.5 | Epoch: 96 | Iter: 292600 | Total Loss: 0.002552 | Recon Loss: 0.002118 | Commit Loss: 0.000868 | Perplexity: 961.850664
2025-09-14 20:01:22,798 Stage: Train 0.5 | Epoch: 96 | Iter: 292800 | Total Loss: 0.002614 | Recon Loss: 0.002181 | Commit Loss: 0.000865 | Perplexity: 958.003167
2025-09-14 20:01:31,524 Stage: Train 0.5 | Epoch: 96 | Iter: 293000 | Total Loss: 0.002542 | Recon Loss: 0.002107 | Commit Loss: 0.000870 | Perplexity: 958.781153
2025-09-14 20:01:40,262 Stage: Train 0.5 | Epoch: 96 | Iter: 293200 | Total Loss: 0.002616 | Recon Loss: 0.002184 | Commit Loss: 0.000864 | Perplexity: 956.751735
2025-09-14 20:01:48,968 Stage: Train 0.5 | Epoch: 96 | Iter: 293400 | Total Loss: 0.002524 | Recon Loss: 0.002094 | Commit Loss: 0.000860 | Perplexity: 957.858747
2025-09-14 20:01:57,712 Stage: Train 0.5 | Epoch: 96 | Iter: 293600 | Total Loss: 0.002583 | Recon Loss: 0.002149 | Commit Loss: 0.000869 | Perplexity: 966.984426
2025-09-14 20:02:06,440 Stage: Train 0.5 | Epoch: 96 | Iter: 293800 | Total Loss: 0.002578 | Recon Loss: 0.002147 | Commit Loss: 0.000862 | Perplexity: 959.196206
2025-09-14 20:02:15,164 Stage: Train 0.5 | Epoch: 96 | Iter: 294000 | Total Loss: 0.002574 | Recon Loss: 0.002140 | Commit Loss: 0.000868 | Perplexity: 963.176388
2025-09-14 20:02:23,897 Stage: Train 0.5 | Epoch: 96 | Iter: 294200 | Total Loss: 0.002608 | Recon Loss: 0.002173 | Commit Loss: 0.000869 | Perplexity: 959.738252
2025-09-14 20:02:32,610 Stage: Train 0.5 | Epoch: 96 | Iter: 294400 | Total Loss: 0.002543 | Recon Loss: 0.002112 | Commit Loss: 0.000863 | Perplexity: 962.988356
2025-09-14 20:02:41,328 Stage: Train 0.5 | Epoch: 96 | Iter: 294600 | Total Loss: 0.002575 | Recon Loss: 0.002140 | Commit Loss: 0.000870 | Perplexity: 965.274876
Trainning Epoch:  59%|█████▉    | 97/165 [3:34:38<2:30:14, 132.56s/it]2025-09-14 20:02:50,089 Stage: Train 0.5 | Epoch: 97 | Iter: 294800 | Total Loss: 0.002584 | Recon Loss: 0.002148 | Commit Loss: 0.000871 | Perplexity: 961.519126
2025-09-14 20:02:58,834 Stage: Train 0.5 | Epoch: 97 | Iter: 295000 | Total Loss: 0.002575 | Recon Loss: 0.002142 | Commit Loss: 0.000865 | Perplexity: 958.381078
2025-09-14 20:03:07,570 Stage: Train 0.5 | Epoch: 97 | Iter: 295200 | Total Loss: 0.002606 | Recon Loss: 0.002174 | Commit Loss: 0.000865 | Perplexity: 962.609272
2025-09-14 20:03:16,349 Stage: Train 0.5 | Epoch: 97 | Iter: 295400 | Total Loss: 0.002555 | Recon Loss: 0.002121 | Commit Loss: 0.000869 | Perplexity: 963.514695
2025-09-14 20:03:25,118 Stage: Train 0.5 | Epoch: 97 | Iter: 295600 | Total Loss: 0.002553 | Recon Loss: 0.002123 | Commit Loss: 0.000861 | Perplexity: 960.281357
2025-09-14 20:03:33,864 Stage: Train 0.5 | Epoch: 97 | Iter: 295800 | Total Loss: 0.002610 | Recon Loss: 0.002170 | Commit Loss: 0.000880 | Perplexity: 965.752284
2025-09-14 20:03:42,627 Stage: Train 0.5 | Epoch: 97 | Iter: 296000 | Total Loss: 0.002585 | Recon Loss: 0.002151 | Commit Loss: 0.000868 | Perplexity: 969.696836
2025-09-14 20:03:51,383 Stage: Train 0.5 | Epoch: 97 | Iter: 296200 | Total Loss: 0.002572 | Recon Loss: 0.002136 | Commit Loss: 0.000871 | Perplexity: 959.383738
2025-09-14 20:04:00,138 Stage: Train 0.5 | Epoch: 97 | Iter: 296400 | Total Loss: 0.002592 | Recon Loss: 0.002160 | Commit Loss: 0.000865 | Perplexity: 966.834287
2025-09-14 20:04:08,886 Stage: Train 0.5 | Epoch: 97 | Iter: 296600 | Total Loss: 0.002556 | Recon Loss: 0.002122 | Commit Loss: 0.000868 | Perplexity: 965.535068
2025-09-14 20:04:17,617 Stage: Train 0.5 | Epoch: 97 | Iter: 296800 | Total Loss: 0.002534 | Recon Loss: 0.002101 | Commit Loss: 0.000867 | Perplexity: 963.863690
2025-09-14 20:04:26,348 Stage: Train 0.5 | Epoch: 97 | Iter: 297000 | Total Loss: 0.002525 | Recon Loss: 0.002090 | Commit Loss: 0.000869 | Perplexity: 962.395058
2025-09-14 20:04:35,080 Stage: Train 0.5 | Epoch: 97 | Iter: 297200 | Total Loss: 0.002601 | Recon Loss: 0.002164 | Commit Loss: 0.000873 | Perplexity: 963.979969
2025-09-14 20:04:43,837 Stage: Train 0.5 | Epoch: 97 | Iter: 297400 | Total Loss: 0.002502 | Recon Loss: 0.002068 | Commit Loss: 0.000870 | Perplexity: 965.351362
2025-09-14 20:04:52,579 Stage: Train 0.5 | Epoch: 97 | Iter: 297600 | Total Loss: 0.002539 | Recon Loss: 0.002106 | Commit Loss: 0.000866 | Perplexity: 964.254172
Trainning Epoch:  59%|█████▉    | 98/165 [3:36:51<2:28:08, 132.67s/it]2025-09-14 20:05:01,324 Stage: Train 0.5 | Epoch: 98 | Iter: 297800 | Total Loss: 0.002592 | Recon Loss: 0.002159 | Commit Loss: 0.000865 | Perplexity: 957.605528
2025-09-14 20:05:10,063 Stage: Train 0.5 | Epoch: 98 | Iter: 298000 | Total Loss: 0.002548 | Recon Loss: 0.002114 | Commit Loss: 0.000868 | Perplexity: 965.101293
2025-09-14 20:05:18,759 Stage: Train 0.5 | Epoch: 98 | Iter: 298200 | Total Loss: 0.002552 | Recon Loss: 0.002118 | Commit Loss: 0.000868 | Perplexity: 962.213083
2025-09-14 20:05:27,483 Stage: Train 0.5 | Epoch: 98 | Iter: 298400 | Total Loss: 0.002582 | Recon Loss: 0.002144 | Commit Loss: 0.000876 | Perplexity: 968.587050
2025-09-14 20:05:36,195 Stage: Train 0.5 | Epoch: 98 | Iter: 298600 | Total Loss: 0.002569 | Recon Loss: 0.002138 | Commit Loss: 0.000861 | Perplexity: 961.686497
2025-09-14 20:05:44,935 Stage: Train 0.5 | Epoch: 98 | Iter: 298800 | Total Loss: 0.002630 | Recon Loss: 0.002203 | Commit Loss: 0.000853 | Perplexity: 960.330581
2025-09-14 20:05:53,668 Stage: Train 0.5 | Epoch: 98 | Iter: 299000 | Total Loss: 0.002558 | Recon Loss: 0.002125 | Commit Loss: 0.000866 | Perplexity: 962.244550
2025-09-14 20:06:02,405 Stage: Train 0.5 | Epoch: 98 | Iter: 299200 | Total Loss: 0.002556 | Recon Loss: 0.002124 | Commit Loss: 0.000864 | Perplexity: 966.453087
2025-09-14 20:06:11,146 Stage: Train 0.5 | Epoch: 98 | Iter: 299400 | Total Loss: 0.002543 | Recon Loss: 0.002112 | Commit Loss: 0.000863 | Perplexity: 966.642878
2025-09-14 20:06:19,913 Stage: Train 0.5 | Epoch: 98 | Iter: 299600 | Total Loss: 0.002624 | Recon Loss: 0.002194 | Commit Loss: 0.000860 | Perplexity: 958.110322
2025-09-14 20:06:28,664 Stage: Train 0.5 | Epoch: 98 | Iter: 299800 | Total Loss: 0.002565 | Recon Loss: 0.002129 | Commit Loss: 0.000872 | Perplexity: 966.407527
2025-09-14 20:06:37,447 Stage: Train 0.5 | Epoch: 98 | Iter: 300000 | Total Loss: 0.002588 | Recon Loss: 0.002156 | Commit Loss: 0.000864 | Perplexity: 963.917477
2025-09-14 20:06:37,448 Saving model at iteration 300000
2025-09-14 20:06:37,794 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_99_step_300000
2025-09-14 20:06:38,026 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_99_step_300000/pytorch_model.bin
2025-09-14 20:06:38,388 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_99_step_300000/optimizer.bin
2025-09-14 20:06:38,388 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_99_step_300000/scheduler.bin
2025-09-14 20:06:38,389 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_99_step_300000/random_states_0.pkl
2025-09-14 20:06:47,569 Stage: Train 0.5 | Epoch: 98 | Iter: 300200 | Total Loss: 0.002594 | Recon Loss: 0.002166 | Commit Loss: 0.000857 | Perplexity: 960.083414
2025-09-14 20:06:56,340 Stage: Train 0.5 | Epoch: 98 | Iter: 300400 | Total Loss: 0.002569 | Recon Loss: 0.002136 | Commit Loss: 0.000866 | Perplexity: 963.939904
2025-09-14 20:07:05,092 Stage: Train 0.5 | Epoch: 98 | Iter: 300600 | Total Loss: 0.002535 | Recon Loss: 0.002103 | Commit Loss: 0.000863 | Perplexity: 961.427826
Trainning Epoch:  60%|██████    | 99/165 [3:39:05<2:26:28, 133.16s/it]2025-09-14 20:07:13,986 Stage: Train 0.5 | Epoch: 99 | Iter: 300800 | Total Loss: 0.002535 | Recon Loss: 0.002101 | Commit Loss: 0.000868 | Perplexity: 963.318637
2025-09-14 20:07:22,776 Stage: Train 0.5 | Epoch: 99 | Iter: 301000 | Total Loss: 0.002605 | Recon Loss: 0.002172 | Commit Loss: 0.000866 | Perplexity: 963.789539
2025-09-14 20:07:31,540 Stage: Train 0.5 | Epoch: 99 | Iter: 301200 | Total Loss: 0.002563 | Recon Loss: 0.002137 | Commit Loss: 0.000853 | Perplexity: 963.212209
2025-09-14 20:07:40,323 Stage: Train 0.5 | Epoch: 99 | Iter: 301400 | Total Loss: 0.002537 | Recon Loss: 0.002107 | Commit Loss: 0.000860 | Perplexity: 962.337319
2025-09-14 20:07:49,095 Stage: Train 0.5 | Epoch: 99 | Iter: 301600 | Total Loss: 0.002563 | Recon Loss: 0.002130 | Commit Loss: 0.000866 | Perplexity: 964.221978
2025-09-14 20:07:57,907 Stage: Train 0.5 | Epoch: 99 | Iter: 301800 | Total Loss: 0.002561 | Recon Loss: 0.002130 | Commit Loss: 0.000862 | Perplexity: 969.004125
2025-09-14 20:08:06,681 Stage: Train 0.5 | Epoch: 99 | Iter: 302000 | Total Loss: 0.002554 | Recon Loss: 0.002124 | Commit Loss: 0.000859 | Perplexity: 961.998191
2025-09-14 20:08:15,428 Stage: Train 0.5 | Epoch: 99 | Iter: 302200 | Total Loss: 0.002561 | Recon Loss: 0.002125 | Commit Loss: 0.000871 | Perplexity: 967.358124
2025-09-14 20:08:24,201 Stage: Train 0.5 | Epoch: 99 | Iter: 302400 | Total Loss: 0.002582 | Recon Loss: 0.002147 | Commit Loss: 0.000870 | Perplexity: 965.446011
2025-09-14 20:08:32,932 Stage: Train 0.5 | Epoch: 99 | Iter: 302600 | Total Loss: 0.002600 | Recon Loss: 0.002169 | Commit Loss: 0.000862 | Perplexity: 960.068652
2025-09-14 20:08:41,711 Stage: Train 0.5 | Epoch: 99 | Iter: 302800 | Total Loss: 0.002573 | Recon Loss: 0.002144 | Commit Loss: 0.000859 | Perplexity: 956.231299
2025-09-14 20:08:50,502 Stage: Train 0.5 | Epoch: 99 | Iter: 303000 | Total Loss: 0.002588 | Recon Loss: 0.002155 | Commit Loss: 0.000866 | Perplexity: 961.540388
2025-09-14 20:08:59,287 Stage: Train 0.5 | Epoch: 99 | Iter: 303200 | Total Loss: 0.002551 | Recon Loss: 0.002117 | Commit Loss: 0.000868 | Perplexity: 962.783141
2025-09-14 20:09:08,068 Stage: Train 0.5 | Epoch: 99 | Iter: 303400 | Total Loss: 0.002561 | Recon Loss: 0.002131 | Commit Loss: 0.000860 | Perplexity: 957.520305
2025-09-14 20:09:16,860 Stage: Train 0.5 | Epoch: 99 | Iter: 303600 | Total Loss: 0.002557 | Recon Loss: 0.002126 | Commit Loss: 0.000862 | Perplexity: 963.959600
2025-09-14 20:09:25,628 Stage: Train 0.5 | Epoch: 99 | Iter: 303800 | Total Loss: 0.002537 | Recon Loss: 0.002106 | Commit Loss: 0.000860 | Perplexity: 960.132938
Trainning Epoch:  61%|██████    | 100/165 [3:41:19<2:24:18, 133.21s/it]2025-09-14 20:09:34,436 Stage: Train 0.5 | Epoch: 100 | Iter: 304000 | Total Loss: 0.002536 | Recon Loss: 0.002105 | Commit Loss: 0.000864 | Perplexity: 967.080932
2025-09-14 20:09:43,181 Stage: Train 0.5 | Epoch: 100 | Iter: 304200 | Total Loss: 0.002618 | Recon Loss: 0.002184 | Commit Loss: 0.000868 | Perplexity: 960.370843
2025-09-14 20:09:51,967 Stage: Train 0.5 | Epoch: 100 | Iter: 304400 | Total Loss: 0.002537 | Recon Loss: 0.002108 | Commit Loss: 0.000859 | Perplexity: 957.928802
2025-09-14 20:10:00,744 Stage: Train 0.5 | Epoch: 100 | Iter: 304600 | Total Loss: 0.002541 | Recon Loss: 0.002117 | Commit Loss: 0.000849 | Perplexity: 961.463526
2025-09-14 20:10:09,552 Stage: Train 0.5 | Epoch: 100 | Iter: 304800 | Total Loss: 0.002571 | Recon Loss: 0.002144 | Commit Loss: 0.000855 | Perplexity: 965.471975
2025-09-14 20:10:18,389 Stage: Train 0.5 | Epoch: 100 | Iter: 305000 | Total Loss: 0.002535 | Recon Loss: 0.002107 | Commit Loss: 0.000855 | Perplexity: 962.475260
2025-09-14 20:10:27,201 Stage: Train 0.5 | Epoch: 100 | Iter: 305200 | Total Loss: 0.002532 | Recon Loss: 0.002107 | Commit Loss: 0.000850 | Perplexity: 958.560614
2025-09-14 20:10:35,976 Stage: Train 0.5 | Epoch: 100 | Iter: 305400 | Total Loss: 0.002522 | Recon Loss: 0.002092 | Commit Loss: 0.000859 | Perplexity: 961.617398
2025-09-14 20:10:44,753 Stage: Train 0.5 | Epoch: 100 | Iter: 305600 | Total Loss: 0.002565 | Recon Loss: 0.002133 | Commit Loss: 0.000865 | Perplexity: 965.826946
2025-09-14 20:10:53,524 Stage: Train 0.5 | Epoch: 100 | Iter: 305800 | Total Loss: 0.002563 | Recon Loss: 0.002130 | Commit Loss: 0.000867 | Perplexity: 966.482921
2025-09-14 20:11:02,297 Stage: Train 0.5 | Epoch: 100 | Iter: 306000 | Total Loss: 0.002531 | Recon Loss: 0.002101 | Commit Loss: 0.000858 | Perplexity: 964.145099
2025-09-14 20:11:11,096 Stage: Train 0.5 | Epoch: 100 | Iter: 306200 | Total Loss: 0.002525 | Recon Loss: 0.002090 | Commit Loss: 0.000869 | Perplexity: 968.104056
2025-09-14 20:11:19,868 Stage: Train 0.5 | Epoch: 100 | Iter: 306400 | Total Loss: 0.002563 | Recon Loss: 0.002133 | Commit Loss: 0.000861 | Perplexity: 961.634774
2025-09-14 20:11:28,664 Stage: Train 0.5 | Epoch: 100 | Iter: 306600 | Total Loss: 0.002558 | Recon Loss: 0.002126 | Commit Loss: 0.000864 | Perplexity: 960.026045
2025-09-14 20:11:37,452 Stage: Train 0.5 | Epoch: 100 | Iter: 306800 | Total Loss: 0.002549 | Recon Loss: 0.002118 | Commit Loss: 0.000862 | Perplexity: 965.164536
Trainning Epoch:  61%|██████    | 101/165 [3:43:32<2:22:10, 133.29s/it]2025-09-14 20:11:46,293 Stage: Train 0.5 | Epoch: 101 | Iter: 307000 | Total Loss: 0.002531 | Recon Loss: 0.002104 | Commit Loss: 0.000855 | Perplexity: 962.309095
2025-09-14 20:11:55,228 Stage: Train 0.5 | Epoch: 101 | Iter: 307200 | Total Loss: 0.002553 | Recon Loss: 0.002123 | Commit Loss: 0.000861 | Perplexity: 964.163361
2025-09-14 20:12:04,055 Stage: Train 0.5 | Epoch: 101 | Iter: 307400 | Total Loss: 0.002575 | Recon Loss: 0.002145 | Commit Loss: 0.000860 | Perplexity: 963.640634
2025-09-14 20:12:12,901 Stage: Train 0.5 | Epoch: 101 | Iter: 307600 | Total Loss: 0.002635 | Recon Loss: 0.002208 | Commit Loss: 0.000854 | Perplexity: 958.982286
2025-09-14 20:12:21,714 Stage: Train 0.5 | Epoch: 101 | Iter: 307800 | Total Loss: 0.002581 | Recon Loss: 0.002148 | Commit Loss: 0.000865 | Perplexity: 962.392030
2025-09-14 20:12:30,478 Stage: Train 0.5 | Epoch: 101 | Iter: 308000 | Total Loss: 0.002563 | Recon Loss: 0.002137 | Commit Loss: 0.000851 | Perplexity: 961.263661
2025-09-14 20:12:39,211 Stage: Train 0.5 | Epoch: 101 | Iter: 308200 | Total Loss: 0.002537 | Recon Loss: 0.002107 | Commit Loss: 0.000861 | Perplexity: 967.700272
2025-09-14 20:12:47,975 Stage: Train 0.5 | Epoch: 101 | Iter: 308400 | Total Loss: 0.002547 | Recon Loss: 0.002118 | Commit Loss: 0.000858 | Perplexity: 962.313764
2025-09-14 20:12:56,757 Stage: Train 0.5 | Epoch: 101 | Iter: 308600 | Total Loss: 0.002518 | Recon Loss: 0.002086 | Commit Loss: 0.000863 | Perplexity: 966.094264
2025-09-14 20:13:05,524 Stage: Train 0.5 | Epoch: 101 | Iter: 308800 | Total Loss: 0.002533 | Recon Loss: 0.002103 | Commit Loss: 0.000861 | Perplexity: 962.760702
2025-09-14 20:13:14,299 Stage: Train 0.5 | Epoch: 101 | Iter: 309000 | Total Loss: 0.002543 | Recon Loss: 0.002115 | Commit Loss: 0.000855 | Perplexity: 965.757623
2025-09-14 20:13:23,086 Stage: Train 0.5 | Epoch: 101 | Iter: 309200 | Total Loss: 0.002514 | Recon Loss: 0.002087 | Commit Loss: 0.000854 | Perplexity: 960.188780
2025-09-14 20:13:31,881 Stage: Train 0.5 | Epoch: 101 | Iter: 309400 | Total Loss: 0.002572 | Recon Loss: 0.002139 | Commit Loss: 0.000866 | Perplexity: 966.442751
2025-09-14 20:13:40,646 Stage: Train 0.5 | Epoch: 101 | Iter: 309600 | Total Loss: 0.002608 | Recon Loss: 0.002177 | Commit Loss: 0.000863 | Perplexity: 959.078856
2025-09-14 20:13:49,401 Stage: Train 0.5 | Epoch: 101 | Iter: 309800 | Total Loss: 0.002497 | Recon Loss: 0.002071 | Commit Loss: 0.000851 | Perplexity: 959.585090
Trainning Epoch:  62%|██████▏   | 102/165 [3:45:46<2:20:03, 133.39s/it]2025-09-14 20:13:58,188 Stage: Train 0.5 | Epoch: 102 | Iter: 310000 | Total Loss: 0.002529 | Recon Loss: 0.002100 | Commit Loss: 0.000857 | Perplexity: 959.299068
2025-09-14 20:14:06,964 Stage: Train 0.5 | Epoch: 102 | Iter: 310200 | Total Loss: 0.002553 | Recon Loss: 0.002125 | Commit Loss: 0.000856 | Perplexity: 959.000459
2025-09-14 20:14:15,738 Stage: Train 0.5 | Epoch: 102 | Iter: 310400 | Total Loss: 0.002515 | Recon Loss: 0.002090 | Commit Loss: 0.000850 | Perplexity: 961.039651
2025-09-14 20:14:24,469 Stage: Train 0.5 | Epoch: 102 | Iter: 310600 | Total Loss: 0.002504 | Recon Loss: 0.002074 | Commit Loss: 0.000860 | Perplexity: 964.122163
2025-09-14 20:14:33,240 Stage: Train 0.5 | Epoch: 102 | Iter: 310800 | Total Loss: 0.002564 | Recon Loss: 0.002134 | Commit Loss: 0.000861 | Perplexity: 961.174994
2025-09-14 20:14:42,006 Stage: Train 0.5 | Epoch: 102 | Iter: 311000 | Total Loss: 0.002577 | Recon Loss: 0.002142 | Commit Loss: 0.000869 | Perplexity: 968.535191
2025-09-14 20:14:50,758 Stage: Train 0.5 | Epoch: 102 | Iter: 311200 | Total Loss: 0.002550 | Recon Loss: 0.002123 | Commit Loss: 0.000854 | Perplexity: 964.443550
2025-09-14 20:14:59,537 Stage: Train 0.5 | Epoch: 102 | Iter: 311400 | Total Loss: 0.002544 | Recon Loss: 0.002115 | Commit Loss: 0.000858 | Perplexity: 964.186092
2025-09-14 20:15:08,316 Stage: Train 0.5 | Epoch: 102 | Iter: 311600 | Total Loss: 0.002506 | Recon Loss: 0.002074 | Commit Loss: 0.000864 | Perplexity: 962.425843
2025-09-14 20:15:17,108 Stage: Train 0.5 | Epoch: 102 | Iter: 311800 | Total Loss: 0.002541 | Recon Loss: 0.002112 | Commit Loss: 0.000859 | Perplexity: 964.687280
2025-09-14 20:15:25,845 Stage: Train 0.5 | Epoch: 102 | Iter: 312000 | Total Loss: 0.002583 | Recon Loss: 0.002153 | Commit Loss: 0.000859 | Perplexity: 959.526488
2025-09-14 20:15:34,691 Stage: Train 0.5 | Epoch: 102 | Iter: 312200 | Total Loss: 0.002590 | Recon Loss: 0.002162 | Commit Loss: 0.000856 | Perplexity: 961.892195
2025-09-14 20:15:43,515 Stage: Train 0.5 | Epoch: 102 | Iter: 312400 | Total Loss: 0.002494 | Recon Loss: 0.002067 | Commit Loss: 0.000855 | Perplexity: 965.107313
2025-09-14 20:15:52,218 Stage: Train 0.5 | Epoch: 102 | Iter: 312600 | Total Loss: 0.002528 | Recon Loss: 0.002098 | Commit Loss: 0.000860 | Perplexity: 961.380244
2025-09-14 20:16:00,929 Stage: Train 0.5 | Epoch: 102 | Iter: 312800 | Total Loss: 0.002604 | Recon Loss: 0.002171 | Commit Loss: 0.000865 | Perplexity: 968.585733
Trainning Epoch:  62%|██████▏   | 103/165 [3:47:59<2:17:45, 133.32s/it]2025-09-14 20:16:09,641 Stage: Train 0.5 | Epoch: 103 | Iter: 313000 | Total Loss: 0.002520 | Recon Loss: 0.002091 | Commit Loss: 0.000856 | Perplexity: 961.443087
2025-09-14 20:16:18,358 Stage: Train 0.5 | Epoch: 103 | Iter: 313200 | Total Loss: 0.002543 | Recon Loss: 0.002116 | Commit Loss: 0.000853 | Perplexity: 965.721466
2025-09-14 20:16:27,057 Stage: Train 0.5 | Epoch: 103 | Iter: 313400 | Total Loss: 0.002556 | Recon Loss: 0.002128 | Commit Loss: 0.000855 | Perplexity: 967.299347
2025-09-14 20:16:35,765 Stage: Train 0.5 | Epoch: 103 | Iter: 313600 | Total Loss: 0.002650 | Recon Loss: 0.002225 | Commit Loss: 0.000850 | Perplexity: 965.758823
2025-09-14 20:16:44,449 Stage: Train 0.5 | Epoch: 103 | Iter: 313800 | Total Loss: 0.002524 | Recon Loss: 0.002093 | Commit Loss: 0.000862 | Perplexity: 963.812702
2025-09-14 20:16:53,172 Stage: Train 0.5 | Epoch: 103 | Iter: 314000 | Total Loss: 0.002518 | Recon Loss: 0.002091 | Commit Loss: 0.000854 | Perplexity: 959.888374
2025-09-14 20:17:01,857 Stage: Train 0.5 | Epoch: 103 | Iter: 314200 | Total Loss: 0.002519 | Recon Loss: 0.002089 | Commit Loss: 0.000861 | Perplexity: 962.525622
2025-09-14 20:17:10,552 Stage: Train 0.5 | Epoch: 103 | Iter: 314400 | Total Loss: 0.002573 | Recon Loss: 0.002140 | Commit Loss: 0.000867 | Perplexity: 963.169536
2025-09-14 20:17:19,275 Stage: Train 0.5 | Epoch: 103 | Iter: 314600 | Total Loss: 0.002551 | Recon Loss: 0.002127 | Commit Loss: 0.000848 | Perplexity: 964.168587
2025-09-14 20:17:27,967 Stage: Train 0.5 | Epoch: 103 | Iter: 314800 | Total Loss: 0.002532 | Recon Loss: 0.002110 | Commit Loss: 0.000844 | Perplexity: 961.222277
2025-09-14 20:17:36,650 Stage: Train 0.5 | Epoch: 103 | Iter: 315000 | Total Loss: 0.002577 | Recon Loss: 0.002147 | Commit Loss: 0.000859 | Perplexity: 961.188692
2025-09-14 20:17:45,347 Stage: Train 0.5 | Epoch: 103 | Iter: 315200 | Total Loss: 0.002608 | Recon Loss: 0.002179 | Commit Loss: 0.000858 | Perplexity: 966.069628
2025-09-14 20:17:54,056 Stage: Train 0.5 | Epoch: 103 | Iter: 315400 | Total Loss: 0.002505 | Recon Loss: 0.002076 | Commit Loss: 0.000858 | Perplexity: 962.778098
2025-09-14 20:18:02,893 Stage: Train 0.5 | Epoch: 103 | Iter: 315600 | Total Loss: 0.002576 | Recon Loss: 0.002150 | Commit Loss: 0.000852 | Perplexity: 961.967862
2025-09-14 20:18:11,597 Stage: Train 0.5 | Epoch: 103 | Iter: 315800 | Total Loss: 0.002505 | Recon Loss: 0.002082 | Commit Loss: 0.000847 | Perplexity: 963.353547
Trainning Epoch:  63%|██████▎   | 104/165 [3:50:11<2:15:14, 133.03s/it]2025-09-14 20:18:20,351 Stage: Train 0.5 | Epoch: 104 | Iter: 316000 | Total Loss: 0.002506 | Recon Loss: 0.002079 | Commit Loss: 0.000855 | Perplexity: 959.577948
2025-09-14 20:18:29,081 Stage: Train 0.5 | Epoch: 104 | Iter: 316200 | Total Loss: 0.002532 | Recon Loss: 0.002102 | Commit Loss: 0.000860 | Perplexity: 965.936111
2025-09-14 20:18:37,859 Stage: Train 0.5 | Epoch: 104 | Iter: 316400 | Total Loss: 0.002495 | Recon Loss: 0.002065 | Commit Loss: 0.000860 | Perplexity: 966.419775
2025-09-14 20:18:46,589 Stage: Train 0.5 | Epoch: 104 | Iter: 316600 | Total Loss: 0.002573 | Recon Loss: 0.002145 | Commit Loss: 0.000856 | Perplexity: 965.443276
2025-09-14 20:18:55,313 Stage: Train 0.5 | Epoch: 104 | Iter: 316800 | Total Loss: 0.002528 | Recon Loss: 0.002101 | Commit Loss: 0.000853 | Perplexity: 966.287676
2025-09-14 20:19:04,050 Stage: Train 0.5 | Epoch: 104 | Iter: 317000 | Total Loss: 0.002542 | Recon Loss: 0.002117 | Commit Loss: 0.000850 | Perplexity: 962.185749
2025-09-14 20:19:12,788 Stage: Train 0.5 | Epoch: 104 | Iter: 317200 | Total Loss: 0.002533 | Recon Loss: 0.002101 | Commit Loss: 0.000864 | Perplexity: 963.940806
2025-09-14 20:19:21,499 Stage: Train 0.5 | Epoch: 104 | Iter: 317400 | Total Loss: 0.002528 | Recon Loss: 0.002107 | Commit Loss: 0.000843 | Perplexity: 956.046854
2025-09-14 20:19:30,214 Stage: Train 0.5 | Epoch: 104 | Iter: 317600 | Total Loss: 0.002577 | Recon Loss: 0.002153 | Commit Loss: 0.000849 | Perplexity: 959.518479
2025-09-14 20:19:38,916 Stage: Train 0.5 | Epoch: 104 | Iter: 317800 | Total Loss: 0.002504 | Recon Loss: 0.002076 | Commit Loss: 0.000857 | Perplexity: 965.039436
2025-09-14 20:19:47,655 Stage: Train 0.5 | Epoch: 104 | Iter: 318000 | Total Loss: 0.002585 | Recon Loss: 0.002164 | Commit Loss: 0.000842 | Perplexity: 957.793476
2025-09-14 20:19:56,448 Stage: Train 0.5 | Epoch: 104 | Iter: 318200 | Total Loss: 0.002504 | Recon Loss: 0.002078 | Commit Loss: 0.000851 | Perplexity: 961.560141
2025-09-14 20:20:05,179 Stage: Train 0.5 | Epoch: 104 | Iter: 318400 | Total Loss: 0.002574 | Recon Loss: 0.002148 | Commit Loss: 0.000850 | Perplexity: 964.825766
2025-09-14 20:20:13,888 Stage: Train 0.5 | Epoch: 104 | Iter: 318600 | Total Loss: 0.002528 | Recon Loss: 0.002101 | Commit Loss: 0.000853 | Perplexity: 966.537841
2025-09-14 20:20:22,583 Stage: Train 0.5 | Epoch: 104 | Iter: 318800 | Total Loss: 0.002515 | Recon Loss: 0.002086 | Commit Loss: 0.000858 | Perplexity: 967.943748
Trainning Epoch:  64%|██████▎   | 105/165 [3:52:24<2:12:54, 132.91s/it]2025-09-14 20:20:31,332 Stage: Train 0.5 | Epoch: 105 | Iter: 319000 | Total Loss: 0.002536 | Recon Loss: 0.002111 | Commit Loss: 0.000849 | Perplexity: 957.796325
2025-09-14 20:20:40,026 Stage: Train 0.5 | Epoch: 105 | Iter: 319200 | Total Loss: 0.002520 | Recon Loss: 0.002091 | Commit Loss: 0.000857 | Perplexity: 968.560638
2025-09-14 20:20:48,701 Stage: Train 0.5 | Epoch: 105 | Iter: 319400 | Total Loss: 0.002572 | Recon Loss: 0.002151 | Commit Loss: 0.000842 | Perplexity: 959.144570
2025-09-14 20:20:57,401 Stage: Train 0.5 | Epoch: 105 | Iter: 319600 | Total Loss: 0.002471 | Recon Loss: 0.002044 | Commit Loss: 0.000854 | Perplexity: 962.610633
2025-09-14 20:21:06,097 Stage: Train 0.5 | Epoch: 105 | Iter: 319800 | Total Loss: 0.002552 | Recon Loss: 0.002126 | Commit Loss: 0.000853 | Perplexity: 963.585945
2025-09-14 20:21:14,801 Stage: Train 0.5 | Epoch: 105 | Iter: 320000 | Total Loss: 0.002549 | Recon Loss: 0.002122 | Commit Loss: 0.000854 | Perplexity: 964.403330
2025-09-14 20:21:14,801 Saving model at iteration 320000
2025-09-14 20:21:15,187 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_106_step_320000
2025-09-14 20:21:15,421 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_106_step_320000/pytorch_model.bin
2025-09-14 20:21:15,793 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_106_step_320000/optimizer.bin
2025-09-14 20:21:15,793 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_106_step_320000/scheduler.bin
2025-09-14 20:21:15,794 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_106_step_320000/random_states_0.pkl
2025-09-14 20:21:24,887 Stage: Train 0.5 | Epoch: 105 | Iter: 320200 | Total Loss: 0.002529 | Recon Loss: 0.002101 | Commit Loss: 0.000857 | Perplexity: 967.461322
2025-09-14 20:21:33,613 Stage: Train 0.5 | Epoch: 105 | Iter: 320400 | Total Loss: 0.002539 | Recon Loss: 0.002112 | Commit Loss: 0.000854 | Perplexity: 963.242491
2025-09-14 20:21:42,349 Stage: Train 0.5 | Epoch: 105 | Iter: 320600 | Total Loss: 0.002507 | Recon Loss: 0.002081 | Commit Loss: 0.000852 | Perplexity: 962.073677
2025-09-14 20:21:51,064 Stage: Train 0.5 | Epoch: 105 | Iter: 320800 | Total Loss: 0.002476 | Recon Loss: 0.002054 | Commit Loss: 0.000846 | Perplexity: 962.745184
2025-09-14 20:21:59,816 Stage: Train 0.5 | Epoch: 105 | Iter: 321000 | Total Loss: 0.002556 | Recon Loss: 0.002132 | Commit Loss: 0.000848 | Perplexity: 960.870404
2025-09-14 20:22:08,520 Stage: Train 0.5 | Epoch: 105 | Iter: 321200 | Total Loss: 0.002515 | Recon Loss: 0.002089 | Commit Loss: 0.000852 | Perplexity: 965.852903
2025-09-14 20:22:17,241 Stage: Train 0.5 | Epoch: 105 | Iter: 321400 | Total Loss: 0.002536 | Recon Loss: 0.002110 | Commit Loss: 0.000853 | Perplexity: 968.333639
2025-09-14 20:22:26,047 Stage: Train 0.5 | Epoch: 105 | Iter: 321600 | Total Loss: 0.002543 | Recon Loss: 0.002117 | Commit Loss: 0.000853 | Perplexity: 961.606486
2025-09-14 20:22:34,844 Stage: Train 0.5 | Epoch: 105 | Iter: 321800 | Total Loss: 0.002533 | Recon Loss: 0.002107 | Commit Loss: 0.000853 | Perplexity: 963.555974
2025-09-14 20:22:43,633 Stage: Train 0.5 | Epoch: 105 | Iter: 322000 | Total Loss: 0.002519 | Recon Loss: 0.002096 | Commit Loss: 0.000844 | Perplexity: 961.173922
Trainning Epoch:  64%|██████▍   | 106/165 [3:54:38<2:11:05, 133.32s/it]2025-09-14 20:22:52,704 Stage: Train 0.5 | Epoch: 106 | Iter: 322200 | Total Loss: 0.002513 | Recon Loss: 0.002085 | Commit Loss: 0.000856 | Perplexity: 964.075889
2025-09-14 20:23:01,440 Stage: Train 0.5 | Epoch: 106 | Iter: 322400 | Total Loss: 0.002483 | Recon Loss: 0.002065 | Commit Loss: 0.000836 | Perplexity: 964.842738
2025-09-14 20:23:10,161 Stage: Train 0.5 | Epoch: 106 | Iter: 322600 | Total Loss: 0.002485 | Recon Loss: 0.002059 | Commit Loss: 0.000851 | Perplexity: 967.912076
2025-09-14 20:23:18,933 Stage: Train 0.5 | Epoch: 106 | Iter: 322800 | Total Loss: 0.002534 | Recon Loss: 0.002110 | Commit Loss: 0.000847 | Perplexity: 962.950350
2025-09-14 20:23:27,716 Stage: Train 0.5 | Epoch: 106 | Iter: 323000 | Total Loss: 0.002545 | Recon Loss: 0.002118 | Commit Loss: 0.000855 | Perplexity: 967.526057
2025-09-14 20:23:36,440 Stage: Train 0.5 | Epoch: 106 | Iter: 323200 | Total Loss: 0.002585 | Recon Loss: 0.002161 | Commit Loss: 0.000848 | Perplexity: 963.020707
2025-09-14 20:23:45,165 Stage: Train 0.5 | Epoch: 106 | Iter: 323400 | Total Loss: 0.002504 | Recon Loss: 0.002077 | Commit Loss: 0.000854 | Perplexity: 968.340434
2025-09-14 20:23:53,902 Stage: Train 0.5 | Epoch: 106 | Iter: 323600 | Total Loss: 0.002529 | Recon Loss: 0.002104 | Commit Loss: 0.000850 | Perplexity: 961.753116
2025-09-14 20:24:02,641 Stage: Train 0.5 | Epoch: 106 | Iter: 323800 | Total Loss: 0.002513 | Recon Loss: 0.002089 | Commit Loss: 0.000847 | Perplexity: 961.254621
2025-09-14 20:24:11,330 Stage: Train 0.5 | Epoch: 106 | Iter: 324000 | Total Loss: 0.002529 | Recon Loss: 0.002101 | Commit Loss: 0.000856 | Perplexity: 969.268607
2025-09-14 20:24:20,040 Stage: Train 0.5 | Epoch: 106 | Iter: 324200 | Total Loss: 0.002483 | Recon Loss: 0.002056 | Commit Loss: 0.000853 | Perplexity: 970.279448
2025-09-14 20:24:28,761 Stage: Train 0.5 | Epoch: 106 | Iter: 324400 | Total Loss: 0.002515 | Recon Loss: 0.002092 | Commit Loss: 0.000847 | Perplexity: 959.018795
2025-09-14 20:24:37,481 Stage: Train 0.5 | Epoch: 106 | Iter: 324600 | Total Loss: 0.002549 | Recon Loss: 0.002125 | Commit Loss: 0.000848 | Perplexity: 961.683396
2025-09-14 20:24:46,162 Stage: Train 0.5 | Epoch: 106 | Iter: 324800 | Total Loss: 0.002544 | Recon Loss: 0.002121 | Commit Loss: 0.000847 | Perplexity: 962.627878
2025-09-14 20:24:54,876 Stage: Train 0.5 | Epoch: 106 | Iter: 325000 | Total Loss: 0.002549 | Recon Loss: 0.002121 | Commit Loss: 0.000857 | Perplexity: 969.395255
Trainning Epoch:  65%|██████▍   | 107/165 [3:56:51<2:08:39, 133.10s/it]2025-09-14 20:25:03,588 Stage: Train 0.5 | Epoch: 107 | Iter: 325200 | Total Loss: 0.002589 | Recon Loss: 0.002161 | Commit Loss: 0.000856 | Perplexity: 964.951732
2025-09-14 20:25:12,320 Stage: Train 0.5 | Epoch: 107 | Iter: 325400 | Total Loss: 0.002476 | Recon Loss: 0.002052 | Commit Loss: 0.000848 | Perplexity: 965.181591
2025-09-14 20:25:21,076 Stage: Train 0.5 | Epoch: 107 | Iter: 325600 | Total Loss: 0.002541 | Recon Loss: 0.002115 | Commit Loss: 0.000853 | Perplexity: 968.355938
2025-09-14 20:25:29,774 Stage: Train 0.5 | Epoch: 107 | Iter: 325800 | Total Loss: 0.002561 | Recon Loss: 0.002138 | Commit Loss: 0.000846 | Perplexity: 962.003772
2025-09-14 20:25:38,472 Stage: Train 0.5 | Epoch: 107 | Iter: 326000 | Total Loss: 0.002490 | Recon Loss: 0.002066 | Commit Loss: 0.000849 | Perplexity: 962.925758
2025-09-14 20:25:47,194 Stage: Train 0.5 | Epoch: 107 | Iter: 326200 | Total Loss: 0.002520 | Recon Loss: 0.002093 | Commit Loss: 0.000854 | Perplexity: 967.683514
2025-09-14 20:25:55,929 Stage: Train 0.5 | Epoch: 107 | Iter: 326400 | Total Loss: 0.002511 | Recon Loss: 0.002084 | Commit Loss: 0.000853 | Perplexity: 966.534113
2025-09-14 20:26:04,664 Stage: Train 0.5 | Epoch: 107 | Iter: 326600 | Total Loss: 0.002530 | Recon Loss: 0.002105 | Commit Loss: 0.000848 | Perplexity: 960.091810
2025-09-14 20:26:13,350 Stage: Train 0.5 | Epoch: 107 | Iter: 326800 | Total Loss: 0.002514 | Recon Loss: 0.002088 | Commit Loss: 0.000852 | Perplexity: 966.668813
2025-09-14 20:26:22,074 Stage: Train 0.5 | Epoch: 107 | Iter: 327000 | Total Loss: 0.002473 | Recon Loss: 0.002047 | Commit Loss: 0.000853 | Perplexity: 966.364118
2025-09-14 20:26:30,776 Stage: Train 0.5 | Epoch: 107 | Iter: 327200 | Total Loss: 0.002512 | Recon Loss: 0.002086 | Commit Loss: 0.000852 | Perplexity: 964.093214
2025-09-14 20:26:39,460 Stage: Train 0.5 | Epoch: 107 | Iter: 327400 | Total Loss: 0.002464 | Recon Loss: 0.002040 | Commit Loss: 0.000848 | Perplexity: 969.606515
2025-09-14 20:26:48,140 Stage: Train 0.5 | Epoch: 107 | Iter: 327600 | Total Loss: 0.002510 | Recon Loss: 0.002083 | Commit Loss: 0.000854 | Perplexity: 967.517502
2025-09-14 20:26:56,832 Stage: Train 0.5 | Epoch: 107 | Iter: 327800 | Total Loss: 0.002497 | Recon Loss: 0.002068 | Commit Loss: 0.000859 | Perplexity: 965.985735
2025-09-14 20:27:05,572 Stage: Train 0.5 | Epoch: 107 | Iter: 328000 | Total Loss: 0.002525 | Recon Loss: 0.002095 | Commit Loss: 0.000860 | Perplexity: 967.597639
Trainning Epoch:  65%|██████▌   | 108/165 [3:59:03<2:06:14, 132.88s/it]2025-09-14 20:27:14,309 Stage: Train 0.5 | Epoch: 108 | Iter: 328200 | Total Loss: 0.002506 | Recon Loss: 0.002076 | Commit Loss: 0.000860 | Perplexity: 963.980448
2025-09-14 20:27:23,008 Stage: Train 0.5 | Epoch: 108 | Iter: 328400 | Total Loss: 0.002573 | Recon Loss: 0.002147 | Commit Loss: 0.000851 | Perplexity: 962.698797
2025-09-14 20:27:31,704 Stage: Train 0.5 | Epoch: 108 | Iter: 328600 | Total Loss: 0.002495 | Recon Loss: 0.002071 | Commit Loss: 0.000848 | Perplexity: 963.073810
2025-09-14 20:27:40,408 Stage: Train 0.5 | Epoch: 108 | Iter: 328800 | Total Loss: 0.002455 | Recon Loss: 0.002032 | Commit Loss: 0.000845 | Perplexity: 964.773599
2025-09-14 20:27:49,128 Stage: Train 0.5 | Epoch: 108 | Iter: 329000 | Total Loss: 0.002551 | Recon Loss: 0.002126 | Commit Loss: 0.000851 | Perplexity: 966.337191
2025-09-14 20:27:57,824 Stage: Train 0.5 | Epoch: 108 | Iter: 329200 | Total Loss: 0.002496 | Recon Loss: 0.002067 | Commit Loss: 0.000858 | Perplexity: 970.389001
2025-09-14 20:28:06,537 Stage: Train 0.5 | Epoch: 108 | Iter: 329400 | Total Loss: 0.002530 | Recon Loss: 0.002104 | Commit Loss: 0.000852 | Perplexity: 965.109854
2025-09-14 20:28:15,217 Stage: Train 0.5 | Epoch: 108 | Iter: 329600 | Total Loss: 0.002491 | Recon Loss: 0.002065 | Commit Loss: 0.000852 | Perplexity: 968.391917
2025-09-14 20:28:23,921 Stage: Train 0.5 | Epoch: 108 | Iter: 329800 | Total Loss: 0.002486 | Recon Loss: 0.002062 | Commit Loss: 0.000847 | Perplexity: 967.989216
2025-09-14 20:28:32,609 Stage: Train 0.5 | Epoch: 108 | Iter: 330000 | Total Loss: 0.002494 | Recon Loss: 0.002066 | Commit Loss: 0.000854 | Perplexity: 970.904423
2025-09-14 20:28:41,332 Stage: Train 0.5 | Epoch: 108 | Iter: 330200 | Total Loss: 0.002554 | Recon Loss: 0.002130 | Commit Loss: 0.000849 | Perplexity: 966.934095
2025-09-14 20:28:50,019 Stage: Train 0.5 | Epoch: 108 | Iter: 330400 | Total Loss: 0.002518 | Recon Loss: 0.002091 | Commit Loss: 0.000854 | Perplexity: 962.977187
2025-09-14 20:28:58,732 Stage: Train 0.5 | Epoch: 108 | Iter: 330600 | Total Loss: 0.002500 | Recon Loss: 0.002076 | Commit Loss: 0.000849 | Perplexity: 963.726884
2025-09-14 20:29:07,461 Stage: Train 0.5 | Epoch: 108 | Iter: 330800 | Total Loss: 0.002489 | Recon Loss: 0.002061 | Commit Loss: 0.000858 | Perplexity: 970.548963
2025-09-14 20:29:16,180 Stage: Train 0.5 | Epoch: 108 | Iter: 331000 | Total Loss: 0.002531 | Recon Loss: 0.002110 | Commit Loss: 0.000842 | Perplexity: 965.244709
Trainning Epoch:  66%|██████▌   | 109/165 [4:01:15<2:03:50, 132.69s/it]2025-09-14 20:29:24,897 Stage: Train 0.5 | Epoch: 109 | Iter: 331200 | Total Loss: 0.002532 | Recon Loss: 0.002108 | Commit Loss: 0.000849 | Perplexity: 962.892912
2025-09-14 20:29:33,608 Stage: Train 0.5 | Epoch: 109 | Iter: 331400 | Total Loss: 0.002492 | Recon Loss: 0.002070 | Commit Loss: 0.000844 | Perplexity: 964.255303
2025-09-14 20:29:42,289 Stage: Train 0.5 | Epoch: 109 | Iter: 331600 | Total Loss: 0.002534 | Recon Loss: 0.002105 | Commit Loss: 0.000857 | Perplexity: 969.074418
2025-09-14 20:29:51,017 Stage: Train 0.5 | Epoch: 109 | Iter: 331800 | Total Loss: 0.002454 | Recon Loss: 0.002033 | Commit Loss: 0.000843 | Perplexity: 960.875129
2025-09-14 20:29:59,780 Stage: Train 0.5 | Epoch: 109 | Iter: 332000 | Total Loss: 0.002533 | Recon Loss: 0.002112 | Commit Loss: 0.000843 | Perplexity: 962.887852
2025-09-14 20:30:08,509 Stage: Train 0.5 | Epoch: 109 | Iter: 332200 | Total Loss: 0.002494 | Recon Loss: 0.002071 | Commit Loss: 0.000846 | Perplexity: 964.393374
2025-09-14 20:30:17,228 Stage: Train 0.5 | Epoch: 109 | Iter: 332400 | Total Loss: 0.002463 | Recon Loss: 0.002042 | Commit Loss: 0.000840 | Perplexity: 963.673034
2025-09-14 20:30:25,920 Stage: Train 0.5 | Epoch: 109 | Iter: 332600 | Total Loss: 0.002513 | Recon Loss: 0.002086 | Commit Loss: 0.000855 | Perplexity: 967.438769
2025-09-14 20:30:34,625 Stage: Train 0.5 | Epoch: 109 | Iter: 332800 | Total Loss: 0.002483 | Recon Loss: 0.002057 | Commit Loss: 0.000852 | Perplexity: 967.925020
2025-09-14 20:30:43,344 Stage: Train 0.5 | Epoch: 109 | Iter: 333000 | Total Loss: 0.002507 | Recon Loss: 0.002081 | Commit Loss: 0.000851 | Perplexity: 965.676060
2025-09-14 20:30:52,046 Stage: Train 0.5 | Epoch: 109 | Iter: 333200 | Total Loss: 0.002506 | Recon Loss: 0.002079 | Commit Loss: 0.000854 | Perplexity: 967.178521
2025-09-14 20:31:00,785 Stage: Train 0.5 | Epoch: 109 | Iter: 333400 | Total Loss: 0.002505 | Recon Loss: 0.002079 | Commit Loss: 0.000853 | Perplexity: 969.746792
2025-09-14 20:31:09,496 Stage: Train 0.5 | Epoch: 109 | Iter: 333600 | Total Loss: 0.002503 | Recon Loss: 0.002079 | Commit Loss: 0.000848 | Perplexity: 966.093427
2025-09-14 20:31:18,216 Stage: Train 0.5 | Epoch: 109 | Iter: 333800 | Total Loss: 0.002522 | Recon Loss: 0.002099 | Commit Loss: 0.000847 | Perplexity: 966.244304
2025-09-14 20:31:26,950 Stage: Train 0.5 | Epoch: 109 | Iter: 334000 | Total Loss: 0.002502 | Recon Loss: 0.002078 | Commit Loss: 0.000848 | Perplexity: 966.900210
Trainning Epoch:  67%|██████▋   | 110/165 [4:03:28<2:01:33, 132.62s/it]2025-09-14 20:31:35,664 Stage: Train 0.5 | Epoch: 110 | Iter: 334200 | Total Loss: 0.002505 | Recon Loss: 0.002084 | Commit Loss: 0.000842 | Perplexity: 964.914771
2025-09-14 20:31:44,380 Stage: Train 0.5 | Epoch: 110 | Iter: 334400 | Total Loss: 0.002488 | Recon Loss: 0.002065 | Commit Loss: 0.000846 | Perplexity: 968.813995
2025-09-14 20:31:53,109 Stage: Train 0.5 | Epoch: 110 | Iter: 334600 | Total Loss: 0.002532 | Recon Loss: 0.002107 | Commit Loss: 0.000850 | Perplexity: 966.768540
2025-09-14 20:32:01,846 Stage: Train 0.5 | Epoch: 110 | Iter: 334800 | Total Loss: 0.002459 | Recon Loss: 0.002038 | Commit Loss: 0.000843 | Perplexity: 966.650180
2025-09-14 20:32:10,584 Stage: Train 0.5 | Epoch: 110 | Iter: 335000 | Total Loss: 0.002502 | Recon Loss: 0.002078 | Commit Loss: 0.000848 | Perplexity: 963.890613
2025-09-14 20:32:19,312 Stage: Train 0.5 | Epoch: 110 | Iter: 335200 | Total Loss: 0.002532 | Recon Loss: 0.002110 | Commit Loss: 0.000844 | Perplexity: 965.072912
2025-09-14 20:32:28,021 Stage: Train 0.5 | Epoch: 110 | Iter: 335400 | Total Loss: 0.002399 | Recon Loss: 0.001981 | Commit Loss: 0.000836 | Perplexity: 964.638695
2025-09-14 20:32:36,733 Stage: Train 0.5 | Epoch: 110 | Iter: 335600 | Total Loss: 0.002539 | Recon Loss: 0.002109 | Commit Loss: 0.000861 | Perplexity: 968.749881
2025-09-14 20:32:45,448 Stage: Train 0.5 | Epoch: 110 | Iter: 335800 | Total Loss: 0.002481 | Recon Loss: 0.002057 | Commit Loss: 0.000849 | Perplexity: 971.891286
2025-09-14 20:32:54,197 Stage: Train 0.5 | Epoch: 110 | Iter: 336000 | Total Loss: 0.002476 | Recon Loss: 0.002053 | Commit Loss: 0.000846 | Perplexity: 965.434403
2025-09-14 20:33:02,929 Stage: Train 0.5 | Epoch: 110 | Iter: 336200 | Total Loss: 0.002519 | Recon Loss: 0.002093 | Commit Loss: 0.000852 | Perplexity: 968.795737
2025-09-14 20:33:11,660 Stage: Train 0.5 | Epoch: 110 | Iter: 336400 | Total Loss: 0.002474 | Recon Loss: 0.002052 | Commit Loss: 0.000844 | Perplexity: 960.854243
2025-09-14 20:33:20,378 Stage: Train 0.5 | Epoch: 110 | Iter: 336600 | Total Loss: 0.002524 | Recon Loss: 0.002098 | Commit Loss: 0.000851 | Perplexity: 968.043134
2025-09-14 20:33:29,096 Stage: Train 0.5 | Epoch: 110 | Iter: 336800 | Total Loss: 0.002510 | Recon Loss: 0.002086 | Commit Loss: 0.000848 | Perplexity: 967.847787
2025-09-14 20:33:37,808 Stage: Train 0.5 | Epoch: 110 | Iter: 337000 | Total Loss: 0.002524 | Recon Loss: 0.002099 | Commit Loss: 0.000850 | Perplexity: 970.075919
2025-09-14 20:33:46,532 Stage: Train 0.5 | Epoch: 110 | Iter: 337200 | Total Loss: 0.002519 | Recon Loss: 0.002089 | Commit Loss: 0.000859 | Perplexity: 969.956158
Trainning Epoch:  67%|██████▋   | 111/165 [4:05:40<1:59:19, 132.59s/it]2025-09-14 20:33:55,238 Stage: Train 0.5 | Epoch: 111 | Iter: 337400 | Total Loss: 0.002492 | Recon Loss: 0.002067 | Commit Loss: 0.000850 | Perplexity: 967.474837
2025-09-14 20:34:03,951 Stage: Train 0.5 | Epoch: 111 | Iter: 337600 | Total Loss: 0.002448 | Recon Loss: 0.002025 | Commit Loss: 0.000848 | Perplexity: 969.065088
2025-09-14 20:34:12,653 Stage: Train 0.5 | Epoch: 111 | Iter: 337800 | Total Loss: 0.002483 | Recon Loss: 0.002059 | Commit Loss: 0.000848 | Perplexity: 970.000152
2025-09-14 20:34:21,343 Stage: Train 0.5 | Epoch: 111 | Iter: 338000 | Total Loss: 0.002515 | Recon Loss: 0.002094 | Commit Loss: 0.000842 | Perplexity: 966.379527
2025-09-14 20:34:30,006 Stage: Train 0.5 | Epoch: 111 | Iter: 338200 | Total Loss: 0.002447 | Recon Loss: 0.002027 | Commit Loss: 0.000839 | Perplexity: 966.045315
2025-09-14 20:34:38,721 Stage: Train 0.5 | Epoch: 111 | Iter: 338400 | Total Loss: 0.002509 | Recon Loss: 0.002085 | Commit Loss: 0.000848 | Perplexity: 968.592444
2025-09-14 20:34:47,420 Stage: Train 0.5 | Epoch: 111 | Iter: 338600 | Total Loss: 0.002501 | Recon Loss: 0.002075 | Commit Loss: 0.000851 | Perplexity: 970.315522
2025-09-14 20:34:56,131 Stage: Train 0.5 | Epoch: 111 | Iter: 338800 | Total Loss: 0.002508 | Recon Loss: 0.002088 | Commit Loss: 0.000841 | Perplexity: 964.315596
2025-09-14 20:35:04,831 Stage: Train 0.5 | Epoch: 111 | Iter: 339000 | Total Loss: 0.002434 | Recon Loss: 0.002012 | Commit Loss: 0.000845 | Perplexity: 966.713759
2025-09-14 20:35:13,542 Stage: Train 0.5 | Epoch: 111 | Iter: 339200 | Total Loss: 0.002526 | Recon Loss: 0.002094 | Commit Loss: 0.000862 | Perplexity: 973.049907
2025-09-14 20:35:22,228 Stage: Train 0.5 | Epoch: 111 | Iter: 339400 | Total Loss: 0.002513 | Recon Loss: 0.002086 | Commit Loss: 0.000854 | Perplexity: 971.317126
2025-09-14 20:35:30,976 Stage: Train 0.5 | Epoch: 111 | Iter: 339600 | Total Loss: 0.002437 | Recon Loss: 0.002014 | Commit Loss: 0.000847 | Perplexity: 965.354204
2025-09-14 20:35:39,698 Stage: Train 0.5 | Epoch: 111 | Iter: 339800 | Total Loss: 0.002558 | Recon Loss: 0.002133 | Commit Loss: 0.000849 | Perplexity: 969.893089
2025-09-14 20:35:48,444 Stage: Train 0.5 | Epoch: 111 | Iter: 340000 | Total Loss: 0.002470 | Recon Loss: 0.002050 | Commit Loss: 0.000841 | Perplexity: 967.549853
2025-09-14 20:35:48,445 Saving model at iteration 340000
2025-09-14 20:35:48,892 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_112_step_340000
2025-09-14 20:35:49,127 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_112_step_340000/pytorch_model.bin
2025-09-14 20:35:49,495 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_112_step_340000/optimizer.bin
2025-09-14 20:35:49,495 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_112_step_340000/scheduler.bin
2025-09-14 20:35:49,496 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_112_step_340000/random_states_0.pkl
2025-09-14 20:35:58,650 Stage: Train 0.5 | Epoch: 111 | Iter: 340200 | Total Loss: 0.002482 | Recon Loss: 0.002059 | Commit Loss: 0.000847 | Perplexity: 966.401202
Trainning Epoch:  68%|██████▊   | 112/165 [4:07:54<1:57:26, 132.95s/it]2025-09-14 20:36:07,378 Stage: Train 0.5 | Epoch: 112 | Iter: 340400 | Total Loss: 0.002474 | Recon Loss: 0.002051 | Commit Loss: 0.000846 | Perplexity: 968.740341
2025-09-14 20:36:16,086 Stage: Train 0.5 | Epoch: 112 | Iter: 340600 | Total Loss: 0.002520 | Recon Loss: 0.002101 | Commit Loss: 0.000838 | Perplexity: 968.163736
2025-09-14 20:36:24,800 Stage: Train 0.5 | Epoch: 112 | Iter: 340800 | Total Loss: 0.002474 | Recon Loss: 0.002050 | Commit Loss: 0.000847 | Perplexity: 969.840525
2025-09-14 20:36:33,499 Stage: Train 0.5 | Epoch: 112 | Iter: 341000 | Total Loss: 0.002491 | Recon Loss: 0.002071 | Commit Loss: 0.000839 | Perplexity: 968.292788
2025-09-14 20:36:42,238 Stage: Train 0.5 | Epoch: 112 | Iter: 341200 | Total Loss: 0.002475 | Recon Loss: 0.002056 | Commit Loss: 0.000837 | Perplexity: 966.991562
2025-09-14 20:36:50,978 Stage: Train 0.5 | Epoch: 112 | Iter: 341400 | Total Loss: 0.002463 | Recon Loss: 0.002041 | Commit Loss: 0.000843 | Perplexity: 968.872136
2025-09-14 20:36:59,696 Stage: Train 0.5 | Epoch: 112 | Iter: 341600 | Total Loss: 0.002487 | Recon Loss: 0.002060 | Commit Loss: 0.000855 | Perplexity: 972.243251
2025-09-14 20:37:08,430 Stage: Train 0.5 | Epoch: 112 | Iter: 341800 | Total Loss: 0.002480 | Recon Loss: 0.002058 | Commit Loss: 0.000844 | Perplexity: 967.222139
2025-09-14 20:37:17,150 Stage: Train 0.5 | Epoch: 112 | Iter: 342000 | Total Loss: 0.002531 | Recon Loss: 0.002103 | Commit Loss: 0.000855 | Perplexity: 969.161990
2025-09-14 20:37:25,899 Stage: Train 0.5 | Epoch: 112 | Iter: 342200 | Total Loss: 0.002505 | Recon Loss: 0.002083 | Commit Loss: 0.000843 | Perplexity: 969.139703
2025-09-14 20:37:34,634 Stage: Train 0.5 | Epoch: 112 | Iter: 342400 | Total Loss: 0.002466 | Recon Loss: 0.002043 | Commit Loss: 0.000844 | Perplexity: 966.237689
2025-09-14 20:37:43,374 Stage: Train 0.5 | Epoch: 112 | Iter: 342600 | Total Loss: 0.002520 | Recon Loss: 0.002098 | Commit Loss: 0.000845 | Perplexity: 965.293056
2025-09-14 20:37:52,084 Stage: Train 0.5 | Epoch: 112 | Iter: 342800 | Total Loss: 0.002413 | Recon Loss: 0.001991 | Commit Loss: 0.000843 | Perplexity: 971.375168
2025-09-14 20:38:00,816 Stage: Train 0.5 | Epoch: 112 | Iter: 343000 | Total Loss: 0.002527 | Recon Loss: 0.002097 | Commit Loss: 0.000860 | Perplexity: 972.225539
2025-09-14 20:38:09,540 Stage: Train 0.5 | Epoch: 112 | Iter: 343200 | Total Loss: 0.002491 | Recon Loss: 0.002074 | Commit Loss: 0.000835 | Perplexity: 961.858392
Trainning Epoch:  68%|██████▊   | 113/165 [4:10:07<1:55:06, 132.82s/it]2025-09-14 20:38:18,268 Stage: Train 0.5 | Epoch: 113 | Iter: 343400 | Total Loss: 0.002536 | Recon Loss: 0.002116 | Commit Loss: 0.000839 | Perplexity: 967.839421
2025-09-14 20:38:26,984 Stage: Train 0.5 | Epoch: 113 | Iter: 343600 | Total Loss: 0.002428 | Recon Loss: 0.002007 | Commit Loss: 0.000842 | Perplexity: 969.920092
2025-09-14 20:38:35,725 Stage: Train 0.5 | Epoch: 113 | Iter: 343800 | Total Loss: 0.002524 | Recon Loss: 0.002098 | Commit Loss: 0.000852 | Perplexity: 971.700583
2025-09-14 20:38:44,464 Stage: Train 0.5 | Epoch: 113 | Iter: 344000 | Total Loss: 0.002438 | Recon Loss: 0.002017 | Commit Loss: 0.000840 | Perplexity: 968.374142
2025-09-14 20:38:53,199 Stage: Train 0.5 | Epoch: 113 | Iter: 344200 | Total Loss: 0.002462 | Recon Loss: 0.002042 | Commit Loss: 0.000842 | Perplexity: 963.886112
2025-09-14 20:39:01,972 Stage: Train 0.5 | Epoch: 113 | Iter: 344400 | Total Loss: 0.002468 | Recon Loss: 0.002045 | Commit Loss: 0.000846 | Perplexity: 967.859510
2025-09-14 20:39:10,756 Stage: Train 0.5 | Epoch: 113 | Iter: 344600 | Total Loss: 0.002473 | Recon Loss: 0.002050 | Commit Loss: 0.000846 | Perplexity: 975.335633
2025-09-14 20:39:19,571 Stage: Train 0.5 | Epoch: 113 | Iter: 344800 | Total Loss: 0.002471 | Recon Loss: 0.002049 | Commit Loss: 0.000843 | Perplexity: 969.817805
2025-09-14 20:39:28,372 Stage: Train 0.5 | Epoch: 113 | Iter: 345000 | Total Loss: 0.002556 | Recon Loss: 0.002134 | Commit Loss: 0.000845 | Perplexity: 967.079203
2025-09-14 20:39:37,117 Stage: Train 0.5 | Epoch: 113 | Iter: 345200 | Total Loss: 0.002436 | Recon Loss: 0.002022 | Commit Loss: 0.000828 | Perplexity: 962.527454
2025-09-14 20:39:45,903 Stage: Train 0.5 | Epoch: 113 | Iter: 345400 | Total Loss: 0.002491 | Recon Loss: 0.002072 | Commit Loss: 0.000839 | Perplexity: 969.251979
2025-09-14 20:39:54,640 Stage: Train 0.5 | Epoch: 113 | Iter: 345600 | Total Loss: 0.002513 | Recon Loss: 0.002091 | Commit Loss: 0.000846 | Perplexity: 970.105824
2025-09-14 20:40:03,404 Stage: Train 0.5 | Epoch: 113 | Iter: 345800 | Total Loss: 0.002475 | Recon Loss: 0.002051 | Commit Loss: 0.000847 | Perplexity: 971.160433
2025-09-14 20:40:12,173 Stage: Train 0.5 | Epoch: 113 | Iter: 346000 | Total Loss: 0.002475 | Recon Loss: 0.002055 | Commit Loss: 0.000840 | Perplexity: 965.245017
2025-09-14 20:40:20,932 Stage: Train 0.5 | Epoch: 113 | Iter: 346200 | Total Loss: 0.002485 | Recon Loss: 0.002065 | Commit Loss: 0.000840 | Perplexity: 968.977868
Trainning Epoch:  69%|██████▉   | 114/165 [4:12:20<1:52:58, 132.90s/it]2025-09-14 20:40:29,708 Stage: Train 0.5 | Epoch: 114 | Iter: 346400 | Total Loss: 0.002547 | Recon Loss: 0.002125 | Commit Loss: 0.000844 | Perplexity: 966.247498
2025-09-14 20:40:38,461 Stage: Train 0.5 | Epoch: 114 | Iter: 346600 | Total Loss: 0.002444 | Recon Loss: 0.002027 | Commit Loss: 0.000834 | Perplexity: 965.842293
2025-09-14 20:40:47,211 Stage: Train 0.5 | Epoch: 114 | Iter: 346800 | Total Loss: 0.002492 | Recon Loss: 0.002073 | Commit Loss: 0.000836 | Perplexity: 968.103517
2025-09-14 20:40:55,983 Stage: Train 0.5 | Epoch: 114 | Iter: 347000 | Total Loss: 0.002473 | Recon Loss: 0.002056 | Commit Loss: 0.000832 | Perplexity: 966.135587
2025-09-14 20:41:04,716 Stage: Train 0.5 | Epoch: 114 | Iter: 347200 | Total Loss: 0.002478 | Recon Loss: 0.002057 | Commit Loss: 0.000842 | Perplexity: 969.810439
2025-09-14 20:41:13,448 Stage: Train 0.5 | Epoch: 114 | Iter: 347400 | Total Loss: 0.002469 | Recon Loss: 0.002049 | Commit Loss: 0.000841 | Perplexity: 974.298897
2025-09-14 20:41:22,218 Stage: Train 0.5 | Epoch: 114 | Iter: 347600 | Total Loss: 0.002486 | Recon Loss: 0.002065 | Commit Loss: 0.000842 | Perplexity: 967.677628
2025-09-14 20:41:31,011 Stage: Train 0.5 | Epoch: 114 | Iter: 347800 | Total Loss: 0.002467 | Recon Loss: 0.002043 | Commit Loss: 0.000849 | Perplexity: 971.986650
2025-09-14 20:41:39,758 Stage: Train 0.5 | Epoch: 114 | Iter: 348000 | Total Loss: 0.002496 | Recon Loss: 0.002073 | Commit Loss: 0.000846 | Perplexity: 970.198483
2025-09-14 20:41:48,508 Stage: Train 0.5 | Epoch: 114 | Iter: 348200 | Total Loss: 0.002478 | Recon Loss: 0.002057 | Commit Loss: 0.000841 | Perplexity: 972.788548
2025-09-14 20:41:57,275 Stage: Train 0.5 | Epoch: 114 | Iter: 348400 | Total Loss: 0.002465 | Recon Loss: 0.002047 | Commit Loss: 0.000835 | Perplexity: 972.667533
2025-09-14 20:42:06,032 Stage: Train 0.5 | Epoch: 114 | Iter: 348600 | Total Loss: 0.002452 | Recon Loss: 0.002032 | Commit Loss: 0.000840 | Perplexity: 971.708152
2025-09-14 20:42:14,811 Stage: Train 0.5 | Epoch: 114 | Iter: 348800 | Total Loss: 0.002473 | Recon Loss: 0.002054 | Commit Loss: 0.000838 | Perplexity: 964.304961
2025-09-14 20:42:23,586 Stage: Train 0.5 | Epoch: 114 | Iter: 349000 | Total Loss: 0.002468 | Recon Loss: 0.002048 | Commit Loss: 0.000840 | Perplexity: 967.740948
2025-09-14 20:42:32,336 Stage: Train 0.5 | Epoch: 114 | Iter: 349200 | Total Loss: 0.002465 | Recon Loss: 0.002043 | Commit Loss: 0.000844 | Perplexity: 967.487527
Trainning Epoch:  70%|██████▉   | 115/165 [4:14:33<1:50:47, 132.95s/it]2025-09-14 20:42:41,097 Stage: Train 0.5 | Epoch: 115 | Iter: 349400 | Total Loss: 0.002480 | Recon Loss: 0.002059 | Commit Loss: 0.000842 | Perplexity: 969.543816
2025-09-14 20:42:49,870 Stage: Train 0.5 | Epoch: 115 | Iter: 349600 | Total Loss: 0.002458 | Recon Loss: 0.002041 | Commit Loss: 0.000834 | Perplexity: 967.752981
2025-09-14 20:42:58,640 Stage: Train 0.5 | Epoch: 115 | Iter: 349800 | Total Loss: 0.002469 | Recon Loss: 0.002052 | Commit Loss: 0.000835 | Perplexity: 971.258069
2025-09-14 20:43:07,425 Stage: Train 0.5 | Epoch: 115 | Iter: 350000 | Total Loss: 0.002445 | Recon Loss: 0.002025 | Commit Loss: 0.000841 | Perplexity: 971.877427
2025-09-14 20:43:16,210 Stage: Train 0.5 | Epoch: 115 | Iter: 350200 | Total Loss: 0.002473 | Recon Loss: 0.002055 | Commit Loss: 0.000836 | Perplexity: 967.215424
2025-09-14 20:43:24,969 Stage: Train 0.5 | Epoch: 115 | Iter: 350400 | Total Loss: 0.002504 | Recon Loss: 0.002085 | Commit Loss: 0.000840 | Perplexity: 970.331140
2025-09-14 20:43:33,718 Stage: Train 0.5 | Epoch: 115 | Iter: 350600 | Total Loss: 0.002464 | Recon Loss: 0.002037 | Commit Loss: 0.000854 | Perplexity: 975.019127
2025-09-14 20:43:42,468 Stage: Train 0.5 | Epoch: 115 | Iter: 350800 | Total Loss: 0.002497 | Recon Loss: 0.002078 | Commit Loss: 0.000837 | Perplexity: 973.341112
2025-09-14 20:43:51,204 Stage: Train 0.5 | Epoch: 115 | Iter: 351000 | Total Loss: 0.002493 | Recon Loss: 0.002078 | Commit Loss: 0.000832 | Perplexity: 968.113119
2025-09-14 20:43:59,965 Stage: Train 0.5 | Epoch: 115 | Iter: 351200 | Total Loss: 0.002483 | Recon Loss: 0.002068 | Commit Loss: 0.000831 | Perplexity: 964.717599
2025-09-14 20:44:08,740 Stage: Train 0.5 | Epoch: 115 | Iter: 351400 | Total Loss: 0.002430 | Recon Loss: 0.002014 | Commit Loss: 0.000831 | Perplexity: 964.848381
2025-09-14 20:44:17,489 Stage: Train 0.5 | Epoch: 115 | Iter: 351600 | Total Loss: 0.002485 | Recon Loss: 0.002063 | Commit Loss: 0.000844 | Perplexity: 974.110244
2025-09-14 20:44:26,256 Stage: Train 0.5 | Epoch: 115 | Iter: 351800 | Total Loss: 0.002463 | Recon Loss: 0.002043 | Commit Loss: 0.000842 | Perplexity: 968.053075
2025-09-14 20:44:34,998 Stage: Train 0.5 | Epoch: 115 | Iter: 352000 | Total Loss: 0.002442 | Recon Loss: 0.002021 | Commit Loss: 0.000841 | Perplexity: 974.989854
2025-09-14 20:44:43,751 Stage: Train 0.5 | Epoch: 115 | Iter: 352200 | Total Loss: 0.002469 | Recon Loss: 0.002049 | Commit Loss: 0.000840 | Perplexity: 972.953364
2025-09-14 20:44:52,525 Stage: Train 0.5 | Epoch: 115 | Iter: 352400 | Total Loss: 0.002469 | Recon Loss: 0.002049 | Commit Loss: 0.000840 | Perplexity: 971.347223
Trainning Epoch:  70%|███████   | 116/165 [4:16:46<1:48:36, 132.99s/it]2025-09-14 20:45:01,271 Stage: Train 0.5 | Epoch: 116 | Iter: 352600 | Total Loss: 0.002468 | Recon Loss: 0.002045 | Commit Loss: 0.000845 | Perplexity: 969.675366
2025-09-14 20:45:10,044 Stage: Train 0.5 | Epoch: 116 | Iter: 352800 | Total Loss: 0.002437 | Recon Loss: 0.002019 | Commit Loss: 0.000836 | Perplexity: 969.227379
2025-09-14 20:45:18,784 Stage: Train 0.5 | Epoch: 116 | Iter: 353000 | Total Loss: 0.002469 | Recon Loss: 0.002050 | Commit Loss: 0.000837 | Perplexity: 968.804601
2025-09-14 20:45:27,570 Stage: Train 0.5 | Epoch: 116 | Iter: 353200 | Total Loss: 0.002427 | Recon Loss: 0.002007 | Commit Loss: 0.000841 | Perplexity: 970.008253
2025-09-14 20:45:36,335 Stage: Train 0.5 | Epoch: 116 | Iter: 353400 | Total Loss: 0.002460 | Recon Loss: 0.002042 | Commit Loss: 0.000835 | Perplexity: 970.089601
2025-09-14 20:45:45,096 Stage: Train 0.5 | Epoch: 116 | Iter: 353600 | Total Loss: 0.002524 | Recon Loss: 0.002100 | Commit Loss: 0.000847 | Perplexity: 974.691289
2025-09-14 20:45:53,868 Stage: Train 0.5 | Epoch: 116 | Iter: 353800 | Total Loss: 0.002466 | Recon Loss: 0.002047 | Commit Loss: 0.000837 | Perplexity: 969.148345
2025-09-14 20:46:02,618 Stage: Train 0.5 | Epoch: 116 | Iter: 354000 | Total Loss: 0.002440 | Recon Loss: 0.002025 | Commit Loss: 0.000831 | Perplexity: 967.805695
2025-09-14 20:46:11,392 Stage: Train 0.5 | Epoch: 116 | Iter: 354200 | Total Loss: 0.002460 | Recon Loss: 0.002042 | Commit Loss: 0.000836 | Perplexity: 967.257216
2025-09-14 20:46:20,176 Stage: Train 0.5 | Epoch: 116 | Iter: 354400 | Total Loss: 0.002465 | Recon Loss: 0.002047 | Commit Loss: 0.000835 | Perplexity: 968.483751
2025-09-14 20:46:28,950 Stage: Train 0.5 | Epoch: 116 | Iter: 354600 | Total Loss: 0.002466 | Recon Loss: 0.002050 | Commit Loss: 0.000832 | Perplexity: 967.423437
2025-09-14 20:46:37,726 Stage: Train 0.5 | Epoch: 116 | Iter: 354800 | Total Loss: 0.002484 | Recon Loss: 0.002062 | Commit Loss: 0.000843 | Perplexity: 970.406665
2025-09-14 20:46:46,507 Stage: Train 0.5 | Epoch: 116 | Iter: 355000 | Total Loss: 0.002476 | Recon Loss: 0.002061 | Commit Loss: 0.000830 | Perplexity: 967.079495
2025-09-14 20:46:55,301 Stage: Train 0.5 | Epoch: 116 | Iter: 355200 | Total Loss: 0.002473 | Recon Loss: 0.002047 | Commit Loss: 0.000851 | Perplexity: 970.643442
2025-09-14 20:47:04,106 Stage: Train 0.5 | Epoch: 116 | Iter: 355400 | Total Loss: 0.002450 | Recon Loss: 0.002033 | Commit Loss: 0.000834 | Perplexity: 970.709064
Trainning Epoch:  71%|███████   | 117/165 [4:18:59<1:46:27, 133.07s/it]2025-09-14 20:47:12,827 Stage: Train 0.5 | Epoch: 117 | Iter: 355600 | Total Loss: 0.002465 | Recon Loss: 0.002048 | Commit Loss: 0.000835 | Perplexity: 971.057333
2025-09-14 20:47:21,532 Stage: Train 0.5 | Epoch: 117 | Iter: 355800 | Total Loss: 0.002484 | Recon Loss: 0.002062 | Commit Loss: 0.000844 | Perplexity: 968.700625
2025-09-14 20:47:30,225 Stage: Train 0.5 | Epoch: 117 | Iter: 356000 | Total Loss: 0.002465 | Recon Loss: 0.002044 | Commit Loss: 0.000843 | Perplexity: 969.891283
2025-09-14 20:47:38,946 Stage: Train 0.5 | Epoch: 117 | Iter: 356200 | Total Loss: 0.002482 | Recon Loss: 0.002067 | Commit Loss: 0.000831 | Perplexity: 970.896548
2025-09-14 20:47:47,660 Stage: Train 0.5 | Epoch: 117 | Iter: 356400 | Total Loss: 0.002443 | Recon Loss: 0.002027 | Commit Loss: 0.000832 | Perplexity: 967.622117
2025-09-14 20:47:56,386 Stage: Train 0.5 | Epoch: 117 | Iter: 356600 | Total Loss: 0.002447 | Recon Loss: 0.002029 | Commit Loss: 0.000837 | Perplexity: 972.013211
2025-09-14 20:48:05,128 Stage: Train 0.5 | Epoch: 117 | Iter: 356800 | Total Loss: 0.002503 | Recon Loss: 0.002081 | Commit Loss: 0.000843 | Perplexity: 974.363260
2025-09-14 20:48:13,837 Stage: Train 0.5 | Epoch: 117 | Iter: 357000 | Total Loss: 0.002426 | Recon Loss: 0.002009 | Commit Loss: 0.000835 | Perplexity: 970.543864
2025-09-14 20:48:22,530 Stage: Train 0.5 | Epoch: 117 | Iter: 357200 | Total Loss: 0.002457 | Recon Loss: 0.002041 | Commit Loss: 0.000834 | Perplexity: 967.122961
2025-09-14 20:48:31,250 Stage: Train 0.5 | Epoch: 117 | Iter: 357400 | Total Loss: 0.002436 | Recon Loss: 0.002022 | Commit Loss: 0.000829 | Perplexity: 969.320833
2025-09-14 20:48:39,969 Stage: Train 0.5 | Epoch: 117 | Iter: 357600 | Total Loss: 0.002450 | Recon Loss: 0.002034 | Commit Loss: 0.000831 | Perplexity: 971.255355
2025-09-14 20:48:48,669 Stage: Train 0.5 | Epoch: 117 | Iter: 357800 | Total Loss: 0.002494 | Recon Loss: 0.002077 | Commit Loss: 0.000835 | Perplexity: 968.771284
2025-09-14 20:48:57,373 Stage: Train 0.5 | Epoch: 117 | Iter: 358000 | Total Loss: 0.002434 | Recon Loss: 0.002017 | Commit Loss: 0.000834 | Perplexity: 971.022710
2025-09-14 20:49:06,078 Stage: Train 0.5 | Epoch: 117 | Iter: 358200 | Total Loss: 0.002453 | Recon Loss: 0.002031 | Commit Loss: 0.000843 | Perplexity: 974.541986
2025-09-14 20:49:14,790 Stage: Train 0.5 | Epoch: 117 | Iter: 358400 | Total Loss: 0.002491 | Recon Loss: 0.002074 | Commit Loss: 0.000834 | Perplexity: 970.682985
Trainning Epoch:  72%|███████▏  | 118/165 [4:21:11<1:44:03, 132.85s/it]2025-09-14 20:49:23,518 Stage: Train 0.5 | Epoch: 118 | Iter: 358600 | Total Loss: 0.002487 | Recon Loss: 0.002072 | Commit Loss: 0.000830 | Perplexity: 969.730808
2025-09-14 20:49:32,229 Stage: Train 0.5 | Epoch: 118 | Iter: 358800 | Total Loss: 0.002440 | Recon Loss: 0.002018 | Commit Loss: 0.000842 | Perplexity: 972.969587
2025-09-14 20:49:40,931 Stage: Train 0.5 | Epoch: 118 | Iter: 359000 | Total Loss: 0.002448 | Recon Loss: 0.002031 | Commit Loss: 0.000833 | Perplexity: 971.914191
2025-09-14 20:49:49,607 Stage: Train 0.5 | Epoch: 118 | Iter: 359200 | Total Loss: 0.002466 | Recon Loss: 0.002046 | Commit Loss: 0.000839 | Perplexity: 970.388097
2025-09-14 20:49:58,320 Stage: Train 0.5 | Epoch: 118 | Iter: 359400 | Total Loss: 0.002432 | Recon Loss: 0.002017 | Commit Loss: 0.000830 | Perplexity: 972.049341
2025-09-14 20:50:07,033 Stage: Train 0.5 | Epoch: 118 | Iter: 359600 | Total Loss: 0.002446 | Recon Loss: 0.002029 | Commit Loss: 0.000835 | Perplexity: 969.729640
2025-09-14 20:50:15,744 Stage: Train 0.5 | Epoch: 118 | Iter: 359800 | Total Loss: 0.002423 | Recon Loss: 0.002010 | Commit Loss: 0.000826 | Perplexity: 969.584816
2025-09-14 20:50:24,427 Stage: Train 0.5 | Epoch: 118 | Iter: 360000 | Total Loss: 0.002459 | Recon Loss: 0.002037 | Commit Loss: 0.000845 | Perplexity: 969.807820
2025-09-14 20:50:24,427 Saving model at iteration 360000
2025-09-14 20:50:24,581 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_119_step_360000
2025-09-14 20:50:24,814 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_119_step_360000/pytorch_model.bin
2025-09-14 20:50:25,184 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_119_step_360000/optimizer.bin
2025-09-14 20:50:25,184 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_119_step_360000/scheduler.bin
2025-09-14 20:50:25,185 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_119_step_360000/random_states_0.pkl
2025-09-14 20:50:33,894 Stage: Train 0.5 | Epoch: 118 | Iter: 360200 | Total Loss: 0.002517 | Recon Loss: 0.002100 | Commit Loss: 0.000834 | Perplexity: 970.340555
2025-09-14 20:50:42,621 Stage: Train 0.5 | Epoch: 118 | Iter: 360400 | Total Loss: 0.002416 | Recon Loss: 0.002005 | Commit Loss: 0.000822 | Perplexity: 962.367168
2025-09-14 20:50:51,349 Stage: Train 0.5 | Epoch: 118 | Iter: 360600 | Total Loss: 0.002445 | Recon Loss: 0.002028 | Commit Loss: 0.000834 | Perplexity: 967.724814
2025-09-14 20:51:00,045 Stage: Train 0.5 | Epoch: 118 | Iter: 360800 | Total Loss: 0.002408 | Recon Loss: 0.001991 | Commit Loss: 0.000834 | Perplexity: 972.367946
2025-09-14 20:51:08,772 Stage: Train 0.5 | Epoch: 118 | Iter: 361000 | Total Loss: 0.002484 | Recon Loss: 0.002068 | Commit Loss: 0.000832 | Perplexity: 970.264411
2025-09-14 20:51:17,508 Stage: Train 0.5 | Epoch: 118 | Iter: 361200 | Total Loss: 0.002456 | Recon Loss: 0.002037 | Commit Loss: 0.000837 | Perplexity: 973.235730
2025-09-14 20:51:26,219 Stage: Train 0.5 | Epoch: 118 | Iter: 361400 | Total Loss: 0.002476 | Recon Loss: 0.002061 | Commit Loss: 0.000830 | Perplexity: 966.006707
Trainning Epoch:  72%|███████▏  | 119/165 [4:23:25<1:41:54, 132.91s/it]2025-09-14 20:51:34,936 Stage: Train 0.5 | Epoch: 119 | Iter: 361600 | Total Loss: 0.002434 | Recon Loss: 0.002015 | Commit Loss: 0.000839 | Perplexity: 969.397753
2025-09-14 20:51:43,628 Stage: Train 0.5 | Epoch: 119 | Iter: 361800 | Total Loss: 0.002442 | Recon Loss: 0.002025 | Commit Loss: 0.000832 | Perplexity: 974.893495
2025-09-14 20:51:52,300 Stage: Train 0.5 | Epoch: 119 | Iter: 362000 | Total Loss: 0.002424 | Recon Loss: 0.002006 | Commit Loss: 0.000835 | Perplexity: 972.252770
2025-09-14 20:52:00,974 Stage: Train 0.5 | Epoch: 119 | Iter: 362200 | Total Loss: 0.002469 | Recon Loss: 0.002054 | Commit Loss: 0.000830 | Perplexity: 969.069964
2025-09-14 20:52:09,666 Stage: Train 0.5 | Epoch: 119 | Iter: 362400 | Total Loss: 0.002437 | Recon Loss: 0.002020 | Commit Loss: 0.000834 | Perplexity: 974.866192
2025-09-14 20:52:18,355 Stage: Train 0.5 | Epoch: 119 | Iter: 362600 | Total Loss: 0.002449 | Recon Loss: 0.002035 | Commit Loss: 0.000829 | Perplexity: 968.501056
2025-09-14 20:52:27,057 Stage: Train 0.5 | Epoch: 119 | Iter: 362800 | Total Loss: 0.002419 | Recon Loss: 0.002000 | Commit Loss: 0.000837 | Perplexity: 970.878672
2025-09-14 20:52:35,749 Stage: Train 0.5 | Epoch: 119 | Iter: 363000 | Total Loss: 0.002451 | Recon Loss: 0.002032 | Commit Loss: 0.000838 | Perplexity: 978.262253
2025-09-14 20:52:44,456 Stage: Train 0.5 | Epoch: 119 | Iter: 363200 | Total Loss: 0.002419 | Recon Loss: 0.002007 | Commit Loss: 0.000824 | Perplexity: 969.658130
2025-09-14 20:52:53,154 Stage: Train 0.5 | Epoch: 119 | Iter: 363400 | Total Loss: 0.002436 | Recon Loss: 0.002021 | Commit Loss: 0.000831 | Perplexity: 968.903599
2025-09-14 20:53:01,876 Stage: Train 0.5 | Epoch: 119 | Iter: 363600 | Total Loss: 0.002393 | Recon Loss: 0.001979 | Commit Loss: 0.000827 | Perplexity: 969.135183
2025-09-14 20:53:10,587 Stage: Train 0.5 | Epoch: 119 | Iter: 363800 | Total Loss: 0.002460 | Recon Loss: 0.002046 | Commit Loss: 0.000829 | Perplexity: 970.923354
2025-09-14 20:53:19,291 Stage: Train 0.5 | Epoch: 119 | Iter: 364000 | Total Loss: 0.002445 | Recon Loss: 0.002027 | Commit Loss: 0.000835 | Perplexity: 967.987739
2025-09-14 20:53:28,009 Stage: Train 0.5 | Epoch: 119 | Iter: 364200 | Total Loss: 0.002452 | Recon Loss: 0.002032 | Commit Loss: 0.000840 | Perplexity: 978.520909
2025-09-14 20:53:36,691 Stage: Train 0.5 | Epoch: 119 | Iter: 364400 | Total Loss: 0.002426 | Recon Loss: 0.002012 | Commit Loss: 0.000829 | Perplexity: 971.499310
Trainning Epoch:  73%|███████▎  | 120/165 [4:25:37<1:39:30, 132.68s/it]2025-09-14 20:53:45,382 Stage: Train 0.5 | Epoch: 120 | Iter: 364600 | Total Loss: 0.002478 | Recon Loss: 0.002065 | Commit Loss: 0.000826 | Perplexity: 965.406805
2025-09-14 20:53:54,097 Stage: Train 0.5 | Epoch: 120 | Iter: 364800 | Total Loss: 0.002414 | Recon Loss: 0.001996 | Commit Loss: 0.000834 | Perplexity: 971.430544
2025-09-14 20:54:02,786 Stage: Train 0.5 | Epoch: 120 | Iter: 365000 | Total Loss: 0.002436 | Recon Loss: 0.002019 | Commit Loss: 0.000834 | Perplexity: 970.418331
2025-09-14 20:54:11,477 Stage: Train 0.5 | Epoch: 120 | Iter: 365200 | Total Loss: 0.002402 | Recon Loss: 0.001990 | Commit Loss: 0.000823 | Perplexity: 970.884951
2025-09-14 20:54:20,137 Stage: Train 0.5 | Epoch: 120 | Iter: 365400 | Total Loss: 0.002470 | Recon Loss: 0.002058 | Commit Loss: 0.000824 | Perplexity: 968.432215
2025-09-14 20:54:28,840 Stage: Train 0.5 | Epoch: 120 | Iter: 365600 | Total Loss: 0.002458 | Recon Loss: 0.002045 | Commit Loss: 0.000826 | Perplexity: 972.587148
2025-09-14 20:54:37,536 Stage: Train 0.5 | Epoch: 120 | Iter: 365800 | Total Loss: 0.002447 | Recon Loss: 0.002031 | Commit Loss: 0.000833 | Perplexity: 972.170824
2025-09-14 20:54:46,256 Stage: Train 0.5 | Epoch: 120 | Iter: 366000 | Total Loss: 0.002448 | Recon Loss: 0.002036 | Commit Loss: 0.000823 | Perplexity: 971.727879
2025-09-14 20:54:54,939 Stage: Train 0.5 | Epoch: 120 | Iter: 366200 | Total Loss: 0.002464 | Recon Loss: 0.002049 | Commit Loss: 0.000830 | Perplexity: 972.793952
2025-09-14 20:55:03,653 Stage: Train 0.5 | Epoch: 120 | Iter: 366400 | Total Loss: 0.002444 | Recon Loss: 0.002029 | Commit Loss: 0.000829 | Perplexity: 974.551461
2025-09-14 20:55:12,352 Stage: Train 0.5 | Epoch: 120 | Iter: 366600 | Total Loss: 0.002481 | Recon Loss: 0.002061 | Commit Loss: 0.000839 | Perplexity: 972.437424
2025-09-14 20:55:21,049 Stage: Train 0.5 | Epoch: 120 | Iter: 366800 | Total Loss: 0.002420 | Recon Loss: 0.002004 | Commit Loss: 0.000830 | Perplexity: 971.928152
2025-09-14 20:55:29,728 Stage: Train 0.5 | Epoch: 120 | Iter: 367000 | Total Loss: 0.002431 | Recon Loss: 0.002016 | Commit Loss: 0.000830 | Perplexity: 968.995641
2025-09-14 20:55:38,432 Stage: Train 0.5 | Epoch: 120 | Iter: 367200 | Total Loss: 0.002417 | Recon Loss: 0.002005 | Commit Loss: 0.000824 | Perplexity: 969.760696
2025-09-14 20:55:47,133 Stage: Train 0.5 | Epoch: 120 | Iter: 367400 | Total Loss: 0.002408 | Recon Loss: 0.001998 | Commit Loss: 0.000821 | Perplexity: 967.243645
Trainning Epoch:  73%|███████▎  | 121/165 [4:27:49<1:37:09, 132.50s/it]2025-09-14 20:55:55,824 Stage: Train 0.5 | Epoch: 121 | Iter: 367600 | Total Loss: 0.002465 | Recon Loss: 0.002043 | Commit Loss: 0.000843 | Perplexity: 974.296329
2025-09-14 20:56:04,538 Stage: Train 0.5 | Epoch: 121 | Iter: 367800 | Total Loss: 0.002434 | Recon Loss: 0.002020 | Commit Loss: 0.000827 | Perplexity: 974.703158
2025-09-14 20:56:13,289 Stage: Train 0.5 | Epoch: 121 | Iter: 368000 | Total Loss: 0.002430 | Recon Loss: 0.002016 | Commit Loss: 0.000829 | Perplexity: 979.129290
2025-09-14 20:56:22,011 Stage: Train 0.5 | Epoch: 121 | Iter: 368200 | Total Loss: 0.002419 | Recon Loss: 0.002003 | Commit Loss: 0.000832 | Perplexity: 974.704465
2025-09-14 20:56:30,712 Stage: Train 0.5 | Epoch: 121 | Iter: 368400 | Total Loss: 0.002440 | Recon Loss: 0.002029 | Commit Loss: 0.000824 | Perplexity: 973.303098
2025-09-14 20:56:39,435 Stage: Train 0.5 | Epoch: 121 | Iter: 368600 | Total Loss: 0.002409 | Recon Loss: 0.001994 | Commit Loss: 0.000829 | Perplexity: 968.171327
2025-09-14 20:56:48,177 Stage: Train 0.5 | Epoch: 121 | Iter: 368800 | Total Loss: 0.002470 | Recon Loss: 0.002059 | Commit Loss: 0.000823 | Perplexity: 968.499678
2025-09-14 20:56:56,880 Stage: Train 0.5 | Epoch: 121 | Iter: 369000 | Total Loss: 0.002443 | Recon Loss: 0.002023 | Commit Loss: 0.000842 | Perplexity: 977.666076
2025-09-14 20:57:05,582 Stage: Train 0.5 | Epoch: 121 | Iter: 369200 | Total Loss: 0.002435 | Recon Loss: 0.002025 | Commit Loss: 0.000818 | Perplexity: 970.227528
2025-09-14 20:57:14,314 Stage: Train 0.5 | Epoch: 121 | Iter: 369400 | Total Loss: 0.002448 | Recon Loss: 0.002037 | Commit Loss: 0.000822 | Perplexity: 968.446354
2025-09-14 20:57:23,036 Stage: Train 0.5 | Epoch: 121 | Iter: 369600 | Total Loss: 0.002436 | Recon Loss: 0.002026 | Commit Loss: 0.000821 | Perplexity: 969.954141
2025-09-14 20:57:31,746 Stage: Train 0.5 | Epoch: 121 | Iter: 369800 | Total Loss: 0.002495 | Recon Loss: 0.002080 | Commit Loss: 0.000830 | Perplexity: 972.941840
2025-09-14 20:57:40,462 Stage: Train 0.5 | Epoch: 121 | Iter: 370000 | Total Loss: 0.002459 | Recon Loss: 0.002044 | Commit Loss: 0.000830 | Perplexity: 969.492090
2025-09-14 20:57:49,176 Stage: Train 0.5 | Epoch: 121 | Iter: 370200 | Total Loss: 0.002418 | Recon Loss: 0.002003 | Commit Loss: 0.000832 | Perplexity: 971.783127
2025-09-14 20:57:57,878 Stage: Train 0.5 | Epoch: 121 | Iter: 370400 | Total Loss: 0.002446 | Recon Loss: 0.002033 | Commit Loss: 0.000825 | Perplexity: 970.181310
2025-09-14 20:58:06,569 Stage: Train 0.5 | Epoch: 121 | Iter: 370600 | Total Loss: 0.002443 | Recon Loss: 0.002029 | Commit Loss: 0.000828 | Perplexity: 969.285352
Trainning Epoch:  74%|███████▍  | 122/165 [4:30:01<1:34:56, 132.47s/it]2025-09-14 20:58:15,274 Stage: Train 0.5 | Epoch: 122 | Iter: 370800 | Total Loss: 0.002453 | Recon Loss: 0.002041 | Commit Loss: 0.000823 | Perplexity: 971.886729
2025-09-14 20:58:23,994 Stage: Train 0.5 | Epoch: 122 | Iter: 371000 | Total Loss: 0.002411 | Recon Loss: 0.002002 | Commit Loss: 0.000818 | Perplexity: 969.213421
2025-09-14 20:58:32,692 Stage: Train 0.5 | Epoch: 122 | Iter: 371200 | Total Loss: 0.002417 | Recon Loss: 0.002006 | Commit Loss: 0.000823 | Perplexity: 972.907556
2025-09-14 20:58:41,388 Stage: Train 0.5 | Epoch: 122 | Iter: 371400 | Total Loss: 0.002440 | Recon Loss: 0.002024 | Commit Loss: 0.000831 | Perplexity: 975.351029
2025-09-14 20:58:50,094 Stage: Train 0.5 | Epoch: 122 | Iter: 371600 | Total Loss: 0.002442 | Recon Loss: 0.002024 | Commit Loss: 0.000836 | Perplexity: 976.076741
2025-09-14 20:58:58,800 Stage: Train 0.5 | Epoch: 122 | Iter: 371800 | Total Loss: 0.002396 | Recon Loss: 0.001977 | Commit Loss: 0.000839 | Perplexity: 974.711621
2025-09-14 20:59:07,498 Stage: Train 0.5 | Epoch: 122 | Iter: 372000 | Total Loss: 0.002429 | Recon Loss: 0.002015 | Commit Loss: 0.000829 | Perplexity: 975.550651
2025-09-14 20:59:16,195 Stage: Train 0.5 | Epoch: 122 | Iter: 372200 | Total Loss: 0.002483 | Recon Loss: 0.002066 | Commit Loss: 0.000834 | Perplexity: 969.944845
2025-09-14 20:59:24,922 Stage: Train 0.5 | Epoch: 122 | Iter: 372400 | Total Loss: 0.002398 | Recon Loss: 0.001985 | Commit Loss: 0.000826 | Perplexity: 969.163324
2025-09-14 20:59:33,623 Stage: Train 0.5 | Epoch: 122 | Iter: 372600 | Total Loss: 0.002402 | Recon Loss: 0.001994 | Commit Loss: 0.000816 | Perplexity: 968.117925
2025-09-14 20:59:42,344 Stage: Train 0.5 | Epoch: 122 | Iter: 372800 | Total Loss: 0.002441 | Recon Loss: 0.002026 | Commit Loss: 0.000831 | Perplexity: 972.555040
2025-09-14 20:59:51,038 Stage: Train 0.5 | Epoch: 122 | Iter: 373000 | Total Loss: 0.002493 | Recon Loss: 0.002082 | Commit Loss: 0.000824 | Perplexity: 968.783622
2025-09-14 20:59:59,730 Stage: Train 0.5 | Epoch: 122 | Iter: 373200 | Total Loss: 0.002465 | Recon Loss: 0.002051 | Commit Loss: 0.000828 | Perplexity: 974.599938
2025-09-14 21:00:08,433 Stage: Train 0.5 | Epoch: 122 | Iter: 373400 | Total Loss: 0.002420 | Recon Loss: 0.002008 | Commit Loss: 0.000822 | Perplexity: 971.966328
2025-09-14 21:00:17,122 Stage: Train 0.5 | Epoch: 122 | Iter: 373600 | Total Loss: 0.002512 | Recon Loss: 0.002093 | Commit Loss: 0.000839 | Perplexity: 977.159731
Trainning Epoch:  75%|███████▍  | 123/165 [4:32:13<1:32:40, 132.39s/it]2025-09-14 21:00:25,816 Stage: Train 0.5 | Epoch: 123 | Iter: 373800 | Total Loss: 0.002417 | Recon Loss: 0.002003 | Commit Loss: 0.000828 | Perplexity: 976.294356
2025-09-14 21:00:34,506 Stage: Train 0.5 | Epoch: 123 | Iter: 374000 | Total Loss: 0.002413 | Recon Loss: 0.002003 | Commit Loss: 0.000822 | Perplexity: 969.510919
2025-09-14 21:00:43,213 Stage: Train 0.5 | Epoch: 123 | Iter: 374200 | Total Loss: 0.002445 | Recon Loss: 0.002039 | Commit Loss: 0.000812 | Perplexity: 968.282644
2025-09-14 21:00:51,936 Stage: Train 0.5 | Epoch: 123 | Iter: 374400 | Total Loss: 0.002409 | Recon Loss: 0.001994 | Commit Loss: 0.000830 | Perplexity: 972.961981
2025-09-14 21:01:00,604 Stage: Train 0.5 | Epoch: 123 | Iter: 374600 | Total Loss: 0.002414 | Recon Loss: 0.002002 | Commit Loss: 0.000823 | Perplexity: 972.479106
2025-09-14 21:01:09,291 Stage: Train 0.5 | Epoch: 123 | Iter: 374800 | Total Loss: 0.002414 | Recon Loss: 0.002004 | Commit Loss: 0.000820 | Perplexity: 975.323704
2025-09-14 21:01:17,978 Stage: Train 0.5 | Epoch: 123 | Iter: 375000 | Total Loss: 0.002403 | Recon Loss: 0.001991 | Commit Loss: 0.000823 | Perplexity: 970.926433
2025-09-14 21:01:26,677 Stage: Train 0.5 | Epoch: 123 | Iter: 375200 | Total Loss: 0.002431 | Recon Loss: 0.002019 | Commit Loss: 0.000824 | Perplexity: 973.682785
2025-09-14 21:01:35,373 Stage: Train 0.5 | Epoch: 123 | Iter: 375400 | Total Loss: 0.002414 | Recon Loss: 0.002003 | Commit Loss: 0.000822 | Perplexity: 973.716006
2025-09-14 21:01:44,078 Stage: Train 0.5 | Epoch: 123 | Iter: 375600 | Total Loss: 0.002449 | Recon Loss: 0.002034 | Commit Loss: 0.000829 | Perplexity: 974.977805
2025-09-14 21:01:52,765 Stage: Train 0.5 | Epoch: 123 | Iter: 375800 | Total Loss: 0.002460 | Recon Loss: 0.002050 | Commit Loss: 0.000821 | Perplexity: 970.481464
2025-09-14 21:02:01,458 Stage: Train 0.5 | Epoch: 123 | Iter: 376000 | Total Loss: 0.002445 | Recon Loss: 0.002036 | Commit Loss: 0.000820 | Perplexity: 968.746356
2025-09-14 21:02:10,145 Stage: Train 0.5 | Epoch: 123 | Iter: 376200 | Total Loss: 0.002445 | Recon Loss: 0.002031 | Commit Loss: 0.000829 | Perplexity: 972.147360
2025-09-14 21:02:18,830 Stage: Train 0.5 | Epoch: 123 | Iter: 376400 | Total Loss: 0.002411 | Recon Loss: 0.001996 | Commit Loss: 0.000830 | Perplexity: 973.518786
2025-09-14 21:02:27,532 Stage: Train 0.5 | Epoch: 123 | Iter: 376600 | Total Loss: 0.002463 | Recon Loss: 0.002052 | Commit Loss: 0.000821 | Perplexity: 975.451658
Trainning Epoch:  75%|███████▌  | 124/165 [4:34:25<1:30:24, 132.29s/it]2025-09-14 21:02:36,252 Stage: Train 0.5 | Epoch: 124 | Iter: 376800 | Total Loss: 0.002431 | Recon Loss: 0.002018 | Commit Loss: 0.000826 | Perplexity: 971.346601
2025-09-14 21:02:44,960 Stage: Train 0.5 | Epoch: 124 | Iter: 377000 | Total Loss: 0.002413 | Recon Loss: 0.002002 | Commit Loss: 0.000822 | Perplexity: 973.871046
2025-09-14 21:02:53,678 Stage: Train 0.5 | Epoch: 124 | Iter: 377200 | Total Loss: 0.002420 | Recon Loss: 0.002010 | Commit Loss: 0.000820 | Perplexity: 975.022314
2025-09-14 21:03:02,423 Stage: Train 0.5 | Epoch: 124 | Iter: 377400 | Total Loss: 0.002449 | Recon Loss: 0.002037 | Commit Loss: 0.000825 | Perplexity: 972.990575
2025-09-14 21:03:11,161 Stage: Train 0.5 | Epoch: 124 | Iter: 377600 | Total Loss: 0.002396 | Recon Loss: 0.001983 | Commit Loss: 0.000826 | Perplexity: 973.618928
2025-09-14 21:03:19,858 Stage: Train 0.5 | Epoch: 124 | Iter: 377800 | Total Loss: 0.002466 | Recon Loss: 0.002055 | Commit Loss: 0.000821 | Perplexity: 971.016261
2025-09-14 21:03:28,568 Stage: Train 0.5 | Epoch: 124 | Iter: 378000 | Total Loss: 0.002438 | Recon Loss: 0.002026 | Commit Loss: 0.000824 | Perplexity: 972.628238
2025-09-14 21:03:37,304 Stage: Train 0.5 | Epoch: 124 | Iter: 378200 | Total Loss: 0.002388 | Recon Loss: 0.001977 | Commit Loss: 0.000823 | Perplexity: 977.224975
2025-09-14 21:03:46,038 Stage: Train 0.5 | Epoch: 124 | Iter: 378400 | Total Loss: 0.002458 | Recon Loss: 0.002051 | Commit Loss: 0.000816 | Perplexity: 969.803852
2025-09-14 21:03:54,756 Stage: Train 0.5 | Epoch: 124 | Iter: 378600 | Total Loss: 0.002408 | Recon Loss: 0.001997 | Commit Loss: 0.000821 | Perplexity: 968.865769
2025-09-14 21:04:03,483 Stage: Train 0.5 | Epoch: 124 | Iter: 378800 | Total Loss: 0.002461 | Recon Loss: 0.002054 | Commit Loss: 0.000814 | Perplexity: 967.134569
2025-09-14 21:04:12,215 Stage: Train 0.5 | Epoch: 124 | Iter: 379000 | Total Loss: 0.002449 | Recon Loss: 0.002037 | Commit Loss: 0.000823 | Perplexity: 978.529238
2025-09-14 21:04:20,960 Stage: Train 0.5 | Epoch: 124 | Iter: 379200 | Total Loss: 0.002439 | Recon Loss: 0.002028 | Commit Loss: 0.000822 | Perplexity: 975.831470
2025-09-14 21:04:29,655 Stage: Train 0.5 | Epoch: 124 | Iter: 379400 | Total Loss: 0.002435 | Recon Loss: 0.002018 | Commit Loss: 0.000833 | Perplexity: 976.729557
2025-09-14 21:04:38,359 Stage: Train 0.5 | Epoch: 124 | Iter: 379600 | Total Loss: 0.002423 | Recon Loss: 0.002014 | Commit Loss: 0.000819 | Perplexity: 969.750173
Trainning Epoch:  76%|███████▌  | 125/165 [4:36:38<1:28:13, 132.35s/it]2025-09-14 21:04:47,088 Stage: Train 0.5 | Epoch: 125 | Iter: 379800 | Total Loss: 0.002369 | Recon Loss: 0.001959 | Commit Loss: 0.000820 | Perplexity: 970.356800
2025-09-14 21:04:55,781 Stage: Train 0.5 | Epoch: 125 | Iter: 380000 | Total Loss: 0.002442 | Recon Loss: 0.002024 | Commit Loss: 0.000836 | Perplexity: 976.721891
2025-09-14 21:04:55,781 Saving model at iteration 380000
2025-09-14 21:04:55,940 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_126_step_380000
2025-09-14 21:04:56,171 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_126_step_380000/pytorch_model.bin
2025-09-14 21:04:56,533 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_126_step_380000/optimizer.bin
2025-09-14 21:04:56,534 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_126_step_380000/scheduler.bin
2025-09-14 21:04:56,534 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_126_step_380000/random_states_0.pkl
2025-09-14 21:05:05,244 Stage: Train 0.5 | Epoch: 125 | Iter: 380200 | Total Loss: 0.002429 | Recon Loss: 0.002019 | Commit Loss: 0.000821 | Perplexity: 968.679901
2025-09-14 21:05:13,966 Stage: Train 0.5 | Epoch: 125 | Iter: 380400 | Total Loss: 0.002435 | Recon Loss: 0.002020 | Commit Loss: 0.000830 | Perplexity: 973.354957
2025-09-14 21:05:22,704 Stage: Train 0.5 | Epoch: 125 | Iter: 380600 | Total Loss: 0.002414 | Recon Loss: 0.002007 | Commit Loss: 0.000816 | Perplexity: 969.799164
2025-09-14 21:05:31,404 Stage: Train 0.5 | Epoch: 125 | Iter: 380800 | Total Loss: 0.002415 | Recon Loss: 0.002005 | Commit Loss: 0.000820 | Perplexity: 972.218209
2025-09-14 21:05:40,090 Stage: Train 0.5 | Epoch: 125 | Iter: 381000 | Total Loss: 0.002418 | Recon Loss: 0.002005 | Commit Loss: 0.000827 | Perplexity: 975.992575
2025-09-14 21:05:48,800 Stage: Train 0.5 | Epoch: 125 | Iter: 381200 | Total Loss: 0.002431 | Recon Loss: 0.002015 | Commit Loss: 0.000831 | Perplexity: 978.270184
2025-09-14 21:05:57,487 Stage: Train 0.5 | Epoch: 125 | Iter: 381400 | Total Loss: 0.002380 | Recon Loss: 0.001969 | Commit Loss: 0.000821 | Perplexity: 973.154875
2025-09-14 21:06:06,188 Stage: Train 0.5 | Epoch: 125 | Iter: 381600 | Total Loss: 0.002442 | Recon Loss: 0.002030 | Commit Loss: 0.000824 | Perplexity: 972.510872
2025-09-14 21:06:14,926 Stage: Train 0.5 | Epoch: 125 | Iter: 381800 | Total Loss: 0.002363 | Recon Loss: 0.001955 | Commit Loss: 0.000816 | Perplexity: 974.648429
2025-09-14 21:06:23,671 Stage: Train 0.5 | Epoch: 125 | Iter: 382000 | Total Loss: 0.002405 | Recon Loss: 0.001992 | Commit Loss: 0.000825 | Perplexity: 977.475408
2025-09-14 21:06:32,358 Stage: Train 0.5 | Epoch: 125 | Iter: 382200 | Total Loss: 0.002427 | Recon Loss: 0.002012 | Commit Loss: 0.000829 | Perplexity: 972.826420
2025-09-14 21:06:41,060 Stage: Train 0.5 | Epoch: 125 | Iter: 382400 | Total Loss: 0.002431 | Recon Loss: 0.002020 | Commit Loss: 0.000823 | Perplexity: 975.948167
2025-09-14 21:06:49,768 Stage: Train 0.5 | Epoch: 125 | Iter: 382600 | Total Loss: 0.002439 | Recon Loss: 0.002030 | Commit Loss: 0.000817 | Perplexity: 971.306636
Trainning Epoch:  76%|███████▋  | 126/165 [4:38:51<1:26:14, 132.68s/it]2025-09-14 21:06:58,864 Stage: Train 0.5 | Epoch: 126 | Iter: 382800 | Total Loss: 0.002461 | Recon Loss: 0.002052 | Commit Loss: 0.000818 | Perplexity: 971.970831
2025-09-14 21:07:07,617 Stage: Train 0.5 | Epoch: 126 | Iter: 383000 | Total Loss: 0.002385 | Recon Loss: 0.001973 | Commit Loss: 0.000824 | Perplexity: 974.196538
2025-09-14 21:07:16,382 Stage: Train 0.5 | Epoch: 126 | Iter: 383200 | Total Loss: 0.002461 | Recon Loss: 0.002049 | Commit Loss: 0.000823 | Perplexity: 976.649852
2025-09-14 21:07:25,117 Stage: Train 0.5 | Epoch: 126 | Iter: 383400 | Total Loss: 0.002362 | Recon Loss: 0.001953 | Commit Loss: 0.000818 | Perplexity: 971.020482
2025-09-14 21:07:33,861 Stage: Train 0.5 | Epoch: 126 | Iter: 383600 | Total Loss: 0.002443 | Recon Loss: 0.002030 | Commit Loss: 0.000827 | Perplexity: 970.962223
2025-09-14 21:07:42,586 Stage: Train 0.5 | Epoch: 126 | Iter: 383800 | Total Loss: 0.002407 | Recon Loss: 0.001999 | Commit Loss: 0.000815 | Perplexity: 970.175716
2025-09-14 21:07:51,318 Stage: Train 0.5 | Epoch: 126 | Iter: 384000 | Total Loss: 0.002382 | Recon Loss: 0.001971 | Commit Loss: 0.000821 | Perplexity: 978.325577
2025-09-14 21:08:00,060 Stage: Train 0.5 | Epoch: 126 | Iter: 384200 | Total Loss: 0.002427 | Recon Loss: 0.002020 | Commit Loss: 0.000814 | Perplexity: 974.154091
2025-09-14 21:08:08,784 Stage: Train 0.5 | Epoch: 126 | Iter: 384400 | Total Loss: 0.002401 | Recon Loss: 0.001994 | Commit Loss: 0.000813 | Perplexity: 975.944948
2025-09-14 21:08:17,528 Stage: Train 0.5 | Epoch: 126 | Iter: 384600 | Total Loss: 0.002419 | Recon Loss: 0.002005 | Commit Loss: 0.000827 | Perplexity: 976.349900
2025-09-14 21:08:26,284 Stage: Train 0.5 | Epoch: 126 | Iter: 384800 | Total Loss: 0.002400 | Recon Loss: 0.001990 | Commit Loss: 0.000819 | Perplexity: 972.428610
2025-09-14 21:08:35,025 Stage: Train 0.5 | Epoch: 126 | Iter: 385000 | Total Loss: 0.002403 | Recon Loss: 0.001994 | Commit Loss: 0.000820 | Perplexity: 971.368826
2025-09-14 21:08:43,756 Stage: Train 0.5 | Epoch: 126 | Iter: 385200 | Total Loss: 0.002414 | Recon Loss: 0.002009 | Commit Loss: 0.000810 | Perplexity: 971.017686
2025-09-14 21:08:52,481 Stage: Train 0.5 | Epoch: 126 | Iter: 385400 | Total Loss: 0.002405 | Recon Loss: 0.001996 | Commit Loss: 0.000817 | Perplexity: 975.746646
2025-09-14 21:09:01,247 Stage: Train 0.5 | Epoch: 126 | Iter: 385600 | Total Loss: 0.002399 | Recon Loss: 0.001993 | Commit Loss: 0.000812 | Perplexity: 976.772007
2025-09-14 21:09:09,945 Stage: Train 0.5 | Epoch: 126 | Iter: 385800 | Total Loss: 0.002429 | Recon Loss: 0.002015 | Commit Loss: 0.000829 | Perplexity: 974.593994
Trainning Epoch:  77%|███████▋  | 127/165 [4:41:04<1:24:02, 132.70s/it]2025-09-14 21:09:18,690 Stage: Train 0.5 | Epoch: 127 | Iter: 386000 | Total Loss: 0.002424 | Recon Loss: 0.002018 | Commit Loss: 0.000812 | Perplexity: 970.198449
2025-09-14 21:09:27,438 Stage: Train 0.5 | Epoch: 127 | Iter: 386200 | Total Loss: 0.002421 | Recon Loss: 0.002013 | Commit Loss: 0.000816 | Perplexity: 976.131641
2025-09-14 21:09:36,148 Stage: Train 0.5 | Epoch: 127 | Iter: 386400 | Total Loss: 0.002384 | Recon Loss: 0.001977 | Commit Loss: 0.000813 | Perplexity: 973.776457
2025-09-14 21:09:44,899 Stage: Train 0.5 | Epoch: 127 | Iter: 386600 | Total Loss: 0.002413 | Recon Loss: 0.002001 | Commit Loss: 0.000822 | Perplexity: 975.048434
2025-09-14 21:09:53,584 Stage: Train 0.5 | Epoch: 127 | Iter: 386800 | Total Loss: 0.002425 | Recon Loss: 0.002014 | Commit Loss: 0.000823 | Perplexity: 975.339040
2025-09-14 21:10:02,291 Stage: Train 0.5 | Epoch: 127 | Iter: 387000 | Total Loss: 0.002405 | Recon Loss: 0.001996 | Commit Loss: 0.000818 | Perplexity: 974.032560
2025-09-14 21:10:10,968 Stage: Train 0.5 | Epoch: 127 | Iter: 387200 | Total Loss: 0.002391 | Recon Loss: 0.001985 | Commit Loss: 0.000813 | Perplexity: 976.284955
2025-09-14 21:10:19,655 Stage: Train 0.5 | Epoch: 127 | Iter: 387400 | Total Loss: 0.002412 | Recon Loss: 0.002002 | Commit Loss: 0.000819 | Perplexity: 975.361399
2025-09-14 21:10:28,440 Stage: Train 0.5 | Epoch: 127 | Iter: 387600 | Total Loss: 0.002381 | Recon Loss: 0.001974 | Commit Loss: 0.000815 | Perplexity: 976.863861
2025-09-14 21:10:37,174 Stage: Train 0.5 | Epoch: 127 | Iter: 387800 | Total Loss: 0.002397 | Recon Loss: 0.001985 | Commit Loss: 0.000824 | Perplexity: 976.623218
2025-09-14 21:10:45,980 Stage: Train 0.5 | Epoch: 127 | Iter: 388000 | Total Loss: 0.002413 | Recon Loss: 0.002002 | Commit Loss: 0.000822 | Perplexity: 977.766142
2025-09-14 21:10:54,782 Stage: Train 0.5 | Epoch: 127 | Iter: 388200 | Total Loss: 0.002362 | Recon Loss: 0.001952 | Commit Loss: 0.000818 | Perplexity: 969.402963
2025-09-14 21:11:03,516 Stage: Train 0.5 | Epoch: 127 | Iter: 388400 | Total Loss: 0.002415 | Recon Loss: 0.002011 | Commit Loss: 0.000808 | Perplexity: 975.779184
2025-09-14 21:11:12,262 Stage: Train 0.5 | Epoch: 127 | Iter: 388600 | Total Loss: 0.002402 | Recon Loss: 0.001998 | Commit Loss: 0.000809 | Perplexity: 972.747041
2025-09-14 21:11:21,005 Stage: Train 0.5 | Epoch: 127 | Iter: 388800 | Total Loss: 0.002429 | Recon Loss: 0.002023 | Commit Loss: 0.000813 | Perplexity: 973.772690
Trainning Epoch:  78%|███████▊  | 128/165 [4:43:17<1:21:50, 132.71s/it]2025-09-14 21:11:29,750 Stage: Train 0.5 | Epoch: 128 | Iter: 389000 | Total Loss: 0.002423 | Recon Loss: 0.002012 | Commit Loss: 0.000822 | Perplexity: 978.540710
2025-09-14 21:11:38,466 Stage: Train 0.5 | Epoch: 128 | Iter: 389200 | Total Loss: 0.002416 | Recon Loss: 0.002010 | Commit Loss: 0.000810 | Perplexity: 974.133516
2025-09-14 21:11:47,211 Stage: Train 0.5 | Epoch: 128 | Iter: 389400 | Total Loss: 0.002342 | Recon Loss: 0.001938 | Commit Loss: 0.000806 | Perplexity: 970.036758
2025-09-14 21:11:55,958 Stage: Train 0.5 | Epoch: 128 | Iter: 389600 | Total Loss: 0.002412 | Recon Loss: 0.002006 | Commit Loss: 0.000812 | Perplexity: 971.269500
2025-09-14 21:12:04,655 Stage: Train 0.5 | Epoch: 128 | Iter: 389800 | Total Loss: 0.002404 | Recon Loss: 0.001997 | Commit Loss: 0.000813 | Perplexity: 976.690975
2025-09-14 21:12:13,402 Stage: Train 0.5 | Epoch: 128 | Iter: 390000 | Total Loss: 0.002391 | Recon Loss: 0.001985 | Commit Loss: 0.000813 | Perplexity: 972.115954
2025-09-14 21:12:22,128 Stage: Train 0.5 | Epoch: 128 | Iter: 390200 | Total Loss: 0.002407 | Recon Loss: 0.001998 | Commit Loss: 0.000818 | Perplexity: 974.497478
2025-09-14 21:12:30,887 Stage: Train 0.5 | Epoch: 128 | Iter: 390400 | Total Loss: 0.002444 | Recon Loss: 0.002032 | Commit Loss: 0.000824 | Perplexity: 976.581206
2025-09-14 21:12:39,601 Stage: Train 0.5 | Epoch: 128 | Iter: 390600 | Total Loss: 0.002363 | Recon Loss: 0.001954 | Commit Loss: 0.000817 | Perplexity: 977.644175
2025-09-14 21:12:48,345 Stage: Train 0.5 | Epoch: 128 | Iter: 390800 | Total Loss: 0.002408 | Recon Loss: 0.002001 | Commit Loss: 0.000814 | Perplexity: 975.549918
2025-09-14 21:12:57,060 Stage: Train 0.5 | Epoch: 128 | Iter: 391000 | Total Loss: 0.002371 | Recon Loss: 0.001967 | Commit Loss: 0.000808 | Perplexity: 976.365361
2025-09-14 21:13:05,840 Stage: Train 0.5 | Epoch: 128 | Iter: 391200 | Total Loss: 0.002408 | Recon Loss: 0.001996 | Commit Loss: 0.000824 | Perplexity: 977.674144
2025-09-14 21:13:14,572 Stage: Train 0.5 | Epoch: 128 | Iter: 391400 | Total Loss: 0.002402 | Recon Loss: 0.001992 | Commit Loss: 0.000820 | Perplexity: 976.962617
2025-09-14 21:13:23,292 Stage: Train 0.5 | Epoch: 128 | Iter: 391600 | Total Loss: 0.002435 | Recon Loss: 0.002026 | Commit Loss: 0.000820 | Perplexity: 975.438756
2025-09-14 21:13:32,056 Stage: Train 0.5 | Epoch: 128 | Iter: 391800 | Total Loss: 0.002392 | Recon Loss: 0.001983 | Commit Loss: 0.000818 | Perplexity: 977.970654
Trainning Epoch:  78%|███████▊  | 129/165 [4:45:30<1:19:37, 132.72s/it]2025-09-14 21:13:40,806 Stage: Train 0.5 | Epoch: 129 | Iter: 392000 | Total Loss: 0.002425 | Recon Loss: 0.002018 | Commit Loss: 0.000813 | Perplexity: 972.062004
2025-09-14 21:13:49,550 Stage: Train 0.5 | Epoch: 129 | Iter: 392200 | Total Loss: 0.002402 | Recon Loss: 0.001993 | Commit Loss: 0.000819 | Perplexity: 976.527497
2025-09-14 21:13:58,300 Stage: Train 0.5 | Epoch: 129 | Iter: 392400 | Total Loss: 0.002376 | Recon Loss: 0.001972 | Commit Loss: 0.000809 | Perplexity: 975.478080
2025-09-14 21:14:07,023 Stage: Train 0.5 | Epoch: 129 | Iter: 392600 | Total Loss: 0.002390 | Recon Loss: 0.001985 | Commit Loss: 0.000809 | Perplexity: 971.751168
2025-09-14 21:14:15,743 Stage: Train 0.5 | Epoch: 129 | Iter: 392800 | Total Loss: 0.002409 | Recon Loss: 0.002006 | Commit Loss: 0.000805 | Perplexity: 974.070821
2025-09-14 21:14:24,508 Stage: Train 0.5 | Epoch: 129 | Iter: 393000 | Total Loss: 0.002376 | Recon Loss: 0.001963 | Commit Loss: 0.000825 | Perplexity: 979.114289
2025-09-14 21:14:33,288 Stage: Train 0.5 | Epoch: 129 | Iter: 393200 | Total Loss: 0.002423 | Recon Loss: 0.002015 | Commit Loss: 0.000816 | Perplexity: 975.513600
2025-09-14 21:14:42,045 Stage: Train 0.5 | Epoch: 129 | Iter: 393400 | Total Loss: 0.002388 | Recon Loss: 0.001983 | Commit Loss: 0.000810 | Perplexity: 975.359174
2025-09-14 21:14:50,775 Stage: Train 0.5 | Epoch: 129 | Iter: 393600 | Total Loss: 0.002360 | Recon Loss: 0.001955 | Commit Loss: 0.000810 | Perplexity: 975.135378
2025-09-14 21:14:59,532 Stage: Train 0.5 | Epoch: 129 | Iter: 393800 | Total Loss: 0.002380 | Recon Loss: 0.001975 | Commit Loss: 0.000811 | Perplexity: 971.647218
2025-09-14 21:15:08,277 Stage: Train 0.5 | Epoch: 129 | Iter: 394000 | Total Loss: 0.002385 | Recon Loss: 0.001980 | Commit Loss: 0.000811 | Perplexity: 972.596662
2025-09-14 21:15:17,047 Stage: Train 0.5 | Epoch: 129 | Iter: 394200 | Total Loss: 0.002421 | Recon Loss: 0.002014 | Commit Loss: 0.000814 | Perplexity: 979.570436
2025-09-14 21:15:25,804 Stage: Train 0.5 | Epoch: 129 | Iter: 394400 | Total Loss: 0.002390 | Recon Loss: 0.001977 | Commit Loss: 0.000825 | Perplexity: 979.129021
2025-09-14 21:15:34,583 Stage: Train 0.5 | Epoch: 129 | Iter: 394600 | Total Loss: 0.002395 | Recon Loss: 0.001985 | Commit Loss: 0.000821 | Perplexity: 977.194777
2025-09-14 21:15:43,352 Stage: Train 0.5 | Epoch: 129 | Iter: 394800 | Total Loss: 0.002377 | Recon Loss: 0.001969 | Commit Loss: 0.000816 | Perplexity: 977.947649
Trainning Epoch:  79%|███████▉  | 130/165 [4:47:43<1:17:27, 132.79s/it]2025-09-14 21:15:52,138 Stage: Train 0.5 | Epoch: 130 | Iter: 395000 | Total Loss: 0.002374 | Recon Loss: 0.001965 | Commit Loss: 0.000817 | Perplexity: 980.976311
2025-09-14 21:16:00,861 Stage: Train 0.5 | Epoch: 130 | Iter: 395200 | Total Loss: 0.002399 | Recon Loss: 0.001992 | Commit Loss: 0.000815 | Perplexity: 981.720562
2025-09-14 21:16:09,614 Stage: Train 0.5 | Epoch: 130 | Iter: 395400 | Total Loss: 0.002384 | Recon Loss: 0.001978 | Commit Loss: 0.000812 | Perplexity: 977.889817
2025-09-14 21:16:18,376 Stage: Train 0.5 | Epoch: 130 | Iter: 395600 | Total Loss: 0.002384 | Recon Loss: 0.001979 | Commit Loss: 0.000810 | Perplexity: 976.456601
2025-09-14 21:16:27,104 Stage: Train 0.5 | Epoch: 130 | Iter: 395800 | Total Loss: 0.002377 | Recon Loss: 0.001969 | Commit Loss: 0.000815 | Perplexity: 978.213076
2025-09-14 21:16:35,852 Stage: Train 0.5 | Epoch: 130 | Iter: 396000 | Total Loss: 0.002409 | Recon Loss: 0.002004 | Commit Loss: 0.000810 | Perplexity: 975.420997
2025-09-14 21:16:44,595 Stage: Train 0.5 | Epoch: 130 | Iter: 396200 | Total Loss: 0.002447 | Recon Loss: 0.002039 | Commit Loss: 0.000816 | Perplexity: 972.265191
2025-09-14 21:16:53,344 Stage: Train 0.5 | Epoch: 130 | Iter: 396400 | Total Loss: 0.002352 | Recon Loss: 0.001946 | Commit Loss: 0.000812 | Perplexity: 977.416245
2025-09-14 21:17:02,142 Stage: Train 0.5 | Epoch: 130 | Iter: 396600 | Total Loss: 0.002408 | Recon Loss: 0.001997 | Commit Loss: 0.000822 | Perplexity: 979.216217
2025-09-14 21:17:10,885 Stage: Train 0.5 | Epoch: 130 | Iter: 396800 | Total Loss: 0.002351 | Recon Loss: 0.001949 | Commit Loss: 0.000803 | Perplexity: 972.663242
2025-09-14 21:17:19,594 Stage: Train 0.5 | Epoch: 130 | Iter: 397000 | Total Loss: 0.002422 | Recon Loss: 0.002017 | Commit Loss: 0.000811 | Perplexity: 977.452151
2025-09-14 21:17:28,342 Stage: Train 0.5 | Epoch: 130 | Iter: 397200 | Total Loss: 0.002397 | Recon Loss: 0.001992 | Commit Loss: 0.000810 | Perplexity: 975.668371
2025-09-14 21:17:37,057 Stage: Train 0.5 | Epoch: 130 | Iter: 397400 | Total Loss: 0.002382 | Recon Loss: 0.001978 | Commit Loss: 0.000808 | Perplexity: 975.535275
2025-09-14 21:17:45,800 Stage: Train 0.5 | Epoch: 130 | Iter: 397600 | Total Loss: 0.002384 | Recon Loss: 0.001975 | Commit Loss: 0.000819 | Perplexity: 979.391765
2025-09-14 21:17:54,496 Stage: Train 0.5 | Epoch: 130 | Iter: 397800 | Total Loss: 0.002424 | Recon Loss: 0.002017 | Commit Loss: 0.000814 | Perplexity: 975.086254
Trainning Epoch:  79%|███████▉  | 131/165 [4:49:55<1:15:14, 132.77s/it]2025-09-14 21:18:03,188 Stage: Train 0.5 | Epoch: 131 | Iter: 398000 | Total Loss: 0.002417 | Recon Loss: 0.002013 | Commit Loss: 0.000809 | Perplexity: 971.139622
2025-09-14 21:18:11,904 Stage: Train 0.5 | Epoch: 131 | Iter: 398200 | Total Loss: 0.002385 | Recon Loss: 0.001980 | Commit Loss: 0.000810 | Perplexity: 970.690815
2025-09-14 21:18:20,603 Stage: Train 0.5 | Epoch: 131 | Iter: 398400 | Total Loss: 0.002383 | Recon Loss: 0.001976 | Commit Loss: 0.000814 | Perplexity: 978.146464
2025-09-14 21:18:29,504 Stage: Train 0.5 | Epoch: 131 | Iter: 398600 | Total Loss: 0.002383 | Recon Loss: 0.001978 | Commit Loss: 0.000810 | Perplexity: 978.512654
2025-09-14 21:18:38,224 Stage: Train 0.5 | Epoch: 131 | Iter: 398800 | Total Loss: 0.002422 | Recon Loss: 0.002016 | Commit Loss: 0.000811 | Perplexity: 977.804364
2025-09-14 21:18:46,920 Stage: Train 0.5 | Epoch: 131 | Iter: 399000 | Total Loss: 0.002365 | Recon Loss: 0.001963 | Commit Loss: 0.000805 | Perplexity: 972.428593
2025-09-14 21:18:55,664 Stage: Train 0.5 | Epoch: 131 | Iter: 399200 | Total Loss: 0.002401 | Recon Loss: 0.001993 | Commit Loss: 0.000817 | Perplexity: 982.618651
2025-09-14 21:19:04,370 Stage: Train 0.5 | Epoch: 131 | Iter: 399400 | Total Loss: 0.002380 | Recon Loss: 0.001973 | Commit Loss: 0.000813 | Perplexity: 978.319395
2025-09-14 21:19:13,096 Stage: Train 0.5 | Epoch: 131 | Iter: 399600 | Total Loss: 0.002351 | Recon Loss: 0.001947 | Commit Loss: 0.000808 | Perplexity: 976.099519
2025-09-14 21:19:21,865 Stage: Train 0.5 | Epoch: 131 | Iter: 399800 | Total Loss: 0.002391 | Recon Loss: 0.001985 | Commit Loss: 0.000813 | Perplexity: 975.059039
2025-09-14 21:19:30,626 Stage: Train 0.5 | Epoch: 131 | Iter: 400000 | Total Loss: 0.002405 | Recon Loss: 0.002001 | Commit Loss: 0.000808 | Perplexity: 972.750267
2025-09-14 21:19:30,626 Saving model at iteration 400000
2025-09-14 21:19:31,097 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_132_step_400000
2025-09-14 21:19:31,328 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_132_step_400000/pytorch_model.bin
2025-09-14 21:19:31,702 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_132_step_400000/optimizer.bin
2025-09-14 21:19:31,702 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_132_step_400000/scheduler.bin
2025-09-14 21:19:31,703 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_132_step_400000/random_states_0.pkl
2025-09-14 21:19:40,836 Stage: Train 0.5 | Epoch: 131 | Iter: 400200 | Total Loss: 0.002353 | Recon Loss: 0.001946 | Commit Loss: 0.000813 | Perplexity: 978.526186
2025-09-14 21:19:49,566 Stage: Train 0.5 | Epoch: 131 | Iter: 400400 | Total Loss: 0.002425 | Recon Loss: 0.002016 | Commit Loss: 0.000819 | Perplexity: 974.995039
2025-09-14 21:19:58,337 Stage: Train 0.5 | Epoch: 131 | Iter: 400600 | Total Loss: 0.002369 | Recon Loss: 0.001967 | Commit Loss: 0.000803 | Perplexity: 973.391531
2025-09-14 21:20:07,113 Stage: Train 0.5 | Epoch: 131 | Iter: 400800 | Total Loss: 0.002423 | Recon Loss: 0.002018 | Commit Loss: 0.000810 | Perplexity: 975.555908
2025-09-14 21:20:15,883 Stage: Train 0.5 | Epoch: 131 | Iter: 401000 | Total Loss: 0.002425 | Recon Loss: 0.002018 | Commit Loss: 0.000813 | Perplexity: 974.744762
Trainning Epoch:  80%|████████  | 132/165 [4:52:10<1:13:17, 133.25s/it]2025-09-14 21:20:24,614 Stage: Train 0.5 | Epoch: 132 | Iter: 401200 | Total Loss: 0.002371 | Recon Loss: 0.001970 | Commit Loss: 0.000802 | Perplexity: 972.274406
2025-09-14 21:20:33,317 Stage: Train 0.5 | Epoch: 132 | Iter: 401400 | Total Loss: 0.002374 | Recon Loss: 0.001966 | Commit Loss: 0.000816 | Perplexity: 980.245811
2025-09-14 21:20:42,061 Stage: Train 0.5 | Epoch: 132 | Iter: 401600 | Total Loss: 0.002414 | Recon Loss: 0.002005 | Commit Loss: 0.000819 | Perplexity: 979.642031
2025-09-14 21:20:50,812 Stage: Train 0.5 | Epoch: 132 | Iter: 401800 | Total Loss: 0.002402 | Recon Loss: 0.001999 | Commit Loss: 0.000808 | Perplexity: 974.775042
2025-09-14 21:20:59,534 Stage: Train 0.5 | Epoch: 132 | Iter: 402000 | Total Loss: 0.002388 | Recon Loss: 0.001982 | Commit Loss: 0.000812 | Perplexity: 978.045656
2025-09-14 21:21:08,264 Stage: Train 0.5 | Epoch: 132 | Iter: 402200 | Total Loss: 0.002382 | Recon Loss: 0.001980 | Commit Loss: 0.000803 | Perplexity: 973.033700
2025-09-14 21:21:17,018 Stage: Train 0.5 | Epoch: 132 | Iter: 402400 | Total Loss: 0.002423 | Recon Loss: 0.002017 | Commit Loss: 0.000812 | Perplexity: 979.275380
2025-09-14 21:21:25,730 Stage: Train 0.5 | Epoch: 132 | Iter: 402600 | Total Loss: 0.002341 | Recon Loss: 0.001938 | Commit Loss: 0.000807 | Perplexity: 973.011601
2025-09-14 21:21:34,442 Stage: Train 0.5 | Epoch: 132 | Iter: 402800 | Total Loss: 0.002373 | Recon Loss: 0.001968 | Commit Loss: 0.000810 | Perplexity: 977.237157
2025-09-14 21:21:43,140 Stage: Train 0.5 | Epoch: 132 | Iter: 403000 | Total Loss: 0.002374 | Recon Loss: 0.001968 | Commit Loss: 0.000811 | Perplexity: 973.467923
2025-09-14 21:21:51,833 Stage: Train 0.5 | Epoch: 132 | Iter: 403200 | Total Loss: 0.002379 | Recon Loss: 0.001975 | Commit Loss: 0.000808 | Perplexity: 974.116727
2025-09-14 21:22:00,520 Stage: Train 0.5 | Epoch: 132 | Iter: 403400 | Total Loss: 0.002392 | Recon Loss: 0.001987 | Commit Loss: 0.000812 | Perplexity: 978.785367
2025-09-14 21:22:09,218 Stage: Train 0.5 | Epoch: 132 | Iter: 403600 | Total Loss: 0.002392 | Recon Loss: 0.001984 | Commit Loss: 0.000816 | Perplexity: 978.220219
2025-09-14 21:22:17,916 Stage: Train 0.5 | Epoch: 132 | Iter: 403800 | Total Loss: 0.002385 | Recon Loss: 0.001982 | Commit Loss: 0.000806 | Perplexity: 973.837376
2025-09-14 21:22:26,584 Stage: Train 0.5 | Epoch: 132 | Iter: 404000 | Total Loss: 0.002407 | Recon Loss: 0.002000 | Commit Loss: 0.000813 | Perplexity: 980.843402
Trainning Epoch:  81%|████████  | 133/165 [4:54:22<1:10:55, 132.98s/it]2025-09-14 21:22:35,316 Stage: Train 0.5 | Epoch: 133 | Iter: 404200 | Total Loss: 0.002392 | Recon Loss: 0.001988 | Commit Loss: 0.000809 | Perplexity: 979.700381
2025-09-14 21:22:44,033 Stage: Train 0.5 | Epoch: 133 | Iter: 404400 | Total Loss: 0.002386 | Recon Loss: 0.001982 | Commit Loss: 0.000809 | Perplexity: 979.735552
2025-09-14 21:22:52,756 Stage: Train 0.5 | Epoch: 133 | Iter: 404600 | Total Loss: 0.002399 | Recon Loss: 0.001996 | Commit Loss: 0.000805 | Perplexity: 980.509120
2025-09-14 21:23:01,461 Stage: Train 0.5 | Epoch: 133 | Iter: 404800 | Total Loss: 0.002398 | Recon Loss: 0.001996 | Commit Loss: 0.000804 | Perplexity: 975.231642
2025-09-14 21:23:10,170 Stage: Train 0.5 | Epoch: 133 | Iter: 405000 | Total Loss: 0.002358 | Recon Loss: 0.001956 | Commit Loss: 0.000806 | Perplexity: 976.714707
2025-09-14 21:23:18,894 Stage: Train 0.5 | Epoch: 133 | Iter: 405200 | Total Loss: 0.002403 | Recon Loss: 0.001996 | Commit Loss: 0.000813 | Perplexity: 979.973452
2025-09-14 21:23:27,635 Stage: Train 0.5 | Epoch: 133 | Iter: 405400 | Total Loss: 0.002366 | Recon Loss: 0.001961 | Commit Loss: 0.000810 | Perplexity: 978.140744
2025-09-14 21:23:36,390 Stage: Train 0.5 | Epoch: 133 | Iter: 405600 | Total Loss: 0.002370 | Recon Loss: 0.001968 | Commit Loss: 0.000803 | Perplexity: 976.893171
2025-09-14 21:23:45,139 Stage: Train 0.5 | Epoch: 133 | Iter: 405800 | Total Loss: 0.002368 | Recon Loss: 0.001966 | Commit Loss: 0.000804 | Perplexity: 976.096170
2025-09-14 21:23:53,911 Stage: Train 0.5 | Epoch: 133 | Iter: 406000 | Total Loss: 0.002364 | Recon Loss: 0.001959 | Commit Loss: 0.000810 | Perplexity: 978.903367
2025-09-14 21:24:02,639 Stage: Train 0.5 | Epoch: 133 | Iter: 406200 | Total Loss: 0.002385 | Recon Loss: 0.001984 | Commit Loss: 0.000804 | Perplexity: 976.224181
2025-09-14 21:24:11,384 Stage: Train 0.5 | Epoch: 133 | Iter: 406400 | Total Loss: 0.002381 | Recon Loss: 0.001977 | Commit Loss: 0.000809 | Perplexity: 977.506162
2025-09-14 21:24:20,134 Stage: Train 0.5 | Epoch: 133 | Iter: 406600 | Total Loss: 0.002385 | Recon Loss: 0.001980 | Commit Loss: 0.000811 | Perplexity: 975.360603
2025-09-14 21:24:28,829 Stage: Train 0.5 | Epoch: 133 | Iter: 406800 | Total Loss: 0.002365 | Recon Loss: 0.001959 | Commit Loss: 0.000812 | Perplexity: 980.401031
2025-09-14 21:24:37,548 Stage: Train 0.5 | Epoch: 133 | Iter: 407000 | Total Loss: 0.002345 | Recon Loss: 0.001944 | Commit Loss: 0.000802 | Perplexity: 978.195674
Trainning Epoch:  81%|████████  | 134/165 [4:56:35<1:08:39, 132.87s/it]2025-09-14 21:24:46,264 Stage: Train 0.5 | Epoch: 134 | Iter: 407200 | Total Loss: 0.002373 | Recon Loss: 0.001971 | Commit Loss: 0.000805 | Perplexity: 970.556186
2025-09-14 21:24:54,990 Stage: Train 0.5 | Epoch: 134 | Iter: 407400 | Total Loss: 0.002377 | Recon Loss: 0.001974 | Commit Loss: 0.000806 | Perplexity: 976.984236
2025-09-14 21:25:03,730 Stage: Train 0.5 | Epoch: 134 | Iter: 407600 | Total Loss: 0.002355 | Recon Loss: 0.001954 | Commit Loss: 0.000803 | Perplexity: 977.065485
2025-09-14 21:25:12,444 Stage: Train 0.5 | Epoch: 134 | Iter: 407800 | Total Loss: 0.002393 | Recon Loss: 0.001990 | Commit Loss: 0.000807 | Perplexity: 977.682974
2025-09-14 21:25:21,144 Stage: Train 0.5 | Epoch: 134 | Iter: 408000 | Total Loss: 0.002378 | Recon Loss: 0.001976 | Commit Loss: 0.000806 | Perplexity: 983.497547
2025-09-14 21:25:29,877 Stage: Train 0.5 | Epoch: 134 | Iter: 408200 | Total Loss: 0.002379 | Recon Loss: 0.001971 | Commit Loss: 0.000815 | Perplexity: 981.694872
2025-09-14 21:25:38,576 Stage: Train 0.5 | Epoch: 134 | Iter: 408400 | Total Loss: 0.002378 | Recon Loss: 0.001974 | Commit Loss: 0.000807 | Perplexity: 977.784097
2025-09-14 21:25:47,297 Stage: Train 0.5 | Epoch: 134 | Iter: 408600 | Total Loss: 0.002363 | Recon Loss: 0.001960 | Commit Loss: 0.000805 | Perplexity: 981.320229
2025-09-14 21:25:56,026 Stage: Train 0.5 | Epoch: 134 | Iter: 408800 | Total Loss: 0.002405 | Recon Loss: 0.002001 | Commit Loss: 0.000809 | Perplexity: 979.830607
2025-09-14 21:26:04,760 Stage: Train 0.5 | Epoch: 134 | Iter: 409000 | Total Loss: 0.002374 | Recon Loss: 0.001974 | Commit Loss: 0.000800 | Perplexity: 973.927369
2025-09-14 21:26:13,494 Stage: Train 0.5 | Epoch: 134 | Iter: 409200 | Total Loss: 0.002352 | Recon Loss: 0.001949 | Commit Loss: 0.000806 | Perplexity: 980.999087
2025-09-14 21:26:22,241 Stage: Train 0.5 | Epoch: 134 | Iter: 409400 | Total Loss: 0.002346 | Recon Loss: 0.001947 | Commit Loss: 0.000797 | Perplexity: 973.881838
2025-09-14 21:26:30,940 Stage: Train 0.5 | Epoch: 134 | Iter: 409600 | Total Loss: 0.002400 | Recon Loss: 0.001995 | Commit Loss: 0.000809 | Perplexity: 981.367410
2025-09-14 21:26:39,657 Stage: Train 0.5 | Epoch: 134 | Iter: 409800 | Total Loss: 0.002357 | Recon Loss: 0.001956 | Commit Loss: 0.000801 | Perplexity: 980.075368
2025-09-14 21:26:48,374 Stage: Train 0.5 | Epoch: 134 | Iter: 410000 | Total Loss: 0.002359 | Recon Loss: 0.001953 | Commit Loss: 0.000813 | Perplexity: 981.978848
Trainning Epoch:  82%|████████▏ | 135/165 [4:58:47<1:06:22, 132.75s/it]2025-09-14 21:26:57,078 Stage: Train 0.5 | Epoch: 135 | Iter: 410200 | Total Loss: 0.002389 | Recon Loss: 0.001985 | Commit Loss: 0.000807 | Perplexity: 978.881985
2025-09-14 21:27:05,829 Stage: Train 0.5 | Epoch: 135 | Iter: 410400 | Total Loss: 0.002310 | Recon Loss: 0.001909 | Commit Loss: 0.000803 | Perplexity: 979.452325
2025-09-14 21:27:14,530 Stage: Train 0.5 | Epoch: 135 | Iter: 410600 | Total Loss: 0.002390 | Recon Loss: 0.001983 | Commit Loss: 0.000812 | Perplexity: 979.513603
2025-09-14 21:27:23,246 Stage: Train 0.5 | Epoch: 135 | Iter: 410800 | Total Loss: 0.002369 | Recon Loss: 0.001972 | Commit Loss: 0.000794 | Perplexity: 973.939642
2025-09-14 21:27:31,998 Stage: Train 0.5 | Epoch: 135 | Iter: 411000 | Total Loss: 0.002362 | Recon Loss: 0.001958 | Commit Loss: 0.000807 | Perplexity: 977.421315
2025-09-14 21:27:40,741 Stage: Train 0.5 | Epoch: 135 | Iter: 411200 | Total Loss: 0.002351 | Recon Loss: 0.001955 | Commit Loss: 0.000793 | Perplexity: 976.244662
2025-09-14 21:27:49,480 Stage: Train 0.5 | Epoch: 135 | Iter: 411400 | Total Loss: 0.002367 | Recon Loss: 0.001968 | Commit Loss: 0.000798 | Perplexity: 976.335539
2025-09-14 21:27:58,220 Stage: Train 0.5 | Epoch: 135 | Iter: 411600 | Total Loss: 0.002364 | Recon Loss: 0.001959 | Commit Loss: 0.000809 | Perplexity: 979.223376
2025-09-14 21:28:06,986 Stage: Train 0.5 | Epoch: 135 | Iter: 411800 | Total Loss: 0.002385 | Recon Loss: 0.001986 | Commit Loss: 0.000798 | Perplexity: 978.810577
2025-09-14 21:28:15,731 Stage: Train 0.5 | Epoch: 135 | Iter: 412000 | Total Loss: 0.002392 | Recon Loss: 0.001992 | Commit Loss: 0.000801 | Perplexity: 975.822548
2025-09-14 21:28:24,504 Stage: Train 0.5 | Epoch: 135 | Iter: 412200 | Total Loss: 0.002356 | Recon Loss: 0.001957 | Commit Loss: 0.000799 | Perplexity: 978.299805
2025-09-14 21:28:33,243 Stage: Train 0.5 | Epoch: 135 | Iter: 412400 | Total Loss: 0.002359 | Recon Loss: 0.001957 | Commit Loss: 0.000804 | Perplexity: 978.181643
2025-09-14 21:28:42,000 Stage: Train 0.5 | Epoch: 135 | Iter: 412600 | Total Loss: 0.002338 | Recon Loss: 0.001932 | Commit Loss: 0.000812 | Perplexity: 980.817526
2025-09-14 21:28:50,748 Stage: Train 0.5 | Epoch: 135 | Iter: 412800 | Total Loss: 0.002331 | Recon Loss: 0.001930 | Commit Loss: 0.000802 | Perplexity: 980.295461
2025-09-14 21:28:59,514 Stage: Train 0.5 | Epoch: 135 | Iter: 413000 | Total Loss: 0.002389 | Recon Loss: 0.001986 | Commit Loss: 0.000806 | Perplexity: 984.508373
Trainning Epoch:  82%|████████▏ | 136/165 [5:01:00<1:04:10, 132.78s/it]2025-09-14 21:29:08,259 Stage: Train 0.5 | Epoch: 136 | Iter: 413200 | Total Loss: 0.002398 | Recon Loss: 0.002000 | Commit Loss: 0.000797 | Perplexity: 973.625525
2025-09-14 21:29:16,997 Stage: Train 0.5 | Epoch: 136 | Iter: 413400 | Total Loss: 0.002348 | Recon Loss: 0.001949 | Commit Loss: 0.000797 | Perplexity: 976.971992
2025-09-14 21:29:25,758 Stage: Train 0.5 | Epoch: 136 | Iter: 413600 | Total Loss: 0.002349 | Recon Loss: 0.001949 | Commit Loss: 0.000799 | Perplexity: 981.530191
2025-09-14 21:29:34,520 Stage: Train 0.5 | Epoch: 136 | Iter: 413800 | Total Loss: 0.002359 | Recon Loss: 0.001961 | Commit Loss: 0.000798 | Perplexity: 980.819823
2025-09-14 21:29:43,269 Stage: Train 0.5 | Epoch: 136 | Iter: 414000 | Total Loss: 0.002359 | Recon Loss: 0.001957 | Commit Loss: 0.000804 | Perplexity: 982.355554
2025-09-14 21:29:51,992 Stage: Train 0.5 | Epoch: 136 | Iter: 414200 | Total Loss: 0.002372 | Recon Loss: 0.001973 | Commit Loss: 0.000798 | Perplexity: 983.201876
2025-09-14 21:30:00,735 Stage: Train 0.5 | Epoch: 136 | Iter: 414400 | Total Loss: 0.002303 | Recon Loss: 0.001907 | Commit Loss: 0.000793 | Perplexity: 977.948972
2025-09-14 21:30:09,498 Stage: Train 0.5 | Epoch: 136 | Iter: 414600 | Total Loss: 0.002368 | Recon Loss: 0.001965 | Commit Loss: 0.000806 | Perplexity: 979.476513
2025-09-14 21:30:18,241 Stage: Train 0.5 | Epoch: 136 | Iter: 414800 | Total Loss: 0.002347 | Recon Loss: 0.001948 | Commit Loss: 0.000798 | Perplexity: 979.307644
2025-09-14 21:30:27,009 Stage: Train 0.5 | Epoch: 136 | Iter: 415000 | Total Loss: 0.002374 | Recon Loss: 0.001970 | Commit Loss: 0.000807 | Perplexity: 982.144569
2025-09-14 21:30:35,750 Stage: Train 0.5 | Epoch: 136 | Iter: 415200 | Total Loss: 0.002365 | Recon Loss: 0.001967 | Commit Loss: 0.000795 | Perplexity: 977.706906
2025-09-14 21:30:44,509 Stage: Train 0.5 | Epoch: 136 | Iter: 415400 | Total Loss: 0.002340 | Recon Loss: 0.001938 | Commit Loss: 0.000803 | Perplexity: 983.870929
2025-09-14 21:30:53,254 Stage: Train 0.5 | Epoch: 136 | Iter: 415600 | Total Loss: 0.002367 | Recon Loss: 0.001965 | Commit Loss: 0.000804 | Perplexity: 976.614635
2025-09-14 21:31:02,004 Stage: Train 0.5 | Epoch: 136 | Iter: 415800 | Total Loss: 0.002378 | Recon Loss: 0.001973 | Commit Loss: 0.000810 | Perplexity: 982.040799
2025-09-14 21:31:10,766 Stage: Train 0.5 | Epoch: 136 | Iter: 416000 | Total Loss: 0.002380 | Recon Loss: 0.001977 | Commit Loss: 0.000807 | Perplexity: 979.501567
2025-09-14 21:31:19,510 Stage: Train 0.5 | Epoch: 136 | Iter: 416200 | Total Loss: 0.002331 | Recon Loss: 0.001931 | Commit Loss: 0.000800 | Perplexity: 977.760275
Trainning Epoch:  83%|████████▎ | 137/165 [5:03:13<1:01:58, 132.82s/it]2025-09-14 21:31:28,254 Stage: Train 0.5 | Epoch: 137 | Iter: 416400 | Total Loss: 0.002367 | Recon Loss: 0.001967 | Commit Loss: 0.000800 | Perplexity: 974.464009
2025-09-14 21:31:36,989 Stage: Train 0.5 | Epoch: 137 | Iter: 416600 | Total Loss: 0.002344 | Recon Loss: 0.001943 | Commit Loss: 0.000801 | Perplexity: 980.044857
2025-09-14 21:31:45,724 Stage: Train 0.5 | Epoch: 137 | Iter: 416800 | Total Loss: 0.002408 | Recon Loss: 0.002010 | Commit Loss: 0.000797 | Perplexity: 979.843906
2025-09-14 21:31:54,497 Stage: Train 0.5 | Epoch: 137 | Iter: 417000 | Total Loss: 0.002332 | Recon Loss: 0.001937 | Commit Loss: 0.000791 | Perplexity: 975.338514
2025-09-14 21:32:03,234 Stage: Train 0.5 | Epoch: 137 | Iter: 417200 | Total Loss: 0.002367 | Recon Loss: 0.001968 | Commit Loss: 0.000798 | Perplexity: 980.425396
2025-09-14 21:32:11,959 Stage: Train 0.5 | Epoch: 137 | Iter: 417400 | Total Loss: 0.002352 | Recon Loss: 0.001952 | Commit Loss: 0.000799 | Perplexity: 981.726235
2025-09-14 21:32:20,676 Stage: Train 0.5 | Epoch: 137 | Iter: 417600 | Total Loss: 0.002353 | Recon Loss: 0.001953 | Commit Loss: 0.000800 | Perplexity: 979.277454
2025-09-14 21:32:29,390 Stage: Train 0.5 | Epoch: 137 | Iter: 417800 | Total Loss: 0.002370 | Recon Loss: 0.001971 | Commit Loss: 0.000800 | Perplexity: 977.040583
2025-09-14 21:32:38,114 Stage: Train 0.5 | Epoch: 137 | Iter: 418000 | Total Loss: 0.002352 | Recon Loss: 0.001955 | Commit Loss: 0.000794 | Perplexity: 976.209706
2025-09-14 21:32:46,837 Stage: Train 0.5 | Epoch: 137 | Iter: 418200 | Total Loss: 0.002378 | Recon Loss: 0.001980 | Commit Loss: 0.000796 | Perplexity: 976.743701
2025-09-14 21:32:55,587 Stage: Train 0.5 | Epoch: 137 | Iter: 418400 | Total Loss: 0.002419 | Recon Loss: 0.002021 | Commit Loss: 0.000796 | Perplexity: 979.385593
2025-09-14 21:33:04,288 Stage: Train 0.5 | Epoch: 137 | Iter: 418600 | Total Loss: 0.002369 | Recon Loss: 0.001968 | Commit Loss: 0.000801 | Perplexity: 984.411059
2025-09-14 21:33:13,024 Stage: Train 0.5 | Epoch: 137 | Iter: 418800 | Total Loss: 0.002356 | Recon Loss: 0.001958 | Commit Loss: 0.000796 | Perplexity: 979.145114
2025-09-14 21:33:21,738 Stage: Train 0.5 | Epoch: 137 | Iter: 419000 | Total Loss: 0.002389 | Recon Loss: 0.001991 | Commit Loss: 0.000796 | Perplexity: 977.507227
2025-09-14 21:33:30,462 Stage: Train 0.5 | Epoch: 137 | Iter: 419200 | Total Loss: 0.002364 | Recon Loss: 0.001966 | Commit Loss: 0.000797 | Perplexity: 981.747652
Trainning Epoch:  84%|████████▎ | 138/165 [5:05:25<59:44, 132.75s/it]  2025-09-14 21:33:39,202 Stage: Train 0.5 | Epoch: 138 | Iter: 419400 | Total Loss: 0.002354 | Recon Loss: 0.001953 | Commit Loss: 0.000802 | Perplexity: 979.125126
2025-09-14 21:33:47,941 Stage: Train 0.5 | Epoch: 138 | Iter: 419600 | Total Loss: 0.002344 | Recon Loss: 0.001945 | Commit Loss: 0.000798 | Perplexity: 979.383449
2025-09-14 21:33:56,624 Stage: Train 0.5 | Epoch: 138 | Iter: 419800 | Total Loss: 0.002334 | Recon Loss: 0.001932 | Commit Loss: 0.000804 | Perplexity: 982.582993
2025-09-14 21:34:05,329 Stage: Train 0.5 | Epoch: 138 | Iter: 420000 | Total Loss: 0.002328 | Recon Loss: 0.001932 | Commit Loss: 0.000793 | Perplexity: 980.765443
2025-09-14 21:34:05,329 Saving model at iteration 420000
2025-09-14 21:34:05,829 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_139_step_420000
2025-09-14 21:34:06,067 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_139_step_420000/pytorch_model.bin
2025-09-14 21:34:06,432 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_139_step_420000/optimizer.bin
2025-09-14 21:34:06,432 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_139_step_420000/scheduler.bin
2025-09-14 21:34:06,433 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_139_step_420000/random_states_0.pkl
2025-09-14 21:34:15,578 Stage: Train 0.5 | Epoch: 138 | Iter: 420200 | Total Loss: 0.002374 | Recon Loss: 0.001970 | Commit Loss: 0.000807 | Perplexity: 983.933283
2025-09-14 21:34:24,302 Stage: Train 0.5 | Epoch: 138 | Iter: 420400 | Total Loss: 0.002295 | Recon Loss: 0.001901 | Commit Loss: 0.000787 | Perplexity: 973.950587
2025-09-14 21:34:33,053 Stage: Train 0.5 | Epoch: 138 | Iter: 420600 | Total Loss: 0.002346 | Recon Loss: 0.001946 | Commit Loss: 0.000799 | Perplexity: 976.889753
2025-09-14 21:34:41,786 Stage: Train 0.5 | Epoch: 138 | Iter: 420800 | Total Loss: 0.002402 | Recon Loss: 0.002003 | Commit Loss: 0.000796 | Perplexity: 980.338146
2025-09-14 21:34:50,526 Stage: Train 0.5 | Epoch: 138 | Iter: 421000 | Total Loss: 0.002318 | Recon Loss: 0.001917 | Commit Loss: 0.000801 | Perplexity: 982.832389
2025-09-14 21:34:59,277 Stage: Train 0.5 | Epoch: 138 | Iter: 421200 | Total Loss: 0.002377 | Recon Loss: 0.001978 | Commit Loss: 0.000799 | Perplexity: 976.962634
2025-09-14 21:35:08,019 Stage: Train 0.5 | Epoch: 138 | Iter: 421400 | Total Loss: 0.002341 | Recon Loss: 0.001944 | Commit Loss: 0.000793 | Perplexity: 976.979298
2025-09-14 21:35:16,740 Stage: Train 0.5 | Epoch: 138 | Iter: 421600 | Total Loss: 0.002353 | Recon Loss: 0.001955 | Commit Loss: 0.000796 | Perplexity: 976.145397
2025-09-14 21:35:25,463 Stage: Train 0.5 | Epoch: 138 | Iter: 421800 | Total Loss: 0.002365 | Recon Loss: 0.001962 | Commit Loss: 0.000806 | Perplexity: 981.128993
2025-09-14 21:35:34,175 Stage: Train 0.5 | Epoch: 138 | Iter: 422000 | Total Loss: 0.002350 | Recon Loss: 0.001950 | Commit Loss: 0.000801 | Perplexity: 984.980577
2025-09-14 21:35:42,904 Stage: Train 0.5 | Epoch: 138 | Iter: 422200 | Total Loss: 0.002323 | Recon Loss: 0.001928 | Commit Loss: 0.000792 | Perplexity: 979.283983
Trainning Epoch:  84%|████████▍ | 139/165 [5:07:40<57:44, 133.24s/it]2025-09-14 21:35:51,920 Stage: Train 0.5 | Epoch: 139 | Iter: 422400 | Total Loss: 0.002354 | Recon Loss: 0.001960 | Commit Loss: 0.000790 | Perplexity: 974.301626
2025-09-14 21:36:00,660 Stage: Train 0.5 | Epoch: 139 | Iter: 422600 | Total Loss: 0.002362 | Recon Loss: 0.001961 | Commit Loss: 0.000801 | Perplexity: 986.308428
2025-09-14 21:36:09,411 Stage: Train 0.5 | Epoch: 139 | Iter: 422800 | Total Loss: 0.002360 | Recon Loss: 0.001957 | Commit Loss: 0.000806 | Perplexity: 983.535562
2025-09-14 21:36:18,146 Stage: Train 0.5 | Epoch: 139 | Iter: 423000 | Total Loss: 0.002333 | Recon Loss: 0.001936 | Commit Loss: 0.000793 | Perplexity: 977.223025
2025-09-14 21:36:26,891 Stage: Train 0.5 | Epoch: 139 | Iter: 423200 | Total Loss: 0.002349 | Recon Loss: 0.001952 | Commit Loss: 0.000796 | Perplexity: 982.502870
2025-09-14 21:36:35,615 Stage: Train 0.5 | Epoch: 139 | Iter: 423400 | Total Loss: 0.002332 | Recon Loss: 0.001938 | Commit Loss: 0.000788 | Perplexity: 977.982827
2025-09-14 21:36:44,361 Stage: Train 0.5 | Epoch: 139 | Iter: 423600 | Total Loss: 0.002332 | Recon Loss: 0.001937 | Commit Loss: 0.000791 | Perplexity: 981.556755
2025-09-14 21:36:53,060 Stage: Train 0.5 | Epoch: 139 | Iter: 423800 | Total Loss: 0.002336 | Recon Loss: 0.001934 | Commit Loss: 0.000803 | Perplexity: 982.434345
2025-09-14 21:37:01,769 Stage: Train 0.5 | Epoch: 139 | Iter: 424000 | Total Loss: 0.002353 | Recon Loss: 0.001950 | Commit Loss: 0.000805 | Perplexity: 980.400969
2025-09-14 21:37:10,468 Stage: Train 0.5 | Epoch: 139 | Iter: 424200 | Total Loss: 0.002338 | Recon Loss: 0.001940 | Commit Loss: 0.000797 | Perplexity: 978.613822
2025-09-14 21:37:19,172 Stage: Train 0.5 | Epoch: 139 | Iter: 424400 | Total Loss: 0.002397 | Recon Loss: 0.001996 | Commit Loss: 0.000801 | Perplexity: 988.088127
2025-09-14 21:37:27,909 Stage: Train 0.5 | Epoch: 139 | Iter: 424600 | Total Loss: 0.002337 | Recon Loss: 0.001938 | Commit Loss: 0.000798 | Perplexity: 982.841780
2025-09-14 21:37:36,665 Stage: Train 0.5 | Epoch: 139 | Iter: 424800 | Total Loss: 0.002310 | Recon Loss: 0.001913 | Commit Loss: 0.000794 | Perplexity: 979.859966
2025-09-14 21:37:45,401 Stage: Train 0.5 | Epoch: 139 | Iter: 425000 | Total Loss: 0.002365 | Recon Loss: 0.001968 | Commit Loss: 0.000793 | Perplexity: 973.310918
2025-09-14 21:37:54,116 Stage: Train 0.5 | Epoch: 139 | Iter: 425200 | Total Loss: 0.002328 | Recon Loss: 0.001929 | Commit Loss: 0.000797 | Perplexity: 982.828938
Trainning Epoch:  85%|████████▍ | 140/165 [5:09:52<55:26, 133.05s/it]2025-09-14 21:38:02,844 Stage: Train 0.5 | Epoch: 140 | Iter: 425400 | Total Loss: 0.002337 | Recon Loss: 0.001946 | Commit Loss: 0.000782 | Perplexity: 975.861676
2025-09-14 21:38:11,545 Stage: Train 0.5 | Epoch: 140 | Iter: 425600 | Total Loss: 0.002333 | Recon Loss: 0.001936 | Commit Loss: 0.000793 | Perplexity: 978.180322
2025-09-14 21:38:20,298 Stage: Train 0.5 | Epoch: 140 | Iter: 425800 | Total Loss: 0.002347 | Recon Loss: 0.001958 | Commit Loss: 0.000779 | Perplexity: 976.662951
2025-09-14 21:38:29,012 Stage: Train 0.5 | Epoch: 140 | Iter: 426000 | Total Loss: 0.002283 | Recon Loss: 0.001888 | Commit Loss: 0.000791 | Perplexity: 981.044645
2025-09-14 21:38:37,713 Stage: Train 0.5 | Epoch: 140 | Iter: 426200 | Total Loss: 0.002344 | Recon Loss: 0.001945 | Commit Loss: 0.000800 | Perplexity: 983.259218
2025-09-14 21:38:46,398 Stage: Train 0.5 | Epoch: 140 | Iter: 426400 | Total Loss: 0.002325 | Recon Loss: 0.001930 | Commit Loss: 0.000790 | Perplexity: 980.325833
2025-09-14 21:38:55,106 Stage: Train 0.5 | Epoch: 140 | Iter: 426600 | Total Loss: 0.002367 | Recon Loss: 0.001971 | Commit Loss: 0.000794 | Perplexity: 979.765804
2025-09-14 21:39:03,820 Stage: Train 0.5 | Epoch: 140 | Iter: 426800 | Total Loss: 0.002326 | Recon Loss: 0.001929 | Commit Loss: 0.000794 | Perplexity: 980.443540
2025-09-14 21:39:12,536 Stage: Train 0.5 | Epoch: 140 | Iter: 427000 | Total Loss: 0.002368 | Recon Loss: 0.001968 | Commit Loss: 0.000800 | Perplexity: 983.689148
2025-09-14 21:39:21,253 Stage: Train 0.5 | Epoch: 140 | Iter: 427200 | Total Loss: 0.002337 | Recon Loss: 0.001940 | Commit Loss: 0.000795 | Perplexity: 981.450636
2025-09-14 21:39:29,993 Stage: Train 0.5 | Epoch: 140 | Iter: 427400 | Total Loss: 0.002356 | Recon Loss: 0.001955 | Commit Loss: 0.000801 | Perplexity: 985.571546
2025-09-14 21:39:38,734 Stage: Train 0.5 | Epoch: 140 | Iter: 427600 | Total Loss: 0.002437 | Recon Loss: 0.002040 | Commit Loss: 0.000793 | Perplexity: 979.090111
2025-09-14 21:39:47,430 Stage: Train 0.5 | Epoch: 140 | Iter: 427800 | Total Loss: 0.002311 | Recon Loss: 0.001915 | Commit Loss: 0.000793 | Perplexity: 980.437216
2025-09-14 21:39:56,169 Stage: Train 0.5 | Epoch: 140 | Iter: 428000 | Total Loss: 0.002354 | Recon Loss: 0.001960 | Commit Loss: 0.000789 | Perplexity: 981.240716
2025-09-14 21:40:04,864 Stage: Train 0.5 | Epoch: 140 | Iter: 428200 | Total Loss: 0.002342 | Recon Loss: 0.001947 | Commit Loss: 0.000790 | Perplexity: 978.584535
Trainning Epoch:  85%|████████▌ | 141/165 [5:12:05<53:08, 132.86s/it]2025-09-14 21:40:13,628 Stage: Train 0.5 | Epoch: 141 | Iter: 428400 | Total Loss: 0.002336 | Recon Loss: 0.001936 | Commit Loss: 0.000800 | Perplexity: 980.184885
2025-09-14 21:40:22,341 Stage: Train 0.5 | Epoch: 141 | Iter: 428600 | Total Loss: 0.002315 | Recon Loss: 0.001919 | Commit Loss: 0.000791 | Perplexity: 981.266625
2025-09-14 21:40:31,072 Stage: Train 0.5 | Epoch: 141 | Iter: 428800 | Total Loss: 0.002358 | Recon Loss: 0.001966 | Commit Loss: 0.000786 | Perplexity: 973.747431
2025-09-14 21:40:39,799 Stage: Train 0.5 | Epoch: 141 | Iter: 429000 | Total Loss: 0.002358 | Recon Loss: 0.001956 | Commit Loss: 0.000803 | Perplexity: 982.562005
2025-09-14 21:40:48,524 Stage: Train 0.5 | Epoch: 141 | Iter: 429200 | Total Loss: 0.002341 | Recon Loss: 0.001947 | Commit Loss: 0.000788 | Perplexity: 980.318824
2025-09-14 21:40:57,222 Stage: Train 0.5 | Epoch: 141 | Iter: 429400 | Total Loss: 0.002377 | Recon Loss: 0.001977 | Commit Loss: 0.000799 | Perplexity: 982.204911
2025-09-14 21:41:05,948 Stage: Train 0.5 | Epoch: 141 | Iter: 429600 | Total Loss: 0.002344 | Recon Loss: 0.001954 | Commit Loss: 0.000781 | Perplexity: 971.335768
2025-09-14 21:41:14,673 Stage: Train 0.5 | Epoch: 141 | Iter: 429800 | Total Loss: 0.002351 | Recon Loss: 0.001949 | Commit Loss: 0.000803 | Perplexity: 985.866273
2025-09-14 21:41:23,408 Stage: Train 0.5 | Epoch: 141 | Iter: 430000 | Total Loss: 0.002322 | Recon Loss: 0.001928 | Commit Loss: 0.000787 | Perplexity: 981.997610
2025-09-14 21:41:32,150 Stage: Train 0.5 | Epoch: 141 | Iter: 430200 | Total Loss: 0.002330 | Recon Loss: 0.001934 | Commit Loss: 0.000793 | Perplexity: 979.547577
2025-09-14 21:41:40,872 Stage: Train 0.5 | Epoch: 141 | Iter: 430400 | Total Loss: 0.002338 | Recon Loss: 0.001942 | Commit Loss: 0.000792 | Perplexity: 983.230148
2025-09-14 21:41:49,593 Stage: Train 0.5 | Epoch: 141 | Iter: 430600 | Total Loss: 0.002324 | Recon Loss: 0.001931 | Commit Loss: 0.000786 | Perplexity: 978.894383
2025-09-14 21:41:58,375 Stage: Train 0.5 | Epoch: 141 | Iter: 430800 | Total Loss: 0.002351 | Recon Loss: 0.001952 | Commit Loss: 0.000799 | Perplexity: 981.650956
2025-09-14 21:42:07,148 Stage: Train 0.5 | Epoch: 141 | Iter: 431000 | Total Loss: 0.002332 | Recon Loss: 0.001937 | Commit Loss: 0.000789 | Perplexity: 981.238079
2025-09-14 21:42:15,960 Stage: Train 0.5 | Epoch: 141 | Iter: 431200 | Total Loss: 0.002290 | Recon Loss: 0.001896 | Commit Loss: 0.000789 | Perplexity: 980.244242
Trainning Epoch:  86%|████████▌ | 142/165 [5:14:18<50:55, 132.84s/it]2025-09-14 21:42:24,743 Stage: Train 0.5 | Epoch: 142 | Iter: 431400 | Total Loss: 0.002330 | Recon Loss: 0.001936 | Commit Loss: 0.000788 | Perplexity: 978.132303
2025-09-14 21:42:33,537 Stage: Train 0.5 | Epoch: 142 | Iter: 431600 | Total Loss: 0.002303 | Recon Loss: 0.001907 | Commit Loss: 0.000791 | Perplexity: 980.730610
2025-09-14 21:42:42,297 Stage: Train 0.5 | Epoch: 142 | Iter: 431800 | Total Loss: 0.002391 | Recon Loss: 0.001995 | Commit Loss: 0.000791 | Perplexity: 980.957587
2025-09-14 21:42:51,087 Stage: Train 0.5 | Epoch: 142 | Iter: 432000 | Total Loss: 0.002372 | Recon Loss: 0.001975 | Commit Loss: 0.000794 | Perplexity: 981.728580
2025-09-14 21:42:59,863 Stage: Train 0.5 | Epoch: 142 | Iter: 432200 | Total Loss: 0.002281 | Recon Loss: 0.001885 | Commit Loss: 0.000792 | Perplexity: 974.909869
2025-09-14 21:43:08,625 Stage: Train 0.5 | Epoch: 142 | Iter: 432400 | Total Loss: 0.002333 | Recon Loss: 0.001939 | Commit Loss: 0.000788 | Perplexity: 979.766605
2025-09-14 21:43:17,412 Stage: Train 0.5 | Epoch: 142 | Iter: 432600 | Total Loss: 0.002375 | Recon Loss: 0.001979 | Commit Loss: 0.000791 | Perplexity: 984.593219
2025-09-14 21:43:26,171 Stage: Train 0.5 | Epoch: 142 | Iter: 432800 | Total Loss: 0.002306 | Recon Loss: 0.001907 | Commit Loss: 0.000799 | Perplexity: 984.527795
2025-09-14 21:43:34,952 Stage: Train 0.5 | Epoch: 142 | Iter: 433000 | Total Loss: 0.002354 | Recon Loss: 0.001960 | Commit Loss: 0.000788 | Perplexity: 978.871523
2025-09-14 21:43:43,736 Stage: Train 0.5 | Epoch: 142 | Iter: 433200 | Total Loss: 0.002324 | Recon Loss: 0.001928 | Commit Loss: 0.000792 | Perplexity: 978.900682
2025-09-14 21:43:52,523 Stage: Train 0.5 | Epoch: 142 | Iter: 433400 | Total Loss: 0.002327 | Recon Loss: 0.001936 | Commit Loss: 0.000781 | Perplexity: 980.662866
2025-09-14 21:44:01,313 Stage: Train 0.5 | Epoch: 142 | Iter: 433600 | Total Loss: 0.002352 | Recon Loss: 0.001960 | Commit Loss: 0.000786 | Perplexity: 980.023996
2025-09-14 21:44:10,080 Stage: Train 0.5 | Epoch: 142 | Iter: 433800 | Total Loss: 0.002343 | Recon Loss: 0.001950 | Commit Loss: 0.000785 | Perplexity: 979.086078
2025-09-14 21:44:18,862 Stage: Train 0.5 | Epoch: 142 | Iter: 434000 | Total Loss: 0.002321 | Recon Loss: 0.001929 | Commit Loss: 0.000784 | Perplexity: 978.892603
2025-09-14 21:44:27,657 Stage: Train 0.5 | Epoch: 142 | Iter: 434200 | Total Loss: 0.002360 | Recon Loss: 0.001968 | Commit Loss: 0.000785 | Perplexity: 980.741653
2025-09-14 21:44:36,408 Stage: Train 0.5 | Epoch: 142 | Iter: 434400 | Total Loss: 0.002339 | Recon Loss: 0.001945 | Commit Loss: 0.000787 | Perplexity: 977.599971
Trainning Epoch:  87%|████████▋ | 143/165 [5:16:31<48:45, 132.98s/it]2025-09-14 21:44:45,138 Stage: Train 0.5 | Epoch: 143 | Iter: 434600 | Total Loss: 0.002319 | Recon Loss: 0.001928 | Commit Loss: 0.000782 | Perplexity: 980.683398
2025-09-14 21:44:53,900 Stage: Train 0.5 | Epoch: 143 | Iter: 434800 | Total Loss: 0.002366 | Recon Loss: 0.001977 | Commit Loss: 0.000778 | Perplexity: 975.470998
2025-09-14 21:45:02,677 Stage: Train 0.5 | Epoch: 143 | Iter: 435000 | Total Loss: 0.002323 | Recon Loss: 0.001927 | Commit Loss: 0.000791 | Perplexity: 980.207027
2025-09-14 21:45:11,424 Stage: Train 0.5 | Epoch: 143 | Iter: 435200 | Total Loss: 0.002313 | Recon Loss: 0.001917 | Commit Loss: 0.000792 | Perplexity: 983.718146
2025-09-14 21:45:20,152 Stage: Train 0.5 | Epoch: 143 | Iter: 435400 | Total Loss: 0.002387 | Recon Loss: 0.001991 | Commit Loss: 0.000791 | Perplexity: 982.673277
2025-09-14 21:45:28,896 Stage: Train 0.5 | Epoch: 143 | Iter: 435600 | Total Loss: 0.002295 | Recon Loss: 0.001906 | Commit Loss: 0.000778 | Perplexity: 979.012150
2025-09-14 21:45:37,735 Stage: Train 0.5 | Epoch: 143 | Iter: 435800 | Total Loss: 0.002292 | Recon Loss: 0.001898 | Commit Loss: 0.000788 | Perplexity: 980.453388
2025-09-14 21:45:46,500 Stage: Train 0.5 | Epoch: 143 | Iter: 436000 | Total Loss: 0.002327 | Recon Loss: 0.001935 | Commit Loss: 0.000783 | Perplexity: 976.934840
2025-09-14 21:45:55,288 Stage: Train 0.5 | Epoch: 143 | Iter: 436200 | Total Loss: 0.002356 | Recon Loss: 0.001957 | Commit Loss: 0.000797 | Perplexity: 981.305911
2025-09-14 21:46:04,038 Stage: Train 0.5 | Epoch: 143 | Iter: 436400 | Total Loss: 0.002316 | Recon Loss: 0.001925 | Commit Loss: 0.000783 | Perplexity: 980.540712
2025-09-14 21:46:12,844 Stage: Train 0.5 | Epoch: 143 | Iter: 436600 | Total Loss: 0.002303 | Recon Loss: 0.001909 | Commit Loss: 0.000789 | Perplexity: 985.265027
2025-09-14 21:46:21,604 Stage: Train 0.5 | Epoch: 143 | Iter: 436800 | Total Loss: 0.002311 | Recon Loss: 0.001915 | Commit Loss: 0.000792 | Perplexity: 983.193698
2025-09-14 21:46:30,363 Stage: Train 0.5 | Epoch: 143 | Iter: 437000 | Total Loss: 0.002329 | Recon Loss: 0.001938 | Commit Loss: 0.000783 | Perplexity: 982.128414
2025-09-14 21:46:39,141 Stage: Train 0.5 | Epoch: 143 | Iter: 437200 | Total Loss: 0.002338 | Recon Loss: 0.001945 | Commit Loss: 0.000787 | Perplexity: 981.391723
2025-09-14 21:46:47,921 Stage: Train 0.5 | Epoch: 143 | Iter: 437400 | Total Loss: 0.002319 | Recon Loss: 0.001927 | Commit Loss: 0.000784 | Perplexity: 980.555148
Trainning Epoch:  87%|████████▋ | 144/165 [5:18:44<46:33, 133.05s/it]2025-09-14 21:46:56,685 Stage: Train 0.5 | Epoch: 144 | Iter: 437600 | Total Loss: 0.002305 | Recon Loss: 0.001918 | Commit Loss: 0.000774 | Perplexity: 971.293339
2025-09-14 21:47:05,448 Stage: Train 0.5 | Epoch: 144 | Iter: 437800 | Total Loss: 0.002291 | Recon Loss: 0.001901 | Commit Loss: 0.000780 | Perplexity: 980.061477
2025-09-14 21:47:14,230 Stage: Train 0.5 | Epoch: 144 | Iter: 438000 | Total Loss: 0.002408 | Recon Loss: 0.002016 | Commit Loss: 0.000784 | Perplexity: 980.368435
2025-09-14 21:47:23,010 Stage: Train 0.5 | Epoch: 144 | Iter: 438200 | Total Loss: 0.002308 | Recon Loss: 0.001915 | Commit Loss: 0.000787 | Perplexity: 979.332668
2025-09-14 21:47:31,792 Stage: Train 0.5 | Epoch: 144 | Iter: 438400 | Total Loss: 0.002340 | Recon Loss: 0.001948 | Commit Loss: 0.000785 | Perplexity: 984.933186
2025-09-14 21:47:40,593 Stage: Train 0.5 | Epoch: 144 | Iter: 438600 | Total Loss: 0.002337 | Recon Loss: 0.001944 | Commit Loss: 0.000786 | Perplexity: 983.312336
2025-09-14 21:47:49,380 Stage: Train 0.5 | Epoch: 144 | Iter: 438800 | Total Loss: 0.002295 | Recon Loss: 0.001904 | Commit Loss: 0.000782 | Perplexity: 980.987089
2025-09-14 21:47:58,168 Stage: Train 0.5 | Epoch: 144 | Iter: 439000 | Total Loss: 0.002308 | Recon Loss: 0.001916 | Commit Loss: 0.000784 | Perplexity: 982.351230
2025-09-14 21:48:06,960 Stage: Train 0.5 | Epoch: 144 | Iter: 439200 | Total Loss: 0.002344 | Recon Loss: 0.001947 | Commit Loss: 0.000793 | Perplexity: 983.040567
2025-09-14 21:48:15,765 Stage: Train 0.5 | Epoch: 144 | Iter: 439400 | Total Loss: 0.002330 | Recon Loss: 0.001936 | Commit Loss: 0.000788 | Perplexity: 980.499527
2025-09-14 21:48:24,564 Stage: Train 0.5 | Epoch: 144 | Iter: 439600 | Total Loss: 0.002283 | Recon Loss: 0.001894 | Commit Loss: 0.000777 | Perplexity: 977.565750
2025-09-14 21:48:33,355 Stage: Train 0.5 | Epoch: 144 | Iter: 439800 | Total Loss: 0.002309 | Recon Loss: 0.001915 | Commit Loss: 0.000788 | Perplexity: 983.666824
2025-09-14 21:48:42,134 Stage: Train 0.5 | Epoch: 144 | Iter: 440000 | Total Loss: 0.002321 | Recon Loss: 0.001928 | Commit Loss: 0.000787 | Perplexity: 981.555399
2025-09-14 21:48:42,134 Saving model at iteration 440000
2025-09-14 21:48:42,456 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_145_step_440000
2025-09-14 21:48:42,696 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_145_step_440000/pytorch_model.bin
2025-09-14 21:48:43,065 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_145_step_440000/optimizer.bin
2025-09-14 21:48:43,065 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_145_step_440000/scheduler.bin
2025-09-14 21:48:43,066 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_145_step_440000/random_states_0.pkl
2025-09-14 21:48:52,134 Stage: Train 0.5 | Epoch: 144 | Iter: 440200 | Total Loss: 0.002317 | Recon Loss: 0.001925 | Commit Loss: 0.000784 | Perplexity: 979.671480
2025-09-14 21:49:00,916 Stage: Train 0.5 | Epoch: 144 | Iter: 440400 | Total Loss: 0.002393 | Recon Loss: 0.002003 | Commit Loss: 0.000780 | Perplexity: 981.031224
Trainning Epoch:  88%|████████▊ | 145/165 [5:20:59<44:30, 133.53s/it]2025-09-14 21:49:09,666 Stage: Train 0.5 | Epoch: 145 | Iter: 440600 | Total Loss: 0.002316 | Recon Loss: 0.001919 | Commit Loss: 0.000794 | Perplexity: 983.745731
2025-09-14 21:49:18,447 Stage: Train 0.5 | Epoch: 145 | Iter: 440800 | Total Loss: 0.002297 | Recon Loss: 0.001905 | Commit Loss: 0.000783 | Perplexity: 978.516221
2025-09-14 21:49:27,198 Stage: Train 0.5 | Epoch: 145 | Iter: 441000 | Total Loss: 0.002325 | Recon Loss: 0.001936 | Commit Loss: 0.000779 | Perplexity: 973.959540
2025-09-14 21:49:35,976 Stage: Train 0.5 | Epoch: 145 | Iter: 441200 | Total Loss: 0.002359 | Recon Loss: 0.001969 | Commit Loss: 0.000781 | Perplexity: 980.009069
2025-09-14 21:49:44,772 Stage: Train 0.5 | Epoch: 145 | Iter: 441400 | Total Loss: 0.002322 | Recon Loss: 0.001929 | Commit Loss: 0.000785 | Perplexity: 980.169398
2025-09-14 21:49:53,527 Stage: Train 0.5 | Epoch: 145 | Iter: 441600 | Total Loss: 0.002348 | Recon Loss: 0.001958 | Commit Loss: 0.000781 | Perplexity: 980.869042
2025-09-14 21:50:02,324 Stage: Train 0.5 | Epoch: 145 | Iter: 441800 | Total Loss: 0.002329 | Recon Loss: 0.001933 | Commit Loss: 0.000791 | Perplexity: 982.271191
2025-09-14 21:50:11,026 Stage: Train 0.5 | Epoch: 145 | Iter: 442000 | Total Loss: 0.002311 | Recon Loss: 0.001920 | Commit Loss: 0.000781 | Perplexity: 982.153303
2025-09-14 21:50:19,760 Stage: Train 0.5 | Epoch: 145 | Iter: 442200 | Total Loss: 0.002280 | Recon Loss: 0.001888 | Commit Loss: 0.000784 | Perplexity: 981.470649
2025-09-14 21:50:28,496 Stage: Train 0.5 | Epoch: 145 | Iter: 442400 | Total Loss: 0.002331 | Recon Loss: 0.001942 | Commit Loss: 0.000778 | Perplexity: 977.128220
2025-09-14 21:50:37,237 Stage: Train 0.5 | Epoch: 145 | Iter: 442600 | Total Loss: 0.002324 | Recon Loss: 0.001934 | Commit Loss: 0.000779 | Perplexity: 981.935923
2025-09-14 21:50:45,955 Stage: Train 0.5 | Epoch: 145 | Iter: 442800 | Total Loss: 0.002367 | Recon Loss: 0.001972 | Commit Loss: 0.000790 | Perplexity: 979.251046
2025-09-14 21:50:54,723 Stage: Train 0.5 | Epoch: 145 | Iter: 443000 | Total Loss: 0.002329 | Recon Loss: 0.001938 | Commit Loss: 0.000782 | Perplexity: 982.307517
2025-09-14 21:51:03,472 Stage: Train 0.5 | Epoch: 145 | Iter: 443200 | Total Loss: 0.002319 | Recon Loss: 0.001926 | Commit Loss: 0.000785 | Perplexity: 988.423407
2025-09-14 21:51:12,228 Stage: Train 0.5 | Epoch: 145 | Iter: 443400 | Total Loss: 0.002320 | Recon Loss: 0.001931 | Commit Loss: 0.000779 | Perplexity: 982.528611
Trainning Epoch:  88%|████████▊ | 146/165 [5:23:12<42:13, 133.36s/it]2025-09-14 21:51:20,976 Stage: Train 0.5 | Epoch: 146 | Iter: 443600 | Total Loss: 0.002314 | Recon Loss: 0.001924 | Commit Loss: 0.000780 | Perplexity: 975.446127
2025-09-14 21:51:29,725 Stage: Train 0.5 | Epoch: 146 | Iter: 443800 | Total Loss: 0.002289 | Recon Loss: 0.001895 | Commit Loss: 0.000787 | Perplexity: 983.767735
2025-09-14 21:51:38,420 Stage: Train 0.5 | Epoch: 146 | Iter: 444000 | Total Loss: 0.002373 | Recon Loss: 0.001980 | Commit Loss: 0.000785 | Perplexity: 983.509469
2025-09-14 21:51:47,153 Stage: Train 0.5 | Epoch: 146 | Iter: 444200 | Total Loss: 0.002265 | Recon Loss: 0.001877 | Commit Loss: 0.000775 | Perplexity: 983.074122
2025-09-14 21:51:55,858 Stage: Train 0.5 | Epoch: 146 | Iter: 444400 | Total Loss: 0.002312 | Recon Loss: 0.001921 | Commit Loss: 0.000783 | Perplexity: 981.761414
2025-09-14 21:52:04,568 Stage: Train 0.5 | Epoch: 146 | Iter: 444600 | Total Loss: 0.002328 | Recon Loss: 0.001937 | Commit Loss: 0.000781 | Perplexity: 984.469052
2025-09-14 21:52:13,254 Stage: Train 0.5 | Epoch: 146 | Iter: 444800 | Total Loss: 0.002332 | Recon Loss: 0.001945 | Commit Loss: 0.000775 | Perplexity: 983.207015
2025-09-14 21:52:21,948 Stage: Train 0.5 | Epoch: 146 | Iter: 445000 | Total Loss: 0.002308 | Recon Loss: 0.001920 | Commit Loss: 0.000777 | Perplexity: 983.169698
2025-09-14 21:52:30,654 Stage: Train 0.5 | Epoch: 146 | Iter: 445200 | Total Loss: 0.002320 | Recon Loss: 0.001932 | Commit Loss: 0.000776 | Perplexity: 980.698152
2025-09-14 21:52:39,335 Stage: Train 0.5 | Epoch: 146 | Iter: 445400 | Total Loss: 0.002320 | Recon Loss: 0.001929 | Commit Loss: 0.000782 | Perplexity: 982.608346
2025-09-14 21:52:48,038 Stage: Train 0.5 | Epoch: 146 | Iter: 445600 | Total Loss: 0.002302 | Recon Loss: 0.001911 | Commit Loss: 0.000781 | Perplexity: 985.340575
2025-09-14 21:52:56,733 Stage: Train 0.5 | Epoch: 146 | Iter: 445800 | Total Loss: 0.002313 | Recon Loss: 0.001917 | Commit Loss: 0.000791 | Perplexity: 981.054480
2025-09-14 21:53:05,438 Stage: Train 0.5 | Epoch: 146 | Iter: 446000 | Total Loss: 0.002352 | Recon Loss: 0.001964 | Commit Loss: 0.000777 | Perplexity: 979.538047
2025-09-14 21:53:14,122 Stage: Train 0.5 | Epoch: 146 | Iter: 446200 | Total Loss: 0.002324 | Recon Loss: 0.001932 | Commit Loss: 0.000783 | Perplexity: 983.614636
2025-09-14 21:53:22,830 Stage: Train 0.5 | Epoch: 146 | Iter: 446400 | Total Loss: 0.002297 | Recon Loss: 0.001907 | Commit Loss: 0.000781 | Perplexity: 982.227921
Trainning Epoch:  89%|████████▉ | 147/165 [5:25:24<39:54, 133.02s/it]2025-09-14 21:53:31,552 Stage: Train 0.5 | Epoch: 147 | Iter: 446600 | Total Loss: 0.002331 | Recon Loss: 0.001937 | Commit Loss: 0.000787 | Perplexity: 981.966975
2025-09-14 21:53:40,261 Stage: Train 0.5 | Epoch: 147 | Iter: 446800 | Total Loss: 0.002373 | Recon Loss: 0.001983 | Commit Loss: 0.000781 | Perplexity: 981.461182
2025-09-14 21:53:49,004 Stage: Train 0.5 | Epoch: 147 | Iter: 447000 | Total Loss: 0.002301 | Recon Loss: 0.001914 | Commit Loss: 0.000772 | Perplexity: 980.000691
2025-09-14 21:53:57,719 Stage: Train 0.5 | Epoch: 147 | Iter: 447200 | Total Loss: 0.002288 | Recon Loss: 0.001899 | Commit Loss: 0.000778 | Perplexity: 982.419498
2025-09-14 21:54:06,433 Stage: Train 0.5 | Epoch: 147 | Iter: 447400 | Total Loss: 0.002324 | Recon Loss: 0.001937 | Commit Loss: 0.000774 | Perplexity: 982.966084
2025-09-14 21:54:15,132 Stage: Train 0.5 | Epoch: 147 | Iter: 447600 | Total Loss: 0.002305 | Recon Loss: 0.001917 | Commit Loss: 0.000776 | Perplexity: 982.512019
2025-09-14 21:54:23,877 Stage: Train 0.5 | Epoch: 147 | Iter: 447800 | Total Loss: 0.002322 | Recon Loss: 0.001929 | Commit Loss: 0.000786 | Perplexity: 983.533874
2025-09-14 21:54:32,594 Stage: Train 0.5 | Epoch: 147 | Iter: 448000 | Total Loss: 0.002297 | Recon Loss: 0.001910 | Commit Loss: 0.000775 | Perplexity: 977.397121
2025-09-14 21:54:41,352 Stage: Train 0.5 | Epoch: 147 | Iter: 448200 | Total Loss: 0.002307 | Recon Loss: 0.001916 | Commit Loss: 0.000783 | Perplexity: 980.219937
2025-09-14 21:54:50,082 Stage: Train 0.5 | Epoch: 147 | Iter: 448400 | Total Loss: 0.002287 | Recon Loss: 0.001896 | Commit Loss: 0.000783 | Perplexity: 982.899542
2025-09-14 21:54:58,820 Stage: Train 0.5 | Epoch: 147 | Iter: 448600 | Total Loss: 0.002282 | Recon Loss: 0.001895 | Commit Loss: 0.000773 | Perplexity: 983.336629
2025-09-14 21:55:07,541 Stage: Train 0.5 | Epoch: 147 | Iter: 448800 | Total Loss: 0.002318 | Recon Loss: 0.001930 | Commit Loss: 0.000776 | Perplexity: 983.849514
2025-09-14 21:55:16,278 Stage: Train 0.5 | Epoch: 147 | Iter: 449000 | Total Loss: 0.002307 | Recon Loss: 0.001919 | Commit Loss: 0.000775 | Perplexity: 982.288362
2025-09-14 21:55:25,009 Stage: Train 0.5 | Epoch: 147 | Iter: 449200 | Total Loss: 0.002304 | Recon Loss: 0.001911 | Commit Loss: 0.000787 | Perplexity: 984.029391
2025-09-14 21:55:33,698 Stage: Train 0.5 | Epoch: 147 | Iter: 449400 | Total Loss: 0.002335 | Recon Loss: 0.001948 | Commit Loss: 0.000774 | Perplexity: 981.798466
2025-09-14 21:55:42,444 Stage: Train 0.5 | Epoch: 147 | Iter: 449600 | Total Loss: 0.002285 | Recon Loss: 0.001895 | Commit Loss: 0.000780 | Perplexity: 986.767941
Trainning Epoch:  90%|████████▉ | 148/165 [5:27:37<37:38, 132.88s/it]2025-09-14 21:55:51,123 Stage: Train 0.5 | Epoch: 148 | Iter: 449800 | Total Loss: 0.002303 | Recon Loss: 0.001915 | Commit Loss: 0.000776 | Perplexity: 975.865028
2025-09-14 21:55:59,843 Stage: Train 0.5 | Epoch: 148 | Iter: 450000 | Total Loss: 0.002258 | Recon Loss: 0.001872 | Commit Loss: 0.000774 | Perplexity: 982.519938
2025-09-14 21:56:08,530 Stage: Train 0.5 | Epoch: 148 | Iter: 450200 | Total Loss: 0.002334 | Recon Loss: 0.001948 | Commit Loss: 0.000772 | Perplexity: 982.981145
2025-09-14 21:56:17,244 Stage: Train 0.5 | Epoch: 148 | Iter: 450400 | Total Loss: 0.002303 | Recon Loss: 0.001916 | Commit Loss: 0.000775 | Perplexity: 981.969212
2025-09-14 21:56:25,972 Stage: Train 0.5 | Epoch: 148 | Iter: 450600 | Total Loss: 0.002321 | Recon Loss: 0.001931 | Commit Loss: 0.000779 | Perplexity: 983.971056
2025-09-14 21:56:34,678 Stage: Train 0.5 | Epoch: 148 | Iter: 450800 | Total Loss: 0.002299 | Recon Loss: 0.001911 | Commit Loss: 0.000777 | Perplexity: 981.197779
2025-09-14 21:56:43,397 Stage: Train 0.5 | Epoch: 148 | Iter: 451000 | Total Loss: 0.002315 | Recon Loss: 0.001925 | Commit Loss: 0.000780 | Perplexity: 981.293836
2025-09-14 21:56:52,104 Stage: Train 0.5 | Epoch: 148 | Iter: 451200 | Total Loss: 0.002311 | Recon Loss: 0.001920 | Commit Loss: 0.000783 | Perplexity: 984.236974
2025-09-14 21:57:00,810 Stage: Train 0.5 | Epoch: 148 | Iter: 451400 | Total Loss: 0.002302 | Recon Loss: 0.001908 | Commit Loss: 0.000788 | Perplexity: 987.584997
2025-09-14 21:57:09,548 Stage: Train 0.5 | Epoch: 148 | Iter: 451600 | Total Loss: 0.002299 | Recon Loss: 0.001908 | Commit Loss: 0.000782 | Perplexity: 983.018431
2025-09-14 21:57:18,243 Stage: Train 0.5 | Epoch: 148 | Iter: 451800 | Total Loss: 0.002324 | Recon Loss: 0.001934 | Commit Loss: 0.000781 | Perplexity: 985.438448
2025-09-14 21:57:26,951 Stage: Train 0.5 | Epoch: 148 | Iter: 452000 | Total Loss: 0.002303 | Recon Loss: 0.001915 | Commit Loss: 0.000776 | Perplexity: 982.662931
2025-09-14 21:57:35,648 Stage: Train 0.5 | Epoch: 148 | Iter: 452200 | Total Loss: 0.002325 | Recon Loss: 0.001935 | Commit Loss: 0.000780 | Perplexity: 984.977626
2025-09-14 21:57:44,363 Stage: Train 0.5 | Epoch: 148 | Iter: 452400 | Total Loss: 0.002307 | Recon Loss: 0.001915 | Commit Loss: 0.000784 | Perplexity: 989.027391
2025-09-14 21:57:53,053 Stage: Train 0.5 | Epoch: 148 | Iter: 452600 | Total Loss: 0.002281 | Recon Loss: 0.001890 | Commit Loss: 0.000781 | Perplexity: 989.284631
Trainning Epoch:  90%|█████████ | 149/165 [5:29:49<35:23, 132.70s/it]2025-09-14 21:58:01,768 Stage: Train 0.5 | Epoch: 149 | Iter: 452800 | Total Loss: 0.002266 | Recon Loss: 0.001879 | Commit Loss: 0.000775 | Perplexity: 979.660431
2025-09-14 21:58:10,444 Stage: Train 0.5 | Epoch: 149 | Iter: 453000 | Total Loss: 0.002358 | Recon Loss: 0.001965 | Commit Loss: 0.000785 | Perplexity: 987.801719
2025-09-14 21:58:19,182 Stage: Train 0.5 | Epoch: 149 | Iter: 453200 | Total Loss: 0.002270 | Recon Loss: 0.001885 | Commit Loss: 0.000770 | Perplexity: 980.665115
2025-09-14 21:58:27,882 Stage: Train 0.5 | Epoch: 149 | Iter: 453400 | Total Loss: 0.002322 | Recon Loss: 0.001927 | Commit Loss: 0.000789 | Perplexity: 984.581892
2025-09-14 21:58:36,584 Stage: Train 0.5 | Epoch: 149 | Iter: 453600 | Total Loss: 0.002296 | Recon Loss: 0.001909 | Commit Loss: 0.000774 | Perplexity: 980.225357
2025-09-14 21:58:45,291 Stage: Train 0.5 | Epoch: 149 | Iter: 453800 | Total Loss: 0.002337 | Recon Loss: 0.001950 | Commit Loss: 0.000775 | Perplexity: 980.860246
2025-09-14 21:58:54,024 Stage: Train 0.5 | Epoch: 149 | Iter: 454000 | Total Loss: 0.002275 | Recon Loss: 0.001892 | Commit Loss: 0.000767 | Perplexity: 979.190084
2025-09-14 21:59:02,765 Stage: Train 0.5 | Epoch: 149 | Iter: 454200 | Total Loss: 0.002278 | Recon Loss: 0.001886 | Commit Loss: 0.000783 | Perplexity: 985.015865
2025-09-14 21:59:11,468 Stage: Train 0.5 | Epoch: 149 | Iter: 454400 | Total Loss: 0.002276 | Recon Loss: 0.001889 | Commit Loss: 0.000775 | Perplexity: 981.066200
2025-09-14 21:59:20,195 Stage: Train 0.5 | Epoch: 149 | Iter: 454600 | Total Loss: 0.002320 | Recon Loss: 0.001932 | Commit Loss: 0.000777 | Perplexity: 982.866774
2025-09-14 21:59:28,911 Stage: Train 0.5 | Epoch: 149 | Iter: 454800 | Total Loss: 0.002289 | Recon Loss: 0.001902 | Commit Loss: 0.000775 | Perplexity: 980.120159
2025-09-14 21:59:37,630 Stage: Train 0.5 | Epoch: 149 | Iter: 455000 | Total Loss: 0.002292 | Recon Loss: 0.001903 | Commit Loss: 0.000778 | Perplexity: 981.497366
2025-09-14 21:59:46,331 Stage: Train 0.5 | Epoch: 149 | Iter: 455200 | Total Loss: 0.002300 | Recon Loss: 0.001912 | Commit Loss: 0.000777 | Perplexity: 981.877920
2025-09-14 21:59:55,081 Stage: Train 0.5 | Epoch: 149 | Iter: 455400 | Total Loss: 0.002312 | Recon Loss: 0.001924 | Commit Loss: 0.000776 | Perplexity: 984.327623
2025-09-14 22:00:03,817 Stage: Train 0.5 | Epoch: 149 | Iter: 455600 | Total Loss: 0.002308 | Recon Loss: 0.001921 | Commit Loss: 0.000773 | Perplexity: 983.585428
Trainning Epoch:  91%|█████████ | 150/165 [5:32:01<33:09, 132.63s/it]2025-09-14 22:00:12,564 Stage: Train 0.5 | Epoch: 150 | Iter: 455800 | Total Loss: 0.002296 | Recon Loss: 0.001906 | Commit Loss: 0.000779 | Perplexity: 983.498864
2025-09-14 22:00:21,297 Stage: Train 0.5 | Epoch: 150 | Iter: 456000 | Total Loss: 0.002297 | Recon Loss: 0.001911 | Commit Loss: 0.000772 | Perplexity: 983.926379
2025-09-14 22:00:30,032 Stage: Train 0.5 | Epoch: 150 | Iter: 456200 | Total Loss: 0.002333 | Recon Loss: 0.001946 | Commit Loss: 0.000773 | Perplexity: 983.256419
2025-09-14 22:00:38,761 Stage: Train 0.5 | Epoch: 150 | Iter: 456400 | Total Loss: 0.002290 | Recon Loss: 0.001907 | Commit Loss: 0.000767 | Perplexity: 983.095208
2025-09-14 22:00:47,517 Stage: Train 0.5 | Epoch: 150 | Iter: 456600 | Total Loss: 0.002347 | Recon Loss: 0.001961 | Commit Loss: 0.000772 | Perplexity: 982.632329
2025-09-14 22:00:56,304 Stage: Train 0.5 | Epoch: 150 | Iter: 456800 | Total Loss: 0.002272 | Recon Loss: 0.001885 | Commit Loss: 0.000774 | Perplexity: 983.711063
2025-09-14 22:01:05,072 Stage: Train 0.5 | Epoch: 150 | Iter: 457000 | Total Loss: 0.002307 | Recon Loss: 0.001921 | Commit Loss: 0.000770 | Perplexity: 980.987634
2025-09-14 22:01:13,914 Stage: Train 0.5 | Epoch: 150 | Iter: 457200 | Total Loss: 0.002268 | Recon Loss: 0.001883 | Commit Loss: 0.000770 | Perplexity: 982.402340
2025-09-14 22:01:22,633 Stage: Train 0.5 | Epoch: 150 | Iter: 457400 | Total Loss: 0.002328 | Recon Loss: 0.001939 | Commit Loss: 0.000776 | Perplexity: 984.340963
2025-09-14 22:01:31,296 Stage: Train 0.5 | Epoch: 150 | Iter: 457600 | Total Loss: 0.002267 | Recon Loss: 0.001877 | Commit Loss: 0.000780 | Perplexity: 983.159125
2025-09-14 22:01:39,988 Stage: Train 0.5 | Epoch: 150 | Iter: 457800 | Total Loss: 0.002296 | Recon Loss: 0.001906 | Commit Loss: 0.000780 | Perplexity: 985.360289
2025-09-14 22:01:48,711 Stage: Train 0.5 | Epoch: 150 | Iter: 458000 | Total Loss: 0.002335 | Recon Loss: 0.001951 | Commit Loss: 0.000768 | Perplexity: 978.142683
2025-09-14 22:01:57,415 Stage: Train 0.5 | Epoch: 150 | Iter: 458200 | Total Loss: 0.002307 | Recon Loss: 0.001920 | Commit Loss: 0.000774 | Perplexity: 983.999300
2025-09-14 22:02:06,128 Stage: Train 0.5 | Epoch: 150 | Iter: 458400 | Total Loss: 0.002293 | Recon Loss: 0.001908 | Commit Loss: 0.000770 | Perplexity: 980.357223
2025-09-14 22:02:14,831 Stage: Train 0.5 | Epoch: 150 | Iter: 458600 | Total Loss: 0.002331 | Recon Loss: 0.001940 | Commit Loss: 0.000783 | Perplexity: 983.350849
Trainning Epoch:  92%|█████████▏| 151/165 [5:34:14<30:56, 132.62s/it]2025-09-14 22:02:23,560 Stage: Train 0.5 | Epoch: 151 | Iter: 458800 | Total Loss: 0.002280 | Recon Loss: 0.001897 | Commit Loss: 0.000767 | Perplexity: 979.141640
2025-09-14 22:02:32,274 Stage: Train 0.5 | Epoch: 151 | Iter: 459000 | Total Loss: 0.002280 | Recon Loss: 0.001895 | Commit Loss: 0.000771 | Perplexity: 983.565242
2025-09-14 22:02:41,000 Stage: Train 0.5 | Epoch: 151 | Iter: 459200 | Total Loss: 0.002302 | Recon Loss: 0.001915 | Commit Loss: 0.000773 | Perplexity: 985.274169
2025-09-14 22:02:49,726 Stage: Train 0.5 | Epoch: 151 | Iter: 459400 | Total Loss: 0.002286 | Recon Loss: 0.001900 | Commit Loss: 0.000771 | Perplexity: 988.242758
2025-09-14 22:02:58,465 Stage: Train 0.5 | Epoch: 151 | Iter: 459600 | Total Loss: 0.002302 | Recon Loss: 0.001913 | Commit Loss: 0.000777 | Perplexity: 981.622311
2025-09-14 22:03:07,178 Stage: Train 0.5 | Epoch: 151 | Iter: 459800 | Total Loss: 0.002257 | Recon Loss: 0.001872 | Commit Loss: 0.000769 | Perplexity: 981.036708
2025-09-14 22:03:15,913 Stage: Train 0.5 | Epoch: 151 | Iter: 460000 | Total Loss: 0.002283 | Recon Loss: 0.001897 | Commit Loss: 0.000772 | Perplexity: 987.460887
2025-09-14 22:03:15,913 Saving model at iteration 460000
2025-09-14 22:03:16,069 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_152_step_460000
2025-09-14 22:03:16,302 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_152_step_460000/pytorch_model.bin
2025-09-14 22:03:16,668 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_152_step_460000/optimizer.bin
2025-09-14 22:03:16,668 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_152_step_460000/scheduler.bin
2025-09-14 22:03:16,669 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_152_step_460000/random_states_0.pkl
2025-09-14 22:03:25,586 Stage: Train 0.5 | Epoch: 151 | Iter: 460200 | Total Loss: 0.002287 | Recon Loss: 0.001904 | Commit Loss: 0.000766 | Perplexity: 982.811465
2025-09-14 22:03:34,328 Stage: Train 0.5 | Epoch: 151 | Iter: 460400 | Total Loss: 0.002308 | Recon Loss: 0.001917 | Commit Loss: 0.000783 | Perplexity: 986.677801
2025-09-14 22:03:43,057 Stage: Train 0.5 | Epoch: 151 | Iter: 460600 | Total Loss: 0.002264 | Recon Loss: 0.001879 | Commit Loss: 0.000771 | Perplexity: 981.987653
2025-09-14 22:03:51,788 Stage: Train 0.5 | Epoch: 151 | Iter: 460800 | Total Loss: 0.002325 | Recon Loss: 0.001937 | Commit Loss: 0.000776 | Perplexity: 981.418929
2025-09-14 22:04:00,516 Stage: Train 0.5 | Epoch: 151 | Iter: 461000 | Total Loss: 0.002287 | Recon Loss: 0.001900 | Commit Loss: 0.000773 | Perplexity: 983.576143
2025-09-14 22:04:09,251 Stage: Train 0.5 | Epoch: 151 | Iter: 461200 | Total Loss: 0.002272 | Recon Loss: 0.001889 | Commit Loss: 0.000766 | Perplexity: 979.837247
2025-09-14 22:04:17,969 Stage: Train 0.5 | Epoch: 151 | Iter: 461400 | Total Loss: 0.002295 | Recon Loss: 0.001906 | Commit Loss: 0.000777 | Perplexity: 982.990226
2025-09-14 22:04:26,672 Stage: Train 0.5 | Epoch: 151 | Iter: 461600 | Total Loss: 0.002288 | Recon Loss: 0.001899 | Commit Loss: 0.000778 | Perplexity: 987.169686
Trainning Epoch:  92%|█████████▏| 152/165 [5:36:27<28:47, 132.89s/it]2025-09-14 22:04:35,395 Stage: Train 0.5 | Epoch: 152 | Iter: 461800 | Total Loss: 0.002304 | Recon Loss: 0.001919 | Commit Loss: 0.000770 | Perplexity: 977.950205
2025-09-14 22:04:44,102 Stage: Train 0.5 | Epoch: 152 | Iter: 462000 | Total Loss: 0.002291 | Recon Loss: 0.001906 | Commit Loss: 0.000769 | Perplexity: 987.554797
2025-09-14 22:04:52,799 Stage: Train 0.5 | Epoch: 152 | Iter: 462200 | Total Loss: 0.002315 | Recon Loss: 0.001932 | Commit Loss: 0.000766 | Perplexity: 982.709181
2025-09-14 22:05:01,499 Stage: Train 0.5 | Epoch: 152 | Iter: 462400 | Total Loss: 0.002301 | Recon Loss: 0.001912 | Commit Loss: 0.000779 | Perplexity: 982.649707
2025-09-14 22:05:10,180 Stage: Train 0.5 | Epoch: 152 | Iter: 462600 | Total Loss: 0.002302 | Recon Loss: 0.001917 | Commit Loss: 0.000769 | Perplexity: 985.040113
2025-09-14 22:05:18,880 Stage: Train 0.5 | Epoch: 152 | Iter: 462800 | Total Loss: 0.002273 | Recon Loss: 0.001885 | Commit Loss: 0.000776 | Perplexity: 983.441817
2025-09-14 22:05:27,562 Stage: Train 0.5 | Epoch: 152 | Iter: 463000 | Total Loss: 0.002299 | Recon Loss: 0.001912 | Commit Loss: 0.000776 | Perplexity: 983.993618
2025-09-14 22:05:36,231 Stage: Train 0.5 | Epoch: 152 | Iter: 463200 | Total Loss: 0.002270 | Recon Loss: 0.001888 | Commit Loss: 0.000764 | Perplexity: 983.026025
2025-09-14 22:05:44,920 Stage: Train 0.5 | Epoch: 152 | Iter: 463400 | Total Loss: 0.002312 | Recon Loss: 0.001927 | Commit Loss: 0.000769 | Perplexity: 984.109863
2025-09-14 22:05:53,602 Stage: Train 0.5 | Epoch: 152 | Iter: 463600 | Total Loss: 0.002284 | Recon Loss: 0.001898 | Commit Loss: 0.000772 | Perplexity: 981.639337
2025-09-14 22:06:02,364 Stage: Train 0.5 | Epoch: 152 | Iter: 463800 | Total Loss: 0.002292 | Recon Loss: 0.001904 | Commit Loss: 0.000777 | Perplexity: 982.840012
2025-09-14 22:06:11,072 Stage: Train 0.5 | Epoch: 152 | Iter: 464000 | Total Loss: 0.002384 | Recon Loss: 0.001993 | Commit Loss: 0.000782 | Perplexity: 990.939412
2025-09-14 22:06:19,756 Stage: Train 0.5 | Epoch: 152 | Iter: 464200 | Total Loss: 0.002273 | Recon Loss: 0.001881 | Commit Loss: 0.000784 | Perplexity: 988.929435
2025-09-14 22:06:28,489 Stage: Train 0.5 | Epoch: 152 | Iter: 464400 | Total Loss: 0.002264 | Recon Loss: 0.001877 | Commit Loss: 0.000775 | Perplexity: 987.209492
2025-09-14 22:06:37,188 Stage: Train 0.5 | Epoch: 152 | Iter: 464600 | Total Loss: 0.002297 | Recon Loss: 0.001911 | Commit Loss: 0.000772 | Perplexity: 980.428913
2025-09-14 22:06:45,904 Stage: Train 0.5 | Epoch: 152 | Iter: 464800 | Total Loss: 0.002297 | Recon Loss: 0.001915 | Commit Loss: 0.000765 | Perplexity: 987.414156
Trainning Epoch:  93%|█████████▎| 153/165 [5:38:40<26:32, 132.68s/it]2025-09-14 22:06:54,644 Stage: Train 0.5 | Epoch: 153 | Iter: 465000 | Total Loss: 0.002279 | Recon Loss: 0.001893 | Commit Loss: 0.000771 | Perplexity: 984.708313
2025-09-14 22:07:03,426 Stage: Train 0.5 | Epoch: 153 | Iter: 465200 | Total Loss: 0.002266 | Recon Loss: 0.001884 | Commit Loss: 0.000765 | Perplexity: 982.033152
2025-09-14 22:07:12,138 Stage: Train 0.5 | Epoch: 153 | Iter: 465400 | Total Loss: 0.002295 | Recon Loss: 0.001913 | Commit Loss: 0.000765 | Perplexity: 984.160013
2025-09-14 22:07:20,840 Stage: Train 0.5 | Epoch: 153 | Iter: 465600 | Total Loss: 0.002233 | Recon Loss: 0.001854 | Commit Loss: 0.000759 | Perplexity: 977.932080
2025-09-14 22:07:29,597 Stage: Train 0.5 | Epoch: 153 | Iter: 465800 | Total Loss: 0.002273 | Recon Loss: 0.001889 | Commit Loss: 0.000768 | Perplexity: 985.838653
2025-09-14 22:07:38,357 Stage: Train 0.5 | Epoch: 153 | Iter: 466000 | Total Loss: 0.002293 | Recon Loss: 0.001906 | Commit Loss: 0.000774 | Perplexity: 985.081294
2025-09-14 22:07:47,092 Stage: Train 0.5 | Epoch: 153 | Iter: 466200 | Total Loss: 0.002289 | Recon Loss: 0.001905 | Commit Loss: 0.000769 | Perplexity: 982.678001
2025-09-14 22:07:55,837 Stage: Train 0.5 | Epoch: 153 | Iter: 466400 | Total Loss: 0.002307 | Recon Loss: 0.001921 | Commit Loss: 0.000772 | Perplexity: 984.170675
2025-09-14 22:08:04,585 Stage: Train 0.5 | Epoch: 153 | Iter: 466600 | Total Loss: 0.002275 | Recon Loss: 0.001889 | Commit Loss: 0.000771 | Perplexity: 985.229823
2025-09-14 22:08:13,305 Stage: Train 0.5 | Epoch: 153 | Iter: 466800 | Total Loss: 0.002298 | Recon Loss: 0.001911 | Commit Loss: 0.000774 | Perplexity: 984.726152
2025-09-14 22:08:22,033 Stage: Train 0.5 | Epoch: 153 | Iter: 467000 | Total Loss: 0.002274 | Recon Loss: 0.001885 | Commit Loss: 0.000777 | Perplexity: 987.166461
2025-09-14 22:08:30,749 Stage: Train 0.5 | Epoch: 153 | Iter: 467200 | Total Loss: 0.002296 | Recon Loss: 0.001909 | Commit Loss: 0.000774 | Perplexity: 987.724469
2025-09-14 22:08:39,484 Stage: Train 0.5 | Epoch: 153 | Iter: 467400 | Total Loss: 0.002277 | Recon Loss: 0.001887 | Commit Loss: 0.000780 | Perplexity: 987.807532
2025-09-14 22:08:48,203 Stage: Train 0.5 | Epoch: 153 | Iter: 467600 | Total Loss: 0.002302 | Recon Loss: 0.001916 | Commit Loss: 0.000772 | Perplexity: 985.284733
2025-09-14 22:08:56,936 Stage: Train 0.5 | Epoch: 153 | Iter: 467800 | Total Loss: 0.002294 | Recon Loss: 0.001906 | Commit Loss: 0.000776 | Perplexity: 985.503341
Trainning Epoch:  93%|█████████▎| 154/165 [5:40:52<24:19, 132.68s/it]2025-09-14 22:09:05,643 Stage: Train 0.5 | Epoch: 154 | Iter: 468000 | Total Loss: 0.002277 | Recon Loss: 0.001891 | Commit Loss: 0.000772 | Perplexity: 985.481853
2025-09-14 22:09:14,376 Stage: Train 0.5 | Epoch: 154 | Iter: 468200 | Total Loss: 0.002352 | Recon Loss: 0.001972 | Commit Loss: 0.000761 | Perplexity: 983.699931
2025-09-14 22:09:23,102 Stage: Train 0.5 | Epoch: 154 | Iter: 468400 | Total Loss: 0.002264 | Recon Loss: 0.001883 | Commit Loss: 0.000762 | Perplexity: 986.235515
2025-09-14 22:09:31,824 Stage: Train 0.5 | Epoch: 154 | Iter: 468600 | Total Loss: 0.002255 | Recon Loss: 0.001869 | Commit Loss: 0.000773 | Perplexity: 983.186121
2025-09-14 22:09:40,571 Stage: Train 0.5 | Epoch: 154 | Iter: 468800 | Total Loss: 0.002262 | Recon Loss: 0.001879 | Commit Loss: 0.000767 | Perplexity: 980.453789
2025-09-14 22:09:49,280 Stage: Train 0.5 | Epoch: 154 | Iter: 469000 | Total Loss: 0.002285 | Recon Loss: 0.001903 | Commit Loss: 0.000764 | Perplexity: 982.402651
2025-09-14 22:09:58,000 Stage: Train 0.5 | Epoch: 154 | Iter: 469200 | Total Loss: 0.002261 | Recon Loss: 0.001874 | Commit Loss: 0.000775 | Perplexity: 987.395427
2025-09-14 22:10:06,718 Stage: Train 0.5 | Epoch: 154 | Iter: 469400 | Total Loss: 0.002303 | Recon Loss: 0.001916 | Commit Loss: 0.000773 | Perplexity: 984.499760
2025-09-14 22:10:15,475 Stage: Train 0.5 | Epoch: 154 | Iter: 469600 | Total Loss: 0.002323 | Recon Loss: 0.001935 | Commit Loss: 0.000776 | Perplexity: 985.509092
2025-09-14 22:10:24,184 Stage: Train 0.5 | Epoch: 154 | Iter: 469800 | Total Loss: 0.002251 | Recon Loss: 0.001865 | Commit Loss: 0.000771 | Perplexity: 982.211143
2025-09-14 22:10:32,890 Stage: Train 0.5 | Epoch: 154 | Iter: 470000 | Total Loss: 0.002261 | Recon Loss: 0.001878 | Commit Loss: 0.000768 | Perplexity: 983.636792
2025-09-14 22:10:41,623 Stage: Train 0.5 | Epoch: 154 | Iter: 470200 | Total Loss: 0.002318 | Recon Loss: 0.001929 | Commit Loss: 0.000777 | Perplexity: 990.132112
2025-09-14 22:10:50,324 Stage: Train 0.5 | Epoch: 154 | Iter: 470400 | Total Loss: 0.002266 | Recon Loss: 0.001875 | Commit Loss: 0.000781 | Perplexity: 992.232060
2025-09-14 22:10:59,009 Stage: Train 0.5 | Epoch: 154 | Iter: 470600 | Total Loss: 0.002281 | Recon Loss: 0.001894 | Commit Loss: 0.000774 | Perplexity: 985.363110
2025-09-14 22:11:07,720 Stage: Train 0.5 | Epoch: 154 | Iter: 470800 | Total Loss: 0.002297 | Recon Loss: 0.001913 | Commit Loss: 0.000769 | Perplexity: 985.350258
Trainning Epoch:  94%|█████████▍| 155/165 [5:43:05<22:06, 132.60s/it]2025-09-14 22:11:16,412 Stage: Train 0.5 | Epoch: 155 | Iter: 471000 | Total Loss: 0.002287 | Recon Loss: 0.001904 | Commit Loss: 0.000766 | Perplexity: 982.072556
2025-09-14 22:11:25,099 Stage: Train 0.5 | Epoch: 155 | Iter: 471200 | Total Loss: 0.002310 | Recon Loss: 0.001921 | Commit Loss: 0.000777 | Perplexity: 986.342480
2025-09-14 22:11:33,831 Stage: Train 0.5 | Epoch: 155 | Iter: 471400 | Total Loss: 0.002262 | Recon Loss: 0.001882 | Commit Loss: 0.000761 | Perplexity: 978.988257
2025-09-14 22:11:42,542 Stage: Train 0.5 | Epoch: 155 | Iter: 471600 | Total Loss: 0.002318 | Recon Loss: 0.001936 | Commit Loss: 0.000766 | Perplexity: 985.803896
2025-09-14 22:11:51,246 Stage: Train 0.5 | Epoch: 155 | Iter: 471800 | Total Loss: 0.002248 | Recon Loss: 0.001861 | Commit Loss: 0.000775 | Perplexity: 985.928142
2025-09-14 22:11:59,938 Stage: Train 0.5 | Epoch: 155 | Iter: 472000 | Total Loss: 0.002310 | Recon Loss: 0.001925 | Commit Loss: 0.000769 | Perplexity: 986.901642
2025-09-14 22:12:08,641 Stage: Train 0.5 | Epoch: 155 | Iter: 472200 | Total Loss: 0.002278 | Recon Loss: 0.001895 | Commit Loss: 0.000766 | Perplexity: 983.143908
2025-09-14 22:12:17,353 Stage: Train 0.5 | Epoch: 155 | Iter: 472400 | Total Loss: 0.002267 | Recon Loss: 0.001880 | Commit Loss: 0.000774 | Perplexity: 987.756358
2025-09-14 22:12:26,058 Stage: Train 0.5 | Epoch: 155 | Iter: 472600 | Total Loss: 0.002285 | Recon Loss: 0.001906 | Commit Loss: 0.000757 | Perplexity: 983.396468
2025-09-14 22:12:34,777 Stage: Train 0.5 | Epoch: 155 | Iter: 472800 | Total Loss: 0.002292 | Recon Loss: 0.001908 | Commit Loss: 0.000768 | Perplexity: 985.100385
2025-09-14 22:12:43,492 Stage: Train 0.5 | Epoch: 155 | Iter: 473000 | Total Loss: 0.002279 | Recon Loss: 0.001893 | Commit Loss: 0.000772 | Perplexity: 987.125352
2025-09-14 22:12:52,231 Stage: Train 0.5 | Epoch: 155 | Iter: 473200 | Total Loss: 0.002285 | Recon Loss: 0.001903 | Commit Loss: 0.000764 | Perplexity: 985.158233
2025-09-14 22:13:00,918 Stage: Train 0.5 | Epoch: 155 | Iter: 473400 | Total Loss: 0.002267 | Recon Loss: 0.001878 | Commit Loss: 0.000777 | Perplexity: 995.201333
2025-09-14 22:13:09,613 Stage: Train 0.5 | Epoch: 155 | Iter: 473600 | Total Loss: 0.002300 | Recon Loss: 0.001916 | Commit Loss: 0.000769 | Perplexity: 986.979244
2025-09-14 22:13:18,331 Stage: Train 0.5 | Epoch: 155 | Iter: 473800 | Total Loss: 0.002269 | Recon Loss: 0.001879 | Commit Loss: 0.000780 | Perplexity: 987.869451
Trainning Epoch:  95%|█████████▍| 156/165 [5:45:17<19:52, 132.50s/it]2025-09-14 22:13:27,054 Stage: Train 0.5 | Epoch: 156 | Iter: 474000 | Total Loss: 0.002304 | Recon Loss: 0.001923 | Commit Loss: 0.000762 | Perplexity: 983.812997
2025-09-14 22:13:35,824 Stage: Train 0.5 | Epoch: 156 | Iter: 474200 | Total Loss: 0.002237 | Recon Loss: 0.001857 | Commit Loss: 0.000760 | Perplexity: 982.692781
2025-09-14 22:13:44,588 Stage: Train 0.5 | Epoch: 156 | Iter: 474400 | Total Loss: 0.002294 | Recon Loss: 0.001909 | Commit Loss: 0.000768 | Perplexity: 984.501475
2025-09-14 22:13:53,348 Stage: Train 0.5 | Epoch: 156 | Iter: 474600 | Total Loss: 0.002284 | Recon Loss: 0.001903 | Commit Loss: 0.000761 | Perplexity: 981.614089
2025-09-14 22:14:02,131 Stage: Train 0.5 | Epoch: 156 | Iter: 474800 | Total Loss: 0.002300 | Recon Loss: 0.001914 | Commit Loss: 0.000771 | Perplexity: 990.066727
2025-09-14 22:14:10,915 Stage: Train 0.5 | Epoch: 156 | Iter: 475000 | Total Loss: 0.002242 | Recon Loss: 0.001861 | Commit Loss: 0.000762 | Perplexity: 986.563464
2025-09-14 22:14:19,692 Stage: Train 0.5 | Epoch: 156 | Iter: 475200 | Total Loss: 0.002276 | Recon Loss: 0.001893 | Commit Loss: 0.000766 | Perplexity: 984.379744
2025-09-14 22:14:28,487 Stage: Train 0.5 | Epoch: 156 | Iter: 475400 | Total Loss: 0.002242 | Recon Loss: 0.001858 | Commit Loss: 0.000766 | Perplexity: 983.065211
2025-09-14 22:14:37,229 Stage: Train 0.5 | Epoch: 156 | Iter: 475600 | Total Loss: 0.002276 | Recon Loss: 0.001890 | Commit Loss: 0.000771 | Perplexity: 990.417704
2025-09-14 22:14:45,980 Stage: Train 0.5 | Epoch: 156 | Iter: 475800 | Total Loss: 0.002295 | Recon Loss: 0.001912 | Commit Loss: 0.000765 | Perplexity: 987.681048
2025-09-14 22:14:54,748 Stage: Train 0.5 | Epoch: 156 | Iter: 476000 | Total Loss: 0.002270 | Recon Loss: 0.001887 | Commit Loss: 0.000766 | Perplexity: 983.674315
2025-09-14 22:15:03,533 Stage: Train 0.5 | Epoch: 156 | Iter: 476200 | Total Loss: 0.002266 | Recon Loss: 0.001880 | Commit Loss: 0.000772 | Perplexity: 986.028094
2025-09-14 22:15:12,332 Stage: Train 0.5 | Epoch: 156 | Iter: 476400 | Total Loss: 0.002309 | Recon Loss: 0.001923 | Commit Loss: 0.000771 | Perplexity: 990.006764
2025-09-14 22:15:21,114 Stage: Train 0.5 | Epoch: 156 | Iter: 476600 | Total Loss: 0.002269 | Recon Loss: 0.001882 | Commit Loss: 0.000774 | Perplexity: 990.395518
2025-09-14 22:15:29,895 Stage: Train 0.5 | Epoch: 156 | Iter: 476800 | Total Loss: 0.002278 | Recon Loss: 0.001895 | Commit Loss: 0.000765 | Perplexity: 987.000899
Trainning Epoch:  95%|█████████▌| 157/165 [5:47:30<17:41, 132.74s/it]2025-09-14 22:15:38,676 Stage: Train 0.5 | Epoch: 157 | Iter: 477000 | Total Loss: 0.002284 | Recon Loss: 0.001892 | Commit Loss: 0.000783 | Perplexity: 989.414971
2025-09-14 22:15:47,451 Stage: Train 0.5 | Epoch: 157 | Iter: 477200 | Total Loss: 0.002275 | Recon Loss: 0.001894 | Commit Loss: 0.000762 | Perplexity: 988.912328
2025-09-14 22:15:56,240 Stage: Train 0.5 | Epoch: 157 | Iter: 477400 | Total Loss: 0.002276 | Recon Loss: 0.001894 | Commit Loss: 0.000765 | Perplexity: 988.134219
2025-09-14 22:16:05,022 Stage: Train 0.5 | Epoch: 157 | Iter: 477600 | Total Loss: 0.002305 | Recon Loss: 0.001921 | Commit Loss: 0.000767 | Perplexity: 990.120870
2025-09-14 22:16:13,818 Stage: Train 0.5 | Epoch: 157 | Iter: 477800 | Total Loss: 0.002267 | Recon Loss: 0.001884 | Commit Loss: 0.000766 | Perplexity: 984.676691
2025-09-14 22:16:22,607 Stage: Train 0.5 | Epoch: 157 | Iter: 478000 | Total Loss: 0.002245 | Recon Loss: 0.001862 | Commit Loss: 0.000767 | Perplexity: 987.771418
2025-09-14 22:16:31,392 Stage: Train 0.5 | Epoch: 157 | Iter: 478200 | Total Loss: 0.002277 | Recon Loss: 0.001895 | Commit Loss: 0.000763 | Perplexity: 984.855696
2025-09-14 22:16:40,188 Stage: Train 0.5 | Epoch: 157 | Iter: 478400 | Total Loss: 0.002262 | Recon Loss: 0.001878 | Commit Loss: 0.000769 | Perplexity: 986.314363
2025-09-14 22:16:48,971 Stage: Train 0.5 | Epoch: 157 | Iter: 478600 | Total Loss: 0.002283 | Recon Loss: 0.001899 | Commit Loss: 0.000768 | Perplexity: 987.740558
2025-09-14 22:16:57,750 Stage: Train 0.5 | Epoch: 157 | Iter: 478800 | Total Loss: 0.002292 | Recon Loss: 0.001909 | Commit Loss: 0.000766 | Perplexity: 983.496262
2025-09-14 22:17:06,519 Stage: Train 0.5 | Epoch: 157 | Iter: 479000 | Total Loss: 0.002229 | Recon Loss: 0.001847 | Commit Loss: 0.000765 | Perplexity: 983.426012
2025-09-14 22:17:15,307 Stage: Train 0.5 | Epoch: 157 | Iter: 479200 | Total Loss: 0.002321 | Recon Loss: 0.001943 | Commit Loss: 0.000757 | Perplexity: 983.728796
2025-09-14 22:17:24,092 Stage: Train 0.5 | Epoch: 157 | Iter: 479400 | Total Loss: 0.002218 | Recon Loss: 0.001835 | Commit Loss: 0.000767 | Perplexity: 986.308146
2025-09-14 22:17:32,860 Stage: Train 0.5 | Epoch: 157 | Iter: 479600 | Total Loss: 0.002266 | Recon Loss: 0.001884 | Commit Loss: 0.000764 | Perplexity: 982.772454
2025-09-14 22:17:41,618 Stage: Train 0.5 | Epoch: 157 | Iter: 479800 | Total Loss: 0.002297 | Recon Loss: 0.001914 | Commit Loss: 0.000766 | Perplexity: 982.112544
2025-09-14 22:17:50,408 Stage: Train 0.5 | Epoch: 157 | Iter: 480000 | Total Loss: 0.002253 | Recon Loss: 0.001869 | Commit Loss: 0.000767 | Perplexity: 990.917513
2025-09-14 22:17:50,408 Saving model at iteration 480000
2025-09-14 22:17:50,888 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_158_step_480000
2025-09-14 22:17:51,125 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_158_step_480000/pytorch_model.bin
2025-09-14 22:17:51,496 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_158_step_480000/optimizer.bin
2025-09-14 22:17:51,496 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_158_step_480000/scheduler.bin
2025-09-14 22:17:51,497 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_158_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 158/165 [5:49:45<15:33, 133.38s/it]2025-09-14 22:18:00,664 Stage: Train 0.5 | Epoch: 158 | Iter: 480200 | Total Loss: 0.002276 | Recon Loss: 0.001898 | Commit Loss: 0.000755 | Perplexity: 981.234924
2025-09-14 22:18:09,449 Stage: Train 0.5 | Epoch: 158 | Iter: 480400 | Total Loss: 0.002263 | Recon Loss: 0.001881 | Commit Loss: 0.000764 | Perplexity: 983.440928
2025-09-14 22:18:18,232 Stage: Train 0.5 | Epoch: 158 | Iter: 480600 | Total Loss: 0.002289 | Recon Loss: 0.001905 | Commit Loss: 0.000768 | Perplexity: 990.212335
2025-09-14 22:18:27,019 Stage: Train 0.5 | Epoch: 158 | Iter: 480800 | Total Loss: 0.002255 | Recon Loss: 0.001870 | Commit Loss: 0.000769 | Perplexity: 989.168541
2025-09-14 22:18:35,776 Stage: Train 0.5 | Epoch: 158 | Iter: 481000 | Total Loss: 0.002293 | Recon Loss: 0.001909 | Commit Loss: 0.000767 | Perplexity: 986.794251
2025-09-14 22:18:44,539 Stage: Train 0.5 | Epoch: 158 | Iter: 481200 | Total Loss: 0.002279 | Recon Loss: 0.001895 | Commit Loss: 0.000769 | Perplexity: 987.246104
2025-09-14 22:18:53,302 Stage: Train 0.5 | Epoch: 158 | Iter: 481400 | Total Loss: 0.002253 | Recon Loss: 0.001875 | Commit Loss: 0.000754 | Perplexity: 985.206891
2025-09-14 22:19:02,090 Stage: Train 0.5 | Epoch: 158 | Iter: 481600 | Total Loss: 0.002259 | Recon Loss: 0.001877 | Commit Loss: 0.000763 | Perplexity: 984.206479
2025-09-14 22:19:10,865 Stage: Train 0.5 | Epoch: 158 | Iter: 481800 | Total Loss: 0.002275 | Recon Loss: 0.001892 | Commit Loss: 0.000767 | Perplexity: 983.192169
2025-09-14 22:19:19,648 Stage: Train 0.5 | Epoch: 158 | Iter: 482000 | Total Loss: 0.002269 | Recon Loss: 0.001881 | Commit Loss: 0.000774 | Perplexity: 991.479865
2025-09-14 22:19:28,432 Stage: Train 0.5 | Epoch: 158 | Iter: 482200 | Total Loss: 0.002283 | Recon Loss: 0.001904 | Commit Loss: 0.000759 | Perplexity: 983.270384
2025-09-14 22:19:37,211 Stage: Train 0.5 | Epoch: 158 | Iter: 482400 | Total Loss: 0.002239 | Recon Loss: 0.001858 | Commit Loss: 0.000761 | Perplexity: 983.744496
2025-09-14 22:19:46,008 Stage: Train 0.5 | Epoch: 158 | Iter: 482600 | Total Loss: 0.002245 | Recon Loss: 0.001861 | Commit Loss: 0.000767 | Perplexity: 985.215860
2025-09-14 22:19:54,786 Stage: Train 0.5 | Epoch: 158 | Iter: 482800 | Total Loss: 0.002257 | Recon Loss: 0.001873 | Commit Loss: 0.000769 | Perplexity: 990.554620
2025-09-14 22:20:03,567 Stage: Train 0.5 | Epoch: 158 | Iter: 483000 | Total Loss: 0.002223 | Recon Loss: 0.001840 | Commit Loss: 0.000766 | Perplexity: 984.058804
Trainning Epoch:  96%|█████████▋| 159/165 [5:51:58<13:20, 133.37s/it]2025-09-14 22:20:12,358 Stage: Train 0.5 | Epoch: 159 | Iter: 483200 | Total Loss: 0.002272 | Recon Loss: 0.001892 | Commit Loss: 0.000760 | Perplexity: 984.310581
2025-09-14 22:20:21,148 Stage: Train 0.5 | Epoch: 159 | Iter: 483400 | Total Loss: 0.002254 | Recon Loss: 0.001874 | Commit Loss: 0.000759 | Perplexity: 986.655917
2025-09-14 22:20:29,939 Stage: Train 0.5 | Epoch: 159 | Iter: 483600 | Total Loss: 0.002243 | Recon Loss: 0.001864 | Commit Loss: 0.000758 | Perplexity: 983.720332
2025-09-14 22:20:38,707 Stage: Train 0.5 | Epoch: 159 | Iter: 483800 | Total Loss: 0.002251 | Recon Loss: 0.001868 | Commit Loss: 0.000766 | Perplexity: 988.235737
2025-09-14 22:20:47,462 Stage: Train 0.5 | Epoch: 159 | Iter: 484000 | Total Loss: 0.002271 | Recon Loss: 0.001889 | Commit Loss: 0.000763 | Perplexity: 987.291882
2025-09-14 22:20:56,232 Stage: Train 0.5 | Epoch: 159 | Iter: 484200 | Total Loss: 0.002248 | Recon Loss: 0.001865 | Commit Loss: 0.000766 | Perplexity: 990.053812
2025-09-14 22:21:04,982 Stage: Train 0.5 | Epoch: 159 | Iter: 484400 | Total Loss: 0.002272 | Recon Loss: 0.001892 | Commit Loss: 0.000759 | Perplexity: 990.540482
2025-09-14 22:21:13,749 Stage: Train 0.5 | Epoch: 159 | Iter: 484600 | Total Loss: 0.002274 | Recon Loss: 0.001895 | Commit Loss: 0.000758 | Perplexity: 984.342982
2025-09-14 22:21:22,554 Stage: Train 0.5 | Epoch: 159 | Iter: 484800 | Total Loss: 0.002245 | Recon Loss: 0.001864 | Commit Loss: 0.000762 | Perplexity: 987.797335
2025-09-14 22:21:31,293 Stage: Train 0.5 | Epoch: 159 | Iter: 485000 | Total Loss: 0.002276 | Recon Loss: 0.001898 | Commit Loss: 0.000755 | Perplexity: 980.336069
2025-09-14 22:21:40,037 Stage: Train 0.5 | Epoch: 159 | Iter: 485200 | Total Loss: 0.002279 | Recon Loss: 0.001899 | Commit Loss: 0.000760 | Perplexity: 985.921417
2025-09-14 22:21:48,775 Stage: Train 0.5 | Epoch: 159 | Iter: 485400 | Total Loss: 0.002252 | Recon Loss: 0.001870 | Commit Loss: 0.000764 | Perplexity: 986.667084
2025-09-14 22:21:57,507 Stage: Train 0.5 | Epoch: 159 | Iter: 485600 | Total Loss: 0.002239 | Recon Loss: 0.001858 | Commit Loss: 0.000762 | Perplexity: 985.732615
2025-09-14 22:22:06,257 Stage: Train 0.5 | Epoch: 159 | Iter: 485800 | Total Loss: 0.002255 | Recon Loss: 0.001872 | Commit Loss: 0.000765 | Perplexity: 988.206718
2025-09-14 22:22:14,963 Stage: Train 0.5 | Epoch: 159 | Iter: 486000 | Total Loss: 0.002305 | Recon Loss: 0.001921 | Commit Loss: 0.000767 | Perplexity: 983.695421
Trainning Epoch:  97%|█████████▋| 160/165 [5:54:11<11:06, 133.28s/it]2025-09-14 22:22:23,683 Stage: Train 0.5 | Epoch: 160 | Iter: 486200 | Total Loss: 0.002263 | Recon Loss: 0.001879 | Commit Loss: 0.000769 | Perplexity: 984.285095
2025-09-14 22:22:32,372 Stage: Train 0.5 | Epoch: 160 | Iter: 486400 | Total Loss: 0.002243 | Recon Loss: 0.001867 | Commit Loss: 0.000753 | Perplexity: 986.846732
2025-09-14 22:22:41,059 Stage: Train 0.5 | Epoch: 160 | Iter: 486600 | Total Loss: 0.002245 | Recon Loss: 0.001859 | Commit Loss: 0.000770 | Perplexity: 987.365670
2025-09-14 22:22:49,780 Stage: Train 0.5 | Epoch: 160 | Iter: 486800 | Total Loss: 0.002233 | Recon Loss: 0.001850 | Commit Loss: 0.000766 | Perplexity: 992.698070
2025-09-14 22:22:58,466 Stage: Train 0.5 | Epoch: 160 | Iter: 487000 | Total Loss: 0.002263 | Recon Loss: 0.001887 | Commit Loss: 0.000753 | Perplexity: 982.603128
2025-09-14 22:23:07,174 Stage: Train 0.5 | Epoch: 160 | Iter: 487200 | Total Loss: 0.002306 | Recon Loss: 0.001924 | Commit Loss: 0.000764 | Perplexity: 988.303153
2025-09-14 22:23:15,879 Stage: Train 0.5 | Epoch: 160 | Iter: 487400 | Total Loss: 0.002297 | Recon Loss: 0.001919 | Commit Loss: 0.000757 | Perplexity: 983.041908
2025-09-14 22:23:24,550 Stage: Train 0.5 | Epoch: 160 | Iter: 487600 | Total Loss: 0.002231 | Recon Loss: 0.001855 | Commit Loss: 0.000751 | Perplexity: 981.806197
2025-09-14 22:23:33,259 Stage: Train 0.5 | Epoch: 160 | Iter: 487800 | Total Loss: 0.002287 | Recon Loss: 0.001903 | Commit Loss: 0.000770 | Perplexity: 987.785508
2025-09-14 22:23:41,977 Stage: Train 0.5 | Epoch: 160 | Iter: 488000 | Total Loss: 0.002234 | Recon Loss: 0.001857 | Commit Loss: 0.000753 | Perplexity: 982.990029
2025-09-14 22:23:50,671 Stage: Train 0.5 | Epoch: 160 | Iter: 488200 | Total Loss: 0.002298 | Recon Loss: 0.001915 | Commit Loss: 0.000766 | Perplexity: 988.027205
2025-09-14 22:23:59,358 Stage: Train 0.5 | Epoch: 160 | Iter: 488400 | Total Loss: 0.002255 | Recon Loss: 0.001872 | Commit Loss: 0.000766 | Perplexity: 988.910992
2025-09-14 22:24:08,060 Stage: Train 0.5 | Epoch: 160 | Iter: 488600 | Total Loss: 0.002227 | Recon Loss: 0.001847 | Commit Loss: 0.000761 | Perplexity: 987.579235
2025-09-14 22:24:16,805 Stage: Train 0.5 | Epoch: 160 | Iter: 488800 | Total Loss: 0.002305 | Recon Loss: 0.001924 | Commit Loss: 0.000760 | Perplexity: 982.678635
2025-09-14 22:24:25,548 Stage: Train 0.5 | Epoch: 160 | Iter: 489000 | Total Loss: 0.002268 | Recon Loss: 0.001886 | Commit Loss: 0.000764 | Perplexity: 990.063134
Trainning Epoch:  98%|█████████▊| 161/165 [5:56:24<08:51, 132.97s/it]2025-09-14 22:24:34,304 Stage: Train 0.5 | Epoch: 161 | Iter: 489200 | Total Loss: 0.002238 | Recon Loss: 0.001856 | Commit Loss: 0.000764 | Perplexity: 985.876807
2025-09-14 22:24:43,015 Stage: Train 0.5 | Epoch: 161 | Iter: 489400 | Total Loss: 0.002252 | Recon Loss: 0.001873 | Commit Loss: 0.000759 | Perplexity: 989.863517
2025-09-14 22:24:51,723 Stage: Train 0.5 | Epoch: 161 | Iter: 489600 | Total Loss: 0.002251 | Recon Loss: 0.001870 | Commit Loss: 0.000762 | Perplexity: 988.693109
2025-09-14 22:25:00,426 Stage: Train 0.5 | Epoch: 161 | Iter: 489800 | Total Loss: 0.002315 | Recon Loss: 0.001936 | Commit Loss: 0.000758 | Perplexity: 982.385031
2025-09-14 22:25:09,122 Stage: Train 0.5 | Epoch: 161 | Iter: 490000 | Total Loss: 0.002232 | Recon Loss: 0.001852 | Commit Loss: 0.000761 | Perplexity: 986.401684
2025-09-14 22:25:17,846 Stage: Train 0.5 | Epoch: 161 | Iter: 490200 | Total Loss: 0.002208 | Recon Loss: 0.001828 | Commit Loss: 0.000759 | Perplexity: 985.975123
2025-09-14 22:25:26,572 Stage: Train 0.5 | Epoch: 161 | Iter: 490400 | Total Loss: 0.002256 | Recon Loss: 0.001876 | Commit Loss: 0.000760 | Perplexity: 986.828547
2025-09-14 22:25:35,265 Stage: Train 0.5 | Epoch: 161 | Iter: 490600 | Total Loss: 0.002237 | Recon Loss: 0.001860 | Commit Loss: 0.000755 | Perplexity: 984.965798
2025-09-14 22:25:43,962 Stage: Train 0.5 | Epoch: 161 | Iter: 490800 | Total Loss: 0.002254 | Recon Loss: 0.001873 | Commit Loss: 0.000760 | Perplexity: 988.438286
2025-09-14 22:25:52,652 Stage: Train 0.5 | Epoch: 161 | Iter: 491000 | Total Loss: 0.002267 | Recon Loss: 0.001886 | Commit Loss: 0.000762 | Perplexity: 987.264105
2025-09-14 22:26:01,390 Stage: Train 0.5 | Epoch: 161 | Iter: 491200 | Total Loss: 0.002261 | Recon Loss: 0.001880 | Commit Loss: 0.000762 | Perplexity: 982.756673
2025-09-14 22:26:10,124 Stage: Train 0.5 | Epoch: 161 | Iter: 491400 | Total Loss: 0.002246 | Recon Loss: 0.001863 | Commit Loss: 0.000765 | Perplexity: 988.010044
2025-09-14 22:26:18,841 Stage: Train 0.5 | Epoch: 161 | Iter: 491600 | Total Loss: 0.002254 | Recon Loss: 0.001875 | Commit Loss: 0.000757 | Perplexity: 986.483121
2025-09-14 22:26:27,564 Stage: Train 0.5 | Epoch: 161 | Iter: 491800 | Total Loss: 0.002259 | Recon Loss: 0.001879 | Commit Loss: 0.000760 | Perplexity: 983.375733
2025-09-14 22:26:36,289 Stage: Train 0.5 | Epoch: 161 | Iter: 492000 | Total Loss: 0.002252 | Recon Loss: 0.001875 | Commit Loss: 0.000753 | Perplexity: 981.611458
Trainning Epoch:  98%|█████████▊| 162/165 [5:58:36<06:38, 132.80s/it]2025-09-14 22:26:45,048 Stage: Train 0.5 | Epoch: 162 | Iter: 492200 | Total Loss: 0.002239 | Recon Loss: 0.001857 | Commit Loss: 0.000765 | Perplexity: 988.148487
2025-09-14 22:26:53,786 Stage: Train 0.5 | Epoch: 162 | Iter: 492400 | Total Loss: 0.002238 | Recon Loss: 0.001857 | Commit Loss: 0.000761 | Perplexity: 994.811808
2025-09-14 22:27:02,504 Stage: Train 0.5 | Epoch: 162 | Iter: 492600 | Total Loss: 0.002246 | Recon Loss: 0.001871 | Commit Loss: 0.000749 | Perplexity: 984.494000
2025-09-14 22:27:11,207 Stage: Train 0.5 | Epoch: 162 | Iter: 492800 | Total Loss: 0.002235 | Recon Loss: 0.001852 | Commit Loss: 0.000764 | Perplexity: 987.976110
2025-09-14 22:27:19,960 Stage: Train 0.5 | Epoch: 162 | Iter: 493000 | Total Loss: 0.002251 | Recon Loss: 0.001868 | Commit Loss: 0.000767 | Perplexity: 990.463395
2025-09-14 22:27:28,674 Stage: Train 0.5 | Epoch: 162 | Iter: 493200 | Total Loss: 0.002220 | Recon Loss: 0.001839 | Commit Loss: 0.000762 | Perplexity: 989.722880
2025-09-14 22:27:37,360 Stage: Train 0.5 | Epoch: 162 | Iter: 493400 | Total Loss: 0.002247 | Recon Loss: 0.001869 | Commit Loss: 0.000755 | Perplexity: 983.614776
2025-09-14 22:27:46,099 Stage: Train 0.5 | Epoch: 162 | Iter: 493600 | Total Loss: 0.002258 | Recon Loss: 0.001875 | Commit Loss: 0.000767 | Perplexity: 988.169579
2025-09-14 22:27:54,811 Stage: Train 0.5 | Epoch: 162 | Iter: 493800 | Total Loss: 0.002251 | Recon Loss: 0.001869 | Commit Loss: 0.000763 | Perplexity: 989.206532
2025-09-14 22:28:03,493 Stage: Train 0.5 | Epoch: 162 | Iter: 494000 | Total Loss: 0.002253 | Recon Loss: 0.001873 | Commit Loss: 0.000758 | Perplexity: 982.951750
2025-09-14 22:28:12,202 Stage: Train 0.5 | Epoch: 162 | Iter: 494200 | Total Loss: 0.002251 | Recon Loss: 0.001875 | Commit Loss: 0.000753 | Perplexity: 983.462553
2025-09-14 22:28:20,915 Stage: Train 0.5 | Epoch: 162 | Iter: 494400 | Total Loss: 0.002245 | Recon Loss: 0.001867 | Commit Loss: 0.000757 | Perplexity: 988.323708
2025-09-14 22:28:29,653 Stage: Train 0.5 | Epoch: 162 | Iter: 494600 | Total Loss: 0.002263 | Recon Loss: 0.001882 | Commit Loss: 0.000763 | Perplexity: 988.270010
2025-09-14 22:28:38,377 Stage: Train 0.5 | Epoch: 162 | Iter: 494800 | Total Loss: 0.002271 | Recon Loss: 0.001893 | Commit Loss: 0.000755 | Perplexity: 982.579917
2025-09-14 22:28:47,108 Stage: Train 0.5 | Epoch: 162 | Iter: 495000 | Total Loss: 0.002259 | Recon Loss: 0.001881 | Commit Loss: 0.000755 | Perplexity: 986.537368
Trainning Epoch:  99%|█████████▉| 163/165 [6:00:49<04:25, 132.69s/it]2025-09-14 22:28:55,826 Stage: Train 0.5 | Epoch: 163 | Iter: 495200 | Total Loss: 0.002239 | Recon Loss: 0.001859 | Commit Loss: 0.000760 | Perplexity: 986.648397
2025-09-14 22:29:04,516 Stage: Train 0.5 | Epoch: 163 | Iter: 495400 | Total Loss: 0.002274 | Recon Loss: 0.001896 | Commit Loss: 0.000756 | Perplexity: 985.690795
2025-09-14 22:29:13,250 Stage: Train 0.5 | Epoch: 163 | Iter: 495600 | Total Loss: 0.002234 | Recon Loss: 0.001859 | Commit Loss: 0.000752 | Perplexity: 981.531198
2025-09-14 22:29:21,948 Stage: Train 0.5 | Epoch: 163 | Iter: 495800 | Total Loss: 0.002224 | Recon Loss: 0.001847 | Commit Loss: 0.000752 | Perplexity: 984.313113
2025-09-14 22:29:30,665 Stage: Train 0.5 | Epoch: 163 | Iter: 496000 | Total Loss: 0.002213 | Recon Loss: 0.001833 | Commit Loss: 0.000760 | Perplexity: 984.667239
2025-09-14 22:29:39,412 Stage: Train 0.5 | Epoch: 163 | Iter: 496200 | Total Loss: 0.002213 | Recon Loss: 0.001835 | Commit Loss: 0.000756 | Perplexity: 984.794129
2025-09-14 22:29:48,079 Stage: Train 0.5 | Epoch: 163 | Iter: 496400 | Total Loss: 0.002290 | Recon Loss: 0.001913 | Commit Loss: 0.000754 | Perplexity: 986.745213
2025-09-14 22:29:56,861 Stage: Train 0.5 | Epoch: 163 | Iter: 496600 | Total Loss: 0.002256 | Recon Loss: 0.001876 | Commit Loss: 0.000759 | Perplexity: 988.672523
2025-09-14 22:30:05,578 Stage: Train 0.5 | Epoch: 163 | Iter: 496800 | Total Loss: 0.002279 | Recon Loss: 0.001898 | Commit Loss: 0.000762 | Perplexity: 994.449468
2025-09-14 22:30:14,291 Stage: Train 0.5 | Epoch: 163 | Iter: 497000 | Total Loss: 0.002270 | Recon Loss: 0.001894 | Commit Loss: 0.000752 | Perplexity: 983.147200
2025-09-14 22:30:23,012 Stage: Train 0.5 | Epoch: 163 | Iter: 497200 | Total Loss: 0.002209 | Recon Loss: 0.001833 | Commit Loss: 0.000751 | Perplexity: 986.316101
2025-09-14 22:30:31,741 Stage: Train 0.5 | Epoch: 163 | Iter: 497400 | Total Loss: 0.002256 | Recon Loss: 0.001880 | Commit Loss: 0.000752 | Perplexity: 983.307032
2025-09-14 22:30:40,483 Stage: Train 0.5 | Epoch: 163 | Iter: 497600 | Total Loss: 0.002242 | Recon Loss: 0.001863 | Commit Loss: 0.000758 | Perplexity: 987.980281
2025-09-14 22:30:49,201 Stage: Train 0.5 | Epoch: 163 | Iter: 497800 | Total Loss: 0.002249 | Recon Loss: 0.001867 | Commit Loss: 0.000765 | Perplexity: 991.906045
2025-09-14 22:30:57,889 Stage: Train 0.5 | Epoch: 163 | Iter: 498000 | Total Loss: 0.002227 | Recon Loss: 0.001852 | Commit Loss: 0.000751 | Perplexity: 980.445737
2025-09-14 22:31:06,618 Stage: Train 0.5 | Epoch: 163 | Iter: 498200 | Total Loss: 0.002330 | Recon Loss: 0.001950 | Commit Loss: 0.000760 | Perplexity: 986.287679
Trainning Epoch:  99%|█████████▉| 164/165 [6:03:01<02:12, 132.62s/it]2025-09-14 22:31:15,326 Stage: Train 0.5 | Epoch: 164 | Iter: 498400 | Total Loss: 0.002241 | Recon Loss: 0.001862 | Commit Loss: 0.000758 | Perplexity: 986.684366
2025-09-14 22:31:24,044 Stage: Train 0.5 | Epoch: 164 | Iter: 498600 | Total Loss: 0.002307 | Recon Loss: 0.001930 | Commit Loss: 0.000755 | Perplexity: 987.878290
2025-09-14 22:31:32,758 Stage: Train 0.5 | Epoch: 164 | Iter: 498800 | Total Loss: 0.002247 | Recon Loss: 0.001870 | Commit Loss: 0.000755 | Perplexity: 985.491214
2025-09-14 22:31:41,469 Stage: Train 0.5 | Epoch: 164 | Iter: 499000 | Total Loss: 0.002238 | Recon Loss: 0.001859 | Commit Loss: 0.000758 | Perplexity: 987.938220
2025-09-14 22:31:50,200 Stage: Train 0.5 | Epoch: 164 | Iter: 499200 | Total Loss: 0.002211 | Recon Loss: 0.001831 | Commit Loss: 0.000759 | Perplexity: 989.695423
2025-09-14 22:31:58,931 Stage: Train 0.5 | Epoch: 164 | Iter: 499400 | Total Loss: 0.002267 | Recon Loss: 0.001888 | Commit Loss: 0.000759 | Perplexity: 986.346917
2025-09-14 22:32:07,662 Stage: Train 0.5 | Epoch: 164 | Iter: 499600 | Total Loss: 0.002216 | Recon Loss: 0.001840 | Commit Loss: 0.000752 | Perplexity: 984.450339
2025-09-14 22:32:16,388 Stage: Train 0.5 | Epoch: 164 | Iter: 499800 | Total Loss: 0.002250 | Recon Loss: 0.001875 | Commit Loss: 0.000751 | Perplexity: 984.580393
2025-09-14 22:32:25,128 Stage: Train 0.5 | Epoch: 164 | Iter: 500000 | Total Loss: 0.002265 | Recon Loss: 0.001887 | Commit Loss: 0.000755 | Perplexity: 983.951842
2025-09-14 22:32:25,128 Saving model at iteration 500000
2025-09-14 22:32:25,628 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_165_step_500000
2025-09-14 22:32:25,861 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_165_step_500000/pytorch_model.bin
2025-09-14 22:32:26,223 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_165_step_500000/optimizer.bin
2025-09-14 22:32:26,224 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_165_step_500000/scheduler.bin
2025-09-14 22:32:26,224 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb4096x2048/models/checkpoint_epoch_165_step_500000/random_states_0.pkl
Trainning Epoch:  99%|█████████▉| 164/165 [6:04:19<02:13, 133.29s/it]
2025-09-14 22:32:26,226 Training finished
