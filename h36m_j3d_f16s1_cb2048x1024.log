[2025-09-14 16:22:27,717] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-09-14 16:22:29,069 
python train_vqvae.py --data_mode joint3d --load_data_file /data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl --num_frames 16 --sample_stride 1 --project_dir vqvae_experiment/h36m_j3d_f16s1_cb2048x1024 --not_find_unused_parameters --nb_code 2048 --codebook_dim 1024
2025-09-14 16:22:48,551 Data loaded with 97196 samples and 1559752 frames
2025-09-14 16:22:49,309 Number of trainable parameters: 29.446659 M
2025-09-14 16:22:49,309 Args: Namespace(data_root='./', num_frames=16, batch_size=32, max_epoch=1000000000.0, total_iter=500000, world_size=1, rank=0, save_interval=20000, warm_up_iter=5000, print_iter=200, learning_rate=0.0002, lr_schedule=[300000], gamma=0.05, weight_decay=0.0001, resume_pth='', device='cuda', project_config='', allow_tf32=False, project_dir='vqvae_experiment/h36m_j3d_f16s1_cb2048x1024', seed=6666, commit_ratio=0.5, nb_code=2048, codebook_dim=1024, load_data_file='/data2/wxs/DATASETS/Human3.6M_for_MotionBERT/h36m_sh_conf_cam_source_final.pkl', data_mode='joint3d', not_find_unused_parameters=True, sample_stride=1)
Trainning Epoch:   0%|          | 0/165 [00:00<?, ?it/s]2025-09-14 16:22:58,473 current_lr 0.000008 at iteration 200
2025-09-14 16:22:58,517 Stage: Warm Up | Epoch: 0 | Iter: 200 | Total Loss: 0.090995 | Recon Loss: 0.088571 | Commit Loss: 0.004848 | Perplexity: 599.385047
2025-09-14 16:23:06,765 current_lr 0.000016 at iteration 400
2025-09-14 16:23:06,808 Stage: Warm Up | Epoch: 0 | Iter: 400 | Total Loss: 0.050167 | Recon Loss: 0.043189 | Commit Loss: 0.013956 | Perplexity: 569.471104
2025-09-14 16:23:15,120 current_lr 0.000024 at iteration 600
2025-09-14 16:23:15,159 Stage: Warm Up | Epoch: 0 | Iter: 600 | Total Loss: 0.050073 | Recon Loss: 0.036661 | Commit Loss: 0.026823 | Perplexity: 579.082235
2025-09-14 16:23:23,577 current_lr 0.000032 at iteration 800
2025-09-14 16:23:23,620 Stage: Warm Up | Epoch: 0 | Iter: 800 | Total Loss: 0.049897 | Recon Loss: 0.032655 | Commit Loss: 0.034484 | Perplexity: 600.080230
2025-09-14 16:23:31,773 current_lr 0.000040 at iteration 1000
2025-09-14 16:23:31,812 Stage: Warm Up | Epoch: 0 | Iter: 1000 | Total Loss: 0.047020 | Recon Loss: 0.029241 | Commit Loss: 0.035556 | Perplexity: 607.917989
2025-09-14 16:23:40,106 current_lr 0.000048 at iteration 1200
2025-09-14 16:23:40,144 Stage: Warm Up | Epoch: 0 | Iter: 1200 | Total Loss: 0.043419 | Recon Loss: 0.027441 | Commit Loss: 0.031954 | Perplexity: 608.550429
2025-09-14 16:23:48,270 current_lr 0.000056 at iteration 1400
2025-09-14 16:23:48,309 Stage: Warm Up | Epoch: 0 | Iter: 1400 | Total Loss: 0.038030 | Recon Loss: 0.024296 | Commit Loss: 0.027467 | Perplexity: 608.826182
2025-09-14 16:23:56,598 current_lr 0.000064 at iteration 1600
2025-09-14 16:23:56,636 Stage: Warm Up | Epoch: 0 | Iter: 1600 | Total Loss: 0.033085 | Recon Loss: 0.021994 | Commit Loss: 0.022184 | Perplexity: 616.256845
2025-09-14 16:24:04,878 current_lr 0.000072 at iteration 1800
2025-09-14 16:24:04,917 Stage: Warm Up | Epoch: 0 | Iter: 1800 | Total Loss: 0.029844 | Recon Loss: 0.020962 | Commit Loss: 0.017763 | Perplexity: 619.012123
2025-09-14 16:24:12,973 current_lr 0.000080 at iteration 2000
2025-09-14 16:24:13,019 Stage: Warm Up | Epoch: 0 | Iter: 2000 | Total Loss: 0.026715 | Recon Loss: 0.019473 | Commit Loss: 0.014484 | Perplexity: 623.031805
2025-09-14 16:24:21,118 current_lr 0.000088 at iteration 2200
2025-09-14 16:24:21,158 Stage: Warm Up | Epoch: 0 | Iter: 2200 | Total Loss: 0.024848 | Recon Loss: 0.019010 | Commit Loss: 0.011677 | Perplexity: 632.151545
2025-09-14 16:24:29,505 current_lr 0.000096 at iteration 2400
2025-09-14 16:24:29,545 Stage: Warm Up | Epoch: 0 | Iter: 2400 | Total Loss: 0.022932 | Recon Loss: 0.017970 | Commit Loss: 0.009924 | Perplexity: 635.605656
2025-09-14 16:24:37,817 current_lr 0.000104 at iteration 2600
2025-09-14 16:24:37,857 Stage: Warm Up | Epoch: 0 | Iter: 2600 | Total Loss: 0.021771 | Recon Loss: 0.017651 | Commit Loss: 0.008240 | Perplexity: 636.768606
2025-09-14 16:24:46,025 current_lr 0.000112 at iteration 2800
2025-09-14 16:24:46,064 Stage: Warm Up | Epoch: 0 | Iter: 2800 | Total Loss: 0.019882 | Recon Loss: 0.016385 | Commit Loss: 0.006994 | Perplexity: 642.762363
2025-09-14 16:24:54,249 current_lr 0.000120 at iteration 3000
2025-09-14 16:24:54,288 Stage: Warm Up | Epoch: 0 | Iter: 3000 | Total Loss: 0.018749 | Recon Loss: 0.015837 | Commit Loss: 0.005824 | Perplexity: 636.515293
Trainning Epoch:   1%|          | 1/165 [02:06<5:46:17, 126.69s/it]2025-09-14 16:25:02,563 current_lr 0.000128 at iteration 3200
2025-09-14 16:25:02,603 Stage: Warm Up | Epoch: 1 | Iter: 3200 | Total Loss: 0.017637 | Recon Loss: 0.015169 | Commit Loss: 0.004936 | Perplexity: 634.771629
2025-09-14 16:25:10,750 current_lr 0.000136 at iteration 3400
2025-09-14 16:25:10,788 Stage: Warm Up | Epoch: 1 | Iter: 3400 | Total Loss: 0.016233 | Recon Loss: 0.014115 | Commit Loss: 0.004237 | Perplexity: 634.067954
2025-09-14 16:25:19,036 current_lr 0.000144 at iteration 3600
2025-09-14 16:25:19,074 Stage: Warm Up | Epoch: 1 | Iter: 3600 | Total Loss: 0.015765 | Recon Loss: 0.013932 | Commit Loss: 0.003666 | Perplexity: 627.328486
2025-09-14 16:25:27,210 current_lr 0.000152 at iteration 3800
2025-09-14 16:25:27,249 Stage: Warm Up | Epoch: 1 | Iter: 3800 | Total Loss: 0.015774 | Recon Loss: 0.014234 | Commit Loss: 0.003081 | Perplexity: 616.544560
2025-09-14 16:25:35,390 current_lr 0.000160 at iteration 4000
2025-09-14 16:25:35,429 Stage: Warm Up | Epoch: 1 | Iter: 4000 | Total Loss: 0.015234 | Recon Loss: 0.013910 | Commit Loss: 0.002648 | Perplexity: 616.908589
2025-09-14 16:25:43,549 current_lr 0.000168 at iteration 4200
2025-09-14 16:25:43,593 Stage: Warm Up | Epoch: 1 | Iter: 4200 | Total Loss: 0.013470 | Recon Loss: 0.012257 | Commit Loss: 0.002426 | Perplexity: 624.585101
2025-09-14 16:25:51,789 current_lr 0.000176 at iteration 4400
2025-09-14 16:25:51,829 Stage: Warm Up | Epoch: 1 | Iter: 4400 | Total Loss: 0.013911 | Recon Loss: 0.012812 | Commit Loss: 0.002199 | Perplexity: 627.729349
2025-09-14 16:25:59,949 current_lr 0.000184 at iteration 4600
2025-09-14 16:25:59,986 Stage: Warm Up | Epoch: 1 | Iter: 4600 | Total Loss: 0.012917 | Recon Loss: 0.011865 | Commit Loss: 0.002103 | Perplexity: 630.886494
2025-09-14 16:26:08,121 current_lr 0.000192 at iteration 4800
2025-09-14 16:26:08,160 Stage: Warm Up | Epoch: 1 | Iter: 4800 | Total Loss: 0.013090 | Recon Loss: 0.012099 | Commit Loss: 0.001984 | Perplexity: 631.180169
2025-09-14 16:26:16,373 current_lr 0.000200 at iteration 5000
2025-09-14 16:26:16,414 Stage: Warm Up | Epoch: 1 | Iter: 5000 | Total Loss: 0.012869 | Recon Loss: 0.011929 | Commit Loss: 0.001880 | Perplexity: 626.771963
2025-09-14 16:26:24,853 Stage: Train 0.5 | Epoch: 1 | Iter: 5200 | Total Loss: 0.012543 | Recon Loss: 0.011653 | Commit Loss: 0.001781 | Perplexity: 630.423522
2025-09-14 16:26:33,075 Stage: Train 0.5 | Epoch: 1 | Iter: 5400 | Total Loss: 0.012639 | Recon Loss: 0.011762 | Commit Loss: 0.001753 | Perplexity: 627.607653
2025-09-14 16:26:41,253 Stage: Train 0.5 | Epoch: 1 | Iter: 5600 | Total Loss: 0.011313 | Recon Loss: 0.010495 | Commit Loss: 0.001637 | Perplexity: 629.011364
2025-09-14 16:26:49,491 Stage: Train 0.5 | Epoch: 1 | Iter: 5800 | Total Loss: 0.011090 | Recon Loss: 0.010266 | Commit Loss: 0.001648 | Perplexity: 622.881168
2025-09-14 16:26:57,715 Stage: Train 0.5 | Epoch: 1 | Iter: 6000 | Total Loss: 0.011278 | Recon Loss: 0.010491 | Commit Loss: 0.001574 | Perplexity: 624.296182
Trainning Epoch:   1%|          | 2/165 [04:11<5:41:18, 125.64s/it]2025-09-14 16:27:05,968 Stage: Train 0.5 | Epoch: 2 | Iter: 6200 | Total Loss: 0.010464 | Recon Loss: 0.009684 | Commit Loss: 0.001561 | Perplexity: 634.233237
2025-09-14 16:27:14,117 Stage: Train 0.5 | Epoch: 2 | Iter: 6400 | Total Loss: 0.010706 | Recon Loss: 0.009930 | Commit Loss: 0.001552 | Perplexity: 635.919730
2025-09-14 16:27:22,292 Stage: Train 0.5 | Epoch: 2 | Iter: 6600 | Total Loss: 0.009826 | Recon Loss: 0.009035 | Commit Loss: 0.001582 | Perplexity: 639.732028
2025-09-14 16:27:30,670 Stage: Train 0.5 | Epoch: 2 | Iter: 6800 | Total Loss: 0.010317 | Recon Loss: 0.009562 | Commit Loss: 0.001511 | Perplexity: 639.473483
2025-09-14 16:27:38,908 Stage: Train 0.5 | Epoch: 2 | Iter: 7000 | Total Loss: 0.009919 | Recon Loss: 0.009160 | Commit Loss: 0.001519 | Perplexity: 638.150497
2025-09-14 16:27:47,396 Stage: Train 0.5 | Epoch: 2 | Iter: 7200 | Total Loss: 0.009770 | Recon Loss: 0.009043 | Commit Loss: 0.001454 | Perplexity: 639.755064
2025-09-14 16:27:55,485 Stage: Train 0.5 | Epoch: 2 | Iter: 7400 | Total Loss: 0.009034 | Recon Loss: 0.008281 | Commit Loss: 0.001507 | Perplexity: 642.205006
2025-09-14 16:28:03,601 Stage: Train 0.5 | Epoch: 2 | Iter: 7600 | Total Loss: 0.009445 | Recon Loss: 0.008742 | Commit Loss: 0.001406 | Perplexity: 646.740189
2025-09-14 16:28:11,716 Stage: Train 0.5 | Epoch: 2 | Iter: 7800 | Total Loss: 0.009302 | Recon Loss: 0.008588 | Commit Loss: 0.001428 | Perplexity: 653.049577
2025-09-14 16:28:19,840 Stage: Train 0.5 | Epoch: 2 | Iter: 8000 | Total Loss: 0.009093 | Recon Loss: 0.008400 | Commit Loss: 0.001386 | Perplexity: 656.530827
2025-09-14 16:28:27,929 Stage: Train 0.5 | Epoch: 2 | Iter: 8200 | Total Loss: 0.008709 | Recon Loss: 0.008016 | Commit Loss: 0.001386 | Perplexity: 658.123064
2025-09-14 16:28:35,981 Stage: Train 0.5 | Epoch: 2 | Iter: 8400 | Total Loss: 0.008617 | Recon Loss: 0.007927 | Commit Loss: 0.001381 | Perplexity: 659.996388
2025-09-14 16:28:44,091 Stage: Train 0.5 | Epoch: 2 | Iter: 8600 | Total Loss: 0.008623 | Recon Loss: 0.007938 | Commit Loss: 0.001370 | Perplexity: 661.314899
2025-09-14 16:28:52,199 Stage: Train 0.5 | Epoch: 2 | Iter: 8800 | Total Loss: 0.008497 | Recon Loss: 0.007799 | Commit Loss: 0.001397 | Perplexity: 665.063608
2025-09-14 16:29:00,316 Stage: Train 0.5 | Epoch: 2 | Iter: 9000 | Total Loss: 0.008441 | Recon Loss: 0.007729 | Commit Loss: 0.001424 | Perplexity: 658.900834
Trainning Epoch:   2%|▏         | 3/165 [06:15<5:37:15, 124.91s/it]2025-09-14 16:29:08,455 Stage: Train 0.5 | Epoch: 3 | Iter: 9200 | Total Loss: 0.008379 | Recon Loss: 0.007672 | Commit Loss: 0.001413 | Perplexity: 660.992880
2025-09-14 16:29:16,581 Stage: Train 0.5 | Epoch: 3 | Iter: 9400 | Total Loss: 0.008439 | Recon Loss: 0.007746 | Commit Loss: 0.001387 | Perplexity: 661.352054
2025-09-14 16:29:24,708 Stage: Train 0.5 | Epoch: 3 | Iter: 9600 | Total Loss: 0.008230 | Recon Loss: 0.007536 | Commit Loss: 0.001388 | Perplexity: 663.379979
2025-09-14 16:29:32,850 Stage: Train 0.5 | Epoch: 3 | Iter: 9800 | Total Loss: 0.008036 | Recon Loss: 0.007331 | Commit Loss: 0.001410 | Perplexity: 661.604059
2025-09-14 16:29:41,001 Stage: Train 0.5 | Epoch: 3 | Iter: 10000 | Total Loss: 0.008066 | Recon Loss: 0.007363 | Commit Loss: 0.001406 | Perplexity: 664.723429
2025-09-14 16:29:49,132 Stage: Train 0.5 | Epoch: 3 | Iter: 10200 | Total Loss: 0.008095 | Recon Loss: 0.007396 | Commit Loss: 0.001399 | Perplexity: 665.240540
2025-09-14 16:29:57,276 Stage: Train 0.5 | Epoch: 3 | Iter: 10400 | Total Loss: 0.008001 | Recon Loss: 0.007292 | Commit Loss: 0.001418 | Perplexity: 667.218054
2025-09-14 16:30:05,420 Stage: Train 0.5 | Epoch: 3 | Iter: 10600 | Total Loss: 0.007913 | Recon Loss: 0.007207 | Commit Loss: 0.001413 | Perplexity: 669.296649
2025-09-14 16:30:13,564 Stage: Train 0.5 | Epoch: 3 | Iter: 10800 | Total Loss: 0.007804 | Recon Loss: 0.007105 | Commit Loss: 0.001399 | Perplexity: 670.986730
2025-09-14 16:30:21,708 Stage: Train 0.5 | Epoch: 3 | Iter: 11000 | Total Loss: 0.007839 | Recon Loss: 0.007155 | Commit Loss: 0.001368 | Perplexity: 668.346992
2025-09-14 16:30:29,869 Stage: Train 0.5 | Epoch: 3 | Iter: 11200 | Total Loss: 0.007638 | Recon Loss: 0.006932 | Commit Loss: 0.001412 | Perplexity: 672.182841
2025-09-14 16:30:38,033 Stage: Train 0.5 | Epoch: 3 | Iter: 11400 | Total Loss: 0.007557 | Recon Loss: 0.006859 | Commit Loss: 0.001397 | Perplexity: 673.493683
2025-09-14 16:30:46,171 Stage: Train 0.5 | Epoch: 3 | Iter: 11600 | Total Loss: 0.007538 | Recon Loss: 0.006821 | Commit Loss: 0.001433 | Perplexity: 674.062068
2025-09-14 16:30:54,313 Stage: Train 0.5 | Epoch: 3 | Iter: 11800 | Total Loss: 0.007455 | Recon Loss: 0.006747 | Commit Loss: 0.001414 | Perplexity: 673.258821
2025-09-14 16:31:02,448 Stage: Train 0.5 | Epoch: 3 | Iter: 12000 | Total Loss: 0.007261 | Recon Loss: 0.006550 | Commit Loss: 0.001421 | Perplexity: 673.006104
Trainning Epoch:   2%|▏         | 4/165 [08:19<5:33:52, 124.43s/it]2025-09-14 16:31:10,576 Stage: Train 0.5 | Epoch: 4 | Iter: 12200 | Total Loss: 0.007442 | Recon Loss: 0.006749 | Commit Loss: 0.001388 | Perplexity: 666.438434
2025-09-14 16:31:18,692 Stage: Train 0.5 | Epoch: 4 | Iter: 12400 | Total Loss: 0.007029 | Recon Loss: 0.006316 | Commit Loss: 0.001426 | Perplexity: 674.543021
2025-09-14 16:31:26,816 Stage: Train 0.5 | Epoch: 4 | Iter: 12600 | Total Loss: 0.007261 | Recon Loss: 0.006561 | Commit Loss: 0.001400 | Perplexity: 670.971932
2025-09-14 16:31:34,940 Stage: Train 0.5 | Epoch: 4 | Iter: 12800 | Total Loss: 0.007327 | Recon Loss: 0.006622 | Commit Loss: 0.001410 | Perplexity: 673.295794
2025-09-14 16:31:43,028 Stage: Train 0.5 | Epoch: 4 | Iter: 13000 | Total Loss: 0.006988 | Recon Loss: 0.006282 | Commit Loss: 0.001412 | Perplexity: 672.335494
2025-09-14 16:31:51,190 Stage: Train 0.5 | Epoch: 4 | Iter: 13200 | Total Loss: 0.006938 | Recon Loss: 0.006230 | Commit Loss: 0.001416 | Perplexity: 671.388742
2025-09-14 16:31:59,296 Stage: Train 0.5 | Epoch: 4 | Iter: 13400 | Total Loss: 0.007023 | Recon Loss: 0.006310 | Commit Loss: 0.001427 | Perplexity: 672.901319
2025-09-14 16:32:07,457 Stage: Train 0.5 | Epoch: 4 | Iter: 13600 | Total Loss: 0.006919 | Recon Loss: 0.006236 | Commit Loss: 0.001365 | Perplexity: 673.053226
2025-09-14 16:32:15,609 Stage: Train 0.5 | Epoch: 4 | Iter: 13800 | Total Loss: 0.006848 | Recon Loss: 0.006140 | Commit Loss: 0.001416 | Perplexity: 672.806501
2025-09-14 16:32:23,757 Stage: Train 0.5 | Epoch: 4 | Iter: 14000 | Total Loss: 0.006854 | Recon Loss: 0.006161 | Commit Loss: 0.001385 | Perplexity: 673.561099
2025-09-14 16:32:31,904 Stage: Train 0.5 | Epoch: 4 | Iter: 14200 | Total Loss: 0.007127 | Recon Loss: 0.006430 | Commit Loss: 0.001394 | Perplexity: 672.481013
2025-09-14 16:32:40,044 Stage: Train 0.5 | Epoch: 4 | Iter: 14400 | Total Loss: 0.006764 | Recon Loss: 0.006065 | Commit Loss: 0.001397 | Perplexity: 673.721930
2025-09-14 16:32:48,220 Stage: Train 0.5 | Epoch: 4 | Iter: 14600 | Total Loss: 0.006556 | Recon Loss: 0.005872 | Commit Loss: 0.001368 | Perplexity: 674.274752
2025-09-14 16:32:56,372 Stage: Train 0.5 | Epoch: 4 | Iter: 14800 | Total Loss: 0.006685 | Recon Loss: 0.005995 | Commit Loss: 0.001380 | Perplexity: 673.775438
2025-09-14 16:33:04,524 Stage: Train 0.5 | Epoch: 4 | Iter: 15000 | Total Loss: 0.006562 | Recon Loss: 0.005863 | Commit Loss: 0.001399 | Perplexity: 674.487502
Trainning Epoch:   3%|▎         | 5/165 [10:22<5:31:01, 124.13s/it]2025-09-14 16:33:12,648 Stage: Train 0.5 | Epoch: 5 | Iter: 15200 | Total Loss: 0.006553 | Recon Loss: 0.005865 | Commit Loss: 0.001377 | Perplexity: 674.232873
2025-09-14 16:33:20,816 Stage: Train 0.5 | Epoch: 5 | Iter: 15400 | Total Loss: 0.006697 | Recon Loss: 0.005997 | Commit Loss: 0.001401 | Perplexity: 675.918085
2025-09-14 16:33:28,925 Stage: Train 0.5 | Epoch: 5 | Iter: 15600 | Total Loss: 0.006516 | Recon Loss: 0.005829 | Commit Loss: 0.001374 | Perplexity: 675.062879
2025-09-14 16:33:37,028 Stage: Train 0.5 | Epoch: 5 | Iter: 15800 | Total Loss: 0.006760 | Recon Loss: 0.006080 | Commit Loss: 0.001361 | Perplexity: 675.290242
2025-09-14 16:33:45,141 Stage: Train 0.5 | Epoch: 5 | Iter: 16000 | Total Loss: 0.006388 | Recon Loss: 0.005710 | Commit Loss: 0.001355 | Perplexity: 677.262443
2025-09-14 16:33:53,262 Stage: Train 0.5 | Epoch: 5 | Iter: 16200 | Total Loss: 0.006536 | Recon Loss: 0.005846 | Commit Loss: 0.001381 | Perplexity: 676.624701
2025-09-14 16:34:01,373 Stage: Train 0.5 | Epoch: 5 | Iter: 16400 | Total Loss: 0.006265 | Recon Loss: 0.005566 | Commit Loss: 0.001397 | Perplexity: 677.027305
2025-09-14 16:34:09,474 Stage: Train 0.5 | Epoch: 5 | Iter: 16600 | Total Loss: 0.006417 | Recon Loss: 0.005744 | Commit Loss: 0.001347 | Perplexity: 674.015933
2025-09-14 16:34:17,580 Stage: Train 0.5 | Epoch: 5 | Iter: 16800 | Total Loss: 0.006293 | Recon Loss: 0.005606 | Commit Loss: 0.001373 | Perplexity: 680.180435
2025-09-14 16:34:25,702 Stage: Train 0.5 | Epoch: 5 | Iter: 17000 | Total Loss: 0.006347 | Recon Loss: 0.005668 | Commit Loss: 0.001359 | Perplexity: 676.989356
2025-09-14 16:34:33,809 Stage: Train 0.5 | Epoch: 5 | Iter: 17200 | Total Loss: 0.006358 | Recon Loss: 0.005692 | Commit Loss: 0.001334 | Perplexity: 679.334819
2025-09-14 16:34:41,915 Stage: Train 0.5 | Epoch: 5 | Iter: 17400 | Total Loss: 0.006270 | Recon Loss: 0.005610 | Commit Loss: 0.001321 | Perplexity: 674.288261
2025-09-14 16:34:50,045 Stage: Train 0.5 | Epoch: 5 | Iter: 17600 | Total Loss: 0.006268 | Recon Loss: 0.005580 | Commit Loss: 0.001377 | Perplexity: 678.739187
2025-09-14 16:34:58,216 Stage: Train 0.5 | Epoch: 5 | Iter: 17800 | Total Loss: 0.006059 | Recon Loss: 0.005394 | Commit Loss: 0.001330 | Perplexity: 673.414729
2025-09-14 16:35:06,425 Stage: Train 0.5 | Epoch: 5 | Iter: 18000 | Total Loss: 0.006487 | Recon Loss: 0.005810 | Commit Loss: 0.001354 | Perplexity: 675.302409
2025-09-14 16:35:14,627 Stage: Train 0.5 | Epoch: 5 | Iter: 18200 | Total Loss: 0.006191 | Recon Loss: 0.005522 | Commit Loss: 0.001337 | Perplexity: 672.062095
Trainning Epoch:   4%|▎         | 6/165 [12:26<5:28:24, 123.93s/it]2025-09-14 16:35:22,792 Stage: Train 0.5 | Epoch: 6 | Iter: 18400 | Total Loss: 0.005942 | Recon Loss: 0.005259 | Commit Loss: 0.001366 | Perplexity: 675.182280
2025-09-14 16:35:30,936 Stage: Train 0.5 | Epoch: 6 | Iter: 18600 | Total Loss: 0.006242 | Recon Loss: 0.005582 | Commit Loss: 0.001320 | Perplexity: 673.862408
2025-09-14 16:35:39,064 Stage: Train 0.5 | Epoch: 6 | Iter: 18800 | Total Loss: 0.005930 | Recon Loss: 0.005242 | Commit Loss: 0.001377 | Perplexity: 677.270206
2025-09-14 16:35:47,193 Stage: Train 0.5 | Epoch: 6 | Iter: 19000 | Total Loss: 0.006027 | Recon Loss: 0.005334 | Commit Loss: 0.001387 | Perplexity: 675.297614
2025-09-14 16:35:55,292 Stage: Train 0.5 | Epoch: 6 | Iter: 19200 | Total Loss: 0.006095 | Recon Loss: 0.005425 | Commit Loss: 0.001340 | Perplexity: 676.094938
2025-09-14 16:36:03,397 Stage: Train 0.5 | Epoch: 6 | Iter: 19400 | Total Loss: 0.006080 | Recon Loss: 0.005407 | Commit Loss: 0.001345 | Perplexity: 673.255389
2025-09-14 16:36:11,509 Stage: Train 0.5 | Epoch: 6 | Iter: 19600 | Total Loss: 0.005919 | Recon Loss: 0.005238 | Commit Loss: 0.001363 | Perplexity: 673.646030
2025-09-14 16:36:19,588 Stage: Train 0.5 | Epoch: 6 | Iter: 19800 | Total Loss: 0.006162 | Recon Loss: 0.005485 | Commit Loss: 0.001353 | Perplexity: 672.719894
2025-09-14 16:36:27,704 Stage: Train 0.5 | Epoch: 6 | Iter: 20000 | Total Loss: 0.006035 | Recon Loss: 0.005364 | Commit Loss: 0.001343 | Perplexity: 670.221254
2025-09-14 16:36:27,704 Saving model at iteration 20000
2025-09-14 16:36:27,852 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_7_step_20000
2025-09-14 16:36:27,997 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_7_step_20000/pytorch_model.bin
2025-09-14 16:36:28,241 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_7_step_20000/optimizer.bin
2025-09-14 16:36:28,241 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_7_step_20000/scheduler.bin
2025-09-14 16:36:28,242 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_7_step_20000/random_states_0.pkl
2025-09-14 16:36:36,740 Stage: Train 0.5 | Epoch: 6 | Iter: 20200 | Total Loss: 0.005861 | Recon Loss: 0.005180 | Commit Loss: 0.001362 | Perplexity: 674.903520
2025-09-14 16:36:44,857 Stage: Train 0.5 | Epoch: 6 | Iter: 20400 | Total Loss: 0.005986 | Recon Loss: 0.005318 | Commit Loss: 0.001336 | Perplexity: 672.064240
2025-09-14 16:36:53,000 Stage: Train 0.5 | Epoch: 6 | Iter: 20600 | Total Loss: 0.005914 | Recon Loss: 0.005252 | Commit Loss: 0.001324 | Perplexity: 673.173047
2025-09-14 16:37:01,124 Stage: Train 0.5 | Epoch: 6 | Iter: 20800 | Total Loss: 0.005955 | Recon Loss: 0.005284 | Commit Loss: 0.001341 | Perplexity: 671.464758
2025-09-14 16:37:09,200 Stage: Train 0.5 | Epoch: 6 | Iter: 21000 | Total Loss: 0.005914 | Recon Loss: 0.005242 | Commit Loss: 0.001345 | Perplexity: 673.103264
2025-09-14 16:37:17,295 Stage: Train 0.5 | Epoch: 6 | Iter: 21200 | Total Loss: 0.005701 | Recon Loss: 0.005021 | Commit Loss: 0.001362 | Perplexity: 671.355369
Trainning Epoch:   4%|▍         | 7/165 [14:30<5:26:42, 124.06s/it]2025-09-14 16:37:25,513 Stage: Train 0.5 | Epoch: 7 | Iter: 21400 | Total Loss: 0.005995 | Recon Loss: 0.005320 | Commit Loss: 0.001348 | Perplexity: 668.610845
2025-09-14 16:37:33,612 Stage: Train 0.5 | Epoch: 7 | Iter: 21600 | Total Loss: 0.005816 | Recon Loss: 0.005135 | Commit Loss: 0.001363 | Perplexity: 673.287719
2025-09-14 16:37:41,783 Stage: Train 0.5 | Epoch: 7 | Iter: 21800 | Total Loss: 0.006006 | Recon Loss: 0.005334 | Commit Loss: 0.001344 | Perplexity: 671.379098
2025-09-14 16:37:49,993 Stage: Train 0.5 | Epoch: 7 | Iter: 22000 | Total Loss: 0.005857 | Recon Loss: 0.005189 | Commit Loss: 0.001336 | Perplexity: 670.197996
2025-09-14 16:37:58,300 Stage: Train 0.5 | Epoch: 7 | Iter: 22200 | Total Loss: 0.005738 | Recon Loss: 0.005079 | Commit Loss: 0.001318 | Perplexity: 670.331687
2025-09-14 16:38:06,600 Stage: Train 0.5 | Epoch: 7 | Iter: 22400 | Total Loss: 0.005665 | Recon Loss: 0.004986 | Commit Loss: 0.001357 | Perplexity: 673.833038
2025-09-14 16:38:14,842 Stage: Train 0.5 | Epoch: 7 | Iter: 22600 | Total Loss: 0.005808 | Recon Loss: 0.005122 | Commit Loss: 0.001371 | Perplexity: 669.784068
2025-09-14 16:38:22,936 Stage: Train 0.5 | Epoch: 7 | Iter: 22800 | Total Loss: 0.005679 | Recon Loss: 0.004998 | Commit Loss: 0.001362 | Perplexity: 667.328852
2025-09-14 16:38:31,050 Stage: Train 0.5 | Epoch: 7 | Iter: 23000 | Total Loss: 0.005650 | Recon Loss: 0.004957 | Commit Loss: 0.001387 | Perplexity: 671.811671
2025-09-14 16:38:39,169 Stage: Train 0.5 | Epoch: 7 | Iter: 23200 | Total Loss: 0.005880 | Recon Loss: 0.005202 | Commit Loss: 0.001355 | Perplexity: 668.196506
2025-09-14 16:38:47,273 Stage: Train 0.5 | Epoch: 7 | Iter: 23400 | Total Loss: 0.005708 | Recon Loss: 0.005039 | Commit Loss: 0.001337 | Perplexity: 666.863494
2025-09-14 16:38:55,381 Stage: Train 0.5 | Epoch: 7 | Iter: 23600 | Total Loss: 0.005770 | Recon Loss: 0.005097 | Commit Loss: 0.001344 | Perplexity: 668.956364
2025-09-14 16:39:03,476 Stage: Train 0.5 | Epoch: 7 | Iter: 23800 | Total Loss: 0.005539 | Recon Loss: 0.004852 | Commit Loss: 0.001373 | Perplexity: 670.358838
2025-09-14 16:39:11,586 Stage: Train 0.5 | Epoch: 7 | Iter: 24000 | Total Loss: 0.005770 | Recon Loss: 0.005096 | Commit Loss: 0.001348 | Perplexity: 667.115131
2025-09-14 16:39:19,705 Stage: Train 0.5 | Epoch: 7 | Iter: 24200 | Total Loss: 0.005649 | Recon Loss: 0.004964 | Commit Loss: 0.001370 | Perplexity: 669.098981
Trainning Epoch:   5%|▍         | 8/165 [16:34<5:24:24, 123.98s/it]2025-09-14 16:39:27,824 Stage: Train 0.5 | Epoch: 8 | Iter: 24400 | Total Loss: 0.005636 | Recon Loss: 0.004944 | Commit Loss: 0.001383 | Perplexity: 666.531494
2025-09-14 16:39:35,933 Stage: Train 0.5 | Epoch: 8 | Iter: 24600 | Total Loss: 0.005606 | Recon Loss: 0.004923 | Commit Loss: 0.001365 | Perplexity: 672.741137
2025-09-14 16:39:44,039 Stage: Train 0.5 | Epoch: 8 | Iter: 24800 | Total Loss: 0.005544 | Recon Loss: 0.004863 | Commit Loss: 0.001363 | Perplexity: 670.573172
2025-09-14 16:39:52,144 Stage: Train 0.5 | Epoch: 8 | Iter: 25000 | Total Loss: 0.005478 | Recon Loss: 0.004798 | Commit Loss: 0.001360 | Perplexity: 671.697961
2025-09-14 16:40:00,316 Stage: Train 0.5 | Epoch: 8 | Iter: 25200 | Total Loss: 0.005629 | Recon Loss: 0.004946 | Commit Loss: 0.001367 | Perplexity: 666.838571
2025-09-14 16:40:08,423 Stage: Train 0.5 | Epoch: 8 | Iter: 25400 | Total Loss: 0.005605 | Recon Loss: 0.004937 | Commit Loss: 0.001335 | Perplexity: 666.974985
2025-09-14 16:40:16,529 Stage: Train 0.5 | Epoch: 8 | Iter: 25600 | Total Loss: 0.005545 | Recon Loss: 0.004867 | Commit Loss: 0.001356 | Perplexity: 669.484490
2025-09-14 16:40:24,640 Stage: Train 0.5 | Epoch: 8 | Iter: 25800 | Total Loss: 0.005425 | Recon Loss: 0.004749 | Commit Loss: 0.001352 | Perplexity: 666.940727
2025-09-14 16:40:32,734 Stage: Train 0.5 | Epoch: 8 | Iter: 26000 | Total Loss: 0.005619 | Recon Loss: 0.004942 | Commit Loss: 0.001355 | Perplexity: 669.588291
2025-09-14 16:40:40,889 Stage: Train 0.5 | Epoch: 8 | Iter: 26200 | Total Loss: 0.005447 | Recon Loss: 0.004754 | Commit Loss: 0.001385 | Perplexity: 670.935010
2025-09-14 16:40:49,045 Stage: Train 0.5 | Epoch: 8 | Iter: 26400 | Total Loss: 0.005660 | Recon Loss: 0.004976 | Commit Loss: 0.001368 | Perplexity: 665.140667
2025-09-14 16:40:57,157 Stage: Train 0.5 | Epoch: 8 | Iter: 26600 | Total Loss: 0.005473 | Recon Loss: 0.004791 | Commit Loss: 0.001363 | Perplexity: 666.636266
2025-09-14 16:41:05,263 Stage: Train 0.5 | Epoch: 8 | Iter: 26800 | Total Loss: 0.005451 | Recon Loss: 0.004770 | Commit Loss: 0.001363 | Perplexity: 664.022134
2025-09-14 16:41:13,353 Stage: Train 0.5 | Epoch: 8 | Iter: 27000 | Total Loss: 0.005547 | Recon Loss: 0.004871 | Commit Loss: 0.001351 | Perplexity: 669.128867
2025-09-14 16:41:21,474 Stage: Train 0.5 | Epoch: 8 | Iter: 27200 | Total Loss: 0.005528 | Recon Loss: 0.004838 | Commit Loss: 0.001381 | Perplexity: 668.474626
Trainning Epoch:   5%|▌         | 9/165 [18:37<5:21:47, 123.76s/it]2025-09-14 16:41:29,549 Stage: Train 0.5 | Epoch: 9 | Iter: 27400 | Total Loss: 0.005437 | Recon Loss: 0.004766 | Commit Loss: 0.001343 | Perplexity: 660.396457
2025-09-14 16:41:37,604 Stage: Train 0.5 | Epoch: 9 | Iter: 27600 | Total Loss: 0.005511 | Recon Loss: 0.004839 | Commit Loss: 0.001344 | Perplexity: 665.163589
2025-09-14 16:41:45,689 Stage: Train 0.5 | Epoch: 9 | Iter: 27800 | Total Loss: 0.005426 | Recon Loss: 0.004738 | Commit Loss: 0.001376 | Perplexity: 666.462280
2025-09-14 16:41:53,806 Stage: Train 0.5 | Epoch: 9 | Iter: 28000 | Total Loss: 0.005376 | Recon Loss: 0.004694 | Commit Loss: 0.001366 | Perplexity: 668.071049
2025-09-14 16:42:01,916 Stage: Train 0.5 | Epoch: 9 | Iter: 28200 | Total Loss: 0.005440 | Recon Loss: 0.004774 | Commit Loss: 0.001333 | Perplexity: 665.118520
2025-09-14 16:42:10,045 Stage: Train 0.5 | Epoch: 9 | Iter: 28400 | Total Loss: 0.005393 | Recon Loss: 0.004720 | Commit Loss: 0.001346 | Perplexity: 665.588240
2025-09-14 16:42:18,209 Stage: Train 0.5 | Epoch: 9 | Iter: 28600 | Total Loss: 0.005418 | Recon Loss: 0.004734 | Commit Loss: 0.001368 | Perplexity: 666.405722
2025-09-14 16:42:26,337 Stage: Train 0.5 | Epoch: 9 | Iter: 28800 | Total Loss: 0.005411 | Recon Loss: 0.004733 | Commit Loss: 0.001356 | Perplexity: 666.693348
2025-09-14 16:42:34,443 Stage: Train 0.5 | Epoch: 9 | Iter: 29000 | Total Loss: 0.005305 | Recon Loss: 0.004622 | Commit Loss: 0.001365 | Perplexity: 665.701335
2025-09-14 16:42:42,539 Stage: Train 0.5 | Epoch: 9 | Iter: 29200 | Total Loss: 0.005381 | Recon Loss: 0.004700 | Commit Loss: 0.001361 | Perplexity: 666.207391
2025-09-14 16:42:50,697 Stage: Train 0.5 | Epoch: 9 | Iter: 29400 | Total Loss: 0.005386 | Recon Loss: 0.004704 | Commit Loss: 0.001363 | Perplexity: 666.155067
2025-09-14 16:42:58,794 Stage: Train 0.5 | Epoch: 9 | Iter: 29600 | Total Loss: 0.005342 | Recon Loss: 0.004659 | Commit Loss: 0.001367 | Perplexity: 665.238060
2025-09-14 16:43:06,917 Stage: Train 0.5 | Epoch: 9 | Iter: 29800 | Total Loss: 0.005329 | Recon Loss: 0.004643 | Commit Loss: 0.001372 | Perplexity: 667.548195
2025-09-14 16:43:15,005 Stage: Train 0.5 | Epoch: 9 | Iter: 30000 | Total Loss: 0.005333 | Recon Loss: 0.004641 | Commit Loss: 0.001383 | Perplexity: 667.446372
2025-09-14 16:43:23,129 Stage: Train 0.5 | Epoch: 9 | Iter: 30200 | Total Loss: 0.005257 | Recon Loss: 0.004571 | Commit Loss: 0.001372 | Perplexity: 665.208888
Trainning Epoch:   6%|▌         | 10/165 [20:41<5:19:17, 123.60s/it]2025-09-14 16:43:31,245 Stage: Train 0.5 | Epoch: 10 | Iter: 30400 | Total Loss: 0.005248 | Recon Loss: 0.004560 | Commit Loss: 0.001376 | Perplexity: 662.707219
2025-09-14 16:43:39,341 Stage: Train 0.5 | Epoch: 10 | Iter: 30600 | Total Loss: 0.005298 | Recon Loss: 0.004611 | Commit Loss: 0.001374 | Perplexity: 665.402351
2025-09-14 16:43:47,437 Stage: Train 0.5 | Epoch: 10 | Iter: 30800 | Total Loss: 0.005361 | Recon Loss: 0.004689 | Commit Loss: 0.001344 | Perplexity: 660.964830
2025-09-14 16:43:55,566 Stage: Train 0.5 | Epoch: 10 | Iter: 31000 | Total Loss: 0.005317 | Recon Loss: 0.004635 | Commit Loss: 0.001365 | Perplexity: 662.695408
2025-09-14 16:44:03,664 Stage: Train 0.5 | Epoch: 10 | Iter: 31200 | Total Loss: 0.005273 | Recon Loss: 0.004585 | Commit Loss: 0.001376 | Perplexity: 665.406602
2025-09-14 16:44:11,772 Stage: Train 0.5 | Epoch: 10 | Iter: 31400 | Total Loss: 0.005333 | Recon Loss: 0.004652 | Commit Loss: 0.001362 | Perplexity: 665.756066
2025-09-14 16:44:19,881 Stage: Train 0.5 | Epoch: 10 | Iter: 31600 | Total Loss: 0.005319 | Recon Loss: 0.004638 | Commit Loss: 0.001361 | Perplexity: 664.744113
2025-09-14 16:44:28,008 Stage: Train 0.5 | Epoch: 10 | Iter: 31800 | Total Loss: 0.005169 | Recon Loss: 0.004488 | Commit Loss: 0.001361 | Perplexity: 664.329529
2025-09-14 16:44:36,141 Stage: Train 0.5 | Epoch: 10 | Iter: 32000 | Total Loss: 0.005221 | Recon Loss: 0.004538 | Commit Loss: 0.001366 | Perplexity: 663.595301
2025-09-14 16:44:44,285 Stage: Train 0.5 | Epoch: 10 | Iter: 32200 | Total Loss: 0.005251 | Recon Loss: 0.004562 | Commit Loss: 0.001378 | Perplexity: 663.020673
2025-09-14 16:44:52,400 Stage: Train 0.5 | Epoch: 10 | Iter: 32400 | Total Loss: 0.005089 | Recon Loss: 0.004402 | Commit Loss: 0.001373 | Perplexity: 661.592548
2025-09-14 16:45:00,528 Stage: Train 0.5 | Epoch: 10 | Iter: 32600 | Total Loss: 0.005477 | Recon Loss: 0.004791 | Commit Loss: 0.001373 | Perplexity: 661.820594
2025-09-14 16:45:08,633 Stage: Train 0.5 | Epoch: 10 | Iter: 32800 | Total Loss: 0.005204 | Recon Loss: 0.004520 | Commit Loss: 0.001368 | Perplexity: 661.660574
2025-09-14 16:45:16,785 Stage: Train 0.5 | Epoch: 10 | Iter: 33000 | Total Loss: 0.005131 | Recon Loss: 0.004460 | Commit Loss: 0.001343 | Perplexity: 664.287970
2025-09-14 16:45:24,924 Stage: Train 0.5 | Epoch: 10 | Iter: 33200 | Total Loss: 0.005201 | Recon Loss: 0.004521 | Commit Loss: 0.001361 | Perplexity: 663.980013
2025-09-14 16:45:33,056 Stage: Train 0.5 | Epoch: 10 | Iter: 33400 | Total Loss: 0.005197 | Recon Loss: 0.004530 | Commit Loss: 0.001333 | Perplexity: 657.289923
Trainning Epoch:   7%|▋         | 11/165 [22:44<5:17:03, 123.53s/it]2025-09-14 16:45:41,212 Stage: Train 0.5 | Epoch: 11 | Iter: 33600 | Total Loss: 0.005120 | Recon Loss: 0.004436 | Commit Loss: 0.001368 | Perplexity: 661.248630
2025-09-14 16:45:49,344 Stage: Train 0.5 | Epoch: 11 | Iter: 33800 | Total Loss: 0.005091 | Recon Loss: 0.004405 | Commit Loss: 0.001372 | Perplexity: 660.646974
2025-09-14 16:45:57,425 Stage: Train 0.5 | Epoch: 11 | Iter: 34000 | Total Loss: 0.005084 | Recon Loss: 0.004407 | Commit Loss: 0.001354 | Perplexity: 663.272238
2025-09-14 16:46:05,545 Stage: Train 0.5 | Epoch: 11 | Iter: 34200 | Total Loss: 0.005062 | Recon Loss: 0.004381 | Commit Loss: 0.001362 | Perplexity: 661.797312
2025-09-14 16:46:13,633 Stage: Train 0.5 | Epoch: 11 | Iter: 34400 | Total Loss: 0.005162 | Recon Loss: 0.004483 | Commit Loss: 0.001358 | Perplexity: 659.388112
2025-09-14 16:46:21,758 Stage: Train 0.5 | Epoch: 11 | Iter: 34600 | Total Loss: 0.005073 | Recon Loss: 0.004394 | Commit Loss: 0.001360 | Perplexity: 660.716234
2025-09-14 16:46:29,917 Stage: Train 0.5 | Epoch: 11 | Iter: 34800 | Total Loss: 0.005128 | Recon Loss: 0.004444 | Commit Loss: 0.001368 | Perplexity: 655.777923
2025-09-14 16:46:38,050 Stage: Train 0.5 | Epoch: 11 | Iter: 35000 | Total Loss: 0.005074 | Recon Loss: 0.004392 | Commit Loss: 0.001363 | Perplexity: 659.935905
2025-09-14 16:46:46,170 Stage: Train 0.5 | Epoch: 11 | Iter: 35200 | Total Loss: 0.005200 | Recon Loss: 0.004520 | Commit Loss: 0.001359 | Perplexity: 659.904222
2025-09-14 16:46:54,432 Stage: Train 0.5 | Epoch: 11 | Iter: 35400 | Total Loss: 0.005085 | Recon Loss: 0.004398 | Commit Loss: 0.001374 | Perplexity: 661.253305
2025-09-14 16:47:02,485 Stage: Train 0.5 | Epoch: 11 | Iter: 35600 | Total Loss: 0.005094 | Recon Loss: 0.004411 | Commit Loss: 0.001366 | Perplexity: 661.690575
2025-09-14 16:47:10,318 Stage: Train 0.5 | Epoch: 11 | Iter: 35800 | Total Loss: 0.005037 | Recon Loss: 0.004349 | Commit Loss: 0.001377 | Perplexity: 661.482816
2025-09-14 16:47:18,161 Stage: Train 0.5 | Epoch: 11 | Iter: 36000 | Total Loss: 0.005044 | Recon Loss: 0.004361 | Commit Loss: 0.001365 | Perplexity: 660.325223
2025-09-14 16:47:26,018 Stage: Train 0.5 | Epoch: 11 | Iter: 36200 | Total Loss: 0.005114 | Recon Loss: 0.004431 | Commit Loss: 0.001365 | Perplexity: 659.763012
2025-09-14 16:47:33,885 Stage: Train 0.5 | Epoch: 11 | Iter: 36400 | Total Loss: 0.005100 | Recon Loss: 0.004410 | Commit Loss: 0.001381 | Perplexity: 656.676693
Trainning Epoch:   7%|▋         | 12/165 [24:46<5:14:02, 123.16s/it]2025-09-14 16:47:41,762 Stage: Train 0.5 | Epoch: 12 | Iter: 36600 | Total Loss: 0.005018 | Recon Loss: 0.004336 | Commit Loss: 0.001364 | Perplexity: 656.836420
2025-09-14 16:47:49,602 Stage: Train 0.5 | Epoch: 12 | Iter: 36800 | Total Loss: 0.005070 | Recon Loss: 0.004381 | Commit Loss: 0.001377 | Perplexity: 657.424896
2025-09-14 16:47:57,421 Stage: Train 0.5 | Epoch: 12 | Iter: 37000 | Total Loss: 0.004954 | Recon Loss: 0.004280 | Commit Loss: 0.001347 | Perplexity: 656.540986
2025-09-14 16:48:05,219 Stage: Train 0.5 | Epoch: 12 | Iter: 37200 | Total Loss: 0.005086 | Recon Loss: 0.004390 | Commit Loss: 0.001393 | Perplexity: 657.801205
2025-09-14 16:48:13,009 Stage: Train 0.5 | Epoch: 12 | Iter: 37400 | Total Loss: 0.004985 | Recon Loss: 0.004285 | Commit Loss: 0.001400 | Perplexity: 662.657637
2025-09-14 16:48:20,809 Stage: Train 0.5 | Epoch: 12 | Iter: 37600 | Total Loss: 0.005141 | Recon Loss: 0.004441 | Commit Loss: 0.001400 | Perplexity: 660.862356
2025-09-14 16:48:28,609 Stage: Train 0.5 | Epoch: 12 | Iter: 37800 | Total Loss: 0.005101 | Recon Loss: 0.004424 | Commit Loss: 0.001355 | Perplexity: 658.576152
2025-09-14 16:48:36,456 Stage: Train 0.5 | Epoch: 12 | Iter: 38000 | Total Loss: 0.005001 | Recon Loss: 0.004316 | Commit Loss: 0.001372 | Perplexity: 658.600421
2025-09-14 16:48:44,154 Stage: Train 0.5 | Epoch: 12 | Iter: 38200 | Total Loss: 0.004916 | Recon Loss: 0.004226 | Commit Loss: 0.001381 | Perplexity: 659.343542
2025-09-14 16:48:51,865 Stage: Train 0.5 | Epoch: 12 | Iter: 38400 | Total Loss: 0.005036 | Recon Loss: 0.004338 | Commit Loss: 0.001396 | Perplexity: 660.615771
2025-09-14 16:48:59,573 Stage: Train 0.5 | Epoch: 12 | Iter: 38600 | Total Loss: 0.005016 | Recon Loss: 0.004321 | Commit Loss: 0.001390 | Perplexity: 658.676878
2025-09-14 16:49:07,292 Stage: Train 0.5 | Epoch: 12 | Iter: 38800 | Total Loss: 0.004921 | Recon Loss: 0.004231 | Commit Loss: 0.001382 | Perplexity: 658.718888
2025-09-14 16:49:15,102 Stage: Train 0.5 | Epoch: 12 | Iter: 39000 | Total Loss: 0.005042 | Recon Loss: 0.004349 | Commit Loss: 0.001386 | Perplexity: 659.008928
2025-09-14 16:49:22,909 Stage: Train 0.5 | Epoch: 12 | Iter: 39200 | Total Loss: 0.005052 | Recon Loss: 0.004360 | Commit Loss: 0.001383 | Perplexity: 658.309977
2025-09-14 16:49:30,714 Stage: Train 0.5 | Epoch: 12 | Iter: 39400 | Total Loss: 0.005063 | Recon Loss: 0.004376 | Commit Loss: 0.001373 | Perplexity: 655.969907
Trainning Epoch:   8%|▊         | 13/165 [26:45<5:08:16, 121.69s/it]2025-09-14 16:49:38,610 Stage: Train 0.5 | Epoch: 13 | Iter: 39600 | Total Loss: 0.004911 | Recon Loss: 0.004235 | Commit Loss: 0.001352 | Perplexity: 652.540999
2025-09-14 16:49:46,454 Stage: Train 0.5 | Epoch: 13 | Iter: 39800 | Total Loss: 0.004934 | Recon Loss: 0.004234 | Commit Loss: 0.001399 | Perplexity: 658.843372
2025-09-14 16:49:54,511 Stage: Train 0.5 | Epoch: 13 | Iter: 40000 | Total Loss: 0.004937 | Recon Loss: 0.004239 | Commit Loss: 0.001395 | Perplexity: 659.297598
2025-09-14 16:49:54,511 Saving model at iteration 40000
2025-09-14 16:49:54,701 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_14_step_40000
2025-09-14 16:49:54,837 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_14_step_40000/pytorch_model.bin
2025-09-14 16:49:55,091 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_14_step_40000/optimizer.bin
2025-09-14 16:49:55,091 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_14_step_40000/scheduler.bin
2025-09-14 16:49:55,092 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_14_step_40000/random_states_0.pkl
2025-09-14 16:50:03,434 Stage: Train 0.5 | Epoch: 13 | Iter: 40200 | Total Loss: 0.004969 | Recon Loss: 0.004280 | Commit Loss: 0.001378 | Perplexity: 656.836319
2025-09-14 16:50:11,590 Stage: Train 0.5 | Epoch: 13 | Iter: 40400 | Total Loss: 0.004845 | Recon Loss: 0.004146 | Commit Loss: 0.001398 | Perplexity: 660.987207
2025-09-14 16:50:19,700 Stage: Train 0.5 | Epoch: 13 | Iter: 40600 | Total Loss: 0.004937 | Recon Loss: 0.004254 | Commit Loss: 0.001365 | Perplexity: 656.663264
2025-09-14 16:50:27,827 Stage: Train 0.5 | Epoch: 13 | Iter: 40800 | Total Loss: 0.004888 | Recon Loss: 0.004195 | Commit Loss: 0.001386 | Perplexity: 656.789408
2025-09-14 16:50:35,926 Stage: Train 0.5 | Epoch: 13 | Iter: 41000 | Total Loss: 0.004977 | Recon Loss: 0.004278 | Commit Loss: 0.001397 | Perplexity: 656.945335
2025-09-14 16:50:44,076 Stage: Train 0.5 | Epoch: 13 | Iter: 41200 | Total Loss: 0.004986 | Recon Loss: 0.004281 | Commit Loss: 0.001410 | Perplexity: 661.746187
2025-09-14 16:50:52,174 Stage: Train 0.5 | Epoch: 13 | Iter: 41400 | Total Loss: 0.004996 | Recon Loss: 0.004303 | Commit Loss: 0.001387 | Perplexity: 654.193662
2025-09-14 16:51:00,284 Stage: Train 0.5 | Epoch: 13 | Iter: 41600 | Total Loss: 0.004983 | Recon Loss: 0.004284 | Commit Loss: 0.001399 | Perplexity: 656.520161
2025-09-14 16:51:08,389 Stage: Train 0.5 | Epoch: 13 | Iter: 41800 | Total Loss: 0.004784 | Recon Loss: 0.004092 | Commit Loss: 0.001384 | Perplexity: 655.777027
2025-09-14 16:51:16,466 Stage: Train 0.5 | Epoch: 13 | Iter: 42000 | Total Loss: 0.005006 | Recon Loss: 0.004310 | Commit Loss: 0.001391 | Perplexity: 656.047495
2025-09-14 16:51:24,297 Stage: Train 0.5 | Epoch: 13 | Iter: 42200 | Total Loss: 0.004954 | Recon Loss: 0.004261 | Commit Loss: 0.001384 | Perplexity: 656.953409
2025-09-14 16:51:31,985 Stage: Train 0.5 | Epoch: 13 | Iter: 42400 | Total Loss: 0.004814 | Recon Loss: 0.004125 | Commit Loss: 0.001378 | Perplexity: 653.567040
Trainning Epoch:   8%|▊         | 14/165 [28:48<5:07:12, 122.07s/it]2025-09-14 16:51:39,993 Stage: Train 0.5 | Epoch: 14 | Iter: 42600 | Total Loss: 0.004889 | Recon Loss: 0.004204 | Commit Loss: 0.001370 | Perplexity: 651.008617
2025-09-14 16:51:47,706 Stage: Train 0.5 | Epoch: 14 | Iter: 42800 | Total Loss: 0.004864 | Recon Loss: 0.004178 | Commit Loss: 0.001373 | Perplexity: 652.820896
2025-09-14 16:51:55,481 Stage: Train 0.5 | Epoch: 14 | Iter: 43000 | Total Loss: 0.004888 | Recon Loss: 0.004191 | Commit Loss: 0.001394 | Perplexity: 654.175864
2025-09-14 16:52:03,366 Stage: Train 0.5 | Epoch: 14 | Iter: 43200 | Total Loss: 0.004944 | Recon Loss: 0.004253 | Commit Loss: 0.001382 | Perplexity: 653.409738
2025-09-14 16:52:11,546 Stage: Train 0.5 | Epoch: 14 | Iter: 43400 | Total Loss: 0.004796 | Recon Loss: 0.004098 | Commit Loss: 0.001396 | Perplexity: 655.475309
2025-09-14 16:52:19,673 Stage: Train 0.5 | Epoch: 14 | Iter: 43600 | Total Loss: 0.004842 | Recon Loss: 0.004145 | Commit Loss: 0.001395 | Perplexity: 655.283952
2025-09-14 16:52:27,820 Stage: Train 0.5 | Epoch: 14 | Iter: 43800 | Total Loss: 0.004850 | Recon Loss: 0.004148 | Commit Loss: 0.001403 | Perplexity: 656.133824
2025-09-14 16:52:35,962 Stage: Train 0.5 | Epoch: 14 | Iter: 44000 | Total Loss: 0.004904 | Recon Loss: 0.004203 | Commit Loss: 0.001401 | Perplexity: 652.917778
2025-09-14 16:52:44,034 Stage: Train 0.5 | Epoch: 14 | Iter: 44200 | Total Loss: 0.004833 | Recon Loss: 0.004134 | Commit Loss: 0.001397 | Perplexity: 656.386936
2025-09-14 16:52:52,132 Stage: Train 0.5 | Epoch: 14 | Iter: 44400 | Total Loss: 0.004870 | Recon Loss: 0.004166 | Commit Loss: 0.001408 | Perplexity: 654.186622
2025-09-14 16:53:00,286 Stage: Train 0.5 | Epoch: 14 | Iter: 44600 | Total Loss: 0.004721 | Recon Loss: 0.004002 | Commit Loss: 0.001437 | Perplexity: 658.171261
2025-09-14 16:53:08,401 Stage: Train 0.5 | Epoch: 14 | Iter: 44800 | Total Loss: 0.004977 | Recon Loss: 0.004275 | Commit Loss: 0.001404 | Perplexity: 651.389359
2025-09-14 16:53:16,480 Stage: Train 0.5 | Epoch: 14 | Iter: 45000 | Total Loss: 0.004805 | Recon Loss: 0.004106 | Commit Loss: 0.001397 | Perplexity: 653.686801
2025-09-14 16:53:24,600 Stage: Train 0.5 | Epoch: 14 | Iter: 45200 | Total Loss: 0.004915 | Recon Loss: 0.004224 | Commit Loss: 0.001382 | Perplexity: 651.344560
2025-09-14 16:53:32,425 Stage: Train 0.5 | Epoch: 14 | Iter: 45400 | Total Loss: 0.004833 | Recon Loss: 0.004136 | Commit Loss: 0.001394 | Perplexity: 651.608510
Trainning Epoch:   9%|▉         | 15/165 [30:49<5:04:49, 121.93s/it]2025-09-14 16:53:40,145 Stage: Train 0.5 | Epoch: 15 | Iter: 45600 | Total Loss: 0.004771 | Recon Loss: 0.004070 | Commit Loss: 0.001402 | Perplexity: 654.439617
2025-09-14 16:53:47,857 Stage: Train 0.5 | Epoch: 15 | Iter: 45800 | Total Loss: 0.004929 | Recon Loss: 0.004230 | Commit Loss: 0.001398 | Perplexity: 653.531048
2025-09-14 16:53:55,590 Stage: Train 0.5 | Epoch: 15 | Iter: 46000 | Total Loss: 0.004730 | Recon Loss: 0.004028 | Commit Loss: 0.001405 | Perplexity: 654.502131
2025-09-14 16:54:03,282 Stage: Train 0.5 | Epoch: 15 | Iter: 46200 | Total Loss: 0.004746 | Recon Loss: 0.004031 | Commit Loss: 0.001431 | Perplexity: 654.315465
2025-09-14 16:54:10,986 Stage: Train 0.5 | Epoch: 15 | Iter: 46400 | Total Loss: 0.004902 | Recon Loss: 0.004199 | Commit Loss: 0.001406 | Perplexity: 653.167039
2025-09-14 16:54:18,702 Stage: Train 0.5 | Epoch: 15 | Iter: 46600 | Total Loss: 0.004994 | Recon Loss: 0.004297 | Commit Loss: 0.001394 | Perplexity: 647.960949
2025-09-14 16:54:26,415 Stage: Train 0.5 | Epoch: 15 | Iter: 46800 | Total Loss: 0.004721 | Recon Loss: 0.004015 | Commit Loss: 0.001411 | Perplexity: 654.474299
2025-09-14 16:54:34,133 Stage: Train 0.5 | Epoch: 15 | Iter: 47000 | Total Loss: 0.004781 | Recon Loss: 0.004079 | Commit Loss: 0.001403 | Perplexity: 654.340182
2025-09-14 16:54:41,870 Stage: Train 0.5 | Epoch: 15 | Iter: 47200 | Total Loss: 0.004722 | Recon Loss: 0.004022 | Commit Loss: 0.001400 | Perplexity: 654.286904
2025-09-14 16:54:49,582 Stage: Train 0.5 | Epoch: 15 | Iter: 47400 | Total Loss: 0.004784 | Recon Loss: 0.004068 | Commit Loss: 0.001431 | Perplexity: 651.418078
2025-09-14 16:54:57,296 Stage: Train 0.5 | Epoch: 15 | Iter: 47600 | Total Loss: 0.004862 | Recon Loss: 0.004158 | Commit Loss: 0.001408 | Perplexity: 655.099501
2025-09-14 16:55:05,054 Stage: Train 0.5 | Epoch: 15 | Iter: 47800 | Total Loss: 0.004760 | Recon Loss: 0.004049 | Commit Loss: 0.001421 | Perplexity: 650.971465
2025-09-14 16:55:12,769 Stage: Train 0.5 | Epoch: 15 | Iter: 48000 | Total Loss: 0.004778 | Recon Loss: 0.004059 | Commit Loss: 0.001437 | Perplexity: 655.437037
2025-09-14 16:55:20,505 Stage: Train 0.5 | Epoch: 15 | Iter: 48200 | Total Loss: 0.004798 | Recon Loss: 0.004092 | Commit Loss: 0.001412 | Perplexity: 653.740549
2025-09-14 16:55:28,197 Stage: Train 0.5 | Epoch: 15 | Iter: 48400 | Total Loss: 0.004783 | Recon Loss: 0.004082 | Commit Loss: 0.001404 | Perplexity: 646.920255
2025-09-14 16:55:35,949 Stage: Train 0.5 | Epoch: 15 | Iter: 48600 | Total Loss: 0.004640 | Recon Loss: 0.003939 | Commit Loss: 0.001403 | Perplexity: 650.588395
Trainning Epoch:  10%|▉         | 16/165 [32:46<4:59:19, 120.53s/it]2025-09-14 16:55:43,661 Stage: Train 0.5 | Epoch: 16 | Iter: 48800 | Total Loss: 0.004787 | Recon Loss: 0.004079 | Commit Loss: 0.001416 | Perplexity: 650.660879
2025-09-14 16:55:51,354 Stage: Train 0.5 | Epoch: 16 | Iter: 49000 | Total Loss: 0.004672 | Recon Loss: 0.003962 | Commit Loss: 0.001420 | Perplexity: 655.879416
2025-09-14 16:55:59,057 Stage: Train 0.5 | Epoch: 16 | Iter: 49200 | Total Loss: 0.004773 | Recon Loss: 0.004056 | Commit Loss: 0.001433 | Perplexity: 652.342733
2025-09-14 16:56:06,809 Stage: Train 0.5 | Epoch: 16 | Iter: 49400 | Total Loss: 0.004694 | Recon Loss: 0.003974 | Commit Loss: 0.001441 | Perplexity: 655.101563
2025-09-14 16:56:14,601 Stage: Train 0.5 | Epoch: 16 | Iter: 49600 | Total Loss: 0.004691 | Recon Loss: 0.003972 | Commit Loss: 0.001438 | Perplexity: 654.517309
2025-09-14 16:56:22,338 Stage: Train 0.5 | Epoch: 16 | Iter: 49800 | Total Loss: 0.004784 | Recon Loss: 0.004071 | Commit Loss: 0.001427 | Perplexity: 653.635276
2025-09-14 16:56:30,081 Stage: Train 0.5 | Epoch: 16 | Iter: 50000 | Total Loss: 0.004795 | Recon Loss: 0.004084 | Commit Loss: 0.001423 | Perplexity: 648.096592
2025-09-14 16:56:37,774 Stage: Train 0.5 | Epoch: 16 | Iter: 50200 | Total Loss: 0.004737 | Recon Loss: 0.004034 | Commit Loss: 0.001407 | Perplexity: 652.370977
2025-09-14 16:56:45,473 Stage: Train 0.5 | Epoch: 16 | Iter: 50400 | Total Loss: 0.004655 | Recon Loss: 0.003952 | Commit Loss: 0.001407 | Perplexity: 649.903247
2025-09-14 16:56:53,179 Stage: Train 0.5 | Epoch: 16 | Iter: 50600 | Total Loss: 0.004764 | Recon Loss: 0.004046 | Commit Loss: 0.001435 | Perplexity: 652.561922
2025-09-14 16:57:00,845 Stage: Train 0.5 | Epoch: 16 | Iter: 50800 | Total Loss: 0.004811 | Recon Loss: 0.004087 | Commit Loss: 0.001448 | Perplexity: 653.617472
2025-09-14 16:57:08,513 Stage: Train 0.5 | Epoch: 16 | Iter: 51000 | Total Loss: 0.004681 | Recon Loss: 0.003970 | Commit Loss: 0.001422 | Perplexity: 652.073515
2025-09-14 16:57:16,225 Stage: Train 0.5 | Epoch: 16 | Iter: 51200 | Total Loss: 0.004763 | Recon Loss: 0.004060 | Commit Loss: 0.001406 | Perplexity: 650.827243
2025-09-14 16:57:23,922 Stage: Train 0.5 | Epoch: 16 | Iter: 51400 | Total Loss: 0.004744 | Recon Loss: 0.004022 | Commit Loss: 0.001443 | Perplexity: 652.717680
2025-09-14 16:57:31,614 Stage: Train 0.5 | Epoch: 16 | Iter: 51600 | Total Loss: 0.004733 | Recon Loss: 0.004018 | Commit Loss: 0.001431 | Perplexity: 650.869135
Trainning Epoch:  10%|█         | 17/165 [34:44<4:54:47, 119.51s/it]2025-09-14 16:57:39,325 Stage: Train 0.5 | Epoch: 17 | Iter: 51800 | Total Loss: 0.004672 | Recon Loss: 0.003954 | Commit Loss: 0.001435 | Perplexity: 649.724647
2025-09-14 16:57:47,001 Stage: Train 0.5 | Epoch: 17 | Iter: 52000 | Total Loss: 0.004784 | Recon Loss: 0.004078 | Commit Loss: 0.001411 | Perplexity: 648.120121
2025-09-14 16:57:54,710 Stage: Train 0.5 | Epoch: 17 | Iter: 52200 | Total Loss: 0.004627 | Recon Loss: 0.003918 | Commit Loss: 0.001420 | Perplexity: 648.711335
2025-09-14 16:58:02,385 Stage: Train 0.5 | Epoch: 17 | Iter: 52400 | Total Loss: 0.004767 | Recon Loss: 0.004045 | Commit Loss: 0.001444 | Perplexity: 651.794139
2025-09-14 16:58:10,082 Stage: Train 0.5 | Epoch: 17 | Iter: 52600 | Total Loss: 0.004689 | Recon Loss: 0.003972 | Commit Loss: 0.001434 | Perplexity: 652.316172
2025-09-14 16:58:17,794 Stage: Train 0.5 | Epoch: 17 | Iter: 52800 | Total Loss: 0.004632 | Recon Loss: 0.003911 | Commit Loss: 0.001441 | Perplexity: 652.443631
2025-09-14 16:58:25,481 Stage: Train 0.5 | Epoch: 17 | Iter: 53000 | Total Loss: 0.004655 | Recon Loss: 0.003938 | Commit Loss: 0.001433 | Perplexity: 651.429614
2025-09-14 16:58:33,174 Stage: Train 0.5 | Epoch: 17 | Iter: 53200 | Total Loss: 0.004747 | Recon Loss: 0.004027 | Commit Loss: 0.001440 | Perplexity: 651.460578
2025-09-14 16:58:40,883 Stage: Train 0.5 | Epoch: 17 | Iter: 53400 | Total Loss: 0.004660 | Recon Loss: 0.003938 | Commit Loss: 0.001445 | Perplexity: 651.615421
2025-09-14 16:58:48,513 Stage: Train 0.5 | Epoch: 17 | Iter: 53600 | Total Loss: 0.004633 | Recon Loss: 0.003909 | Commit Loss: 0.001447 | Perplexity: 650.008675
2025-09-14 16:58:56,197 Stage: Train 0.5 | Epoch: 17 | Iter: 53800 | Total Loss: 0.004637 | Recon Loss: 0.003917 | Commit Loss: 0.001439 | Perplexity: 648.650138
2025-09-14 16:59:03,833 Stage: Train 0.5 | Epoch: 17 | Iter: 54000 | Total Loss: 0.004658 | Recon Loss: 0.003949 | Commit Loss: 0.001418 | Perplexity: 648.777526
2025-09-14 16:59:11,659 Stage: Train 0.5 | Epoch: 17 | Iter: 54200 | Total Loss: 0.004727 | Recon Loss: 0.004008 | Commit Loss: 0.001438 | Perplexity: 647.364492
2025-09-14 16:59:19,462 Stage: Train 0.5 | Epoch: 17 | Iter: 54400 | Total Loss: 0.004637 | Recon Loss: 0.003919 | Commit Loss: 0.001437 | Perplexity: 651.259067
2025-09-14 16:59:27,269 Stage: Train 0.5 | Epoch: 17 | Iter: 54600 | Total Loss: 0.004646 | Recon Loss: 0.003911 | Commit Loss: 0.001469 | Perplexity: 652.214245
Trainning Epoch:  11%|█         | 18/165 [36:41<4:51:03, 118.80s/it]2025-09-14 16:59:35,053 Stage: Train 0.5 | Epoch: 18 | Iter: 54800 | Total Loss: 0.004650 | Recon Loss: 0.003933 | Commit Loss: 0.001435 | Perplexity: 644.397914
2025-09-14 16:59:42,838 Stage: Train 0.5 | Epoch: 18 | Iter: 55000 | Total Loss: 0.004658 | Recon Loss: 0.003940 | Commit Loss: 0.001438 | Perplexity: 651.252024
2025-09-14 16:59:50,619 Stage: Train 0.5 | Epoch: 18 | Iter: 55200 | Total Loss: 0.004530 | Recon Loss: 0.003819 | Commit Loss: 0.001422 | Perplexity: 648.845557
2025-09-14 16:59:58,370 Stage: Train 0.5 | Epoch: 18 | Iter: 55400 | Total Loss: 0.004801 | Recon Loss: 0.004087 | Commit Loss: 0.001428 | Perplexity: 649.692859
2025-09-14 17:00:06,197 Stage: Train 0.5 | Epoch: 18 | Iter: 55600 | Total Loss: 0.004679 | Recon Loss: 0.003963 | Commit Loss: 0.001432 | Perplexity: 649.792345
2025-09-14 17:00:14,021 Stage: Train 0.5 | Epoch: 18 | Iter: 55800 | Total Loss: 0.004626 | Recon Loss: 0.003909 | Commit Loss: 0.001432 | Perplexity: 649.742832
2025-09-14 17:00:21,865 Stage: Train 0.5 | Epoch: 18 | Iter: 56000 | Total Loss: 0.004625 | Recon Loss: 0.003904 | Commit Loss: 0.001442 | Perplexity: 648.827927
2025-09-14 17:00:29,642 Stage: Train 0.5 | Epoch: 18 | Iter: 56200 | Total Loss: 0.004598 | Recon Loss: 0.003873 | Commit Loss: 0.001450 | Perplexity: 648.009579
2025-09-14 17:00:37,434 Stage: Train 0.5 | Epoch: 18 | Iter: 56400 | Total Loss: 0.004733 | Recon Loss: 0.004001 | Commit Loss: 0.001464 | Perplexity: 648.378795
2025-09-14 17:00:45,241 Stage: Train 0.5 | Epoch: 18 | Iter: 56600 | Total Loss: 0.004535 | Recon Loss: 0.003815 | Commit Loss: 0.001438 | Perplexity: 650.768886
2025-09-14 17:00:53,018 Stage: Train 0.5 | Epoch: 18 | Iter: 56800 | Total Loss: 0.004665 | Recon Loss: 0.003938 | Commit Loss: 0.001454 | Perplexity: 649.720133
2025-09-14 17:01:00,870 Stage: Train 0.5 | Epoch: 18 | Iter: 57000 | Total Loss: 0.004657 | Recon Loss: 0.003929 | Commit Loss: 0.001457 | Perplexity: 649.714604
2025-09-14 17:01:08,646 Stage: Train 0.5 | Epoch: 18 | Iter: 57200 | Total Loss: 0.004727 | Recon Loss: 0.004010 | Commit Loss: 0.001433 | Perplexity: 648.174335
2025-09-14 17:01:16,393 Stage: Train 0.5 | Epoch: 18 | Iter: 57400 | Total Loss: 0.004618 | Recon Loss: 0.003909 | Commit Loss: 0.001419 | Perplexity: 645.466596
2025-09-14 17:01:24,286 Stage: Train 0.5 | Epoch: 18 | Iter: 57600 | Total Loss: 0.004580 | Recon Loss: 0.003855 | Commit Loss: 0.001451 | Perplexity: 650.204922
Trainning Epoch:  12%|█▏        | 19/165 [38:39<4:49:00, 118.77s/it]2025-09-14 17:01:32,277 Stage: Train 0.5 | Epoch: 19 | Iter: 57800 | Total Loss: 0.004699 | Recon Loss: 0.003986 | Commit Loss: 0.001426 | Perplexity: 647.464019
2025-09-14 17:01:40,117 Stage: Train 0.5 | Epoch: 19 | Iter: 58000 | Total Loss: 0.004613 | Recon Loss: 0.003887 | Commit Loss: 0.001452 | Perplexity: 650.234696
2025-09-14 17:01:48,002 Stage: Train 0.5 | Epoch: 19 | Iter: 58200 | Total Loss: 0.004629 | Recon Loss: 0.003911 | Commit Loss: 0.001435 | Perplexity: 649.638840
2025-09-14 17:01:56,160 Stage: Train 0.5 | Epoch: 19 | Iter: 58400 | Total Loss: 0.004620 | Recon Loss: 0.003906 | Commit Loss: 0.001427 | Perplexity: 648.340383
2025-09-14 17:02:04,258 Stage: Train 0.5 | Epoch: 19 | Iter: 58600 | Total Loss: 0.004656 | Recon Loss: 0.003928 | Commit Loss: 0.001457 | Perplexity: 649.350161
2025-09-14 17:02:12,374 Stage: Train 0.5 | Epoch: 19 | Iter: 58800 | Total Loss: 0.004533 | Recon Loss: 0.003813 | Commit Loss: 0.001440 | Perplexity: 647.733843
2025-09-14 17:02:20,480 Stage: Train 0.5 | Epoch: 19 | Iter: 59000 | Total Loss: 0.004601 | Recon Loss: 0.003867 | Commit Loss: 0.001469 | Perplexity: 652.457522
2025-09-14 17:02:28,619 Stage: Train 0.5 | Epoch: 19 | Iter: 59200 | Total Loss: 0.004618 | Recon Loss: 0.003892 | Commit Loss: 0.001453 | Perplexity: 651.512917
2025-09-14 17:02:36,510 Stage: Train 0.5 | Epoch: 19 | Iter: 59400 | Total Loss: 0.004548 | Recon Loss: 0.003821 | Commit Loss: 0.001454 | Perplexity: 648.329818
2025-09-14 17:02:44,325 Stage: Train 0.5 | Epoch: 19 | Iter: 59600 | Total Loss: 0.004586 | Recon Loss: 0.003869 | Commit Loss: 0.001434 | Perplexity: 648.088925
2025-09-14 17:02:52,105 Stage: Train 0.5 | Epoch: 19 | Iter: 59800 | Total Loss: 0.004598 | Recon Loss: 0.003867 | Commit Loss: 0.001462 | Perplexity: 650.807458
2025-09-14 17:02:59,917 Stage: Train 0.5 | Epoch: 19 | Iter: 60000 | Total Loss: 0.004554 | Recon Loss: 0.003828 | Commit Loss: 0.001452 | Perplexity: 651.083971
2025-09-14 17:02:59,917 Saving model at iteration 60000
2025-09-14 17:03:00,470 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_20_step_60000
2025-09-14 17:03:00,604 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_20_step_60000/pytorch_model.bin
2025-09-14 17:03:00,845 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_20_step_60000/optimizer.bin
2025-09-14 17:03:00,845 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_20_step_60000/scheduler.bin
2025-09-14 17:03:00,846 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_20_step_60000/random_states_0.pkl
2025-09-14 17:03:08,643 Stage: Train 0.5 | Epoch: 19 | Iter: 60200 | Total Loss: 0.004599 | Recon Loss: 0.003884 | Commit Loss: 0.001431 | Perplexity: 647.722921
2025-09-14 17:03:16,463 Stage: Train 0.5 | Epoch: 19 | Iter: 60400 | Total Loss: 0.004608 | Recon Loss: 0.003893 | Commit Loss: 0.001430 | Perplexity: 645.351735
2025-09-14 17:03:24,221 Stage: Train 0.5 | Epoch: 19 | Iter: 60600 | Total Loss: 0.004551 | Recon Loss: 0.003826 | Commit Loss: 0.001450 | Perplexity: 648.119669
Trainning Epoch:  12%|█▏        | 20/165 [40:41<4:48:47, 119.50s/it]2025-09-14 17:03:32,079 Stage: Train 0.5 | Epoch: 20 | Iter: 60800 | Total Loss: 0.004552 | Recon Loss: 0.003829 | Commit Loss: 0.001446 | Perplexity: 645.855810
2025-09-14 17:03:40,207 Stage: Train 0.5 | Epoch: 20 | Iter: 61000 | Total Loss: 0.004494 | Recon Loss: 0.003766 | Commit Loss: 0.001457 | Perplexity: 651.488237
2025-09-14 17:03:48,326 Stage: Train 0.5 | Epoch: 20 | Iter: 61200 | Total Loss: 0.004528 | Recon Loss: 0.003808 | Commit Loss: 0.001440 | Perplexity: 648.017552
2025-09-14 17:03:56,420 Stage: Train 0.5 | Epoch: 20 | Iter: 61400 | Total Loss: 0.004497 | Recon Loss: 0.003759 | Commit Loss: 0.001476 | Perplexity: 651.125686
2025-09-14 17:04:04,516 Stage: Train 0.5 | Epoch: 20 | Iter: 61600 | Total Loss: 0.004568 | Recon Loss: 0.003837 | Commit Loss: 0.001463 | Perplexity: 650.534251
2025-09-14 17:04:12,622 Stage: Train 0.5 | Epoch: 20 | Iter: 61800 | Total Loss: 0.004566 | Recon Loss: 0.003837 | Commit Loss: 0.001459 | Perplexity: 649.274106
2025-09-14 17:04:20,732 Stage: Train 0.5 | Epoch: 20 | Iter: 62000 | Total Loss: 0.004555 | Recon Loss: 0.003827 | Commit Loss: 0.001456 | Perplexity: 647.935001
2025-09-14 17:04:28,856 Stage: Train 0.5 | Epoch: 20 | Iter: 62200 | Total Loss: 0.004586 | Recon Loss: 0.003858 | Commit Loss: 0.001456 | Perplexity: 647.813675
2025-09-14 17:04:37,029 Stage: Train 0.5 | Epoch: 20 | Iter: 62400 | Total Loss: 0.004472 | Recon Loss: 0.003742 | Commit Loss: 0.001459 | Perplexity: 647.668596
2025-09-14 17:04:45,179 Stage: Train 0.5 | Epoch: 20 | Iter: 62600 | Total Loss: 0.004548 | Recon Loss: 0.003820 | Commit Loss: 0.001455 | Perplexity: 646.761922
2025-09-14 17:04:53,286 Stage: Train 0.5 | Epoch: 20 | Iter: 62800 | Total Loss: 0.004627 | Recon Loss: 0.003896 | Commit Loss: 0.001462 | Perplexity: 647.026316
2025-09-14 17:05:01,255 Stage: Train 0.5 | Epoch: 20 | Iter: 63000 | Total Loss: 0.004609 | Recon Loss: 0.003875 | Commit Loss: 0.001468 | Perplexity: 647.500441
2025-09-14 17:05:09,367 Stage: Train 0.5 | Epoch: 20 | Iter: 63200 | Total Loss: 0.004507 | Recon Loss: 0.003780 | Commit Loss: 0.001453 | Perplexity: 644.723902
2025-09-14 17:05:17,293 Stage: Train 0.5 | Epoch: 20 | Iter: 63400 | Total Loss: 0.004547 | Recon Loss: 0.003823 | Commit Loss: 0.001448 | Perplexity: 649.412233
2025-09-14 17:05:25,063 Stage: Train 0.5 | Epoch: 20 | Iter: 63600 | Total Loss: 0.004523 | Recon Loss: 0.003798 | Commit Loss: 0.001450 | Perplexity: 645.514035
Trainning Epoch:  13%|█▎        | 21/165 [42:43<4:48:55, 120.38s/it]2025-09-14 17:05:32,964 Stage: Train 0.5 | Epoch: 21 | Iter: 63800 | Total Loss: 0.004525 | Recon Loss: 0.003802 | Commit Loss: 0.001446 | Perplexity: 648.379112
2025-09-14 17:05:41,066 Stage: Train 0.5 | Epoch: 21 | Iter: 64000 | Total Loss: 0.004674 | Recon Loss: 0.003956 | Commit Loss: 0.001436 | Perplexity: 645.823227
2025-09-14 17:05:49,266 Stage: Train 0.5 | Epoch: 21 | Iter: 64200 | Total Loss: 0.004510 | Recon Loss: 0.003782 | Commit Loss: 0.001454 | Perplexity: 649.374314
2025-09-14 17:05:57,432 Stage: Train 0.5 | Epoch: 21 | Iter: 64400 | Total Loss: 0.004463 | Recon Loss: 0.003742 | Commit Loss: 0.001443 | Perplexity: 649.056893
2025-09-14 17:06:05,583 Stage: Train 0.5 | Epoch: 21 | Iter: 64600 | Total Loss: 0.004570 | Recon Loss: 0.003838 | Commit Loss: 0.001463 | Perplexity: 652.015190
2025-09-14 17:06:13,728 Stage: Train 0.5 | Epoch: 21 | Iter: 64800 | Total Loss: 0.004554 | Recon Loss: 0.003835 | Commit Loss: 0.001438 | Perplexity: 645.913315
2025-09-14 17:06:21,884 Stage: Train 0.5 | Epoch: 21 | Iter: 65000 | Total Loss: 0.004417 | Recon Loss: 0.003686 | Commit Loss: 0.001462 | Perplexity: 648.012418
2025-09-14 17:06:29,959 Stage: Train 0.5 | Epoch: 21 | Iter: 65200 | Total Loss: 0.004569 | Recon Loss: 0.003847 | Commit Loss: 0.001443 | Perplexity: 646.251101
2025-09-14 17:06:38,071 Stage: Train 0.5 | Epoch: 21 | Iter: 65400 | Total Loss: 0.004553 | Recon Loss: 0.003821 | Commit Loss: 0.001465 | Perplexity: 645.700893
2025-09-14 17:06:46,146 Stage: Train 0.5 | Epoch: 21 | Iter: 65600 | Total Loss: 0.004480 | Recon Loss: 0.003742 | Commit Loss: 0.001475 | Perplexity: 647.889203
2025-09-14 17:06:54,406 Stage: Train 0.5 | Epoch: 21 | Iter: 65800 | Total Loss: 0.004506 | Recon Loss: 0.003766 | Commit Loss: 0.001479 | Perplexity: 651.412435
2025-09-14 17:07:02,186 Stage: Train 0.5 | Epoch: 21 | Iter: 66000 | Total Loss: 0.004574 | Recon Loss: 0.003840 | Commit Loss: 0.001466 | Perplexity: 649.526858
2025-09-14 17:07:09,934 Stage: Train 0.5 | Epoch: 21 | Iter: 66200 | Total Loss: 0.004538 | Recon Loss: 0.003804 | Commit Loss: 0.001466 | Perplexity: 649.412765
2025-09-14 17:07:17,694 Stage: Train 0.5 | Epoch: 21 | Iter: 66400 | Total Loss: 0.004520 | Recon Loss: 0.003786 | Commit Loss: 0.001470 | Perplexity: 648.636489
2025-09-14 17:07:25,451 Stage: Train 0.5 | Epoch: 21 | Iter: 66600 | Total Loss: 0.004501 | Recon Loss: 0.003760 | Commit Loss: 0.001482 | Perplexity: 648.858430
2025-09-14 17:07:33,173 Stage: Train 0.5 | Epoch: 21 | Iter: 66800 | Total Loss: 0.004556 | Recon Loss: 0.003814 | Commit Loss: 0.001483 | Perplexity: 649.278404
Trainning Epoch:  13%|█▎        | 22/165 [44:45<4:47:50, 120.77s/it]2025-09-14 17:07:40,937 Stage: Train 0.5 | Epoch: 22 | Iter: 67000 | Total Loss: 0.004480 | Recon Loss: 0.003752 | Commit Loss: 0.001456 | Perplexity: 645.248165
2025-09-14 17:07:48,637 Stage: Train 0.5 | Epoch: 22 | Iter: 67200 | Total Loss: 0.004519 | Recon Loss: 0.003785 | Commit Loss: 0.001468 | Perplexity: 648.181958
2025-09-14 17:07:56,345 Stage: Train 0.5 | Epoch: 22 | Iter: 67400 | Total Loss: 0.004469 | Recon Loss: 0.003740 | Commit Loss: 0.001458 | Perplexity: 647.834319
2025-09-14 17:08:04,074 Stage: Train 0.5 | Epoch: 22 | Iter: 67600 | Total Loss: 0.004551 | Recon Loss: 0.003811 | Commit Loss: 0.001482 | Perplexity: 649.234821
2025-09-14 17:08:11,797 Stage: Train 0.5 | Epoch: 22 | Iter: 67800 | Total Loss: 0.004429 | Recon Loss: 0.003708 | Commit Loss: 0.001441 | Perplexity: 647.511531
2025-09-14 17:08:19,502 Stage: Train 0.5 | Epoch: 22 | Iter: 68000 | Total Loss: 0.004440 | Recon Loss: 0.003701 | Commit Loss: 0.001478 | Perplexity: 649.941969
2025-09-14 17:08:27,211 Stage: Train 0.5 | Epoch: 22 | Iter: 68200 | Total Loss: 0.004488 | Recon Loss: 0.003747 | Commit Loss: 0.001481 | Perplexity: 648.455227
2025-09-14 17:08:34,927 Stage: Train 0.5 | Epoch: 22 | Iter: 68400 | Total Loss: 0.004554 | Recon Loss: 0.003824 | Commit Loss: 0.001460 | Perplexity: 648.153005
2025-09-14 17:08:42,649 Stage: Train 0.5 | Epoch: 22 | Iter: 68600 | Total Loss: 0.004477 | Recon Loss: 0.003752 | Commit Loss: 0.001450 | Perplexity: 647.620569
2025-09-14 17:08:50,457 Stage: Train 0.5 | Epoch: 22 | Iter: 68800 | Total Loss: 0.004481 | Recon Loss: 0.003761 | Commit Loss: 0.001440 | Perplexity: 647.250942
2025-09-14 17:08:58,245 Stage: Train 0.5 | Epoch: 22 | Iter: 69000 | Total Loss: 0.004440 | Recon Loss: 0.003708 | Commit Loss: 0.001465 | Perplexity: 647.631551
2025-09-14 17:09:06,078 Stage: Train 0.5 | Epoch: 22 | Iter: 69200 | Total Loss: 0.004427 | Recon Loss: 0.003693 | Commit Loss: 0.001467 | Perplexity: 649.056996
2025-09-14 17:09:13,861 Stage: Train 0.5 | Epoch: 22 | Iter: 69400 | Total Loss: 0.004530 | Recon Loss: 0.003801 | Commit Loss: 0.001458 | Perplexity: 648.862001
2025-09-14 17:09:21,689 Stage: Train 0.5 | Epoch: 22 | Iter: 69600 | Total Loss: 0.004353 | Recon Loss: 0.003619 | Commit Loss: 0.001468 | Perplexity: 648.185980
2025-09-14 17:09:29,497 Stage: Train 0.5 | Epoch: 22 | Iter: 69800 | Total Loss: 0.004468 | Recon Loss: 0.003731 | Commit Loss: 0.001474 | Perplexity: 647.644025
Trainning Epoch:  14%|█▍        | 23/165 [46:43<4:43:43, 119.88s/it]2025-09-14 17:09:37,314 Stage: Train 0.5 | Epoch: 23 | Iter: 70000 | Total Loss: 0.004411 | Recon Loss: 0.003679 | Commit Loss: 0.001464 | Perplexity: 647.703971
2025-09-14 17:09:45,130 Stage: Train 0.5 | Epoch: 23 | Iter: 70200 | Total Loss: 0.004476 | Recon Loss: 0.003742 | Commit Loss: 0.001468 | Perplexity: 648.198718
2025-09-14 17:09:52,934 Stage: Train 0.5 | Epoch: 23 | Iter: 70400 | Total Loss: 0.004419 | Recon Loss: 0.003686 | Commit Loss: 0.001467 | Perplexity: 647.266281
2025-09-14 17:10:00,814 Stage: Train 0.5 | Epoch: 23 | Iter: 70600 | Total Loss: 0.004446 | Recon Loss: 0.003718 | Commit Loss: 0.001455 | Perplexity: 646.535454
2025-09-14 17:10:08,642 Stage: Train 0.5 | Epoch: 23 | Iter: 70800 | Total Loss: 0.004400 | Recon Loss: 0.003671 | Commit Loss: 0.001458 | Perplexity: 648.500534
2025-09-14 17:10:16,417 Stage: Train 0.5 | Epoch: 23 | Iter: 71000 | Total Loss: 0.004467 | Recon Loss: 0.003733 | Commit Loss: 0.001469 | Perplexity: 648.856943
2025-09-14 17:10:24,221 Stage: Train 0.5 | Epoch: 23 | Iter: 71200 | Total Loss: 0.004340 | Recon Loss: 0.003602 | Commit Loss: 0.001477 | Perplexity: 651.566396
2025-09-14 17:10:32,030 Stage: Train 0.5 | Epoch: 23 | Iter: 71400 | Total Loss: 0.004514 | Recon Loss: 0.003779 | Commit Loss: 0.001470 | Perplexity: 648.357570
2025-09-14 17:10:39,862 Stage: Train 0.5 | Epoch: 23 | Iter: 71600 | Total Loss: 0.004519 | Recon Loss: 0.003793 | Commit Loss: 0.001452 | Perplexity: 643.692906
2025-09-14 17:10:47,638 Stage: Train 0.5 | Epoch: 23 | Iter: 71800 | Total Loss: 0.004420 | Recon Loss: 0.003697 | Commit Loss: 0.001447 | Perplexity: 645.488057
2025-09-14 17:10:55,457 Stage: Train 0.5 | Epoch: 23 | Iter: 72000 | Total Loss: 0.004449 | Recon Loss: 0.003716 | Commit Loss: 0.001468 | Perplexity: 651.081244
2025-09-14 17:11:03,318 Stage: Train 0.5 | Epoch: 23 | Iter: 72200 | Total Loss: 0.004448 | Recon Loss: 0.003709 | Commit Loss: 0.001479 | Perplexity: 651.070927
2025-09-14 17:11:11,106 Stage: Train 0.5 | Epoch: 23 | Iter: 72400 | Total Loss: 0.004390 | Recon Loss: 0.003648 | Commit Loss: 0.001484 | Perplexity: 650.734193
2025-09-14 17:11:18,922 Stage: Train 0.5 | Epoch: 23 | Iter: 72600 | Total Loss: 0.004366 | Recon Loss: 0.003637 | Commit Loss: 0.001458 | Perplexity: 646.348684
2025-09-14 17:11:26,769 Stage: Train 0.5 | Epoch: 23 | Iter: 72800 | Total Loss: 0.004360 | Recon Loss: 0.003629 | Commit Loss: 0.001462 | Perplexity: 646.928982
Trainning Epoch:  15%|█▍        | 24/165 [48:41<4:40:56, 119.55s/it]2025-09-14 17:11:34,557 Stage: Train 0.5 | Epoch: 24 | Iter: 73000 | Total Loss: 0.004371 | Recon Loss: 0.003633 | Commit Loss: 0.001476 | Perplexity: 646.705313
2025-09-14 17:11:42,278 Stage: Train 0.5 | Epoch: 24 | Iter: 73200 | Total Loss: 0.004469 | Recon Loss: 0.003738 | Commit Loss: 0.001460 | Perplexity: 647.652885
2025-09-14 17:11:49,981 Stage: Train 0.5 | Epoch: 24 | Iter: 73400 | Total Loss: 0.004365 | Recon Loss: 0.003639 | Commit Loss: 0.001453 | Perplexity: 649.581228
2025-09-14 17:11:57,730 Stage: Train 0.5 | Epoch: 24 | Iter: 73600 | Total Loss: 0.004333 | Recon Loss: 0.003589 | Commit Loss: 0.001488 | Perplexity: 650.846458
2025-09-14 17:12:05,471 Stage: Train 0.5 | Epoch: 24 | Iter: 73800 | Total Loss: 0.004473 | Recon Loss: 0.003746 | Commit Loss: 0.001453 | Perplexity: 644.907797
2025-09-14 17:12:13,226 Stage: Train 0.5 | Epoch: 24 | Iter: 74000 | Total Loss: 0.004343 | Recon Loss: 0.003601 | Commit Loss: 0.001484 | Perplexity: 649.354599
2025-09-14 17:12:20,925 Stage: Train 0.5 | Epoch: 24 | Iter: 74200 | Total Loss: 0.004508 | Recon Loss: 0.003775 | Commit Loss: 0.001467 | Perplexity: 647.592200
2025-09-14 17:12:28,649 Stage: Train 0.5 | Epoch: 24 | Iter: 74400 | Total Loss: 0.004334 | Recon Loss: 0.003605 | Commit Loss: 0.001458 | Perplexity: 647.616601
2025-09-14 17:12:36,338 Stage: Train 0.5 | Epoch: 24 | Iter: 74600 | Total Loss: 0.004390 | Recon Loss: 0.003653 | Commit Loss: 0.001473 | Perplexity: 647.626764
2025-09-14 17:12:44,037 Stage: Train 0.5 | Epoch: 24 | Iter: 74800 | Total Loss: 0.004355 | Recon Loss: 0.003619 | Commit Loss: 0.001473 | Perplexity: 647.881513
2025-09-14 17:12:51,778 Stage: Train 0.5 | Epoch: 24 | Iter: 75000 | Total Loss: 0.004457 | Recon Loss: 0.003725 | Commit Loss: 0.001464 | Perplexity: 648.064275
2025-09-14 17:12:59,497 Stage: Train 0.5 | Epoch: 24 | Iter: 75200 | Total Loss: 0.004415 | Recon Loss: 0.003678 | Commit Loss: 0.001475 | Perplexity: 647.807440
2025-09-14 17:13:07,209 Stage: Train 0.5 | Epoch: 24 | Iter: 75400 | Total Loss: 0.004377 | Recon Loss: 0.003649 | Commit Loss: 0.001457 | Perplexity: 649.738560
2025-09-14 17:13:14,922 Stage: Train 0.5 | Epoch: 24 | Iter: 75600 | Total Loss: 0.004348 | Recon Loss: 0.003613 | Commit Loss: 0.001469 | Perplexity: 645.810215
2025-09-14 17:13:22,621 Stage: Train 0.5 | Epoch: 24 | Iter: 75800 | Total Loss: 0.004352 | Recon Loss: 0.003619 | Commit Loss: 0.001466 | Perplexity: 645.594032
Trainning Epoch:  15%|█▌        | 25/165 [50:39<4:37:20, 118.86s/it]2025-09-14 17:13:30,340 Stage: Train 0.5 | Epoch: 25 | Iter: 76000 | Total Loss: 0.004348 | Recon Loss: 0.003621 | Commit Loss: 0.001453 | Perplexity: 645.577347
2025-09-14 17:13:38,067 Stage: Train 0.5 | Epoch: 25 | Iter: 76200 | Total Loss: 0.004321 | Recon Loss: 0.003581 | Commit Loss: 0.001480 | Perplexity: 648.419880
2025-09-14 17:13:45,909 Stage: Train 0.5 | Epoch: 25 | Iter: 76400 | Total Loss: 0.004345 | Recon Loss: 0.003608 | Commit Loss: 0.001474 | Perplexity: 649.614012
2025-09-14 17:13:53,753 Stage: Train 0.5 | Epoch: 25 | Iter: 76600 | Total Loss: 0.004374 | Recon Loss: 0.003636 | Commit Loss: 0.001476 | Perplexity: 647.345087
2025-09-14 17:14:01,550 Stage: Train 0.5 | Epoch: 25 | Iter: 76800 | Total Loss: 0.004346 | Recon Loss: 0.003606 | Commit Loss: 0.001481 | Perplexity: 650.615693
2025-09-14 17:14:09,290 Stage: Train 0.5 | Epoch: 25 | Iter: 77000 | Total Loss: 0.004368 | Recon Loss: 0.003634 | Commit Loss: 0.001469 | Perplexity: 647.312555
2025-09-14 17:14:17,074 Stage: Train 0.5 | Epoch: 25 | Iter: 77200 | Total Loss: 0.004344 | Recon Loss: 0.003618 | Commit Loss: 0.001452 | Perplexity: 648.731940
2025-09-14 17:14:24,845 Stage: Train 0.5 | Epoch: 25 | Iter: 77400 | Total Loss: 0.004359 | Recon Loss: 0.003627 | Commit Loss: 0.001463 | Perplexity: 645.092951
2025-09-14 17:14:32,642 Stage: Train 0.5 | Epoch: 25 | Iter: 77600 | Total Loss: 0.004386 | Recon Loss: 0.003647 | Commit Loss: 0.001478 | Perplexity: 648.814607
2025-09-14 17:14:40,447 Stage: Train 0.5 | Epoch: 25 | Iter: 77800 | Total Loss: 0.004397 | Recon Loss: 0.003664 | Commit Loss: 0.001468 | Perplexity: 649.410284
2025-09-14 17:14:48,277 Stage: Train 0.5 | Epoch: 25 | Iter: 78000 | Total Loss: 0.004458 | Recon Loss: 0.003731 | Commit Loss: 0.001455 | Perplexity: 645.322334
2025-09-14 17:14:56,133 Stage: Train 0.5 | Epoch: 25 | Iter: 78200 | Total Loss: 0.004292 | Recon Loss: 0.003566 | Commit Loss: 0.001451 | Perplexity: 647.107298
2025-09-14 17:15:04,041 Stage: Train 0.5 | Epoch: 25 | Iter: 78400 | Total Loss: 0.004395 | Recon Loss: 0.003659 | Commit Loss: 0.001471 | Perplexity: 648.230121
2025-09-14 17:15:11,833 Stage: Train 0.5 | Epoch: 25 | Iter: 78600 | Total Loss: 0.004321 | Recon Loss: 0.003589 | Commit Loss: 0.001464 | Perplexity: 649.827793
2025-09-14 17:15:19,627 Stage: Train 0.5 | Epoch: 25 | Iter: 78800 | Total Loss: 0.004242 | Recon Loss: 0.003512 | Commit Loss: 0.001461 | Perplexity: 650.472230
Trainning Epoch:  16%|█▌        | 26/165 [52:37<4:35:08, 118.76s/it]2025-09-14 17:15:27,398 Stage: Train 0.5 | Epoch: 26 | Iter: 79000 | Total Loss: 0.004272 | Recon Loss: 0.003532 | Commit Loss: 0.001480 | Perplexity: 644.529855
2025-09-14 17:15:35,153 Stage: Train 0.5 | Epoch: 26 | Iter: 79200 | Total Loss: 0.004332 | Recon Loss: 0.003607 | Commit Loss: 0.001450 | Perplexity: 645.343279
2025-09-14 17:15:42,897 Stage: Train 0.5 | Epoch: 26 | Iter: 79400 | Total Loss: 0.004372 | Recon Loss: 0.003641 | Commit Loss: 0.001462 | Perplexity: 647.590837
2025-09-14 17:15:50,650 Stage: Train 0.5 | Epoch: 26 | Iter: 79600 | Total Loss: 0.004317 | Recon Loss: 0.003582 | Commit Loss: 0.001470 | Perplexity: 647.781032
2025-09-14 17:15:58,337 Stage: Train 0.5 | Epoch: 26 | Iter: 79800 | Total Loss: 0.004356 | Recon Loss: 0.003627 | Commit Loss: 0.001458 | Perplexity: 647.762798
2025-09-14 17:16:06,051 Stage: Train 0.5 | Epoch: 26 | Iter: 80000 | Total Loss: 0.004265 | Recon Loss: 0.003531 | Commit Loss: 0.001468 | Perplexity: 646.872903
2025-09-14 17:16:06,052 Saving model at iteration 80000
2025-09-14 17:16:06,587 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_27_step_80000
2025-09-14 17:16:06,720 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_27_step_80000/pytorch_model.bin
2025-09-14 17:16:06,970 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_27_step_80000/optimizer.bin
2025-09-14 17:16:06,970 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_27_step_80000/scheduler.bin
2025-09-14 17:16:06,971 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_27_step_80000/random_states_0.pkl
2025-09-14 17:16:14,665 Stage: Train 0.5 | Epoch: 26 | Iter: 80200 | Total Loss: 0.004347 | Recon Loss: 0.003614 | Commit Loss: 0.001465 | Perplexity: 646.528358
2025-09-14 17:16:22,359 Stage: Train 0.5 | Epoch: 26 | Iter: 80400 | Total Loss: 0.004383 | Recon Loss: 0.003643 | Commit Loss: 0.001480 | Perplexity: 647.633298
2025-09-14 17:16:30,078 Stage: Train 0.5 | Epoch: 26 | Iter: 80600 | Total Loss: 0.004256 | Recon Loss: 0.003528 | Commit Loss: 0.001455 | Perplexity: 647.748605
2025-09-14 17:16:37,896 Stage: Train 0.5 | Epoch: 26 | Iter: 80800 | Total Loss: 0.004392 | Recon Loss: 0.003653 | Commit Loss: 0.001477 | Perplexity: 650.767241
2025-09-14 17:16:46,108 Stage: Train 0.5 | Epoch: 26 | Iter: 81000 | Total Loss: 0.004344 | Recon Loss: 0.003605 | Commit Loss: 0.001480 | Perplexity: 649.614084
2025-09-14 17:16:54,250 Stage: Train 0.5 | Epoch: 26 | Iter: 81200 | Total Loss: 0.004437 | Recon Loss: 0.003711 | Commit Loss: 0.001452 | Perplexity: 646.653452
2025-09-14 17:17:02,348 Stage: Train 0.5 | Epoch: 26 | Iter: 81400 | Total Loss: 0.004282 | Recon Loss: 0.003556 | Commit Loss: 0.001451 | Perplexity: 646.932024
2025-09-14 17:17:10,237 Stage: Train 0.5 | Epoch: 26 | Iter: 81600 | Total Loss: 0.004405 | Recon Loss: 0.003677 | Commit Loss: 0.001456 | Perplexity: 646.724835
2025-09-14 17:17:17,979 Stage: Train 0.5 | Epoch: 26 | Iter: 81800 | Total Loss: 0.004281 | Recon Loss: 0.003556 | Commit Loss: 0.001451 | Perplexity: 649.504797
2025-09-14 17:17:25,714 Stage: Train 0.5 | Epoch: 26 | Iter: 82000 | Total Loss: 0.004303 | Recon Loss: 0.003569 | Commit Loss: 0.001468 | Perplexity: 649.590875
Trainning Epoch:  16%|█▋        | 27/165 [54:37<4:33:56, 119.10s/it]2025-09-14 17:17:33,541 Stage: Train 0.5 | Epoch: 27 | Iter: 82200 | Total Loss: 0.004315 | Recon Loss: 0.003577 | Commit Loss: 0.001475 | Perplexity: 648.255029
2025-09-14 17:17:41,218 Stage: Train 0.5 | Epoch: 27 | Iter: 82400 | Total Loss: 0.004385 | Recon Loss: 0.003657 | Commit Loss: 0.001456 | Perplexity: 648.301070
2025-09-14 17:17:48,910 Stage: Train 0.5 | Epoch: 27 | Iter: 82600 | Total Loss: 0.004308 | Recon Loss: 0.003572 | Commit Loss: 0.001473 | Perplexity: 648.912426
2025-09-14 17:17:56,646 Stage: Train 0.5 | Epoch: 27 | Iter: 82800 | Total Loss: 0.004300 | Recon Loss: 0.003566 | Commit Loss: 0.001468 | Perplexity: 648.304096
2025-09-14 17:18:04,377 Stage: Train 0.5 | Epoch: 27 | Iter: 83000 | Total Loss: 0.004378 | Recon Loss: 0.003644 | Commit Loss: 0.001467 | Perplexity: 647.007659
2025-09-14 17:18:12,108 Stage: Train 0.5 | Epoch: 27 | Iter: 83200 | Total Loss: 0.004398 | Recon Loss: 0.003678 | Commit Loss: 0.001440 | Perplexity: 646.446916
2025-09-14 17:18:19,910 Stage: Train 0.5 | Epoch: 27 | Iter: 83400 | Total Loss: 0.004335 | Recon Loss: 0.003611 | Commit Loss: 0.001447 | Perplexity: 647.411440
2025-09-14 17:18:27,669 Stage: Train 0.5 | Epoch: 27 | Iter: 83600 | Total Loss: 0.004285 | Recon Loss: 0.003560 | Commit Loss: 0.001450 | Perplexity: 647.000988
2025-09-14 17:18:35,437 Stage: Train 0.5 | Epoch: 27 | Iter: 83800 | Total Loss: 0.004202 | Recon Loss: 0.003481 | Commit Loss: 0.001442 | Perplexity: 646.904143
2025-09-14 17:18:43,246 Stage: Train 0.5 | Epoch: 27 | Iter: 84000 | Total Loss: 0.004324 | Recon Loss: 0.003585 | Commit Loss: 0.001479 | Perplexity: 650.328924
2025-09-14 17:18:51,019 Stage: Train 0.5 | Epoch: 27 | Iter: 84200 | Total Loss: 0.004376 | Recon Loss: 0.003649 | Commit Loss: 0.001455 | Perplexity: 647.394161
2025-09-14 17:18:58,824 Stage: Train 0.5 | Epoch: 27 | Iter: 84400 | Total Loss: 0.004185 | Recon Loss: 0.003466 | Commit Loss: 0.001440 | Perplexity: 645.536271
2025-09-14 17:19:06,642 Stage: Train 0.5 | Epoch: 27 | Iter: 84600 | Total Loss: 0.004343 | Recon Loss: 0.003604 | Commit Loss: 0.001478 | Perplexity: 650.049773
2025-09-14 17:19:14,402 Stage: Train 0.5 | Epoch: 27 | Iter: 84800 | Total Loss: 0.004246 | Recon Loss: 0.003509 | Commit Loss: 0.001474 | Perplexity: 651.165785
2025-09-14 17:19:22,173 Stage: Train 0.5 | Epoch: 27 | Iter: 85000 | Total Loss: 0.004361 | Recon Loss: 0.003639 | Commit Loss: 0.001444 | Perplexity: 646.633831
Trainning Epoch:  17%|█▋        | 28/165 [56:35<4:31:05, 118.72s/it]2025-09-14 17:19:29,901 Stage: Train 0.5 | Epoch: 28 | Iter: 85200 | Total Loss: 0.004241 | Recon Loss: 0.003510 | Commit Loss: 0.001462 | Perplexity: 646.968627
2025-09-14 17:19:37,573 Stage: Train 0.5 | Epoch: 28 | Iter: 85400 | Total Loss: 0.004237 | Recon Loss: 0.003509 | Commit Loss: 0.001457 | Perplexity: 647.949259
2025-09-14 17:19:45,270 Stage: Train 0.5 | Epoch: 28 | Iter: 85600 | Total Loss: 0.004272 | Recon Loss: 0.003544 | Commit Loss: 0.001457 | Perplexity: 651.271037
2025-09-14 17:19:52,965 Stage: Train 0.5 | Epoch: 28 | Iter: 85800 | Total Loss: 0.004295 | Recon Loss: 0.003565 | Commit Loss: 0.001460 | Perplexity: 651.191015
2025-09-14 17:20:00,666 Stage: Train 0.5 | Epoch: 28 | Iter: 86000 | Total Loss: 0.004208 | Recon Loss: 0.003478 | Commit Loss: 0.001459 | Perplexity: 647.156961
2025-09-14 17:20:08,350 Stage: Train 0.5 | Epoch: 28 | Iter: 86200 | Total Loss: 0.004209 | Recon Loss: 0.003482 | Commit Loss: 0.001454 | Perplexity: 648.952054
2025-09-14 17:20:16,061 Stage: Train 0.5 | Epoch: 28 | Iter: 86400 | Total Loss: 0.004286 | Recon Loss: 0.003554 | Commit Loss: 0.001462 | Perplexity: 646.175800
2025-09-14 17:20:23,753 Stage: Train 0.5 | Epoch: 28 | Iter: 86600 | Total Loss: 0.004242 | Recon Loss: 0.003507 | Commit Loss: 0.001471 | Perplexity: 647.367351
2025-09-14 17:20:31,449 Stage: Train 0.5 | Epoch: 28 | Iter: 86800 | Total Loss: 0.004222 | Recon Loss: 0.003497 | Commit Loss: 0.001451 | Perplexity: 647.683971
2025-09-14 17:20:39,130 Stage: Train 0.5 | Epoch: 28 | Iter: 87000 | Total Loss: 0.004253 | Recon Loss: 0.003524 | Commit Loss: 0.001457 | Perplexity: 647.440495
2025-09-14 17:20:46,809 Stage: Train 0.5 | Epoch: 28 | Iter: 87200 | Total Loss: 0.004219 | Recon Loss: 0.003486 | Commit Loss: 0.001466 | Perplexity: 649.227168
2025-09-14 17:20:54,498 Stage: Train 0.5 | Epoch: 28 | Iter: 87400 | Total Loss: 0.004284 | Recon Loss: 0.003550 | Commit Loss: 0.001469 | Perplexity: 649.961199
2025-09-14 17:21:02,503 Stage: Train 0.5 | Epoch: 28 | Iter: 87600 | Total Loss: 0.004266 | Recon Loss: 0.003541 | Commit Loss: 0.001450 | Perplexity: 649.006836
2025-09-14 17:21:10,505 Stage: Train 0.5 | Epoch: 28 | Iter: 87800 | Total Loss: 0.004256 | Recon Loss: 0.003520 | Commit Loss: 0.001472 | Perplexity: 652.168746
2025-09-14 17:21:18,452 Stage: Train 0.5 | Epoch: 28 | Iter: 88000 | Total Loss: 0.004226 | Recon Loss: 0.003493 | Commit Loss: 0.001466 | Perplexity: 648.513485
Trainning Epoch:  18%|█▊        | 29/165 [58:33<4:28:34, 118.49s/it]2025-09-14 17:21:26,562 Stage: Train 0.5 | Epoch: 29 | Iter: 88200 | Total Loss: 0.004194 | Recon Loss: 0.003470 | Commit Loss: 0.001447 | Perplexity: 646.904068
2025-09-14 17:21:34,631 Stage: Train 0.5 | Epoch: 29 | Iter: 88400 | Total Loss: 0.004201 | Recon Loss: 0.003472 | Commit Loss: 0.001457 | Perplexity: 648.544790
2025-09-14 17:21:42,782 Stage: Train 0.5 | Epoch: 29 | Iter: 88600 | Total Loss: 0.004305 | Recon Loss: 0.003576 | Commit Loss: 0.001459 | Perplexity: 650.309930
2025-09-14 17:21:50,878 Stage: Train 0.5 | Epoch: 29 | Iter: 88800 | Total Loss: 0.004250 | Recon Loss: 0.003518 | Commit Loss: 0.001464 | Perplexity: 650.832752
2025-09-14 17:21:58,686 Stage: Train 0.5 | Epoch: 29 | Iter: 89000 | Total Loss: 0.004229 | Recon Loss: 0.003505 | Commit Loss: 0.001448 | Perplexity: 648.287281
2025-09-14 17:22:06,794 Stage: Train 0.5 | Epoch: 29 | Iter: 89200 | Total Loss: 0.004260 | Recon Loss: 0.003536 | Commit Loss: 0.001447 | Perplexity: 645.196758
2025-09-14 17:22:14,889 Stage: Train 0.5 | Epoch: 29 | Iter: 89400 | Total Loss: 0.004246 | Recon Loss: 0.003523 | Commit Loss: 0.001447 | Perplexity: 650.095678
2025-09-14 17:22:22,970 Stage: Train 0.5 | Epoch: 29 | Iter: 89600 | Total Loss: 0.004200 | Recon Loss: 0.003469 | Commit Loss: 0.001463 | Perplexity: 649.559385
2025-09-14 17:22:30,934 Stage: Train 0.5 | Epoch: 29 | Iter: 89800 | Total Loss: 0.004265 | Recon Loss: 0.003535 | Commit Loss: 0.001461 | Perplexity: 648.144915
2025-09-14 17:22:38,657 Stage: Train 0.5 | Epoch: 29 | Iter: 90000 | Total Loss: 0.004264 | Recon Loss: 0.003527 | Commit Loss: 0.001475 | Perplexity: 650.407516
2025-09-14 17:22:46,358 Stage: Train 0.5 | Epoch: 29 | Iter: 90200 | Total Loss: 0.004193 | Recon Loss: 0.003460 | Commit Loss: 0.001465 | Perplexity: 650.850677
2025-09-14 17:22:54,118 Stage: Train 0.5 | Epoch: 29 | Iter: 90400 | Total Loss: 0.004252 | Recon Loss: 0.003526 | Commit Loss: 0.001452 | Perplexity: 648.772158
2025-09-14 17:23:01,802 Stage: Train 0.5 | Epoch: 29 | Iter: 90600 | Total Loss: 0.004207 | Recon Loss: 0.003471 | Commit Loss: 0.001473 | Perplexity: 650.547798
2025-09-14 17:23:09,501 Stage: Train 0.5 | Epoch: 29 | Iter: 90800 | Total Loss: 0.004177 | Recon Loss: 0.003449 | Commit Loss: 0.001457 | Perplexity: 649.421325
2025-09-14 17:23:17,181 Stage: Train 0.5 | Epoch: 29 | Iter: 91000 | Total Loss: 0.004329 | Recon Loss: 0.003601 | Commit Loss: 0.001457 | Perplexity: 648.642791
Trainning Epoch:  18%|█▊        | 30/165 [1:00:33<4:27:35, 118.93s/it]2025-09-14 17:23:24,877 Stage: Train 0.5 | Epoch: 30 | Iter: 91200 | Total Loss: 0.004224 | Recon Loss: 0.003496 | Commit Loss: 0.001455 | Perplexity: 646.945621
2025-09-14 17:23:32,621 Stage: Train 0.5 | Epoch: 30 | Iter: 91400 | Total Loss: 0.004156 | Recon Loss: 0.003425 | Commit Loss: 0.001462 | Perplexity: 649.380327
2025-09-14 17:23:40,315 Stage: Train 0.5 | Epoch: 30 | Iter: 91600 | Total Loss: 0.004278 | Recon Loss: 0.003564 | Commit Loss: 0.001427 | Perplexity: 645.758169
2025-09-14 17:23:48,009 Stage: Train 0.5 | Epoch: 30 | Iter: 91800 | Total Loss: 0.004177 | Recon Loss: 0.003450 | Commit Loss: 0.001453 | Perplexity: 644.920081
2025-09-14 17:23:55,705 Stage: Train 0.5 | Epoch: 30 | Iter: 92000 | Total Loss: 0.004179 | Recon Loss: 0.003454 | Commit Loss: 0.001450 | Perplexity: 647.437409
2025-09-14 17:24:03,381 Stage: Train 0.5 | Epoch: 30 | Iter: 92200 | Total Loss: 0.004144 | Recon Loss: 0.003426 | Commit Loss: 0.001437 | Perplexity: 648.394663
2025-09-14 17:24:11,101 Stage: Train 0.5 | Epoch: 30 | Iter: 92400 | Total Loss: 0.004121 | Recon Loss: 0.003390 | Commit Loss: 0.001462 | Perplexity: 652.281032
2025-09-14 17:24:18,854 Stage: Train 0.5 | Epoch: 30 | Iter: 92600 | Total Loss: 0.004186 | Recon Loss: 0.003463 | Commit Loss: 0.001448 | Perplexity: 650.009832
2025-09-14 17:24:26,581 Stage: Train 0.5 | Epoch: 30 | Iter: 92800 | Total Loss: 0.004115 | Recon Loss: 0.003389 | Commit Loss: 0.001452 | Perplexity: 648.206611
2025-09-14 17:24:34,286 Stage: Train 0.5 | Epoch: 30 | Iter: 93000 | Total Loss: 0.004194 | Recon Loss: 0.003464 | Commit Loss: 0.001461 | Perplexity: 650.229576
2025-09-14 17:24:41,970 Stage: Train 0.5 | Epoch: 30 | Iter: 93200 | Total Loss: 0.004086 | Recon Loss: 0.003352 | Commit Loss: 0.001469 | Perplexity: 650.727144
2025-09-14 17:24:49,680 Stage: Train 0.5 | Epoch: 30 | Iter: 93400 | Total Loss: 0.004187 | Recon Loss: 0.003457 | Commit Loss: 0.001461 | Perplexity: 650.130917
2025-09-14 17:24:57,473 Stage: Train 0.5 | Epoch: 30 | Iter: 93600 | Total Loss: 0.004209 | Recon Loss: 0.003492 | Commit Loss: 0.001435 | Perplexity: 645.497264
2025-09-14 17:25:05,258 Stage: Train 0.5 | Epoch: 30 | Iter: 93800 | Total Loss: 0.004152 | Recon Loss: 0.003428 | Commit Loss: 0.001447 | Perplexity: 648.498823
2025-09-14 17:25:12,978 Stage: Train 0.5 | Epoch: 30 | Iter: 94000 | Total Loss: 0.004183 | Recon Loss: 0.003456 | Commit Loss: 0.001454 | Perplexity: 648.354930
Trainning Epoch:  19%|█▉        | 31/165 [1:02:30<4:24:33, 118.46s/it]2025-09-14 17:25:20,778 Stage: Train 0.5 | Epoch: 31 | Iter: 94200 | Total Loss: 0.004200 | Recon Loss: 0.003472 | Commit Loss: 0.001457 | Perplexity: 648.901221
2025-09-14 17:25:28,530 Stage: Train 0.5 | Epoch: 31 | Iter: 94400 | Total Loss: 0.004209 | Recon Loss: 0.003481 | Commit Loss: 0.001454 | Perplexity: 648.696689
2025-09-14 17:25:36,273 Stage: Train 0.5 | Epoch: 31 | Iter: 94600 | Total Loss: 0.004171 | Recon Loss: 0.003445 | Commit Loss: 0.001452 | Perplexity: 649.081382
2025-09-14 17:25:43,966 Stage: Train 0.5 | Epoch: 31 | Iter: 94800 | Total Loss: 0.004140 | Recon Loss: 0.003421 | Commit Loss: 0.001438 | Perplexity: 645.555818
2025-09-14 17:25:51,709 Stage: Train 0.5 | Epoch: 31 | Iter: 95000 | Total Loss: 0.004162 | Recon Loss: 0.003438 | Commit Loss: 0.001448 | Perplexity: 649.135962
2025-09-14 17:25:59,430 Stage: Train 0.5 | Epoch: 31 | Iter: 95200 | Total Loss: 0.004215 | Recon Loss: 0.003491 | Commit Loss: 0.001447 | Perplexity: 647.128236
2025-09-14 17:26:07,194 Stage: Train 0.5 | Epoch: 31 | Iter: 95400 | Total Loss: 0.004159 | Recon Loss: 0.003442 | Commit Loss: 0.001435 | Perplexity: 645.453582
2025-09-14 17:26:15,017 Stage: Train 0.5 | Epoch: 31 | Iter: 95600 | Total Loss: 0.004144 | Recon Loss: 0.003418 | Commit Loss: 0.001452 | Perplexity: 649.628834
2025-09-14 17:26:22,849 Stage: Train 0.5 | Epoch: 31 | Iter: 95800 | Total Loss: 0.004177 | Recon Loss: 0.003457 | Commit Loss: 0.001441 | Perplexity: 650.067319
2025-09-14 17:26:30,573 Stage: Train 0.5 | Epoch: 31 | Iter: 96000 | Total Loss: 0.004257 | Recon Loss: 0.003533 | Commit Loss: 0.001449 | Perplexity: 646.752250
2025-09-14 17:26:38,298 Stage: Train 0.5 | Epoch: 31 | Iter: 96200 | Total Loss: 0.004106 | Recon Loss: 0.003379 | Commit Loss: 0.001453 | Perplexity: 648.559540
2025-09-14 17:26:46,065 Stage: Train 0.5 | Epoch: 31 | Iter: 96400 | Total Loss: 0.004183 | Recon Loss: 0.003458 | Commit Loss: 0.001449 | Perplexity: 649.786992
2025-09-14 17:26:53,801 Stage: Train 0.5 | Epoch: 31 | Iter: 96600 | Total Loss: 0.004142 | Recon Loss: 0.003418 | Commit Loss: 0.001448 | Perplexity: 647.323810
2025-09-14 17:27:01,529 Stage: Train 0.5 | Epoch: 31 | Iter: 96800 | Total Loss: 0.004091 | Recon Loss: 0.003367 | Commit Loss: 0.001449 | Perplexity: 651.968141
2025-09-14 17:27:09,265 Stage: Train 0.5 | Epoch: 31 | Iter: 97000 | Total Loss: 0.004233 | Recon Loss: 0.003503 | Commit Loss: 0.001460 | Perplexity: 650.485107
2025-09-14 17:27:16,989 Stage: Train 0.5 | Epoch: 31 | Iter: 97200 | Total Loss: 0.004143 | Recon Loss: 0.003422 | Commit Loss: 0.001444 | Perplexity: 647.410202
Trainning Epoch:  19%|█▉        | 32/165 [1:04:28<4:22:04, 118.23s/it]2025-09-14 17:27:24,727 Stage: Train 0.5 | Epoch: 32 | Iter: 97400 | Total Loss: 0.004141 | Recon Loss: 0.003419 | Commit Loss: 0.001443 | Perplexity: 648.066994
2025-09-14 17:27:32,670 Stage: Train 0.5 | Epoch: 32 | Iter: 97600 | Total Loss: 0.004119 | Recon Loss: 0.003396 | Commit Loss: 0.001446 | Perplexity: 648.995380
2025-09-14 17:27:40,860 Stage: Train 0.5 | Epoch: 32 | Iter: 97800 | Total Loss: 0.004152 | Recon Loss: 0.003435 | Commit Loss: 0.001435 | Perplexity: 645.256514
2025-09-14 17:27:48,865 Stage: Train 0.5 | Epoch: 32 | Iter: 98000 | Total Loss: 0.004141 | Recon Loss: 0.003419 | Commit Loss: 0.001444 | Perplexity: 648.285844
2025-09-14 17:27:56,851 Stage: Train 0.5 | Epoch: 32 | Iter: 98200 | Total Loss: 0.004107 | Recon Loss: 0.003381 | Commit Loss: 0.001452 | Perplexity: 646.220137
2025-09-14 17:28:04,974 Stage: Train 0.5 | Epoch: 32 | Iter: 98400 | Total Loss: 0.004156 | Recon Loss: 0.003431 | Commit Loss: 0.001449 | Perplexity: 648.117841
2025-09-14 17:28:13,041 Stage: Train 0.5 | Epoch: 32 | Iter: 98600 | Total Loss: 0.004090 | Recon Loss: 0.003367 | Commit Loss: 0.001445 | Perplexity: 649.964471
2025-09-14 17:28:20,939 Stage: Train 0.5 | Epoch: 32 | Iter: 98800 | Total Loss: 0.004112 | Recon Loss: 0.003395 | Commit Loss: 0.001434 | Perplexity: 646.961549
2025-09-14 17:28:29,044 Stage: Train 0.5 | Epoch: 32 | Iter: 99000 | Total Loss: 0.004123 | Recon Loss: 0.003398 | Commit Loss: 0.001450 | Perplexity: 652.174420
2025-09-14 17:28:37,176 Stage: Train 0.5 | Epoch: 32 | Iter: 99200 | Total Loss: 0.004110 | Recon Loss: 0.003378 | Commit Loss: 0.001464 | Perplexity: 648.489575
2025-09-14 17:28:45,302 Stage: Train 0.5 | Epoch: 32 | Iter: 99400 | Total Loss: 0.004164 | Recon Loss: 0.003432 | Commit Loss: 0.001464 | Perplexity: 649.977899
2025-09-14 17:28:53,442 Stage: Train 0.5 | Epoch: 32 | Iter: 99600 | Total Loss: 0.004108 | Recon Loss: 0.003384 | Commit Loss: 0.001447 | Perplexity: 648.673423
2025-09-14 17:29:01,515 Stage: Train 0.5 | Epoch: 32 | Iter: 99800 | Total Loss: 0.004167 | Recon Loss: 0.003439 | Commit Loss: 0.001457 | Perplexity: 653.714937
2025-09-14 17:29:09,646 Stage: Train 0.5 | Epoch: 32 | Iter: 100000 | Total Loss: 0.004241 | Recon Loss: 0.003518 | Commit Loss: 0.001446 | Perplexity: 648.517901
2025-09-14 17:29:09,646 Saving model at iteration 100000
2025-09-14 17:29:10,181 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_33_step_100000
2025-09-14 17:29:10,316 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_33_step_100000/pytorch_model.bin
2025-09-14 17:29:10,564 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_33_step_100000/optimizer.bin
2025-09-14 17:29:10,564 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_33_step_100000/scheduler.bin
2025-09-14 17:29:10,565 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_33_step_100000/random_states_0.pkl
2025-09-14 17:29:18,667 Stage: Train 0.5 | Epoch: 32 | Iter: 100200 | Total Loss: 0.004195 | Recon Loss: 0.003484 | Commit Loss: 0.001422 | Perplexity: 645.034872
Trainning Epoch:  20%|██        | 33/165 [1:06:31<4:23:31, 119.79s/it]2025-09-14 17:29:26,962 Stage: Train 0.5 | Epoch: 33 | Iter: 100400 | Total Loss: 0.004119 | Recon Loss: 0.003399 | Commit Loss: 0.001440 | Perplexity: 645.921310
2025-09-14 17:29:35,088 Stage: Train 0.5 | Epoch: 33 | Iter: 100600 | Total Loss: 0.004082 | Recon Loss: 0.003367 | Commit Loss: 0.001431 | Perplexity: 648.054453
2025-09-14 17:29:43,212 Stage: Train 0.5 | Epoch: 33 | Iter: 100800 | Total Loss: 0.004063 | Recon Loss: 0.003341 | Commit Loss: 0.001445 | Perplexity: 648.976443
2025-09-14 17:29:51,358 Stage: Train 0.5 | Epoch: 33 | Iter: 101000 | Total Loss: 0.004142 | Recon Loss: 0.003424 | Commit Loss: 0.001436 | Perplexity: 646.266917
2025-09-14 17:29:59,417 Stage: Train 0.5 | Epoch: 33 | Iter: 101200 | Total Loss: 0.004097 | Recon Loss: 0.003371 | Commit Loss: 0.001450 | Perplexity: 649.619738
2025-09-14 17:30:07,523 Stage: Train 0.5 | Epoch: 33 | Iter: 101400 | Total Loss: 0.004121 | Recon Loss: 0.003399 | Commit Loss: 0.001443 | Perplexity: 648.483647
2025-09-14 17:30:15,587 Stage: Train 0.5 | Epoch: 33 | Iter: 101600 | Total Loss: 0.004132 | Recon Loss: 0.003410 | Commit Loss: 0.001444 | Perplexity: 647.920594
2025-09-14 17:30:23,405 Stage: Train 0.5 | Epoch: 33 | Iter: 101800 | Total Loss: 0.004127 | Recon Loss: 0.003407 | Commit Loss: 0.001439 | Perplexity: 647.061327
2025-09-14 17:30:31,109 Stage: Train 0.5 | Epoch: 33 | Iter: 102000 | Total Loss: 0.004079 | Recon Loss: 0.003361 | Commit Loss: 0.001437 | Perplexity: 647.467590
2025-09-14 17:30:39,175 Stage: Train 0.5 | Epoch: 33 | Iter: 102200 | Total Loss: 0.004170 | Recon Loss: 0.003452 | Commit Loss: 0.001437 | Perplexity: 648.675443
2025-09-14 17:30:47,298 Stage: Train 0.5 | Epoch: 33 | Iter: 102400 | Total Loss: 0.004069 | Recon Loss: 0.003349 | Commit Loss: 0.001440 | Perplexity: 649.006334
2025-09-14 17:30:55,416 Stage: Train 0.5 | Epoch: 33 | Iter: 102600 | Total Loss: 0.004089 | Recon Loss: 0.003367 | Commit Loss: 0.001445 | Perplexity: 647.550428
2025-09-14 17:31:03,541 Stage: Train 0.5 | Epoch: 33 | Iter: 102800 | Total Loss: 0.004151 | Recon Loss: 0.003428 | Commit Loss: 0.001444 | Perplexity: 648.272012
2025-09-14 17:31:11,682 Stage: Train 0.5 | Epoch: 33 | Iter: 103000 | Total Loss: 0.004020 | Recon Loss: 0.003302 | Commit Loss: 0.001435 | Perplexity: 648.114546
2025-09-14 17:31:19,565 Stage: Train 0.5 | Epoch: 33 | Iter: 103200 | Total Loss: 0.004150 | Recon Loss: 0.003430 | Commit Loss: 0.001438 | Perplexity: 648.294966
Trainning Epoch:  21%|██        | 34/165 [1:08:33<4:23:02, 120.48s/it]2025-09-14 17:31:27,365 Stage: Train 0.5 | Epoch: 34 | Iter: 103400 | Total Loss: 0.004078 | Recon Loss: 0.003358 | Commit Loss: 0.001441 | Perplexity: 647.705481
2025-09-14 17:31:35,157 Stage: Train 0.5 | Epoch: 34 | Iter: 103600 | Total Loss: 0.004138 | Recon Loss: 0.003418 | Commit Loss: 0.001441 | Perplexity: 649.344417
2025-09-14 17:31:42,981 Stage: Train 0.5 | Epoch: 34 | Iter: 103800 | Total Loss: 0.004105 | Recon Loss: 0.003376 | Commit Loss: 0.001459 | Perplexity: 649.127561
2025-09-14 17:31:50,797 Stage: Train 0.5 | Epoch: 34 | Iter: 104000 | Total Loss: 0.004027 | Recon Loss: 0.003305 | Commit Loss: 0.001445 | Perplexity: 651.803956
2025-09-14 17:31:58,835 Stage: Train 0.5 | Epoch: 34 | Iter: 104200 | Total Loss: 0.004082 | Recon Loss: 0.003357 | Commit Loss: 0.001451 | Perplexity: 652.034480
2025-09-14 17:32:06,838 Stage: Train 0.5 | Epoch: 34 | Iter: 104400 | Total Loss: 0.004072 | Recon Loss: 0.003354 | Commit Loss: 0.001435 | Perplexity: 650.579720
2025-09-14 17:32:14,567 Stage: Train 0.5 | Epoch: 34 | Iter: 104600 | Total Loss: 0.004095 | Recon Loss: 0.003380 | Commit Loss: 0.001429 | Perplexity: 644.708855
2025-09-14 17:32:22,529 Stage: Train 0.5 | Epoch: 34 | Iter: 104800 | Total Loss: 0.004060 | Recon Loss: 0.003339 | Commit Loss: 0.001444 | Perplexity: 649.626885
2025-09-14 17:32:30,366 Stage: Train 0.5 | Epoch: 34 | Iter: 105000 | Total Loss: 0.004077 | Recon Loss: 0.003357 | Commit Loss: 0.001440 | Perplexity: 648.360515
2025-09-14 17:32:38,216 Stage: Train 0.5 | Epoch: 34 | Iter: 105200 | Total Loss: 0.004057 | Recon Loss: 0.003338 | Commit Loss: 0.001438 | Perplexity: 651.704588
2025-09-14 17:32:46,031 Stage: Train 0.5 | Epoch: 34 | Iter: 105400 | Total Loss: 0.003991 | Recon Loss: 0.003271 | Commit Loss: 0.001441 | Perplexity: 652.966701
2025-09-14 17:32:53,837 Stage: Train 0.5 | Epoch: 34 | Iter: 105600 | Total Loss: 0.004069 | Recon Loss: 0.003344 | Commit Loss: 0.001449 | Perplexity: 649.173788
2025-09-14 17:33:01,657 Stage: Train 0.5 | Epoch: 34 | Iter: 105800 | Total Loss: 0.004109 | Recon Loss: 0.003393 | Commit Loss: 0.001432 | Perplexity: 648.860612
2025-09-14 17:33:09,503 Stage: Train 0.5 | Epoch: 34 | Iter: 106000 | Total Loss: 0.004070 | Recon Loss: 0.003352 | Commit Loss: 0.001436 | Perplexity: 650.646100
2025-09-14 17:33:17,410 Stage: Train 0.5 | Epoch: 34 | Iter: 106200 | Total Loss: 0.004080 | Recon Loss: 0.003361 | Commit Loss: 0.001439 | Perplexity: 650.569946
Trainning Epoch:  21%|██        | 35/165 [1:10:33<4:20:19, 120.15s/it]2025-09-14 17:33:25,283 Stage: Train 0.5 | Epoch: 35 | Iter: 106400 | Total Loss: 0.004081 | Recon Loss: 0.003357 | Commit Loss: 0.001448 | Perplexity: 647.723126
2025-09-14 17:33:33,121 Stage: Train 0.5 | Epoch: 35 | Iter: 106600 | Total Loss: 0.004071 | Recon Loss: 0.003355 | Commit Loss: 0.001431 | Perplexity: 648.946189
2025-09-14 17:33:40,934 Stage: Train 0.5 | Epoch: 35 | Iter: 106800 | Total Loss: 0.004026 | Recon Loss: 0.003311 | Commit Loss: 0.001430 | Perplexity: 649.851456
2025-09-14 17:33:48,802 Stage: Train 0.5 | Epoch: 35 | Iter: 107000 | Total Loss: 0.004075 | Recon Loss: 0.003353 | Commit Loss: 0.001443 | Perplexity: 652.114786
2025-09-14 17:33:56,589 Stage: Train 0.5 | Epoch: 35 | Iter: 107200 | Total Loss: 0.004003 | Recon Loss: 0.003291 | Commit Loss: 0.001423 | Perplexity: 651.027216
2025-09-14 17:34:04,390 Stage: Train 0.5 | Epoch: 35 | Iter: 107400 | Total Loss: 0.004106 | Recon Loss: 0.003387 | Commit Loss: 0.001438 | Perplexity: 649.610786
2025-09-14 17:34:12,197 Stage: Train 0.5 | Epoch: 35 | Iter: 107600 | Total Loss: 0.004083 | Recon Loss: 0.003361 | Commit Loss: 0.001444 | Perplexity: 652.883654
2025-09-14 17:34:20,057 Stage: Train 0.5 | Epoch: 35 | Iter: 107800 | Total Loss: 0.004053 | Recon Loss: 0.003339 | Commit Loss: 0.001428 | Perplexity: 651.103888
2025-09-14 17:34:27,857 Stage: Train 0.5 | Epoch: 35 | Iter: 108000 | Total Loss: 0.004022 | Recon Loss: 0.003305 | Commit Loss: 0.001433 | Perplexity: 650.674575
2025-09-14 17:34:35,875 Stage: Train 0.5 | Epoch: 35 | Iter: 108200 | Total Loss: 0.004055 | Recon Loss: 0.003335 | Commit Loss: 0.001441 | Perplexity: 652.359283
2025-09-14 17:34:44,016 Stage: Train 0.5 | Epoch: 35 | Iter: 108400 | Total Loss: 0.004035 | Recon Loss: 0.003326 | Commit Loss: 0.001417 | Perplexity: 649.510486
2025-09-14 17:34:52,172 Stage: Train 0.5 | Epoch: 35 | Iter: 108600 | Total Loss: 0.004099 | Recon Loss: 0.003382 | Commit Loss: 0.001434 | Perplexity: 649.727602
2025-09-14 17:35:00,309 Stage: Train 0.5 | Epoch: 35 | Iter: 108800 | Total Loss: 0.004088 | Recon Loss: 0.003378 | Commit Loss: 0.001419 | Perplexity: 646.867993
2025-09-14 17:35:08,454 Stage: Train 0.5 | Epoch: 35 | Iter: 109000 | Total Loss: 0.004068 | Recon Loss: 0.003352 | Commit Loss: 0.001431 | Perplexity: 644.985047
2025-09-14 17:35:16,592 Stage: Train 0.5 | Epoch: 35 | Iter: 109200 | Total Loss: 0.004107 | Recon Loss: 0.003387 | Commit Loss: 0.001440 | Perplexity: 649.746800
Trainning Epoch:  22%|██▏       | 36/165 [1:12:34<4:18:49, 120.38s/it]2025-09-14 17:35:24,748 Stage: Train 0.5 | Epoch: 36 | Iter: 109400 | Total Loss: 0.003981 | Recon Loss: 0.003269 | Commit Loss: 0.001424 | Perplexity: 649.114029
2025-09-14 17:35:32,832 Stage: Train 0.5 | Epoch: 36 | Iter: 109600 | Total Loss: 0.004030 | Recon Loss: 0.003312 | Commit Loss: 0.001436 | Perplexity: 651.494042
2025-09-14 17:35:40,669 Stage: Train 0.5 | Epoch: 36 | Iter: 109800 | Total Loss: 0.004033 | Recon Loss: 0.003320 | Commit Loss: 0.001425 | Perplexity: 650.422561
2025-09-14 17:35:48,485 Stage: Train 0.5 | Epoch: 36 | Iter: 110000 | Total Loss: 0.004040 | Recon Loss: 0.003322 | Commit Loss: 0.001434 | Perplexity: 651.478244
2025-09-14 17:35:56,270 Stage: Train 0.5 | Epoch: 36 | Iter: 110200 | Total Loss: 0.004063 | Recon Loss: 0.003350 | Commit Loss: 0.001425 | Perplexity: 650.956304
2025-09-14 17:36:04,067 Stage: Train 0.5 | Epoch: 36 | Iter: 110400 | Total Loss: 0.004001 | Recon Loss: 0.003291 | Commit Loss: 0.001420 | Perplexity: 650.355935
2025-09-14 17:36:11,858 Stage: Train 0.5 | Epoch: 36 | Iter: 110600 | Total Loss: 0.004029 | Recon Loss: 0.003321 | Commit Loss: 0.001415 | Perplexity: 648.911310
2025-09-14 17:36:19,666 Stage: Train 0.5 | Epoch: 36 | Iter: 110800 | Total Loss: 0.003983 | Recon Loss: 0.003273 | Commit Loss: 0.001420 | Perplexity: 650.594502
2025-09-14 17:36:27,542 Stage: Train 0.5 | Epoch: 36 | Iter: 111000 | Total Loss: 0.004045 | Recon Loss: 0.003332 | Commit Loss: 0.001427 | Perplexity: 651.912829
2025-09-14 17:36:35,370 Stage: Train 0.5 | Epoch: 36 | Iter: 111200 | Total Loss: 0.003983 | Recon Loss: 0.003270 | Commit Loss: 0.001425 | Perplexity: 650.130856
2025-09-14 17:36:43,247 Stage: Train 0.5 | Epoch: 36 | Iter: 111400 | Total Loss: 0.004029 | Recon Loss: 0.003309 | Commit Loss: 0.001439 | Perplexity: 655.447159
2025-09-14 17:36:51,133 Stage: Train 0.5 | Epoch: 36 | Iter: 111600 | Total Loss: 0.004033 | Recon Loss: 0.003325 | Commit Loss: 0.001416 | Perplexity: 650.101996
2025-09-14 17:36:58,897 Stage: Train 0.5 | Epoch: 36 | Iter: 111800 | Total Loss: 0.004071 | Recon Loss: 0.003355 | Commit Loss: 0.001432 | Perplexity: 650.813674
2025-09-14 17:37:06,647 Stage: Train 0.5 | Epoch: 36 | Iter: 112000 | Total Loss: 0.004064 | Recon Loss: 0.003355 | Commit Loss: 0.001419 | Perplexity: 652.731230
2025-09-14 17:37:14,485 Stage: Train 0.5 | Epoch: 36 | Iter: 112200 | Total Loss: 0.003997 | Recon Loss: 0.003281 | Commit Loss: 0.001432 | Perplexity: 651.602490
2025-09-14 17:37:22,634 Stage: Train 0.5 | Epoch: 36 | Iter: 112400 | Total Loss: 0.004020 | Recon Loss: 0.003307 | Commit Loss: 0.001426 | Perplexity: 652.610322
Trainning Epoch:  22%|██▏       | 37/165 [1:14:33<4:16:12, 120.10s/it]2025-09-14 17:37:30,831 Stage: Train 0.5 | Epoch: 37 | Iter: 112600 | Total Loss: 0.003984 | Recon Loss: 0.003271 | Commit Loss: 0.001426 | Perplexity: 653.106702
2025-09-14 17:37:38,999 Stage: Train 0.5 | Epoch: 37 | Iter: 112800 | Total Loss: 0.004043 | Recon Loss: 0.003336 | Commit Loss: 0.001414 | Perplexity: 652.078055
2025-09-14 17:37:47,145 Stage: Train 0.5 | Epoch: 37 | Iter: 113000 | Total Loss: 0.004072 | Recon Loss: 0.003365 | Commit Loss: 0.001414 | Perplexity: 650.627257
2025-09-14 17:37:55,186 Stage: Train 0.5 | Epoch: 37 | Iter: 113200 | Total Loss: 0.003999 | Recon Loss: 0.003280 | Commit Loss: 0.001439 | Perplexity: 652.889201
2025-09-14 17:38:03,005 Stage: Train 0.5 | Epoch: 37 | Iter: 113400 | Total Loss: 0.004020 | Recon Loss: 0.003310 | Commit Loss: 0.001420 | Perplexity: 648.779675
2025-09-14 17:38:10,783 Stage: Train 0.5 | Epoch: 37 | Iter: 113600 | Total Loss: 0.004018 | Recon Loss: 0.003316 | Commit Loss: 0.001404 | Perplexity: 651.763314
2025-09-14 17:38:18,790 Stage: Train 0.5 | Epoch: 37 | Iter: 113800 | Total Loss: 0.003984 | Recon Loss: 0.003272 | Commit Loss: 0.001423 | Perplexity: 655.103755
2025-09-14 17:38:26,565 Stage: Train 0.5 | Epoch: 37 | Iter: 114000 | Total Loss: 0.003959 | Recon Loss: 0.003250 | Commit Loss: 0.001418 | Perplexity: 650.142689
2025-09-14 17:38:34,317 Stage: Train 0.5 | Epoch: 37 | Iter: 114200 | Total Loss: 0.004024 | Recon Loss: 0.003322 | Commit Loss: 0.001403 | Perplexity: 649.540768
2025-09-14 17:38:42,053 Stage: Train 0.5 | Epoch: 37 | Iter: 114400 | Total Loss: 0.003972 | Recon Loss: 0.003270 | Commit Loss: 0.001406 | Perplexity: 653.315957
2025-09-14 17:38:49,802 Stage: Train 0.5 | Epoch: 37 | Iter: 114600 | Total Loss: 0.003961 | Recon Loss: 0.003247 | Commit Loss: 0.001426 | Perplexity: 651.264039
2025-09-14 17:38:57,546 Stage: Train 0.5 | Epoch: 37 | Iter: 114800 | Total Loss: 0.004057 | Recon Loss: 0.003347 | Commit Loss: 0.001420 | Perplexity: 654.971394
2025-09-14 17:39:05,261 Stage: Train 0.5 | Epoch: 37 | Iter: 115000 | Total Loss: 0.003970 | Recon Loss: 0.003255 | Commit Loss: 0.001431 | Perplexity: 651.759443
2025-09-14 17:39:13,014 Stage: Train 0.5 | Epoch: 37 | Iter: 115200 | Total Loss: 0.004011 | Recon Loss: 0.003292 | Commit Loss: 0.001438 | Perplexity: 651.877607
2025-09-14 17:39:20,741 Stage: Train 0.5 | Epoch: 37 | Iter: 115400 | Total Loss: 0.003995 | Recon Loss: 0.003281 | Commit Loss: 0.001427 | Perplexity: 650.153759
Trainning Epoch:  23%|██▎       | 38/165 [1:16:33<4:13:52, 119.94s/it]2025-09-14 17:39:28,530 Stage: Train 0.5 | Epoch: 38 | Iter: 115600 | Total Loss: 0.003984 | Recon Loss: 0.003281 | Commit Loss: 0.001406 | Perplexity: 648.394866
2025-09-14 17:39:36,342 Stage: Train 0.5 | Epoch: 38 | Iter: 115800 | Total Loss: 0.003944 | Recon Loss: 0.003235 | Commit Loss: 0.001419 | Perplexity: 651.063726
2025-09-14 17:39:44,221 Stage: Train 0.5 | Epoch: 38 | Iter: 116000 | Total Loss: 0.004027 | Recon Loss: 0.003320 | Commit Loss: 0.001415 | Perplexity: 652.311314
2025-09-14 17:39:51,948 Stage: Train 0.5 | Epoch: 38 | Iter: 116200 | Total Loss: 0.003956 | Recon Loss: 0.003249 | Commit Loss: 0.001416 | Perplexity: 650.549236
2025-09-14 17:39:59,677 Stage: Train 0.5 | Epoch: 38 | Iter: 116400 | Total Loss: 0.004015 | Recon Loss: 0.003310 | Commit Loss: 0.001408 | Perplexity: 651.161870
2025-09-14 17:40:07,382 Stage: Train 0.5 | Epoch: 38 | Iter: 116600 | Total Loss: 0.003962 | Recon Loss: 0.003255 | Commit Loss: 0.001414 | Perplexity: 652.108820
2025-09-14 17:40:15,071 Stage: Train 0.5 | Epoch: 38 | Iter: 116800 | Total Loss: 0.004025 | Recon Loss: 0.003324 | Commit Loss: 0.001403 | Perplexity: 649.186141
2025-09-14 17:40:22,769 Stage: Train 0.5 | Epoch: 38 | Iter: 117000 | Total Loss: 0.004097 | Recon Loss: 0.003380 | Commit Loss: 0.001434 | Perplexity: 654.707076
2025-09-14 17:40:30,513 Stage: Train 0.5 | Epoch: 38 | Iter: 117200 | Total Loss: 0.003934 | Recon Loss: 0.003230 | Commit Loss: 0.001409 | Perplexity: 650.713545
2025-09-14 17:40:38,257 Stage: Train 0.5 | Epoch: 38 | Iter: 117400 | Total Loss: 0.004063 | Recon Loss: 0.003353 | Commit Loss: 0.001420 | Perplexity: 654.131458
2025-09-14 17:40:45,969 Stage: Train 0.5 | Epoch: 38 | Iter: 117600 | Total Loss: 0.003934 | Recon Loss: 0.003226 | Commit Loss: 0.001416 | Perplexity: 652.009048
2025-09-14 17:40:53,685 Stage: Train 0.5 | Epoch: 38 | Iter: 117800 | Total Loss: 0.003906 | Recon Loss: 0.003189 | Commit Loss: 0.001435 | Perplexity: 654.957466
2025-09-14 17:41:01,461 Stage: Train 0.5 | Epoch: 38 | Iter: 118000 | Total Loss: 0.003952 | Recon Loss: 0.003245 | Commit Loss: 0.001415 | Perplexity: 651.862987
2025-09-14 17:41:09,158 Stage: Train 0.5 | Epoch: 38 | Iter: 118200 | Total Loss: 0.003972 | Recon Loss: 0.003265 | Commit Loss: 0.001413 | Perplexity: 655.198879
2025-09-14 17:41:16,853 Stage: Train 0.5 | Epoch: 38 | Iter: 118400 | Total Loss: 0.003930 | Recon Loss: 0.003223 | Commit Loss: 0.001415 | Perplexity: 653.544399
Trainning Epoch:  24%|██▎       | 39/165 [1:18:30<4:10:22, 119.23s/it]2025-09-14 17:41:24,585 Stage: Train 0.5 | Epoch: 39 | Iter: 118600 | Total Loss: 0.003943 | Recon Loss: 0.003235 | Commit Loss: 0.001415 | Perplexity: 651.492874
2025-09-14 17:41:32,345 Stage: Train 0.5 | Epoch: 39 | Iter: 118800 | Total Loss: 0.003920 | Recon Loss: 0.003213 | Commit Loss: 0.001413 | Perplexity: 653.735232
2025-09-14 17:41:40,029 Stage: Train 0.5 | Epoch: 39 | Iter: 119000 | Total Loss: 0.003932 | Recon Loss: 0.003228 | Commit Loss: 0.001410 | Perplexity: 651.993007
2025-09-14 17:41:47,746 Stage: Train 0.5 | Epoch: 39 | Iter: 119200 | Total Loss: 0.003953 | Recon Loss: 0.003254 | Commit Loss: 0.001397 | Perplexity: 650.834832
2025-09-14 17:41:55,441 Stage: Train 0.5 | Epoch: 39 | Iter: 119400 | Total Loss: 0.003933 | Recon Loss: 0.003232 | Commit Loss: 0.001400 | Perplexity: 653.729382
2025-09-14 17:42:03,170 Stage: Train 0.5 | Epoch: 39 | Iter: 119600 | Total Loss: 0.003952 | Recon Loss: 0.003250 | Commit Loss: 0.001404 | Perplexity: 651.880746
2025-09-14 17:42:10,885 Stage: Train 0.5 | Epoch: 39 | Iter: 119800 | Total Loss: 0.003884 | Recon Loss: 0.003181 | Commit Loss: 0.001406 | Perplexity: 653.016550
2025-09-14 17:42:18,565 Stage: Train 0.5 | Epoch: 39 | Iter: 120000 | Total Loss: 0.003974 | Recon Loss: 0.003265 | Commit Loss: 0.001420 | Perplexity: 653.325639
2025-09-14 17:42:18,566 Saving model at iteration 120000
2025-09-14 17:42:18,713 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_40_step_120000
2025-09-14 17:42:18,850 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_40_step_120000/pytorch_model.bin
2025-09-14 17:42:19,102 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_40_step_120000/optimizer.bin
2025-09-14 17:42:19,102 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_40_step_120000/scheduler.bin
2025-09-14 17:42:19,103 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_40_step_120000/random_states_0.pkl
2025-09-14 17:42:27,198 Stage: Train 0.5 | Epoch: 39 | Iter: 120200 | Total Loss: 0.003994 | Recon Loss: 0.003285 | Commit Loss: 0.001418 | Perplexity: 654.890594
2025-09-14 17:42:34,897 Stage: Train 0.5 | Epoch: 39 | Iter: 120400 | Total Loss: 0.003903 | Recon Loss: 0.003199 | Commit Loss: 0.001408 | Perplexity: 652.901071
2025-09-14 17:42:42,642 Stage: Train 0.5 | Epoch: 39 | Iter: 120600 | Total Loss: 0.003946 | Recon Loss: 0.003243 | Commit Loss: 0.001406 | Perplexity: 652.600540
2025-09-14 17:42:50,314 Stage: Train 0.5 | Epoch: 39 | Iter: 120800 | Total Loss: 0.004015 | Recon Loss: 0.003312 | Commit Loss: 0.001408 | Perplexity: 653.154652
2025-09-14 17:42:58,021 Stage: Train 0.5 | Epoch: 39 | Iter: 121000 | Total Loss: 0.003915 | Recon Loss: 0.003205 | Commit Loss: 0.001420 | Perplexity: 657.711748
2025-09-14 17:43:05,765 Stage: Train 0.5 | Epoch: 39 | Iter: 121200 | Total Loss: 0.003967 | Recon Loss: 0.003258 | Commit Loss: 0.001419 | Perplexity: 655.677856
2025-09-14 17:43:13,461 Stage: Train 0.5 | Epoch: 39 | Iter: 121400 | Total Loss: 0.003851 | Recon Loss: 0.003146 | Commit Loss: 0.001409 | Perplexity: 657.266992
Trainning Epoch:  24%|██▍       | 40/165 [1:20:28<4:07:41, 118.89s/it]2025-09-14 17:43:21,193 Stage: Train 0.5 | Epoch: 40 | Iter: 121600 | Total Loss: 0.003911 | Recon Loss: 0.003207 | Commit Loss: 0.001408 | Perplexity: 654.201870
2025-09-14 17:43:28,889 Stage: Train 0.5 | Epoch: 40 | Iter: 121800 | Total Loss: 0.003937 | Recon Loss: 0.003236 | Commit Loss: 0.001402 | Perplexity: 655.578043
2025-09-14 17:43:36,594 Stage: Train 0.5 | Epoch: 40 | Iter: 122000 | Total Loss: 0.003893 | Recon Loss: 0.003201 | Commit Loss: 0.001385 | Perplexity: 652.980316
2025-09-14 17:43:44,309 Stage: Train 0.5 | Epoch: 40 | Iter: 122200 | Total Loss: 0.004083 | Recon Loss: 0.003380 | Commit Loss: 0.001406 | Perplexity: 654.368423
2025-09-14 17:43:51,954 Stage: Train 0.5 | Epoch: 40 | Iter: 122400 | Total Loss: 0.003799 | Recon Loss: 0.003107 | Commit Loss: 0.001384 | Perplexity: 650.789072
2025-09-14 17:43:59,657 Stage: Train 0.5 | Epoch: 40 | Iter: 122600 | Total Loss: 0.003935 | Recon Loss: 0.003236 | Commit Loss: 0.001398 | Perplexity: 655.182073
2025-09-14 17:44:07,494 Stage: Train 0.5 | Epoch: 40 | Iter: 122800 | Total Loss: 0.003856 | Recon Loss: 0.003155 | Commit Loss: 0.001402 | Perplexity: 653.902991
2025-09-14 17:44:15,474 Stage: Train 0.5 | Epoch: 40 | Iter: 123000 | Total Loss: 0.003906 | Recon Loss: 0.003203 | Commit Loss: 0.001405 | Perplexity: 653.511538
2025-09-14 17:44:23,421 Stage: Train 0.5 | Epoch: 40 | Iter: 123200 | Total Loss: 0.003975 | Recon Loss: 0.003272 | Commit Loss: 0.001406 | Perplexity: 654.596001
2025-09-14 17:44:31,226 Stage: Train 0.5 | Epoch: 40 | Iter: 123400 | Total Loss: 0.003913 | Recon Loss: 0.003221 | Commit Loss: 0.001383 | Perplexity: 651.437110
2025-09-14 17:44:38,939 Stage: Train 0.5 | Epoch: 40 | Iter: 123600 | Total Loss: 0.003954 | Recon Loss: 0.003254 | Commit Loss: 0.001400 | Perplexity: 656.187627
2025-09-14 17:44:46,677 Stage: Train 0.5 | Epoch: 40 | Iter: 123800 | Total Loss: 0.003869 | Recon Loss: 0.003172 | Commit Loss: 0.001393 | Perplexity: 654.378305
2025-09-14 17:44:54,369 Stage: Train 0.5 | Epoch: 40 | Iter: 124000 | Total Loss: 0.003916 | Recon Loss: 0.003216 | Commit Loss: 0.001401 | Perplexity: 652.971945
2025-09-14 17:45:02,090 Stage: Train 0.5 | Epoch: 40 | Iter: 124200 | Total Loss: 0.003912 | Recon Loss: 0.003210 | Commit Loss: 0.001403 | Perplexity: 656.540688
2025-09-14 17:45:09,870 Stage: Train 0.5 | Epoch: 40 | Iter: 124400 | Total Loss: 0.003854 | Recon Loss: 0.003167 | Commit Loss: 0.001373 | Perplexity: 653.477043
Trainning Epoch:  25%|██▍       | 41/165 [1:22:26<4:05:07, 118.61s/it]2025-09-14 17:45:17,713 Stage: Train 0.5 | Epoch: 41 | Iter: 124600 | Total Loss: 0.003897 | Recon Loss: 0.003196 | Commit Loss: 0.001402 | Perplexity: 653.063147
2025-09-14 17:45:25,453 Stage: Train 0.5 | Epoch: 41 | Iter: 124800 | Total Loss: 0.003916 | Recon Loss: 0.003220 | Commit Loss: 0.001392 | Perplexity: 653.918524
2025-09-14 17:45:33,189 Stage: Train 0.5 | Epoch: 41 | Iter: 125000 | Total Loss: 0.003892 | Recon Loss: 0.003202 | Commit Loss: 0.001379 | Perplexity: 652.293036
2025-09-14 17:45:40,889 Stage: Train 0.5 | Epoch: 41 | Iter: 125200 | Total Loss: 0.003837 | Recon Loss: 0.003145 | Commit Loss: 0.001384 | Perplexity: 654.620383
2025-09-14 17:45:48,617 Stage: Train 0.5 | Epoch: 41 | Iter: 125400 | Total Loss: 0.003898 | Recon Loss: 0.003200 | Commit Loss: 0.001396 | Perplexity: 656.007361
2025-09-14 17:45:56,308 Stage: Train 0.5 | Epoch: 41 | Iter: 125600 | Total Loss: 0.003880 | Recon Loss: 0.003182 | Commit Loss: 0.001397 | Perplexity: 653.026327
2025-09-14 17:46:04,062 Stage: Train 0.5 | Epoch: 41 | Iter: 125800 | Total Loss: 0.003952 | Recon Loss: 0.003252 | Commit Loss: 0.001401 | Perplexity: 655.771700
2025-09-14 17:46:11,819 Stage: Train 0.5 | Epoch: 41 | Iter: 126000 | Total Loss: 0.003913 | Recon Loss: 0.003218 | Commit Loss: 0.001390 | Perplexity: 652.885735
2025-09-14 17:46:19,538 Stage: Train 0.5 | Epoch: 41 | Iter: 126200 | Total Loss: 0.003947 | Recon Loss: 0.003259 | Commit Loss: 0.001375 | Perplexity: 653.639506
2025-09-14 17:46:27,273 Stage: Train 0.5 | Epoch: 41 | Iter: 126400 | Total Loss: 0.003777 | Recon Loss: 0.003085 | Commit Loss: 0.001385 | Perplexity: 655.739857
2025-09-14 17:46:34,997 Stage: Train 0.5 | Epoch: 41 | Iter: 126600 | Total Loss: 0.003970 | Recon Loss: 0.003277 | Commit Loss: 0.001385 | Perplexity: 653.515981
2025-09-14 17:46:42,762 Stage: Train 0.5 | Epoch: 41 | Iter: 126800 | Total Loss: 0.003897 | Recon Loss: 0.003196 | Commit Loss: 0.001402 | Perplexity: 657.785326
2025-09-14 17:46:50,506 Stage: Train 0.5 | Epoch: 41 | Iter: 127000 | Total Loss: 0.003874 | Recon Loss: 0.003186 | Commit Loss: 0.001377 | Perplexity: 653.493818
2025-09-14 17:46:58,231 Stage: Train 0.5 | Epoch: 41 | Iter: 127200 | Total Loss: 0.003903 | Recon Loss: 0.003204 | Commit Loss: 0.001398 | Perplexity: 657.151004
2025-09-14 17:47:05,949 Stage: Train 0.5 | Epoch: 41 | Iter: 127400 | Total Loss: 0.003871 | Recon Loss: 0.003170 | Commit Loss: 0.001402 | Perplexity: 655.180201
Trainning Epoch:  25%|██▌       | 42/165 [1:24:24<4:02:26, 118.26s/it]2025-09-14 17:47:13,680 Stage: Train 0.5 | Epoch: 42 | Iter: 127600 | Total Loss: 0.003920 | Recon Loss: 0.003219 | Commit Loss: 0.001404 | Perplexity: 653.721261
2025-09-14 17:47:21,384 Stage: Train 0.5 | Epoch: 42 | Iter: 127800 | Total Loss: 0.003887 | Recon Loss: 0.003202 | Commit Loss: 0.001370 | Perplexity: 654.206823
2025-09-14 17:47:29,082 Stage: Train 0.5 | Epoch: 42 | Iter: 128000 | Total Loss: 0.003870 | Recon Loss: 0.003178 | Commit Loss: 0.001384 | Perplexity: 659.153858
2025-09-14 17:47:36,761 Stage: Train 0.5 | Epoch: 42 | Iter: 128200 | Total Loss: 0.003814 | Recon Loss: 0.003113 | Commit Loss: 0.001403 | Perplexity: 657.573245
2025-09-14 17:47:44,461 Stage: Train 0.5 | Epoch: 42 | Iter: 128400 | Total Loss: 0.003956 | Recon Loss: 0.003268 | Commit Loss: 0.001376 | Perplexity: 651.480840
2025-09-14 17:47:52,380 Stage: Train 0.5 | Epoch: 42 | Iter: 128600 | Total Loss: 0.003882 | Recon Loss: 0.003192 | Commit Loss: 0.001380 | Perplexity: 654.740100
2025-09-14 17:48:00,512 Stage: Train 0.5 | Epoch: 42 | Iter: 128800 | Total Loss: 0.003848 | Recon Loss: 0.003160 | Commit Loss: 0.001376 | Perplexity: 654.251686
2025-09-14 17:48:08,610 Stage: Train 0.5 | Epoch: 42 | Iter: 129000 | Total Loss: 0.003898 | Recon Loss: 0.003207 | Commit Loss: 0.001383 | Perplexity: 654.868091
2025-09-14 17:48:16,695 Stage: Train 0.5 | Epoch: 42 | Iter: 129200 | Total Loss: 0.003919 | Recon Loss: 0.003231 | Commit Loss: 0.001376 | Perplexity: 654.698903
2025-09-14 17:48:24,601 Stage: Train 0.5 | Epoch: 42 | Iter: 129400 | Total Loss: 0.003800 | Recon Loss: 0.003105 | Commit Loss: 0.001389 | Perplexity: 655.548777
2025-09-14 17:48:32,273 Stage: Train 0.5 | Epoch: 42 | Iter: 129600 | Total Loss: 0.003882 | Recon Loss: 0.003184 | Commit Loss: 0.001396 | Perplexity: 656.960457
2025-09-14 17:48:39,969 Stage: Train 0.5 | Epoch: 42 | Iter: 129800 | Total Loss: 0.003825 | Recon Loss: 0.003119 | Commit Loss: 0.001411 | Perplexity: 654.866953
2025-09-14 17:48:47,793 Stage: Train 0.5 | Epoch: 42 | Iter: 130000 | Total Loss: 0.003869 | Recon Loss: 0.003173 | Commit Loss: 0.001392 | Perplexity: 654.456184
2025-09-14 17:48:55,497 Stage: Train 0.5 | Epoch: 42 | Iter: 130200 | Total Loss: 0.003917 | Recon Loss: 0.003218 | Commit Loss: 0.001398 | Perplexity: 659.176649
2025-09-14 17:49:03,167 Stage: Train 0.5 | Epoch: 42 | Iter: 130400 | Total Loss: 0.003821 | Recon Loss: 0.003134 | Commit Loss: 0.001375 | Perplexity: 652.732678
2025-09-14 17:49:11,134 Stage: Train 0.5 | Epoch: 42 | Iter: 130600 | Total Loss: 0.003859 | Recon Loss: 0.003172 | Commit Loss: 0.001374 | Perplexity: 650.791918
Trainning Epoch:  26%|██▌       | 43/165 [1:26:23<4:00:55, 118.49s/it]2025-09-14 17:49:19,267 Stage: Train 0.5 | Epoch: 43 | Iter: 130800 | Total Loss: 0.003858 | Recon Loss: 0.003166 | Commit Loss: 0.001384 | Perplexity: 654.731034
2025-09-14 17:49:27,377 Stage: Train 0.5 | Epoch: 43 | Iter: 131000 | Total Loss: 0.003836 | Recon Loss: 0.003152 | Commit Loss: 0.001368 | Perplexity: 653.440524
2025-09-14 17:49:35,491 Stage: Train 0.5 | Epoch: 43 | Iter: 131200 | Total Loss: 0.003838 | Recon Loss: 0.003149 | Commit Loss: 0.001380 | Perplexity: 654.691404
2025-09-14 17:49:43,238 Stage: Train 0.5 | Epoch: 43 | Iter: 131400 | Total Loss: 0.003893 | Recon Loss: 0.003191 | Commit Loss: 0.001404 | Perplexity: 659.236907
2025-09-14 17:49:50,953 Stage: Train 0.5 | Epoch: 43 | Iter: 131600 | Total Loss: 0.003769 | Recon Loss: 0.003087 | Commit Loss: 0.001364 | Perplexity: 652.668804
2025-09-14 17:49:58,669 Stage: Train 0.5 | Epoch: 43 | Iter: 131800 | Total Loss: 0.003920 | Recon Loss: 0.003224 | Commit Loss: 0.001392 | Perplexity: 656.105737
2025-09-14 17:50:06,828 Stage: Train 0.5 | Epoch: 43 | Iter: 132000 | Total Loss: 0.003846 | Recon Loss: 0.003160 | Commit Loss: 0.001372 | Perplexity: 655.518399
2025-09-14 17:50:14,973 Stage: Train 0.5 | Epoch: 43 | Iter: 132200 | Total Loss: 0.003868 | Recon Loss: 0.003178 | Commit Loss: 0.001381 | Perplexity: 658.947201
2025-09-14 17:50:23,083 Stage: Train 0.5 | Epoch: 43 | Iter: 132400 | Total Loss: 0.003826 | Recon Loss: 0.003132 | Commit Loss: 0.001387 | Perplexity: 654.757190
2025-09-14 17:50:30,837 Stage: Train 0.5 | Epoch: 43 | Iter: 132600 | Total Loss: 0.003853 | Recon Loss: 0.003165 | Commit Loss: 0.001376 | Perplexity: 654.868412
2025-09-14 17:50:38,619 Stage: Train 0.5 | Epoch: 43 | Iter: 132800 | Total Loss: 0.003804 | Recon Loss: 0.003115 | Commit Loss: 0.001377 | Perplexity: 653.705933
2025-09-14 17:50:46,470 Stage: Train 0.5 | Epoch: 43 | Iter: 133000 | Total Loss: 0.003857 | Recon Loss: 0.003164 | Commit Loss: 0.001386 | Perplexity: 658.314540
2025-09-14 17:50:54,370 Stage: Train 0.5 | Epoch: 43 | Iter: 133200 | Total Loss: 0.003858 | Recon Loss: 0.003175 | Commit Loss: 0.001367 | Perplexity: 656.311657
2025-09-14 17:51:02,249 Stage: Train 0.5 | Epoch: 43 | Iter: 133400 | Total Loss: 0.003831 | Recon Loss: 0.003142 | Commit Loss: 0.001378 | Perplexity: 654.703906
2025-09-14 17:51:10,082 Stage: Train 0.5 | Epoch: 43 | Iter: 133600 | Total Loss: 0.003895 | Recon Loss: 0.003209 | Commit Loss: 0.001372 | Perplexity: 655.173907
Trainning Epoch:  27%|██▋       | 44/165 [1:28:23<4:00:06, 119.06s/it]2025-09-14 17:51:17,914 Stage: Train 0.5 | Epoch: 44 | Iter: 133800 | Total Loss: 0.003843 | Recon Loss: 0.003148 | Commit Loss: 0.001391 | Perplexity: 656.972630
2025-09-14 17:51:25,741 Stage: Train 0.5 | Epoch: 44 | Iter: 134000 | Total Loss: 0.003842 | Recon Loss: 0.003153 | Commit Loss: 0.001378 | Perplexity: 654.745654
2025-09-14 17:51:33,630 Stage: Train 0.5 | Epoch: 44 | Iter: 134200 | Total Loss: 0.003879 | Recon Loss: 0.003191 | Commit Loss: 0.001375 | Perplexity: 657.359198
2025-09-14 17:51:41,472 Stage: Train 0.5 | Epoch: 44 | Iter: 134400 | Total Loss: 0.003881 | Recon Loss: 0.003186 | Commit Loss: 0.001390 | Perplexity: 657.053010
2025-09-14 17:51:49,336 Stage: Train 0.5 | Epoch: 44 | Iter: 134600 | Total Loss: 0.003826 | Recon Loss: 0.003139 | Commit Loss: 0.001374 | Perplexity: 654.432373
2025-09-14 17:51:57,234 Stage: Train 0.5 | Epoch: 44 | Iter: 134800 | Total Loss: 0.003785 | Recon Loss: 0.003105 | Commit Loss: 0.001360 | Perplexity: 654.111805
2025-09-14 17:52:05,294 Stage: Train 0.5 | Epoch: 44 | Iter: 135000 | Total Loss: 0.003852 | Recon Loss: 0.003163 | Commit Loss: 0.001378 | Perplexity: 657.028500
2025-09-14 17:52:13,162 Stage: Train 0.5 | Epoch: 44 | Iter: 135200 | Total Loss: 0.003796 | Recon Loss: 0.003116 | Commit Loss: 0.001359 | Perplexity: 655.776379
2025-09-14 17:52:20,961 Stage: Train 0.5 | Epoch: 44 | Iter: 135400 | Total Loss: 0.003857 | Recon Loss: 0.003174 | Commit Loss: 0.001365 | Perplexity: 657.880701
2025-09-14 17:52:28,777 Stage: Train 0.5 | Epoch: 44 | Iter: 135600 | Total Loss: 0.003815 | Recon Loss: 0.003126 | Commit Loss: 0.001378 | Perplexity: 661.562156
2025-09-14 17:52:36,550 Stage: Train 0.5 | Epoch: 44 | Iter: 135800 | Total Loss: 0.003794 | Recon Loss: 0.003108 | Commit Loss: 0.001373 | Perplexity: 655.831808
2025-09-14 17:52:44,432 Stage: Train 0.5 | Epoch: 44 | Iter: 136000 | Total Loss: 0.003839 | Recon Loss: 0.003154 | Commit Loss: 0.001370 | Perplexity: 654.694099
2025-09-14 17:52:52,169 Stage: Train 0.5 | Epoch: 44 | Iter: 136200 | Total Loss: 0.003797 | Recon Loss: 0.003112 | Commit Loss: 0.001371 | Perplexity: 655.444820
2025-09-14 17:52:59,961 Stage: Train 0.5 | Epoch: 44 | Iter: 136400 | Total Loss: 0.003826 | Recon Loss: 0.003144 | Commit Loss: 0.001363 | Perplexity: 657.100041
2025-09-14 17:53:07,775 Stage: Train 0.5 | Epoch: 44 | Iter: 136600 | Total Loss: 0.003843 | Recon Loss: 0.003163 | Commit Loss: 0.001361 | Perplexity: 654.333935
Trainning Epoch:  27%|██▋       | 45/165 [1:30:22<3:58:12, 119.11s/it]2025-09-14 17:53:15,807 Stage: Train 0.5 | Epoch: 45 | Iter: 136800 | Total Loss: 0.003818 | Recon Loss: 0.003130 | Commit Loss: 0.001375 | Perplexity: 654.449928
2025-09-14 17:53:23,886 Stage: Train 0.5 | Epoch: 45 | Iter: 137000 | Total Loss: 0.003747 | Recon Loss: 0.003068 | Commit Loss: 0.001358 | Perplexity: 654.487150
2025-09-14 17:53:31,971 Stage: Train 0.5 | Epoch: 45 | Iter: 137200 | Total Loss: 0.003834 | Recon Loss: 0.003147 | Commit Loss: 0.001375 | Perplexity: 658.245852
2025-09-14 17:53:40,050 Stage: Train 0.5 | Epoch: 45 | Iter: 137400 | Total Loss: 0.003781 | Recon Loss: 0.003100 | Commit Loss: 0.001361 | Perplexity: 654.731195
2025-09-14 17:53:48,163 Stage: Train 0.5 | Epoch: 45 | Iter: 137600 | Total Loss: 0.003775 | Recon Loss: 0.003094 | Commit Loss: 0.001362 | Perplexity: 654.448911
2025-09-14 17:53:56,066 Stage: Train 0.5 | Epoch: 45 | Iter: 137800 | Total Loss: 0.003783 | Recon Loss: 0.003104 | Commit Loss: 0.001358 | Perplexity: 657.988623
2025-09-14 17:54:03,737 Stage: Train 0.5 | Epoch: 45 | Iter: 138000 | Total Loss: 0.003836 | Recon Loss: 0.003155 | Commit Loss: 0.001361 | Perplexity: 658.855657
2025-09-14 17:54:11,429 Stage: Train 0.5 | Epoch: 45 | Iter: 138200 | Total Loss: 0.003761 | Recon Loss: 0.003076 | Commit Loss: 0.001369 | Perplexity: 654.398704
2025-09-14 17:54:19,170 Stage: Train 0.5 | Epoch: 45 | Iter: 138400 | Total Loss: 0.003748 | Recon Loss: 0.003069 | Commit Loss: 0.001359 | Perplexity: 655.681488
2025-09-14 17:54:26,883 Stage: Train 0.5 | Epoch: 45 | Iter: 138600 | Total Loss: 0.003759 | Recon Loss: 0.003083 | Commit Loss: 0.001351 | Perplexity: 656.775031
2025-09-14 17:54:34,590 Stage: Train 0.5 | Epoch: 45 | Iter: 138800 | Total Loss: 0.003851 | Recon Loss: 0.003164 | Commit Loss: 0.001375 | Perplexity: 660.182393
2025-09-14 17:54:42,282 Stage: Train 0.5 | Epoch: 45 | Iter: 139000 | Total Loss: 0.003763 | Recon Loss: 0.003082 | Commit Loss: 0.001362 | Perplexity: 655.381648
2025-09-14 17:54:49,973 Stage: Train 0.5 | Epoch: 45 | Iter: 139200 | Total Loss: 0.003840 | Recon Loss: 0.003155 | Commit Loss: 0.001370 | Perplexity: 659.096695
2025-09-14 17:54:57,741 Stage: Train 0.5 | Epoch: 45 | Iter: 139400 | Total Loss: 0.003796 | Recon Loss: 0.003108 | Commit Loss: 0.001376 | Perplexity: 660.115248
2025-09-14 17:55:05,546 Stage: Train 0.5 | Epoch: 45 | Iter: 139600 | Total Loss: 0.003777 | Recon Loss: 0.003099 | Commit Loss: 0.001355 | Perplexity: 658.002288
Trainning Epoch:  28%|██▊       | 46/165 [1:32:21<3:56:15, 119.12s/it]2025-09-14 17:55:13,406 Stage: Train 0.5 | Epoch: 46 | Iter: 139800 | Total Loss: 0.003833 | Recon Loss: 0.003151 | Commit Loss: 0.001363 | Perplexity: 658.146238
2025-09-14 17:55:21,235 Stage: Train 0.5 | Epoch: 46 | Iter: 140000 | Total Loss: 0.003786 | Recon Loss: 0.003105 | Commit Loss: 0.001362 | Perplexity: 660.764160
2025-09-14 17:55:21,235 Saving model at iteration 140000
2025-09-14 17:55:21,378 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_47_step_140000
2025-09-14 17:55:21,511 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_47_step_140000/pytorch_model.bin
2025-09-14 17:55:21,760 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_47_step_140000/optimizer.bin
2025-09-14 17:55:21,760 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_47_step_140000/scheduler.bin
2025-09-14 17:55:21,761 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_47_step_140000/random_states_0.pkl
2025-09-14 17:55:29,778 Stage: Train 0.5 | Epoch: 46 | Iter: 140200 | Total Loss: 0.003773 | Recon Loss: 0.003093 | Commit Loss: 0.001359 | Perplexity: 660.066255
2025-09-14 17:55:37,438 Stage: Train 0.5 | Epoch: 46 | Iter: 140400 | Total Loss: 0.003802 | Recon Loss: 0.003130 | Commit Loss: 0.001344 | Perplexity: 656.306044
2025-09-14 17:55:45,133 Stage: Train 0.5 | Epoch: 46 | Iter: 140600 | Total Loss: 0.003742 | Recon Loss: 0.003064 | Commit Loss: 0.001356 | Perplexity: 661.323411
2025-09-14 17:55:52,821 Stage: Train 0.5 | Epoch: 46 | Iter: 140800 | Total Loss: 0.003725 | Recon Loss: 0.003047 | Commit Loss: 0.001356 | Perplexity: 658.482501
2025-09-14 17:56:00,485 Stage: Train 0.5 | Epoch: 46 | Iter: 141000 | Total Loss: 0.003750 | Recon Loss: 0.003066 | Commit Loss: 0.001368 | Perplexity: 660.013551
2025-09-14 17:56:08,150 Stage: Train 0.5 | Epoch: 46 | Iter: 141200 | Total Loss: 0.003809 | Recon Loss: 0.003131 | Commit Loss: 0.001357 | Perplexity: 659.017539
2025-09-14 17:56:15,828 Stage: Train 0.5 | Epoch: 46 | Iter: 141400 | Total Loss: 0.003810 | Recon Loss: 0.003129 | Commit Loss: 0.001360 | Perplexity: 659.457973
2025-09-14 17:56:23,501 Stage: Train 0.5 | Epoch: 46 | Iter: 141600 | Total Loss: 0.003765 | Recon Loss: 0.003088 | Commit Loss: 0.001354 | Perplexity: 657.999900
2025-09-14 17:56:31,246 Stage: Train 0.5 | Epoch: 46 | Iter: 141800 | Total Loss: 0.003791 | Recon Loss: 0.003107 | Commit Loss: 0.001368 | Perplexity: 660.489472
2025-09-14 17:56:39,022 Stage: Train 0.5 | Epoch: 46 | Iter: 142000 | Total Loss: 0.003760 | Recon Loss: 0.003086 | Commit Loss: 0.001348 | Perplexity: 657.976589
2025-09-14 17:56:47,109 Stage: Train 0.5 | Epoch: 46 | Iter: 142200 | Total Loss: 0.003758 | Recon Loss: 0.003086 | Commit Loss: 0.001343 | Perplexity: 657.843318
2025-09-14 17:56:55,232 Stage: Train 0.5 | Epoch: 46 | Iter: 142400 | Total Loss: 0.003753 | Recon Loss: 0.003079 | Commit Loss: 0.001348 | Perplexity: 658.273827
2025-09-14 17:57:03,341 Stage: Train 0.5 | Epoch: 46 | Iter: 142600 | Total Loss: 0.003797 | Recon Loss: 0.003124 | Commit Loss: 0.001345 | Perplexity: 656.814251
Trainning Epoch:  28%|██▊       | 47/165 [1:34:21<3:54:42, 119.34s/it]2025-09-14 17:57:11,706 Stage: Train 0.5 | Epoch: 47 | Iter: 142800 | Total Loss: 0.003787 | Recon Loss: 0.003107 | Commit Loss: 0.001360 | Perplexity: 659.131277
2025-09-14 17:57:19,422 Stage: Train 0.5 | Epoch: 47 | Iter: 143000 | Total Loss: 0.003744 | Recon Loss: 0.003069 | Commit Loss: 0.001350 | Perplexity: 659.538086
2025-09-14 17:57:27,123 Stage: Train 0.5 | Epoch: 47 | Iter: 143200 | Total Loss: 0.003766 | Recon Loss: 0.003095 | Commit Loss: 0.001340 | Perplexity: 656.196470
2025-09-14 17:57:34,829 Stage: Train 0.5 | Epoch: 47 | Iter: 143400 | Total Loss: 0.003775 | Recon Loss: 0.003099 | Commit Loss: 0.001351 | Perplexity: 658.417301
2025-09-14 17:57:42,565 Stage: Train 0.5 | Epoch: 47 | Iter: 143600 | Total Loss: 0.003779 | Recon Loss: 0.003094 | Commit Loss: 0.001370 | Perplexity: 663.517429
2025-09-14 17:57:50,258 Stage: Train 0.5 | Epoch: 47 | Iter: 143800 | Total Loss: 0.003683 | Recon Loss: 0.003012 | Commit Loss: 0.001341 | Perplexity: 656.086664
2025-09-14 17:57:57,966 Stage: Train 0.5 | Epoch: 47 | Iter: 144000 | Total Loss: 0.003773 | Recon Loss: 0.003099 | Commit Loss: 0.001349 | Perplexity: 659.661685
2025-09-14 17:58:05,709 Stage: Train 0.5 | Epoch: 47 | Iter: 144200 | Total Loss: 0.003703 | Recon Loss: 0.003031 | Commit Loss: 0.001344 | Perplexity: 660.320399
2025-09-14 17:58:13,445 Stage: Train 0.5 | Epoch: 47 | Iter: 144400 | Total Loss: 0.003707 | Recon Loss: 0.003032 | Commit Loss: 0.001349 | Perplexity: 660.649254
2025-09-14 17:58:21,237 Stage: Train 0.5 | Epoch: 47 | Iter: 144600 | Total Loss: 0.003756 | Recon Loss: 0.003077 | Commit Loss: 0.001357 | Perplexity: 660.096650
2025-09-14 17:58:28,970 Stage: Train 0.5 | Epoch: 47 | Iter: 144800 | Total Loss: 0.003735 | Recon Loss: 0.003058 | Commit Loss: 0.001355 | Perplexity: 661.090662
2025-09-14 17:58:36,717 Stage: Train 0.5 | Epoch: 47 | Iter: 145000 | Total Loss: 0.003778 | Recon Loss: 0.003099 | Commit Loss: 0.001358 | Perplexity: 661.608856
2025-09-14 17:58:44,422 Stage: Train 0.5 | Epoch: 47 | Iter: 145200 | Total Loss: 0.003754 | Recon Loss: 0.003070 | Commit Loss: 0.001369 | Perplexity: 662.861104
2025-09-14 17:58:52,129 Stage: Train 0.5 | Epoch: 47 | Iter: 145400 | Total Loss: 0.003735 | Recon Loss: 0.003067 | Commit Loss: 0.001336 | Perplexity: 658.487318
2025-09-14 17:58:59,849 Stage: Train 0.5 | Epoch: 47 | Iter: 145600 | Total Loss: 0.003798 | Recon Loss: 0.003124 | Commit Loss: 0.001348 | Perplexity: 661.723480
2025-09-14 17:59:07,565 Stage: Train 0.5 | Epoch: 47 | Iter: 145800 | Total Loss: 0.003719 | Recon Loss: 0.003043 | Commit Loss: 0.001352 | Perplexity: 662.218405
Trainning Epoch:  29%|██▉       | 48/165 [1:36:19<3:51:33, 118.74s/it]2025-09-14 17:59:15,330 Stage: Train 0.5 | Epoch: 48 | Iter: 146000 | Total Loss: 0.003817 | Recon Loss: 0.003142 | Commit Loss: 0.001350 | Perplexity: 661.663113
2025-09-14 17:59:23,029 Stage: Train 0.5 | Epoch: 48 | Iter: 146200 | Total Loss: 0.003706 | Recon Loss: 0.003031 | Commit Loss: 0.001351 | Perplexity: 662.137108
2025-09-14 17:59:30,697 Stage: Train 0.5 | Epoch: 48 | Iter: 146400 | Total Loss: 0.003726 | Recon Loss: 0.003052 | Commit Loss: 0.001348 | Perplexity: 663.492792
2025-09-14 17:59:38,438 Stage: Train 0.5 | Epoch: 48 | Iter: 146600 | Total Loss: 0.003746 | Recon Loss: 0.003074 | Commit Loss: 0.001343 | Perplexity: 660.763878
2025-09-14 17:59:46,177 Stage: Train 0.5 | Epoch: 48 | Iter: 146800 | Total Loss: 0.003709 | Recon Loss: 0.003039 | Commit Loss: 0.001340 | Perplexity: 661.185199
2025-09-14 17:59:53,894 Stage: Train 0.5 | Epoch: 48 | Iter: 147000 | Total Loss: 0.003759 | Recon Loss: 0.003086 | Commit Loss: 0.001345 | Perplexity: 660.236309
2025-09-14 18:00:01,626 Stage: Train 0.5 | Epoch: 48 | Iter: 147200 | Total Loss: 0.003728 | Recon Loss: 0.003058 | Commit Loss: 0.001339 | Perplexity: 662.127999
2025-09-14 18:00:09,305 Stage: Train 0.5 | Epoch: 48 | Iter: 147400 | Total Loss: 0.003747 | Recon Loss: 0.003076 | Commit Loss: 0.001343 | Perplexity: 657.069370
2025-09-14 18:00:17,049 Stage: Train 0.5 | Epoch: 48 | Iter: 147600 | Total Loss: 0.003719 | Recon Loss: 0.003046 | Commit Loss: 0.001346 | Perplexity: 659.401048
2025-09-14 18:00:24,765 Stage: Train 0.5 | Epoch: 48 | Iter: 147800 | Total Loss: 0.003797 | Recon Loss: 0.003128 | Commit Loss: 0.001337 | Perplexity: 659.170007
2025-09-14 18:00:32,477 Stage: Train 0.5 | Epoch: 48 | Iter: 148000 | Total Loss: 0.003741 | Recon Loss: 0.003065 | Commit Loss: 0.001352 | Perplexity: 659.767222
2025-09-14 18:00:40,213 Stage: Train 0.5 | Epoch: 48 | Iter: 148200 | Total Loss: 0.003818 | Recon Loss: 0.003153 | Commit Loss: 0.001330 | Perplexity: 660.731238
2025-09-14 18:00:47,962 Stage: Train 0.5 | Epoch: 48 | Iter: 148400 | Total Loss: 0.003693 | Recon Loss: 0.003028 | Commit Loss: 0.001330 | Perplexity: 659.951077
2025-09-14 18:00:55,697 Stage: Train 0.5 | Epoch: 48 | Iter: 148600 | Total Loss: 0.003818 | Recon Loss: 0.003160 | Commit Loss: 0.001317 | Perplexity: 657.016346
2025-09-14 18:01:03,401 Stage: Train 0.5 | Epoch: 48 | Iter: 148800 | Total Loss: 0.003686 | Recon Loss: 0.003023 | Commit Loss: 0.001327 | Perplexity: 658.893272
Trainning Epoch:  30%|██▉       | 49/165 [1:38:16<3:48:46, 118.33s/it]2025-09-14 18:01:11,520 Stage: Train 0.5 | Epoch: 49 | Iter: 149000 | Total Loss: 0.003717 | Recon Loss: 0.003060 | Commit Loss: 0.001315 | Perplexity: 657.463836
2025-09-14 18:01:19,682 Stage: Train 0.5 | Epoch: 49 | Iter: 149200 | Total Loss: 0.003692 | Recon Loss: 0.003021 | Commit Loss: 0.001341 | Perplexity: 660.637306
2025-09-14 18:01:27,785 Stage: Train 0.5 | Epoch: 49 | Iter: 149400 | Total Loss: 0.003673 | Recon Loss: 0.003010 | Commit Loss: 0.001326 | Perplexity: 658.771624
2025-09-14 18:01:35,617 Stage: Train 0.5 | Epoch: 49 | Iter: 149600 | Total Loss: 0.003737 | Recon Loss: 0.003074 | Commit Loss: 0.001326 | Perplexity: 659.530638
2025-09-14 18:01:43,313 Stage: Train 0.5 | Epoch: 49 | Iter: 149800 | Total Loss: 0.003671 | Recon Loss: 0.003006 | Commit Loss: 0.001329 | Perplexity: 662.500824
2025-09-14 18:01:51,016 Stage: Train 0.5 | Epoch: 49 | Iter: 150000 | Total Loss: 0.003763 | Recon Loss: 0.003095 | Commit Loss: 0.001336 | Perplexity: 660.668575
2025-09-14 18:01:58,690 Stage: Train 0.5 | Epoch: 49 | Iter: 150200 | Total Loss: 0.003649 | Recon Loss: 0.002992 | Commit Loss: 0.001314 | Perplexity: 660.933578
2025-09-14 18:02:06,965 Stage: Train 0.5 | Epoch: 49 | Iter: 150400 | Total Loss: 0.003688 | Recon Loss: 0.003025 | Commit Loss: 0.001326 | Perplexity: 662.098541
2025-09-14 18:02:15,110 Stage: Train 0.5 | Epoch: 49 | Iter: 150600 | Total Loss: 0.003714 | Recon Loss: 0.003046 | Commit Loss: 0.001336 | Perplexity: 661.324534
2025-09-14 18:02:23,009 Stage: Train 0.5 | Epoch: 49 | Iter: 150800 | Total Loss: 0.003705 | Recon Loss: 0.003032 | Commit Loss: 0.001347 | Perplexity: 661.508316
2025-09-14 18:02:30,838 Stage: Train 0.5 | Epoch: 49 | Iter: 151000 | Total Loss: 0.003704 | Recon Loss: 0.003036 | Commit Loss: 0.001335 | Perplexity: 660.722742
2025-09-14 18:02:38,837 Stage: Train 0.5 | Epoch: 49 | Iter: 151200 | Total Loss: 0.003695 | Recon Loss: 0.003035 | Commit Loss: 0.001320 | Perplexity: 658.917642
2025-09-14 18:02:46,865 Stage: Train 0.5 | Epoch: 49 | Iter: 151400 | Total Loss: 0.003705 | Recon Loss: 0.003034 | Commit Loss: 0.001341 | Perplexity: 663.083477
2025-09-14 18:02:54,726 Stage: Train 0.5 | Epoch: 49 | Iter: 151600 | Total Loss: 0.003750 | Recon Loss: 0.003087 | Commit Loss: 0.001326 | Perplexity: 658.606780
2025-09-14 18:03:02,621 Stage: Train 0.5 | Epoch: 49 | Iter: 151800 | Total Loss: 0.003675 | Recon Loss: 0.003006 | Commit Loss: 0.001339 | Perplexity: 662.332978
Trainning Epoch:  30%|███       | 50/165 [1:40:17<3:48:09, 119.04s/it]2025-09-14 18:03:10,467 Stage: Train 0.5 | Epoch: 50 | Iter: 152000 | Total Loss: 0.003676 | Recon Loss: 0.003016 | Commit Loss: 0.001321 | Perplexity: 656.889233
2025-09-14 18:03:18,227 Stage: Train 0.5 | Epoch: 50 | Iter: 152200 | Total Loss: 0.003667 | Recon Loss: 0.003004 | Commit Loss: 0.001327 | Perplexity: 663.162740
2025-09-14 18:03:25,981 Stage: Train 0.5 | Epoch: 50 | Iter: 152400 | Total Loss: 0.003690 | Recon Loss: 0.003023 | Commit Loss: 0.001334 | Perplexity: 661.356518
2025-09-14 18:03:33,762 Stage: Train 0.5 | Epoch: 50 | Iter: 152600 | Total Loss: 0.003703 | Recon Loss: 0.003040 | Commit Loss: 0.001327 | Perplexity: 661.152841
2025-09-14 18:03:41,525 Stage: Train 0.5 | Epoch: 50 | Iter: 152800 | Total Loss: 0.003726 | Recon Loss: 0.003064 | Commit Loss: 0.001323 | Perplexity: 661.721026
2025-09-14 18:03:49,287 Stage: Train 0.5 | Epoch: 50 | Iter: 153000 | Total Loss: 0.003683 | Recon Loss: 0.003015 | Commit Loss: 0.001338 | Perplexity: 663.899291
2025-09-14 18:03:57,094 Stage: Train 0.5 | Epoch: 50 | Iter: 153200 | Total Loss: 0.003718 | Recon Loss: 0.003053 | Commit Loss: 0.001329 | Perplexity: 659.908150
2025-09-14 18:04:04,949 Stage: Train 0.5 | Epoch: 50 | Iter: 153400 | Total Loss: 0.003696 | Recon Loss: 0.003045 | Commit Loss: 0.001303 | Perplexity: 659.688758
2025-09-14 18:04:12,841 Stage: Train 0.5 | Epoch: 50 | Iter: 153600 | Total Loss: 0.003658 | Recon Loss: 0.002999 | Commit Loss: 0.001319 | Perplexity: 663.126699
2025-09-14 18:04:20,706 Stage: Train 0.5 | Epoch: 50 | Iter: 153800 | Total Loss: 0.003717 | Recon Loss: 0.003046 | Commit Loss: 0.001342 | Perplexity: 665.995710
2025-09-14 18:04:28,572 Stage: Train 0.5 | Epoch: 50 | Iter: 154000 | Total Loss: 0.003646 | Recon Loss: 0.002994 | Commit Loss: 0.001304 | Perplexity: 662.711239
2025-09-14 18:04:36,433 Stage: Train 0.5 | Epoch: 50 | Iter: 154200 | Total Loss: 0.003697 | Recon Loss: 0.003038 | Commit Loss: 0.001319 | Perplexity: 661.183567
2025-09-14 18:04:44,315 Stage: Train 0.5 | Epoch: 50 | Iter: 154400 | Total Loss: 0.003652 | Recon Loss: 0.002992 | Commit Loss: 0.001321 | Perplexity: 663.148841
2025-09-14 18:04:52,432 Stage: Train 0.5 | Epoch: 50 | Iter: 154600 | Total Loss: 0.003720 | Recon Loss: 0.003058 | Commit Loss: 0.001325 | Perplexity: 663.156664
2025-09-14 18:05:00,570 Stage: Train 0.5 | Epoch: 50 | Iter: 154800 | Total Loss: 0.003630 | Recon Loss: 0.002972 | Commit Loss: 0.001316 | Perplexity: 660.990095
Trainning Epoch:  31%|███       | 51/165 [1:42:16<3:46:25, 119.17s/it]2025-09-14 18:05:08,501 Stage: Train 0.5 | Epoch: 51 | Iter: 155000 | Total Loss: 0.003657 | Recon Loss: 0.003001 | Commit Loss: 0.001311 | Perplexity: 661.627806
2025-09-14 18:05:16,386 Stage: Train 0.5 | Epoch: 51 | Iter: 155200 | Total Loss: 0.003616 | Recon Loss: 0.002956 | Commit Loss: 0.001320 | Perplexity: 661.435224
2025-09-14 18:05:24,241 Stage: Train 0.5 | Epoch: 51 | Iter: 155400 | Total Loss: 0.003630 | Recon Loss: 0.002972 | Commit Loss: 0.001315 | Perplexity: 664.547156
2025-09-14 18:05:32,135 Stage: Train 0.5 | Epoch: 51 | Iter: 155600 | Total Loss: 0.003693 | Recon Loss: 0.003035 | Commit Loss: 0.001315 | Perplexity: 661.743787
2025-09-14 18:05:40,014 Stage: Train 0.5 | Epoch: 51 | Iter: 155800 | Total Loss: 0.003636 | Recon Loss: 0.002981 | Commit Loss: 0.001310 | Perplexity: 664.063336
2025-09-14 18:05:47,885 Stage: Train 0.5 | Epoch: 51 | Iter: 156000 | Total Loss: 0.003686 | Recon Loss: 0.003025 | Commit Loss: 0.001322 | Perplexity: 664.318818
2025-09-14 18:05:55,737 Stage: Train 0.5 | Epoch: 51 | Iter: 156200 | Total Loss: 0.003654 | Recon Loss: 0.002992 | Commit Loss: 0.001323 | Perplexity: 664.831084
2025-09-14 18:06:03,613 Stage: Train 0.5 | Epoch: 51 | Iter: 156400 | Total Loss: 0.003646 | Recon Loss: 0.002989 | Commit Loss: 0.001313 | Perplexity: 661.936103
2025-09-14 18:06:11,470 Stage: Train 0.5 | Epoch: 51 | Iter: 156600 | Total Loss: 0.003680 | Recon Loss: 0.003016 | Commit Loss: 0.001328 | Perplexity: 663.945050
2025-09-14 18:06:19,309 Stage: Train 0.5 | Epoch: 51 | Iter: 156800 | Total Loss: 0.003731 | Recon Loss: 0.003079 | Commit Loss: 0.001304 | Perplexity: 663.463076
2025-09-14 18:06:27,169 Stage: Train 0.5 | Epoch: 51 | Iter: 157000 | Total Loss: 0.003635 | Recon Loss: 0.002974 | Commit Loss: 0.001321 | Perplexity: 664.933969
2025-09-14 18:06:34,965 Stage: Train 0.5 | Epoch: 51 | Iter: 157200 | Total Loss: 0.003687 | Recon Loss: 0.003031 | Commit Loss: 0.001312 | Perplexity: 665.500788
2025-09-14 18:06:42,881 Stage: Train 0.5 | Epoch: 51 | Iter: 157400 | Total Loss: 0.003618 | Recon Loss: 0.002957 | Commit Loss: 0.001323 | Perplexity: 662.692392
2025-09-14 18:06:50,711 Stage: Train 0.5 | Epoch: 51 | Iter: 157600 | Total Loss: 0.003621 | Recon Loss: 0.002965 | Commit Loss: 0.001312 | Perplexity: 664.501900
2025-09-14 18:06:58,708 Stage: Train 0.5 | Epoch: 51 | Iter: 157800 | Total Loss: 0.003672 | Recon Loss: 0.003012 | Commit Loss: 0.001319 | Perplexity: 663.898382
Trainning Epoch:  32%|███▏      | 52/165 [1:44:16<3:44:49, 119.37s/it]2025-09-14 18:07:06,861 Stage: Train 0.5 | Epoch: 52 | Iter: 158000 | Total Loss: 0.003663 | Recon Loss: 0.003004 | Commit Loss: 0.001317 | Perplexity: 663.540959
2025-09-14 18:07:14,765 Stage: Train 0.5 | Epoch: 52 | Iter: 158200 | Total Loss: 0.003704 | Recon Loss: 0.003052 | Commit Loss: 0.001304 | Perplexity: 663.444355
2025-09-14 18:07:22,546 Stage: Train 0.5 | Epoch: 52 | Iter: 158400 | Total Loss: 0.003648 | Recon Loss: 0.002995 | Commit Loss: 0.001306 | Perplexity: 663.613898
2025-09-14 18:07:30,378 Stage: Train 0.5 | Epoch: 52 | Iter: 158600 | Total Loss: 0.003611 | Recon Loss: 0.002957 | Commit Loss: 0.001308 | Perplexity: 666.991316
2025-09-14 18:07:38,237 Stage: Train 0.5 | Epoch: 52 | Iter: 158800 | Total Loss: 0.003681 | Recon Loss: 0.003024 | Commit Loss: 0.001313 | Perplexity: 664.821740
2025-09-14 18:07:46,053 Stage: Train 0.5 | Epoch: 52 | Iter: 159000 | Total Loss: 0.003652 | Recon Loss: 0.003003 | Commit Loss: 0.001297 | Perplexity: 663.336339
2025-09-14 18:07:54,057 Stage: Train 0.5 | Epoch: 52 | Iter: 159200 | Total Loss: 0.003626 | Recon Loss: 0.002973 | Commit Loss: 0.001307 | Perplexity: 662.992148
2025-09-14 18:08:02,138 Stage: Train 0.5 | Epoch: 52 | Iter: 159400 | Total Loss: 0.003633 | Recon Loss: 0.002987 | Commit Loss: 0.001292 | Perplexity: 661.564216
2025-09-14 18:08:10,264 Stage: Train 0.5 | Epoch: 52 | Iter: 159600 | Total Loss: 0.003629 | Recon Loss: 0.002985 | Commit Loss: 0.001288 | Perplexity: 662.334632
2025-09-14 18:08:18,396 Stage: Train 0.5 | Epoch: 52 | Iter: 159800 | Total Loss: 0.003652 | Recon Loss: 0.002991 | Commit Loss: 0.001322 | Perplexity: 666.323445
2025-09-14 18:08:26,321 Stage: Train 0.5 | Epoch: 52 | Iter: 160000 | Total Loss: 0.003638 | Recon Loss: 0.002993 | Commit Loss: 0.001291 | Perplexity: 663.569179
2025-09-14 18:08:26,321 Saving model at iteration 160000
2025-09-14 18:08:26,867 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_53_step_160000
2025-09-14 18:08:27,001 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_53_step_160000/pytorch_model.bin
2025-09-14 18:08:27,245 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_53_step_160000/optimizer.bin
2025-09-14 18:08:27,246 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_53_step_160000/scheduler.bin
2025-09-14 18:08:27,246 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_53_step_160000/random_states_0.pkl
2025-09-14 18:08:34,994 Stage: Train 0.5 | Epoch: 52 | Iter: 160200 | Total Loss: 0.003619 | Recon Loss: 0.002972 | Commit Loss: 0.001294 | Perplexity: 661.838067
2025-09-14 18:08:42,812 Stage: Train 0.5 | Epoch: 52 | Iter: 160400 | Total Loss: 0.003736 | Recon Loss: 0.003080 | Commit Loss: 0.001312 | Perplexity: 666.056168
2025-09-14 18:08:50,605 Stage: Train 0.5 | Epoch: 52 | Iter: 160600 | Total Loss: 0.003648 | Recon Loss: 0.003001 | Commit Loss: 0.001295 | Perplexity: 661.093053
2025-09-14 18:08:58,295 Stage: Train 0.5 | Epoch: 52 | Iter: 160800 | Total Loss: 0.003574 | Recon Loss: 0.002929 | Commit Loss: 0.001289 | Perplexity: 661.932930
2025-09-14 18:09:06,011 Stage: Train 0.5 | Epoch: 52 | Iter: 161000 | Total Loss: 0.003610 | Recon Loss: 0.002966 | Commit Loss: 0.001288 | Perplexity: 663.612551
Trainning Epoch:  32%|███▏      | 53/165 [1:46:17<3:43:43, 119.85s/it]2025-09-14 18:09:14,398 Stage: Train 0.5 | Epoch: 53 | Iter: 161200 | Total Loss: 0.003612 | Recon Loss: 0.002962 | Commit Loss: 0.001300 | Perplexity: 664.441751
2025-09-14 18:09:22,118 Stage: Train 0.5 | Epoch: 53 | Iter: 161400 | Total Loss: 0.003625 | Recon Loss: 0.002971 | Commit Loss: 0.001309 | Perplexity: 666.460532
2025-09-14 18:09:29,757 Stage: Train 0.5 | Epoch: 53 | Iter: 161600 | Total Loss: 0.003595 | Recon Loss: 0.002952 | Commit Loss: 0.001286 | Perplexity: 661.164835
2025-09-14 18:09:37,394 Stage: Train 0.5 | Epoch: 53 | Iter: 161800 | Total Loss: 0.003603 | Recon Loss: 0.002954 | Commit Loss: 0.001299 | Perplexity: 666.303293
2025-09-14 18:09:45,488 Stage: Train 0.5 | Epoch: 53 | Iter: 162000 | Total Loss: 0.003573 | Recon Loss: 0.002920 | Commit Loss: 0.001306 | Perplexity: 666.225480
2025-09-14 18:09:53,602 Stage: Train 0.5 | Epoch: 53 | Iter: 162200 | Total Loss: 0.003678 | Recon Loss: 0.003033 | Commit Loss: 0.001291 | Perplexity: 663.950781
2025-09-14 18:10:01,703 Stage: Train 0.5 | Epoch: 53 | Iter: 162400 | Total Loss: 0.003574 | Recon Loss: 0.002923 | Commit Loss: 0.001301 | Perplexity: 665.608340
2025-09-14 18:10:09,521 Stage: Train 0.5 | Epoch: 53 | Iter: 162600 | Total Loss: 0.003623 | Recon Loss: 0.002974 | Commit Loss: 0.001298 | Perplexity: 662.829874
2025-09-14 18:10:17,265 Stage: Train 0.5 | Epoch: 53 | Iter: 162800 | Total Loss: 0.003647 | Recon Loss: 0.002993 | Commit Loss: 0.001308 | Perplexity: 666.815605
2025-09-14 18:10:24,941 Stage: Train 0.5 | Epoch: 53 | Iter: 163000 | Total Loss: 0.003597 | Recon Loss: 0.002953 | Commit Loss: 0.001288 | Perplexity: 664.857573
2025-09-14 18:10:32,646 Stage: Train 0.5 | Epoch: 53 | Iter: 163200 | Total Loss: 0.003625 | Recon Loss: 0.002973 | Commit Loss: 0.001303 | Perplexity: 664.199282
2025-09-14 18:10:40,337 Stage: Train 0.5 | Epoch: 53 | Iter: 163400 | Total Loss: 0.003638 | Recon Loss: 0.002996 | Commit Loss: 0.001284 | Perplexity: 663.162704
2025-09-14 18:10:48,045 Stage: Train 0.5 | Epoch: 53 | Iter: 163600 | Total Loss: 0.003637 | Recon Loss: 0.002990 | Commit Loss: 0.001292 | Perplexity: 665.150878
2025-09-14 18:10:55,745 Stage: Train 0.5 | Epoch: 53 | Iter: 163800 | Total Loss: 0.003637 | Recon Loss: 0.002986 | Commit Loss: 0.001303 | Perplexity: 668.359323
2025-09-14 18:11:03,433 Stage: Train 0.5 | Epoch: 53 | Iter: 164000 | Total Loss: 0.003579 | Recon Loss: 0.002935 | Commit Loss: 0.001289 | Perplexity: 666.745719
Trainning Epoch:  33%|███▎      | 54/165 [1:48:16<3:41:01, 119.47s/it]2025-09-14 18:11:11,170 Stage: Train 0.5 | Epoch: 54 | Iter: 164200 | Total Loss: 0.003670 | Recon Loss: 0.003023 | Commit Loss: 0.001293 | Perplexity: 664.575963
2025-09-14 18:11:18,893 Stage: Train 0.5 | Epoch: 54 | Iter: 164400 | Total Loss: 0.003586 | Recon Loss: 0.002941 | Commit Loss: 0.001289 | Perplexity: 663.380330
2025-09-14 18:11:26,638 Stage: Train 0.5 | Epoch: 54 | Iter: 164600 | Total Loss: 0.003631 | Recon Loss: 0.002984 | Commit Loss: 0.001293 | Perplexity: 666.346999
2025-09-14 18:11:34,334 Stage: Train 0.5 | Epoch: 54 | Iter: 164800 | Total Loss: 0.003575 | Recon Loss: 0.002935 | Commit Loss: 0.001279 | Perplexity: 663.777879
2025-09-14 18:11:42,074 Stage: Train 0.5 | Epoch: 54 | Iter: 165000 | Total Loss: 0.003602 | Recon Loss: 0.002962 | Commit Loss: 0.001280 | Perplexity: 662.333721
2025-09-14 18:11:49,794 Stage: Train 0.5 | Epoch: 54 | Iter: 165200 | Total Loss: 0.003576 | Recon Loss: 0.002928 | Commit Loss: 0.001297 | Perplexity: 668.941659
2025-09-14 18:11:57,514 Stage: Train 0.5 | Epoch: 54 | Iter: 165400 | Total Loss: 0.003636 | Recon Loss: 0.002979 | Commit Loss: 0.001314 | Perplexity: 668.466945
2025-09-14 18:12:05,193 Stage: Train 0.5 | Epoch: 54 | Iter: 165600 | Total Loss: 0.003598 | Recon Loss: 0.002964 | Commit Loss: 0.001269 | Perplexity: 660.997885
2025-09-14 18:12:12,909 Stage: Train 0.5 | Epoch: 54 | Iter: 165800 | Total Loss: 0.003573 | Recon Loss: 0.002924 | Commit Loss: 0.001298 | Perplexity: 668.101708
2025-09-14 18:12:20,574 Stage: Train 0.5 | Epoch: 54 | Iter: 166000 | Total Loss: 0.003576 | Recon Loss: 0.002932 | Commit Loss: 0.001288 | Perplexity: 665.734815
2025-09-14 18:12:28,282 Stage: Train 0.5 | Epoch: 54 | Iter: 166200 | Total Loss: 0.003591 | Recon Loss: 0.002950 | Commit Loss: 0.001282 | Perplexity: 664.119464
2025-09-14 18:12:36,026 Stage: Train 0.5 | Epoch: 54 | Iter: 166400 | Total Loss: 0.003617 | Recon Loss: 0.002972 | Commit Loss: 0.001290 | Perplexity: 665.777823
2025-09-14 18:12:43,726 Stage: Train 0.5 | Epoch: 54 | Iter: 166600 | Total Loss: 0.003657 | Recon Loss: 0.003017 | Commit Loss: 0.001280 | Perplexity: 665.481921
2025-09-14 18:12:51,397 Stage: Train 0.5 | Epoch: 54 | Iter: 166800 | Total Loss: 0.003596 | Recon Loss: 0.002955 | Commit Loss: 0.001281 | Perplexity: 667.289556
2025-09-14 18:12:59,137 Stage: Train 0.5 | Epoch: 54 | Iter: 167000 | Total Loss: 0.003640 | Recon Loss: 0.003002 | Commit Loss: 0.001277 | Perplexity: 663.766001
Trainning Epoch:  33%|███▎      | 55/165 [1:50:13<3:37:44, 118.77s/it]2025-09-14 18:13:06,817 Stage: Train 0.5 | Epoch: 55 | Iter: 167200 | Total Loss: 0.003553 | Recon Loss: 0.002913 | Commit Loss: 0.001279 | Perplexity: 664.155035
2025-09-14 18:13:14,514 Stage: Train 0.5 | Epoch: 55 | Iter: 167400 | Total Loss: 0.003624 | Recon Loss: 0.002994 | Commit Loss: 0.001261 | Perplexity: 664.062010
2025-09-14 18:13:22,237 Stage: Train 0.5 | Epoch: 55 | Iter: 167600 | Total Loss: 0.003575 | Recon Loss: 0.002944 | Commit Loss: 0.001263 | Perplexity: 664.653516
2025-09-14 18:13:29,969 Stage: Train 0.5 | Epoch: 55 | Iter: 167800 | Total Loss: 0.003557 | Recon Loss: 0.002916 | Commit Loss: 0.001283 | Perplexity: 662.334376
2025-09-14 18:13:37,682 Stage: Train 0.5 | Epoch: 55 | Iter: 168000 | Total Loss: 0.003559 | Recon Loss: 0.002915 | Commit Loss: 0.001288 | Perplexity: 665.483902
2025-09-14 18:13:45,377 Stage: Train 0.5 | Epoch: 55 | Iter: 168200 | Total Loss: 0.003601 | Recon Loss: 0.002960 | Commit Loss: 0.001282 | Perplexity: 661.944711
2025-09-14 18:13:53,077 Stage: Train 0.5 | Epoch: 55 | Iter: 168400 | Total Loss: 0.003567 | Recon Loss: 0.002930 | Commit Loss: 0.001274 | Perplexity: 666.121042
2025-09-14 18:14:00,730 Stage: Train 0.5 | Epoch: 55 | Iter: 168600 | Total Loss: 0.003593 | Recon Loss: 0.002958 | Commit Loss: 0.001269 | Perplexity: 663.315816
2025-09-14 18:14:08,441 Stage: Train 0.5 | Epoch: 55 | Iter: 168800 | Total Loss: 0.003613 | Recon Loss: 0.002967 | Commit Loss: 0.001293 | Perplexity: 667.520363
2025-09-14 18:14:16,181 Stage: Train 0.5 | Epoch: 55 | Iter: 169000 | Total Loss: 0.003618 | Recon Loss: 0.002976 | Commit Loss: 0.001284 | Perplexity: 664.382091
2025-09-14 18:14:23,861 Stage: Train 0.5 | Epoch: 55 | Iter: 169200 | Total Loss: 0.003604 | Recon Loss: 0.002958 | Commit Loss: 0.001291 | Perplexity: 665.984656
2025-09-14 18:14:31,549 Stage: Train 0.5 | Epoch: 55 | Iter: 169400 | Total Loss: 0.003584 | Recon Loss: 0.002943 | Commit Loss: 0.001282 | Perplexity: 664.360583
2025-09-14 18:14:39,254 Stage: Train 0.5 | Epoch: 55 | Iter: 169600 | Total Loss: 0.003573 | Recon Loss: 0.002932 | Commit Loss: 0.001282 | Perplexity: 666.477630
2025-09-14 18:14:46,949 Stage: Train 0.5 | Epoch: 55 | Iter: 169800 | Total Loss: 0.003573 | Recon Loss: 0.002937 | Commit Loss: 0.001272 | Perplexity: 665.767874
2025-09-14 18:14:54,678 Stage: Train 0.5 | Epoch: 55 | Iter: 170000 | Total Loss: 0.003582 | Recon Loss: 0.002951 | Commit Loss: 0.001261 | Perplexity: 663.453149
Trainning Epoch:  34%|███▍      | 56/165 [1:52:10<3:34:48, 118.24s/it]2025-09-14 18:15:02,369 Stage: Train 0.5 | Epoch: 56 | Iter: 170200 | Total Loss: 0.003644 | Recon Loss: 0.003006 | Commit Loss: 0.001276 | Perplexity: 664.239984
2025-09-14 18:15:10,089 Stage: Train 0.5 | Epoch: 56 | Iter: 170400 | Total Loss: 0.003578 | Recon Loss: 0.002933 | Commit Loss: 0.001289 | Perplexity: 669.832758
2025-09-14 18:15:17,754 Stage: Train 0.5 | Epoch: 56 | Iter: 170600 | Total Loss: 0.003578 | Recon Loss: 0.002939 | Commit Loss: 0.001279 | Perplexity: 664.795791
2025-09-14 18:15:25,453 Stage: Train 0.5 | Epoch: 56 | Iter: 170800 | Total Loss: 0.003559 | Recon Loss: 0.002921 | Commit Loss: 0.001277 | Perplexity: 666.144135
2025-09-14 18:15:33,193 Stage: Train 0.5 | Epoch: 56 | Iter: 171000 | Total Loss: 0.003532 | Recon Loss: 0.002900 | Commit Loss: 0.001264 | Perplexity: 666.032855
2025-09-14 18:15:40,901 Stage: Train 0.5 | Epoch: 56 | Iter: 171200 | Total Loss: 0.003557 | Recon Loss: 0.002918 | Commit Loss: 0.001278 | Perplexity: 665.225650
2025-09-14 18:15:48,578 Stage: Train 0.5 | Epoch: 56 | Iter: 171400 | Total Loss: 0.003587 | Recon Loss: 0.002947 | Commit Loss: 0.001279 | Perplexity: 667.004313
2025-09-14 18:15:56,377 Stage: Train 0.5 | Epoch: 56 | Iter: 171600 | Total Loss: 0.003550 | Recon Loss: 0.002919 | Commit Loss: 0.001262 | Perplexity: 665.330431
2025-09-14 18:16:04,142 Stage: Train 0.5 | Epoch: 56 | Iter: 171800 | Total Loss: 0.003609 | Recon Loss: 0.002972 | Commit Loss: 0.001274 | Perplexity: 666.943405
2025-09-14 18:16:11,982 Stage: Train 0.5 | Epoch: 56 | Iter: 172000 | Total Loss: 0.003562 | Recon Loss: 0.002932 | Commit Loss: 0.001261 | Perplexity: 663.085121
2025-09-14 18:16:19,837 Stage: Train 0.5 | Epoch: 56 | Iter: 172200 | Total Loss: 0.003563 | Recon Loss: 0.002930 | Commit Loss: 0.001266 | Perplexity: 663.150952
2025-09-14 18:16:27,641 Stage: Train 0.5 | Epoch: 56 | Iter: 172400 | Total Loss: 0.003519 | Recon Loss: 0.002883 | Commit Loss: 0.001273 | Perplexity: 664.605763
2025-09-14 18:16:35,460 Stage: Train 0.5 | Epoch: 56 | Iter: 172600 | Total Loss: 0.003508 | Recon Loss: 0.002871 | Commit Loss: 0.001274 | Perplexity: 665.943040
2025-09-14 18:16:43,614 Stage: Train 0.5 | Epoch: 56 | Iter: 172800 | Total Loss: 0.003563 | Recon Loss: 0.002930 | Commit Loss: 0.001265 | Perplexity: 665.433751
2025-09-14 18:16:51,644 Stage: Train 0.5 | Epoch: 56 | Iter: 173000 | Total Loss: 0.003547 | Recon Loss: 0.002914 | Commit Loss: 0.001265 | Perplexity: 663.174688
Trainning Epoch:  35%|███▍      | 57/165 [1:54:08<3:32:59, 118.33s/it]2025-09-14 18:16:59,453 Stage: Train 0.5 | Epoch: 57 | Iter: 173200 | Total Loss: 0.003556 | Recon Loss: 0.002915 | Commit Loss: 0.001283 | Perplexity: 668.005390
2025-09-14 18:17:07,274 Stage: Train 0.5 | Epoch: 57 | Iter: 173400 | Total Loss: 0.003562 | Recon Loss: 0.002928 | Commit Loss: 0.001268 | Perplexity: 663.956917
2025-09-14 18:17:15,277 Stage: Train 0.5 | Epoch: 57 | Iter: 173600 | Total Loss: 0.003515 | Recon Loss: 0.002881 | Commit Loss: 0.001268 | Perplexity: 665.947749
2025-09-14 18:17:23,345 Stage: Train 0.5 | Epoch: 57 | Iter: 173800 | Total Loss: 0.003538 | Recon Loss: 0.002902 | Commit Loss: 0.001273 | Perplexity: 665.492748
2025-09-14 18:17:31,134 Stage: Train 0.5 | Epoch: 57 | Iter: 174000 | Total Loss: 0.003502 | Recon Loss: 0.002868 | Commit Loss: 0.001269 | Perplexity: 666.083672
2025-09-14 18:17:38,897 Stage: Train 0.5 | Epoch: 57 | Iter: 174200 | Total Loss: 0.003561 | Recon Loss: 0.002927 | Commit Loss: 0.001268 | Perplexity: 664.348704
2025-09-14 18:17:46,621 Stage: Train 0.5 | Epoch: 57 | Iter: 174400 | Total Loss: 0.003603 | Recon Loss: 0.002966 | Commit Loss: 0.001273 | Perplexity: 666.334842
2025-09-14 18:17:54,354 Stage: Train 0.5 | Epoch: 57 | Iter: 174600 | Total Loss: 0.003517 | Recon Loss: 0.002882 | Commit Loss: 0.001271 | Perplexity: 668.714310
2025-09-14 18:18:02,122 Stage: Train 0.5 | Epoch: 57 | Iter: 174800 | Total Loss: 0.003494 | Recon Loss: 0.002860 | Commit Loss: 0.001269 | Perplexity: 669.523086
2025-09-14 18:18:09,829 Stage: Train 0.5 | Epoch: 57 | Iter: 175000 | Total Loss: 0.003534 | Recon Loss: 0.002900 | Commit Loss: 0.001267 | Perplexity: 668.002715
2025-09-14 18:18:17,557 Stage: Train 0.5 | Epoch: 57 | Iter: 175200 | Total Loss: 0.003553 | Recon Loss: 0.002920 | Commit Loss: 0.001266 | Perplexity: 666.211313
2025-09-14 18:18:25,306 Stage: Train 0.5 | Epoch: 57 | Iter: 175400 | Total Loss: 0.003566 | Recon Loss: 0.002927 | Commit Loss: 0.001278 | Perplexity: 667.091579
2025-09-14 18:18:33,078 Stage: Train 0.5 | Epoch: 57 | Iter: 175600 | Total Loss: 0.003511 | Recon Loss: 0.002878 | Commit Loss: 0.001268 | Perplexity: 666.148915
2025-09-14 18:18:40,857 Stage: Train 0.5 | Epoch: 57 | Iter: 175800 | Total Loss: 0.003550 | Recon Loss: 0.002920 | Commit Loss: 0.001260 | Perplexity: 664.454319
2025-09-14 18:18:48,658 Stage: Train 0.5 | Epoch: 57 | Iter: 176000 | Total Loss: 0.003519 | Recon Loss: 0.002881 | Commit Loss: 0.001277 | Perplexity: 668.599348
2025-09-14 18:18:56,449 Stage: Train 0.5 | Epoch: 57 | Iter: 176200 | Total Loss: 0.003537 | Recon Loss: 0.002901 | Commit Loss: 0.001273 | Perplexity: 666.339944
Trainning Epoch:  35%|███▌      | 58/165 [1:56:07<3:31:06, 118.38s/it]2025-09-14 18:19:04,289 Stage: Train 0.5 | Epoch: 58 | Iter: 176400 | Total Loss: 0.003519 | Recon Loss: 0.002887 | Commit Loss: 0.001263 | Perplexity: 664.288458
2025-09-14 18:19:12,009 Stage: Train 0.5 | Epoch: 58 | Iter: 176600 | Total Loss: 0.003497 | Recon Loss: 0.002877 | Commit Loss: 0.001240 | Perplexity: 664.066882
2025-09-14 18:19:19,794 Stage: Train 0.5 | Epoch: 58 | Iter: 176800 | Total Loss: 0.003540 | Recon Loss: 0.002908 | Commit Loss: 0.001264 | Perplexity: 666.050064
2025-09-14 18:19:27,643 Stage: Train 0.5 | Epoch: 58 | Iter: 177000 | Total Loss: 0.003529 | Recon Loss: 0.002897 | Commit Loss: 0.001264 | Perplexity: 666.171154
2025-09-14 18:19:35,634 Stage: Train 0.5 | Epoch: 58 | Iter: 177200 | Total Loss: 0.003528 | Recon Loss: 0.002896 | Commit Loss: 0.001263 | Perplexity: 666.654039
2025-09-14 18:19:43,738 Stage: Train 0.5 | Epoch: 58 | Iter: 177400 | Total Loss: 0.003585 | Recon Loss: 0.002952 | Commit Loss: 0.001266 | Perplexity: 669.763332
2025-09-14 18:19:51,790 Stage: Train 0.5 | Epoch: 58 | Iter: 177600 | Total Loss: 0.003515 | Recon Loss: 0.002891 | Commit Loss: 0.001249 | Perplexity: 664.969742
2025-09-14 18:19:59,637 Stage: Train 0.5 | Epoch: 58 | Iter: 177800 | Total Loss: 0.003514 | Recon Loss: 0.002881 | Commit Loss: 0.001265 | Perplexity: 666.864624
2025-09-14 18:20:07,442 Stage: Train 0.5 | Epoch: 58 | Iter: 178000 | Total Loss: 0.003535 | Recon Loss: 0.002904 | Commit Loss: 0.001260 | Perplexity: 667.667130
2025-09-14 18:20:15,181 Stage: Train 0.5 | Epoch: 58 | Iter: 178200 | Total Loss: 0.003512 | Recon Loss: 0.002884 | Commit Loss: 0.001256 | Perplexity: 665.487733
2025-09-14 18:20:22,910 Stage: Train 0.5 | Epoch: 58 | Iter: 178400 | Total Loss: 0.003557 | Recon Loss: 0.002921 | Commit Loss: 0.001271 | Perplexity: 670.774520
2025-09-14 18:20:30,626 Stage: Train 0.5 | Epoch: 58 | Iter: 178600 | Total Loss: 0.003591 | Recon Loss: 0.002960 | Commit Loss: 0.001263 | Perplexity: 669.054574
2025-09-14 18:20:38,313 Stage: Train 0.5 | Epoch: 58 | Iter: 178800 | Total Loss: 0.003496 | Recon Loss: 0.002867 | Commit Loss: 0.001259 | Perplexity: 668.038056
2025-09-14 18:20:46,021 Stage: Train 0.5 | Epoch: 58 | Iter: 179000 | Total Loss: 0.003528 | Recon Loss: 0.002900 | Commit Loss: 0.001256 | Perplexity: 666.596264
2025-09-14 18:20:53,738 Stage: Train 0.5 | Epoch: 58 | Iter: 179200 | Total Loss: 0.003519 | Recon Loss: 0.002890 | Commit Loss: 0.001256 | Perplexity: 666.021361
Trainning Epoch:  36%|███▌      | 59/165 [1:58:06<3:29:20, 118.49s/it]2025-09-14 18:21:01,550 Stage: Train 0.5 | Epoch: 59 | Iter: 179400 | Total Loss: 0.003558 | Recon Loss: 0.002931 | Commit Loss: 0.001254 | Perplexity: 663.995898
2025-09-14 18:21:09,642 Stage: Train 0.5 | Epoch: 59 | Iter: 179600 | Total Loss: 0.003512 | Recon Loss: 0.002880 | Commit Loss: 0.001264 | Perplexity: 667.356148
2025-09-14 18:21:17,349 Stage: Train 0.5 | Epoch: 59 | Iter: 179800 | Total Loss: 0.003471 | Recon Loss: 0.002838 | Commit Loss: 0.001267 | Perplexity: 668.033909
2025-09-14 18:21:25,021 Stage: Train 0.5 | Epoch: 59 | Iter: 180000 | Total Loss: 0.003523 | Recon Loss: 0.002907 | Commit Loss: 0.001233 | Perplexity: 665.668476
2025-09-14 18:21:25,021 Saving model at iteration 180000
2025-09-14 18:21:25,165 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_60_step_180000
2025-09-14 18:21:25,300 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_60_step_180000/pytorch_model.bin
2025-09-14 18:21:25,538 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_60_step_180000/optimizer.bin
2025-09-14 18:21:25,539 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_60_step_180000/scheduler.bin
2025-09-14 18:21:25,540 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_60_step_180000/random_states_0.pkl
2025-09-14 18:21:33,313 Stage: Train 0.5 | Epoch: 59 | Iter: 180200 | Total Loss: 0.003458 | Recon Loss: 0.002836 | Commit Loss: 0.001244 | Perplexity: 666.518727
2025-09-14 18:21:41,046 Stage: Train 0.5 | Epoch: 59 | Iter: 180400 | Total Loss: 0.003530 | Recon Loss: 0.002902 | Commit Loss: 0.001254 | Perplexity: 666.879543
2025-09-14 18:21:48,766 Stage: Train 0.5 | Epoch: 59 | Iter: 180600 | Total Loss: 0.003542 | Recon Loss: 0.002915 | Commit Loss: 0.001255 | Perplexity: 669.793666
2025-09-14 18:21:56,477 Stage: Train 0.5 | Epoch: 59 | Iter: 180800 | Total Loss: 0.003503 | Recon Loss: 0.002878 | Commit Loss: 0.001250 | Perplexity: 666.712534
2025-09-14 18:22:04,173 Stage: Train 0.5 | Epoch: 59 | Iter: 181000 | Total Loss: 0.003472 | Recon Loss: 0.002846 | Commit Loss: 0.001252 | Perplexity: 664.119174
2025-09-14 18:22:11,881 Stage: Train 0.5 | Epoch: 59 | Iter: 181200 | Total Loss: 0.003489 | Recon Loss: 0.002868 | Commit Loss: 0.001243 | Perplexity: 666.893145
2025-09-14 18:22:19,594 Stage: Train 0.5 | Epoch: 59 | Iter: 181400 | Total Loss: 0.003551 | Recon Loss: 0.002928 | Commit Loss: 0.001246 | Perplexity: 667.810580
2025-09-14 18:22:27,286 Stage: Train 0.5 | Epoch: 59 | Iter: 181600 | Total Loss: 0.003535 | Recon Loss: 0.002912 | Commit Loss: 0.001246 | Perplexity: 668.380256
2025-09-14 18:22:34,917 Stage: Train 0.5 | Epoch: 59 | Iter: 181800 | Total Loss: 0.003492 | Recon Loss: 0.002868 | Commit Loss: 0.001249 | Perplexity: 666.550763
2025-09-14 18:22:42,582 Stage: Train 0.5 | Epoch: 59 | Iter: 182000 | Total Loss: 0.003573 | Recon Loss: 0.002951 | Commit Loss: 0.001242 | Perplexity: 666.708281
2025-09-14 18:22:50,241 Stage: Train 0.5 | Epoch: 59 | Iter: 182200 | Total Loss: 0.003471 | Recon Loss: 0.002847 | Commit Loss: 0.001247 | Perplexity: 667.526481
Trainning Epoch:  36%|███▋      | 60/165 [2:00:04<3:27:15, 118.43s/it]2025-09-14 18:22:58,297 Stage: Train 0.5 | Epoch: 60 | Iter: 182400 | Total Loss: 0.003495 | Recon Loss: 0.002872 | Commit Loss: 0.001246 | Perplexity: 670.136755
2025-09-14 18:23:05,980 Stage: Train 0.5 | Epoch: 60 | Iter: 182600 | Total Loss: 0.003490 | Recon Loss: 0.002874 | Commit Loss: 0.001233 | Perplexity: 663.818168
2025-09-14 18:23:13,669 Stage: Train 0.5 | Epoch: 60 | Iter: 182800 | Total Loss: 0.003554 | Recon Loss: 0.002931 | Commit Loss: 0.001245 | Perplexity: 668.164364
2025-09-14 18:23:21,371 Stage: Train 0.5 | Epoch: 60 | Iter: 183000 | Total Loss: 0.003426 | Recon Loss: 0.002803 | Commit Loss: 0.001246 | Perplexity: 669.055999
2025-09-14 18:23:29,093 Stage: Train 0.5 | Epoch: 60 | Iter: 183200 | Total Loss: 0.003495 | Recon Loss: 0.002877 | Commit Loss: 0.001236 | Perplexity: 665.679676
2025-09-14 18:23:36,779 Stage: Train 0.5 | Epoch: 60 | Iter: 183400 | Total Loss: 0.003506 | Recon Loss: 0.002881 | Commit Loss: 0.001251 | Perplexity: 667.926366
2025-09-14 18:23:44,478 Stage: Train 0.5 | Epoch: 60 | Iter: 183600 | Total Loss: 0.003479 | Recon Loss: 0.002862 | Commit Loss: 0.001233 | Perplexity: 665.781467
2025-09-14 18:23:52,183 Stage: Train 0.5 | Epoch: 60 | Iter: 183800 | Total Loss: 0.003472 | Recon Loss: 0.002849 | Commit Loss: 0.001246 | Perplexity: 666.027337
2025-09-14 18:23:59,886 Stage: Train 0.5 | Epoch: 60 | Iter: 184000 | Total Loss: 0.003427 | Recon Loss: 0.002810 | Commit Loss: 0.001234 | Perplexity: 667.657960
2025-09-14 18:24:07,601 Stage: Train 0.5 | Epoch: 60 | Iter: 184200 | Total Loss: 0.003513 | Recon Loss: 0.002894 | Commit Loss: 0.001238 | Perplexity: 667.924054
2025-09-14 18:24:15,321 Stage: Train 0.5 | Epoch: 60 | Iter: 184400 | Total Loss: 0.003468 | Recon Loss: 0.002847 | Commit Loss: 0.001242 | Perplexity: 667.065100
2025-09-14 18:24:23,026 Stage: Train 0.5 | Epoch: 60 | Iter: 184600 | Total Loss: 0.003489 | Recon Loss: 0.002870 | Commit Loss: 0.001239 | Perplexity: 667.706728
2025-09-14 18:24:30,733 Stage: Train 0.5 | Epoch: 60 | Iter: 184800 | Total Loss: 0.003455 | Recon Loss: 0.002839 | Commit Loss: 0.001232 | Perplexity: 663.541663
2025-09-14 18:24:38,427 Stage: Train 0.5 | Epoch: 60 | Iter: 185000 | Total Loss: 0.003482 | Recon Loss: 0.002857 | Commit Loss: 0.001250 | Perplexity: 671.241953
2025-09-14 18:24:46,154 Stage: Train 0.5 | Epoch: 60 | Iter: 185200 | Total Loss: 0.003424 | Recon Loss: 0.002806 | Commit Loss: 0.001235 | Perplexity: 666.091961
Trainning Epoch:  37%|███▋      | 61/165 [2:02:01<3:24:32, 118.01s/it]2025-09-14 18:24:53,867 Stage: Train 0.5 | Epoch: 61 | Iter: 185400 | Total Loss: 0.003557 | Recon Loss: 0.002934 | Commit Loss: 0.001246 | Perplexity: 664.865942
2025-09-14 18:25:01,565 Stage: Train 0.5 | Epoch: 61 | Iter: 185600 | Total Loss: 0.003518 | Recon Loss: 0.002902 | Commit Loss: 0.001231 | Perplexity: 665.200474
2025-09-14 18:25:09,261 Stage: Train 0.5 | Epoch: 61 | Iter: 185800 | Total Loss: 0.003482 | Recon Loss: 0.002868 | Commit Loss: 0.001226 | Perplexity: 665.313401
2025-09-14 18:25:16,945 Stage: Train 0.5 | Epoch: 61 | Iter: 186000 | Total Loss: 0.003434 | Recon Loss: 0.002818 | Commit Loss: 0.001232 | Perplexity: 665.633166
2025-09-14 18:25:24,630 Stage: Train 0.5 | Epoch: 61 | Iter: 186200 | Total Loss: 0.003467 | Recon Loss: 0.002847 | Commit Loss: 0.001240 | Perplexity: 670.447276
2025-09-14 18:25:32,668 Stage: Train 0.5 | Epoch: 61 | Iter: 186400 | Total Loss: 0.003500 | Recon Loss: 0.002882 | Commit Loss: 0.001236 | Perplexity: 667.664661
2025-09-14 18:25:40,775 Stage: Train 0.5 | Epoch: 61 | Iter: 186600 | Total Loss: 0.003452 | Recon Loss: 0.002832 | Commit Loss: 0.001240 | Perplexity: 669.992616
2025-09-14 18:25:48,593 Stage: Train 0.5 | Epoch: 61 | Iter: 186800 | Total Loss: 0.003522 | Recon Loss: 0.002906 | Commit Loss: 0.001233 | Perplexity: 669.067400
2025-09-14 18:25:56,301 Stage: Train 0.5 | Epoch: 61 | Iter: 187000 | Total Loss: 0.003436 | Recon Loss: 0.002822 | Commit Loss: 0.001229 | Perplexity: 665.939222
2025-09-14 18:26:04,037 Stage: Train 0.5 | Epoch: 61 | Iter: 187200 | Total Loss: 0.003551 | Recon Loss: 0.002931 | Commit Loss: 0.001239 | Perplexity: 666.419828
2025-09-14 18:26:11,714 Stage: Train 0.5 | Epoch: 61 | Iter: 187400 | Total Loss: 0.003466 | Recon Loss: 0.002853 | Commit Loss: 0.001226 | Perplexity: 668.098183
2025-09-14 18:26:19,421 Stage: Train 0.5 | Epoch: 61 | Iter: 187600 | Total Loss: 0.003514 | Recon Loss: 0.002902 | Commit Loss: 0.001222 | Perplexity: 665.579042
2025-09-14 18:26:27,150 Stage: Train 0.5 | Epoch: 61 | Iter: 187800 | Total Loss: 0.003473 | Recon Loss: 0.002864 | Commit Loss: 0.001218 | Perplexity: 666.668731
2025-09-14 18:26:34,809 Stage: Train 0.5 | Epoch: 61 | Iter: 188000 | Total Loss: 0.003489 | Recon Loss: 0.002875 | Commit Loss: 0.001228 | Perplexity: 666.799208
2025-09-14 18:26:42,615 Stage: Train 0.5 | Epoch: 61 | Iter: 188200 | Total Loss: 0.003455 | Recon Loss: 0.002843 | Commit Loss: 0.001224 | Perplexity: 666.777045
Trainning Epoch:  38%|███▊      | 62/165 [2:03:59<3:22:33, 117.99s/it]2025-09-14 18:26:50,330 Stage: Train 0.5 | Epoch: 62 | Iter: 188400 | Total Loss: 0.003500 | Recon Loss: 0.002883 | Commit Loss: 0.001233 | Perplexity: 664.576431
2025-09-14 18:26:58,089 Stage: Train 0.5 | Epoch: 62 | Iter: 188600 | Total Loss: 0.003462 | Recon Loss: 0.002850 | Commit Loss: 0.001224 | Perplexity: 666.252822
2025-09-14 18:27:05,854 Stage: Train 0.5 | Epoch: 62 | Iter: 188800 | Total Loss: 0.003463 | Recon Loss: 0.002852 | Commit Loss: 0.001222 | Perplexity: 668.355046
2025-09-14 18:27:13,573 Stage: Train 0.5 | Epoch: 62 | Iter: 189000 | Total Loss: 0.003430 | Recon Loss: 0.002817 | Commit Loss: 0.001225 | Perplexity: 668.439487
2025-09-14 18:27:21,317 Stage: Train 0.5 | Epoch: 62 | Iter: 189200 | Total Loss: 0.003417 | Recon Loss: 0.002800 | Commit Loss: 0.001233 | Perplexity: 671.600164
2025-09-14 18:27:29,037 Stage: Train 0.5 | Epoch: 62 | Iter: 189400 | Total Loss: 0.003428 | Recon Loss: 0.002808 | Commit Loss: 0.001239 | Perplexity: 671.268076
2025-09-14 18:27:36,722 Stage: Train 0.5 | Epoch: 62 | Iter: 189600 | Total Loss: 0.003438 | Recon Loss: 0.002821 | Commit Loss: 0.001235 | Perplexity: 669.398274
2025-09-14 18:27:44,429 Stage: Train 0.5 | Epoch: 62 | Iter: 189800 | Total Loss: 0.003499 | Recon Loss: 0.002879 | Commit Loss: 0.001239 | Perplexity: 668.726602
2025-09-14 18:27:52,117 Stage: Train 0.5 | Epoch: 62 | Iter: 190000 | Total Loss: 0.003413 | Recon Loss: 0.002808 | Commit Loss: 0.001211 | Perplexity: 666.494626
2025-09-14 18:27:59,818 Stage: Train 0.5 | Epoch: 62 | Iter: 190200 | Total Loss: 0.003519 | Recon Loss: 0.002900 | Commit Loss: 0.001238 | Perplexity: 667.641075
2025-09-14 18:28:07,574 Stage: Train 0.5 | Epoch: 62 | Iter: 190400 | Total Loss: 0.003413 | Recon Loss: 0.002805 | Commit Loss: 0.001217 | Perplexity: 665.992712
2025-09-14 18:28:15,301 Stage: Train 0.5 | Epoch: 62 | Iter: 190600 | Total Loss: 0.003431 | Recon Loss: 0.002818 | Commit Loss: 0.001225 | Perplexity: 668.987083
2025-09-14 18:28:23,014 Stage: Train 0.5 | Epoch: 62 | Iter: 190800 | Total Loss: 0.003455 | Recon Loss: 0.002840 | Commit Loss: 0.001230 | Perplexity: 670.092169
2025-09-14 18:28:30,738 Stage: Train 0.5 | Epoch: 62 | Iter: 191000 | Total Loss: 0.003421 | Recon Loss: 0.002811 | Commit Loss: 0.001220 | Perplexity: 670.645875
2025-09-14 18:28:38,422 Stage: Train 0.5 | Epoch: 62 | Iter: 191200 | Total Loss: 0.003460 | Recon Loss: 0.002850 | Commit Loss: 0.001220 | Perplexity: 665.672999
Trainning Epoch:  38%|███▊      | 63/165 [2:05:56<3:20:14, 117.79s/it]2025-09-14 18:28:46,181 Stage: Train 0.5 | Epoch: 63 | Iter: 191400 | Total Loss: 0.003435 | Recon Loss: 0.002832 | Commit Loss: 0.001206 | Perplexity: 663.976662
2025-09-14 18:28:53,902 Stage: Train 0.5 | Epoch: 63 | Iter: 191600 | Total Loss: 0.003442 | Recon Loss: 0.002829 | Commit Loss: 0.001225 | Perplexity: 675.309144
2025-09-14 18:29:01,586 Stage: Train 0.5 | Epoch: 63 | Iter: 191800 | Total Loss: 0.003462 | Recon Loss: 0.002850 | Commit Loss: 0.001224 | Perplexity: 670.287269
2025-09-14 18:29:09,273 Stage: Train 0.5 | Epoch: 63 | Iter: 192000 | Total Loss: 0.003449 | Recon Loss: 0.002835 | Commit Loss: 0.001227 | Perplexity: 669.822480
2025-09-14 18:29:16,989 Stage: Train 0.5 | Epoch: 63 | Iter: 192200 | Total Loss: 0.003474 | Recon Loss: 0.002864 | Commit Loss: 0.001220 | Perplexity: 666.295457
2025-09-14 18:29:24,709 Stage: Train 0.5 | Epoch: 63 | Iter: 192400 | Total Loss: 0.003366 | Recon Loss: 0.002760 | Commit Loss: 0.001212 | Perplexity: 668.328407
2025-09-14 18:29:32,410 Stage: Train 0.5 | Epoch: 63 | Iter: 192600 | Total Loss: 0.003456 | Recon Loss: 0.002847 | Commit Loss: 0.001218 | Perplexity: 667.514872
2025-09-14 18:29:40,130 Stage: Train 0.5 | Epoch: 63 | Iter: 192800 | Total Loss: 0.003421 | Recon Loss: 0.002812 | Commit Loss: 0.001219 | Perplexity: 669.491965
2025-09-14 18:29:47,857 Stage: Train 0.5 | Epoch: 63 | Iter: 193000 | Total Loss: 0.003442 | Recon Loss: 0.002829 | Commit Loss: 0.001226 | Perplexity: 668.481908
2025-09-14 18:29:55,621 Stage: Train 0.5 | Epoch: 63 | Iter: 193200 | Total Loss: 0.003397 | Recon Loss: 0.002795 | Commit Loss: 0.001204 | Perplexity: 668.269341
2025-09-14 18:30:03,286 Stage: Train 0.5 | Epoch: 63 | Iter: 193400 | Total Loss: 0.003396 | Recon Loss: 0.002784 | Commit Loss: 0.001224 | Perplexity: 671.651113
2025-09-14 18:30:10,990 Stage: Train 0.5 | Epoch: 63 | Iter: 193600 | Total Loss: 0.003468 | Recon Loss: 0.002863 | Commit Loss: 0.001209 | Perplexity: 667.937245
2025-09-14 18:30:18,713 Stage: Train 0.5 | Epoch: 63 | Iter: 193800 | Total Loss: 0.003451 | Recon Loss: 0.002839 | Commit Loss: 0.001224 | Perplexity: 673.426610
2025-09-14 18:30:26,455 Stage: Train 0.5 | Epoch: 63 | Iter: 194000 | Total Loss: 0.003403 | Recon Loss: 0.002797 | Commit Loss: 0.001212 | Perplexity: 669.240609
2025-09-14 18:30:34,173 Stage: Train 0.5 | Epoch: 63 | Iter: 194200 | Total Loss: 0.003438 | Recon Loss: 0.002830 | Commit Loss: 0.001217 | Perplexity: 669.144890
2025-09-14 18:30:41,910 Stage: Train 0.5 | Epoch: 63 | Iter: 194400 | Total Loss: 0.003382 | Recon Loss: 0.002773 | Commit Loss: 0.001218 | Perplexity: 667.385387
Trainning Epoch:  39%|███▉      | 64/165 [2:07:53<3:17:58, 117.61s/it]2025-09-14 18:30:49,643 Stage: Train 0.5 | Epoch: 64 | Iter: 194600 | Total Loss: 0.003462 | Recon Loss: 0.002857 | Commit Loss: 0.001212 | Perplexity: 670.452399
2025-09-14 18:30:57,374 Stage: Train 0.5 | Epoch: 64 | Iter: 194800 | Total Loss: 0.003355 | Recon Loss: 0.002756 | Commit Loss: 0.001198 | Perplexity: 668.344954
2025-09-14 18:31:05,101 Stage: Train 0.5 | Epoch: 64 | Iter: 195000 | Total Loss: 0.003362 | Recon Loss: 0.002752 | Commit Loss: 0.001220 | Perplexity: 670.262162
2025-09-14 18:31:12,814 Stage: Train 0.5 | Epoch: 64 | Iter: 195200 | Total Loss: 0.003430 | Recon Loss: 0.002826 | Commit Loss: 0.001209 | Perplexity: 666.443446
2025-09-14 18:31:20,489 Stage: Train 0.5 | Epoch: 64 | Iter: 195400 | Total Loss: 0.003384 | Recon Loss: 0.002779 | Commit Loss: 0.001210 | Perplexity: 669.645672
2025-09-14 18:31:28,233 Stage: Train 0.5 | Epoch: 64 | Iter: 195600 | Total Loss: 0.003490 | Recon Loss: 0.002872 | Commit Loss: 0.001236 | Perplexity: 673.157293
2025-09-14 18:31:36,007 Stage: Train 0.5 | Epoch: 64 | Iter: 195800 | Total Loss: 0.003376 | Recon Loss: 0.002776 | Commit Loss: 0.001199 | Perplexity: 668.734589
2025-09-14 18:31:43,794 Stage: Train 0.5 | Epoch: 64 | Iter: 196000 | Total Loss: 0.003437 | Recon Loss: 0.002824 | Commit Loss: 0.001225 | Perplexity: 672.155192
2025-09-14 18:31:51,510 Stage: Train 0.5 | Epoch: 64 | Iter: 196200 | Total Loss: 0.003410 | Recon Loss: 0.002805 | Commit Loss: 0.001209 | Perplexity: 669.813491
2025-09-14 18:31:59,470 Stage: Train 0.5 | Epoch: 64 | Iter: 196400 | Total Loss: 0.003475 | Recon Loss: 0.002868 | Commit Loss: 0.001214 | Perplexity: 669.500650
2025-09-14 18:32:07,213 Stage: Train 0.5 | Epoch: 64 | Iter: 196600 | Total Loss: 0.003403 | Recon Loss: 0.002802 | Commit Loss: 0.001202 | Perplexity: 668.325730
2025-09-14 18:32:14,967 Stage: Train 0.5 | Epoch: 64 | Iter: 196800 | Total Loss: 0.003430 | Recon Loss: 0.002821 | Commit Loss: 0.001218 | Perplexity: 671.205237
2025-09-14 18:32:22,695 Stage: Train 0.5 | Epoch: 64 | Iter: 197000 | Total Loss: 0.003441 | Recon Loss: 0.002836 | Commit Loss: 0.001210 | Perplexity: 671.716578
2025-09-14 18:32:30,473 Stage: Train 0.5 | Epoch: 64 | Iter: 197200 | Total Loss: 0.003444 | Recon Loss: 0.002836 | Commit Loss: 0.001217 | Perplexity: 669.739218
2025-09-14 18:32:38,185 Stage: Train 0.5 | Epoch: 64 | Iter: 197400 | Total Loss: 0.003439 | Recon Loss: 0.002836 | Commit Loss: 0.001207 | Perplexity: 670.159890
Trainning Epoch:  39%|███▉      | 65/165 [2:09:51<3:16:05, 117.65s/it]2025-09-14 18:32:45,921 Stage: Train 0.5 | Epoch: 65 | Iter: 197600 | Total Loss: 0.003366 | Recon Loss: 0.002767 | Commit Loss: 0.001197 | Perplexity: 665.886974
2025-09-14 18:32:53,654 Stage: Train 0.5 | Epoch: 65 | Iter: 197800 | Total Loss: 0.003427 | Recon Loss: 0.002818 | Commit Loss: 0.001218 | Perplexity: 673.300574
2025-09-14 18:33:01,334 Stage: Train 0.5 | Epoch: 65 | Iter: 198000 | Total Loss: 0.003388 | Recon Loss: 0.002789 | Commit Loss: 0.001196 | Perplexity: 669.108206
2025-09-14 18:33:09,006 Stage: Train 0.5 | Epoch: 65 | Iter: 198200 | Total Loss: 0.003402 | Recon Loss: 0.002797 | Commit Loss: 0.001209 | Perplexity: 670.669121
2025-09-14 18:33:16,642 Stage: Train 0.5 | Epoch: 65 | Iter: 198400 | Total Loss: 0.003433 | Recon Loss: 0.002820 | Commit Loss: 0.001227 | Perplexity: 674.115806
2025-09-14 18:33:24,274 Stage: Train 0.5 | Epoch: 65 | Iter: 198600 | Total Loss: 0.003391 | Recon Loss: 0.002788 | Commit Loss: 0.001207 | Perplexity: 673.263351
2025-09-14 18:33:32,197 Stage: Train 0.5 | Epoch: 65 | Iter: 198800 | Total Loss: 0.003374 | Recon Loss: 0.002772 | Commit Loss: 0.001205 | Perplexity: 670.431736
2025-09-14 18:33:40,063 Stage: Train 0.5 | Epoch: 65 | Iter: 199000 | Total Loss: 0.003407 | Recon Loss: 0.002803 | Commit Loss: 0.001209 | Perplexity: 672.702208
2025-09-14 18:33:47,889 Stage: Train 0.5 | Epoch: 65 | Iter: 199200 | Total Loss: 0.003434 | Recon Loss: 0.002831 | Commit Loss: 0.001206 | Perplexity: 671.546600
2025-09-14 18:33:55,697 Stage: Train 0.5 | Epoch: 65 | Iter: 199400 | Total Loss: 0.003403 | Recon Loss: 0.002801 | Commit Loss: 0.001204 | Perplexity: 670.012640
2025-09-14 18:34:03,518 Stage: Train 0.5 | Epoch: 65 | Iter: 199600 | Total Loss: 0.003364 | Recon Loss: 0.002759 | Commit Loss: 0.001211 | Perplexity: 673.011128
2025-09-14 18:34:11,331 Stage: Train 0.5 | Epoch: 65 | Iter: 199800 | Total Loss: 0.003382 | Recon Loss: 0.002782 | Commit Loss: 0.001200 | Perplexity: 670.025113
2025-09-14 18:34:19,308 Stage: Train 0.5 | Epoch: 65 | Iter: 200000 | Total Loss: 0.003359 | Recon Loss: 0.002750 | Commit Loss: 0.001218 | Perplexity: 675.835305
2025-09-14 18:34:19,309 Saving model at iteration 200000
2025-09-14 18:34:19,459 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_66_step_200000
2025-09-14 18:34:19,595 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_66_step_200000/pytorch_model.bin
2025-09-14 18:34:19,835 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_66_step_200000/optimizer.bin
2025-09-14 18:34:19,835 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_66_step_200000/scheduler.bin
2025-09-14 18:34:19,836 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_66_step_200000/random_states_0.pkl
2025-09-14 18:34:28,336 Stage: Train 0.5 | Epoch: 65 | Iter: 200200 | Total Loss: 0.003363 | Recon Loss: 0.002765 | Commit Loss: 0.001195 | Perplexity: 670.398963
2025-09-14 18:34:36,048 Stage: Train 0.5 | Epoch: 65 | Iter: 200400 | Total Loss: 0.003436 | Recon Loss: 0.002828 | Commit Loss: 0.001215 | Perplexity: 671.941741
Trainning Epoch:  40%|████      | 66/165 [2:11:50<3:14:56, 118.15s/it]2025-09-14 18:34:43,789 Stage: Train 0.5 | Epoch: 66 | Iter: 200600 | Total Loss: 0.003377 | Recon Loss: 0.002777 | Commit Loss: 0.001202 | Perplexity: 669.349214
2025-09-14 18:34:51,522 Stage: Train 0.5 | Epoch: 66 | Iter: 200800 | Total Loss: 0.003409 | Recon Loss: 0.002804 | Commit Loss: 0.001210 | Perplexity: 676.078260
2025-09-14 18:34:59,202 Stage: Train 0.5 | Epoch: 66 | Iter: 201000 | Total Loss: 0.003353 | Recon Loss: 0.002746 | Commit Loss: 0.001213 | Perplexity: 675.038889
2025-09-14 18:35:06,949 Stage: Train 0.5 | Epoch: 66 | Iter: 201200 | Total Loss: 0.003386 | Recon Loss: 0.002794 | Commit Loss: 0.001184 | Perplexity: 669.140569
2025-09-14 18:35:14,677 Stage: Train 0.5 | Epoch: 66 | Iter: 201400 | Total Loss: 0.003413 | Recon Loss: 0.002817 | Commit Loss: 0.001192 | Perplexity: 669.070854
2025-09-14 18:35:22,514 Stage: Train 0.5 | Epoch: 66 | Iter: 201600 | Total Loss: 0.003451 | Recon Loss: 0.002850 | Commit Loss: 0.001203 | Perplexity: 671.111522
2025-09-14 18:35:30,602 Stage: Train 0.5 | Epoch: 66 | Iter: 201800 | Total Loss: 0.003410 | Recon Loss: 0.002817 | Commit Loss: 0.001186 | Perplexity: 669.293263
2025-09-14 18:35:38,732 Stage: Train 0.5 | Epoch: 66 | Iter: 202000 | Total Loss: 0.003378 | Recon Loss: 0.002784 | Commit Loss: 0.001188 | Perplexity: 669.369571
2025-09-14 18:35:46,855 Stage: Train 0.5 | Epoch: 66 | Iter: 202200 | Total Loss: 0.003444 | Recon Loss: 0.002850 | Commit Loss: 0.001189 | Perplexity: 670.180960
2025-09-14 18:35:55,033 Stage: Train 0.5 | Epoch: 66 | Iter: 202400 | Total Loss: 0.003372 | Recon Loss: 0.002773 | Commit Loss: 0.001197 | Perplexity: 672.190618
2025-09-14 18:36:03,045 Stage: Train 0.5 | Epoch: 66 | Iter: 202600 | Total Loss: 0.003371 | Recon Loss: 0.002775 | Commit Loss: 0.001192 | Perplexity: 670.403426
2025-09-14 18:36:10,940 Stage: Train 0.5 | Epoch: 66 | Iter: 202800 | Total Loss: 0.003371 | Recon Loss: 0.002773 | Commit Loss: 0.001195 | Perplexity: 670.989368
2025-09-14 18:36:18,809 Stage: Train 0.5 | Epoch: 66 | Iter: 203000 | Total Loss: 0.003372 | Recon Loss: 0.002770 | Commit Loss: 0.001204 | Perplexity: 672.425214
2025-09-14 18:36:26,702 Stage: Train 0.5 | Epoch: 66 | Iter: 203200 | Total Loss: 0.003375 | Recon Loss: 0.002777 | Commit Loss: 0.001195 | Perplexity: 669.947521
2025-09-14 18:36:34,558 Stage: Train 0.5 | Epoch: 66 | Iter: 203400 | Total Loss: 0.003470 | Recon Loss: 0.002871 | Commit Loss: 0.001196 | Perplexity: 671.694789
Trainning Epoch:  41%|████      | 67/165 [2:13:51<3:13:56, 118.74s/it]2025-09-14 18:36:42,453 Stage: Train 0.5 | Epoch: 67 | Iter: 203600 | Total Loss: 0.003340 | Recon Loss: 0.002738 | Commit Loss: 0.001204 | Perplexity: 674.295215
2025-09-14 18:36:50,361 Stage: Train 0.5 | Epoch: 67 | Iter: 203800 | Total Loss: 0.003375 | Recon Loss: 0.002774 | Commit Loss: 0.001201 | Perplexity: 674.656904
2025-09-14 18:36:58,468 Stage: Train 0.5 | Epoch: 67 | Iter: 204000 | Total Loss: 0.003376 | Recon Loss: 0.002783 | Commit Loss: 0.001186 | Perplexity: 671.152820
2025-09-14 18:37:06,599 Stage: Train 0.5 | Epoch: 67 | Iter: 204200 | Total Loss: 0.003396 | Recon Loss: 0.002802 | Commit Loss: 0.001188 | Perplexity: 670.756487
2025-09-14 18:37:14,713 Stage: Train 0.5 | Epoch: 67 | Iter: 204400 | Total Loss: 0.003367 | Recon Loss: 0.002778 | Commit Loss: 0.001178 | Perplexity: 669.547641
2025-09-14 18:37:22,553 Stage: Train 0.5 | Epoch: 67 | Iter: 204600 | Total Loss: 0.003357 | Recon Loss: 0.002758 | Commit Loss: 0.001196 | Perplexity: 670.855822
2025-09-14 18:37:30,503 Stage: Train 0.5 | Epoch: 67 | Iter: 204800 | Total Loss: 0.003418 | Recon Loss: 0.002825 | Commit Loss: 0.001187 | Perplexity: 670.574494
2025-09-14 18:37:38,466 Stage: Train 0.5 | Epoch: 67 | Iter: 205000 | Total Loss: 0.003351 | Recon Loss: 0.002756 | Commit Loss: 0.001190 | Perplexity: 672.248352
2025-09-14 18:37:46,270 Stage: Train 0.5 | Epoch: 67 | Iter: 205200 | Total Loss: 0.003368 | Recon Loss: 0.002777 | Commit Loss: 0.001183 | Perplexity: 670.774481
2025-09-14 18:37:54,160 Stage: Train 0.5 | Epoch: 67 | Iter: 205400 | Total Loss: 0.003339 | Recon Loss: 0.002747 | Commit Loss: 0.001185 | Perplexity: 669.785071
2025-09-14 18:38:02,312 Stage: Train 0.5 | Epoch: 67 | Iter: 205600 | Total Loss: 0.003371 | Recon Loss: 0.002779 | Commit Loss: 0.001184 | Perplexity: 671.156476
2025-09-14 18:38:10,309 Stage: Train 0.5 | Epoch: 67 | Iter: 205800 | Total Loss: 0.003393 | Recon Loss: 0.002797 | Commit Loss: 0.001192 | Perplexity: 674.475548
2025-09-14 18:38:18,106 Stage: Train 0.5 | Epoch: 67 | Iter: 206000 | Total Loss: 0.003333 | Recon Loss: 0.002737 | Commit Loss: 0.001192 | Perplexity: 672.974874
2025-09-14 18:38:25,952 Stage: Train 0.5 | Epoch: 67 | Iter: 206200 | Total Loss: 0.003357 | Recon Loss: 0.002760 | Commit Loss: 0.001193 | Perplexity: 673.888113
2025-09-14 18:38:33,929 Stage: Train 0.5 | Epoch: 67 | Iter: 206400 | Total Loss: 0.003336 | Recon Loss: 0.002752 | Commit Loss: 0.001169 | Perplexity: 668.118163
Trainning Epoch:  41%|████      | 68/165 [2:15:51<3:12:59, 119.37s/it]2025-09-14 18:38:41,810 Stage: Train 0.5 | Epoch: 68 | Iter: 206600 | Total Loss: 0.003335 | Recon Loss: 0.002740 | Commit Loss: 0.001190 | Perplexity: 674.458445
2025-09-14 18:38:49,686 Stage: Train 0.5 | Epoch: 68 | Iter: 206800 | Total Loss: 0.003330 | Recon Loss: 0.002742 | Commit Loss: 0.001176 | Perplexity: 671.521410
2025-09-14 18:38:57,688 Stage: Train 0.5 | Epoch: 68 | Iter: 207000 | Total Loss: 0.003357 | Recon Loss: 0.002767 | Commit Loss: 0.001180 | Perplexity: 672.749493
2025-09-14 18:39:05,832 Stage: Train 0.5 | Epoch: 68 | Iter: 207200 | Total Loss: 0.003360 | Recon Loss: 0.002773 | Commit Loss: 0.001174 | Perplexity: 672.948910
2025-09-14 18:39:13,986 Stage: Train 0.5 | Epoch: 68 | Iter: 207400 | Total Loss: 0.003303 | Recon Loss: 0.002721 | Commit Loss: 0.001164 | Perplexity: 672.004760
2025-09-14 18:39:22,142 Stage: Train 0.5 | Epoch: 68 | Iter: 207600 | Total Loss: 0.003375 | Recon Loss: 0.002785 | Commit Loss: 0.001181 | Perplexity: 673.005618
2025-09-14 18:39:30,082 Stage: Train 0.5 | Epoch: 68 | Iter: 207800 | Total Loss: 0.003383 | Recon Loss: 0.002793 | Commit Loss: 0.001180 | Perplexity: 677.246297
2025-09-14 18:39:37,982 Stage: Train 0.5 | Epoch: 68 | Iter: 208000 | Total Loss: 0.003330 | Recon Loss: 0.002740 | Commit Loss: 0.001180 | Perplexity: 674.876265
2025-09-14 18:39:45,988 Stage: Train 0.5 | Epoch: 68 | Iter: 208200 | Total Loss: 0.003340 | Recon Loss: 0.002744 | Commit Loss: 0.001191 | Perplexity: 676.714934
2025-09-14 18:39:54,147 Stage: Train 0.5 | Epoch: 68 | Iter: 208400 | Total Loss: 0.003358 | Recon Loss: 0.002769 | Commit Loss: 0.001178 | Perplexity: 676.038784
2025-09-14 18:40:02,293 Stage: Train 0.5 | Epoch: 68 | Iter: 208600 | Total Loss: 0.003299 | Recon Loss: 0.002710 | Commit Loss: 0.001178 | Perplexity: 674.896590
2025-09-14 18:40:10,465 Stage: Train 0.5 | Epoch: 68 | Iter: 208800 | Total Loss: 0.003304 | Recon Loss: 0.002711 | Commit Loss: 0.001188 | Perplexity: 675.455468
2025-09-14 18:40:18,608 Stage: Train 0.5 | Epoch: 68 | Iter: 209000 | Total Loss: 0.003414 | Recon Loss: 0.002822 | Commit Loss: 0.001185 | Perplexity: 672.097604
2025-09-14 18:40:26,738 Stage: Train 0.5 | Epoch: 68 | Iter: 209200 | Total Loss: 0.003351 | Recon Loss: 0.002755 | Commit Loss: 0.001192 | Perplexity: 672.964739
2025-09-14 18:40:34,634 Stage: Train 0.5 | Epoch: 68 | Iter: 209400 | Total Loss: 0.003351 | Recon Loss: 0.002757 | Commit Loss: 0.001187 | Perplexity: 674.220523
2025-09-14 18:40:42,482 Stage: Train 0.5 | Epoch: 68 | Iter: 209600 | Total Loss: 0.003353 | Recon Loss: 0.002764 | Commit Loss: 0.001178 | Perplexity: 672.616215
Trainning Epoch:  42%|████▏     | 69/165 [2:17:54<3:12:20, 120.21s/it]2025-09-14 18:40:50,297 Stage: Train 0.5 | Epoch: 69 | Iter: 209800 | Total Loss: 0.003331 | Recon Loss: 0.002754 | Commit Loss: 0.001153 | Perplexity: 667.811554
2025-09-14 18:40:58,049 Stage: Train 0.5 | Epoch: 69 | Iter: 210000 | Total Loss: 0.003325 | Recon Loss: 0.002738 | Commit Loss: 0.001175 | Perplexity: 673.658398
2025-09-14 18:41:05,678 Stage: Train 0.5 | Epoch: 69 | Iter: 210200 | Total Loss: 0.003345 | Recon Loss: 0.002746 | Commit Loss: 0.001197 | Perplexity: 674.706301
2025-09-14 18:41:13,609 Stage: Train 0.5 | Epoch: 69 | Iter: 210400 | Total Loss: 0.003341 | Recon Loss: 0.002760 | Commit Loss: 0.001163 | Perplexity: 672.481823
2025-09-14 18:41:21,393 Stage: Train 0.5 | Epoch: 69 | Iter: 210600 | Total Loss: 0.003318 | Recon Loss: 0.002736 | Commit Loss: 0.001164 | Perplexity: 671.451626
2025-09-14 18:41:29,131 Stage: Train 0.5 | Epoch: 69 | Iter: 210800 | Total Loss: 0.003312 | Recon Loss: 0.002720 | Commit Loss: 0.001185 | Perplexity: 672.796918
2025-09-14 18:41:36,837 Stage: Train 0.5 | Epoch: 69 | Iter: 211000 | Total Loss: 0.003324 | Recon Loss: 0.002740 | Commit Loss: 0.001169 | Perplexity: 672.721556
2025-09-14 18:41:44,558 Stage: Train 0.5 | Epoch: 69 | Iter: 211200 | Total Loss: 0.003307 | Recon Loss: 0.002724 | Commit Loss: 0.001165 | Perplexity: 673.940993
2025-09-14 18:41:52,290 Stage: Train 0.5 | Epoch: 69 | Iter: 211400 | Total Loss: 0.003385 | Recon Loss: 0.002797 | Commit Loss: 0.001176 | Perplexity: 675.820377
2025-09-14 18:41:59,966 Stage: Train 0.5 | Epoch: 69 | Iter: 211600 | Total Loss: 0.003387 | Recon Loss: 0.002797 | Commit Loss: 0.001179 | Perplexity: 671.780752
2025-09-14 18:42:07,662 Stage: Train 0.5 | Epoch: 69 | Iter: 211800 | Total Loss: 0.003346 | Recon Loss: 0.002759 | Commit Loss: 0.001173 | Perplexity: 672.532002
2025-09-14 18:42:15,409 Stage: Train 0.5 | Epoch: 69 | Iter: 212000 | Total Loss: 0.003329 | Recon Loss: 0.002744 | Commit Loss: 0.001171 | Perplexity: 676.037769
2025-09-14 18:42:23,134 Stage: Train 0.5 | Epoch: 69 | Iter: 212200 | Total Loss: 0.003340 | Recon Loss: 0.002761 | Commit Loss: 0.001158 | Perplexity: 669.193869
2025-09-14 18:42:30,871 Stage: Train 0.5 | Epoch: 69 | Iter: 212400 | Total Loss: 0.003349 | Recon Loss: 0.002760 | Commit Loss: 0.001179 | Perplexity: 673.535159
2025-09-14 18:42:38,593 Stage: Train 0.5 | Epoch: 69 | Iter: 212600 | Total Loss: 0.003319 | Recon Loss: 0.002736 | Commit Loss: 0.001165 | Perplexity: 672.752793
Trainning Epoch:  42%|████▏     | 70/165 [2:19:51<3:09:05, 119.43s/it]2025-09-14 18:42:46,377 Stage: Train 0.5 | Epoch: 70 | Iter: 212800 | Total Loss: 0.003347 | Recon Loss: 0.002769 | Commit Loss: 0.001157 | Perplexity: 669.851633
2025-09-14 18:42:54,125 Stage: Train 0.5 | Epoch: 70 | Iter: 213000 | Total Loss: 0.003325 | Recon Loss: 0.002740 | Commit Loss: 0.001170 | Perplexity: 671.689897
2025-09-14 18:43:01,870 Stage: Train 0.5 | Epoch: 70 | Iter: 213200 | Total Loss: 0.003262 | Recon Loss: 0.002681 | Commit Loss: 0.001162 | Perplexity: 674.242500
2025-09-14 18:43:09,597 Stage: Train 0.5 | Epoch: 70 | Iter: 213400 | Total Loss: 0.003305 | Recon Loss: 0.002723 | Commit Loss: 0.001163 | Perplexity: 674.228189
2025-09-14 18:43:17,353 Stage: Train 0.5 | Epoch: 70 | Iter: 213600 | Total Loss: 0.003340 | Recon Loss: 0.002755 | Commit Loss: 0.001169 | Perplexity: 672.877227
2025-09-14 18:43:25,145 Stage: Train 0.5 | Epoch: 70 | Iter: 213800 | Total Loss: 0.003326 | Recon Loss: 0.002745 | Commit Loss: 0.001162 | Perplexity: 671.952935
2025-09-14 18:43:32,957 Stage: Train 0.5 | Epoch: 70 | Iter: 214000 | Total Loss: 0.003309 | Recon Loss: 0.002723 | Commit Loss: 0.001171 | Perplexity: 674.810981
2025-09-14 18:43:40,719 Stage: Train 0.5 | Epoch: 70 | Iter: 214200 | Total Loss: 0.003334 | Recon Loss: 0.002751 | Commit Loss: 0.001166 | Perplexity: 674.486674
2025-09-14 18:43:48,470 Stage: Train 0.5 | Epoch: 70 | Iter: 214400 | Total Loss: 0.003364 | Recon Loss: 0.002776 | Commit Loss: 0.001177 | Perplexity: 674.278559
2025-09-14 18:43:56,186 Stage: Train 0.5 | Epoch: 70 | Iter: 214600 | Total Loss: 0.003325 | Recon Loss: 0.002742 | Commit Loss: 0.001167 | Perplexity: 676.104885
2025-09-14 18:44:03,893 Stage: Train 0.5 | Epoch: 70 | Iter: 214800 | Total Loss: 0.003306 | Recon Loss: 0.002725 | Commit Loss: 0.001163 | Perplexity: 672.021255
2025-09-14 18:44:11,617 Stage: Train 0.5 | Epoch: 70 | Iter: 215000 | Total Loss: 0.003351 | Recon Loss: 0.002773 | Commit Loss: 0.001158 | Perplexity: 673.003693
2025-09-14 18:44:19,317 Stage: Train 0.5 | Epoch: 70 | Iter: 215200 | Total Loss: 0.003336 | Recon Loss: 0.002751 | Commit Loss: 0.001169 | Perplexity: 673.903289
2025-09-14 18:44:27,022 Stage: Train 0.5 | Epoch: 70 | Iter: 215400 | Total Loss: 0.003325 | Recon Loss: 0.002738 | Commit Loss: 0.001173 | Perplexity: 675.376000
2025-09-14 18:44:34,733 Stage: Train 0.5 | Epoch: 70 | Iter: 215600 | Total Loss: 0.003282 | Recon Loss: 0.002700 | Commit Loss: 0.001164 | Perplexity: 673.533960
Trainning Epoch:  43%|████▎     | 71/165 [2:21:49<3:06:13, 118.86s/it]2025-09-14 18:44:42,417 Stage: Train 0.5 | Epoch: 71 | Iter: 215800 | Total Loss: 0.003333 | Recon Loss: 0.002755 | Commit Loss: 0.001156 | Perplexity: 671.974518
2025-09-14 18:44:50,121 Stage: Train 0.5 | Epoch: 71 | Iter: 216000 | Total Loss: 0.003283 | Recon Loss: 0.002704 | Commit Loss: 0.001159 | Perplexity: 674.913504
2025-09-14 18:44:57,834 Stage: Train 0.5 | Epoch: 71 | Iter: 216200 | Total Loss: 0.003305 | Recon Loss: 0.002729 | Commit Loss: 0.001151 | Perplexity: 672.262586
2025-09-14 18:45:05,566 Stage: Train 0.5 | Epoch: 71 | Iter: 216400 | Total Loss: 0.003313 | Recon Loss: 0.002736 | Commit Loss: 0.001154 | Perplexity: 672.119997
2025-09-14 18:45:13,294 Stage: Train 0.5 | Epoch: 71 | Iter: 216600 | Total Loss: 0.003310 | Recon Loss: 0.002724 | Commit Loss: 0.001172 | Perplexity: 675.399777
2025-09-14 18:45:20,969 Stage: Train 0.5 | Epoch: 71 | Iter: 216800 | Total Loss: 0.003296 | Recon Loss: 0.002709 | Commit Loss: 0.001172 | Perplexity: 675.120065
2025-09-14 18:45:28,685 Stage: Train 0.5 | Epoch: 71 | Iter: 217000 | Total Loss: 0.003303 | Recon Loss: 0.002721 | Commit Loss: 0.001163 | Perplexity: 674.069443
2025-09-14 18:45:36,423 Stage: Train 0.5 | Epoch: 71 | Iter: 217200 | Total Loss: 0.003359 | Recon Loss: 0.002776 | Commit Loss: 0.001166 | Perplexity: 672.123396
2025-09-14 18:45:44,154 Stage: Train 0.5 | Epoch: 71 | Iter: 217400 | Total Loss: 0.003313 | Recon Loss: 0.002732 | Commit Loss: 0.001161 | Perplexity: 672.514758
2025-09-14 18:45:51,897 Stage: Train 0.5 | Epoch: 71 | Iter: 217600 | Total Loss: 0.003294 | Recon Loss: 0.002717 | Commit Loss: 0.001153 | Perplexity: 673.615366
2025-09-14 18:45:59,630 Stage: Train 0.5 | Epoch: 71 | Iter: 217800 | Total Loss: 0.003284 | Recon Loss: 0.002709 | Commit Loss: 0.001149 | Perplexity: 672.969091
2025-09-14 18:46:07,370 Stage: Train 0.5 | Epoch: 71 | Iter: 218000 | Total Loss: 0.003354 | Recon Loss: 0.002776 | Commit Loss: 0.001156 | Perplexity: 673.674857
2025-09-14 18:46:15,135 Stage: Train 0.5 | Epoch: 71 | Iter: 218200 | Total Loss: 0.003293 | Recon Loss: 0.002713 | Commit Loss: 0.001160 | Perplexity: 673.886609
2025-09-14 18:46:23,209 Stage: Train 0.5 | Epoch: 71 | Iter: 218400 | Total Loss: 0.003269 | Recon Loss: 0.002692 | Commit Loss: 0.001154 | Perplexity: 672.384286
2025-09-14 18:46:30,965 Stage: Train 0.5 | Epoch: 71 | Iter: 218600 | Total Loss: 0.003370 | Recon Loss: 0.002787 | Commit Loss: 0.001165 | Perplexity: 672.613322
Trainning Epoch:  44%|████▎     | 72/165 [2:23:46<3:03:43, 118.54s/it]2025-09-14 18:46:38,743 Stage: Train 0.5 | Epoch: 72 | Iter: 218800 | Total Loss: 0.003280 | Recon Loss: 0.002703 | Commit Loss: 0.001154 | Perplexity: 671.030491
2025-09-14 18:46:46,466 Stage: Train 0.5 | Epoch: 72 | Iter: 219000 | Total Loss: 0.003331 | Recon Loss: 0.002753 | Commit Loss: 0.001157 | Perplexity: 674.069793
2025-09-14 18:46:54,290 Stage: Train 0.5 | Epoch: 72 | Iter: 219200 | Total Loss: 0.003301 | Recon Loss: 0.002726 | Commit Loss: 0.001151 | Perplexity: 674.070079
2025-09-14 18:47:02,061 Stage: Train 0.5 | Epoch: 72 | Iter: 219400 | Total Loss: 0.003330 | Recon Loss: 0.002745 | Commit Loss: 0.001172 | Perplexity: 675.022801
2025-09-14 18:47:09,846 Stage: Train 0.5 | Epoch: 72 | Iter: 219600 | Total Loss: 0.003277 | Recon Loss: 0.002707 | Commit Loss: 0.001141 | Perplexity: 673.779677
2025-09-14 18:47:17,626 Stage: Train 0.5 | Epoch: 72 | Iter: 219800 | Total Loss: 0.003325 | Recon Loss: 0.002747 | Commit Loss: 0.001157 | Perplexity: 675.930719
2025-09-14 18:47:25,361 Stage: Train 0.5 | Epoch: 72 | Iter: 220000 | Total Loss: 0.003341 | Recon Loss: 0.002765 | Commit Loss: 0.001154 | Perplexity: 675.681832
2025-09-14 18:47:25,361 Saving model at iteration 220000
2025-09-14 18:47:25,628 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_73_step_220000
2025-09-14 18:47:25,762 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_73_step_220000/pytorch_model.bin
2025-09-14 18:47:26,002 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_73_step_220000/optimizer.bin
2025-09-14 18:47:26,002 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_73_step_220000/scheduler.bin
2025-09-14 18:47:26,003 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_73_step_220000/random_states_0.pkl
2025-09-14 18:47:33,681 Stage: Train 0.5 | Epoch: 72 | Iter: 220200 | Total Loss: 0.003305 | Recon Loss: 0.002723 | Commit Loss: 0.001162 | Perplexity: 674.988170
2025-09-14 18:47:41,434 Stage: Train 0.5 | Epoch: 72 | Iter: 220400 | Total Loss: 0.003288 | Recon Loss: 0.002713 | Commit Loss: 0.001150 | Perplexity: 675.400753
2025-09-14 18:47:49,155 Stage: Train 0.5 | Epoch: 72 | Iter: 220600 | Total Loss: 0.003277 | Recon Loss: 0.002702 | Commit Loss: 0.001150 | Perplexity: 674.890286
2025-09-14 18:47:57,170 Stage: Train 0.5 | Epoch: 72 | Iter: 220800 | Total Loss: 0.003268 | Recon Loss: 0.002692 | Commit Loss: 0.001151 | Perplexity: 672.163593
2025-09-14 18:48:04,965 Stage: Train 0.5 | Epoch: 72 | Iter: 221000 | Total Loss: 0.003237 | Recon Loss: 0.002667 | Commit Loss: 0.001141 | Perplexity: 673.106532
2025-09-14 18:48:12,770 Stage: Train 0.5 | Epoch: 72 | Iter: 221200 | Total Loss: 0.003254 | Recon Loss: 0.002677 | Commit Loss: 0.001154 | Perplexity: 672.734458
2025-09-14 18:48:20,530 Stage: Train 0.5 | Epoch: 72 | Iter: 221400 | Total Loss: 0.003291 | Recon Loss: 0.002715 | Commit Loss: 0.001153 | Perplexity: 673.385783
2025-09-14 18:48:28,337 Stage: Train 0.5 | Epoch: 72 | Iter: 221600 | Total Loss: 0.003286 | Recon Loss: 0.002706 | Commit Loss: 0.001160 | Perplexity: 673.862718
Trainning Epoch:  44%|████▍     | 73/165 [2:25:45<3:01:54, 118.64s/it]2025-09-14 18:48:36,161 Stage: Train 0.5 | Epoch: 73 | Iter: 221800 | Total Loss: 0.003239 | Recon Loss: 0.002666 | Commit Loss: 0.001146 | Perplexity: 670.032305
2025-09-14 18:48:43,941 Stage: Train 0.5 | Epoch: 73 | Iter: 222000 | Total Loss: 0.003270 | Recon Loss: 0.002692 | Commit Loss: 0.001157 | Perplexity: 674.739239
2025-09-14 18:48:51,737 Stage: Train 0.5 | Epoch: 73 | Iter: 222200 | Total Loss: 0.003291 | Recon Loss: 0.002716 | Commit Loss: 0.001150 | Perplexity: 670.995623
2025-09-14 18:48:59,590 Stage: Train 0.5 | Epoch: 73 | Iter: 222400 | Total Loss: 0.003263 | Recon Loss: 0.002691 | Commit Loss: 0.001143 | Perplexity: 673.939932
2025-09-14 18:49:07,438 Stage: Train 0.5 | Epoch: 73 | Iter: 222600 | Total Loss: 0.003284 | Recon Loss: 0.002710 | Commit Loss: 0.001148 | Perplexity: 674.399166
2025-09-14 18:49:15,233 Stage: Train 0.5 | Epoch: 73 | Iter: 222800 | Total Loss: 0.003278 | Recon Loss: 0.002709 | Commit Loss: 0.001136 | Perplexity: 674.577109
2025-09-14 18:49:23,063 Stage: Train 0.5 | Epoch: 73 | Iter: 223000 | Total Loss: 0.003303 | Recon Loss: 0.002726 | Commit Loss: 0.001155 | Perplexity: 678.254468
2025-09-14 18:49:31,168 Stage: Train 0.5 | Epoch: 73 | Iter: 223200 | Total Loss: 0.003290 | Recon Loss: 0.002712 | Commit Loss: 0.001154 | Perplexity: 676.916747
2025-09-14 18:49:39,137 Stage: Train 0.5 | Epoch: 73 | Iter: 223400 | Total Loss: 0.003297 | Recon Loss: 0.002725 | Commit Loss: 0.001144 | Perplexity: 675.135052
2025-09-14 18:49:47,017 Stage: Train 0.5 | Epoch: 73 | Iter: 223600 | Total Loss: 0.003240 | Recon Loss: 0.002666 | Commit Loss: 0.001148 | Perplexity: 676.210284
2025-09-14 18:49:54,877 Stage: Train 0.5 | Epoch: 73 | Iter: 223800 | Total Loss: 0.003232 | Recon Loss: 0.002662 | Commit Loss: 0.001141 | Perplexity: 672.472141
2025-09-14 18:50:02,765 Stage: Train 0.5 | Epoch: 73 | Iter: 224000 | Total Loss: 0.003288 | Recon Loss: 0.002711 | Commit Loss: 0.001153 | Perplexity: 675.520747
2025-09-14 18:50:10,642 Stage: Train 0.5 | Epoch: 73 | Iter: 224200 | Total Loss: 0.003287 | Recon Loss: 0.002715 | Commit Loss: 0.001144 | Perplexity: 675.631046
2025-09-14 18:50:18,578 Stage: Train 0.5 | Epoch: 73 | Iter: 224400 | Total Loss: 0.003262 | Recon Loss: 0.002687 | Commit Loss: 0.001150 | Perplexity: 677.023371
2025-09-14 18:50:26,477 Stage: Train 0.5 | Epoch: 73 | Iter: 224600 | Total Loss: 0.003227 | Recon Loss: 0.002661 | Commit Loss: 0.001131 | Perplexity: 673.219371
2025-09-14 18:50:34,341 Stage: Train 0.5 | Epoch: 73 | Iter: 224800 | Total Loss: 0.003307 | Recon Loss: 0.002729 | Commit Loss: 0.001155 | Perplexity: 676.267915
Trainning Epoch:  45%|████▍     | 74/165 [2:27:45<3:00:24, 118.95s/it]2025-09-14 18:50:42,226 Stage: Train 0.5 | Epoch: 74 | Iter: 225000 | Total Loss: 0.003231 | Recon Loss: 0.002662 | Commit Loss: 0.001138 | Perplexity: 674.007294
2025-09-14 18:50:50,085 Stage: Train 0.5 | Epoch: 74 | Iter: 225200 | Total Loss: 0.003276 | Recon Loss: 0.002704 | Commit Loss: 0.001144 | Perplexity: 678.021015
2025-09-14 18:50:57,931 Stage: Train 0.5 | Epoch: 74 | Iter: 225400 | Total Loss: 0.003256 | Recon Loss: 0.002687 | Commit Loss: 0.001140 | Perplexity: 674.622943
2025-09-14 18:51:05,790 Stage: Train 0.5 | Epoch: 74 | Iter: 225600 | Total Loss: 0.003267 | Recon Loss: 0.002693 | Commit Loss: 0.001147 | Perplexity: 676.275242
2025-09-14 18:51:13,642 Stage: Train 0.5 | Epoch: 74 | Iter: 225800 | Total Loss: 0.003234 | Recon Loss: 0.002666 | Commit Loss: 0.001137 | Perplexity: 675.497111
2025-09-14 18:51:21,493 Stage: Train 0.5 | Epoch: 74 | Iter: 226000 | Total Loss: 0.003250 | Recon Loss: 0.002679 | Commit Loss: 0.001143 | Perplexity: 677.340761
2025-09-14 18:51:29,356 Stage: Train 0.5 | Epoch: 74 | Iter: 226200 | Total Loss: 0.003245 | Recon Loss: 0.002678 | Commit Loss: 0.001134 | Perplexity: 678.375886
2025-09-14 18:51:37,134 Stage: Train 0.5 | Epoch: 74 | Iter: 226400 | Total Loss: 0.003288 | Recon Loss: 0.002713 | Commit Loss: 0.001150 | Perplexity: 677.471465
2025-09-14 18:51:44,870 Stage: Train 0.5 | Epoch: 74 | Iter: 226600 | Total Loss: 0.003282 | Recon Loss: 0.002720 | Commit Loss: 0.001125 | Perplexity: 674.809097
2025-09-14 18:51:52,593 Stage: Train 0.5 | Epoch: 74 | Iter: 226800 | Total Loss: 0.003227 | Recon Loss: 0.002657 | Commit Loss: 0.001140 | Perplexity: 677.332073
2025-09-14 18:52:00,450 Stage: Train 0.5 | Epoch: 74 | Iter: 227000 | Total Loss: 0.003264 | Recon Loss: 0.002688 | Commit Loss: 0.001152 | Perplexity: 678.443094
2025-09-14 18:52:08,616 Stage: Train 0.5 | Epoch: 74 | Iter: 227200 | Total Loss: 0.003224 | Recon Loss: 0.002653 | Commit Loss: 0.001141 | Perplexity: 674.752157
2025-09-14 18:52:16,726 Stage: Train 0.5 | Epoch: 74 | Iter: 227400 | Total Loss: 0.003308 | Recon Loss: 0.002744 | Commit Loss: 0.001129 | Perplexity: 673.059679
2025-09-14 18:52:24,561 Stage: Train 0.5 | Epoch: 74 | Iter: 227600 | Total Loss: 0.003265 | Recon Loss: 0.002697 | Commit Loss: 0.001136 | Perplexity: 674.922232
2025-09-14 18:52:32,278 Stage: Train 0.5 | Epoch: 74 | Iter: 227800 | Total Loss: 0.003226 | Recon Loss: 0.002654 | Commit Loss: 0.001143 | Perplexity: 675.838038
Trainning Epoch:  45%|████▌     | 75/165 [2:29:44<2:58:37, 119.08s/it]2025-09-14 18:52:39,973 Stage: Train 0.5 | Epoch: 75 | Iter: 228000 | Total Loss: 0.003284 | Recon Loss: 0.002716 | Commit Loss: 0.001136 | Perplexity: 673.263527
2025-09-14 18:52:47,697 Stage: Train 0.5 | Epoch: 75 | Iter: 228200 | Total Loss: 0.003211 | Recon Loss: 0.002646 | Commit Loss: 0.001129 | Perplexity: 673.531895
2025-09-14 18:52:55,433 Stage: Train 0.5 | Epoch: 75 | Iter: 228400 | Total Loss: 0.003228 | Recon Loss: 0.002663 | Commit Loss: 0.001129 | Perplexity: 676.667909
2025-09-14 18:53:03,189 Stage: Train 0.5 | Epoch: 75 | Iter: 228600 | Total Loss: 0.003294 | Recon Loss: 0.002728 | Commit Loss: 0.001132 | Perplexity: 675.109545
2025-09-14 18:53:10,905 Stage: Train 0.5 | Epoch: 75 | Iter: 228800 | Total Loss: 0.003244 | Recon Loss: 0.002680 | Commit Loss: 0.001130 | Perplexity: 675.737396
2025-09-14 18:53:18,649 Stage: Train 0.5 | Epoch: 75 | Iter: 229000 | Total Loss: 0.003276 | Recon Loss: 0.002708 | Commit Loss: 0.001136 | Perplexity: 679.242830
2025-09-14 18:53:26,346 Stage: Train 0.5 | Epoch: 75 | Iter: 229200 | Total Loss: 0.003233 | Recon Loss: 0.002670 | Commit Loss: 0.001127 | Perplexity: 672.679413
2025-09-14 18:53:34,086 Stage: Train 0.5 | Epoch: 75 | Iter: 229400 | Total Loss: 0.003243 | Recon Loss: 0.002674 | Commit Loss: 0.001138 | Perplexity: 679.053532
2025-09-14 18:53:41,838 Stage: Train 0.5 | Epoch: 75 | Iter: 229600 | Total Loss: 0.003263 | Recon Loss: 0.002695 | Commit Loss: 0.001138 | Perplexity: 673.973418
2025-09-14 18:53:49,595 Stage: Train 0.5 | Epoch: 75 | Iter: 229800 | Total Loss: 0.003272 | Recon Loss: 0.002702 | Commit Loss: 0.001141 | Perplexity: 677.843273
2025-09-14 18:53:57,346 Stage: Train 0.5 | Epoch: 75 | Iter: 230000 | Total Loss: 0.003274 | Recon Loss: 0.002713 | Commit Loss: 0.001122 | Perplexity: 674.847088
2025-09-14 18:54:05,026 Stage: Train 0.5 | Epoch: 75 | Iter: 230200 | Total Loss: 0.003235 | Recon Loss: 0.002673 | Commit Loss: 0.001125 | Perplexity: 675.728904
2025-09-14 18:54:12,805 Stage: Train 0.5 | Epoch: 75 | Iter: 230400 | Total Loss: 0.003261 | Recon Loss: 0.002697 | Commit Loss: 0.001130 | Perplexity: 675.300858
2025-09-14 18:54:20,928 Stage: Train 0.5 | Epoch: 75 | Iter: 230600 | Total Loss: 0.003246 | Recon Loss: 0.002675 | Commit Loss: 0.001143 | Perplexity: 677.288088
2025-09-14 18:54:28,665 Stage: Train 0.5 | Epoch: 75 | Iter: 230800 | Total Loss: 0.003288 | Recon Loss: 0.002718 | Commit Loss: 0.001140 | Perplexity: 678.547746
Trainning Epoch:  46%|████▌     | 76/165 [2:31:42<2:56:05, 118.72s/it]2025-09-14 18:54:36,377 Stage: Train 0.5 | Epoch: 76 | Iter: 231000 | Total Loss: 0.003265 | Recon Loss: 0.002702 | Commit Loss: 0.001127 | Perplexity: 675.699137
2025-09-14 18:54:44,117 Stage: Train 0.5 | Epoch: 76 | Iter: 231200 | Total Loss: 0.003203 | Recon Loss: 0.002641 | Commit Loss: 0.001123 | Perplexity: 675.098658
2025-09-14 18:54:51,789 Stage: Train 0.5 | Epoch: 76 | Iter: 231400 | Total Loss: 0.003238 | Recon Loss: 0.002671 | Commit Loss: 0.001133 | Perplexity: 676.836425
2025-09-14 18:54:59,485 Stage: Train 0.5 | Epoch: 76 | Iter: 231600 | Total Loss: 0.003202 | Recon Loss: 0.002635 | Commit Loss: 0.001135 | Perplexity: 676.861418
2025-09-14 18:55:07,309 Stage: Train 0.5 | Epoch: 76 | Iter: 231800 | Total Loss: 0.003225 | Recon Loss: 0.002649 | Commit Loss: 0.001152 | Perplexity: 681.389235
2025-09-14 18:55:15,145 Stage: Train 0.5 | Epoch: 76 | Iter: 232000 | Total Loss: 0.003220 | Recon Loss: 0.002657 | Commit Loss: 0.001127 | Perplexity: 676.545043
2025-09-14 18:55:22,969 Stage: Train 0.5 | Epoch: 76 | Iter: 232200 | Total Loss: 0.003281 | Recon Loss: 0.002718 | Commit Loss: 0.001127 | Perplexity: 677.276026
2025-09-14 18:55:30,986 Stage: Train 0.5 | Epoch: 76 | Iter: 232400 | Total Loss: 0.003254 | Recon Loss: 0.002690 | Commit Loss: 0.001127 | Perplexity: 676.368766
2025-09-14 18:55:38,991 Stage: Train 0.5 | Epoch: 76 | Iter: 232600 | Total Loss: 0.003208 | Recon Loss: 0.002649 | Commit Loss: 0.001118 | Perplexity: 676.368534
2025-09-14 18:55:46,825 Stage: Train 0.5 | Epoch: 76 | Iter: 232800 | Total Loss: 0.003245 | Recon Loss: 0.002678 | Commit Loss: 0.001134 | Perplexity: 678.902450
2025-09-14 18:55:54,666 Stage: Train 0.5 | Epoch: 76 | Iter: 233000 | Total Loss: 0.003219 | Recon Loss: 0.002657 | Commit Loss: 0.001125 | Perplexity: 676.431082
2025-09-14 18:56:02,770 Stage: Train 0.5 | Epoch: 76 | Iter: 233200 | Total Loss: 0.003234 | Recon Loss: 0.002671 | Commit Loss: 0.001126 | Perplexity: 678.364734
2025-09-14 18:56:10,709 Stage: Train 0.5 | Epoch: 76 | Iter: 233400 | Total Loss: 0.003280 | Recon Loss: 0.002716 | Commit Loss: 0.001128 | Perplexity: 674.891728
2025-09-14 18:56:18,561 Stage: Train 0.5 | Epoch: 76 | Iter: 233600 | Total Loss: 0.003241 | Recon Loss: 0.002675 | Commit Loss: 0.001134 | Perplexity: 675.995779
2025-09-14 18:56:26,421 Stage: Train 0.5 | Epoch: 76 | Iter: 233800 | Total Loss: 0.003199 | Recon Loss: 0.002640 | Commit Loss: 0.001117 | Perplexity: 674.694963
Trainning Epoch:  47%|████▋     | 77/165 [2:33:42<2:54:22, 118.89s/it]2025-09-14 18:56:34,281 Stage: Train 0.5 | Epoch: 77 | Iter: 234000 | Total Loss: 0.003271 | Recon Loss: 0.002708 | Commit Loss: 0.001127 | Perplexity: 678.581625
2025-09-14 18:56:42,164 Stage: Train 0.5 | Epoch: 77 | Iter: 234200 | Total Loss: 0.003200 | Recon Loss: 0.002637 | Commit Loss: 0.001126 | Perplexity: 676.680032
2025-09-14 18:56:50,260 Stage: Train 0.5 | Epoch: 77 | Iter: 234400 | Total Loss: 0.003279 | Recon Loss: 0.002714 | Commit Loss: 0.001129 | Perplexity: 682.187691
2025-09-14 18:56:58,293 Stage: Train 0.5 | Epoch: 77 | Iter: 234600 | Total Loss: 0.003180 | Recon Loss: 0.002621 | Commit Loss: 0.001118 | Perplexity: 676.433929
2025-09-14 18:57:06,109 Stage: Train 0.5 | Epoch: 77 | Iter: 234800 | Total Loss: 0.003242 | Recon Loss: 0.002687 | Commit Loss: 0.001110 | Perplexity: 675.918555
2025-09-14 18:57:13,933 Stage: Train 0.5 | Epoch: 77 | Iter: 235000 | Total Loss: 0.003254 | Recon Loss: 0.002696 | Commit Loss: 0.001116 | Perplexity: 678.077877
2025-09-14 18:57:21,689 Stage: Train 0.5 | Epoch: 77 | Iter: 235200 | Total Loss: 0.003157 | Recon Loss: 0.002599 | Commit Loss: 0.001117 | Perplexity: 677.733399
2025-09-14 18:57:29,409 Stage: Train 0.5 | Epoch: 77 | Iter: 235400 | Total Loss: 0.003278 | Recon Loss: 0.002706 | Commit Loss: 0.001144 | Perplexity: 679.235551
2025-09-14 18:57:37,093 Stage: Train 0.5 | Epoch: 77 | Iter: 235600 | Total Loss: 0.003205 | Recon Loss: 0.002647 | Commit Loss: 0.001117 | Perplexity: 678.119615
2025-09-14 18:57:44,798 Stage: Train 0.5 | Epoch: 77 | Iter: 235800 | Total Loss: 0.003262 | Recon Loss: 0.002703 | Commit Loss: 0.001118 | Perplexity: 674.959956
2025-09-14 18:57:52,481 Stage: Train 0.5 | Epoch: 77 | Iter: 236000 | Total Loss: 0.003240 | Recon Loss: 0.002684 | Commit Loss: 0.001112 | Perplexity: 675.187589
2025-09-14 18:58:00,207 Stage: Train 0.5 | Epoch: 77 | Iter: 236200 | Total Loss: 0.003225 | Recon Loss: 0.002661 | Commit Loss: 0.001128 | Perplexity: 680.387516
2025-09-14 18:58:08,023 Stage: Train 0.5 | Epoch: 77 | Iter: 236400 | Total Loss: 0.003203 | Recon Loss: 0.002642 | Commit Loss: 0.001123 | Perplexity: 677.876085
2025-09-14 18:58:15,874 Stage: Train 0.5 | Epoch: 77 | Iter: 236600 | Total Loss: 0.003214 | Recon Loss: 0.002658 | Commit Loss: 0.001113 | Perplexity: 677.177055
2025-09-14 18:58:23,705 Stage: Train 0.5 | Epoch: 77 | Iter: 236800 | Total Loss: 0.003232 | Recon Loss: 0.002676 | Commit Loss: 0.001111 | Perplexity: 678.569543
Trainning Epoch:  47%|████▋     | 78/165 [2:35:40<2:52:19, 118.85s/it]2025-09-14 18:58:31,566 Stage: Train 0.5 | Epoch: 78 | Iter: 237000 | Total Loss: 0.003271 | Recon Loss: 0.002713 | Commit Loss: 0.001115 | Perplexity: 676.225640
2025-09-14 18:58:39,382 Stage: Train 0.5 | Epoch: 78 | Iter: 237200 | Total Loss: 0.003182 | Recon Loss: 0.002630 | Commit Loss: 0.001106 | Perplexity: 677.538494
2025-09-14 18:58:47,203 Stage: Train 0.5 | Epoch: 78 | Iter: 237400 | Total Loss: 0.003235 | Recon Loss: 0.002670 | Commit Loss: 0.001129 | Perplexity: 680.992965
2025-09-14 18:58:55,172 Stage: Train 0.5 | Epoch: 78 | Iter: 237600 | Total Loss: 0.003208 | Recon Loss: 0.002645 | Commit Loss: 0.001124 | Perplexity: 681.041898
2025-09-14 18:59:03,336 Stage: Train 0.5 | Epoch: 78 | Iter: 237800 | Total Loss: 0.003157 | Recon Loss: 0.002603 | Commit Loss: 0.001109 | Perplexity: 678.051370
2025-09-14 18:59:11,488 Stage: Train 0.5 | Epoch: 78 | Iter: 238000 | Total Loss: 0.003203 | Recon Loss: 0.002645 | Commit Loss: 0.001115 | Perplexity: 676.026632
2025-09-14 18:59:19,599 Stage: Train 0.5 | Epoch: 78 | Iter: 238200 | Total Loss: 0.003186 | Recon Loss: 0.002630 | Commit Loss: 0.001111 | Perplexity: 679.371480
2025-09-14 18:59:27,709 Stage: Train 0.5 | Epoch: 78 | Iter: 238400 | Total Loss: 0.003195 | Recon Loss: 0.002639 | Commit Loss: 0.001112 | Perplexity: 679.101456
2025-09-14 18:59:35,846 Stage: Train 0.5 | Epoch: 78 | Iter: 238600 | Total Loss: 0.003241 | Recon Loss: 0.002682 | Commit Loss: 0.001119 | Perplexity: 680.225530
2025-09-14 18:59:43,956 Stage: Train 0.5 | Epoch: 78 | Iter: 238800 | Total Loss: 0.003196 | Recon Loss: 0.002642 | Commit Loss: 0.001108 | Perplexity: 675.368511
2025-09-14 18:59:52,082 Stage: Train 0.5 | Epoch: 78 | Iter: 239000 | Total Loss: 0.003159 | Recon Loss: 0.002604 | Commit Loss: 0.001108 | Perplexity: 676.672486
2025-09-14 19:00:00,176 Stage: Train 0.5 | Epoch: 78 | Iter: 239200 | Total Loss: 0.003219 | Recon Loss: 0.002664 | Commit Loss: 0.001111 | Perplexity: 677.517799
2025-09-14 19:00:08,306 Stage: Train 0.5 | Epoch: 78 | Iter: 239400 | Total Loss: 0.003171 | Recon Loss: 0.002613 | Commit Loss: 0.001115 | Perplexity: 677.800637
2025-09-14 19:00:16,441 Stage: Train 0.5 | Epoch: 78 | Iter: 239600 | Total Loss: 0.003224 | Recon Loss: 0.002671 | Commit Loss: 0.001104 | Perplexity: 674.176164
2025-09-14 19:00:24,258 Stage: Train 0.5 | Epoch: 78 | Iter: 239800 | Total Loss: 0.003263 | Recon Loss: 0.002706 | Commit Loss: 0.001112 | Perplexity: 679.831855
2025-09-14 19:00:32,033 Stage: Train 0.5 | Epoch: 78 | Iter: 240000 | Total Loss: 0.003201 | Recon Loss: 0.002642 | Commit Loss: 0.001117 | Perplexity: 678.259481
2025-09-14 19:00:32,033 Saving model at iteration 240000
2025-09-14 19:00:32,178 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_79_step_240000
2025-09-14 19:00:32,307 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_79_step_240000/pytorch_model.bin
2025-09-14 19:00:32,548 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_79_step_240000/optimizer.bin
2025-09-14 19:00:32,549 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_79_step_240000/scheduler.bin
2025-09-14 19:00:32,549 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_79_step_240000/random_states_0.pkl
Trainning Epoch:  48%|████▊     | 79/165 [2:37:43<2:52:01, 120.02s/it]2025-09-14 19:00:40,529 Stage: Train 0.5 | Epoch: 79 | Iter: 240200 | Total Loss: 0.003176 | Recon Loss: 0.002623 | Commit Loss: 0.001106 | Perplexity: 677.456864
2025-09-14 19:00:48,253 Stage: Train 0.5 | Epoch: 79 | Iter: 240400 | Total Loss: 0.003283 | Recon Loss: 0.002723 | Commit Loss: 0.001119 | Perplexity: 680.100434
2025-09-14 19:00:56,002 Stage: Train 0.5 | Epoch: 79 | Iter: 240600 | Total Loss: 0.003227 | Recon Loss: 0.002670 | Commit Loss: 0.001115 | Perplexity: 679.580643
2025-09-14 19:01:03,737 Stage: Train 0.5 | Epoch: 79 | Iter: 240800 | Total Loss: 0.003150 | Recon Loss: 0.002593 | Commit Loss: 0.001115 | Perplexity: 679.386834
2025-09-14 19:01:11,482 Stage: Train 0.5 | Epoch: 79 | Iter: 241000 | Total Loss: 0.003213 | Recon Loss: 0.002657 | Commit Loss: 0.001111 | Perplexity: 678.731366
2025-09-14 19:01:19,226 Stage: Train 0.5 | Epoch: 79 | Iter: 241200 | Total Loss: 0.003219 | Recon Loss: 0.002662 | Commit Loss: 0.001114 | Perplexity: 680.349647
2025-09-14 19:01:26,921 Stage: Train 0.5 | Epoch: 79 | Iter: 241400 | Total Loss: 0.003152 | Recon Loss: 0.002599 | Commit Loss: 0.001106 | Perplexity: 679.535196
2025-09-14 19:01:34,645 Stage: Train 0.5 | Epoch: 79 | Iter: 241600 | Total Loss: 0.003146 | Recon Loss: 0.002588 | Commit Loss: 0.001117 | Perplexity: 676.568290
2025-09-14 19:01:42,363 Stage: Train 0.5 | Epoch: 79 | Iter: 241800 | Total Loss: 0.003182 | Recon Loss: 0.002625 | Commit Loss: 0.001113 | Perplexity: 679.717760
2025-09-14 19:01:50,090 Stage: Train 0.5 | Epoch: 79 | Iter: 242000 | Total Loss: 0.003245 | Recon Loss: 0.002685 | Commit Loss: 0.001120 | Perplexity: 679.332479
2025-09-14 19:01:57,866 Stage: Train 0.5 | Epoch: 79 | Iter: 242200 | Total Loss: 0.003180 | Recon Loss: 0.002628 | Commit Loss: 0.001103 | Perplexity: 678.233935
2025-09-14 19:02:05,597 Stage: Train 0.5 | Epoch: 79 | Iter: 242400 | Total Loss: 0.003176 | Recon Loss: 0.002628 | Commit Loss: 0.001097 | Perplexity: 676.515934
2025-09-14 19:02:13,434 Stage: Train 0.5 | Epoch: 79 | Iter: 242600 | Total Loss: 0.003204 | Recon Loss: 0.002649 | Commit Loss: 0.001109 | Perplexity: 678.807290
2025-09-14 19:02:21,205 Stage: Train 0.5 | Epoch: 79 | Iter: 242800 | Total Loss: 0.003218 | Recon Loss: 0.002664 | Commit Loss: 0.001109 | Perplexity: 678.021295
2025-09-14 19:02:28,973 Stage: Train 0.5 | Epoch: 79 | Iter: 243000 | Total Loss: 0.003234 | Recon Loss: 0.002677 | Commit Loss: 0.001114 | Perplexity: 681.413859
Trainning Epoch:  48%|████▊     | 80/165 [2:39:41<2:49:01, 119.31s/it]2025-09-14 19:02:36,754 Stage: Train 0.5 | Epoch: 80 | Iter: 243200 | Total Loss: 0.003213 | Recon Loss: 0.002657 | Commit Loss: 0.001112 | Perplexity: 679.510945
2025-09-14 19:02:44,497 Stage: Train 0.5 | Epoch: 80 | Iter: 243400 | Total Loss: 0.003158 | Recon Loss: 0.002609 | Commit Loss: 0.001099 | Perplexity: 676.216563
2025-09-14 19:02:52,262 Stage: Train 0.5 | Epoch: 80 | Iter: 243600 | Total Loss: 0.003168 | Recon Loss: 0.002620 | Commit Loss: 0.001095 | Perplexity: 677.216444
2025-09-14 19:03:00,037 Stage: Train 0.5 | Epoch: 80 | Iter: 243800 | Total Loss: 0.003173 | Recon Loss: 0.002620 | Commit Loss: 0.001105 | Perplexity: 678.419232
2025-09-14 19:03:07,811 Stage: Train 0.5 | Epoch: 80 | Iter: 244000 | Total Loss: 0.003202 | Recon Loss: 0.002650 | Commit Loss: 0.001104 | Perplexity: 681.398932
2025-09-14 19:03:15,553 Stage: Train 0.5 | Epoch: 80 | Iter: 244200 | Total Loss: 0.003164 | Recon Loss: 0.002609 | Commit Loss: 0.001110 | Perplexity: 678.520818
2025-09-14 19:03:23,293 Stage: Train 0.5 | Epoch: 80 | Iter: 244400 | Total Loss: 0.003123 | Recon Loss: 0.002574 | Commit Loss: 0.001098 | Perplexity: 678.147584
2025-09-14 19:03:31,049 Stage: Train 0.5 | Epoch: 80 | Iter: 244600 | Total Loss: 0.003187 | Recon Loss: 0.002626 | Commit Loss: 0.001121 | Perplexity: 681.995231
2025-09-14 19:03:38,862 Stage: Train 0.5 | Epoch: 80 | Iter: 244800 | Total Loss: 0.003195 | Recon Loss: 0.002640 | Commit Loss: 0.001111 | Perplexity: 677.816473
2025-09-14 19:03:46,597 Stage: Train 0.5 | Epoch: 80 | Iter: 245000 | Total Loss: 0.003165 | Recon Loss: 0.002613 | Commit Loss: 0.001104 | Perplexity: 678.635836
2025-09-14 19:03:54,330 Stage: Train 0.5 | Epoch: 80 | Iter: 245200 | Total Loss: 0.003172 | Recon Loss: 0.002623 | Commit Loss: 0.001099 | Perplexity: 676.924616
2025-09-14 19:04:02,090 Stage: Train 0.5 | Epoch: 80 | Iter: 245400 | Total Loss: 0.003169 | Recon Loss: 0.002622 | Commit Loss: 0.001094 | Perplexity: 679.349435
2025-09-14 19:04:09,865 Stage: Train 0.5 | Epoch: 80 | Iter: 245600 | Total Loss: 0.003219 | Recon Loss: 0.002674 | Commit Loss: 0.001091 | Perplexity: 679.371168
2025-09-14 19:04:17,598 Stage: Train 0.5 | Epoch: 80 | Iter: 245800 | Total Loss: 0.003159 | Recon Loss: 0.002608 | Commit Loss: 0.001103 | Perplexity: 679.229791
2025-09-14 19:04:25,325 Stage: Train 0.5 | Epoch: 80 | Iter: 246000 | Total Loss: 0.003166 | Recon Loss: 0.002616 | Commit Loss: 0.001100 | Perplexity: 679.174310
Trainning Epoch:  49%|████▉     | 81/165 [2:41:39<2:46:24, 118.86s/it]2025-09-14 19:04:33,030 Stage: Train 0.5 | Epoch: 81 | Iter: 246200 | Total Loss: 0.003183 | Recon Loss: 0.002632 | Commit Loss: 0.001101 | Perplexity: 675.872092
2025-09-14 19:04:40,702 Stage: Train 0.5 | Epoch: 81 | Iter: 246400 | Total Loss: 0.003143 | Recon Loss: 0.002593 | Commit Loss: 0.001100 | Perplexity: 679.358039
2025-09-14 19:04:48,361 Stage: Train 0.5 | Epoch: 81 | Iter: 246600 | Total Loss: 0.003190 | Recon Loss: 0.002645 | Commit Loss: 0.001092 | Perplexity: 679.497655
2025-09-14 19:04:56,036 Stage: Train 0.5 | Epoch: 81 | Iter: 246800 | Total Loss: 0.003181 | Recon Loss: 0.002633 | Commit Loss: 0.001095 | Perplexity: 677.303047
2025-09-14 19:05:04,215 Stage: Train 0.5 | Epoch: 81 | Iter: 247000 | Total Loss: 0.003161 | Recon Loss: 0.002605 | Commit Loss: 0.001113 | Perplexity: 680.803158
2025-09-14 19:05:12,181 Stage: Train 0.5 | Epoch: 81 | Iter: 247200 | Total Loss: 0.003173 | Recon Loss: 0.002622 | Commit Loss: 0.001103 | Perplexity: 679.722884
2025-09-14 19:05:19,982 Stage: Train 0.5 | Epoch: 81 | Iter: 247400 | Total Loss: 0.003193 | Recon Loss: 0.002643 | Commit Loss: 0.001099 | Perplexity: 680.089049
2025-09-14 19:05:28,063 Stage: Train 0.5 | Epoch: 81 | Iter: 247600 | Total Loss: 0.003157 | Recon Loss: 0.002605 | Commit Loss: 0.001105 | Perplexity: 678.664297
2025-09-14 19:05:36,196 Stage: Train 0.5 | Epoch: 81 | Iter: 247800 | Total Loss: 0.003211 | Recon Loss: 0.002667 | Commit Loss: 0.001087 | Perplexity: 679.148119
2025-09-14 19:05:44,109 Stage: Train 0.5 | Epoch: 81 | Iter: 248000 | Total Loss: 0.003144 | Recon Loss: 0.002596 | Commit Loss: 0.001095 | Perplexity: 678.626004
2025-09-14 19:05:51,964 Stage: Train 0.5 | Epoch: 81 | Iter: 248200 | Total Loss: 0.003174 | Recon Loss: 0.002625 | Commit Loss: 0.001098 | Perplexity: 679.499196
2025-09-14 19:05:59,867 Stage: Train 0.5 | Epoch: 81 | Iter: 248400 | Total Loss: 0.003172 | Recon Loss: 0.002620 | Commit Loss: 0.001104 | Perplexity: 682.470247
2025-09-14 19:06:07,801 Stage: Train 0.5 | Epoch: 81 | Iter: 248600 | Total Loss: 0.003145 | Recon Loss: 0.002594 | Commit Loss: 0.001103 | Perplexity: 680.322169
2025-09-14 19:06:15,669 Stage: Train 0.5 | Epoch: 81 | Iter: 248800 | Total Loss: 0.003169 | Recon Loss: 0.002621 | Commit Loss: 0.001097 | Perplexity: 679.030735
2025-09-14 19:06:23,659 Stage: Train 0.5 | Epoch: 81 | Iter: 249000 | Total Loss: 0.003189 | Recon Loss: 0.002643 | Commit Loss: 0.001093 | Perplexity: 680.536703
Trainning Epoch:  50%|████▉     | 82/165 [2:43:39<2:44:54, 119.21s/it]2025-09-14 19:06:31,774 Stage: Train 0.5 | Epoch: 82 | Iter: 249200 | Total Loss: 0.003143 | Recon Loss: 0.002596 | Commit Loss: 0.001094 | Perplexity: 680.846274
2025-09-14 19:06:39,845 Stage: Train 0.5 | Epoch: 82 | Iter: 249400 | Total Loss: 0.003162 | Recon Loss: 0.002612 | Commit Loss: 0.001099 | Perplexity: 680.536585
2025-09-14 19:06:47,653 Stage: Train 0.5 | Epoch: 82 | Iter: 249600 | Total Loss: 0.003142 | Recon Loss: 0.002597 | Commit Loss: 0.001090 | Perplexity: 678.043977
2025-09-14 19:06:55,466 Stage: Train 0.5 | Epoch: 82 | Iter: 249800 | Total Loss: 0.003161 | Recon Loss: 0.002616 | Commit Loss: 0.001088 | Perplexity: 678.604396
2025-09-14 19:07:03,245 Stage: Train 0.5 | Epoch: 82 | Iter: 250000 | Total Loss: 0.003162 | Recon Loss: 0.002609 | Commit Loss: 0.001106 | Perplexity: 682.252143
2025-09-14 19:07:11,021 Stage: Train 0.5 | Epoch: 82 | Iter: 250200 | Total Loss: 0.003135 | Recon Loss: 0.002583 | Commit Loss: 0.001103 | Perplexity: 679.402486
2025-09-14 19:07:18,837 Stage: Train 0.5 | Epoch: 82 | Iter: 250400 | Total Loss: 0.003151 | Recon Loss: 0.002611 | Commit Loss: 0.001080 | Perplexity: 677.237070
2025-09-14 19:07:26,613 Stage: Train 0.5 | Epoch: 82 | Iter: 250600 | Total Loss: 0.003132 | Recon Loss: 0.002589 | Commit Loss: 0.001087 | Perplexity: 681.586527
2025-09-14 19:07:34,394 Stage: Train 0.5 | Epoch: 82 | Iter: 250800 | Total Loss: 0.003199 | Recon Loss: 0.002650 | Commit Loss: 0.001097 | Perplexity: 680.763226
2025-09-14 19:07:42,202 Stage: Train 0.5 | Epoch: 82 | Iter: 251000 | Total Loss: 0.003125 | Recon Loss: 0.002585 | Commit Loss: 0.001079 | Perplexity: 678.103178
2025-09-14 19:07:50,025 Stage: Train 0.5 | Epoch: 82 | Iter: 251200 | Total Loss: 0.003156 | Recon Loss: 0.002612 | Commit Loss: 0.001087 | Perplexity: 680.314660
2025-09-14 19:07:57,813 Stage: Train 0.5 | Epoch: 82 | Iter: 251400 | Total Loss: 0.003137 | Recon Loss: 0.002588 | Commit Loss: 0.001098 | Perplexity: 681.390988
2025-09-14 19:08:05,750 Stage: Train 0.5 | Epoch: 82 | Iter: 251600 | Total Loss: 0.003127 | Recon Loss: 0.002585 | Commit Loss: 0.001084 | Perplexity: 677.580632
2025-09-14 19:08:13,926 Stage: Train 0.5 | Epoch: 82 | Iter: 251800 | Total Loss: 0.003195 | Recon Loss: 0.002648 | Commit Loss: 0.001094 | Perplexity: 680.149126
2025-09-14 19:08:21,874 Stage: Train 0.5 | Epoch: 82 | Iter: 252000 | Total Loss: 0.003121 | Recon Loss: 0.002577 | Commit Loss: 0.001088 | Perplexity: 677.640677
Trainning Epoch:  50%|█████     | 83/165 [2:45:38<2:43:05, 119.33s/it]2025-09-14 19:08:29,794 Stage: Train 0.5 | Epoch: 83 | Iter: 252200 | Total Loss: 0.003154 | Recon Loss: 0.002611 | Commit Loss: 0.001087 | Perplexity: 677.698236
2025-09-14 19:08:37,706 Stage: Train 0.5 | Epoch: 83 | Iter: 252400 | Total Loss: 0.003103 | Recon Loss: 0.002566 | Commit Loss: 0.001073 | Perplexity: 676.888127
2025-09-14 19:08:45,588 Stage: Train 0.5 | Epoch: 83 | Iter: 252600 | Total Loss: 0.003167 | Recon Loss: 0.002627 | Commit Loss: 0.001081 | Perplexity: 680.087937
2025-09-14 19:08:53,445 Stage: Train 0.5 | Epoch: 83 | Iter: 252800 | Total Loss: 0.003147 | Recon Loss: 0.002604 | Commit Loss: 0.001087 | Perplexity: 681.396084
2025-09-14 19:09:01,310 Stage: Train 0.5 | Epoch: 83 | Iter: 253000 | Total Loss: 0.003155 | Recon Loss: 0.002611 | Commit Loss: 0.001087 | Perplexity: 679.867185
2025-09-14 19:09:09,145 Stage: Train 0.5 | Epoch: 83 | Iter: 253200 | Total Loss: 0.003147 | Recon Loss: 0.002606 | Commit Loss: 0.001083 | Perplexity: 678.627332
2025-09-14 19:09:17,085 Stage: Train 0.5 | Epoch: 83 | Iter: 253400 | Total Loss: 0.003131 | Recon Loss: 0.002594 | Commit Loss: 0.001073 | Perplexity: 679.089347
2025-09-14 19:09:25,119 Stage: Train 0.5 | Epoch: 83 | Iter: 253600 | Total Loss: 0.003096 | Recon Loss: 0.002553 | Commit Loss: 0.001085 | Perplexity: 680.879825
2025-09-14 19:09:32,956 Stage: Train 0.5 | Epoch: 83 | Iter: 253800 | Total Loss: 0.003119 | Recon Loss: 0.002571 | Commit Loss: 0.001096 | Perplexity: 682.247733
2025-09-14 19:09:40,809 Stage: Train 0.5 | Epoch: 83 | Iter: 254000 | Total Loss: 0.003176 | Recon Loss: 0.002628 | Commit Loss: 0.001095 | Perplexity: 682.524055
2025-09-14 19:09:48,671 Stage: Train 0.5 | Epoch: 83 | Iter: 254200 | Total Loss: 0.003127 | Recon Loss: 0.002590 | Commit Loss: 0.001074 | Perplexity: 679.632310
2025-09-14 19:09:56,501 Stage: Train 0.5 | Epoch: 83 | Iter: 254400 | Total Loss: 0.003188 | Recon Loss: 0.002647 | Commit Loss: 0.001082 | Perplexity: 678.451153
2025-09-14 19:10:04,324 Stage: Train 0.5 | Epoch: 83 | Iter: 254600 | Total Loss: 0.003103 | Recon Loss: 0.002562 | Commit Loss: 0.001081 | Perplexity: 679.509088
2025-09-14 19:10:12,093 Stage: Train 0.5 | Epoch: 83 | Iter: 254800 | Total Loss: 0.003138 | Recon Loss: 0.002598 | Commit Loss: 0.001080 | Perplexity: 679.019498
2025-09-14 19:10:19,933 Stage: Train 0.5 | Epoch: 83 | Iter: 255000 | Total Loss: 0.003171 | Recon Loss: 0.002633 | Commit Loss: 0.001076 | Perplexity: 677.522202
Trainning Epoch:  51%|█████     | 84/165 [2:47:38<2:41:08, 119.37s/it]2025-09-14 19:10:27,762 Stage: Train 0.5 | Epoch: 84 | Iter: 255200 | Total Loss: 0.003211 | Recon Loss: 0.002676 | Commit Loss: 0.001070 | Perplexity: 677.606593
2025-09-14 19:10:35,581 Stage: Train 0.5 | Epoch: 84 | Iter: 255400 | Total Loss: 0.003116 | Recon Loss: 0.002579 | Commit Loss: 0.001076 | Perplexity: 680.473972
2025-09-14 19:10:43,441 Stage: Train 0.5 | Epoch: 84 | Iter: 255600 | Total Loss: 0.003089 | Recon Loss: 0.002549 | Commit Loss: 0.001079 | Perplexity: 680.657210
2025-09-14 19:10:51,234 Stage: Train 0.5 | Epoch: 84 | Iter: 255800 | Total Loss: 0.003150 | Recon Loss: 0.002616 | Commit Loss: 0.001066 | Perplexity: 676.017091
2025-09-14 19:10:59,057 Stage: Train 0.5 | Epoch: 84 | Iter: 256000 | Total Loss: 0.003157 | Recon Loss: 0.002620 | Commit Loss: 0.001075 | Perplexity: 679.757535
2025-09-14 19:11:06,893 Stage: Train 0.5 | Epoch: 84 | Iter: 256200 | Total Loss: 0.003142 | Recon Loss: 0.002598 | Commit Loss: 0.001088 | Perplexity: 680.145374
2025-09-14 19:11:14,670 Stage: Train 0.5 | Epoch: 84 | Iter: 256400 | Total Loss: 0.003126 | Recon Loss: 0.002588 | Commit Loss: 0.001076 | Perplexity: 676.480987
2025-09-14 19:11:22,473 Stage: Train 0.5 | Epoch: 84 | Iter: 256600 | Total Loss: 0.003102 | Recon Loss: 0.002565 | Commit Loss: 0.001075 | Perplexity: 681.126448
2025-09-14 19:11:30,265 Stage: Train 0.5 | Epoch: 84 | Iter: 256800 | Total Loss: 0.003170 | Recon Loss: 0.002631 | Commit Loss: 0.001080 | Perplexity: 679.114294
2025-09-14 19:11:38,042 Stage: Train 0.5 | Epoch: 84 | Iter: 257000 | Total Loss: 0.003118 | Recon Loss: 0.002576 | Commit Loss: 0.001083 | Perplexity: 681.079737
2025-09-14 19:11:45,886 Stage: Train 0.5 | Epoch: 84 | Iter: 257200 | Total Loss: 0.003108 | Recon Loss: 0.002570 | Commit Loss: 0.001076 | Perplexity: 678.406839
2025-09-14 19:11:53,689 Stage: Train 0.5 | Epoch: 84 | Iter: 257400 | Total Loss: 0.003145 | Recon Loss: 0.002607 | Commit Loss: 0.001076 | Perplexity: 682.868038
2025-09-14 19:12:01,543 Stage: Train 0.5 | Epoch: 84 | Iter: 257600 | Total Loss: 0.003130 | Recon Loss: 0.002593 | Commit Loss: 0.001074 | Perplexity: 679.456763
2025-09-14 19:12:09,325 Stage: Train 0.5 | Epoch: 84 | Iter: 257800 | Total Loss: 0.003114 | Recon Loss: 0.002573 | Commit Loss: 0.001081 | Perplexity: 679.253699
2025-09-14 19:12:17,038 Stage: Train 0.5 | Epoch: 84 | Iter: 258000 | Total Loss: 0.003090 | Recon Loss: 0.002547 | Commit Loss: 0.001086 | Perplexity: 682.763956
2025-09-14 19:12:24,762 Stage: Train 0.5 | Epoch: 84 | Iter: 258200 | Total Loss: 0.003165 | Recon Loss: 0.002628 | Commit Loss: 0.001073 | Perplexity: 680.645151
Trainning Epoch:  52%|█████▏    | 85/165 [2:49:36<2:38:48, 119.10s/it]2025-09-14 19:12:32,477 Stage: Train 0.5 | Epoch: 85 | Iter: 258400 | Total Loss: 0.003080 | Recon Loss: 0.002545 | Commit Loss: 0.001070 | Perplexity: 677.974135
2025-09-14 19:12:40,555 Stage: Train 0.5 | Epoch: 85 | Iter: 258600 | Total Loss: 0.003125 | Recon Loss: 0.002588 | Commit Loss: 0.001075 | Perplexity: 680.543819
2025-09-14 19:12:48,702 Stage: Train 0.5 | Epoch: 85 | Iter: 258800 | Total Loss: 0.003080 | Recon Loss: 0.002547 | Commit Loss: 0.001066 | Perplexity: 681.379716
2025-09-14 19:12:56,820 Stage: Train 0.5 | Epoch: 85 | Iter: 259000 | Total Loss: 0.003102 | Recon Loss: 0.002564 | Commit Loss: 0.001077 | Perplexity: 679.860081
2025-09-14 19:13:04,966 Stage: Train 0.5 | Epoch: 85 | Iter: 259200 | Total Loss: 0.003113 | Recon Loss: 0.002570 | Commit Loss: 0.001087 | Perplexity: 677.273643
2025-09-14 19:13:13,104 Stage: Train 0.5 | Epoch: 85 | Iter: 259400 | Total Loss: 0.003040 | Recon Loss: 0.002505 | Commit Loss: 0.001070 | Perplexity: 677.354533
2025-09-14 19:13:21,238 Stage: Train 0.5 | Epoch: 85 | Iter: 259600 | Total Loss: 0.003143 | Recon Loss: 0.002603 | Commit Loss: 0.001078 | Perplexity: 679.502489
2025-09-14 19:13:29,362 Stage: Train 0.5 | Epoch: 85 | Iter: 259800 | Total Loss: 0.003126 | Recon Loss: 0.002591 | Commit Loss: 0.001070 | Perplexity: 680.229841
2025-09-14 19:13:37,492 Stage: Train 0.5 | Epoch: 85 | Iter: 260000 | Total Loss: 0.003092 | Recon Loss: 0.002557 | Commit Loss: 0.001069 | Perplexity: 680.798407
2025-09-14 19:13:37,492 Saving model at iteration 260000
2025-09-14 19:13:37,641 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_86_step_260000
2025-09-14 19:13:37,774 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_86_step_260000/pytorch_model.bin
2025-09-14 19:13:38,015 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_86_step_260000/optimizer.bin
2025-09-14 19:13:38,016 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_86_step_260000/scheduler.bin
2025-09-14 19:13:38,016 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_86_step_260000/random_states_0.pkl
2025-09-14 19:13:46,114 Stage: Train 0.5 | Epoch: 85 | Iter: 260200 | Total Loss: 0.003103 | Recon Loss: 0.002566 | Commit Loss: 0.001075 | Perplexity: 681.665648
2025-09-14 19:13:54,252 Stage: Train 0.5 | Epoch: 85 | Iter: 260400 | Total Loss: 0.003117 | Recon Loss: 0.002583 | Commit Loss: 0.001070 | Perplexity: 680.234612
2025-09-14 19:14:02,411 Stage: Train 0.5 | Epoch: 85 | Iter: 260600 | Total Loss: 0.003116 | Recon Loss: 0.002578 | Commit Loss: 0.001076 | Perplexity: 683.935467
2025-09-14 19:14:10,570 Stage: Train 0.5 | Epoch: 85 | Iter: 260800 | Total Loss: 0.003093 | Recon Loss: 0.002558 | Commit Loss: 0.001071 | Perplexity: 682.708774
2025-09-14 19:14:18,526 Stage: Train 0.5 | Epoch: 85 | Iter: 261000 | Total Loss: 0.003137 | Recon Loss: 0.002596 | Commit Loss: 0.001082 | Perplexity: 682.327260
2025-09-14 19:14:26,345 Stage: Train 0.5 | Epoch: 85 | Iter: 261200 | Total Loss: 0.003137 | Recon Loss: 0.002599 | Commit Loss: 0.001076 | Perplexity: 682.574099
Trainning Epoch:  52%|█████▏    | 86/165 [2:51:39<2:38:23, 120.30s/it]2025-09-14 19:14:34,162 Stage: Train 0.5 | Epoch: 86 | Iter: 261400 | Total Loss: 0.003104 | Recon Loss: 0.002573 | Commit Loss: 0.001062 | Perplexity: 679.920052
2025-09-14 19:14:42,003 Stage: Train 0.5 | Epoch: 86 | Iter: 261600 | Total Loss: 0.003129 | Recon Loss: 0.002598 | Commit Loss: 0.001063 | Perplexity: 681.046053
2025-09-14 19:14:49,825 Stage: Train 0.5 | Epoch: 86 | Iter: 261800 | Total Loss: 0.003157 | Recon Loss: 0.002624 | Commit Loss: 0.001066 | Perplexity: 679.128761
2025-09-14 19:14:57,664 Stage: Train 0.5 | Epoch: 86 | Iter: 262000 | Total Loss: 0.003084 | Recon Loss: 0.002547 | Commit Loss: 0.001072 | Perplexity: 681.717694
2025-09-14 19:15:05,441 Stage: Train 0.5 | Epoch: 86 | Iter: 262200 | Total Loss: 0.003119 | Recon Loss: 0.002587 | Commit Loss: 0.001065 | Perplexity: 681.507008
2025-09-14 19:15:13,214 Stage: Train 0.5 | Epoch: 86 | Iter: 262400 | Total Loss: 0.003110 | Recon Loss: 0.002576 | Commit Loss: 0.001067 | Perplexity: 679.647480
2025-09-14 19:15:21,215 Stage: Train 0.5 | Epoch: 86 | Iter: 262600 | Total Loss: 0.003061 | Recon Loss: 0.002526 | Commit Loss: 0.001071 | Perplexity: 680.519025
2025-09-14 19:15:29,311 Stage: Train 0.5 | Epoch: 86 | Iter: 262800 | Total Loss: 0.003155 | Recon Loss: 0.002619 | Commit Loss: 0.001073 | Perplexity: 683.700884
2025-09-14 19:15:37,123 Stage: Train 0.5 | Epoch: 86 | Iter: 263000 | Total Loss: 0.003102 | Recon Loss: 0.002569 | Commit Loss: 0.001067 | Perplexity: 681.310879
2025-09-14 19:15:44,817 Stage: Train 0.5 | Epoch: 86 | Iter: 263200 | Total Loss: 0.003093 | Recon Loss: 0.002556 | Commit Loss: 0.001073 | Perplexity: 680.309211
2025-09-14 19:15:52,502 Stage: Train 0.5 | Epoch: 86 | Iter: 263400 | Total Loss: 0.003137 | Recon Loss: 0.002603 | Commit Loss: 0.001067 | Perplexity: 680.441343
2025-09-14 19:16:00,213 Stage: Train 0.5 | Epoch: 86 | Iter: 263600 | Total Loss: 0.003111 | Recon Loss: 0.002572 | Commit Loss: 0.001077 | Perplexity: 679.373341
2025-09-14 19:16:07,927 Stage: Train 0.5 | Epoch: 86 | Iter: 263800 | Total Loss: 0.003113 | Recon Loss: 0.002581 | Commit Loss: 0.001064 | Perplexity: 679.115733
2025-09-14 19:16:15,610 Stage: Train 0.5 | Epoch: 86 | Iter: 264000 | Total Loss: 0.003129 | Recon Loss: 0.002593 | Commit Loss: 0.001071 | Perplexity: 677.758102
2025-09-14 19:16:23,338 Stage: Train 0.5 | Epoch: 86 | Iter: 264200 | Total Loss: 0.003068 | Recon Loss: 0.002533 | Commit Loss: 0.001070 | Perplexity: 680.309538
Trainning Epoch:  53%|█████▎    | 87/165 [2:53:38<2:35:39, 119.74s/it]2025-09-14 19:16:31,101 Stage: Train 0.5 | Epoch: 87 | Iter: 264400 | Total Loss: 0.003089 | Recon Loss: 0.002560 | Commit Loss: 0.001059 | Perplexity: 676.995848
2025-09-14 19:16:38,809 Stage: Train 0.5 | Epoch: 87 | Iter: 264600 | Total Loss: 0.003074 | Recon Loss: 0.002538 | Commit Loss: 0.001071 | Perplexity: 686.120324
2025-09-14 19:16:46,489 Stage: Train 0.5 | Epoch: 87 | Iter: 264800 | Total Loss: 0.003132 | Recon Loss: 0.002593 | Commit Loss: 0.001078 | Perplexity: 683.807118
2025-09-14 19:16:54,201 Stage: Train 0.5 | Epoch: 87 | Iter: 265000 | Total Loss: 0.003060 | Recon Loss: 0.002525 | Commit Loss: 0.001071 | Perplexity: 681.759719
2025-09-14 19:17:01,905 Stage: Train 0.5 | Epoch: 87 | Iter: 265200 | Total Loss: 0.003101 | Recon Loss: 0.002567 | Commit Loss: 0.001069 | Perplexity: 680.996373
2025-09-14 19:17:09,630 Stage: Train 0.5 | Epoch: 87 | Iter: 265400 | Total Loss: 0.003054 | Recon Loss: 0.002522 | Commit Loss: 0.001066 | Perplexity: 681.905718
2025-09-14 19:17:17,329 Stage: Train 0.5 | Epoch: 87 | Iter: 265600 | Total Loss: 0.003106 | Recon Loss: 0.002571 | Commit Loss: 0.001071 | Perplexity: 681.258862
2025-09-14 19:17:25,037 Stage: Train 0.5 | Epoch: 87 | Iter: 265800 | Total Loss: 0.003157 | Recon Loss: 0.002626 | Commit Loss: 0.001061 | Perplexity: 677.276149
2025-09-14 19:17:32,753 Stage: Train 0.5 | Epoch: 87 | Iter: 266000 | Total Loss: 0.003047 | Recon Loss: 0.002521 | Commit Loss: 0.001051 | Perplexity: 680.746912
2025-09-14 19:17:40,477 Stage: Train 0.5 | Epoch: 87 | Iter: 266200 | Total Loss: 0.003099 | Recon Loss: 0.002569 | Commit Loss: 0.001061 | Perplexity: 680.943938
2025-09-14 19:17:48,229 Stage: Train 0.5 | Epoch: 87 | Iter: 266400 | Total Loss: 0.003064 | Recon Loss: 0.002532 | Commit Loss: 0.001063 | Perplexity: 682.246675
2025-09-14 19:17:55,901 Stage: Train 0.5 | Epoch: 87 | Iter: 266600 | Total Loss: 0.003013 | Recon Loss: 0.002483 | Commit Loss: 0.001060 | Perplexity: 681.918943
2025-09-14 19:18:03,594 Stage: Train 0.5 | Epoch: 87 | Iter: 266800 | Total Loss: 0.003084 | Recon Loss: 0.002553 | Commit Loss: 0.001063 | Perplexity: 680.533797
2025-09-14 19:18:11,343 Stage: Train 0.5 | Epoch: 87 | Iter: 267000 | Total Loss: 0.003103 | Recon Loss: 0.002567 | Commit Loss: 0.001072 | Perplexity: 683.161867
2025-09-14 19:18:19,053 Stage: Train 0.5 | Epoch: 87 | Iter: 267200 | Total Loss: 0.003077 | Recon Loss: 0.002542 | Commit Loss: 0.001068 | Perplexity: 679.122559
Trainning Epoch:  53%|█████▎    | 88/165 [2:55:35<2:32:40, 118.97s/it]2025-09-14 19:18:26,801 Stage: Train 0.5 | Epoch: 88 | Iter: 267400 | Total Loss: 0.003052 | Recon Loss: 0.002531 | Commit Loss: 0.001042 | Perplexity: 675.764321
2025-09-14 19:18:34,526 Stage: Train 0.5 | Epoch: 88 | Iter: 267600 | Total Loss: 0.003077 | Recon Loss: 0.002551 | Commit Loss: 0.001052 | Perplexity: 678.134565
2025-09-14 19:18:42,266 Stage: Train 0.5 | Epoch: 88 | Iter: 267800 | Total Loss: 0.003074 | Recon Loss: 0.002549 | Commit Loss: 0.001049 | Perplexity: 679.885859
2025-09-14 19:18:49,958 Stage: Train 0.5 | Epoch: 88 | Iter: 268000 | Total Loss: 0.003058 | Recon Loss: 0.002528 | Commit Loss: 0.001059 | Perplexity: 682.163145
2025-09-14 19:18:57,634 Stage: Train 0.5 | Epoch: 88 | Iter: 268200 | Total Loss: 0.003103 | Recon Loss: 0.002566 | Commit Loss: 0.001074 | Perplexity: 683.028029
2025-09-14 19:19:05,350 Stage: Train 0.5 | Epoch: 88 | Iter: 268400 | Total Loss: 0.003071 | Recon Loss: 0.002547 | Commit Loss: 0.001049 | Perplexity: 680.575094
2025-09-14 19:19:13,078 Stage: Train 0.5 | Epoch: 88 | Iter: 268600 | Total Loss: 0.003112 | Recon Loss: 0.002585 | Commit Loss: 0.001055 | Perplexity: 678.871802
2025-09-14 19:19:20,778 Stage: Train 0.5 | Epoch: 88 | Iter: 268800 | Total Loss: 0.003118 | Recon Loss: 0.002590 | Commit Loss: 0.001056 | Perplexity: 680.574688
2025-09-14 19:19:28,475 Stage: Train 0.5 | Epoch: 88 | Iter: 269000 | Total Loss: 0.003056 | Recon Loss: 0.002532 | Commit Loss: 0.001050 | Perplexity: 680.184264
2025-09-14 19:19:36,205 Stage: Train 0.5 | Epoch: 88 | Iter: 269200 | Total Loss: 0.003069 | Recon Loss: 0.002540 | Commit Loss: 0.001058 | Perplexity: 680.287918
2025-09-14 19:19:43,885 Stage: Train 0.5 | Epoch: 88 | Iter: 269400 | Total Loss: 0.003152 | Recon Loss: 0.002628 | Commit Loss: 0.001048 | Perplexity: 680.266755
2025-09-14 19:19:51,566 Stage: Train 0.5 | Epoch: 88 | Iter: 269600 | Total Loss: 0.003067 | Recon Loss: 0.002534 | Commit Loss: 0.001066 | Perplexity: 682.475191
2025-09-14 19:19:59,271 Stage: Train 0.5 | Epoch: 88 | Iter: 269800 | Total Loss: 0.003140 | Recon Loss: 0.002610 | Commit Loss: 0.001061 | Perplexity: 679.655945
2025-09-14 19:20:06,985 Stage: Train 0.5 | Epoch: 88 | Iter: 270000 | Total Loss: 0.003081 | Recon Loss: 0.002559 | Commit Loss: 0.001044 | Perplexity: 679.374861
2025-09-14 19:20:14,694 Stage: Train 0.5 | Epoch: 88 | Iter: 270200 | Total Loss: 0.003053 | Recon Loss: 0.002519 | Commit Loss: 0.001067 | Perplexity: 681.422386
Trainning Epoch:  54%|█████▍    | 89/165 [2:57:32<2:29:59, 118.41s/it]2025-09-14 19:20:22,429 Stage: Train 0.5 | Epoch: 89 | Iter: 270400 | Total Loss: 0.003045 | Recon Loss: 0.002519 | Commit Loss: 0.001053 | Perplexity: 679.260780
2025-09-14 19:20:30,106 Stage: Train 0.5 | Epoch: 89 | Iter: 270600 | Total Loss: 0.003073 | Recon Loss: 0.002542 | Commit Loss: 0.001062 | Perplexity: 682.366299
2025-09-14 19:20:37,785 Stage: Train 0.5 | Epoch: 89 | Iter: 270800 | Total Loss: 0.003103 | Recon Loss: 0.002578 | Commit Loss: 0.001050 | Perplexity: 679.403033
2025-09-14 19:20:45,589 Stage: Train 0.5 | Epoch: 89 | Iter: 271000 | Total Loss: 0.003079 | Recon Loss: 0.002543 | Commit Loss: 0.001071 | Perplexity: 684.541325
2025-09-14 19:20:53,688 Stage: Train 0.5 | Epoch: 89 | Iter: 271200 | Total Loss: 0.003032 | Recon Loss: 0.002504 | Commit Loss: 0.001055 | Perplexity: 683.485492
2025-09-14 19:21:01,785 Stage: Train 0.5 | Epoch: 89 | Iter: 271400 | Total Loss: 0.003060 | Recon Loss: 0.002528 | Commit Loss: 0.001064 | Perplexity: 684.017245
2025-09-14 19:21:09,794 Stage: Train 0.5 | Epoch: 89 | Iter: 271600 | Total Loss: 0.003079 | Recon Loss: 0.002553 | Commit Loss: 0.001052 | Perplexity: 679.256508
2025-09-14 19:21:17,565 Stage: Train 0.5 | Epoch: 89 | Iter: 271800 | Total Loss: 0.003086 | Recon Loss: 0.002559 | Commit Loss: 0.001054 | Perplexity: 682.193913
2025-09-14 19:21:25,393 Stage: Train 0.5 | Epoch: 89 | Iter: 272000 | Total Loss: 0.003078 | Recon Loss: 0.002548 | Commit Loss: 0.001060 | Perplexity: 682.737039
2025-09-14 19:21:33,222 Stage: Train 0.5 | Epoch: 89 | Iter: 272200 | Total Loss: 0.003068 | Recon Loss: 0.002544 | Commit Loss: 0.001048 | Perplexity: 681.680212
2025-09-14 19:21:41,033 Stage: Train 0.5 | Epoch: 89 | Iter: 272400 | Total Loss: 0.003019 | Recon Loss: 0.002497 | Commit Loss: 0.001045 | Perplexity: 679.522997
2025-09-14 19:21:48,870 Stage: Train 0.5 | Epoch: 89 | Iter: 272600 | Total Loss: 0.003054 | Recon Loss: 0.002529 | Commit Loss: 0.001050 | Perplexity: 679.648053
2025-09-14 19:21:56,695 Stage: Train 0.5 | Epoch: 89 | Iter: 272800 | Total Loss: 0.003062 | Recon Loss: 0.002540 | Commit Loss: 0.001046 | Perplexity: 681.894876
2025-09-14 19:22:04,510 Stage: Train 0.5 | Epoch: 89 | Iter: 273000 | Total Loss: 0.003091 | Recon Loss: 0.002568 | Commit Loss: 0.001046 | Perplexity: 680.807115
2025-09-14 19:22:12,309 Stage: Train 0.5 | Epoch: 89 | Iter: 273200 | Total Loss: 0.003002 | Recon Loss: 0.002478 | Commit Loss: 0.001047 | Perplexity: 680.939738
2025-09-14 19:22:20,109 Stage: Train 0.5 | Epoch: 89 | Iter: 273400 | Total Loss: 0.003088 | Recon Loss: 0.002558 | Commit Loss: 0.001061 | Perplexity: 683.974698
Trainning Epoch:  55%|█████▍    | 90/165 [2:59:31<2:28:17, 118.64s/it]2025-09-14 19:22:27,901 Stage: Train 0.5 | Epoch: 90 | Iter: 273600 | Total Loss: 0.003067 | Recon Loss: 0.002544 | Commit Loss: 0.001048 | Perplexity: 679.030844
2025-09-14 19:22:35,658 Stage: Train 0.5 | Epoch: 90 | Iter: 273800 | Total Loss: 0.003075 | Recon Loss: 0.002543 | Commit Loss: 0.001063 | Perplexity: 685.553641
2025-09-14 19:22:43,455 Stage: Train 0.5 | Epoch: 90 | Iter: 274000 | Total Loss: 0.003053 | Recon Loss: 0.002529 | Commit Loss: 0.001048 | Perplexity: 681.236964
2025-09-14 19:22:51,238 Stage: Train 0.5 | Epoch: 90 | Iter: 274200 | Total Loss: 0.003040 | Recon Loss: 0.002511 | Commit Loss: 0.001059 | Perplexity: 684.967619
2025-09-14 19:22:59,242 Stage: Train 0.5 | Epoch: 90 | Iter: 274400 | Total Loss: 0.003112 | Recon Loss: 0.002588 | Commit Loss: 0.001048 | Perplexity: 678.157292
2025-09-14 19:23:07,332 Stage: Train 0.5 | Epoch: 90 | Iter: 274600 | Total Loss: 0.003031 | Recon Loss: 0.002511 | Commit Loss: 0.001040 | Perplexity: 681.913870
2025-09-14 19:23:15,133 Stage: Train 0.5 | Epoch: 90 | Iter: 274800 | Total Loss: 0.003004 | Recon Loss: 0.002484 | Commit Loss: 0.001038 | Perplexity: 680.026107
2025-09-14 19:23:22,913 Stage: Train 0.5 | Epoch: 90 | Iter: 275000 | Total Loss: 0.003032 | Recon Loss: 0.002508 | Commit Loss: 0.001049 | Perplexity: 684.175313
2025-09-14 19:23:30,685 Stage: Train 0.5 | Epoch: 90 | Iter: 275200 | Total Loss: 0.003084 | Recon Loss: 0.002559 | Commit Loss: 0.001051 | Perplexity: 682.359632
2025-09-14 19:23:38,465 Stage: Train 0.5 | Epoch: 90 | Iter: 275400 | Total Loss: 0.003066 | Recon Loss: 0.002547 | Commit Loss: 0.001039 | Perplexity: 680.205850
2025-09-14 19:23:46,274 Stage: Train 0.5 | Epoch: 90 | Iter: 275600 | Total Loss: 0.003013 | Recon Loss: 0.002494 | Commit Loss: 0.001037 | Perplexity: 679.098073
2025-09-14 19:23:54,062 Stage: Train 0.5 | Epoch: 90 | Iter: 275800 | Total Loss: 0.003088 | Recon Loss: 0.002560 | Commit Loss: 0.001056 | Perplexity: 683.497510
2025-09-14 19:24:01,865 Stage: Train 0.5 | Epoch: 90 | Iter: 276000 | Total Loss: 0.003062 | Recon Loss: 0.002542 | Commit Loss: 0.001041 | Perplexity: 681.173786
2025-09-14 19:24:09,662 Stage: Train 0.5 | Epoch: 90 | Iter: 276200 | Total Loss: 0.003052 | Recon Loss: 0.002532 | Commit Loss: 0.001041 | Perplexity: 680.144826
2025-09-14 19:24:17,450 Stage: Train 0.5 | Epoch: 90 | Iter: 276400 | Total Loss: 0.003014 | Recon Loss: 0.002491 | Commit Loss: 0.001046 | Perplexity: 678.649551
Trainning Epoch:  55%|█████▌    | 91/165 [3:01:30<2:26:23, 118.70s/it]2025-09-14 19:24:25,285 Stage: Train 0.5 | Epoch: 91 | Iter: 276600 | Total Loss: 0.003058 | Recon Loss: 0.002531 | Commit Loss: 0.001054 | Perplexity: 681.841754
2025-09-14 19:24:33,125 Stage: Train 0.5 | Epoch: 91 | Iter: 276800 | Total Loss: 0.003036 | Recon Loss: 0.002512 | Commit Loss: 0.001048 | Perplexity: 681.281667
2025-09-14 19:24:40,945 Stage: Train 0.5 | Epoch: 91 | Iter: 277000 | Total Loss: 0.003064 | Recon Loss: 0.002538 | Commit Loss: 0.001051 | Perplexity: 683.687884
2025-09-14 19:24:48,758 Stage: Train 0.5 | Epoch: 91 | Iter: 277200 | Total Loss: 0.003080 | Recon Loss: 0.002559 | Commit Loss: 0.001042 | Perplexity: 681.555717
2025-09-14 19:24:56,551 Stage: Train 0.5 | Epoch: 91 | Iter: 277400 | Total Loss: 0.003058 | Recon Loss: 0.002538 | Commit Loss: 0.001040 | Perplexity: 681.314409
2025-09-14 19:25:04,639 Stage: Train 0.5 | Epoch: 91 | Iter: 277600 | Total Loss: 0.003039 | Recon Loss: 0.002519 | Commit Loss: 0.001041 | Perplexity: 684.847901
2025-09-14 19:25:12,411 Stage: Train 0.5 | Epoch: 91 | Iter: 277800 | Total Loss: 0.003054 | Recon Loss: 0.002534 | Commit Loss: 0.001040 | Perplexity: 682.941950
2025-09-14 19:25:20,094 Stage: Train 0.5 | Epoch: 91 | Iter: 278000 | Total Loss: 0.003085 | Recon Loss: 0.002564 | Commit Loss: 0.001042 | Perplexity: 680.054352
2025-09-14 19:25:27,760 Stage: Train 0.5 | Epoch: 91 | Iter: 278200 | Total Loss: 0.003005 | Recon Loss: 0.002490 | Commit Loss: 0.001030 | Perplexity: 678.684216
2025-09-14 19:25:35,625 Stage: Train 0.5 | Epoch: 91 | Iter: 278400 | Total Loss: 0.003031 | Recon Loss: 0.002508 | Commit Loss: 0.001047 | Perplexity: 684.758495
2025-09-14 19:25:43,426 Stage: Train 0.5 | Epoch: 91 | Iter: 278600 | Total Loss: 0.003092 | Recon Loss: 0.002573 | Commit Loss: 0.001039 | Perplexity: 683.225504
2025-09-14 19:25:51,178 Stage: Train 0.5 | Epoch: 91 | Iter: 278800 | Total Loss: 0.003065 | Recon Loss: 0.002539 | Commit Loss: 0.001052 | Perplexity: 682.608965
2025-09-14 19:25:58,913 Stage: Train 0.5 | Epoch: 91 | Iter: 279000 | Total Loss: 0.003057 | Recon Loss: 0.002534 | Commit Loss: 0.001045 | Perplexity: 682.203471
2025-09-14 19:26:06,655 Stage: Train 0.5 | Epoch: 91 | Iter: 279200 | Total Loss: 0.003077 | Recon Loss: 0.002554 | Commit Loss: 0.001046 | Perplexity: 679.039008
2025-09-14 19:26:14,401 Stage: Train 0.5 | Epoch: 91 | Iter: 279400 | Total Loss: 0.003013 | Recon Loss: 0.002492 | Commit Loss: 0.001042 | Perplexity: 681.977942
Trainning Epoch:  56%|█████▌    | 92/165 [3:03:28<2:24:18, 118.60s/it]2025-09-14 19:26:22,153 Stage: Train 0.5 | Epoch: 92 | Iter: 279600 | Total Loss: 0.003059 | Recon Loss: 0.002544 | Commit Loss: 0.001030 | Perplexity: 679.917572
2025-09-14 19:26:29,914 Stage: Train 0.5 | Epoch: 92 | Iter: 279800 | Total Loss: 0.003004 | Recon Loss: 0.002486 | Commit Loss: 0.001035 | Perplexity: 678.866280
2025-09-14 19:26:37,694 Stage: Train 0.5 | Epoch: 92 | Iter: 280000 | Total Loss: 0.003110 | Recon Loss: 0.002589 | Commit Loss: 0.001041 | Perplexity: 681.272943
2025-09-14 19:26:37,694 Saving model at iteration 280000
2025-09-14 19:26:37,837 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_93_step_280000
2025-09-14 19:26:37,966 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_93_step_280000/pytorch_model.bin
2025-09-14 19:26:38,215 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_93_step_280000/optimizer.bin
2025-09-14 19:26:38,215 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_93_step_280000/scheduler.bin
2025-09-14 19:26:38,216 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_93_step_280000/random_states_0.pkl
2025-09-14 19:26:45,985 Stage: Train 0.5 | Epoch: 92 | Iter: 280200 | Total Loss: 0.002991 | Recon Loss: 0.002470 | Commit Loss: 0.001042 | Perplexity: 684.111230
2025-09-14 19:26:53,705 Stage: Train 0.5 | Epoch: 92 | Iter: 280400 | Total Loss: 0.003069 | Recon Loss: 0.002550 | Commit Loss: 0.001038 | Perplexity: 682.251367
2025-09-14 19:27:01,465 Stage: Train 0.5 | Epoch: 92 | Iter: 280600 | Total Loss: 0.003016 | Recon Loss: 0.002500 | Commit Loss: 0.001031 | Perplexity: 680.315775
2025-09-14 19:27:09,234 Stage: Train 0.5 | Epoch: 92 | Iter: 280800 | Total Loss: 0.003027 | Recon Loss: 0.002505 | Commit Loss: 0.001045 | Perplexity: 685.108904
2025-09-14 19:27:16,998 Stage: Train 0.5 | Epoch: 92 | Iter: 281000 | Total Loss: 0.003020 | Recon Loss: 0.002499 | Commit Loss: 0.001040 | Perplexity: 682.948964
2025-09-14 19:27:24,836 Stage: Train 0.5 | Epoch: 92 | Iter: 281200 | Total Loss: 0.003023 | Recon Loss: 0.002505 | Commit Loss: 0.001035 | Perplexity: 680.706970
2025-09-14 19:27:32,895 Stage: Train 0.5 | Epoch: 92 | Iter: 281400 | Total Loss: 0.003086 | Recon Loss: 0.002566 | Commit Loss: 0.001041 | Perplexity: 679.381659
2025-09-14 19:27:40,761 Stage: Train 0.5 | Epoch: 92 | Iter: 281600 | Total Loss: 0.003059 | Recon Loss: 0.002535 | Commit Loss: 0.001047 | Perplexity: 683.308030
2025-09-14 19:27:48,429 Stage: Train 0.5 | Epoch: 92 | Iter: 281800 | Total Loss: 0.003060 | Recon Loss: 0.002538 | Commit Loss: 0.001043 | Perplexity: 681.229540
2025-09-14 19:27:56,133 Stage: Train 0.5 | Epoch: 92 | Iter: 282000 | Total Loss: 0.003033 | Recon Loss: 0.002518 | Commit Loss: 0.001028 | Perplexity: 680.902645
2025-09-14 19:28:03,822 Stage: Train 0.5 | Epoch: 92 | Iter: 282200 | Total Loss: 0.003063 | Recon Loss: 0.002545 | Commit Loss: 0.001036 | Perplexity: 681.055195
2025-09-14 19:28:11,544 Stage: Train 0.5 | Epoch: 92 | Iter: 282400 | Total Loss: 0.003055 | Recon Loss: 0.002536 | Commit Loss: 0.001038 | Perplexity: 680.575789
Trainning Epoch:  56%|█████▋    | 93/165 [3:05:27<2:22:19, 118.60s/it]2025-09-14 19:28:19,226 Stage: Train 0.5 | Epoch: 93 | Iter: 282600 | Total Loss: 0.003045 | Recon Loss: 0.002528 | Commit Loss: 0.001036 | Perplexity: 680.922108
2025-09-14 19:28:26,933 Stage: Train 0.5 | Epoch: 93 | Iter: 282800 | Total Loss: 0.003008 | Recon Loss: 0.002493 | Commit Loss: 0.001030 | Perplexity: 683.532251
2025-09-14 19:28:34,602 Stage: Train 0.5 | Epoch: 93 | Iter: 283000 | Total Loss: 0.003031 | Recon Loss: 0.002512 | Commit Loss: 0.001039 | Perplexity: 682.684055
2025-09-14 19:28:42,306 Stage: Train 0.5 | Epoch: 93 | Iter: 283200 | Total Loss: 0.003050 | Recon Loss: 0.002533 | Commit Loss: 0.001035 | Perplexity: 682.589814
2025-09-14 19:28:50,018 Stage: Train 0.5 | Epoch: 93 | Iter: 283400 | Total Loss: 0.003024 | Recon Loss: 0.002503 | Commit Loss: 0.001042 | Perplexity: 682.541654
2025-09-14 19:28:57,709 Stage: Train 0.5 | Epoch: 93 | Iter: 283600 | Total Loss: 0.003064 | Recon Loss: 0.002550 | Commit Loss: 0.001029 | Perplexity: 681.115294
2025-09-14 19:29:05,411 Stage: Train 0.5 | Epoch: 93 | Iter: 283800 | Total Loss: 0.003060 | Recon Loss: 0.002543 | Commit Loss: 0.001033 | Perplexity: 681.051943
2025-09-14 19:29:13,138 Stage: Train 0.5 | Epoch: 93 | Iter: 284000 | Total Loss: 0.003013 | Recon Loss: 0.002496 | Commit Loss: 0.001033 | Perplexity: 679.916970
2025-09-14 19:29:20,841 Stage: Train 0.5 | Epoch: 93 | Iter: 284200 | Total Loss: 0.003017 | Recon Loss: 0.002493 | Commit Loss: 0.001047 | Perplexity: 682.007790
2025-09-14 19:29:28,561 Stage: Train 0.5 | Epoch: 93 | Iter: 284400 | Total Loss: 0.003019 | Recon Loss: 0.002504 | Commit Loss: 0.001031 | Perplexity: 680.835519
2025-09-14 19:29:36,388 Stage: Train 0.5 | Epoch: 93 | Iter: 284600 | Total Loss: 0.003051 | Recon Loss: 0.002536 | Commit Loss: 0.001028 | Perplexity: 681.289960
2025-09-14 19:29:44,513 Stage: Train 0.5 | Epoch: 93 | Iter: 284800 | Total Loss: 0.003042 | Recon Loss: 0.002526 | Commit Loss: 0.001031 | Perplexity: 683.606782
2025-09-14 19:29:52,626 Stage: Train 0.5 | Epoch: 93 | Iter: 285000 | Total Loss: 0.003019 | Recon Loss: 0.002503 | Commit Loss: 0.001031 | Perplexity: 682.646873
2025-09-14 19:30:00,510 Stage: Train 0.5 | Epoch: 93 | Iter: 285200 | Total Loss: 0.003060 | Recon Loss: 0.002543 | Commit Loss: 0.001034 | Perplexity: 682.484375
2025-09-14 19:30:08,235 Stage: Train 0.5 | Epoch: 93 | Iter: 285400 | Total Loss: 0.003037 | Recon Loss: 0.002522 | Commit Loss: 0.001030 | Perplexity: 681.448338
Trainning Epoch:  57%|█████▋    | 94/165 [3:07:25<2:20:11, 118.47s/it]2025-09-14 19:30:15,968 Stage: Train 0.5 | Epoch: 94 | Iter: 285600 | Total Loss: 0.003045 | Recon Loss: 0.002530 | Commit Loss: 0.001029 | Perplexity: 679.181726
2025-09-14 19:30:23,737 Stage: Train 0.5 | Epoch: 94 | Iter: 285800 | Total Loss: 0.002979 | Recon Loss: 0.002468 | Commit Loss: 0.001022 | Perplexity: 679.045007
2025-09-14 19:30:31,501 Stage: Train 0.5 | Epoch: 94 | Iter: 286000 | Total Loss: 0.003137 | Recon Loss: 0.002624 | Commit Loss: 0.001025 | Perplexity: 678.274460
2025-09-14 19:30:39,277 Stage: Train 0.5 | Epoch: 94 | Iter: 286200 | Total Loss: 0.002958 | Recon Loss: 0.002449 | Commit Loss: 0.001018 | Perplexity: 679.484897
2025-09-14 19:30:47,013 Stage: Train 0.5 | Epoch: 94 | Iter: 286400 | Total Loss: 0.003029 | Recon Loss: 0.002514 | Commit Loss: 0.001030 | Perplexity: 681.313739
2025-09-14 19:30:54,794 Stage: Train 0.5 | Epoch: 94 | Iter: 286600 | Total Loss: 0.003035 | Recon Loss: 0.002527 | Commit Loss: 0.001017 | Perplexity: 680.399347
2025-09-14 19:31:02,665 Stage: Train 0.5 | Epoch: 94 | Iter: 286800 | Total Loss: 0.003090 | Recon Loss: 0.002573 | Commit Loss: 0.001033 | Perplexity: 683.654086
2025-09-14 19:31:10,806 Stage: Train 0.5 | Epoch: 94 | Iter: 287000 | Total Loss: 0.003003 | Recon Loss: 0.002488 | Commit Loss: 0.001030 | Perplexity: 680.015300
2025-09-14 19:31:18,912 Stage: Train 0.5 | Epoch: 94 | Iter: 287200 | Total Loss: 0.003000 | Recon Loss: 0.002487 | Commit Loss: 0.001026 | Perplexity: 681.025121
2025-09-14 19:31:26,662 Stage: Train 0.5 | Epoch: 94 | Iter: 287400 | Total Loss: 0.003005 | Recon Loss: 0.002488 | Commit Loss: 0.001034 | Perplexity: 683.059869
2025-09-14 19:31:34,366 Stage: Train 0.5 | Epoch: 94 | Iter: 287600 | Total Loss: 0.003046 | Recon Loss: 0.002529 | Commit Loss: 0.001033 | Perplexity: 678.542569
2025-09-14 19:31:42,086 Stage: Train 0.5 | Epoch: 94 | Iter: 287800 | Total Loss: 0.003018 | Recon Loss: 0.002508 | Commit Loss: 0.001020 | Perplexity: 680.079368
2025-09-14 19:31:50,019 Stage: Train 0.5 | Epoch: 94 | Iter: 288000 | Total Loss: 0.003023 | Recon Loss: 0.002505 | Commit Loss: 0.001035 | Perplexity: 683.089772
2025-09-14 19:31:57,914 Stage: Train 0.5 | Epoch: 94 | Iter: 288200 | Total Loss: 0.002998 | Recon Loss: 0.002483 | Commit Loss: 0.001029 | Perplexity: 683.363756
2025-09-14 19:32:05,706 Stage: Train 0.5 | Epoch: 94 | Iter: 288400 | Total Loss: 0.003039 | Recon Loss: 0.002524 | Commit Loss: 0.001031 | Perplexity: 679.533729
2025-09-14 19:32:13,748 Stage: Train 0.5 | Epoch: 94 | Iter: 288600 | Total Loss: 0.003057 | Recon Loss: 0.002537 | Commit Loss: 0.001039 | Perplexity: 686.260562
Trainning Epoch:  58%|█████▊    | 95/165 [3:09:24<2:18:29, 118.71s/it]2025-09-14 19:32:21,927 Stage: Train 0.5 | Epoch: 95 | Iter: 288800 | Total Loss: 0.002987 | Recon Loss: 0.002469 | Commit Loss: 0.001034 | Perplexity: 681.889494
2025-09-14 19:32:30,057 Stage: Train 0.5 | Epoch: 95 | Iter: 289000 | Total Loss: 0.003038 | Recon Loss: 0.002527 | Commit Loss: 0.001022 | Perplexity: 681.210669
2025-09-14 19:32:38,137 Stage: Train 0.5 | Epoch: 95 | Iter: 289200 | Total Loss: 0.002948 | Recon Loss: 0.002436 | Commit Loss: 0.001025 | Perplexity: 683.006741
2025-09-14 19:32:45,941 Stage: Train 0.5 | Epoch: 95 | Iter: 289400 | Total Loss: 0.002994 | Recon Loss: 0.002479 | Commit Loss: 0.001030 | Perplexity: 686.475395
2025-09-14 19:32:53,777 Stage: Train 0.5 | Epoch: 95 | Iter: 289600 | Total Loss: 0.003019 | Recon Loss: 0.002506 | Commit Loss: 0.001026 | Perplexity: 683.316033
2025-09-14 19:33:01,626 Stage: Train 0.5 | Epoch: 95 | Iter: 289800 | Total Loss: 0.003014 | Recon Loss: 0.002502 | Commit Loss: 0.001023 | Perplexity: 680.206386
2025-09-14 19:33:09,433 Stage: Train 0.5 | Epoch: 95 | Iter: 290000 | Total Loss: 0.003036 | Recon Loss: 0.002520 | Commit Loss: 0.001031 | Perplexity: 683.099116
2025-09-14 19:33:17,516 Stage: Train 0.5 | Epoch: 95 | Iter: 290200 | Total Loss: 0.003016 | Recon Loss: 0.002501 | Commit Loss: 0.001030 | Perplexity: 681.806035
2025-09-14 19:33:25,598 Stage: Train 0.5 | Epoch: 95 | Iter: 290400 | Total Loss: 0.002974 | Recon Loss: 0.002461 | Commit Loss: 0.001027 | Perplexity: 680.604907
2025-09-14 19:33:33,373 Stage: Train 0.5 | Epoch: 95 | Iter: 290600 | Total Loss: 0.002989 | Recon Loss: 0.002474 | Commit Loss: 0.001030 | Perplexity: 681.211649
2025-09-14 19:33:41,162 Stage: Train 0.5 | Epoch: 95 | Iter: 290800 | Total Loss: 0.003039 | Recon Loss: 0.002524 | Commit Loss: 0.001031 | Perplexity: 684.869994
2025-09-14 19:33:48,933 Stage: Train 0.5 | Epoch: 95 | Iter: 291000 | Total Loss: 0.003004 | Recon Loss: 0.002496 | Commit Loss: 0.001016 | Perplexity: 680.018329
2025-09-14 19:33:56,836 Stage: Train 0.5 | Epoch: 95 | Iter: 291200 | Total Loss: 0.003033 | Recon Loss: 0.002520 | Commit Loss: 0.001026 | Perplexity: 681.180847
2025-09-14 19:34:04,797 Stage: Train 0.5 | Epoch: 95 | Iter: 291400 | Total Loss: 0.002969 | Recon Loss: 0.002454 | Commit Loss: 0.001030 | Perplexity: 681.800188
2025-09-14 19:34:12,587 Stage: Train 0.5 | Epoch: 95 | Iter: 291600 | Total Loss: 0.003046 | Recon Loss: 0.002531 | Commit Loss: 0.001031 | Perplexity: 680.200535
Trainning Epoch:  58%|█████▊    | 96/165 [3:11:25<2:17:04, 119.19s/it]2025-09-14 19:34:20,372 Stage: Train 0.5 | Epoch: 96 | Iter: 291800 | Total Loss: 0.003014 | Recon Loss: 0.002501 | Commit Loss: 0.001025 | Perplexity: 682.804187
2025-09-14 19:34:28,137 Stage: Train 0.5 | Epoch: 96 | Iter: 292000 | Total Loss: 0.002971 | Recon Loss: 0.002460 | Commit Loss: 0.001023 | Perplexity: 682.246174
2025-09-14 19:34:35,958 Stage: Train 0.5 | Epoch: 96 | Iter: 292200 | Total Loss: 0.002997 | Recon Loss: 0.002485 | Commit Loss: 0.001025 | Perplexity: 682.300161
2025-09-14 19:34:43,990 Stage: Train 0.5 | Epoch: 96 | Iter: 292400 | Total Loss: 0.002954 | Recon Loss: 0.002447 | Commit Loss: 0.001013 | Perplexity: 676.469816
2025-09-14 19:34:51,834 Stage: Train 0.5 | Epoch: 96 | Iter: 292600 | Total Loss: 0.003012 | Recon Loss: 0.002496 | Commit Loss: 0.001031 | Perplexity: 681.269563
2025-09-14 19:34:59,884 Stage: Train 0.5 | Epoch: 96 | Iter: 292800 | Total Loss: 0.003054 | Recon Loss: 0.002547 | Commit Loss: 0.001014 | Perplexity: 679.469546
2025-09-14 19:35:07,865 Stage: Train 0.5 | Epoch: 96 | Iter: 293000 | Total Loss: 0.002982 | Recon Loss: 0.002470 | Commit Loss: 0.001025 | Perplexity: 680.250213
2025-09-14 19:35:15,730 Stage: Train 0.5 | Epoch: 96 | Iter: 293200 | Total Loss: 0.003017 | Recon Loss: 0.002503 | Commit Loss: 0.001029 | Perplexity: 685.816073
2025-09-14 19:35:23,566 Stage: Train 0.5 | Epoch: 96 | Iter: 293400 | Total Loss: 0.003011 | Recon Loss: 0.002502 | Commit Loss: 0.001017 | Perplexity: 679.267030
2025-09-14 19:35:31,401 Stage: Train 0.5 | Epoch: 96 | Iter: 293600 | Total Loss: 0.002995 | Recon Loss: 0.002477 | Commit Loss: 0.001035 | Perplexity: 682.584162
2025-09-14 19:35:39,262 Stage: Train 0.5 | Epoch: 96 | Iter: 293800 | Total Loss: 0.003060 | Recon Loss: 0.002545 | Commit Loss: 0.001031 | Perplexity: 684.742246
2025-09-14 19:35:47,097 Stage: Train 0.5 | Epoch: 96 | Iter: 294000 | Total Loss: 0.002997 | Recon Loss: 0.002484 | Commit Loss: 0.001025 | Perplexity: 681.294242
2025-09-14 19:35:54,942 Stage: Train 0.5 | Epoch: 96 | Iter: 294200 | Total Loss: 0.002937 | Recon Loss: 0.002425 | Commit Loss: 0.001025 | Perplexity: 681.492097
2025-09-14 19:36:02,717 Stage: Train 0.5 | Epoch: 96 | Iter: 294400 | Total Loss: 0.003013 | Recon Loss: 0.002500 | Commit Loss: 0.001026 | Perplexity: 682.102586
2025-09-14 19:36:10,390 Stage: Train 0.5 | Epoch: 96 | Iter: 294600 | Total Loss: 0.003030 | Recon Loss: 0.002510 | Commit Loss: 0.001040 | Perplexity: 682.832931
Trainning Epoch:  59%|█████▉    | 97/165 [3:13:24<2:15:05, 119.20s/it]2025-09-14 19:36:18,065 Stage: Train 0.5 | Epoch: 97 | Iter: 294800 | Total Loss: 0.003000 | Recon Loss: 0.002486 | Commit Loss: 0.001026 | Perplexity: 681.290087
2025-09-14 19:36:25,948 Stage: Train 0.5 | Epoch: 97 | Iter: 295000 | Total Loss: 0.002963 | Recon Loss: 0.002454 | Commit Loss: 0.001018 | Perplexity: 680.174165
2025-09-14 19:36:33,873 Stage: Train 0.5 | Epoch: 97 | Iter: 295200 | Total Loss: 0.002982 | Recon Loss: 0.002476 | Commit Loss: 0.001011 | Perplexity: 679.007812
2025-09-14 19:36:41,753 Stage: Train 0.5 | Epoch: 97 | Iter: 295400 | Total Loss: 0.002976 | Recon Loss: 0.002464 | Commit Loss: 0.001024 | Perplexity: 683.888904
2025-09-14 19:36:49,593 Stage: Train 0.5 | Epoch: 97 | Iter: 295600 | Total Loss: 0.003029 | Recon Loss: 0.002513 | Commit Loss: 0.001033 | Perplexity: 685.428462
2025-09-14 19:36:57,461 Stage: Train 0.5 | Epoch: 97 | Iter: 295800 | Total Loss: 0.002957 | Recon Loss: 0.002445 | Commit Loss: 0.001022 | Perplexity: 681.972941
2025-09-14 19:37:05,341 Stage: Train 0.5 | Epoch: 97 | Iter: 296000 | Total Loss: 0.003001 | Recon Loss: 0.002483 | Commit Loss: 0.001035 | Perplexity: 684.290630
2025-09-14 19:37:13,239 Stage: Train 0.5 | Epoch: 97 | Iter: 296200 | Total Loss: 0.003002 | Recon Loss: 0.002489 | Commit Loss: 0.001026 | Perplexity: 682.315917
2025-09-14 19:37:21,346 Stage: Train 0.5 | Epoch: 97 | Iter: 296400 | Total Loss: 0.002946 | Recon Loss: 0.002440 | Commit Loss: 0.001012 | Perplexity: 680.627329
2025-09-14 19:37:29,367 Stage: Train 0.5 | Epoch: 97 | Iter: 296600 | Total Loss: 0.002989 | Recon Loss: 0.002474 | Commit Loss: 0.001030 | Perplexity: 682.412747
2025-09-14 19:37:37,260 Stage: Train 0.5 | Epoch: 97 | Iter: 296800 | Total Loss: 0.002961 | Recon Loss: 0.002452 | Commit Loss: 0.001019 | Perplexity: 681.208422
2025-09-14 19:37:45,214 Stage: Train 0.5 | Epoch: 97 | Iter: 297000 | Total Loss: 0.003015 | Recon Loss: 0.002502 | Commit Loss: 0.001025 | Perplexity: 682.877321
2025-09-14 19:37:53,031 Stage: Train 0.5 | Epoch: 97 | Iter: 297200 | Total Loss: 0.003015 | Recon Loss: 0.002503 | Commit Loss: 0.001025 | Perplexity: 684.305848
2025-09-14 19:38:00,822 Stage: Train 0.5 | Epoch: 97 | Iter: 297400 | Total Loss: 0.003012 | Recon Loss: 0.002503 | Commit Loss: 0.001018 | Perplexity: 681.608414
2025-09-14 19:38:08,686 Stage: Train 0.5 | Epoch: 97 | Iter: 297600 | Total Loss: 0.002955 | Recon Loss: 0.002446 | Commit Loss: 0.001017 | Perplexity: 683.237499
Trainning Epoch:  59%|█████▉    | 98/165 [3:15:24<2:13:19, 119.39s/it]2025-09-14 19:38:16,435 Stage: Train 0.5 | Epoch: 98 | Iter: 297800 | Total Loss: 0.003055 | Recon Loss: 0.002546 | Commit Loss: 0.001019 | Perplexity: 681.569145
2025-09-14 19:38:24,121 Stage: Train 0.5 | Epoch: 98 | Iter: 298000 | Total Loss: 0.002964 | Recon Loss: 0.002460 | Commit Loss: 0.001007 | Perplexity: 684.231999
2025-09-14 19:38:31,882 Stage: Train 0.5 | Epoch: 98 | Iter: 298200 | Total Loss: 0.003015 | Recon Loss: 0.002505 | Commit Loss: 0.001020 | Perplexity: 683.684529
2025-09-14 19:38:40,022 Stage: Train 0.5 | Epoch: 98 | Iter: 298400 | Total Loss: 0.002995 | Recon Loss: 0.002485 | Commit Loss: 0.001021 | Perplexity: 683.024998
2025-09-14 19:38:48,157 Stage: Train 0.5 | Epoch: 98 | Iter: 298600 | Total Loss: 0.003002 | Recon Loss: 0.002492 | Commit Loss: 0.001020 | Perplexity: 679.807023
2025-09-14 19:38:56,280 Stage: Train 0.5 | Epoch: 98 | Iter: 298800 | Total Loss: 0.002979 | Recon Loss: 0.002469 | Commit Loss: 0.001021 | Perplexity: 683.054289
2025-09-14 19:39:04,403 Stage: Train 0.5 | Epoch: 98 | Iter: 299000 | Total Loss: 0.003056 | Recon Loss: 0.002547 | Commit Loss: 0.001018 | Perplexity: 681.734474
2025-09-14 19:39:12,530 Stage: Train 0.5 | Epoch: 98 | Iter: 299200 | Total Loss: 0.002982 | Recon Loss: 0.002477 | Commit Loss: 0.001009 | Perplexity: 681.775866
2025-09-14 19:39:20,637 Stage: Train 0.5 | Epoch: 98 | Iter: 299400 | Total Loss: 0.002971 | Recon Loss: 0.002467 | Commit Loss: 0.001007 | Perplexity: 681.339057
2025-09-14 19:39:28,756 Stage: Train 0.5 | Epoch: 98 | Iter: 299600 | Total Loss: 0.002984 | Recon Loss: 0.002472 | Commit Loss: 0.001023 | Perplexity: 683.695219
2025-09-14 19:39:36,702 Stage: Train 0.5 | Epoch: 98 | Iter: 299800 | Total Loss: 0.003069 | Recon Loss: 0.002557 | Commit Loss: 0.001024 | Perplexity: 681.719955
2025-09-14 19:39:44,541 Stage: Train 0.5 | Epoch: 98 | Iter: 300000 | Total Loss: 0.002975 | Recon Loss: 0.002473 | Commit Loss: 0.001003 | Perplexity: 679.440391
2025-09-14 19:39:44,541 Saving model at iteration 300000
2025-09-14 19:39:45,012 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_99_step_300000
2025-09-14 19:39:45,150 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_99_step_300000/pytorch_model.bin
2025-09-14 19:39:45,392 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_99_step_300000/optimizer.bin
2025-09-14 19:39:45,393 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_99_step_300000/scheduler.bin
2025-09-14 19:39:45,393 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_99_step_300000/random_states_0.pkl
2025-09-14 19:39:53,275 Stage: Train 0.5 | Epoch: 98 | Iter: 300200 | Total Loss: 0.002948 | Recon Loss: 0.002442 | Commit Loss: 0.001012 | Perplexity: 680.858920
2025-09-14 19:40:01,266 Stage: Train 0.5 | Epoch: 98 | Iter: 300400 | Total Loss: 0.002993 | Recon Loss: 0.002484 | Commit Loss: 0.001018 | Perplexity: 682.613466
2025-09-14 19:40:09,431 Stage: Train 0.5 | Epoch: 98 | Iter: 300600 | Total Loss: 0.002991 | Recon Loss: 0.002482 | Commit Loss: 0.001019 | Perplexity: 680.362844
Trainning Epoch:  60%|██████    | 99/165 [3:17:26<2:12:21, 120.32s/it]2025-09-14 19:40:17,550 Stage: Train 0.5 | Epoch: 99 | Iter: 300800 | Total Loss: 0.002960 | Recon Loss: 0.002451 | Commit Loss: 0.001020 | Perplexity: 681.300363
2025-09-14 19:40:25,722 Stage: Train 0.5 | Epoch: 99 | Iter: 301000 | Total Loss: 0.002962 | Recon Loss: 0.002456 | Commit Loss: 0.001013 | Perplexity: 685.048753
2025-09-14 19:40:33,842 Stage: Train 0.5 | Epoch: 99 | Iter: 301200 | Total Loss: 0.002969 | Recon Loss: 0.002466 | Commit Loss: 0.001007 | Perplexity: 679.029657
2025-09-14 19:40:41,991 Stage: Train 0.5 | Epoch: 99 | Iter: 301400 | Total Loss: 0.003025 | Recon Loss: 0.002518 | Commit Loss: 0.001015 | Perplexity: 684.258580
2025-09-14 19:40:49,930 Stage: Train 0.5 | Epoch: 99 | Iter: 301600 | Total Loss: 0.002970 | Recon Loss: 0.002462 | Commit Loss: 0.001014 | Perplexity: 680.373104
2025-09-14 19:40:57,778 Stage: Train 0.5 | Epoch: 99 | Iter: 301800 | Total Loss: 0.002988 | Recon Loss: 0.002479 | Commit Loss: 0.001019 | Perplexity: 683.394718
2025-09-14 19:41:05,852 Stage: Train 0.5 | Epoch: 99 | Iter: 302000 | Total Loss: 0.002994 | Recon Loss: 0.002486 | Commit Loss: 0.001015 | Perplexity: 684.175784
2025-09-14 19:41:13,944 Stage: Train 0.5 | Epoch: 99 | Iter: 302200 | Total Loss: 0.003028 | Recon Loss: 0.002521 | Commit Loss: 0.001013 | Perplexity: 680.091015
2025-09-14 19:41:22,095 Stage: Train 0.5 | Epoch: 99 | Iter: 302400 | Total Loss: 0.002935 | Recon Loss: 0.002431 | Commit Loss: 0.001008 | Perplexity: 685.404306
2025-09-14 19:41:30,244 Stage: Train 0.5 | Epoch: 99 | Iter: 302600 | Total Loss: 0.002993 | Recon Loss: 0.002483 | Commit Loss: 0.001020 | Perplexity: 682.925091
2025-09-14 19:41:38,378 Stage: Train 0.5 | Epoch: 99 | Iter: 302800 | Total Loss: 0.002953 | Recon Loss: 0.002448 | Commit Loss: 0.001011 | Perplexity: 684.368390
2025-09-14 19:41:46,526 Stage: Train 0.5 | Epoch: 99 | Iter: 303000 | Total Loss: 0.002972 | Recon Loss: 0.002465 | Commit Loss: 0.001015 | Perplexity: 683.255199
2025-09-14 19:41:54,538 Stage: Train 0.5 | Epoch: 99 | Iter: 303200 | Total Loss: 0.002961 | Recon Loss: 0.002453 | Commit Loss: 0.001016 | Perplexity: 683.982672
2025-09-14 19:42:02,328 Stage: Train 0.5 | Epoch: 99 | Iter: 303400 | Total Loss: 0.002963 | Recon Loss: 0.002458 | Commit Loss: 0.001009 | Perplexity: 682.346954
2025-09-14 19:42:10,129 Stage: Train 0.5 | Epoch: 99 | Iter: 303600 | Total Loss: 0.002956 | Recon Loss: 0.002455 | Commit Loss: 0.001004 | Perplexity: 681.448362
2025-09-14 19:42:17,971 Stage: Train 0.5 | Epoch: 99 | Iter: 303800 | Total Loss: 0.002979 | Recon Loss: 0.002471 | Commit Loss: 0.001016 | Perplexity: 681.838446
Trainning Epoch:  61%|██████    | 100/165 [3:19:28<2:10:52, 120.81s/it]2025-09-14 19:42:25,790 Stage: Train 0.5 | Epoch: 100 | Iter: 304000 | Total Loss: 0.003000 | Recon Loss: 0.002494 | Commit Loss: 0.001012 | Perplexity: 680.964037
2025-09-14 19:42:33,585 Stage: Train 0.5 | Epoch: 100 | Iter: 304200 | Total Loss: 0.002958 | Recon Loss: 0.002451 | Commit Loss: 0.001015 | Perplexity: 683.850069
2025-09-14 19:42:41,405 Stage: Train 0.5 | Epoch: 100 | Iter: 304400 | Total Loss: 0.002932 | Recon Loss: 0.002432 | Commit Loss: 0.001000 | Perplexity: 681.917651
2025-09-14 19:42:49,166 Stage: Train 0.5 | Epoch: 100 | Iter: 304600 | Total Loss: 0.002977 | Recon Loss: 0.002477 | Commit Loss: 0.001001 | Perplexity: 682.870365
2025-09-14 19:42:56,966 Stage: Train 0.5 | Epoch: 100 | Iter: 304800 | Total Loss: 0.002969 | Recon Loss: 0.002462 | Commit Loss: 0.001013 | Perplexity: 682.994900
2025-09-14 19:43:04,721 Stage: Train 0.5 | Epoch: 100 | Iter: 305000 | Total Loss: 0.002922 | Recon Loss: 0.002419 | Commit Loss: 0.001007 | Perplexity: 679.703528
2025-09-14 19:43:12,536 Stage: Train 0.5 | Epoch: 100 | Iter: 305200 | Total Loss: 0.002941 | Recon Loss: 0.002435 | Commit Loss: 0.001012 | Perplexity: 682.511265
2025-09-14 19:43:20,350 Stage: Train 0.5 | Epoch: 100 | Iter: 305400 | Total Loss: 0.002975 | Recon Loss: 0.002467 | Commit Loss: 0.001017 | Perplexity: 684.970129
2025-09-14 19:43:28,202 Stage: Train 0.5 | Epoch: 100 | Iter: 305600 | Total Loss: 0.002942 | Recon Loss: 0.002437 | Commit Loss: 0.001011 | Perplexity: 681.029424
2025-09-14 19:43:36,001 Stage: Train 0.5 | Epoch: 100 | Iter: 305800 | Total Loss: 0.002949 | Recon Loss: 0.002445 | Commit Loss: 0.001009 | Perplexity: 682.064367
2025-09-14 19:43:43,732 Stage: Train 0.5 | Epoch: 100 | Iter: 306000 | Total Loss: 0.002985 | Recon Loss: 0.002480 | Commit Loss: 0.001009 | Perplexity: 685.391270
2025-09-14 19:43:51,506 Stage: Train 0.5 | Epoch: 100 | Iter: 306200 | Total Loss: 0.002997 | Recon Loss: 0.002490 | Commit Loss: 0.001013 | Perplexity: 683.178226
2025-09-14 19:43:59,255 Stage: Train 0.5 | Epoch: 100 | Iter: 306400 | Total Loss: 0.002997 | Recon Loss: 0.002493 | Commit Loss: 0.001008 | Perplexity: 683.343492
2025-09-14 19:44:07,434 Stage: Train 0.5 | Epoch: 100 | Iter: 306600 | Total Loss: 0.002940 | Recon Loss: 0.002439 | Commit Loss: 0.001003 | Perplexity: 680.636746
2025-09-14 19:44:15,614 Stage: Train 0.5 | Epoch: 100 | Iter: 306800 | Total Loss: 0.003009 | Recon Loss: 0.002503 | Commit Loss: 0.001014 | Perplexity: 683.937350
Trainning Epoch:  61%|██████    | 101/165 [3:21:27<2:08:21, 120.34s/it]2025-09-14 19:44:23,781 Stage: Train 0.5 | Epoch: 101 | Iter: 307000 | Total Loss: 0.002971 | Recon Loss: 0.002469 | Commit Loss: 0.001004 | Perplexity: 681.447675
2025-09-14 19:44:31,741 Stage: Train 0.5 | Epoch: 101 | Iter: 307200 | Total Loss: 0.002954 | Recon Loss: 0.002447 | Commit Loss: 0.001014 | Perplexity: 686.603627
2025-09-14 19:44:39,509 Stage: Train 0.5 | Epoch: 101 | Iter: 307400 | Total Loss: 0.003002 | Recon Loss: 0.002495 | Commit Loss: 0.001013 | Perplexity: 683.620454
2025-09-14 19:44:47,274 Stage: Train 0.5 | Epoch: 101 | Iter: 307600 | Total Loss: 0.002974 | Recon Loss: 0.002473 | Commit Loss: 0.001002 | Perplexity: 684.009476
2025-09-14 19:44:55,045 Stage: Train 0.5 | Epoch: 101 | Iter: 307800 | Total Loss: 0.002925 | Recon Loss: 0.002421 | Commit Loss: 0.001006 | Perplexity: 684.663441
2025-09-14 19:45:02,793 Stage: Train 0.5 | Epoch: 101 | Iter: 308000 | Total Loss: 0.002963 | Recon Loss: 0.002462 | Commit Loss: 0.001001 | Perplexity: 679.553507
2025-09-14 19:45:10,545 Stage: Train 0.5 | Epoch: 101 | Iter: 308200 | Total Loss: 0.002990 | Recon Loss: 0.002490 | Commit Loss: 0.000999 | Perplexity: 682.957125
2025-09-14 19:45:18,282 Stage: Train 0.5 | Epoch: 101 | Iter: 308400 | Total Loss: 0.002925 | Recon Loss: 0.002426 | Commit Loss: 0.000999 | Perplexity: 681.582739
2025-09-14 19:45:26,025 Stage: Train 0.5 | Epoch: 101 | Iter: 308600 | Total Loss: 0.002940 | Recon Loss: 0.002434 | Commit Loss: 0.001011 | Perplexity: 685.936416
2025-09-14 19:45:33,802 Stage: Train 0.5 | Epoch: 101 | Iter: 308800 | Total Loss: 0.002949 | Recon Loss: 0.002451 | Commit Loss: 0.000996 | Perplexity: 679.671626
2025-09-14 19:45:41,529 Stage: Train 0.5 | Epoch: 101 | Iter: 309000 | Total Loss: 0.002934 | Recon Loss: 0.002429 | Commit Loss: 0.001011 | Perplexity: 683.519792
2025-09-14 19:45:49,310 Stage: Train 0.5 | Epoch: 101 | Iter: 309200 | Total Loss: 0.002942 | Recon Loss: 0.002439 | Commit Loss: 0.001007 | Perplexity: 682.883379
2025-09-14 19:45:57,046 Stage: Train 0.5 | Epoch: 101 | Iter: 309400 | Total Loss: 0.002941 | Recon Loss: 0.002438 | Commit Loss: 0.001005 | Perplexity: 682.402538
2025-09-14 19:46:04,845 Stage: Train 0.5 | Epoch: 101 | Iter: 309600 | Total Loss: 0.002940 | Recon Loss: 0.002435 | Commit Loss: 0.001010 | Perplexity: 683.339337
2025-09-14 19:46:12,585 Stage: Train 0.5 | Epoch: 101 | Iter: 309800 | Total Loss: 0.002970 | Recon Loss: 0.002468 | Commit Loss: 0.001004 | Perplexity: 683.135738
Trainning Epoch:  62%|██████▏   | 102/165 [3:23:26<2:05:43, 119.74s/it]2025-09-14 19:46:20,338 Stage: Train 0.5 | Epoch: 102 | Iter: 310000 | Total Loss: 0.002930 | Recon Loss: 0.002427 | Commit Loss: 0.001006 | Perplexity: 682.363345
2025-09-14 19:46:28,272 Stage: Train 0.5 | Epoch: 102 | Iter: 310200 | Total Loss: 0.002970 | Recon Loss: 0.002471 | Commit Loss: 0.000997 | Perplexity: 682.408195
2025-09-14 19:46:36,378 Stage: Train 0.5 | Epoch: 102 | Iter: 310400 | Total Loss: 0.002947 | Recon Loss: 0.002446 | Commit Loss: 0.001001 | Perplexity: 681.868116
2025-09-14 19:46:44,457 Stage: Train 0.5 | Epoch: 102 | Iter: 310600 | Total Loss: 0.002923 | Recon Loss: 0.002427 | Commit Loss: 0.000993 | Perplexity: 682.996506
2025-09-14 19:46:52,209 Stage: Train 0.5 | Epoch: 102 | Iter: 310800 | Total Loss: 0.002949 | Recon Loss: 0.002446 | Commit Loss: 0.001005 | Perplexity: 684.014900
2025-09-14 19:46:59,906 Stage: Train 0.5 | Epoch: 102 | Iter: 311000 | Total Loss: 0.002929 | Recon Loss: 0.002426 | Commit Loss: 0.001005 | Perplexity: 683.622000
2025-09-14 19:47:07,621 Stage: Train 0.5 | Epoch: 102 | Iter: 311200 | Total Loss: 0.002945 | Recon Loss: 0.002443 | Commit Loss: 0.001004 | Perplexity: 683.027520
2025-09-14 19:47:15,329 Stage: Train 0.5 | Epoch: 102 | Iter: 311400 | Total Loss: 0.002951 | Recon Loss: 0.002456 | Commit Loss: 0.000990 | Perplexity: 680.515954
2025-09-14 19:47:23,053 Stage: Train 0.5 | Epoch: 102 | Iter: 311600 | Total Loss: 0.002910 | Recon Loss: 0.002413 | Commit Loss: 0.000993 | Perplexity: 683.216270
2025-09-14 19:47:30,778 Stage: Train 0.5 | Epoch: 102 | Iter: 311800 | Total Loss: 0.002907 | Recon Loss: 0.002408 | Commit Loss: 0.000998 | Perplexity: 684.478011
2025-09-14 19:47:38,458 Stage: Train 0.5 | Epoch: 102 | Iter: 312000 | Total Loss: 0.002960 | Recon Loss: 0.002453 | Commit Loss: 0.001013 | Perplexity: 684.345013
2025-09-14 19:47:46,137 Stage: Train 0.5 | Epoch: 102 | Iter: 312200 | Total Loss: 0.002974 | Recon Loss: 0.002476 | Commit Loss: 0.000996 | Perplexity: 682.251742
2025-09-14 19:47:53,822 Stage: Train 0.5 | Epoch: 102 | Iter: 312400 | Total Loss: 0.002935 | Recon Loss: 0.002432 | Commit Loss: 0.001006 | Perplexity: 685.082656
2025-09-14 19:48:01,543 Stage: Train 0.5 | Epoch: 102 | Iter: 312600 | Total Loss: 0.002947 | Recon Loss: 0.002442 | Commit Loss: 0.001010 | Perplexity: 686.439211
2025-09-14 19:48:09,289 Stage: Train 0.5 | Epoch: 102 | Iter: 312800 | Total Loss: 0.002969 | Recon Loss: 0.002466 | Commit Loss: 0.001006 | Perplexity: 686.126931
Trainning Epoch:  62%|██████▏   | 103/165 [3:25:24<2:03:13, 119.26s/it]2025-09-14 19:48:17,025 Stage: Train 0.5 | Epoch: 103 | Iter: 313000 | Total Loss: 0.002901 | Recon Loss: 0.002402 | Commit Loss: 0.000998 | Perplexity: 684.029393
2025-09-14 19:48:24,753 Stage: Train 0.5 | Epoch: 103 | Iter: 313200 | Total Loss: 0.002952 | Recon Loss: 0.002451 | Commit Loss: 0.001003 | Perplexity: 681.211205
2025-09-14 19:48:32,481 Stage: Train 0.5 | Epoch: 103 | Iter: 313400 | Total Loss: 0.002959 | Recon Loss: 0.002461 | Commit Loss: 0.000998 | Perplexity: 684.087068
2025-09-14 19:48:40,169 Stage: Train 0.5 | Epoch: 103 | Iter: 313600 | Total Loss: 0.002926 | Recon Loss: 0.002425 | Commit Loss: 0.001002 | Perplexity: 683.598708
2025-09-14 19:48:47,875 Stage: Train 0.5 | Epoch: 103 | Iter: 313800 | Total Loss: 0.002907 | Recon Loss: 0.002413 | Commit Loss: 0.000987 | Perplexity: 684.705061
2025-09-14 19:48:55,613 Stage: Train 0.5 | Epoch: 103 | Iter: 314000 | Total Loss: 0.002923 | Recon Loss: 0.002423 | Commit Loss: 0.000998 | Perplexity: 683.274187
2025-09-14 19:49:03,321 Stage: Train 0.5 | Epoch: 103 | Iter: 314200 | Total Loss: 0.002942 | Recon Loss: 0.002443 | Commit Loss: 0.000996 | Perplexity: 683.737657
2025-09-14 19:49:11,057 Stage: Train 0.5 | Epoch: 103 | Iter: 314400 | Total Loss: 0.002938 | Recon Loss: 0.002441 | Commit Loss: 0.000993 | Perplexity: 686.586313
2025-09-14 19:49:18,753 Stage: Train 0.5 | Epoch: 103 | Iter: 314600 | Total Loss: 0.002962 | Recon Loss: 0.002464 | Commit Loss: 0.000997 | Perplexity: 685.045800
2025-09-14 19:49:26,449 Stage: Train 0.5 | Epoch: 103 | Iter: 314800 | Total Loss: 0.002910 | Recon Loss: 0.002411 | Commit Loss: 0.000997 | Perplexity: 685.079444
2025-09-14 19:49:34,182 Stage: Train 0.5 | Epoch: 103 | Iter: 315000 | Total Loss: 0.002945 | Recon Loss: 0.002445 | Commit Loss: 0.001002 | Perplexity: 683.607124
2025-09-14 19:49:41,895 Stage: Train 0.5 | Epoch: 103 | Iter: 315200 | Total Loss: 0.002922 | Recon Loss: 0.002426 | Commit Loss: 0.000991 | Perplexity: 681.138734
2025-09-14 19:49:49,626 Stage: Train 0.5 | Epoch: 103 | Iter: 315400 | Total Loss: 0.003014 | Recon Loss: 0.002519 | Commit Loss: 0.000991 | Perplexity: 683.559759
2025-09-14 19:49:57,337 Stage: Train 0.5 | Epoch: 103 | Iter: 315600 | Total Loss: 0.002845 | Recon Loss: 0.002349 | Commit Loss: 0.000993 | Perplexity: 679.213036
2025-09-14 19:50:05,037 Stage: Train 0.5 | Epoch: 103 | Iter: 315800 | Total Loss: 0.002957 | Recon Loss: 0.002463 | Commit Loss: 0.000989 | Perplexity: 681.266393
Trainning Epoch:  63%|██████▎   | 104/165 [3:27:21<2:00:37, 118.64s/it]2025-09-14 19:50:12,766 Stage: Train 0.5 | Epoch: 104 | Iter: 316000 | Total Loss: 0.002897 | Recon Loss: 0.002401 | Commit Loss: 0.000992 | Perplexity: 683.520158
2025-09-14 19:50:20,489 Stage: Train 0.5 | Epoch: 104 | Iter: 316200 | Total Loss: 0.002907 | Recon Loss: 0.002412 | Commit Loss: 0.000989 | Perplexity: 682.326593
2025-09-14 19:50:28,221 Stage: Train 0.5 | Epoch: 104 | Iter: 316400 | Total Loss: 0.002992 | Recon Loss: 0.002492 | Commit Loss: 0.001000 | Perplexity: 684.927818
2025-09-14 19:50:35,933 Stage: Train 0.5 | Epoch: 104 | Iter: 316600 | Total Loss: 0.002895 | Recon Loss: 0.002399 | Commit Loss: 0.000990 | Perplexity: 681.090429
2025-09-14 19:50:43,653 Stage: Train 0.5 | Epoch: 104 | Iter: 316800 | Total Loss: 0.002878 | Recon Loss: 0.002387 | Commit Loss: 0.000983 | Perplexity: 685.791171
2025-09-14 19:50:51,383 Stage: Train 0.5 | Epoch: 104 | Iter: 317000 | Total Loss: 0.002946 | Recon Loss: 0.002452 | Commit Loss: 0.000987 | Perplexity: 683.443693
2025-09-14 19:50:59,106 Stage: Train 0.5 | Epoch: 104 | Iter: 317200 | Total Loss: 0.002922 | Recon Loss: 0.002428 | Commit Loss: 0.000987 | Perplexity: 682.279113
2025-09-14 19:51:06,817 Stage: Train 0.5 | Epoch: 104 | Iter: 317400 | Total Loss: 0.002881 | Recon Loss: 0.002389 | Commit Loss: 0.000983 | Perplexity: 683.224597
2025-09-14 19:51:14,541 Stage: Train 0.5 | Epoch: 104 | Iter: 317600 | Total Loss: 0.002931 | Recon Loss: 0.002438 | Commit Loss: 0.000985 | Perplexity: 680.972134
2025-09-14 19:51:22,285 Stage: Train 0.5 | Epoch: 104 | Iter: 317800 | Total Loss: 0.002984 | Recon Loss: 0.002492 | Commit Loss: 0.000984 | Perplexity: 684.548878
2025-09-14 19:51:29,991 Stage: Train 0.5 | Epoch: 104 | Iter: 318000 | Total Loss: 0.002903 | Recon Loss: 0.002411 | Commit Loss: 0.000985 | Perplexity: 684.917040
2025-09-14 19:51:37,701 Stage: Train 0.5 | Epoch: 104 | Iter: 318200 | Total Loss: 0.002903 | Recon Loss: 0.002409 | Commit Loss: 0.000986 | Perplexity: 681.584574
2025-09-14 19:51:45,405 Stage: Train 0.5 | Epoch: 104 | Iter: 318400 | Total Loss: 0.002939 | Recon Loss: 0.002448 | Commit Loss: 0.000983 | Perplexity: 679.241634
2025-09-14 19:51:53,119 Stage: Train 0.5 | Epoch: 104 | Iter: 318600 | Total Loss: 0.002896 | Recon Loss: 0.002400 | Commit Loss: 0.000992 | Perplexity: 683.067854
2025-09-14 19:52:00,856 Stage: Train 0.5 | Epoch: 104 | Iter: 318800 | Total Loss: 0.002951 | Recon Loss: 0.002458 | Commit Loss: 0.000987 | Perplexity: 682.367980
Trainning Epoch:  64%|██████▎   | 105/165 [3:29:18<1:58:14, 118.24s/it]2025-09-14 19:52:08,598 Stage: Train 0.5 | Epoch: 105 | Iter: 319000 | Total Loss: 0.002898 | Recon Loss: 0.002397 | Commit Loss: 0.001003 | Perplexity: 686.170249
2025-09-14 19:52:16,286 Stage: Train 0.5 | Epoch: 105 | Iter: 319200 | Total Loss: 0.002900 | Recon Loss: 0.002411 | Commit Loss: 0.000978 | Perplexity: 680.670931
2025-09-14 19:52:23,986 Stage: Train 0.5 | Epoch: 105 | Iter: 319400 | Total Loss: 0.002908 | Recon Loss: 0.002413 | Commit Loss: 0.000991 | Perplexity: 683.427471
2025-09-14 19:52:31,657 Stage: Train 0.5 | Epoch: 105 | Iter: 319600 | Total Loss: 0.002930 | Recon Loss: 0.002436 | Commit Loss: 0.000989 | Perplexity: 684.587099
2025-09-14 19:52:39,353 Stage: Train 0.5 | Epoch: 105 | Iter: 319800 | Total Loss: 0.002929 | Recon Loss: 0.002437 | Commit Loss: 0.000984 | Perplexity: 681.649576
2025-09-14 19:52:47,045 Stage: Train 0.5 | Epoch: 105 | Iter: 320000 | Total Loss: 0.002917 | Recon Loss: 0.002424 | Commit Loss: 0.000986 | Perplexity: 682.287879
2025-09-14 19:52:47,045 Saving model at iteration 320000
2025-09-14 19:52:47,189 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_106_step_320000
2025-09-14 19:52:47,321 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_106_step_320000/pytorch_model.bin
2025-09-14 19:52:47,571 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_106_step_320000/optimizer.bin
2025-09-14 19:52:47,571 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_106_step_320000/scheduler.bin
2025-09-14 19:52:47,572 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_106_step_320000/random_states_0.pkl
2025-09-14 19:52:55,451 Stage: Train 0.5 | Epoch: 105 | Iter: 320200 | Total Loss: 0.002955 | Recon Loss: 0.002464 | Commit Loss: 0.000982 | Perplexity: 682.719396
2025-09-14 19:53:03,130 Stage: Train 0.5 | Epoch: 105 | Iter: 320400 | Total Loss: 0.002879 | Recon Loss: 0.002391 | Commit Loss: 0.000976 | Perplexity: 681.084292
2025-09-14 19:53:10,842 Stage: Train 0.5 | Epoch: 105 | Iter: 320600 | Total Loss: 0.002902 | Recon Loss: 0.002408 | Commit Loss: 0.000988 | Perplexity: 683.836714
2025-09-14 19:53:18,617 Stage: Train 0.5 | Epoch: 105 | Iter: 320800 | Total Loss: 0.002882 | Recon Loss: 0.002391 | Commit Loss: 0.000983 | Perplexity: 682.682174
2025-09-14 19:53:26,345 Stage: Train 0.5 | Epoch: 105 | Iter: 321000 | Total Loss: 0.002941 | Recon Loss: 0.002445 | Commit Loss: 0.000991 | Perplexity: 682.159738
2025-09-14 19:53:34,097 Stage: Train 0.5 | Epoch: 105 | Iter: 321200 | Total Loss: 0.002893 | Recon Loss: 0.002405 | Commit Loss: 0.000976 | Perplexity: 681.573448
2025-09-14 19:53:41,833 Stage: Train 0.5 | Epoch: 105 | Iter: 321400 | Total Loss: 0.002913 | Recon Loss: 0.002424 | Commit Loss: 0.000977 | Perplexity: 682.554554
2025-09-14 19:53:49,566 Stage: Train 0.5 | Epoch: 105 | Iter: 321600 | Total Loss: 0.002896 | Recon Loss: 0.002405 | Commit Loss: 0.000983 | Perplexity: 683.378490
2025-09-14 19:53:57,362 Stage: Train 0.5 | Epoch: 105 | Iter: 321800 | Total Loss: 0.002939 | Recon Loss: 0.002452 | Commit Loss: 0.000974 | Perplexity: 680.808968
2025-09-14 19:54:05,161 Stage: Train 0.5 | Epoch: 105 | Iter: 322000 | Total Loss: 0.002896 | Recon Loss: 0.002405 | Commit Loss: 0.000982 | Perplexity: 683.822249
Trainning Epoch:  64%|██████▍   | 106/165 [3:31:16<1:56:13, 118.19s/it]2025-09-14 19:54:12,924 Stage: Train 0.5 | Epoch: 106 | Iter: 322200 | Total Loss: 0.002949 | Recon Loss: 0.002456 | Commit Loss: 0.000985 | Perplexity: 681.232430
2025-09-14 19:54:20,662 Stage: Train 0.5 | Epoch: 106 | Iter: 322400 | Total Loss: 0.002914 | Recon Loss: 0.002424 | Commit Loss: 0.000980 | Perplexity: 681.368275
2025-09-14 19:54:28,489 Stage: Train 0.5 | Epoch: 106 | Iter: 322600 | Total Loss: 0.002911 | Recon Loss: 0.002429 | Commit Loss: 0.000964 | Perplexity: 679.722999
2025-09-14 19:54:36,313 Stage: Train 0.5 | Epoch: 106 | Iter: 322800 | Total Loss: 0.002929 | Recon Loss: 0.002440 | Commit Loss: 0.000977 | Perplexity: 683.818734
2025-09-14 19:54:44,121 Stage: Train 0.5 | Epoch: 106 | Iter: 323000 | Total Loss: 0.002890 | Recon Loss: 0.002398 | Commit Loss: 0.000985 | Perplexity: 685.004874
2025-09-14 19:54:51,923 Stage: Train 0.5 | Epoch: 106 | Iter: 323200 | Total Loss: 0.002852 | Recon Loss: 0.002366 | Commit Loss: 0.000974 | Perplexity: 685.874407
2025-09-14 19:54:59,687 Stage: Train 0.5 | Epoch: 106 | Iter: 323400 | Total Loss: 0.002936 | Recon Loss: 0.002447 | Commit Loss: 0.000979 | Perplexity: 682.715924
2025-09-14 19:55:07,509 Stage: Train 0.5 | Epoch: 106 | Iter: 323600 | Total Loss: 0.002898 | Recon Loss: 0.002406 | Commit Loss: 0.000984 | Perplexity: 684.646331
2025-09-14 19:55:15,357 Stage: Train 0.5 | Epoch: 106 | Iter: 323800 | Total Loss: 0.002891 | Recon Loss: 0.002403 | Commit Loss: 0.000977 | Perplexity: 683.964500
2025-09-14 19:55:23,186 Stage: Train 0.5 | Epoch: 106 | Iter: 324000 | Total Loss: 0.002886 | Recon Loss: 0.002398 | Commit Loss: 0.000977 | Perplexity: 683.081473
2025-09-14 19:55:31,021 Stage: Train 0.5 | Epoch: 106 | Iter: 324200 | Total Loss: 0.002901 | Recon Loss: 0.002410 | Commit Loss: 0.000983 | Perplexity: 682.452171
2025-09-14 19:55:38,870 Stage: Train 0.5 | Epoch: 106 | Iter: 324400 | Total Loss: 0.002938 | Recon Loss: 0.002446 | Commit Loss: 0.000985 | Perplexity: 683.590791
2025-09-14 19:55:46,722 Stage: Train 0.5 | Epoch: 106 | Iter: 324600 | Total Loss: 0.002898 | Recon Loss: 0.002409 | Commit Loss: 0.000978 | Perplexity: 684.045865
2025-09-14 19:55:54,574 Stage: Train 0.5 | Epoch: 106 | Iter: 324800 | Total Loss: 0.002933 | Recon Loss: 0.002439 | Commit Loss: 0.000987 | Perplexity: 682.820790
2025-09-14 19:56:02,433 Stage: Train 0.5 | Epoch: 106 | Iter: 325000 | Total Loss: 0.002902 | Recon Loss: 0.002413 | Commit Loss: 0.000979 | Perplexity: 684.682095
Trainning Epoch:  65%|██████▍   | 107/165 [3:33:15<1:54:24, 118.35s/it]2025-09-14 19:56:10,212 Stage: Train 0.5 | Epoch: 107 | Iter: 325200 | Total Loss: 0.002893 | Recon Loss: 0.002402 | Commit Loss: 0.000982 | Perplexity: 683.965867
2025-09-14 19:56:17,926 Stage: Train 0.5 | Epoch: 107 | Iter: 325400 | Total Loss: 0.002919 | Recon Loss: 0.002431 | Commit Loss: 0.000975 | Perplexity: 682.667214
2025-09-14 19:56:25,633 Stage: Train 0.5 | Epoch: 107 | Iter: 325600 | Total Loss: 0.002904 | Recon Loss: 0.002412 | Commit Loss: 0.000984 | Perplexity: 684.882479
2025-09-14 19:56:33,361 Stage: Train 0.5 | Epoch: 107 | Iter: 325800 | Total Loss: 0.002874 | Recon Loss: 0.002393 | Commit Loss: 0.000963 | Perplexity: 683.803322
2025-09-14 19:56:41,090 Stage: Train 0.5 | Epoch: 107 | Iter: 326000 | Total Loss: 0.002916 | Recon Loss: 0.002427 | Commit Loss: 0.000978 | Perplexity: 684.699469
2025-09-14 19:56:48,829 Stage: Train 0.5 | Epoch: 107 | Iter: 326200 | Total Loss: 0.002902 | Recon Loss: 0.002414 | Commit Loss: 0.000976 | Perplexity: 684.591497
2025-09-14 19:56:56,555 Stage: Train 0.5 | Epoch: 107 | Iter: 326400 | Total Loss: 0.002895 | Recon Loss: 0.002408 | Commit Loss: 0.000975 | Perplexity: 682.651117
2025-09-14 19:57:04,298 Stage: Train 0.5 | Epoch: 107 | Iter: 326600 | Total Loss: 0.002862 | Recon Loss: 0.002375 | Commit Loss: 0.000974 | Perplexity: 685.994215
2025-09-14 19:57:12,041 Stage: Train 0.5 | Epoch: 107 | Iter: 326800 | Total Loss: 0.002877 | Recon Loss: 0.002385 | Commit Loss: 0.000985 | Perplexity: 683.550362
2025-09-14 19:57:19,786 Stage: Train 0.5 | Epoch: 107 | Iter: 327000 | Total Loss: 0.002867 | Recon Loss: 0.002378 | Commit Loss: 0.000979 | Perplexity: 686.049145
2025-09-14 19:57:27,522 Stage: Train 0.5 | Epoch: 107 | Iter: 327200 | Total Loss: 0.002885 | Recon Loss: 0.002394 | Commit Loss: 0.000984 | Perplexity: 685.767895
2025-09-14 19:57:35,277 Stage: Train 0.5 | Epoch: 107 | Iter: 327400 | Total Loss: 0.002857 | Recon Loss: 0.002371 | Commit Loss: 0.000971 | Perplexity: 679.860965
2025-09-14 19:57:42,994 Stage: Train 0.5 | Epoch: 107 | Iter: 327600 | Total Loss: 0.002946 | Recon Loss: 0.002459 | Commit Loss: 0.000974 | Perplexity: 683.657809
2025-09-14 19:57:50,705 Stage: Train 0.5 | Epoch: 107 | Iter: 327800 | Total Loss: 0.002867 | Recon Loss: 0.002378 | Commit Loss: 0.000978 | Perplexity: 685.512513
2025-09-14 19:57:58,425 Stage: Train 0.5 | Epoch: 107 | Iter: 328000 | Total Loss: 0.002882 | Recon Loss: 0.002392 | Commit Loss: 0.000980 | Perplexity: 686.423359
Trainning Epoch:  65%|██████▌   | 108/165 [3:35:13<1:52:10, 118.08s/it]2025-09-14 19:58:06,187 Stage: Train 0.5 | Epoch: 108 | Iter: 328200 | Total Loss: 0.002867 | Recon Loss: 0.002378 | Commit Loss: 0.000978 | Perplexity: 682.820758
2025-09-14 19:58:13,961 Stage: Train 0.5 | Epoch: 108 | Iter: 328400 | Total Loss: 0.002893 | Recon Loss: 0.002407 | Commit Loss: 0.000971 | Perplexity: 683.817217
2025-09-14 19:58:21,937 Stage: Train 0.5 | Epoch: 108 | Iter: 328600 | Total Loss: 0.002926 | Recon Loss: 0.002437 | Commit Loss: 0.000979 | Perplexity: 684.714225
2025-09-14 19:58:29,974 Stage: Train 0.5 | Epoch: 108 | Iter: 328800 | Total Loss: 0.002859 | Recon Loss: 0.002372 | Commit Loss: 0.000972 | Perplexity: 683.145964
2025-09-14 19:58:37,670 Stage: Train 0.5 | Epoch: 108 | Iter: 329000 | Total Loss: 0.002867 | Recon Loss: 0.002376 | Commit Loss: 0.000981 | Perplexity: 684.111406
2025-09-14 19:58:45,403 Stage: Train 0.5 | Epoch: 108 | Iter: 329200 | Total Loss: 0.002879 | Recon Loss: 0.002395 | Commit Loss: 0.000968 | Perplexity: 683.340808
2025-09-14 19:58:53,190 Stage: Train 0.5 | Epoch: 108 | Iter: 329400 | Total Loss: 0.002895 | Recon Loss: 0.002409 | Commit Loss: 0.000973 | Perplexity: 683.182728
2025-09-14 19:59:00,894 Stage: Train 0.5 | Epoch: 108 | Iter: 329600 | Total Loss: 0.002874 | Recon Loss: 0.002389 | Commit Loss: 0.000970 | Perplexity: 684.341082
2025-09-14 19:59:08,570 Stage: Train 0.5 | Epoch: 108 | Iter: 329800 | Total Loss: 0.002875 | Recon Loss: 0.002390 | Commit Loss: 0.000970 | Perplexity: 684.984959
2025-09-14 19:59:16,253 Stage: Train 0.5 | Epoch: 108 | Iter: 330000 | Total Loss: 0.002862 | Recon Loss: 0.002378 | Commit Loss: 0.000968 | Perplexity: 684.002245
2025-09-14 19:59:23,941 Stage: Train 0.5 | Epoch: 108 | Iter: 330200 | Total Loss: 0.002855 | Recon Loss: 0.002370 | Commit Loss: 0.000969 | Perplexity: 681.646858
2025-09-14 19:59:31,677 Stage: Train 0.5 | Epoch: 108 | Iter: 330400 | Total Loss: 0.002912 | Recon Loss: 0.002424 | Commit Loss: 0.000977 | Perplexity: 685.765324
2025-09-14 19:59:39,413 Stage: Train 0.5 | Epoch: 108 | Iter: 330600 | Total Loss: 0.002871 | Recon Loss: 0.002387 | Commit Loss: 0.000968 | Perplexity: 684.851061
2025-09-14 19:59:47,133 Stage: Train 0.5 | Epoch: 108 | Iter: 330800 | Total Loss: 0.002893 | Recon Loss: 0.002406 | Commit Loss: 0.000974 | Perplexity: 684.873793
2025-09-14 19:59:54,837 Stage: Train 0.5 | Epoch: 108 | Iter: 331000 | Total Loss: 0.002878 | Recon Loss: 0.002393 | Commit Loss: 0.000971 | Perplexity: 682.548771
Trainning Epoch:  66%|██████▌   | 109/165 [3:37:10<1:50:08, 118.01s/it]2025-09-14 20:00:02,550 Stage: Train 0.5 | Epoch: 109 | Iter: 331200 | Total Loss: 0.002865 | Recon Loss: 0.002381 | Commit Loss: 0.000969 | Perplexity: 681.335365
2025-09-14 20:00:10,257 Stage: Train 0.5 | Epoch: 109 | Iter: 331400 | Total Loss: 0.002887 | Recon Loss: 0.002405 | Commit Loss: 0.000966 | Perplexity: 685.378601
2025-09-14 20:00:17,941 Stage: Train 0.5 | Epoch: 109 | Iter: 331600 | Total Loss: 0.002870 | Recon Loss: 0.002386 | Commit Loss: 0.000967 | Perplexity: 683.568815
2025-09-14 20:00:25,637 Stage: Train 0.5 | Epoch: 109 | Iter: 331800 | Total Loss: 0.002870 | Recon Loss: 0.002389 | Commit Loss: 0.000961 | Perplexity: 682.707377
2025-09-14 20:00:33,309 Stage: Train 0.5 | Epoch: 109 | Iter: 332000 | Total Loss: 0.002874 | Recon Loss: 0.002397 | Commit Loss: 0.000955 | Perplexity: 683.122894
2025-09-14 20:00:41,057 Stage: Train 0.5 | Epoch: 109 | Iter: 332200 | Total Loss: 0.002864 | Recon Loss: 0.002384 | Commit Loss: 0.000959 | Perplexity: 682.111402
2025-09-14 20:00:48,729 Stage: Train 0.5 | Epoch: 109 | Iter: 332400 | Total Loss: 0.002881 | Recon Loss: 0.002398 | Commit Loss: 0.000966 | Perplexity: 683.247733
2025-09-14 20:00:56,450 Stage: Train 0.5 | Epoch: 109 | Iter: 332600 | Total Loss: 0.002882 | Recon Loss: 0.002397 | Commit Loss: 0.000970 | Perplexity: 683.166326
2025-09-14 20:01:04,149 Stage: Train 0.5 | Epoch: 109 | Iter: 332800 | Total Loss: 0.002855 | Recon Loss: 0.002368 | Commit Loss: 0.000974 | Perplexity: 686.044623
2025-09-14 20:01:11,853 Stage: Train 0.5 | Epoch: 109 | Iter: 333000 | Total Loss: 0.002854 | Recon Loss: 0.002372 | Commit Loss: 0.000962 | Perplexity: 682.794330
2025-09-14 20:01:19,554 Stage: Train 0.5 | Epoch: 109 | Iter: 333200 | Total Loss: 0.002869 | Recon Loss: 0.002381 | Commit Loss: 0.000977 | Perplexity: 685.671810
2025-09-14 20:01:27,233 Stage: Train 0.5 | Epoch: 109 | Iter: 333400 | Total Loss: 0.002920 | Recon Loss: 0.002432 | Commit Loss: 0.000975 | Perplexity: 685.409755
2025-09-14 20:01:34,922 Stage: Train 0.5 | Epoch: 109 | Iter: 333600 | Total Loss: 0.002953 | Recon Loss: 0.002465 | Commit Loss: 0.000976 | Perplexity: 685.099140
2025-09-14 20:01:42,657 Stage: Train 0.5 | Epoch: 109 | Iter: 333800 | Total Loss: 0.002861 | Recon Loss: 0.002377 | Commit Loss: 0.000969 | Perplexity: 685.192861
2025-09-14 20:01:50,382 Stage: Train 0.5 | Epoch: 109 | Iter: 334000 | Total Loss: 0.002892 | Recon Loss: 0.002409 | Commit Loss: 0.000967 | Perplexity: 683.955193
Trainning Epoch:  67%|██████▋   | 110/165 [3:39:08<1:47:54, 117.72s/it]2025-09-14 20:01:58,129 Stage: Train 0.5 | Epoch: 110 | Iter: 334200 | Total Loss: 0.002943 | Recon Loss: 0.002458 | Commit Loss: 0.000969 | Perplexity: 681.178788
2025-09-14 20:02:05,813 Stage: Train 0.5 | Epoch: 110 | Iter: 334400 | Total Loss: 0.002859 | Recon Loss: 0.002377 | Commit Loss: 0.000963 | Perplexity: 684.865874
2025-09-14 20:02:13,570 Stage: Train 0.5 | Epoch: 110 | Iter: 334600 | Total Loss: 0.002859 | Recon Loss: 0.002379 | Commit Loss: 0.000958 | Perplexity: 683.843199
2025-09-14 20:02:21,294 Stage: Train 0.5 | Epoch: 110 | Iter: 334800 | Total Loss: 0.002869 | Recon Loss: 0.002389 | Commit Loss: 0.000961 | Perplexity: 682.856063
2025-09-14 20:02:29,035 Stage: Train 0.5 | Epoch: 110 | Iter: 335000 | Total Loss: 0.002842 | Recon Loss: 0.002361 | Commit Loss: 0.000962 | Perplexity: 686.620628
2025-09-14 20:02:36,742 Stage: Train 0.5 | Epoch: 110 | Iter: 335200 | Total Loss: 0.002842 | Recon Loss: 0.002363 | Commit Loss: 0.000958 | Perplexity: 686.283280
2025-09-14 20:02:44,481 Stage: Train 0.5 | Epoch: 110 | Iter: 335400 | Total Loss: 0.002872 | Recon Loss: 0.002391 | Commit Loss: 0.000964 | Perplexity: 682.545381
2025-09-14 20:02:52,173 Stage: Train 0.5 | Epoch: 110 | Iter: 335600 | Total Loss: 0.002919 | Recon Loss: 0.002434 | Commit Loss: 0.000971 | Perplexity: 684.889870
2025-09-14 20:02:59,917 Stage: Train 0.5 | Epoch: 110 | Iter: 335800 | Total Loss: 0.002870 | Recon Loss: 0.002390 | Commit Loss: 0.000959 | Perplexity: 685.037053
2025-09-14 20:03:07,658 Stage: Train 0.5 | Epoch: 110 | Iter: 336000 | Total Loss: 0.002889 | Recon Loss: 0.002401 | Commit Loss: 0.000976 | Perplexity: 684.821136
2025-09-14 20:03:15,373 Stage: Train 0.5 | Epoch: 110 | Iter: 336200 | Total Loss: 0.002839 | Recon Loss: 0.002357 | Commit Loss: 0.000964 | Perplexity: 686.407101
2025-09-14 20:03:23,128 Stage: Train 0.5 | Epoch: 110 | Iter: 336400 | Total Loss: 0.002871 | Recon Loss: 0.002388 | Commit Loss: 0.000966 | Perplexity: 686.966680
2025-09-14 20:03:30,820 Stage: Train 0.5 | Epoch: 110 | Iter: 336600 | Total Loss: 0.002852 | Recon Loss: 0.002373 | Commit Loss: 0.000958 | Perplexity: 683.588182
2025-09-14 20:03:38,533 Stage: Train 0.5 | Epoch: 110 | Iter: 336800 | Total Loss: 0.002867 | Recon Loss: 0.002382 | Commit Loss: 0.000970 | Perplexity: 684.324655
2025-09-14 20:03:46,229 Stage: Train 0.5 | Epoch: 110 | Iter: 337000 | Total Loss: 0.002886 | Recon Loss: 0.002406 | Commit Loss: 0.000960 | Perplexity: 683.672259
2025-09-14 20:03:53,921 Stage: Train 0.5 | Epoch: 110 | Iter: 337200 | Total Loss: 0.002864 | Recon Loss: 0.002390 | Commit Loss: 0.000950 | Perplexity: 682.765644
Trainning Epoch:  67%|██████▋   | 111/165 [3:41:05<1:45:49, 117.59s/it]2025-09-14 20:04:01,595 Stage: Train 0.5 | Epoch: 111 | Iter: 337400 | Total Loss: 0.002848 | Recon Loss: 0.002369 | Commit Loss: 0.000957 | Perplexity: 683.859599
2025-09-14 20:04:09,322 Stage: Train 0.5 | Epoch: 111 | Iter: 337600 | Total Loss: 0.002876 | Recon Loss: 0.002395 | Commit Loss: 0.000963 | Perplexity: 684.606425
2025-09-14 20:04:17,054 Stage: Train 0.5 | Epoch: 111 | Iter: 337800 | Total Loss: 0.002911 | Recon Loss: 0.002427 | Commit Loss: 0.000968 | Perplexity: 687.933419
2025-09-14 20:04:24,786 Stage: Train 0.5 | Epoch: 111 | Iter: 338000 | Total Loss: 0.002828 | Recon Loss: 0.002347 | Commit Loss: 0.000961 | Perplexity: 684.786503
2025-09-14 20:04:32,450 Stage: Train 0.5 | Epoch: 111 | Iter: 338200 | Total Loss: 0.002891 | Recon Loss: 0.002415 | Commit Loss: 0.000951 | Perplexity: 680.881364
2025-09-14 20:04:40,121 Stage: Train 0.5 | Epoch: 111 | Iter: 338400 | Total Loss: 0.002869 | Recon Loss: 0.002388 | Commit Loss: 0.000961 | Perplexity: 681.843597
2025-09-14 20:04:47,822 Stage: Train 0.5 | Epoch: 111 | Iter: 338600 | Total Loss: 0.002859 | Recon Loss: 0.002377 | Commit Loss: 0.000965 | Perplexity: 682.954575
2025-09-14 20:04:55,505 Stage: Train 0.5 | Epoch: 111 | Iter: 338800 | Total Loss: 0.002814 | Recon Loss: 0.002339 | Commit Loss: 0.000949 | Perplexity: 680.194155
2025-09-14 20:05:03,248 Stage: Train 0.5 | Epoch: 111 | Iter: 339000 | Total Loss: 0.002890 | Recon Loss: 0.002412 | Commit Loss: 0.000958 | Perplexity: 685.163459
2025-09-14 20:05:10,922 Stage: Train 0.5 | Epoch: 111 | Iter: 339200 | Total Loss: 0.002863 | Recon Loss: 0.002384 | Commit Loss: 0.000958 | Perplexity: 684.449344
2025-09-14 20:05:18,594 Stage: Train 0.5 | Epoch: 111 | Iter: 339400 | Total Loss: 0.002838 | Recon Loss: 0.002363 | Commit Loss: 0.000951 | Perplexity: 684.979361
2025-09-14 20:05:26,282 Stage: Train 0.5 | Epoch: 111 | Iter: 339600 | Total Loss: 0.002872 | Recon Loss: 0.002393 | Commit Loss: 0.000958 | Perplexity: 683.132851
2025-09-14 20:05:33,957 Stage: Train 0.5 | Epoch: 111 | Iter: 339800 | Total Loss: 0.002863 | Recon Loss: 0.002386 | Commit Loss: 0.000953 | Perplexity: 683.826125
2025-09-14 20:05:41,665 Stage: Train 0.5 | Epoch: 111 | Iter: 340000 | Total Loss: 0.002872 | Recon Loss: 0.002391 | Commit Loss: 0.000963 | Perplexity: 683.140049
2025-09-14 20:05:41,665 Saving model at iteration 340000
2025-09-14 20:05:41,809 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_112_step_340000
2025-09-14 20:05:41,955 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_112_step_340000/pytorch_model.bin
2025-09-14 20:05:42,212 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_112_step_340000/optimizer.bin
2025-09-14 20:05:42,212 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_112_step_340000/scheduler.bin
2025-09-14 20:05:42,213 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_112_step_340000/random_states_0.pkl
2025-09-14 20:05:50,237 Stage: Train 0.5 | Epoch: 111 | Iter: 340200 | Total Loss: 0.002851 | Recon Loss: 0.002376 | Commit Loss: 0.000949 | Perplexity: 678.039969
Trainning Epoch:  68%|██████▊   | 112/165 [3:43:03<1:43:56, 117.66s/it]2025-09-14 20:05:58,013 Stage: Train 0.5 | Epoch: 112 | Iter: 340400 | Total Loss: 0.002877 | Recon Loss: 0.002399 | Commit Loss: 0.000956 | Perplexity: 683.014972
2025-09-14 20:06:05,750 Stage: Train 0.5 | Epoch: 112 | Iter: 340600 | Total Loss: 0.002880 | Recon Loss: 0.002401 | Commit Loss: 0.000957 | Perplexity: 685.113608
2025-09-14 20:06:13,570 Stage: Train 0.5 | Epoch: 112 | Iter: 340800 | Total Loss: 0.002823 | Recon Loss: 0.002347 | Commit Loss: 0.000952 | Perplexity: 687.295111
2025-09-14 20:06:21,538 Stage: Train 0.5 | Epoch: 112 | Iter: 341000 | Total Loss: 0.002868 | Recon Loss: 0.002385 | Commit Loss: 0.000967 | Perplexity: 687.038479
2025-09-14 20:06:29,478 Stage: Train 0.5 | Epoch: 112 | Iter: 341200 | Total Loss: 0.002872 | Recon Loss: 0.002398 | Commit Loss: 0.000948 | Perplexity: 682.020988
2025-09-14 20:06:37,140 Stage: Train 0.5 | Epoch: 112 | Iter: 341400 | Total Loss: 0.002878 | Recon Loss: 0.002394 | Commit Loss: 0.000967 | Perplexity: 687.568549
2025-09-14 20:06:44,842 Stage: Train 0.5 | Epoch: 112 | Iter: 341600 | Total Loss: 0.002894 | Recon Loss: 0.002420 | Commit Loss: 0.000948 | Perplexity: 682.456340
2025-09-14 20:06:52,565 Stage: Train 0.5 | Epoch: 112 | Iter: 341800 | Total Loss: 0.002882 | Recon Loss: 0.002407 | Commit Loss: 0.000950 | Perplexity: 686.008323
2025-09-14 20:07:00,281 Stage: Train 0.5 | Epoch: 112 | Iter: 342000 | Total Loss: 0.002893 | Recon Loss: 0.002416 | Commit Loss: 0.000954 | Perplexity: 686.773479
2025-09-14 20:07:07,969 Stage: Train 0.5 | Epoch: 112 | Iter: 342200 | Total Loss: 0.002819 | Recon Loss: 0.002341 | Commit Loss: 0.000956 | Perplexity: 686.548345
2025-09-14 20:07:15,905 Stage: Train 0.5 | Epoch: 112 | Iter: 342400 | Total Loss: 0.002847 | Recon Loss: 0.002368 | Commit Loss: 0.000958 | Perplexity: 684.916189
2025-09-14 20:07:24,069 Stage: Train 0.5 | Epoch: 112 | Iter: 342600 | Total Loss: 0.002832 | Recon Loss: 0.002354 | Commit Loss: 0.000955 | Perplexity: 684.047888
2025-09-14 20:07:32,164 Stage: Train 0.5 | Epoch: 112 | Iter: 342800 | Total Loss: 0.002855 | Recon Loss: 0.002379 | Commit Loss: 0.000953 | Perplexity: 685.745053
2025-09-14 20:07:40,300 Stage: Train 0.5 | Epoch: 112 | Iter: 343000 | Total Loss: 0.002850 | Recon Loss: 0.002367 | Commit Loss: 0.000966 | Perplexity: 684.919610
2025-09-14 20:07:48,418 Stage: Train 0.5 | Epoch: 112 | Iter: 343200 | Total Loss: 0.002832 | Recon Loss: 0.002356 | Commit Loss: 0.000952 | Perplexity: 683.652829
Trainning Epoch:  68%|██████▊   | 113/165 [3:45:03<1:42:32, 118.32s/it]2025-09-14 20:07:56,762 Stage: Train 0.5 | Epoch: 113 | Iter: 343400 | Total Loss: 0.002846 | Recon Loss: 0.002365 | Commit Loss: 0.000962 | Perplexity: 685.120203
2025-09-14 20:08:04,895 Stage: Train 0.5 | Epoch: 113 | Iter: 343600 | Total Loss: 0.002864 | Recon Loss: 0.002386 | Commit Loss: 0.000954 | Perplexity: 687.086791
2025-09-14 20:08:13,018 Stage: Train 0.5 | Epoch: 113 | Iter: 343800 | Total Loss: 0.002837 | Recon Loss: 0.002361 | Commit Loss: 0.000952 | Perplexity: 687.201781
2025-09-14 20:08:20,927 Stage: Train 0.5 | Epoch: 113 | Iter: 344000 | Total Loss: 0.002800 | Recon Loss: 0.002319 | Commit Loss: 0.000962 | Perplexity: 686.479427
2025-09-14 20:08:28,793 Stage: Train 0.5 | Epoch: 113 | Iter: 344200 | Total Loss: 0.002872 | Recon Loss: 0.002392 | Commit Loss: 0.000958 | Perplexity: 687.852148
2025-09-14 20:08:36,622 Stage: Train 0.5 | Epoch: 113 | Iter: 344400 | Total Loss: 0.002856 | Recon Loss: 0.002373 | Commit Loss: 0.000965 | Perplexity: 684.638473
2025-09-14 20:08:44,373 Stage: Train 0.5 | Epoch: 113 | Iter: 344600 | Total Loss: 0.002833 | Recon Loss: 0.002357 | Commit Loss: 0.000953 | Perplexity: 684.173925
2025-09-14 20:08:52,098 Stage: Train 0.5 | Epoch: 113 | Iter: 344800 | Total Loss: 0.002878 | Recon Loss: 0.002400 | Commit Loss: 0.000956 | Perplexity: 681.482628
2025-09-14 20:08:59,837 Stage: Train 0.5 | Epoch: 113 | Iter: 345000 | Total Loss: 0.002848 | Recon Loss: 0.002376 | Commit Loss: 0.000943 | Perplexity: 682.919652
2025-09-14 20:09:07,569 Stage: Train 0.5 | Epoch: 113 | Iter: 345200 | Total Loss: 0.002840 | Recon Loss: 0.002363 | Commit Loss: 0.000953 | Perplexity: 682.478489
2025-09-14 20:09:15,338 Stage: Train 0.5 | Epoch: 113 | Iter: 345400 | Total Loss: 0.002848 | Recon Loss: 0.002378 | Commit Loss: 0.000941 | Perplexity: 683.832299
2025-09-14 20:09:23,174 Stage: Train 0.5 | Epoch: 113 | Iter: 345600 | Total Loss: 0.002832 | Recon Loss: 0.002356 | Commit Loss: 0.000952 | Perplexity: 684.882321
2025-09-14 20:09:31,001 Stage: Train 0.5 | Epoch: 113 | Iter: 345800 | Total Loss: 0.002833 | Recon Loss: 0.002356 | Commit Loss: 0.000953 | Perplexity: 685.026705
2025-09-14 20:09:38,838 Stage: Train 0.5 | Epoch: 113 | Iter: 346000 | Total Loss: 0.002820 | Recon Loss: 0.002343 | Commit Loss: 0.000954 | Perplexity: 684.802885
2025-09-14 20:09:46,667 Stage: Train 0.5 | Epoch: 113 | Iter: 346200 | Total Loss: 0.002781 | Recon Loss: 0.002309 | Commit Loss: 0.000944 | Perplexity: 681.701170
Trainning Epoch:  69%|██████▉   | 114/165 [3:47:02<1:40:52, 118.68s/it]2025-09-14 20:09:54,494 Stage: Train 0.5 | Epoch: 114 | Iter: 346400 | Total Loss: 0.002857 | Recon Loss: 0.002382 | Commit Loss: 0.000950 | Perplexity: 684.230032
2025-09-14 20:10:02,338 Stage: Train 0.5 | Epoch: 114 | Iter: 346600 | Total Loss: 0.002773 | Recon Loss: 0.002307 | Commit Loss: 0.000932 | Perplexity: 682.807596
2025-09-14 20:10:10,155 Stage: Train 0.5 | Epoch: 114 | Iter: 346800 | Total Loss: 0.002844 | Recon Loss: 0.002366 | Commit Loss: 0.000956 | Perplexity: 688.403091
2025-09-14 20:10:17,975 Stage: Train 0.5 | Epoch: 114 | Iter: 347000 | Total Loss: 0.002872 | Recon Loss: 0.002398 | Commit Loss: 0.000947 | Perplexity: 684.744498
2025-09-14 20:10:25,781 Stage: Train 0.5 | Epoch: 114 | Iter: 347200 | Total Loss: 0.002813 | Recon Loss: 0.002346 | Commit Loss: 0.000934 | Perplexity: 682.381258
2025-09-14 20:10:33,621 Stage: Train 0.5 | Epoch: 114 | Iter: 347400 | Total Loss: 0.002834 | Recon Loss: 0.002355 | Commit Loss: 0.000958 | Perplexity: 686.013302
2025-09-14 20:10:41,459 Stage: Train 0.5 | Epoch: 114 | Iter: 347600 | Total Loss: 0.002809 | Recon Loss: 0.002337 | Commit Loss: 0.000945 | Perplexity: 686.510604
2025-09-14 20:10:49,271 Stage: Train 0.5 | Epoch: 114 | Iter: 347800 | Total Loss: 0.002854 | Recon Loss: 0.002382 | Commit Loss: 0.000944 | Perplexity: 683.745009
2025-09-14 20:10:56,985 Stage: Train 0.5 | Epoch: 114 | Iter: 348000 | Total Loss: 0.002807 | Recon Loss: 0.002337 | Commit Loss: 0.000940 | Perplexity: 683.677278
2025-09-14 20:11:04,749 Stage: Train 0.5 | Epoch: 114 | Iter: 348200 | Total Loss: 0.002807 | Recon Loss: 0.002339 | Commit Loss: 0.000936 | Perplexity: 683.835655
2025-09-14 20:11:12,502 Stage: Train 0.5 | Epoch: 114 | Iter: 348400 | Total Loss: 0.002825 | Recon Loss: 0.002350 | Commit Loss: 0.000951 | Perplexity: 684.825508
2025-09-14 20:11:20,262 Stage: Train 0.5 | Epoch: 114 | Iter: 348600 | Total Loss: 0.002830 | Recon Loss: 0.002356 | Commit Loss: 0.000949 | Perplexity: 685.304796
2025-09-14 20:11:28,010 Stage: Train 0.5 | Epoch: 114 | Iter: 348800 | Total Loss: 0.002801 | Recon Loss: 0.002327 | Commit Loss: 0.000949 | Perplexity: 685.548376
2025-09-14 20:11:35,743 Stage: Train 0.5 | Epoch: 114 | Iter: 349000 | Total Loss: 0.002813 | Recon Loss: 0.002339 | Commit Loss: 0.000948 | Perplexity: 686.111942
2025-09-14 20:11:43,467 Stage: Train 0.5 | Epoch: 114 | Iter: 349200 | Total Loss: 0.002823 | Recon Loss: 0.002349 | Commit Loss: 0.000949 | Perplexity: 685.612536
Trainning Epoch:  70%|██████▉   | 115/165 [3:49:00<1:38:47, 118.54s/it]2025-09-14 20:11:51,214 Stage: Train 0.5 | Epoch: 115 | Iter: 349400 | Total Loss: 0.002811 | Recon Loss: 0.002336 | Commit Loss: 0.000949 | Perplexity: 683.205024
2025-09-14 20:11:58,949 Stage: Train 0.5 | Epoch: 115 | Iter: 349600 | Total Loss: 0.002807 | Recon Loss: 0.002340 | Commit Loss: 0.000933 | Perplexity: 685.486182
2025-09-14 20:12:06,674 Stage: Train 0.5 | Epoch: 115 | Iter: 349800 | Total Loss: 0.002834 | Recon Loss: 0.002360 | Commit Loss: 0.000948 | Perplexity: 686.010266
2025-09-14 20:12:14,436 Stage: Train 0.5 | Epoch: 115 | Iter: 350000 | Total Loss: 0.002811 | Recon Loss: 0.002344 | Commit Loss: 0.000933 | Perplexity: 683.971055
2025-09-14 20:12:22,242 Stage: Train 0.5 | Epoch: 115 | Iter: 350200 | Total Loss: 0.002810 | Recon Loss: 0.002339 | Commit Loss: 0.000941 | Perplexity: 683.831950
2025-09-14 20:12:30,057 Stage: Train 0.5 | Epoch: 115 | Iter: 350400 | Total Loss: 0.002811 | Recon Loss: 0.002338 | Commit Loss: 0.000945 | Perplexity: 687.188277
2025-09-14 20:12:37,934 Stage: Train 0.5 | Epoch: 115 | Iter: 350600 | Total Loss: 0.002799 | Recon Loss: 0.002329 | Commit Loss: 0.000940 | Perplexity: 687.397603
2025-09-14 20:12:45,721 Stage: Train 0.5 | Epoch: 115 | Iter: 350800 | Total Loss: 0.002863 | Recon Loss: 0.002388 | Commit Loss: 0.000949 | Perplexity: 685.967290
2025-09-14 20:12:53,561 Stage: Train 0.5 | Epoch: 115 | Iter: 351000 | Total Loss: 0.002794 | Recon Loss: 0.002325 | Commit Loss: 0.000938 | Perplexity: 684.546698
2025-09-14 20:13:01,382 Stage: Train 0.5 | Epoch: 115 | Iter: 351200 | Total Loss: 0.002796 | Recon Loss: 0.002325 | Commit Loss: 0.000942 | Perplexity: 683.107141
2025-09-14 20:13:09,102 Stage: Train 0.5 | Epoch: 115 | Iter: 351400 | Total Loss: 0.002796 | Recon Loss: 0.002329 | Commit Loss: 0.000934 | Perplexity: 684.638222
2025-09-14 20:13:16,823 Stage: Train 0.5 | Epoch: 115 | Iter: 351600 | Total Loss: 0.002853 | Recon Loss: 0.002378 | Commit Loss: 0.000950 | Perplexity: 686.579902
2025-09-14 20:13:24,522 Stage: Train 0.5 | Epoch: 115 | Iter: 351800 | Total Loss: 0.002815 | Recon Loss: 0.002346 | Commit Loss: 0.000939 | Perplexity: 685.368466
2025-09-14 20:13:32,234 Stage: Train 0.5 | Epoch: 115 | Iter: 352000 | Total Loss: 0.002819 | Recon Loss: 0.002347 | Commit Loss: 0.000943 | Perplexity: 685.112958
2025-09-14 20:13:39,971 Stage: Train 0.5 | Epoch: 115 | Iter: 352200 | Total Loss: 0.002793 | Recon Loss: 0.002324 | Commit Loss: 0.000938 | Perplexity: 684.738945
2025-09-14 20:13:47,825 Stage: Train 0.5 | Epoch: 115 | Iter: 352400 | Total Loss: 0.002860 | Recon Loss: 0.002385 | Commit Loss: 0.000949 | Perplexity: 686.905563
Trainning Epoch:  70%|███████   | 116/165 [3:50:58<1:36:41, 118.41s/it]2025-09-14 20:13:55,735 Stage: Train 0.5 | Epoch: 116 | Iter: 352600 | Total Loss: 0.002795 | Recon Loss: 0.002322 | Commit Loss: 0.000945 | Perplexity: 684.458756
2025-09-14 20:14:03,774 Stage: Train 0.5 | Epoch: 116 | Iter: 352800 | Total Loss: 0.002823 | Recon Loss: 0.002349 | Commit Loss: 0.000949 | Perplexity: 687.794652
2025-09-14 20:14:11,882 Stage: Train 0.5 | Epoch: 116 | Iter: 353000 | Total Loss: 0.002841 | Recon Loss: 0.002366 | Commit Loss: 0.000950 | Perplexity: 688.441403
2025-09-14 20:14:19,654 Stage: Train 0.5 | Epoch: 116 | Iter: 353200 | Total Loss: 0.002832 | Recon Loss: 0.002368 | Commit Loss: 0.000928 | Perplexity: 684.758347
2025-09-14 20:14:27,525 Stage: Train 0.5 | Epoch: 116 | Iter: 353400 | Total Loss: 0.002774 | Recon Loss: 0.002306 | Commit Loss: 0.000935 | Perplexity: 686.477660
2025-09-14 20:14:35,375 Stage: Train 0.5 | Epoch: 116 | Iter: 353600 | Total Loss: 0.002818 | Recon Loss: 0.002357 | Commit Loss: 0.000922 | Perplexity: 680.357094
2025-09-14 20:14:43,281 Stage: Train 0.5 | Epoch: 116 | Iter: 353800 | Total Loss: 0.002822 | Recon Loss: 0.002358 | Commit Loss: 0.000928 | Perplexity: 681.236998
2025-09-14 20:14:51,109 Stage: Train 0.5 | Epoch: 116 | Iter: 354000 | Total Loss: 0.002842 | Recon Loss: 0.002372 | Commit Loss: 0.000940 | Perplexity: 684.657587
2025-09-14 20:14:58,926 Stage: Train 0.5 | Epoch: 116 | Iter: 354200 | Total Loss: 0.002789 | Recon Loss: 0.002321 | Commit Loss: 0.000937 | Perplexity: 685.114316
2025-09-14 20:15:06,762 Stage: Train 0.5 | Epoch: 116 | Iter: 354400 | Total Loss: 0.002796 | Recon Loss: 0.002328 | Commit Loss: 0.000936 | Perplexity: 685.298592
2025-09-14 20:15:14,554 Stage: Train 0.5 | Epoch: 116 | Iter: 354600 | Total Loss: 0.002834 | Recon Loss: 0.002370 | Commit Loss: 0.000928 | Perplexity: 682.063263
2025-09-14 20:15:22,349 Stage: Train 0.5 | Epoch: 116 | Iter: 354800 | Total Loss: 0.002773 | Recon Loss: 0.002310 | Commit Loss: 0.000926 | Perplexity: 684.414688
2025-09-14 20:15:30,217 Stage: Train 0.5 | Epoch: 116 | Iter: 355000 | Total Loss: 0.002788 | Recon Loss: 0.002324 | Commit Loss: 0.000929 | Perplexity: 685.544287
2025-09-14 20:15:38,489 Stage: Train 0.5 | Epoch: 116 | Iter: 355200 | Total Loss: 0.002790 | Recon Loss: 0.002320 | Commit Loss: 0.000939 | Perplexity: 686.940152
2025-09-14 20:15:46,353 Stage: Train 0.5 | Epoch: 116 | Iter: 355400 | Total Loss: 0.002867 | Recon Loss: 0.002388 | Commit Loss: 0.000957 | Perplexity: 687.696447
Trainning Epoch:  71%|███████   | 117/165 [3:52:58<1:35:06, 118.89s/it]2025-09-14 20:15:54,302 Stage: Train 0.5 | Epoch: 117 | Iter: 355600 | Total Loss: 0.002790 | Recon Loss: 0.002321 | Commit Loss: 0.000937 | Perplexity: 681.891423
2025-09-14 20:16:02,447 Stage: Train 0.5 | Epoch: 117 | Iter: 355800 | Total Loss: 0.002817 | Recon Loss: 0.002352 | Commit Loss: 0.000931 | Perplexity: 687.068157
2025-09-14 20:16:10,543 Stage: Train 0.5 | Epoch: 117 | Iter: 356000 | Total Loss: 0.002816 | Recon Loss: 0.002349 | Commit Loss: 0.000934 | Perplexity: 686.463401
2025-09-14 20:16:18,417 Stage: Train 0.5 | Epoch: 117 | Iter: 356200 | Total Loss: 0.002771 | Recon Loss: 0.002304 | Commit Loss: 0.000935 | Perplexity: 681.976088
2025-09-14 20:16:26,454 Stage: Train 0.5 | Epoch: 117 | Iter: 356400 | Total Loss: 0.002822 | Recon Loss: 0.002352 | Commit Loss: 0.000940 | Perplexity: 685.397268
2025-09-14 20:16:34,564 Stage: Train 0.5 | Epoch: 117 | Iter: 356600 | Total Loss: 0.002754 | Recon Loss: 0.002290 | Commit Loss: 0.000929 | Perplexity: 685.809817
2025-09-14 20:16:42,446 Stage: Train 0.5 | Epoch: 117 | Iter: 356800 | Total Loss: 0.002828 | Recon Loss: 0.002367 | Commit Loss: 0.000923 | Perplexity: 682.000100
2025-09-14 20:16:50,229 Stage: Train 0.5 | Epoch: 117 | Iter: 357000 | Total Loss: 0.002791 | Recon Loss: 0.002327 | Commit Loss: 0.000929 | Perplexity: 685.993971
2025-09-14 20:16:57,909 Stage: Train 0.5 | Epoch: 117 | Iter: 357200 | Total Loss: 0.002802 | Recon Loss: 0.002331 | Commit Loss: 0.000941 | Perplexity: 687.695919
2025-09-14 20:17:05,630 Stage: Train 0.5 | Epoch: 117 | Iter: 357400 | Total Loss: 0.002830 | Recon Loss: 0.002359 | Commit Loss: 0.000942 | Perplexity: 686.568238
2025-09-14 20:17:13,347 Stage: Train 0.5 | Epoch: 117 | Iter: 357600 | Total Loss: 0.002768 | Recon Loss: 0.002297 | Commit Loss: 0.000942 | Perplexity: 688.689617
2025-09-14 20:17:21,049 Stage: Train 0.5 | Epoch: 117 | Iter: 357800 | Total Loss: 0.002800 | Recon Loss: 0.002330 | Commit Loss: 0.000941 | Perplexity: 684.061068
2025-09-14 20:17:28,754 Stage: Train 0.5 | Epoch: 117 | Iter: 358000 | Total Loss: 0.002815 | Recon Loss: 0.002351 | Commit Loss: 0.000929 | Perplexity: 687.412802
2025-09-14 20:17:36,469 Stage: Train 0.5 | Epoch: 117 | Iter: 358200 | Total Loss: 0.002776 | Recon Loss: 0.002311 | Commit Loss: 0.000930 | Perplexity: 682.610061
2025-09-14 20:17:44,134 Stage: Train 0.5 | Epoch: 117 | Iter: 358400 | Total Loss: 0.002801 | Recon Loss: 0.002338 | Commit Loss: 0.000926 | Perplexity: 683.803618
Trainning Epoch:  72%|███████▏  | 118/165 [3:54:58<1:33:12, 118.98s/it]2025-09-14 20:17:51,853 Stage: Train 0.5 | Epoch: 118 | Iter: 358600 | Total Loss: 0.002814 | Recon Loss: 0.002349 | Commit Loss: 0.000930 | Perplexity: 683.561315
2025-09-14 20:17:59,600 Stage: Train 0.5 | Epoch: 118 | Iter: 358800 | Total Loss: 0.002792 | Recon Loss: 0.002323 | Commit Loss: 0.000938 | Perplexity: 688.169712
2025-09-14 20:18:07,429 Stage: Train 0.5 | Epoch: 118 | Iter: 359000 | Total Loss: 0.002824 | Recon Loss: 0.002353 | Commit Loss: 0.000942 | Perplexity: 688.429721
2025-09-14 20:18:15,178 Stage: Train 0.5 | Epoch: 118 | Iter: 359200 | Total Loss: 0.002759 | Recon Loss: 0.002296 | Commit Loss: 0.000926 | Perplexity: 683.329124
2025-09-14 20:18:22,913 Stage: Train 0.5 | Epoch: 118 | Iter: 359400 | Total Loss: 0.002803 | Recon Loss: 0.002338 | Commit Loss: 0.000930 | Perplexity: 683.042988
2025-09-14 20:18:30,661 Stage: Train 0.5 | Epoch: 118 | Iter: 359600 | Total Loss: 0.002822 | Recon Loss: 0.002356 | Commit Loss: 0.000931 | Perplexity: 684.495375
2025-09-14 20:18:38,326 Stage: Train 0.5 | Epoch: 118 | Iter: 359800 | Total Loss: 0.002833 | Recon Loss: 0.002365 | Commit Loss: 0.000936 | Perplexity: 685.123206
2025-09-14 20:18:46,114 Stage: Train 0.5 | Epoch: 118 | Iter: 360000 | Total Loss: 0.002802 | Recon Loss: 0.002339 | Commit Loss: 0.000927 | Perplexity: 685.529859
2025-09-14 20:18:46,114 Saving model at iteration 360000
2025-09-14 20:18:46,261 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_119_step_360000
2025-09-14 20:18:46,397 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_119_step_360000/pytorch_model.bin
2025-09-14 20:18:46,647 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_119_step_360000/optimizer.bin
2025-09-14 20:18:46,647 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_119_step_360000/scheduler.bin
2025-09-14 20:18:46,648 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_119_step_360000/random_states_0.pkl
2025-09-14 20:18:54,354 Stage: Train 0.5 | Epoch: 118 | Iter: 360200 | Total Loss: 0.002759 | Recon Loss: 0.002291 | Commit Loss: 0.000936 | Perplexity: 690.271204
2025-09-14 20:19:02,137 Stage: Train 0.5 | Epoch: 118 | Iter: 360400 | Total Loss: 0.002799 | Recon Loss: 0.002336 | Commit Loss: 0.000926 | Perplexity: 684.057631
2025-09-14 20:19:09,946 Stage: Train 0.5 | Epoch: 118 | Iter: 360600 | Total Loss: 0.002813 | Recon Loss: 0.002349 | Commit Loss: 0.000928 | Perplexity: 684.612618
2025-09-14 20:19:17,770 Stage: Train 0.5 | Epoch: 118 | Iter: 360800 | Total Loss: 0.002790 | Recon Loss: 0.002326 | Commit Loss: 0.000928 | Perplexity: 684.794103
2025-09-14 20:19:25,780 Stage: Train 0.5 | Epoch: 118 | Iter: 361000 | Total Loss: 0.002807 | Recon Loss: 0.002342 | Commit Loss: 0.000929 | Perplexity: 686.704874
2025-09-14 20:19:33,892 Stage: Train 0.5 | Epoch: 118 | Iter: 361200 | Total Loss: 0.002796 | Recon Loss: 0.002331 | Commit Loss: 0.000931 | Perplexity: 687.188118
2025-09-14 20:19:41,981 Stage: Train 0.5 | Epoch: 118 | Iter: 361400 | Total Loss: 0.002762 | Recon Loss: 0.002302 | Commit Loss: 0.000918 | Perplexity: 683.571516
Trainning Epoch:  72%|███████▏  | 119/165 [3:56:57<1:31:26, 119.27s/it]2025-09-14 20:19:50,510 Stage: Train 0.5 | Epoch: 119 | Iter: 361600 | Total Loss: 0.002776 | Recon Loss: 0.002313 | Commit Loss: 0.000927 | Perplexity: 684.214277
2025-09-14 20:19:58,627 Stage: Train 0.5 | Epoch: 119 | Iter: 361800 | Total Loss: 0.002799 | Recon Loss: 0.002337 | Commit Loss: 0.000925 | Perplexity: 687.028271
2025-09-14 20:20:06,572 Stage: Train 0.5 | Epoch: 119 | Iter: 362000 | Total Loss: 0.002745 | Recon Loss: 0.002284 | Commit Loss: 0.000921 | Perplexity: 681.861513
2025-09-14 20:20:14,390 Stage: Train 0.5 | Epoch: 119 | Iter: 362200 | Total Loss: 0.002769 | Recon Loss: 0.002309 | Commit Loss: 0.000921 | Perplexity: 684.408590
2025-09-14 20:20:22,312 Stage: Train 0.5 | Epoch: 119 | Iter: 362400 | Total Loss: 0.002807 | Recon Loss: 0.002341 | Commit Loss: 0.000933 | Perplexity: 684.840100
2025-09-14 20:20:30,365 Stage: Train 0.5 | Epoch: 119 | Iter: 362600 | Total Loss: 0.002787 | Recon Loss: 0.002324 | Commit Loss: 0.000926 | Perplexity: 687.116899
2025-09-14 20:20:38,302 Stage: Train 0.5 | Epoch: 119 | Iter: 362800 | Total Loss: 0.002760 | Recon Loss: 0.002291 | Commit Loss: 0.000938 | Perplexity: 687.828939
2025-09-14 20:20:46,097 Stage: Train 0.5 | Epoch: 119 | Iter: 363000 | Total Loss: 0.002807 | Recon Loss: 0.002349 | Commit Loss: 0.000915 | Perplexity: 684.770250
2025-09-14 20:20:53,914 Stage: Train 0.5 | Epoch: 119 | Iter: 363200 | Total Loss: 0.002749 | Recon Loss: 0.002287 | Commit Loss: 0.000923 | Perplexity: 685.120579
2025-09-14 20:21:01,717 Stage: Train 0.5 | Epoch: 119 | Iter: 363400 | Total Loss: 0.002817 | Recon Loss: 0.002353 | Commit Loss: 0.000929 | Perplexity: 686.793581
2025-09-14 20:21:09,505 Stage: Train 0.5 | Epoch: 119 | Iter: 363600 | Total Loss: 0.002840 | Recon Loss: 0.002371 | Commit Loss: 0.000938 | Perplexity: 686.616092
2025-09-14 20:21:17,317 Stage: Train 0.5 | Epoch: 119 | Iter: 363800 | Total Loss: 0.002832 | Recon Loss: 0.002367 | Commit Loss: 0.000929 | Perplexity: 686.594248
2025-09-14 20:21:25,141 Stage: Train 0.5 | Epoch: 119 | Iter: 364000 | Total Loss: 0.002792 | Recon Loss: 0.002328 | Commit Loss: 0.000929 | Perplexity: 687.254475
2025-09-14 20:21:32,938 Stage: Train 0.5 | Epoch: 119 | Iter: 364200 | Total Loss: 0.002738 | Recon Loss: 0.002275 | Commit Loss: 0.000926 | Perplexity: 683.167674
2025-09-14 20:21:40,781 Stage: Train 0.5 | Epoch: 119 | Iter: 364400 | Total Loss: 0.002835 | Recon Loss: 0.002376 | Commit Loss: 0.000919 | Perplexity: 683.256443
Trainning Epoch:  73%|███████▎  | 120/165 [3:58:57<1:29:33, 119.41s/it]2025-09-14 20:21:48,604 Stage: Train 0.5 | Epoch: 120 | Iter: 364600 | Total Loss: 0.002735 | Recon Loss: 0.002278 | Commit Loss: 0.000915 | Perplexity: 681.620548
2025-09-14 20:21:56,417 Stage: Train 0.5 | Epoch: 120 | Iter: 364800 | Total Loss: 0.002772 | Recon Loss: 0.002310 | Commit Loss: 0.000925 | Perplexity: 686.746261
2025-09-14 20:22:04,257 Stage: Train 0.5 | Epoch: 120 | Iter: 365000 | Total Loss: 0.002785 | Recon Loss: 0.002320 | Commit Loss: 0.000929 | Perplexity: 684.268703
2025-09-14 20:22:12,058 Stage: Train 0.5 | Epoch: 120 | Iter: 365200 | Total Loss: 0.002865 | Recon Loss: 0.002411 | Commit Loss: 0.000907 | Perplexity: 684.550543
2025-09-14 20:22:20,076 Stage: Train 0.5 | Epoch: 120 | Iter: 365400 | Total Loss: 0.002756 | Recon Loss: 0.002296 | Commit Loss: 0.000919 | Perplexity: 685.252547
2025-09-14 20:22:27,975 Stage: Train 0.5 | Epoch: 120 | Iter: 365600 | Total Loss: 0.002796 | Recon Loss: 0.002338 | Commit Loss: 0.000917 | Perplexity: 684.803596
2025-09-14 20:22:35,830 Stage: Train 0.5 | Epoch: 120 | Iter: 365800 | Total Loss: 0.002775 | Recon Loss: 0.002318 | Commit Loss: 0.000915 | Perplexity: 685.223297
2025-09-14 20:22:43,621 Stage: Train 0.5 | Epoch: 120 | Iter: 366000 | Total Loss: 0.002791 | Recon Loss: 0.002334 | Commit Loss: 0.000915 | Perplexity: 686.898524
2025-09-14 20:22:51,437 Stage: Train 0.5 | Epoch: 120 | Iter: 366200 | Total Loss: 0.002759 | Recon Loss: 0.002299 | Commit Loss: 0.000919 | Perplexity: 684.964648
2025-09-14 20:22:59,237 Stage: Train 0.5 | Epoch: 120 | Iter: 366400 | Total Loss: 0.002794 | Recon Loss: 0.002334 | Commit Loss: 0.000921 | Perplexity: 683.347718
2025-09-14 20:23:07,037 Stage: Train 0.5 | Epoch: 120 | Iter: 366600 | Total Loss: 0.002781 | Recon Loss: 0.002318 | Commit Loss: 0.000927 | Perplexity: 684.873852
2025-09-14 20:23:14,817 Stage: Train 0.5 | Epoch: 120 | Iter: 366800 | Total Loss: 0.002735 | Recon Loss: 0.002276 | Commit Loss: 0.000919 | Perplexity: 686.342558
2025-09-14 20:23:22,649 Stage: Train 0.5 | Epoch: 120 | Iter: 367000 | Total Loss: 0.002775 | Recon Loss: 0.002313 | Commit Loss: 0.000924 | Perplexity: 686.949134
2025-09-14 20:23:30,457 Stage: Train 0.5 | Epoch: 120 | Iter: 367200 | Total Loss: 0.002785 | Recon Loss: 0.002324 | Commit Loss: 0.000923 | Perplexity: 689.060493
2025-09-14 20:23:38,206 Stage: Train 0.5 | Epoch: 120 | Iter: 367400 | Total Loss: 0.002774 | Recon Loss: 0.002310 | Commit Loss: 0.000928 | Perplexity: 687.433657
Trainning Epoch:  73%|███████▎  | 121/165 [4:00:56<1:27:26, 119.25s/it]2025-09-14 20:23:45,981 Stage: Train 0.5 | Epoch: 121 | Iter: 367600 | Total Loss: 0.002854 | Recon Loss: 0.002390 | Commit Loss: 0.000928 | Perplexity: 686.482879
2025-09-14 20:23:53,645 Stage: Train 0.5 | Epoch: 121 | Iter: 367800 | Total Loss: 0.002760 | Recon Loss: 0.002305 | Commit Loss: 0.000908 | Perplexity: 687.360094
2025-09-14 20:24:01,301 Stage: Train 0.5 | Epoch: 121 | Iter: 368000 | Total Loss: 0.002764 | Recon Loss: 0.002303 | Commit Loss: 0.000923 | Perplexity: 686.713064
2025-09-14 20:24:09,005 Stage: Train 0.5 | Epoch: 121 | Iter: 368200 | Total Loss: 0.002766 | Recon Loss: 0.002306 | Commit Loss: 0.000920 | Perplexity: 687.745799
2025-09-14 20:24:16,678 Stage: Train 0.5 | Epoch: 121 | Iter: 368400 | Total Loss: 0.002743 | Recon Loss: 0.002286 | Commit Loss: 0.000914 | Perplexity: 687.699600
2025-09-14 20:24:24,345 Stage: Train 0.5 | Epoch: 121 | Iter: 368600 | Total Loss: 0.002825 | Recon Loss: 0.002363 | Commit Loss: 0.000924 | Perplexity: 685.175877
2025-09-14 20:24:32,066 Stage: Train 0.5 | Epoch: 121 | Iter: 368800 | Total Loss: 0.002744 | Recon Loss: 0.002284 | Commit Loss: 0.000919 | Perplexity: 686.199085
2025-09-14 20:24:39,833 Stage: Train 0.5 | Epoch: 121 | Iter: 369000 | Total Loss: 0.002781 | Recon Loss: 0.002323 | Commit Loss: 0.000915 | Perplexity: 684.566669
2025-09-14 20:24:47,906 Stage: Train 0.5 | Epoch: 121 | Iter: 369200 | Total Loss: 0.002747 | Recon Loss: 0.002292 | Commit Loss: 0.000911 | Perplexity: 682.544138
2025-09-14 20:24:55,995 Stage: Train 0.5 | Epoch: 121 | Iter: 369400 | Total Loss: 0.002761 | Recon Loss: 0.002297 | Commit Loss: 0.000928 | Perplexity: 686.785368
2025-09-14 20:25:04,092 Stage: Train 0.5 | Epoch: 121 | Iter: 369600 | Total Loss: 0.002772 | Recon Loss: 0.002316 | Commit Loss: 0.000912 | Perplexity: 688.206996
2025-09-14 20:25:12,194 Stage: Train 0.5 | Epoch: 121 | Iter: 369800 | Total Loss: 0.002745 | Recon Loss: 0.002288 | Commit Loss: 0.000915 | Perplexity: 685.734201
2025-09-14 20:25:20,302 Stage: Train 0.5 | Epoch: 121 | Iter: 370000 | Total Loss: 0.002766 | Recon Loss: 0.002307 | Commit Loss: 0.000918 | Perplexity: 682.765896
2025-09-14 20:25:28,414 Stage: Train 0.5 | Epoch: 121 | Iter: 370200 | Total Loss: 0.002770 | Recon Loss: 0.002316 | Commit Loss: 0.000909 | Perplexity: 683.115255
2025-09-14 20:25:36,520 Stage: Train 0.5 | Epoch: 121 | Iter: 370400 | Total Loss: 0.002767 | Recon Loss: 0.002307 | Commit Loss: 0.000921 | Perplexity: 688.118340
2025-09-14 20:25:44,221 Stage: Train 0.5 | Epoch: 121 | Iter: 370600 | Total Loss: 0.002746 | Recon Loss: 0.002287 | Commit Loss: 0.000917 | Perplexity: 685.441051
Trainning Epoch:  74%|███████▍  | 122/165 [4:02:56<1:25:33, 119.39s/it]2025-09-14 20:25:52,038 Stage: Train 0.5 | Epoch: 122 | Iter: 370800 | Total Loss: 0.002757 | Recon Loss: 0.002299 | Commit Loss: 0.000917 | Perplexity: 686.687186
2025-09-14 20:25:59,829 Stage: Train 0.5 | Epoch: 122 | Iter: 371000 | Total Loss: 0.002793 | Recon Loss: 0.002333 | Commit Loss: 0.000920 | Perplexity: 687.285876
2025-09-14 20:26:07,662 Stage: Train 0.5 | Epoch: 122 | Iter: 371200 | Total Loss: 0.002724 | Recon Loss: 0.002268 | Commit Loss: 0.000913 | Perplexity: 686.928951
2025-09-14 20:26:15,476 Stage: Train 0.5 | Epoch: 122 | Iter: 371400 | Total Loss: 0.002784 | Recon Loss: 0.002324 | Commit Loss: 0.000919 | Perplexity: 686.286625
2025-09-14 20:26:23,258 Stage: Train 0.5 | Epoch: 122 | Iter: 371600 | Total Loss: 0.002718 | Recon Loss: 0.002262 | Commit Loss: 0.000912 | Perplexity: 687.839467
2025-09-14 20:26:31,126 Stage: Train 0.5 | Epoch: 122 | Iter: 371800 | Total Loss: 0.002780 | Recon Loss: 0.002320 | Commit Loss: 0.000920 | Perplexity: 686.177776
2025-09-14 20:26:38,967 Stage: Train 0.5 | Epoch: 122 | Iter: 372000 | Total Loss: 0.002743 | Recon Loss: 0.002285 | Commit Loss: 0.000916 | Perplexity: 686.874479
2025-09-14 20:26:46,865 Stage: Train 0.5 | Epoch: 122 | Iter: 372200 | Total Loss: 0.002793 | Recon Loss: 0.002334 | Commit Loss: 0.000916 | Perplexity: 688.112439
2025-09-14 20:26:54,989 Stage: Train 0.5 | Epoch: 122 | Iter: 372400 | Total Loss: 0.002739 | Recon Loss: 0.002280 | Commit Loss: 0.000918 | Perplexity: 688.215741
2025-09-14 20:27:03,112 Stage: Train 0.5 | Epoch: 122 | Iter: 372600 | Total Loss: 0.002779 | Recon Loss: 0.002319 | Commit Loss: 0.000920 | Perplexity: 686.362245
2025-09-14 20:27:11,189 Stage: Train 0.5 | Epoch: 122 | Iter: 372800 | Total Loss: 0.002759 | Recon Loss: 0.002300 | Commit Loss: 0.000919 | Perplexity: 690.508243
2025-09-14 20:27:19,146 Stage: Train 0.5 | Epoch: 122 | Iter: 373000 | Total Loss: 0.002741 | Recon Loss: 0.002287 | Commit Loss: 0.000908 | Perplexity: 685.995436
2025-09-14 20:27:27,116 Stage: Train 0.5 | Epoch: 122 | Iter: 373200 | Total Loss: 0.002759 | Recon Loss: 0.002300 | Commit Loss: 0.000918 | Perplexity: 686.430550
2025-09-14 20:27:35,201 Stage: Train 0.5 | Epoch: 122 | Iter: 373400 | Total Loss: 0.002747 | Recon Loss: 0.002292 | Commit Loss: 0.000910 | Perplexity: 683.803015
2025-09-14 20:27:43,017 Stage: Train 0.5 | Epoch: 122 | Iter: 373600 | Total Loss: 0.002759 | Recon Loss: 0.002302 | Commit Loss: 0.000914 | Perplexity: 685.589258
Trainning Epoch:  75%|███████▍  | 123/165 [4:04:56<1:23:45, 119.66s/it]2025-09-14 20:27:50,853 Stage: Train 0.5 | Epoch: 123 | Iter: 373800 | Total Loss: 0.002733 | Recon Loss: 0.002285 | Commit Loss: 0.000896 | Perplexity: 681.103682
2025-09-14 20:27:58,678 Stage: Train 0.5 | Epoch: 123 | Iter: 374000 | Total Loss: 0.002730 | Recon Loss: 0.002270 | Commit Loss: 0.000919 | Perplexity: 688.379004
2025-09-14 20:28:06,521 Stage: Train 0.5 | Epoch: 123 | Iter: 374200 | Total Loss: 0.002778 | Recon Loss: 0.002325 | Commit Loss: 0.000906 | Perplexity: 684.383362
2025-09-14 20:28:14,321 Stage: Train 0.5 | Epoch: 123 | Iter: 374400 | Total Loss: 0.002790 | Recon Loss: 0.002337 | Commit Loss: 0.000907 | Perplexity: 685.418318
2025-09-14 20:28:22,138 Stage: Train 0.5 | Epoch: 123 | Iter: 374600 | Total Loss: 0.002767 | Recon Loss: 0.002310 | Commit Loss: 0.000914 | Perplexity: 688.263995
2025-09-14 20:28:30,114 Stage: Train 0.5 | Epoch: 123 | Iter: 374800 | Total Loss: 0.002752 | Recon Loss: 0.002296 | Commit Loss: 0.000912 | Perplexity: 688.650999
2025-09-14 20:28:37,907 Stage: Train 0.5 | Epoch: 123 | Iter: 375000 | Total Loss: 0.002769 | Recon Loss: 0.002316 | Commit Loss: 0.000906 | Perplexity: 688.486509
2025-09-14 20:28:45,710 Stage: Train 0.5 | Epoch: 123 | Iter: 375200 | Total Loss: 0.002781 | Recon Loss: 0.002325 | Commit Loss: 0.000912 | Perplexity: 688.158116
2025-09-14 20:28:53,551 Stage: Train 0.5 | Epoch: 123 | Iter: 375400 | Total Loss: 0.002773 | Recon Loss: 0.002321 | Commit Loss: 0.000906 | Perplexity: 686.272227
2025-09-14 20:29:01,377 Stage: Train 0.5 | Epoch: 123 | Iter: 375600 | Total Loss: 0.002744 | Recon Loss: 0.002292 | Commit Loss: 0.000904 | Perplexity: 684.920782
2025-09-14 20:29:09,170 Stage: Train 0.5 | Epoch: 123 | Iter: 375800 | Total Loss: 0.002728 | Recon Loss: 0.002276 | Commit Loss: 0.000903 | Perplexity: 685.020434
2025-09-14 20:29:16,981 Stage: Train 0.5 | Epoch: 123 | Iter: 376000 | Total Loss: 0.002753 | Recon Loss: 0.002296 | Commit Loss: 0.000913 | Perplexity: 688.158523
2025-09-14 20:29:24,895 Stage: Train 0.5 | Epoch: 123 | Iter: 376200 | Total Loss: 0.002740 | Recon Loss: 0.002288 | Commit Loss: 0.000904 | Perplexity: 684.606361
2025-09-14 20:29:32,910 Stage: Train 0.5 | Epoch: 123 | Iter: 376400 | Total Loss: 0.002747 | Recon Loss: 0.002293 | Commit Loss: 0.000906 | Perplexity: 686.450055
2025-09-14 20:29:40,870 Stage: Train 0.5 | Epoch: 123 | Iter: 376600 | Total Loss: 0.002743 | Recon Loss: 0.002287 | Commit Loss: 0.000911 | Perplexity: 688.203951
Trainning Epoch:  75%|███████▌  | 124/165 [4:06:56<1:21:43, 119.61s/it]2025-09-14 20:29:48,986 Stage: Train 0.5 | Epoch: 124 | Iter: 376800 | Total Loss: 0.002802 | Recon Loss: 0.002345 | Commit Loss: 0.000913 | Perplexity: 685.977226
2025-09-14 20:29:57,079 Stage: Train 0.5 | Epoch: 124 | Iter: 377000 | Total Loss: 0.002744 | Recon Loss: 0.002297 | Commit Loss: 0.000895 | Perplexity: 684.910974
2025-09-14 20:30:05,208 Stage: Train 0.5 | Epoch: 124 | Iter: 377200 | Total Loss: 0.002695 | Recon Loss: 0.002243 | Commit Loss: 0.000903 | Perplexity: 685.574577
2025-09-14 20:30:13,346 Stage: Train 0.5 | Epoch: 124 | Iter: 377400 | Total Loss: 0.002769 | Recon Loss: 0.002318 | Commit Loss: 0.000903 | Perplexity: 687.014413
2025-09-14 20:30:21,520 Stage: Train 0.5 | Epoch: 124 | Iter: 377600 | Total Loss: 0.002741 | Recon Loss: 0.002288 | Commit Loss: 0.000906 | Perplexity: 687.370416
2025-09-14 20:30:29,675 Stage: Train 0.5 | Epoch: 124 | Iter: 377800 | Total Loss: 0.002780 | Recon Loss: 0.002328 | Commit Loss: 0.000905 | Perplexity: 687.539381
2025-09-14 20:30:37,721 Stage: Train 0.5 | Epoch: 124 | Iter: 378000 | Total Loss: 0.002721 | Recon Loss: 0.002271 | Commit Loss: 0.000901 | Perplexity: 687.820286
2025-09-14 20:30:45,623 Stage: Train 0.5 | Epoch: 124 | Iter: 378200 | Total Loss: 0.002728 | Recon Loss: 0.002275 | Commit Loss: 0.000905 | Perplexity: 685.266271
2025-09-14 20:30:53,521 Stage: Train 0.5 | Epoch: 124 | Iter: 378400 | Total Loss: 0.002743 | Recon Loss: 0.002291 | Commit Loss: 0.000905 | Perplexity: 686.831802
2025-09-14 20:31:01,389 Stage: Train 0.5 | Epoch: 124 | Iter: 378600 | Total Loss: 0.002780 | Recon Loss: 0.002319 | Commit Loss: 0.000921 | Perplexity: 687.253270
2025-09-14 20:31:09,261 Stage: Train 0.5 | Epoch: 124 | Iter: 378800 | Total Loss: 0.002691 | Recon Loss: 0.002242 | Commit Loss: 0.000898 | Perplexity: 687.491567
2025-09-14 20:31:17,153 Stage: Train 0.5 | Epoch: 124 | Iter: 379000 | Total Loss: 0.002780 | Recon Loss: 0.002330 | Commit Loss: 0.000899 | Perplexity: 687.816534
2025-09-14 20:31:25,135 Stage: Train 0.5 | Epoch: 124 | Iter: 379200 | Total Loss: 0.002726 | Recon Loss: 0.002276 | Commit Loss: 0.000899 | Perplexity: 683.272732
2025-09-14 20:31:32,865 Stage: Train 0.5 | Epoch: 124 | Iter: 379400 | Total Loss: 0.002732 | Recon Loss: 0.002281 | Commit Loss: 0.000903 | Perplexity: 687.674783
2025-09-14 20:31:40,566 Stage: Train 0.5 | Epoch: 124 | Iter: 379600 | Total Loss: 0.002743 | Recon Loss: 0.002292 | Commit Loss: 0.000902 | Perplexity: 688.018706
Trainning Epoch:  76%|███████▌  | 125/165 [4:08:57<1:20:00, 120.01s/it]2025-09-14 20:31:48,321 Stage: Train 0.5 | Epoch: 125 | Iter: 379800 | Total Loss: 0.002754 | Recon Loss: 0.002305 | Commit Loss: 0.000896 | Perplexity: 684.835200
2025-09-14 20:31:55,998 Stage: Train 0.5 | Epoch: 125 | Iter: 380000 | Total Loss: 0.002735 | Recon Loss: 0.002289 | Commit Loss: 0.000894 | Perplexity: 686.976758
2025-09-14 20:31:55,998 Saving model at iteration 380000
2025-09-14 20:31:56,142 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_126_step_380000
2025-09-14 20:31:56,285 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_126_step_380000/pytorch_model.bin
2025-09-14 20:31:56,545 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_126_step_380000/optimizer.bin
2025-09-14 20:31:56,546 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_126_step_380000/scheduler.bin
2025-09-14 20:31:56,546 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_126_step_380000/random_states_0.pkl
2025-09-14 20:32:04,553 Stage: Train 0.5 | Epoch: 125 | Iter: 380200 | Total Loss: 0.002755 | Recon Loss: 0.002307 | Commit Loss: 0.000895 | Perplexity: 685.231544
2025-09-14 20:32:12,261 Stage: Train 0.5 | Epoch: 125 | Iter: 380400 | Total Loss: 0.002744 | Recon Loss: 0.002292 | Commit Loss: 0.000904 | Perplexity: 690.714357
2025-09-14 20:32:20,134 Stage: Train 0.5 | Epoch: 125 | Iter: 380600 | Total Loss: 0.002710 | Recon Loss: 0.002254 | Commit Loss: 0.000911 | Perplexity: 686.502557
2025-09-14 20:32:28,288 Stage: Train 0.5 | Epoch: 125 | Iter: 380800 | Total Loss: 0.002733 | Recon Loss: 0.002282 | Commit Loss: 0.000902 | Perplexity: 689.551897
2025-09-14 20:32:36,447 Stage: Train 0.5 | Epoch: 125 | Iter: 381000 | Total Loss: 0.002742 | Recon Loss: 0.002296 | Commit Loss: 0.000893 | Perplexity: 684.058196
2025-09-14 20:32:44,592 Stage: Train 0.5 | Epoch: 125 | Iter: 381200 | Total Loss: 0.002737 | Recon Loss: 0.002288 | Commit Loss: 0.000897 | Perplexity: 688.195842
2025-09-14 20:32:52,721 Stage: Train 0.5 | Epoch: 125 | Iter: 381400 | Total Loss: 0.002756 | Recon Loss: 0.002311 | Commit Loss: 0.000891 | Perplexity: 685.272580
2025-09-14 20:33:00,834 Stage: Train 0.5 | Epoch: 125 | Iter: 381600 | Total Loss: 0.002764 | Recon Loss: 0.002313 | Commit Loss: 0.000903 | Perplexity: 688.451995
2025-09-14 20:33:08,938 Stage: Train 0.5 | Epoch: 125 | Iter: 381800 | Total Loss: 0.002725 | Recon Loss: 0.002277 | Commit Loss: 0.000896 | Perplexity: 688.199821
2025-09-14 20:33:17,048 Stage: Train 0.5 | Epoch: 125 | Iter: 382000 | Total Loss: 0.002780 | Recon Loss: 0.002329 | Commit Loss: 0.000903 | Perplexity: 686.162698
2025-09-14 20:33:25,196 Stage: Train 0.5 | Epoch: 125 | Iter: 382200 | Total Loss: 0.002743 | Recon Loss: 0.002293 | Commit Loss: 0.000900 | Perplexity: 686.576039
2025-09-14 20:33:33,328 Stage: Train 0.5 | Epoch: 125 | Iter: 382400 | Total Loss: 0.002737 | Recon Loss: 0.002287 | Commit Loss: 0.000901 | Perplexity: 686.303260
2025-09-14 20:33:41,426 Stage: Train 0.5 | Epoch: 125 | Iter: 382600 | Total Loss: 0.002784 | Recon Loss: 0.002334 | Commit Loss: 0.000900 | Perplexity: 687.872728
Trainning Epoch:  76%|███████▋  | 126/165 [4:10:59<1:18:31, 120.81s/it]2025-09-14 20:33:49,490 Stage: Train 0.5 | Epoch: 126 | Iter: 382800 | Total Loss: 0.002720 | Recon Loss: 0.002270 | Commit Loss: 0.000900 | Perplexity: 686.077923
2025-09-14 20:33:57,241 Stage: Train 0.5 | Epoch: 126 | Iter: 383000 | Total Loss: 0.002736 | Recon Loss: 0.002289 | Commit Loss: 0.000894 | Perplexity: 685.936852
2025-09-14 20:34:05,013 Stage: Train 0.5 | Epoch: 126 | Iter: 383200 | Total Loss: 0.002719 | Recon Loss: 0.002270 | Commit Loss: 0.000898 | Perplexity: 688.893863
2025-09-14 20:34:12,758 Stage: Train 0.5 | Epoch: 126 | Iter: 383400 | Total Loss: 0.002751 | Recon Loss: 0.002304 | Commit Loss: 0.000894 | Perplexity: 687.441642
2025-09-14 20:34:20,564 Stage: Train 0.5 | Epoch: 126 | Iter: 383600 | Total Loss: 0.002747 | Recon Loss: 0.002294 | Commit Loss: 0.000905 | Perplexity: 689.003486
2025-09-14 20:34:28,431 Stage: Train 0.5 | Epoch: 126 | Iter: 383800 | Total Loss: 0.002730 | Recon Loss: 0.002282 | Commit Loss: 0.000897 | Perplexity: 686.910925
2025-09-14 20:34:36,306 Stage: Train 0.5 | Epoch: 126 | Iter: 384000 | Total Loss: 0.002765 | Recon Loss: 0.002318 | Commit Loss: 0.000894 | Perplexity: 683.922319
2025-09-14 20:34:44,192 Stage: Train 0.5 | Epoch: 126 | Iter: 384200 | Total Loss: 0.002775 | Recon Loss: 0.002328 | Commit Loss: 0.000894 | Perplexity: 686.494242
2025-09-14 20:34:52,035 Stage: Train 0.5 | Epoch: 126 | Iter: 384400 | Total Loss: 0.002746 | Recon Loss: 0.002296 | Commit Loss: 0.000900 | Perplexity: 688.703246
2025-09-14 20:34:59,861 Stage: Train 0.5 | Epoch: 126 | Iter: 384600 | Total Loss: 0.002714 | Recon Loss: 0.002268 | Commit Loss: 0.000892 | Perplexity: 686.105692
2025-09-14 20:35:07,850 Stage: Train 0.5 | Epoch: 126 | Iter: 384800 | Total Loss: 0.002707 | Recon Loss: 0.002260 | Commit Loss: 0.000894 | Perplexity: 686.054731
2025-09-14 20:35:15,954 Stage: Train 0.5 | Epoch: 126 | Iter: 385000 | Total Loss: 0.002698 | Recon Loss: 0.002252 | Commit Loss: 0.000892 | Perplexity: 686.204785
2025-09-14 20:35:24,067 Stage: Train 0.5 | Epoch: 126 | Iter: 385200 | Total Loss: 0.002734 | Recon Loss: 0.002286 | Commit Loss: 0.000896 | Perplexity: 686.369502
2025-09-14 20:35:32,191 Stage: Train 0.5 | Epoch: 126 | Iter: 385400 | Total Loss: 0.002714 | Recon Loss: 0.002263 | Commit Loss: 0.000902 | Perplexity: 684.121205
2025-09-14 20:35:40,261 Stage: Train 0.5 | Epoch: 126 | Iter: 385600 | Total Loss: 0.002731 | Recon Loss: 0.002284 | Commit Loss: 0.000893 | Perplexity: 684.193687
2025-09-14 20:35:48,340 Stage: Train 0.5 | Epoch: 126 | Iter: 385800 | Total Loss: 0.002676 | Recon Loss: 0.002227 | Commit Loss: 0.000897 | Perplexity: 690.505071
Trainning Epoch:  77%|███████▋  | 127/165 [4:13:00<1:16:25, 120.68s/it]2025-09-14 20:35:56,468 Stage: Train 0.5 | Epoch: 127 | Iter: 386000 | Total Loss: 0.002708 | Recon Loss: 0.002258 | Commit Loss: 0.000899 | Perplexity: 687.061021
2025-09-14 20:36:04,608 Stage: Train 0.5 | Epoch: 127 | Iter: 386200 | Total Loss: 0.002730 | Recon Loss: 0.002279 | Commit Loss: 0.000902 | Perplexity: 688.644493
2025-09-14 20:36:12,859 Stage: Train 0.5 | Epoch: 127 | Iter: 386400 | Total Loss: 0.002710 | Recon Loss: 0.002262 | Commit Loss: 0.000897 | Perplexity: 687.836519
2025-09-14 20:36:20,968 Stage: Train 0.5 | Epoch: 127 | Iter: 386600 | Total Loss: 0.002721 | Recon Loss: 0.002276 | Commit Loss: 0.000890 | Perplexity: 685.994795
2025-09-14 20:36:29,080 Stage: Train 0.5 | Epoch: 127 | Iter: 386800 | Total Loss: 0.002706 | Recon Loss: 0.002264 | Commit Loss: 0.000884 | Perplexity: 687.086862
2025-09-14 20:36:36,853 Stage: Train 0.5 | Epoch: 127 | Iter: 387000 | Total Loss: 0.002743 | Recon Loss: 0.002295 | Commit Loss: 0.000895 | Perplexity: 685.776954
2025-09-14 20:36:44,585 Stage: Train 0.5 | Epoch: 127 | Iter: 387200 | Total Loss: 0.002726 | Recon Loss: 0.002280 | Commit Loss: 0.000892 | Perplexity: 687.923458
2025-09-14 20:36:52,285 Stage: Train 0.5 | Epoch: 127 | Iter: 387400 | Total Loss: 0.002743 | Recon Loss: 0.002294 | Commit Loss: 0.000899 | Perplexity: 688.345029
2025-09-14 20:36:59,997 Stage: Train 0.5 | Epoch: 127 | Iter: 387600 | Total Loss: 0.002710 | Recon Loss: 0.002265 | Commit Loss: 0.000891 | Perplexity: 688.734989
2025-09-14 20:37:07,737 Stage: Train 0.5 | Epoch: 127 | Iter: 387800 | Total Loss: 0.002696 | Recon Loss: 0.002254 | Commit Loss: 0.000884 | Perplexity: 685.428336
2025-09-14 20:37:15,461 Stage: Train 0.5 | Epoch: 127 | Iter: 388000 | Total Loss: 0.002696 | Recon Loss: 0.002251 | Commit Loss: 0.000889 | Perplexity: 688.044173
2025-09-14 20:37:23,217 Stage: Train 0.5 | Epoch: 127 | Iter: 388200 | Total Loss: 0.002715 | Recon Loss: 0.002268 | Commit Loss: 0.000892 | Perplexity: 687.976453
2025-09-14 20:37:30,938 Stage: Train 0.5 | Epoch: 127 | Iter: 388400 | Total Loss: 0.002711 | Recon Loss: 0.002266 | Commit Loss: 0.000889 | Perplexity: 684.069325
2025-09-14 20:37:38,733 Stage: Train 0.5 | Epoch: 127 | Iter: 388600 | Total Loss: 0.002746 | Recon Loss: 0.002298 | Commit Loss: 0.000896 | Perplexity: 689.228004
2025-09-14 20:37:46,497 Stage: Train 0.5 | Epoch: 127 | Iter: 388800 | Total Loss: 0.002697 | Recon Loss: 0.002249 | Commit Loss: 0.000895 | Perplexity: 689.263168
Trainning Epoch:  78%|███████▊  | 128/165 [4:14:59<1:14:12, 120.35s/it]2025-09-14 20:37:54,258 Stage: Train 0.5 | Epoch: 128 | Iter: 389000 | Total Loss: 0.002702 | Recon Loss: 0.002252 | Commit Loss: 0.000900 | Perplexity: 688.387603
2025-09-14 20:38:02,001 Stage: Train 0.5 | Epoch: 128 | Iter: 389200 | Total Loss: 0.002675 | Recon Loss: 0.002230 | Commit Loss: 0.000891 | Perplexity: 687.726637
2025-09-14 20:38:09,773 Stage: Train 0.5 | Epoch: 128 | Iter: 389400 | Total Loss: 0.002691 | Recon Loss: 0.002247 | Commit Loss: 0.000889 | Perplexity: 688.020724
2025-09-14 20:38:17,497 Stage: Train 0.5 | Epoch: 128 | Iter: 389600 | Total Loss: 0.002702 | Recon Loss: 0.002254 | Commit Loss: 0.000895 | Perplexity: 686.825044
2025-09-14 20:38:25,209 Stage: Train 0.5 | Epoch: 128 | Iter: 389800 | Total Loss: 0.002702 | Recon Loss: 0.002257 | Commit Loss: 0.000889 | Perplexity: 687.666221
2025-09-14 20:38:32,901 Stage: Train 0.5 | Epoch: 128 | Iter: 390000 | Total Loss: 0.002689 | Recon Loss: 0.002244 | Commit Loss: 0.000889 | Perplexity: 687.630589
2025-09-14 20:38:40,704 Stage: Train 0.5 | Epoch: 128 | Iter: 390200 | Total Loss: 0.002712 | Recon Loss: 0.002265 | Commit Loss: 0.000895 | Perplexity: 689.414802
2025-09-14 20:38:48,513 Stage: Train 0.5 | Epoch: 128 | Iter: 390400 | Total Loss: 0.002699 | Recon Loss: 0.002254 | Commit Loss: 0.000889 | Perplexity: 685.720069
2025-09-14 20:38:56,248 Stage: Train 0.5 | Epoch: 128 | Iter: 390600 | Total Loss: 0.002681 | Recon Loss: 0.002241 | Commit Loss: 0.000880 | Perplexity: 686.302171
2025-09-14 20:39:04,005 Stage: Train 0.5 | Epoch: 128 | Iter: 390800 | Total Loss: 0.002744 | Recon Loss: 0.002300 | Commit Loss: 0.000887 | Perplexity: 686.897437
2025-09-14 20:39:11,784 Stage: Train 0.5 | Epoch: 128 | Iter: 391000 | Total Loss: 0.002722 | Recon Loss: 0.002278 | Commit Loss: 0.000888 | Perplexity: 688.360444
2025-09-14 20:39:19,755 Stage: Train 0.5 | Epoch: 128 | Iter: 391200 | Total Loss: 0.002707 | Recon Loss: 0.002260 | Commit Loss: 0.000893 | Perplexity: 687.554917
2025-09-14 20:39:27,848 Stage: Train 0.5 | Epoch: 128 | Iter: 391400 | Total Loss: 0.002714 | Recon Loss: 0.002271 | Commit Loss: 0.000886 | Perplexity: 687.354869
2025-09-14 20:39:35,765 Stage: Train 0.5 | Epoch: 128 | Iter: 391600 | Total Loss: 0.002694 | Recon Loss: 0.002251 | Commit Loss: 0.000887 | Perplexity: 687.553853
2025-09-14 20:39:43,662 Stage: Train 0.5 | Epoch: 128 | Iter: 391800 | Total Loss: 0.002704 | Recon Loss: 0.002253 | Commit Loss: 0.000903 | Perplexity: 692.006777
Trainning Epoch:  78%|███████▊  | 129/165 [4:16:58<1:11:55, 119.86s/it]2025-09-14 20:39:51,605 Stage: Train 0.5 | Epoch: 129 | Iter: 392000 | Total Loss: 0.002749 | Recon Loss: 0.002305 | Commit Loss: 0.000887 | Perplexity: 686.633808
2025-09-14 20:39:59,467 Stage: Train 0.5 | Epoch: 129 | Iter: 392200 | Total Loss: 0.002737 | Recon Loss: 0.002295 | Commit Loss: 0.000886 | Perplexity: 687.199727
2025-09-14 20:40:07,334 Stage: Train 0.5 | Epoch: 129 | Iter: 392400 | Total Loss: 0.002680 | Recon Loss: 0.002239 | Commit Loss: 0.000882 | Perplexity: 685.685803
2025-09-14 20:40:15,153 Stage: Train 0.5 | Epoch: 129 | Iter: 392600 | Total Loss: 0.002672 | Recon Loss: 0.002230 | Commit Loss: 0.000883 | Perplexity: 689.259058
2025-09-14 20:40:22,938 Stage: Train 0.5 | Epoch: 129 | Iter: 392800 | Total Loss: 0.002696 | Recon Loss: 0.002251 | Commit Loss: 0.000891 | Perplexity: 687.165953
2025-09-14 20:40:30,884 Stage: Train 0.5 | Epoch: 129 | Iter: 393000 | Total Loss: 0.002655 | Recon Loss: 0.002212 | Commit Loss: 0.000887 | Perplexity: 690.366906
2025-09-14 20:40:38,857 Stage: Train 0.5 | Epoch: 129 | Iter: 393200 | Total Loss: 0.002705 | Recon Loss: 0.002263 | Commit Loss: 0.000884 | Perplexity: 684.053342
2025-09-14 20:40:46,665 Stage: Train 0.5 | Epoch: 129 | Iter: 393400 | Total Loss: 0.002680 | Recon Loss: 0.002237 | Commit Loss: 0.000887 | Perplexity: 689.874691
2025-09-14 20:40:54,606 Stage: Train 0.5 | Epoch: 129 | Iter: 393600 | Total Loss: 0.002723 | Recon Loss: 0.002278 | Commit Loss: 0.000889 | Perplexity: 692.686042
2025-09-14 20:41:02,435 Stage: Train 0.5 | Epoch: 129 | Iter: 393800 | Total Loss: 0.002705 | Recon Loss: 0.002259 | Commit Loss: 0.000891 | Perplexity: 689.194277
2025-09-14 20:41:10,229 Stage: Train 0.5 | Epoch: 129 | Iter: 394000 | Total Loss: 0.002704 | Recon Loss: 0.002259 | Commit Loss: 0.000890 | Perplexity: 688.187002
2025-09-14 20:41:18,211 Stage: Train 0.5 | Epoch: 129 | Iter: 394200 | Total Loss: 0.002702 | Recon Loss: 0.002256 | Commit Loss: 0.000892 | Perplexity: 687.455141
2025-09-14 20:41:26,344 Stage: Train 0.5 | Epoch: 129 | Iter: 394400 | Total Loss: 0.002734 | Recon Loss: 0.002293 | Commit Loss: 0.000882 | Perplexity: 687.010343
2025-09-14 20:41:34,472 Stage: Train 0.5 | Epoch: 129 | Iter: 394600 | Total Loss: 0.002682 | Recon Loss: 0.002239 | Commit Loss: 0.000887 | Perplexity: 687.961305
2025-09-14 20:41:42,337 Stage: Train 0.5 | Epoch: 129 | Iter: 394800 | Total Loss: 0.002709 | Recon Loss: 0.002265 | Commit Loss: 0.000888 | Perplexity: 690.781741
Trainning Epoch:  79%|███████▉  | 130/165 [4:18:58<1:09:57, 119.94s/it]2025-09-14 20:41:50,201 Stage: Train 0.5 | Epoch: 130 | Iter: 395000 | Total Loss: 0.002693 | Recon Loss: 0.002253 | Commit Loss: 0.000880 | Perplexity: 687.731658
2025-09-14 20:41:58,070 Stage: Train 0.5 | Epoch: 130 | Iter: 395200 | Total Loss: 0.002703 | Recon Loss: 0.002261 | Commit Loss: 0.000884 | Perplexity: 685.438368
2025-09-14 20:42:05,802 Stage: Train 0.5 | Epoch: 130 | Iter: 395400 | Total Loss: 0.002710 | Recon Loss: 0.002266 | Commit Loss: 0.000889 | Perplexity: 690.778498
2025-09-14 20:42:13,485 Stage: Train 0.5 | Epoch: 130 | Iter: 395600 | Total Loss: 0.002668 | Recon Loss: 0.002229 | Commit Loss: 0.000878 | Perplexity: 687.803020
2025-09-14 20:42:21,225 Stage: Train 0.5 | Epoch: 130 | Iter: 395800 | Total Loss: 0.002744 | Recon Loss: 0.002305 | Commit Loss: 0.000877 | Perplexity: 687.028530
2025-09-14 20:42:28,913 Stage: Train 0.5 | Epoch: 130 | Iter: 396000 | Total Loss: 0.002709 | Recon Loss: 0.002264 | Commit Loss: 0.000890 | Perplexity: 688.356884
2025-09-14 20:42:36,605 Stage: Train 0.5 | Epoch: 130 | Iter: 396200 | Total Loss: 0.002676 | Recon Loss: 0.002240 | Commit Loss: 0.000872 | Perplexity: 685.916335
2025-09-14 20:42:44,582 Stage: Train 0.5 | Epoch: 130 | Iter: 396400 | Total Loss: 0.002688 | Recon Loss: 0.002249 | Commit Loss: 0.000880 | Perplexity: 688.814626
2025-09-14 20:42:52,711 Stage: Train 0.5 | Epoch: 130 | Iter: 396600 | Total Loss: 0.002716 | Recon Loss: 0.002276 | Commit Loss: 0.000881 | Perplexity: 685.508046
2025-09-14 20:43:00,874 Stage: Train 0.5 | Epoch: 130 | Iter: 396800 | Total Loss: 0.002717 | Recon Loss: 0.002281 | Commit Loss: 0.000873 | Perplexity: 686.608355
2025-09-14 20:43:09,058 Stage: Train 0.5 | Epoch: 130 | Iter: 397000 | Total Loss: 0.002702 | Recon Loss: 0.002262 | Commit Loss: 0.000879 | Perplexity: 687.385089
2025-09-14 20:43:16,986 Stage: Train 0.5 | Epoch: 130 | Iter: 397200 | Total Loss: 0.002639 | Recon Loss: 0.002197 | Commit Loss: 0.000883 | Perplexity: 689.495790
2025-09-14 20:43:24,921 Stage: Train 0.5 | Epoch: 130 | Iter: 397400 | Total Loss: 0.002731 | Recon Loss: 0.002290 | Commit Loss: 0.000883 | Perplexity: 688.188100
2025-09-14 20:43:32,750 Stage: Train 0.5 | Epoch: 130 | Iter: 397600 | Total Loss: 0.002673 | Recon Loss: 0.002237 | Commit Loss: 0.000872 | Perplexity: 687.594528
2025-09-14 20:43:40,542 Stage: Train 0.5 | Epoch: 130 | Iter: 397800 | Total Loss: 0.002711 | Recon Loss: 0.002271 | Commit Loss: 0.000881 | Perplexity: 689.818507
Trainning Epoch:  79%|███████▉  | 131/165 [4:20:58<1:07:55, 119.87s/it]2025-09-14 20:43:48,417 Stage: Train 0.5 | Epoch: 131 | Iter: 398000 | Total Loss: 0.002709 | Recon Loss: 0.002270 | Commit Loss: 0.000877 | Perplexity: 685.750052
2025-09-14 20:43:56,209 Stage: Train 0.5 | Epoch: 131 | Iter: 398200 | Total Loss: 0.002636 | Recon Loss: 0.002198 | Commit Loss: 0.000877 | Perplexity: 690.287607
2025-09-14 20:44:04,033 Stage: Train 0.5 | Epoch: 131 | Iter: 398400 | Total Loss: 0.002731 | Recon Loss: 0.002293 | Commit Loss: 0.000878 | Perplexity: 689.474481
2025-09-14 20:44:11,858 Stage: Train 0.5 | Epoch: 131 | Iter: 398600 | Total Loss: 0.002708 | Recon Loss: 0.002263 | Commit Loss: 0.000890 | Perplexity: 690.231003
2025-09-14 20:44:19,685 Stage: Train 0.5 | Epoch: 131 | Iter: 398800 | Total Loss: 0.002689 | Recon Loss: 0.002257 | Commit Loss: 0.000865 | Perplexity: 685.458047
2025-09-14 20:44:27,497 Stage: Train 0.5 | Epoch: 131 | Iter: 399000 | Total Loss: 0.002699 | Recon Loss: 0.002257 | Commit Loss: 0.000883 | Perplexity: 689.348096
2025-09-14 20:44:35,265 Stage: Train 0.5 | Epoch: 131 | Iter: 399200 | Total Loss: 0.002683 | Recon Loss: 0.002241 | Commit Loss: 0.000884 | Perplexity: 689.292052
2025-09-14 20:44:43,081 Stage: Train 0.5 | Epoch: 131 | Iter: 399400 | Total Loss: 0.002712 | Recon Loss: 0.002273 | Commit Loss: 0.000879 | Perplexity: 687.098879
2025-09-14 20:44:50,914 Stage: Train 0.5 | Epoch: 131 | Iter: 399600 | Total Loss: 0.002695 | Recon Loss: 0.002259 | Commit Loss: 0.000871 | Perplexity: 687.571202
2025-09-14 20:44:58,730 Stage: Train 0.5 | Epoch: 131 | Iter: 399800 | Total Loss: 0.002716 | Recon Loss: 0.002276 | Commit Loss: 0.000880 | Perplexity: 690.024055
2025-09-14 20:45:06,505 Stage: Train 0.5 | Epoch: 131 | Iter: 400000 | Total Loss: 0.002687 | Recon Loss: 0.002249 | Commit Loss: 0.000876 | Perplexity: 687.162330
2025-09-14 20:45:06,505 Saving model at iteration 400000
2025-09-14 20:45:06,736 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_132_step_400000
2025-09-14 20:45:06,876 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_132_step_400000/pytorch_model.bin
2025-09-14 20:45:07,122 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_132_step_400000/optimizer.bin
2025-09-14 20:45:07,123 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_132_step_400000/scheduler.bin
2025-09-14 20:45:07,124 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_132_step_400000/random_states_0.pkl
2025-09-14 20:45:14,901 Stage: Train 0.5 | Epoch: 131 | Iter: 400200 | Total Loss: 0.002712 | Recon Loss: 0.002270 | Commit Loss: 0.000885 | Perplexity: 690.973042
2025-09-14 20:45:22,689 Stage: Train 0.5 | Epoch: 131 | Iter: 400400 | Total Loss: 0.002612 | Recon Loss: 0.002173 | Commit Loss: 0.000878 | Perplexity: 687.348496
2025-09-14 20:45:30,477 Stage: Train 0.5 | Epoch: 131 | Iter: 400600 | Total Loss: 0.002676 | Recon Loss: 0.002237 | Commit Loss: 0.000878 | Perplexity: 687.548565
2025-09-14 20:45:38,278 Stage: Train 0.5 | Epoch: 131 | Iter: 400800 | Total Loss: 0.002671 | Recon Loss: 0.002232 | Commit Loss: 0.000878 | Perplexity: 688.233681
2025-09-14 20:45:46,078 Stage: Train 0.5 | Epoch: 131 | Iter: 401000 | Total Loss: 0.002669 | Recon Loss: 0.002227 | Commit Loss: 0.000884 | Perplexity: 685.738421
Trainning Epoch:  80%|████████  | 132/165 [4:22:57<1:05:48, 119.66s/it]2025-09-14 20:45:54,210 Stage: Train 0.5 | Epoch: 132 | Iter: 401200 | Total Loss: 0.002715 | Recon Loss: 0.002272 | Commit Loss: 0.000886 | Perplexity: 689.528788
2025-09-14 20:46:02,258 Stage: Train 0.5 | Epoch: 132 | Iter: 401400 | Total Loss: 0.002669 | Recon Loss: 0.002232 | Commit Loss: 0.000873 | Perplexity: 685.548874
2025-09-14 20:46:10,103 Stage: Train 0.5 | Epoch: 132 | Iter: 401600 | Total Loss: 0.002687 | Recon Loss: 0.002252 | Commit Loss: 0.000871 | Perplexity: 685.563514
2025-09-14 20:46:17,884 Stage: Train 0.5 | Epoch: 132 | Iter: 401800 | Total Loss: 0.002650 | Recon Loss: 0.002209 | Commit Loss: 0.000882 | Perplexity: 692.213180
2025-09-14 20:46:25,654 Stage: Train 0.5 | Epoch: 132 | Iter: 402000 | Total Loss: 0.002733 | Recon Loss: 0.002293 | Commit Loss: 0.000881 | Perplexity: 689.530443
2025-09-14 20:46:33,381 Stage: Train 0.5 | Epoch: 132 | Iter: 402200 | Total Loss: 0.002621 | Recon Loss: 0.002188 | Commit Loss: 0.000866 | Perplexity: 686.428951
2025-09-14 20:46:41,078 Stage: Train 0.5 | Epoch: 132 | Iter: 402400 | Total Loss: 0.002642 | Recon Loss: 0.002204 | Commit Loss: 0.000876 | Perplexity: 689.168620
2025-09-14 20:46:48,777 Stage: Train 0.5 | Epoch: 132 | Iter: 402600 | Total Loss: 0.002683 | Recon Loss: 0.002240 | Commit Loss: 0.000885 | Perplexity: 690.714141
2025-09-14 20:46:56,494 Stage: Train 0.5 | Epoch: 132 | Iter: 402800 | Total Loss: 0.002644 | Recon Loss: 0.002211 | Commit Loss: 0.000866 | Perplexity: 688.529721
2025-09-14 20:47:04,414 Stage: Train 0.5 | Epoch: 132 | Iter: 403000 | Total Loss: 0.002673 | Recon Loss: 0.002235 | Commit Loss: 0.000876 | Perplexity: 690.499327
2025-09-14 20:47:12,145 Stage: Train 0.5 | Epoch: 132 | Iter: 403200 | Total Loss: 0.002677 | Recon Loss: 0.002236 | Commit Loss: 0.000883 | Perplexity: 691.214603
2025-09-14 20:47:19,928 Stage: Train 0.5 | Epoch: 132 | Iter: 403400 | Total Loss: 0.002649 | Recon Loss: 0.002214 | Commit Loss: 0.000870 | Perplexity: 688.256025
2025-09-14 20:47:27,727 Stage: Train 0.5 | Epoch: 132 | Iter: 403600 | Total Loss: 0.002670 | Recon Loss: 0.002230 | Commit Loss: 0.000881 | Perplexity: 689.607603
2025-09-14 20:47:35,478 Stage: Train 0.5 | Epoch: 132 | Iter: 403800 | Total Loss: 0.002686 | Recon Loss: 0.002247 | Commit Loss: 0.000878 | Perplexity: 689.163490
2025-09-14 20:47:43,245 Stage: Train 0.5 | Epoch: 132 | Iter: 404000 | Total Loss: 0.002686 | Recon Loss: 0.002252 | Commit Loss: 0.000867 | Perplexity: 685.003687
Trainning Epoch:  81%|████████  | 133/165 [4:24:56<1:03:39, 119.35s/it]2025-09-14 20:47:50,981 Stage: Train 0.5 | Epoch: 133 | Iter: 404200 | Total Loss: 0.002673 | Recon Loss: 0.002232 | Commit Loss: 0.000883 | Perplexity: 690.543198
2025-09-14 20:47:58,701 Stage: Train 0.5 | Epoch: 133 | Iter: 404400 | Total Loss: 0.002673 | Recon Loss: 0.002237 | Commit Loss: 0.000871 | Perplexity: 688.736089
2025-09-14 20:48:06,433 Stage: Train 0.5 | Epoch: 133 | Iter: 404600 | Total Loss: 0.002690 | Recon Loss: 0.002252 | Commit Loss: 0.000874 | Perplexity: 688.094634
2025-09-14 20:48:14,265 Stage: Train 0.5 | Epoch: 133 | Iter: 404800 | Total Loss: 0.002665 | Recon Loss: 0.002234 | Commit Loss: 0.000862 | Perplexity: 687.314161
2025-09-14 20:48:22,113 Stage: Train 0.5 | Epoch: 133 | Iter: 405000 | Total Loss: 0.002717 | Recon Loss: 0.002278 | Commit Loss: 0.000878 | Perplexity: 687.242584
2025-09-14 20:48:29,997 Stage: Train 0.5 | Epoch: 133 | Iter: 405200 | Total Loss: 0.002662 | Recon Loss: 0.002229 | Commit Loss: 0.000866 | Perplexity: 689.700643
2025-09-14 20:48:37,886 Stage: Train 0.5 | Epoch: 133 | Iter: 405400 | Total Loss: 0.002646 | Recon Loss: 0.002210 | Commit Loss: 0.000873 | Perplexity: 690.100194
2025-09-14 20:48:45,710 Stage: Train 0.5 | Epoch: 133 | Iter: 405600 | Total Loss: 0.002678 | Recon Loss: 0.002241 | Commit Loss: 0.000873 | Perplexity: 689.020647
2025-09-14 20:48:53,638 Stage: Train 0.5 | Epoch: 133 | Iter: 405800 | Total Loss: 0.002716 | Recon Loss: 0.002283 | Commit Loss: 0.000866 | Perplexity: 687.456222
2025-09-14 20:49:01,742 Stage: Train 0.5 | Epoch: 133 | Iter: 406000 | Total Loss: 0.002658 | Recon Loss: 0.002222 | Commit Loss: 0.000873 | Perplexity: 690.122969
2025-09-14 20:49:09,890 Stage: Train 0.5 | Epoch: 133 | Iter: 406200 | Total Loss: 0.002647 | Recon Loss: 0.002217 | Commit Loss: 0.000860 | Perplexity: 686.700678
2025-09-14 20:49:18,006 Stage: Train 0.5 | Epoch: 133 | Iter: 406400 | Total Loss: 0.002684 | Recon Loss: 0.002249 | Commit Loss: 0.000871 | Perplexity: 688.355358
2025-09-14 20:49:26,168 Stage: Train 0.5 | Epoch: 133 | Iter: 406600 | Total Loss: 0.002684 | Recon Loss: 0.002252 | Commit Loss: 0.000863 | Perplexity: 686.854254
2025-09-14 20:49:34,297 Stage: Train 0.5 | Epoch: 133 | Iter: 406800 | Total Loss: 0.002650 | Recon Loss: 0.002211 | Commit Loss: 0.000879 | Perplexity: 693.874231
2025-09-14 20:49:42,433 Stage: Train 0.5 | Epoch: 133 | Iter: 407000 | Total Loss: 0.002645 | Recon Loss: 0.002210 | Commit Loss: 0.000869 | Perplexity: 690.291596
Trainning Epoch:  81%|████████  | 134/165 [4:26:56<1:01:53, 119.80s/it]2025-09-14 20:49:50,562 Stage: Train 0.5 | Epoch: 134 | Iter: 407200 | Total Loss: 0.002669 | Recon Loss: 0.002238 | Commit Loss: 0.000861 | Perplexity: 684.256245
2025-09-14 20:49:58,656 Stage: Train 0.5 | Epoch: 134 | Iter: 407400 | Total Loss: 0.002665 | Recon Loss: 0.002231 | Commit Loss: 0.000868 | Perplexity: 687.518148
2025-09-14 20:50:06,776 Stage: Train 0.5 | Epoch: 134 | Iter: 407600 | Total Loss: 0.002709 | Recon Loss: 0.002273 | Commit Loss: 0.000873 | Perplexity: 689.240827
2025-09-14 20:50:14,895 Stage: Train 0.5 | Epoch: 134 | Iter: 407800 | Total Loss: 0.002653 | Recon Loss: 0.002218 | Commit Loss: 0.000870 | Perplexity: 688.616598
2025-09-14 20:50:22,691 Stage: Train 0.5 | Epoch: 134 | Iter: 408000 | Total Loss: 0.002621 | Recon Loss: 0.002197 | Commit Loss: 0.000849 | Perplexity: 683.435985
2025-09-14 20:50:30,509 Stage: Train 0.5 | Epoch: 134 | Iter: 408200 | Total Loss: 0.002654 | Recon Loss: 0.002218 | Commit Loss: 0.000871 | Perplexity: 689.793429
2025-09-14 20:50:38,317 Stage: Train 0.5 | Epoch: 134 | Iter: 408400 | Total Loss: 0.002688 | Recon Loss: 0.002256 | Commit Loss: 0.000865 | Perplexity: 687.778376
2025-09-14 20:50:46,121 Stage: Train 0.5 | Epoch: 134 | Iter: 408600 | Total Loss: 0.002694 | Recon Loss: 0.002258 | Commit Loss: 0.000871 | Perplexity: 688.203311
2025-09-14 20:50:53,910 Stage: Train 0.5 | Epoch: 134 | Iter: 408800 | Total Loss: 0.002642 | Recon Loss: 0.002209 | Commit Loss: 0.000866 | Perplexity: 691.395188
2025-09-14 20:51:01,741 Stage: Train 0.5 | Epoch: 134 | Iter: 409000 | Total Loss: 0.002668 | Recon Loss: 0.002230 | Commit Loss: 0.000877 | Perplexity: 690.323526
2025-09-14 20:51:09,554 Stage: Train 0.5 | Epoch: 134 | Iter: 409200 | Total Loss: 0.002668 | Recon Loss: 0.002235 | Commit Loss: 0.000866 | Perplexity: 688.039585
2025-09-14 20:51:17,369 Stage: Train 0.5 | Epoch: 134 | Iter: 409400 | Total Loss: 0.002667 | Recon Loss: 0.002236 | Commit Loss: 0.000863 | Perplexity: 691.547498
2025-09-14 20:51:25,149 Stage: Train 0.5 | Epoch: 134 | Iter: 409600 | Total Loss: 0.002661 | Recon Loss: 0.002227 | Commit Loss: 0.000868 | Perplexity: 687.424693
2025-09-14 20:51:32,985 Stage: Train 0.5 | Epoch: 134 | Iter: 409800 | Total Loss: 0.002660 | Recon Loss: 0.002233 | Commit Loss: 0.000855 | Perplexity: 687.305522
2025-09-14 20:51:40,797 Stage: Train 0.5 | Epoch: 134 | Iter: 410000 | Total Loss: 0.002706 | Recon Loss: 0.002270 | Commit Loss: 0.000871 | Perplexity: 689.770616
Trainning Epoch:  82%|████████▏ | 135/165 [4:28:56<59:52, 119.74s/it]  2025-09-14 20:51:48,495 Stage: Train 0.5 | Epoch: 135 | Iter: 410200 | Total Loss: 0.002626 | Recon Loss: 0.002198 | Commit Loss: 0.000855 | Perplexity: 686.457027
2025-09-14 20:51:56,182 Stage: Train 0.5 | Epoch: 135 | Iter: 410400 | Total Loss: 0.002646 | Recon Loss: 0.002212 | Commit Loss: 0.000867 | Perplexity: 689.887855
2025-09-14 20:52:03,846 Stage: Train 0.5 | Epoch: 135 | Iter: 410600 | Total Loss: 0.002648 | Recon Loss: 0.002215 | Commit Loss: 0.000865 | Perplexity: 689.195852
2025-09-14 20:52:11,512 Stage: Train 0.5 | Epoch: 135 | Iter: 410800 | Total Loss: 0.002637 | Recon Loss: 0.002208 | Commit Loss: 0.000858 | Perplexity: 687.810531
2025-09-14 20:52:19,234 Stage: Train 0.5 | Epoch: 135 | Iter: 411000 | Total Loss: 0.002677 | Recon Loss: 0.002244 | Commit Loss: 0.000867 | Perplexity: 689.727426
2025-09-14 20:52:26,906 Stage: Train 0.5 | Epoch: 135 | Iter: 411200 | Total Loss: 0.002658 | Recon Loss: 0.002227 | Commit Loss: 0.000863 | Perplexity: 689.215472
2025-09-14 20:52:34,725 Stage: Train 0.5 | Epoch: 135 | Iter: 411400 | Total Loss: 0.002625 | Recon Loss: 0.002195 | Commit Loss: 0.000858 | Perplexity: 691.728747
2025-09-14 20:52:42,514 Stage: Train 0.5 | Epoch: 135 | Iter: 411600 | Total Loss: 0.002640 | Recon Loss: 0.002208 | Commit Loss: 0.000864 | Perplexity: 687.390352
2025-09-14 20:52:50,270 Stage: Train 0.5 | Epoch: 135 | Iter: 411800 | Total Loss: 0.002711 | Recon Loss: 0.002281 | Commit Loss: 0.000861 | Perplexity: 686.180300
2025-09-14 20:52:58,038 Stage: Train 0.5 | Epoch: 135 | Iter: 412000 | Total Loss: 0.002639 | Recon Loss: 0.002211 | Commit Loss: 0.000857 | Perplexity: 688.022014
2025-09-14 20:53:05,838 Stage: Train 0.5 | Epoch: 135 | Iter: 412200 | Total Loss: 0.002657 | Recon Loss: 0.002226 | Commit Loss: 0.000862 | Perplexity: 688.679597
2025-09-14 20:53:13,790 Stage: Train 0.5 | Epoch: 135 | Iter: 412400 | Total Loss: 0.002658 | Recon Loss: 0.002223 | Commit Loss: 0.000870 | Perplexity: 691.024720
2025-09-14 20:53:21,758 Stage: Train 0.5 | Epoch: 135 | Iter: 412600 | Total Loss: 0.002671 | Recon Loss: 0.002239 | Commit Loss: 0.000864 | Perplexity: 687.942970
2025-09-14 20:53:29,586 Stage: Train 0.5 | Epoch: 135 | Iter: 412800 | Total Loss: 0.002641 | Recon Loss: 0.002211 | Commit Loss: 0.000860 | Perplexity: 686.800683
2025-09-14 20:53:37,374 Stage: Train 0.5 | Epoch: 135 | Iter: 413000 | Total Loss: 0.002648 | Recon Loss: 0.002215 | Commit Loss: 0.000865 | Perplexity: 686.880728
Trainning Epoch:  82%|████████▏ | 136/165 [4:30:54<57:40, 119.32s/it]2025-09-14 20:53:45,423 Stage: Train 0.5 | Epoch: 136 | Iter: 413200 | Total Loss: 0.002671 | Recon Loss: 0.002239 | Commit Loss: 0.000863 | Perplexity: 689.001786
2025-09-14 20:53:53,571 Stage: Train 0.5 | Epoch: 136 | Iter: 413400 | Total Loss: 0.002669 | Recon Loss: 0.002233 | Commit Loss: 0.000871 | Perplexity: 692.748906
2025-09-14 20:54:01,448 Stage: Train 0.5 | Epoch: 136 | Iter: 413600 | Total Loss: 0.002666 | Recon Loss: 0.002237 | Commit Loss: 0.000859 | Perplexity: 687.557254
2025-09-14 20:54:09,181 Stage: Train 0.5 | Epoch: 136 | Iter: 413800 | Total Loss: 0.002671 | Recon Loss: 0.002246 | Commit Loss: 0.000850 | Perplexity: 688.861864
2025-09-14 20:54:16,886 Stage: Train 0.5 | Epoch: 136 | Iter: 414000 | Total Loss: 0.002609 | Recon Loss: 0.002179 | Commit Loss: 0.000861 | Perplexity: 687.690069
2025-09-14 20:54:24,585 Stage: Train 0.5 | Epoch: 136 | Iter: 414200 | Total Loss: 0.002668 | Recon Loss: 0.002238 | Commit Loss: 0.000860 | Perplexity: 687.654060
2025-09-14 20:54:32,394 Stage: Train 0.5 | Epoch: 136 | Iter: 414400 | Total Loss: 0.002633 | Recon Loss: 0.002205 | Commit Loss: 0.000857 | Perplexity: 688.602610
2025-09-14 20:54:40,196 Stage: Train 0.5 | Epoch: 136 | Iter: 414600 | Total Loss: 0.002655 | Recon Loss: 0.002230 | Commit Loss: 0.000851 | Perplexity: 687.996996
2025-09-14 20:54:47,961 Stage: Train 0.5 | Epoch: 136 | Iter: 414800 | Total Loss: 0.002677 | Recon Loss: 0.002252 | Commit Loss: 0.000850 | Perplexity: 686.599156
2025-09-14 20:54:55,749 Stage: Train 0.5 | Epoch: 136 | Iter: 415000 | Total Loss: 0.002634 | Recon Loss: 0.002205 | Commit Loss: 0.000859 | Perplexity: 687.700737
2025-09-14 20:55:03,498 Stage: Train 0.5 | Epoch: 136 | Iter: 415200 | Total Loss: 0.002659 | Recon Loss: 0.002230 | Commit Loss: 0.000858 | Perplexity: 687.135745
2025-09-14 20:55:11,297 Stage: Train 0.5 | Epoch: 136 | Iter: 415400 | Total Loss: 0.002619 | Recon Loss: 0.002191 | Commit Loss: 0.000856 | Perplexity: 690.450406
2025-09-14 20:55:19,068 Stage: Train 0.5 | Epoch: 136 | Iter: 415600 | Total Loss: 0.002668 | Recon Loss: 0.002234 | Commit Loss: 0.000868 | Perplexity: 690.760281
2025-09-14 20:55:26,874 Stage: Train 0.5 | Epoch: 136 | Iter: 415800 | Total Loss: 0.002632 | Recon Loss: 0.002201 | Commit Loss: 0.000863 | Perplexity: 690.865116
2025-09-14 20:55:34,609 Stage: Train 0.5 | Epoch: 136 | Iter: 416000 | Total Loss: 0.002678 | Recon Loss: 0.002242 | Commit Loss: 0.000872 | Perplexity: 691.687943
2025-09-14 20:55:42,273 Stage: Train 0.5 | Epoch: 136 | Iter: 416200 | Total Loss: 0.002665 | Recon Loss: 0.002234 | Commit Loss: 0.000861 | Perplexity: 688.422271
Trainning Epoch:  83%|████████▎ | 137/165 [4:32:53<55:33, 119.04s/it]2025-09-14 20:55:49,961 Stage: Train 0.5 | Epoch: 137 | Iter: 416400 | Total Loss: 0.002624 | Recon Loss: 0.002195 | Commit Loss: 0.000858 | Perplexity: 688.358437
2025-09-14 20:55:57,654 Stage: Train 0.5 | Epoch: 137 | Iter: 416600 | Total Loss: 0.002598 | Recon Loss: 0.002170 | Commit Loss: 0.000856 | Perplexity: 690.529787
2025-09-14 20:56:05,318 Stage: Train 0.5 | Epoch: 137 | Iter: 416800 | Total Loss: 0.002661 | Recon Loss: 0.002236 | Commit Loss: 0.000849 | Perplexity: 686.942224
2025-09-14 20:56:12,985 Stage: Train 0.5 | Epoch: 137 | Iter: 417000 | Total Loss: 0.002639 | Recon Loss: 0.002213 | Commit Loss: 0.000851 | Perplexity: 686.828535
2025-09-14 20:56:20,685 Stage: Train 0.5 | Epoch: 137 | Iter: 417200 | Total Loss: 0.002618 | Recon Loss: 0.002190 | Commit Loss: 0.000857 | Perplexity: 689.490329
2025-09-14 20:56:28,354 Stage: Train 0.5 | Epoch: 137 | Iter: 417400 | Total Loss: 0.002657 | Recon Loss: 0.002228 | Commit Loss: 0.000859 | Perplexity: 689.601934
2025-09-14 20:56:36,025 Stage: Train 0.5 | Epoch: 137 | Iter: 417600 | Total Loss: 0.002669 | Recon Loss: 0.002235 | Commit Loss: 0.000866 | Perplexity: 689.657639
2025-09-14 20:56:43,726 Stage: Train 0.5 | Epoch: 137 | Iter: 417800 | Total Loss: 0.002658 | Recon Loss: 0.002226 | Commit Loss: 0.000865 | Perplexity: 689.672228
2025-09-14 20:56:51,433 Stage: Train 0.5 | Epoch: 137 | Iter: 418000 | Total Loss: 0.002641 | Recon Loss: 0.002212 | Commit Loss: 0.000859 | Perplexity: 690.651237
2025-09-14 20:56:59,162 Stage: Train 0.5 | Epoch: 137 | Iter: 418200 | Total Loss: 0.002685 | Recon Loss: 0.002258 | Commit Loss: 0.000855 | Perplexity: 686.465171
2025-09-14 20:57:06,846 Stage: Train 0.5 | Epoch: 137 | Iter: 418400 | Total Loss: 0.002620 | Recon Loss: 0.002191 | Commit Loss: 0.000858 | Perplexity: 687.897420
2025-09-14 20:57:14,530 Stage: Train 0.5 | Epoch: 137 | Iter: 418600 | Total Loss: 0.002625 | Recon Loss: 0.002197 | Commit Loss: 0.000855 | Perplexity: 688.360442
2025-09-14 20:57:22,197 Stage: Train 0.5 | Epoch: 137 | Iter: 418800 | Total Loss: 0.002663 | Recon Loss: 0.002235 | Commit Loss: 0.000855 | Perplexity: 689.646389
2025-09-14 20:57:29,885 Stage: Train 0.5 | Epoch: 137 | Iter: 419000 | Total Loss: 0.002632 | Recon Loss: 0.002197 | Commit Loss: 0.000871 | Perplexity: 691.134218
2025-09-14 20:57:37,596 Stage: Train 0.5 | Epoch: 137 | Iter: 419200 | Total Loss: 0.002631 | Recon Loss: 0.002199 | Commit Loss: 0.000864 | Perplexity: 690.287865
Trainning Epoch:  84%|████████▎ | 138/165 [4:34:50<53:15, 118.37s/it]2025-09-14 20:57:45,345 Stage: Train 0.5 | Epoch: 138 | Iter: 419400 | Total Loss: 0.002697 | Recon Loss: 0.002271 | Commit Loss: 0.000853 | Perplexity: 687.341255
2025-09-14 20:57:53,033 Stage: Train 0.5 | Epoch: 138 | Iter: 419600 | Total Loss: 0.002621 | Recon Loss: 0.002197 | Commit Loss: 0.000849 | Perplexity: 688.892316
2025-09-14 20:58:00,725 Stage: Train 0.5 | Epoch: 138 | Iter: 419800 | Total Loss: 0.002627 | Recon Loss: 0.002204 | Commit Loss: 0.000846 | Perplexity: 687.711883
2025-09-14 20:58:08,413 Stage: Train 0.5 | Epoch: 138 | Iter: 420000 | Total Loss: 0.002616 | Recon Loss: 0.002186 | Commit Loss: 0.000860 | Perplexity: 689.421330
2025-09-14 20:58:08,414 Saving model at iteration 420000
2025-09-14 20:58:08,558 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_139_step_420000
2025-09-14 20:58:08,704 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_139_step_420000/pytorch_model.bin
2025-09-14 20:58:08,947 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_139_step_420000/optimizer.bin
2025-09-14 20:58:08,947 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_139_step_420000/scheduler.bin
2025-09-14 20:58:08,948 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_139_step_420000/random_states_0.pkl
2025-09-14 20:58:16,633 Stage: Train 0.5 | Epoch: 138 | Iter: 420200 | Total Loss: 0.002654 | Recon Loss: 0.002224 | Commit Loss: 0.000860 | Perplexity: 692.781087
2025-09-14 20:58:24,301 Stage: Train 0.5 | Epoch: 138 | Iter: 420400 | Total Loss: 0.002636 | Recon Loss: 0.002208 | Commit Loss: 0.000854 | Perplexity: 688.750181
2025-09-14 20:58:31,974 Stage: Train 0.5 | Epoch: 138 | Iter: 420600 | Total Loss: 0.002651 | Recon Loss: 0.002221 | Commit Loss: 0.000859 | Perplexity: 688.503397
2025-09-14 20:58:39,645 Stage: Train 0.5 | Epoch: 138 | Iter: 420800 | Total Loss: 0.002601 | Recon Loss: 0.002174 | Commit Loss: 0.000853 | Perplexity: 684.835588
2025-09-14 20:58:47,301 Stage: Train 0.5 | Epoch: 138 | Iter: 421000 | Total Loss: 0.002641 | Recon Loss: 0.002209 | Commit Loss: 0.000864 | Perplexity: 692.845622
2025-09-14 20:58:55,039 Stage: Train 0.5 | Epoch: 138 | Iter: 421200 | Total Loss: 0.002601 | Recon Loss: 0.002175 | Commit Loss: 0.000852 | Perplexity: 687.913440
2025-09-14 20:59:02,746 Stage: Train 0.5 | Epoch: 138 | Iter: 421400 | Total Loss: 0.002671 | Recon Loss: 0.002240 | Commit Loss: 0.000863 | Perplexity: 693.225403
2025-09-14 20:59:10,430 Stage: Train 0.5 | Epoch: 138 | Iter: 421600 | Total Loss: 0.002657 | Recon Loss: 0.002229 | Commit Loss: 0.000856 | Perplexity: 686.016537
2025-09-14 20:59:18,105 Stage: Train 0.5 | Epoch: 138 | Iter: 421800 | Total Loss: 0.002655 | Recon Loss: 0.002229 | Commit Loss: 0.000853 | Perplexity: 689.172693
2025-09-14 20:59:25,789 Stage: Train 0.5 | Epoch: 138 | Iter: 422000 | Total Loss: 0.002630 | Recon Loss: 0.002202 | Commit Loss: 0.000855 | Perplexity: 688.508885
2025-09-14 20:59:33,525 Stage: Train 0.5 | Epoch: 138 | Iter: 422200 | Total Loss: 0.002631 | Recon Loss: 0.002209 | Commit Loss: 0.000843 | Perplexity: 684.998193
Trainning Epoch:  84%|████████▍ | 139/165 [4:36:47<51:09, 118.08s/it]2025-09-14 20:59:41,239 Stage: Train 0.5 | Epoch: 139 | Iter: 422400 | Total Loss: 0.002620 | Recon Loss: 0.002192 | Commit Loss: 0.000856 | Perplexity: 690.824058
2025-09-14 20:59:48,958 Stage: Train 0.5 | Epoch: 139 | Iter: 422600 | Total Loss: 0.002622 | Recon Loss: 0.002198 | Commit Loss: 0.000847 | Perplexity: 687.075613
2025-09-14 20:59:56,670 Stage: Train 0.5 | Epoch: 139 | Iter: 422800 | Total Loss: 0.002626 | Recon Loss: 0.002199 | Commit Loss: 0.000854 | Perplexity: 688.102065
2025-09-14 21:00:04,387 Stage: Train 0.5 | Epoch: 139 | Iter: 423000 | Total Loss: 0.002650 | Recon Loss: 0.002222 | Commit Loss: 0.000857 | Perplexity: 690.022159
2025-09-14 21:00:12,121 Stage: Train 0.5 | Epoch: 139 | Iter: 423200 | Total Loss: 0.002618 | Recon Loss: 0.002191 | Commit Loss: 0.000854 | Perplexity: 691.198316
2025-09-14 21:00:19,881 Stage: Train 0.5 | Epoch: 139 | Iter: 423400 | Total Loss: 0.002624 | Recon Loss: 0.002198 | Commit Loss: 0.000853 | Perplexity: 690.334939
2025-09-14 21:00:27,593 Stage: Train 0.5 | Epoch: 139 | Iter: 423600 | Total Loss: 0.002628 | Recon Loss: 0.002204 | Commit Loss: 0.000848 | Perplexity: 688.776224
2025-09-14 21:00:35,286 Stage: Train 0.5 | Epoch: 139 | Iter: 423800 | Total Loss: 0.002626 | Recon Loss: 0.002201 | Commit Loss: 0.000850 | Perplexity: 686.298596
2025-09-14 21:00:43,018 Stage: Train 0.5 | Epoch: 139 | Iter: 424000 | Total Loss: 0.002654 | Recon Loss: 0.002229 | Commit Loss: 0.000851 | Perplexity: 688.039990
2025-09-14 21:00:50,743 Stage: Train 0.5 | Epoch: 139 | Iter: 424200 | Total Loss: 0.002645 | Recon Loss: 0.002216 | Commit Loss: 0.000858 | Perplexity: 688.590789
2025-09-14 21:00:58,457 Stage: Train 0.5 | Epoch: 139 | Iter: 424400 | Total Loss: 0.002640 | Recon Loss: 0.002210 | Commit Loss: 0.000861 | Perplexity: 689.440479
2025-09-14 21:01:06,186 Stage: Train 0.5 | Epoch: 139 | Iter: 424600 | Total Loss: 0.002628 | Recon Loss: 0.002199 | Commit Loss: 0.000858 | Perplexity: 690.831895
2025-09-14 21:01:13,878 Stage: Train 0.5 | Epoch: 139 | Iter: 424800 | Total Loss: 0.002632 | Recon Loss: 0.002205 | Commit Loss: 0.000853 | Perplexity: 690.687818
2025-09-14 21:01:21,566 Stage: Train 0.5 | Epoch: 139 | Iter: 425000 | Total Loss: 0.002632 | Recon Loss: 0.002201 | Commit Loss: 0.000862 | Perplexity: 689.006771
2025-09-14 21:01:29,309 Stage: Train 0.5 | Epoch: 139 | Iter: 425200 | Total Loss: 0.002619 | Recon Loss: 0.002191 | Commit Loss: 0.000857 | Perplexity: 691.576485
Trainning Epoch:  85%|████████▍ | 140/165 [4:38:44<49:05, 117.84s/it]2025-09-14 21:01:37,074 Stage: Train 0.5 | Epoch: 140 | Iter: 425400 | Total Loss: 0.002632 | Recon Loss: 0.002209 | Commit Loss: 0.000846 | Perplexity: 687.290154
2025-09-14 21:01:44,742 Stage: Train 0.5 | Epoch: 140 | Iter: 425600 | Total Loss: 0.002667 | Recon Loss: 0.002241 | Commit Loss: 0.000853 | Perplexity: 690.229253
2025-09-14 21:01:52,439 Stage: Train 0.5 | Epoch: 140 | Iter: 425800 | Total Loss: 0.002640 | Recon Loss: 0.002213 | Commit Loss: 0.000855 | Perplexity: 692.029652
2025-09-14 21:02:00,123 Stage: Train 0.5 | Epoch: 140 | Iter: 426000 | Total Loss: 0.002624 | Recon Loss: 0.002194 | Commit Loss: 0.000859 | Perplexity: 689.075778
2025-09-14 21:02:07,794 Stage: Train 0.5 | Epoch: 140 | Iter: 426200 | Total Loss: 0.002576 | Recon Loss: 0.002153 | Commit Loss: 0.000846 | Perplexity: 685.950379
2025-09-14 21:02:15,494 Stage: Train 0.5 | Epoch: 140 | Iter: 426400 | Total Loss: 0.002650 | Recon Loss: 0.002222 | Commit Loss: 0.000855 | Perplexity: 688.579614
2025-09-14 21:02:23,165 Stage: Train 0.5 | Epoch: 140 | Iter: 426600 | Total Loss: 0.002630 | Recon Loss: 0.002202 | Commit Loss: 0.000855 | Perplexity: 691.601676
2025-09-14 21:02:30,874 Stage: Train 0.5 | Epoch: 140 | Iter: 426800 | Total Loss: 0.002621 | Recon Loss: 0.002196 | Commit Loss: 0.000849 | Perplexity: 691.378852
2025-09-14 21:02:38,569 Stage: Train 0.5 | Epoch: 140 | Iter: 427000 | Total Loss: 0.002622 | Recon Loss: 0.002195 | Commit Loss: 0.000854 | Perplexity: 689.346174
2025-09-14 21:02:46,293 Stage: Train 0.5 | Epoch: 140 | Iter: 427200 | Total Loss: 0.002632 | Recon Loss: 0.002200 | Commit Loss: 0.000862 | Perplexity: 692.950905
2025-09-14 21:02:54,029 Stage: Train 0.5 | Epoch: 140 | Iter: 427400 | Total Loss: 0.002623 | Recon Loss: 0.002198 | Commit Loss: 0.000849 | Perplexity: 689.802536
2025-09-14 21:03:01,718 Stage: Train 0.5 | Epoch: 140 | Iter: 427600 | Total Loss: 0.002622 | Recon Loss: 0.002198 | Commit Loss: 0.000848 | Perplexity: 689.532098
2025-09-14 21:03:09,389 Stage: Train 0.5 | Epoch: 140 | Iter: 427800 | Total Loss: 0.002618 | Recon Loss: 0.002191 | Commit Loss: 0.000853 | Perplexity: 688.393672
2025-09-14 21:03:17,097 Stage: Train 0.5 | Epoch: 140 | Iter: 428000 | Total Loss: 0.002598 | Recon Loss: 0.002172 | Commit Loss: 0.000853 | Perplexity: 684.928859
2025-09-14 21:03:24,785 Stage: Train 0.5 | Epoch: 140 | Iter: 428200 | Total Loss: 0.002597 | Recon Loss: 0.002174 | Commit Loss: 0.000846 | Perplexity: 689.227426
Trainning Epoch:  85%|████████▌ | 141/165 [4:40:41<47:01, 117.55s/it]2025-09-14 21:03:32,509 Stage: Train 0.5 | Epoch: 141 | Iter: 428400 | Total Loss: 0.002651 | Recon Loss: 0.002226 | Commit Loss: 0.000849 | Perplexity: 690.078192
2025-09-14 21:03:40,178 Stage: Train 0.5 | Epoch: 141 | Iter: 428600 | Total Loss: 0.002668 | Recon Loss: 0.002242 | Commit Loss: 0.000851 | Perplexity: 688.873148
2025-09-14 21:03:47,878 Stage: Train 0.5 | Epoch: 141 | Iter: 428800 | Total Loss: 0.002644 | Recon Loss: 0.002216 | Commit Loss: 0.000857 | Perplexity: 693.265255
2025-09-14 21:03:55,570 Stage: Train 0.5 | Epoch: 141 | Iter: 429000 | Total Loss: 0.002598 | Recon Loss: 0.002171 | Commit Loss: 0.000854 | Perplexity: 690.261295
2025-09-14 21:04:03,320 Stage: Train 0.5 | Epoch: 141 | Iter: 429200 | Total Loss: 0.002580 | Recon Loss: 0.002155 | Commit Loss: 0.000851 | Perplexity: 690.037969
2025-09-14 21:04:11,413 Stage: Train 0.5 | Epoch: 141 | Iter: 429400 | Total Loss: 0.002651 | Recon Loss: 0.002227 | Commit Loss: 0.000849 | Perplexity: 690.380865
2025-09-14 21:04:19,533 Stage: Train 0.5 | Epoch: 141 | Iter: 429600 | Total Loss: 0.002599 | Recon Loss: 0.002168 | Commit Loss: 0.000862 | Perplexity: 692.090031
2025-09-14 21:04:27,631 Stage: Train 0.5 | Epoch: 141 | Iter: 429800 | Total Loss: 0.002608 | Recon Loss: 0.002182 | Commit Loss: 0.000851 | Perplexity: 689.559023
2025-09-14 21:04:35,717 Stage: Train 0.5 | Epoch: 141 | Iter: 430000 | Total Loss: 0.002632 | Recon Loss: 0.002205 | Commit Loss: 0.000855 | Perplexity: 689.812227
2025-09-14 21:04:43,766 Stage: Train 0.5 | Epoch: 141 | Iter: 430200 | Total Loss: 0.002597 | Recon Loss: 0.002174 | Commit Loss: 0.000846 | Perplexity: 690.606643
2025-09-14 21:04:51,489 Stage: Train 0.5 | Epoch: 141 | Iter: 430400 | Total Loss: 0.002561 | Recon Loss: 0.002141 | Commit Loss: 0.000840 | Perplexity: 684.008675
2025-09-14 21:04:59,194 Stage: Train 0.5 | Epoch: 141 | Iter: 430600 | Total Loss: 0.002628 | Recon Loss: 0.002201 | Commit Loss: 0.000854 | Perplexity: 689.837605
2025-09-14 21:05:06,869 Stage: Train 0.5 | Epoch: 141 | Iter: 430800 | Total Loss: 0.002590 | Recon Loss: 0.002167 | Commit Loss: 0.000846 | Perplexity: 688.480004
2025-09-14 21:05:14,562 Stage: Train 0.5 | Epoch: 141 | Iter: 431000 | Total Loss: 0.002596 | Recon Loss: 0.002175 | Commit Loss: 0.000841 | Perplexity: 690.088929
2025-09-14 21:05:22,242 Stage: Train 0.5 | Epoch: 141 | Iter: 431200 | Total Loss: 0.002624 | Recon Loss: 0.002197 | Commit Loss: 0.000855 | Perplexity: 686.668157
Trainning Epoch:  86%|████████▌ | 142/165 [4:42:40<45:13, 117.96s/it]2025-09-14 21:05:29,963 Stage: Train 0.5 | Epoch: 142 | Iter: 431400 | Total Loss: 0.002652 | Recon Loss: 0.002226 | Commit Loss: 0.000853 | Perplexity: 688.980963
2025-09-14 21:05:37,651 Stage: Train 0.5 | Epoch: 142 | Iter: 431600 | Total Loss: 0.002613 | Recon Loss: 0.002194 | Commit Loss: 0.000838 | Perplexity: 688.808831
2025-09-14 21:05:45,418 Stage: Train 0.5 | Epoch: 142 | Iter: 431800 | Total Loss: 0.002635 | Recon Loss: 0.002215 | Commit Loss: 0.000839 | Perplexity: 689.328694
2025-09-14 21:05:53,190 Stage: Train 0.5 | Epoch: 142 | Iter: 432000 | Total Loss: 0.002600 | Recon Loss: 0.002176 | Commit Loss: 0.000847 | Perplexity: 690.379858
2025-09-14 21:06:00,992 Stage: Train 0.5 | Epoch: 142 | Iter: 432200 | Total Loss: 0.002575 | Recon Loss: 0.002155 | Commit Loss: 0.000840 | Perplexity: 687.622882
2025-09-14 21:06:08,814 Stage: Train 0.5 | Epoch: 142 | Iter: 432400 | Total Loss: 0.002607 | Recon Loss: 0.002185 | Commit Loss: 0.000844 | Perplexity: 688.182418
2025-09-14 21:06:16,610 Stage: Train 0.5 | Epoch: 142 | Iter: 432600 | Total Loss: 0.002583 | Recon Loss: 0.002159 | Commit Loss: 0.000847 | Perplexity: 690.919043
2025-09-14 21:06:24,741 Stage: Train 0.5 | Epoch: 142 | Iter: 432800 | Total Loss: 0.002601 | Recon Loss: 0.002181 | Commit Loss: 0.000838 | Perplexity: 687.660777
2025-09-14 21:06:32,545 Stage: Train 0.5 | Epoch: 142 | Iter: 433000 | Total Loss: 0.002605 | Recon Loss: 0.002181 | Commit Loss: 0.000848 | Perplexity: 690.215550
2025-09-14 21:06:40,233 Stage: Train 0.5 | Epoch: 142 | Iter: 433200 | Total Loss: 0.002628 | Recon Loss: 0.002209 | Commit Loss: 0.000838 | Perplexity: 687.892723
2025-09-14 21:06:47,929 Stage: Train 0.5 | Epoch: 142 | Iter: 433400 | Total Loss: 0.002587 | Recon Loss: 0.002165 | Commit Loss: 0.000845 | Perplexity: 691.000669
2025-09-14 21:06:55,639 Stage: Train 0.5 | Epoch: 142 | Iter: 433600 | Total Loss: 0.002628 | Recon Loss: 0.002201 | Commit Loss: 0.000853 | Perplexity: 689.553910
2025-09-14 21:07:03,628 Stage: Train 0.5 | Epoch: 142 | Iter: 433800 | Total Loss: 0.002610 | Recon Loss: 0.002186 | Commit Loss: 0.000849 | Perplexity: 688.484574
2025-09-14 21:07:11,523 Stage: Train 0.5 | Epoch: 142 | Iter: 434000 | Total Loss: 0.002639 | Recon Loss: 0.002219 | Commit Loss: 0.000841 | Perplexity: 687.222202
2025-09-14 21:07:19,186 Stage: Train 0.5 | Epoch: 142 | Iter: 434200 | Total Loss: 0.002625 | Recon Loss: 0.002198 | Commit Loss: 0.000855 | Perplexity: 690.061571
2025-09-14 21:07:26,845 Stage: Train 0.5 | Epoch: 142 | Iter: 434400 | Total Loss: 0.002629 | Recon Loss: 0.002205 | Commit Loss: 0.000849 | Perplexity: 689.324424
Trainning Epoch:  87%|████████▋ | 143/165 [4:44:38<43:17, 118.08s/it]2025-09-14 21:07:34,861 Stage: Train 0.5 | Epoch: 143 | Iter: 434600 | Total Loss: 0.002584 | Recon Loss: 0.002165 | Commit Loss: 0.000837 | Perplexity: 688.469780
2025-09-14 21:07:42,947 Stage: Train 0.5 | Epoch: 143 | Iter: 434800 | Total Loss: 0.002594 | Recon Loss: 0.002170 | Commit Loss: 0.000846 | Perplexity: 690.679097
2025-09-14 21:07:51,048 Stage: Train 0.5 | Epoch: 143 | Iter: 435000 | Total Loss: 0.002605 | Recon Loss: 0.002184 | Commit Loss: 0.000843 | Perplexity: 691.963165
2025-09-14 21:07:59,208 Stage: Train 0.5 | Epoch: 143 | Iter: 435200 | Total Loss: 0.002640 | Recon Loss: 0.002221 | Commit Loss: 0.000838 | Perplexity: 690.052016
2025-09-14 21:08:07,331 Stage: Train 0.5 | Epoch: 143 | Iter: 435400 | Total Loss: 0.002603 | Recon Loss: 0.002180 | Commit Loss: 0.000845 | Perplexity: 686.962788
2025-09-14 21:08:15,463 Stage: Train 0.5 | Epoch: 143 | Iter: 435600 | Total Loss: 0.002620 | Recon Loss: 0.002196 | Commit Loss: 0.000848 | Perplexity: 688.076759
2025-09-14 21:08:23,461 Stage: Train 0.5 | Epoch: 143 | Iter: 435800 | Total Loss: 0.002615 | Recon Loss: 0.002197 | Commit Loss: 0.000835 | Perplexity: 690.978647
2025-09-14 21:08:31,238 Stage: Train 0.5 | Epoch: 143 | Iter: 436000 | Total Loss: 0.002589 | Recon Loss: 0.002167 | Commit Loss: 0.000843 | Perplexity: 690.123716
2025-09-14 21:08:38,953 Stage: Train 0.5 | Epoch: 143 | Iter: 436200 | Total Loss: 0.002579 | Recon Loss: 0.002153 | Commit Loss: 0.000852 | Perplexity: 693.674749
2025-09-14 21:08:46,621 Stage: Train 0.5 | Epoch: 143 | Iter: 436400 | Total Loss: 0.002621 | Recon Loss: 0.002205 | Commit Loss: 0.000832 | Perplexity: 686.008571
2025-09-14 21:08:54,293 Stage: Train 0.5 | Epoch: 143 | Iter: 436600 | Total Loss: 0.002589 | Recon Loss: 0.002161 | Commit Loss: 0.000856 | Perplexity: 688.928620
2025-09-14 21:09:02,280 Stage: Train 0.5 | Epoch: 143 | Iter: 436800 | Total Loss: 0.002586 | Recon Loss: 0.002164 | Commit Loss: 0.000843 | Perplexity: 690.066560
2025-09-14 21:09:10,423 Stage: Train 0.5 | Epoch: 143 | Iter: 437000 | Total Loss: 0.002624 | Recon Loss: 0.002201 | Commit Loss: 0.000846 | Perplexity: 690.093571
2025-09-14 21:09:18,500 Stage: Train 0.5 | Epoch: 143 | Iter: 437200 | Total Loss: 0.002557 | Recon Loss: 0.002139 | Commit Loss: 0.000836 | Perplexity: 689.245903
2025-09-14 21:09:26,610 Stage: Train 0.5 | Epoch: 143 | Iter: 437400 | Total Loss: 0.002605 | Recon Loss: 0.002180 | Commit Loss: 0.000848 | Perplexity: 690.333816
Trainning Epoch:  87%|████████▋ | 144/165 [4:46:40<41:40, 119.08s/it]2025-09-14 21:09:34,776 Stage: Train 0.5 | Epoch: 144 | Iter: 437600 | Total Loss: 0.002624 | Recon Loss: 0.002201 | Commit Loss: 0.000846 | Perplexity: 689.266859
2025-09-14 21:09:42,793 Stage: Train 0.5 | Epoch: 144 | Iter: 437800 | Total Loss: 0.002596 | Recon Loss: 0.002182 | Commit Loss: 0.000828 | Perplexity: 686.961645
2025-09-14 21:09:50,643 Stage: Train 0.5 | Epoch: 144 | Iter: 438000 | Total Loss: 0.002611 | Recon Loss: 0.002186 | Commit Loss: 0.000851 | Perplexity: 692.058716
2025-09-14 21:09:58,469 Stage: Train 0.5 | Epoch: 144 | Iter: 438200 | Total Loss: 0.002606 | Recon Loss: 0.002188 | Commit Loss: 0.000836 | Perplexity: 688.723804
2025-09-14 21:10:06,285 Stage: Train 0.5 | Epoch: 144 | Iter: 438400 | Total Loss: 0.002574 | Recon Loss: 0.002154 | Commit Loss: 0.000840 | Perplexity: 693.060085
2025-09-14 21:10:14,107 Stage: Train 0.5 | Epoch: 144 | Iter: 438600 | Total Loss: 0.002617 | Recon Loss: 0.002193 | Commit Loss: 0.000846 | Perplexity: 688.751413
2025-09-14 21:10:21,798 Stage: Train 0.5 | Epoch: 144 | Iter: 438800 | Total Loss: 0.002561 | Recon Loss: 0.002145 | Commit Loss: 0.000832 | Perplexity: 688.333880
2025-09-14 21:10:29,469 Stage: Train 0.5 | Epoch: 144 | Iter: 439000 | Total Loss: 0.002606 | Recon Loss: 0.002186 | Commit Loss: 0.000840 | Perplexity: 691.044637
2025-09-14 21:10:37,105 Stage: Train 0.5 | Epoch: 144 | Iter: 439200 | Total Loss: 0.002620 | Recon Loss: 0.002201 | Commit Loss: 0.000838 | Perplexity: 691.242354
2025-09-14 21:10:44,777 Stage: Train 0.5 | Epoch: 144 | Iter: 439400 | Total Loss: 0.002554 | Recon Loss: 0.002134 | Commit Loss: 0.000840 | Perplexity: 690.161200
2025-09-14 21:10:52,771 Stage: Train 0.5 | Epoch: 144 | Iter: 439600 | Total Loss: 0.002607 | Recon Loss: 0.002190 | Commit Loss: 0.000833 | Perplexity: 688.087429
2025-09-14 21:11:00,621 Stage: Train 0.5 | Epoch: 144 | Iter: 439800 | Total Loss: 0.002619 | Recon Loss: 0.002197 | Commit Loss: 0.000845 | Perplexity: 690.587677
2025-09-14 21:11:08,465 Stage: Train 0.5 | Epoch: 144 | Iter: 440000 | Total Loss: 0.002614 | Recon Loss: 0.002191 | Commit Loss: 0.000846 | Perplexity: 691.560943
2025-09-14 21:11:08,465 Saving model at iteration 440000
2025-09-14 21:11:08,614 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_145_step_440000
2025-09-14 21:11:08,754 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_145_step_440000/pytorch_model.bin
2025-09-14 21:11:09,006 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_145_step_440000/optimizer.bin
2025-09-14 21:11:09,006 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_145_step_440000/scheduler.bin
2025-09-14 21:11:09,007 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_145_step_440000/random_states_0.pkl
2025-09-14 21:11:17,030 Stage: Train 0.5 | Epoch: 144 | Iter: 440200 | Total Loss: 0.002588 | Recon Loss: 0.002167 | Commit Loss: 0.000842 | Perplexity: 690.971775
2025-09-14 21:11:24,999 Stage: Train 0.5 | Epoch: 144 | Iter: 440400 | Total Loss: 0.002617 | Recon Loss: 0.002199 | Commit Loss: 0.000836 | Perplexity: 687.579561
Trainning Epoch:  88%|████████▊ | 145/165 [4:48:40<39:46, 119.33s/it]2025-09-14 21:11:33,132 Stage: Train 0.5 | Epoch: 145 | Iter: 440600 | Total Loss: 0.002584 | Recon Loss: 0.002165 | Commit Loss: 0.000838 | Perplexity: 688.131722
2025-09-14 21:11:41,248 Stage: Train 0.5 | Epoch: 145 | Iter: 440800 | Total Loss: 0.002599 | Recon Loss: 0.002182 | Commit Loss: 0.000835 | Perplexity: 689.579055
2025-09-14 21:11:49,382 Stage: Train 0.5 | Epoch: 145 | Iter: 441000 | Total Loss: 0.002574 | Recon Loss: 0.002153 | Commit Loss: 0.000841 | Perplexity: 691.918750
2025-09-14 21:11:57,522 Stage: Train 0.5 | Epoch: 145 | Iter: 441200 | Total Loss: 0.002576 | Recon Loss: 0.002160 | Commit Loss: 0.000831 | Perplexity: 686.982910
2025-09-14 21:12:05,626 Stage: Train 0.5 | Epoch: 145 | Iter: 441400 | Total Loss: 0.002608 | Recon Loss: 0.002192 | Commit Loss: 0.000832 | Perplexity: 689.554835
2025-09-14 21:12:13,758 Stage: Train 0.5 | Epoch: 145 | Iter: 441600 | Total Loss: 0.002618 | Recon Loss: 0.002200 | Commit Loss: 0.000836 | Perplexity: 690.916615
2025-09-14 21:12:21,890 Stage: Train 0.5 | Epoch: 145 | Iter: 441800 | Total Loss: 0.002590 | Recon Loss: 0.002176 | Commit Loss: 0.000827 | Perplexity: 688.622703
2025-09-14 21:12:30,031 Stage: Train 0.5 | Epoch: 145 | Iter: 442000 | Total Loss: 0.002555 | Recon Loss: 0.002135 | Commit Loss: 0.000841 | Perplexity: 692.397152
2025-09-14 21:12:38,148 Stage: Train 0.5 | Epoch: 145 | Iter: 442200 | Total Loss: 0.002584 | Recon Loss: 0.002167 | Commit Loss: 0.000834 | Perplexity: 690.737815
2025-09-14 21:12:46,244 Stage: Train 0.5 | Epoch: 145 | Iter: 442400 | Total Loss: 0.002602 | Recon Loss: 0.002184 | Commit Loss: 0.000837 | Perplexity: 688.869845
2025-09-14 21:12:54,367 Stage: Train 0.5 | Epoch: 145 | Iter: 442600 | Total Loss: 0.002628 | Recon Loss: 0.002212 | Commit Loss: 0.000832 | Perplexity: 691.325330
2025-09-14 21:13:02,580 Stage: Train 0.5 | Epoch: 145 | Iter: 442800 | Total Loss: 0.002603 | Recon Loss: 0.002184 | Commit Loss: 0.000837 | Perplexity: 688.628915
2025-09-14 21:13:10,566 Stage: Train 0.5 | Epoch: 145 | Iter: 443000 | Total Loss: 0.002607 | Recon Loss: 0.002188 | Commit Loss: 0.000839 | Perplexity: 689.846502
2025-09-14 21:13:18,490 Stage: Train 0.5 | Epoch: 145 | Iter: 443200 | Total Loss: 0.002597 | Recon Loss: 0.002179 | Commit Loss: 0.000836 | Perplexity: 689.878712
2025-09-14 21:13:26,608 Stage: Train 0.5 | Epoch: 145 | Iter: 443400 | Total Loss: 0.002570 | Recon Loss: 0.002149 | Commit Loss: 0.000842 | Perplexity: 687.923142
Trainning Epoch:  88%|████████▊ | 146/165 [4:50:43<38:09, 120.47s/it]2025-09-14 21:13:34,754 Stage: Train 0.5 | Epoch: 146 | Iter: 443600 | Total Loss: 0.002622 | Recon Loss: 0.002202 | Commit Loss: 0.000841 | Perplexity: 688.533260
2025-09-14 21:13:42,924 Stage: Train 0.5 | Epoch: 146 | Iter: 443800 | Total Loss: 0.002578 | Recon Loss: 0.002162 | Commit Loss: 0.000832 | Perplexity: 686.987894
2025-09-14 21:13:51,022 Stage: Train 0.5 | Epoch: 146 | Iter: 444000 | Total Loss: 0.002553 | Recon Loss: 0.002138 | Commit Loss: 0.000829 | Perplexity: 689.751454
2025-09-14 21:13:59,212 Stage: Train 0.5 | Epoch: 146 | Iter: 444200 | Total Loss: 0.002579 | Recon Loss: 0.002159 | Commit Loss: 0.000839 | Perplexity: 692.755677
2025-09-14 21:14:07,347 Stage: Train 0.5 | Epoch: 146 | Iter: 444400 | Total Loss: 0.002588 | Recon Loss: 0.002171 | Commit Loss: 0.000834 | Perplexity: 688.781196
2025-09-14 21:14:15,477 Stage: Train 0.5 | Epoch: 146 | Iter: 444600 | Total Loss: 0.002591 | Recon Loss: 0.002174 | Commit Loss: 0.000835 | Perplexity: 687.797600
2025-09-14 21:14:23,630 Stage: Train 0.5 | Epoch: 146 | Iter: 444800 | Total Loss: 0.002568 | Recon Loss: 0.002149 | Commit Loss: 0.000840 | Perplexity: 691.648394
2025-09-14 21:14:31,794 Stage: Train 0.5 | Epoch: 146 | Iter: 445000 | Total Loss: 0.002580 | Recon Loss: 0.002163 | Commit Loss: 0.000833 | Perplexity: 688.834995
2025-09-14 21:14:39,908 Stage: Train 0.5 | Epoch: 146 | Iter: 445200 | Total Loss: 0.002540 | Recon Loss: 0.002128 | Commit Loss: 0.000825 | Perplexity: 686.617809
2025-09-14 21:14:48,030 Stage: Train 0.5 | Epoch: 146 | Iter: 445400 | Total Loss: 0.002604 | Recon Loss: 0.002189 | Commit Loss: 0.000831 | Perplexity: 690.291383
2025-09-14 21:14:55,958 Stage: Train 0.5 | Epoch: 146 | Iter: 445600 | Total Loss: 0.002573 | Recon Loss: 0.002153 | Commit Loss: 0.000841 | Perplexity: 690.696980
2025-09-14 21:15:03,782 Stage: Train 0.5 | Epoch: 146 | Iter: 445800 | Total Loss: 0.002579 | Recon Loss: 0.002160 | Commit Loss: 0.000838 | Perplexity: 691.143496
2025-09-14 21:15:11,568 Stage: Train 0.5 | Epoch: 146 | Iter: 446000 | Total Loss: 0.002583 | Recon Loss: 0.002163 | Commit Loss: 0.000841 | Perplexity: 691.900566
2025-09-14 21:15:19,389 Stage: Train 0.5 | Epoch: 146 | Iter: 446200 | Total Loss: 0.002565 | Recon Loss: 0.002146 | Commit Loss: 0.000839 | Perplexity: 693.096429
2025-09-14 21:15:27,193 Stage: Train 0.5 | Epoch: 146 | Iter: 446400 | Total Loss: 0.002573 | Recon Loss: 0.002156 | Commit Loss: 0.000834 | Perplexity: 687.849217
Trainning Epoch:  89%|████████▉ | 147/165 [4:52:45<36:15, 120.88s/it]2025-09-14 21:15:35,025 Stage: Train 0.5 | Epoch: 147 | Iter: 446600 | Total Loss: 0.002625 | Recon Loss: 0.002208 | Commit Loss: 0.000835 | Perplexity: 687.787906
2025-09-14 21:15:42,793 Stage: Train 0.5 | Epoch: 147 | Iter: 446800 | Total Loss: 0.002586 | Recon Loss: 0.002173 | Commit Loss: 0.000825 | Perplexity: 688.293201
2025-09-14 21:15:50,728 Stage: Train 0.5 | Epoch: 147 | Iter: 447000 | Total Loss: 0.002596 | Recon Loss: 0.002180 | Commit Loss: 0.000831 | Perplexity: 689.183950
2025-09-14 21:15:58,573 Stage: Train 0.5 | Epoch: 147 | Iter: 447200 | Total Loss: 0.002556 | Recon Loss: 0.002137 | Commit Loss: 0.000838 | Perplexity: 691.888477
2025-09-14 21:16:06,330 Stage: Train 0.5 | Epoch: 147 | Iter: 447400 | Total Loss: 0.002563 | Recon Loss: 0.002150 | Commit Loss: 0.000825 | Perplexity: 691.019460
2025-09-14 21:16:14,094 Stage: Train 0.5 | Epoch: 147 | Iter: 447600 | Total Loss: 0.002579 | Recon Loss: 0.002165 | Commit Loss: 0.000828 | Perplexity: 689.479572
2025-09-14 21:16:22,049 Stage: Train 0.5 | Epoch: 147 | Iter: 447800 | Total Loss: 0.002578 | Recon Loss: 0.002158 | Commit Loss: 0.000840 | Perplexity: 690.462756
2025-09-14 21:16:30,110 Stage: Train 0.5 | Epoch: 147 | Iter: 448000 | Total Loss: 0.002565 | Recon Loss: 0.002146 | Commit Loss: 0.000837 | Perplexity: 690.190001
2025-09-14 21:16:37,921 Stage: Train 0.5 | Epoch: 147 | Iter: 448200 | Total Loss: 0.002566 | Recon Loss: 0.002149 | Commit Loss: 0.000832 | Perplexity: 692.370765
2025-09-14 21:16:45,722 Stage: Train 0.5 | Epoch: 147 | Iter: 448400 | Total Loss: 0.002574 | Recon Loss: 0.002159 | Commit Loss: 0.000831 | Perplexity: 691.089333
2025-09-14 21:16:53,549 Stage: Train 0.5 | Epoch: 147 | Iter: 448600 | Total Loss: 0.002576 | Recon Loss: 0.002160 | Commit Loss: 0.000833 | Perplexity: 688.235804
2025-09-14 21:17:01,366 Stage: Train 0.5 | Epoch: 147 | Iter: 448800 | Total Loss: 0.002570 | Recon Loss: 0.002152 | Commit Loss: 0.000837 | Perplexity: 691.050889
2025-09-14 21:17:09,130 Stage: Train 0.5 | Epoch: 147 | Iter: 449000 | Total Loss: 0.002610 | Recon Loss: 0.002189 | Commit Loss: 0.000843 | Perplexity: 691.662904
2025-09-14 21:17:16,913 Stage: Train 0.5 | Epoch: 147 | Iter: 449200 | Total Loss: 0.002541 | Recon Loss: 0.002125 | Commit Loss: 0.000831 | Perplexity: 688.876121
2025-09-14 21:17:24,728 Stage: Train 0.5 | Epoch: 147 | Iter: 449400 | Total Loss: 0.002565 | Recon Loss: 0.002148 | Commit Loss: 0.000834 | Perplexity: 691.220333
2025-09-14 21:17:32,566 Stage: Train 0.5 | Epoch: 147 | Iter: 449600 | Total Loss: 0.002594 | Recon Loss: 0.002178 | Commit Loss: 0.000832 | Perplexity: 689.977435
Trainning Epoch:  90%|████████▉ | 148/165 [4:54:44<34:05, 120.34s/it]2025-09-14 21:17:40,418 Stage: Train 0.5 | Epoch: 148 | Iter: 449800 | Total Loss: 0.002557 | Recon Loss: 0.002141 | Commit Loss: 0.000831 | Perplexity: 691.002638
2025-09-14 21:17:48,213 Stage: Train 0.5 | Epoch: 148 | Iter: 450000 | Total Loss: 0.002595 | Recon Loss: 0.002180 | Commit Loss: 0.000832 | Perplexity: 689.767154
2025-09-14 21:17:56,017 Stage: Train 0.5 | Epoch: 148 | Iter: 450200 | Total Loss: 0.002592 | Recon Loss: 0.002173 | Commit Loss: 0.000838 | Perplexity: 694.215756
2025-09-14 21:18:03,919 Stage: Train 0.5 | Epoch: 148 | Iter: 450400 | Total Loss: 0.002566 | Recon Loss: 0.002149 | Commit Loss: 0.000835 | Perplexity: 691.019982
2025-09-14 21:18:11,983 Stage: Train 0.5 | Epoch: 148 | Iter: 450600 | Total Loss: 0.002576 | Recon Loss: 0.002156 | Commit Loss: 0.000838 | Perplexity: 692.018743
2025-09-14 21:18:20,045 Stage: Train 0.5 | Epoch: 148 | Iter: 450800 | Total Loss: 0.002574 | Recon Loss: 0.002160 | Commit Loss: 0.000828 | Perplexity: 691.249811
2025-09-14 21:18:28,497 Stage: Train 0.5 | Epoch: 148 | Iter: 451000 | Total Loss: 0.002539 | Recon Loss: 0.002127 | Commit Loss: 0.000825 | Perplexity: 689.435073
2025-09-14 21:18:36,784 Stage: Train 0.5 | Epoch: 148 | Iter: 451200 | Total Loss: 0.002571 | Recon Loss: 0.002160 | Commit Loss: 0.000822 | Perplexity: 686.640078
2025-09-14 21:18:45,016 Stage: Train 0.5 | Epoch: 148 | Iter: 451400 | Total Loss: 0.002547 | Recon Loss: 0.002133 | Commit Loss: 0.000829 | Perplexity: 688.893180
2025-09-14 21:18:53,234 Stage: Train 0.5 | Epoch: 148 | Iter: 451600 | Total Loss: 0.002549 | Recon Loss: 0.002132 | Commit Loss: 0.000835 | Perplexity: 689.495169
2025-09-14 21:19:01,425 Stage: Train 0.5 | Epoch: 148 | Iter: 451800 | Total Loss: 0.002549 | Recon Loss: 0.002137 | Commit Loss: 0.000824 | Perplexity: 689.756441
2025-09-14 21:19:09,628 Stage: Train 0.5 | Epoch: 148 | Iter: 452000 | Total Loss: 0.002598 | Recon Loss: 0.002185 | Commit Loss: 0.000826 | Perplexity: 689.805531
2025-09-14 21:19:17,822 Stage: Train 0.5 | Epoch: 148 | Iter: 452200 | Total Loss: 0.002572 | Recon Loss: 0.002156 | Commit Loss: 0.000831 | Perplexity: 691.600057
2025-09-14 21:19:26,009 Stage: Train 0.5 | Epoch: 148 | Iter: 452400 | Total Loss: 0.002544 | Recon Loss: 0.002131 | Commit Loss: 0.000826 | Perplexity: 688.253630
2025-09-14 21:19:34,250 Stage: Train 0.5 | Epoch: 148 | Iter: 452600 | Total Loss: 0.002596 | Recon Loss: 0.002178 | Commit Loss: 0.000837 | Perplexity: 693.150280
Trainning Epoch:  90%|█████████ | 149/165 [4:56:47<32:19, 121.22s/it]2025-09-14 21:19:42,462 Stage: Train 0.5 | Epoch: 149 | Iter: 452800 | Total Loss: 0.002543 | Recon Loss: 0.002127 | Commit Loss: 0.000830 | Perplexity: 690.689025
2025-09-14 21:19:50,677 Stage: Train 0.5 | Epoch: 149 | Iter: 453000 | Total Loss: 0.002573 | Recon Loss: 0.002160 | Commit Loss: 0.000827 | Perplexity: 691.803412
2025-09-14 21:19:58,812 Stage: Train 0.5 | Epoch: 149 | Iter: 453200 | Total Loss: 0.002588 | Recon Loss: 0.002175 | Commit Loss: 0.000825 | Perplexity: 689.809401
2025-09-14 21:20:07,074 Stage: Train 0.5 | Epoch: 149 | Iter: 453400 | Total Loss: 0.002574 | Recon Loss: 0.002162 | Commit Loss: 0.000824 | Perplexity: 689.498821
2025-09-14 21:20:15,270 Stage: Train 0.5 | Epoch: 149 | Iter: 453600 | Total Loss: 0.002556 | Recon Loss: 0.002146 | Commit Loss: 0.000819 | Perplexity: 686.835040
2025-09-14 21:20:23,484 Stage: Train 0.5 | Epoch: 149 | Iter: 453800 | Total Loss: 0.002524 | Recon Loss: 0.002111 | Commit Loss: 0.000824 | Perplexity: 688.377180
2025-09-14 21:20:31,688 Stage: Train 0.5 | Epoch: 149 | Iter: 454000 | Total Loss: 0.002551 | Recon Loss: 0.002139 | Commit Loss: 0.000825 | Perplexity: 689.613531
2025-09-14 21:20:39,873 Stage: Train 0.5 | Epoch: 149 | Iter: 454200 | Total Loss: 0.002537 | Recon Loss: 0.002125 | Commit Loss: 0.000825 | Perplexity: 688.998180
2025-09-14 21:20:48,116 Stage: Train 0.5 | Epoch: 149 | Iter: 454400 | Total Loss: 0.002571 | Recon Loss: 0.002158 | Commit Loss: 0.000826 | Perplexity: 689.907075
2025-09-14 21:20:56,305 Stage: Train 0.5 | Epoch: 149 | Iter: 454600 | Total Loss: 0.002580 | Recon Loss: 0.002167 | Commit Loss: 0.000826 | Perplexity: 689.847700
2025-09-14 21:21:04,519 Stage: Train 0.5 | Epoch: 149 | Iter: 454800 | Total Loss: 0.002584 | Recon Loss: 0.002171 | Commit Loss: 0.000825 | Perplexity: 690.470804
2025-09-14 21:21:12,748 Stage: Train 0.5 | Epoch: 149 | Iter: 455000 | Total Loss: 0.002537 | Recon Loss: 0.002125 | Commit Loss: 0.000824 | Perplexity: 691.028573
2025-09-14 21:21:20,970 Stage: Train 0.5 | Epoch: 149 | Iter: 455200 | Total Loss: 0.002574 | Recon Loss: 0.002163 | Commit Loss: 0.000822 | Perplexity: 688.620004
2025-09-14 21:21:29,209 Stage: Train 0.5 | Epoch: 149 | Iter: 455400 | Total Loss: 0.002572 | Recon Loss: 0.002165 | Commit Loss: 0.000815 | Perplexity: 690.869685
2025-09-14 21:21:37,394 Stage: Train 0.5 | Epoch: 149 | Iter: 455600 | Total Loss: 0.002562 | Recon Loss: 0.002146 | Commit Loss: 0.000833 | Perplexity: 690.788581
Trainning Epoch:  91%|█████████ | 150/165 [4:58:52<30:33, 122.27s/it]2025-09-14 21:21:45,620 Stage: Train 0.5 | Epoch: 150 | Iter: 455800 | Total Loss: 0.002542 | Recon Loss: 0.002134 | Commit Loss: 0.000815 | Perplexity: 685.687695
2025-09-14 21:21:53,788 Stage: Train 0.5 | Epoch: 150 | Iter: 456000 | Total Loss: 0.002530 | Recon Loss: 0.002121 | Commit Loss: 0.000818 | Perplexity: 688.730826
2025-09-14 21:22:02,006 Stage: Train 0.5 | Epoch: 150 | Iter: 456200 | Total Loss: 0.002535 | Recon Loss: 0.002124 | Commit Loss: 0.000822 | Perplexity: 690.476597
2025-09-14 21:22:10,310 Stage: Train 0.5 | Epoch: 150 | Iter: 456400 | Total Loss: 0.002557 | Recon Loss: 0.002146 | Commit Loss: 0.000822 | Perplexity: 692.207894
2025-09-14 21:22:18,554 Stage: Train 0.5 | Epoch: 150 | Iter: 456600 | Total Loss: 0.002552 | Recon Loss: 0.002141 | Commit Loss: 0.000823 | Perplexity: 691.537914
2025-09-14 21:22:26,741 Stage: Train 0.5 | Epoch: 150 | Iter: 456800 | Total Loss: 0.002564 | Recon Loss: 0.002148 | Commit Loss: 0.000832 | Perplexity: 693.260935
2025-09-14 21:22:34,941 Stage: Train 0.5 | Epoch: 150 | Iter: 457000 | Total Loss: 0.002590 | Recon Loss: 0.002170 | Commit Loss: 0.000840 | Perplexity: 690.491618
2025-09-14 21:22:43,224 Stage: Train 0.5 | Epoch: 150 | Iter: 457200 | Total Loss: 0.002558 | Recon Loss: 0.002146 | Commit Loss: 0.000824 | Perplexity: 689.145288
2025-09-14 21:22:51,482 Stage: Train 0.5 | Epoch: 150 | Iter: 457400 | Total Loss: 0.002597 | Recon Loss: 0.002187 | Commit Loss: 0.000820 | Perplexity: 688.644063
2025-09-14 21:22:59,697 Stage: Train 0.5 | Epoch: 150 | Iter: 457600 | Total Loss: 0.002510 | Recon Loss: 0.002102 | Commit Loss: 0.000815 | Perplexity: 688.364561
2025-09-14 21:23:07,922 Stage: Train 0.5 | Epoch: 150 | Iter: 457800 | Total Loss: 0.002543 | Recon Loss: 0.002132 | Commit Loss: 0.000823 | Perplexity: 689.936501
2025-09-14 21:23:16,130 Stage: Train 0.5 | Epoch: 150 | Iter: 458000 | Total Loss: 0.002548 | Recon Loss: 0.002131 | Commit Loss: 0.000835 | Perplexity: 692.040067
2025-09-14 21:23:24,365 Stage: Train 0.5 | Epoch: 150 | Iter: 458200 | Total Loss: 0.002560 | Recon Loss: 0.002147 | Commit Loss: 0.000827 | Perplexity: 691.481590
2025-09-14 21:23:32,633 Stage: Train 0.5 | Epoch: 150 | Iter: 458400 | Total Loss: 0.002569 | Recon Loss: 0.002156 | Commit Loss: 0.000827 | Perplexity: 692.214810
2025-09-14 21:23:40,842 Stage: Train 0.5 | Epoch: 150 | Iter: 458600 | Total Loss: 0.002520 | Recon Loss: 0.002105 | Commit Loss: 0.000829 | Perplexity: 688.862038
Trainning Epoch:  92%|█████████▏| 151/165 [5:00:57<28:43, 123.10s/it]2025-09-14 21:23:49,107 Stage: Train 0.5 | Epoch: 151 | Iter: 458800 | Total Loss: 0.002558 | Recon Loss: 0.002144 | Commit Loss: 0.000827 | Perplexity: 690.726085
2025-09-14 21:23:57,328 Stage: Train 0.5 | Epoch: 151 | Iter: 459000 | Total Loss: 0.002585 | Recon Loss: 0.002175 | Commit Loss: 0.000820 | Perplexity: 689.278836
2025-09-14 21:24:05,520 Stage: Train 0.5 | Epoch: 151 | Iter: 459200 | Total Loss: 0.002493 | Recon Loss: 0.002085 | Commit Loss: 0.000816 | Perplexity: 690.834337
2025-09-14 21:24:13,741 Stage: Train 0.5 | Epoch: 151 | Iter: 459400 | Total Loss: 0.002516 | Recon Loss: 0.002105 | Commit Loss: 0.000822 | Perplexity: 689.424100
2025-09-14 21:24:21,951 Stage: Train 0.5 | Epoch: 151 | Iter: 459600 | Total Loss: 0.002570 | Recon Loss: 0.002159 | Commit Loss: 0.000823 | Perplexity: 693.006483
2025-09-14 21:24:30,195 Stage: Train 0.5 | Epoch: 151 | Iter: 459800 | Total Loss: 0.002516 | Recon Loss: 0.002108 | Commit Loss: 0.000817 | Perplexity: 688.658460
2025-09-14 21:24:38,443 Stage: Train 0.5 | Epoch: 151 | Iter: 460000 | Total Loss: 0.002570 | Recon Loss: 0.002159 | Commit Loss: 0.000821 | Perplexity: 687.000764
2025-09-14 21:24:38,443 Saving model at iteration 460000
2025-09-14 21:24:38,991 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_152_step_460000
2025-09-14 21:24:39,132 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_152_step_460000/pytorch_model.bin
2025-09-14 21:24:39,388 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_152_step_460000/optimizer.bin
2025-09-14 21:24:39,389 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_152_step_460000/scheduler.bin
2025-09-14 21:24:39,390 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_152_step_460000/random_states_0.pkl
2025-09-14 21:24:47,625 Stage: Train 0.5 | Epoch: 151 | Iter: 460200 | Total Loss: 0.002557 | Recon Loss: 0.002144 | Commit Loss: 0.000826 | Perplexity: 692.059316
2025-09-14 21:24:55,818 Stage: Train 0.5 | Epoch: 151 | Iter: 460400 | Total Loss: 0.002521 | Recon Loss: 0.002114 | Commit Loss: 0.000814 | Perplexity: 690.321162
2025-09-14 21:25:04,007 Stage: Train 0.5 | Epoch: 151 | Iter: 460600 | Total Loss: 0.002568 | Recon Loss: 0.002155 | Commit Loss: 0.000825 | Perplexity: 691.444772
2025-09-14 21:25:12,222 Stage: Train 0.5 | Epoch: 151 | Iter: 460800 | Total Loss: 0.002587 | Recon Loss: 0.002173 | Commit Loss: 0.000827 | Perplexity: 692.774021
2025-09-14 21:25:20,426 Stage: Train 0.5 | Epoch: 151 | Iter: 461000 | Total Loss: 0.002522 | Recon Loss: 0.002112 | Commit Loss: 0.000820 | Perplexity: 692.727312
2025-09-14 21:25:28,630 Stage: Train 0.5 | Epoch: 151 | Iter: 461200 | Total Loss: 0.002550 | Recon Loss: 0.002134 | Commit Loss: 0.000832 | Perplexity: 692.013333
2025-09-14 21:25:36,873 Stage: Train 0.5 | Epoch: 151 | Iter: 461400 | Total Loss: 0.002576 | Recon Loss: 0.002163 | Commit Loss: 0.000826 | Perplexity: 695.077407
2025-09-14 21:25:45,090 Stage: Train 0.5 | Epoch: 151 | Iter: 461600 | Total Loss: 0.002545 | Recon Loss: 0.002137 | Commit Loss: 0.000816 | Perplexity: 690.201854
Trainning Epoch:  92%|█████████▏| 152/165 [5:03:03<26:50, 123.91s/it]2025-09-14 21:25:53,338 Stage: Train 0.5 | Epoch: 152 | Iter: 461800 | Total Loss: 0.002570 | Recon Loss: 0.002158 | Commit Loss: 0.000824 | Perplexity: 690.514956
2025-09-14 21:26:01,579 Stage: Train 0.5 | Epoch: 152 | Iter: 462000 | Total Loss: 0.002524 | Recon Loss: 0.002117 | Commit Loss: 0.000815 | Perplexity: 691.582693
2025-09-14 21:26:09,775 Stage: Train 0.5 | Epoch: 152 | Iter: 462200 | Total Loss: 0.002562 | Recon Loss: 0.002151 | Commit Loss: 0.000822 | Perplexity: 690.515057
2025-09-14 21:26:18,000 Stage: Train 0.5 | Epoch: 152 | Iter: 462400 | Total Loss: 0.002568 | Recon Loss: 0.002161 | Commit Loss: 0.000814 | Perplexity: 691.354217
2025-09-14 21:26:26,188 Stage: Train 0.5 | Epoch: 152 | Iter: 462600 | Total Loss: 0.002511 | Recon Loss: 0.002103 | Commit Loss: 0.000816 | Perplexity: 693.768680
2025-09-14 21:26:34,434 Stage: Train 0.5 | Epoch: 152 | Iter: 462800 | Total Loss: 0.002554 | Recon Loss: 0.002140 | Commit Loss: 0.000828 | Perplexity: 691.769883
2025-09-14 21:26:42,634 Stage: Train 0.5 | Epoch: 152 | Iter: 463000 | Total Loss: 0.002589 | Recon Loss: 0.002181 | Commit Loss: 0.000816 | Perplexity: 689.047308
2025-09-14 21:26:50,828 Stage: Train 0.5 | Epoch: 152 | Iter: 463200 | Total Loss: 0.002528 | Recon Loss: 0.002119 | Commit Loss: 0.000818 | Perplexity: 689.324987
2025-09-14 21:26:59,005 Stage: Train 0.5 | Epoch: 152 | Iter: 463400 | Total Loss: 0.002573 | Recon Loss: 0.002161 | Commit Loss: 0.000824 | Perplexity: 693.692926
2025-09-14 21:27:07,174 Stage: Train 0.5 | Epoch: 152 | Iter: 463600 | Total Loss: 0.002556 | Recon Loss: 0.002143 | Commit Loss: 0.000826 | Perplexity: 690.466784
2025-09-14 21:27:15,404 Stage: Train 0.5 | Epoch: 152 | Iter: 463800 | Total Loss: 0.002552 | Recon Loss: 0.002143 | Commit Loss: 0.000817 | Perplexity: 691.377469
2025-09-14 21:27:23,619 Stage: Train 0.5 | Epoch: 152 | Iter: 464000 | Total Loss: 0.002564 | Recon Loss: 0.002158 | Commit Loss: 0.000813 | Perplexity: 690.697092
2025-09-14 21:27:31,834 Stage: Train 0.5 | Epoch: 152 | Iter: 464200 | Total Loss: 0.002502 | Recon Loss: 0.002100 | Commit Loss: 0.000804 | Perplexity: 686.349607
2025-09-14 21:27:40,055 Stage: Train 0.5 | Epoch: 152 | Iter: 464400 | Total Loss: 0.002520 | Recon Loss: 0.002115 | Commit Loss: 0.000809 | Perplexity: 691.940786
2025-09-14 21:27:48,266 Stage: Train 0.5 | Epoch: 152 | Iter: 464600 | Total Loss: 0.002525 | Recon Loss: 0.002109 | Commit Loss: 0.000831 | Perplexity: 692.575404
2025-09-14 21:27:56,490 Stage: Train 0.5 | Epoch: 152 | Iter: 464800 | Total Loss: 0.002589 | Recon Loss: 0.002179 | Commit Loss: 0.000820 | Perplexity: 690.008156
Trainning Epoch:  93%|█████████▎| 153/165 [5:05:07<24:49, 124.15s/it]2025-09-14 21:28:04,754 Stage: Train 0.5 | Epoch: 153 | Iter: 465000 | Total Loss: 0.002535 | Recon Loss: 0.002131 | Commit Loss: 0.000808 | Perplexity: 687.806080
2025-09-14 21:28:13,013 Stage: Train 0.5 | Epoch: 153 | Iter: 465200 | Total Loss: 0.002545 | Recon Loss: 0.002140 | Commit Loss: 0.000811 | Perplexity: 691.384507
2025-09-14 21:28:21,214 Stage: Train 0.5 | Epoch: 153 | Iter: 465400 | Total Loss: 0.002521 | Recon Loss: 0.002113 | Commit Loss: 0.000817 | Perplexity: 692.494329
2025-09-14 21:28:29,448 Stage: Train 0.5 | Epoch: 153 | Iter: 465600 | Total Loss: 0.002525 | Recon Loss: 0.002119 | Commit Loss: 0.000811 | Perplexity: 690.283659
2025-09-14 21:28:37,650 Stage: Train 0.5 | Epoch: 153 | Iter: 465800 | Total Loss: 0.002549 | Recon Loss: 0.002141 | Commit Loss: 0.000817 | Perplexity: 691.108533
2025-09-14 21:28:45,893 Stage: Train 0.5 | Epoch: 153 | Iter: 466000 | Total Loss: 0.002557 | Recon Loss: 0.002151 | Commit Loss: 0.000812 | Perplexity: 690.417211
2025-09-14 21:28:54,163 Stage: Train 0.5 | Epoch: 153 | Iter: 466200 | Total Loss: 0.002551 | Recon Loss: 0.002142 | Commit Loss: 0.000819 | Perplexity: 690.612552
2025-09-14 21:29:02,361 Stage: Train 0.5 | Epoch: 153 | Iter: 466400 | Total Loss: 0.002555 | Recon Loss: 0.002143 | Commit Loss: 0.000824 | Perplexity: 693.424567
2025-09-14 21:29:10,590 Stage: Train 0.5 | Epoch: 153 | Iter: 466600 | Total Loss: 0.002506 | Recon Loss: 0.002098 | Commit Loss: 0.000815 | Perplexity: 693.510617
2025-09-14 21:29:18,801 Stage: Train 0.5 | Epoch: 153 | Iter: 466800 | Total Loss: 0.002582 | Recon Loss: 0.002174 | Commit Loss: 0.000815 | Perplexity: 689.006429
2025-09-14 21:29:27,058 Stage: Train 0.5 | Epoch: 153 | Iter: 467000 | Total Loss: 0.002540 | Recon Loss: 0.002128 | Commit Loss: 0.000823 | Perplexity: 694.842083
2025-09-14 21:29:35,275 Stage: Train 0.5 | Epoch: 153 | Iter: 467200 | Total Loss: 0.002529 | Recon Loss: 0.002119 | Commit Loss: 0.000820 | Perplexity: 693.321136
2025-09-14 21:29:43,506 Stage: Train 0.5 | Epoch: 153 | Iter: 467400 | Total Loss: 0.002523 | Recon Loss: 0.002115 | Commit Loss: 0.000817 | Perplexity: 688.971368
2025-09-14 21:29:51,700 Stage: Train 0.5 | Epoch: 153 | Iter: 467600 | Total Loss: 0.002577 | Recon Loss: 0.002169 | Commit Loss: 0.000816 | Perplexity: 689.925649
2025-09-14 21:29:59,893 Stage: Train 0.5 | Epoch: 153 | Iter: 467800 | Total Loss: 0.002534 | Recon Loss: 0.002125 | Commit Loss: 0.000817 | Perplexity: 690.187848
Trainning Epoch:  93%|█████████▎| 154/165 [5:07:12<22:48, 124.40s/it]2025-09-14 21:30:08,125 Stage: Train 0.5 | Epoch: 154 | Iter: 468000 | Total Loss: 0.002512 | Recon Loss: 0.002106 | Commit Loss: 0.000813 | Perplexity: 688.849333
2025-09-14 21:30:16,342 Stage: Train 0.5 | Epoch: 154 | Iter: 468200 | Total Loss: 0.002535 | Recon Loss: 0.002128 | Commit Loss: 0.000813 | Perplexity: 691.020403
2025-09-14 21:30:24,518 Stage: Train 0.5 | Epoch: 154 | Iter: 468400 | Total Loss: 0.002527 | Recon Loss: 0.002121 | Commit Loss: 0.000811 | Perplexity: 691.622335
2025-09-14 21:30:32,722 Stage: Train 0.5 | Epoch: 154 | Iter: 468600 | Total Loss: 0.002549 | Recon Loss: 0.002143 | Commit Loss: 0.000812 | Perplexity: 690.645999
2025-09-14 21:30:40,930 Stage: Train 0.5 | Epoch: 154 | Iter: 468800 | Total Loss: 0.002558 | Recon Loss: 0.002151 | Commit Loss: 0.000814 | Perplexity: 691.416828
2025-09-14 21:30:49,152 Stage: Train 0.5 | Epoch: 154 | Iter: 469000 | Total Loss: 0.002486 | Recon Loss: 0.002081 | Commit Loss: 0.000809 | Perplexity: 693.419586
2025-09-14 21:30:57,357 Stage: Train 0.5 | Epoch: 154 | Iter: 469200 | Total Loss: 0.002587 | Recon Loss: 0.002183 | Commit Loss: 0.000809 | Perplexity: 692.689952
2025-09-14 21:31:05,583 Stage: Train 0.5 | Epoch: 154 | Iter: 469400 | Total Loss: 0.002539 | Recon Loss: 0.002131 | Commit Loss: 0.000816 | Perplexity: 692.613425
2025-09-14 21:31:13,809 Stage: Train 0.5 | Epoch: 154 | Iter: 469600 | Total Loss: 0.002500 | Recon Loss: 0.002093 | Commit Loss: 0.000814 | Perplexity: 691.156812
2025-09-14 21:31:22,039 Stage: Train 0.5 | Epoch: 154 | Iter: 469800 | Total Loss: 0.002559 | Recon Loss: 0.002149 | Commit Loss: 0.000819 | Perplexity: 689.854489
2025-09-14 21:31:30,254 Stage: Train 0.5 | Epoch: 154 | Iter: 470000 | Total Loss: 0.002535 | Recon Loss: 0.002128 | Commit Loss: 0.000816 | Perplexity: 693.949279
2025-09-14 21:31:38,468 Stage: Train 0.5 | Epoch: 154 | Iter: 470200 | Total Loss: 0.002488 | Recon Loss: 0.002082 | Commit Loss: 0.000813 | Perplexity: 691.342835
2025-09-14 21:31:46,675 Stage: Train 0.5 | Epoch: 154 | Iter: 470400 | Total Loss: 0.002537 | Recon Loss: 0.002126 | Commit Loss: 0.000821 | Perplexity: 692.543211
2025-09-14 21:31:54,859 Stage: Train 0.5 | Epoch: 154 | Iter: 470600 | Total Loss: 0.002542 | Recon Loss: 0.002135 | Commit Loss: 0.000815 | Perplexity: 692.301194
2025-09-14 21:32:03,055 Stage: Train 0.5 | Epoch: 154 | Iter: 470800 | Total Loss: 0.002531 | Recon Loss: 0.002122 | Commit Loss: 0.000816 | Perplexity: 690.231050
Trainning Epoch:  94%|█████████▍| 155/165 [5:09:17<20:44, 124.49s/it]2025-09-14 21:32:11,293 Stage: Train 0.5 | Epoch: 155 | Iter: 471000 | Total Loss: 0.002539 | Recon Loss: 0.002134 | Commit Loss: 0.000810 | Perplexity: 690.536301
2025-09-14 21:32:19,480 Stage: Train 0.5 | Epoch: 155 | Iter: 471200 | Total Loss: 0.002515 | Recon Loss: 0.002110 | Commit Loss: 0.000810 | Perplexity: 693.505236
2025-09-14 21:32:27,676 Stage: Train 0.5 | Epoch: 155 | Iter: 471400 | Total Loss: 0.002537 | Recon Loss: 0.002133 | Commit Loss: 0.000808 | Perplexity: 690.847599
2025-09-14 21:32:35,921 Stage: Train 0.5 | Epoch: 155 | Iter: 471600 | Total Loss: 0.002510 | Recon Loss: 0.002101 | Commit Loss: 0.000819 | Perplexity: 693.119769
2025-09-14 21:32:44,149 Stage: Train 0.5 | Epoch: 155 | Iter: 471800 | Total Loss: 0.002540 | Recon Loss: 0.002138 | Commit Loss: 0.000805 | Perplexity: 691.572877
2025-09-14 21:32:52,406 Stage: Train 0.5 | Epoch: 155 | Iter: 472000 | Total Loss: 0.002528 | Recon Loss: 0.002119 | Commit Loss: 0.000818 | Perplexity: 693.305968
2025-09-14 21:33:00,601 Stage: Train 0.5 | Epoch: 155 | Iter: 472200 | Total Loss: 0.002515 | Recon Loss: 0.002111 | Commit Loss: 0.000807 | Perplexity: 690.755054
2025-09-14 21:33:08,782 Stage: Train 0.5 | Epoch: 155 | Iter: 472400 | Total Loss: 0.002515 | Recon Loss: 0.002108 | Commit Loss: 0.000813 | Perplexity: 692.516670
2025-09-14 21:33:16,966 Stage: Train 0.5 | Epoch: 155 | Iter: 472600 | Total Loss: 0.002505 | Recon Loss: 0.002097 | Commit Loss: 0.000816 | Perplexity: 693.696184
2025-09-14 21:33:25,152 Stage: Train 0.5 | Epoch: 155 | Iter: 472800 | Total Loss: 0.002521 | Recon Loss: 0.002115 | Commit Loss: 0.000811 | Perplexity: 692.386674
2025-09-14 21:33:33,401 Stage: Train 0.5 | Epoch: 155 | Iter: 473000 | Total Loss: 0.002546 | Recon Loss: 0.002142 | Commit Loss: 0.000808 | Perplexity: 688.198899
2025-09-14 21:33:41,669 Stage: Train 0.5 | Epoch: 155 | Iter: 473200 | Total Loss: 0.002513 | Recon Loss: 0.002108 | Commit Loss: 0.000810 | Perplexity: 688.342385
2025-09-14 21:33:49,918 Stage: Train 0.5 | Epoch: 155 | Iter: 473400 | Total Loss: 0.002525 | Recon Loss: 0.002118 | Commit Loss: 0.000813 | Perplexity: 689.619560
2025-09-14 21:33:58,169 Stage: Train 0.5 | Epoch: 155 | Iter: 473600 | Total Loss: 0.002518 | Recon Loss: 0.002114 | Commit Loss: 0.000806 | Perplexity: 693.785603
2025-09-14 21:34:06,410 Stage: Train 0.5 | Epoch: 155 | Iter: 473800 | Total Loss: 0.002524 | Recon Loss: 0.002120 | Commit Loss: 0.000809 | Perplexity: 690.953234
Trainning Epoch:  95%|█████████▍| 156/165 [5:11:22<18:41, 124.63s/it]2025-09-14 21:34:14,648 Stage: Train 0.5 | Epoch: 156 | Iter: 474000 | Total Loss: 0.002523 | Recon Loss: 0.002117 | Commit Loss: 0.000813 | Perplexity: 692.591326
2025-09-14 21:34:22,858 Stage: Train 0.5 | Epoch: 156 | Iter: 474200 | Total Loss: 0.002525 | Recon Loss: 0.002125 | Commit Loss: 0.000801 | Perplexity: 688.515352
2025-09-14 21:34:31,094 Stage: Train 0.5 | Epoch: 156 | Iter: 474400 | Total Loss: 0.002553 | Recon Loss: 0.002149 | Commit Loss: 0.000807 | Perplexity: 691.073393
2025-09-14 21:34:39,304 Stage: Train 0.5 | Epoch: 156 | Iter: 474600 | Total Loss: 0.002521 | Recon Loss: 0.002115 | Commit Loss: 0.000812 | Perplexity: 693.079595
2025-09-14 21:34:47,520 Stage: Train 0.5 | Epoch: 156 | Iter: 474800 | Total Loss: 0.002502 | Recon Loss: 0.002100 | Commit Loss: 0.000803 | Perplexity: 689.979812
2025-09-14 21:34:55,689 Stage: Train 0.5 | Epoch: 156 | Iter: 475000 | Total Loss: 0.002531 | Recon Loss: 0.002126 | Commit Loss: 0.000810 | Perplexity: 690.255936
2025-09-14 21:35:03,909 Stage: Train 0.5 | Epoch: 156 | Iter: 475200 | Total Loss: 0.002522 | Recon Loss: 0.002117 | Commit Loss: 0.000810 | Perplexity: 694.168817
2025-09-14 21:35:12,097 Stage: Train 0.5 | Epoch: 156 | Iter: 475400 | Total Loss: 0.002523 | Recon Loss: 0.002118 | Commit Loss: 0.000810 | Perplexity: 691.086628
2025-09-14 21:35:20,312 Stage: Train 0.5 | Epoch: 156 | Iter: 475600 | Total Loss: 0.002528 | Recon Loss: 0.002123 | Commit Loss: 0.000811 | Perplexity: 692.594452
2025-09-14 21:35:28,538 Stage: Train 0.5 | Epoch: 156 | Iter: 475800 | Total Loss: 0.002514 | Recon Loss: 0.002105 | Commit Loss: 0.000819 | Perplexity: 693.533948
2025-09-14 21:35:36,773 Stage: Train 0.5 | Epoch: 156 | Iter: 476000 | Total Loss: 0.002536 | Recon Loss: 0.002131 | Commit Loss: 0.000810 | Perplexity: 691.144344
2025-09-14 21:35:45,006 Stage: Train 0.5 | Epoch: 156 | Iter: 476200 | Total Loss: 0.002535 | Recon Loss: 0.002132 | Commit Loss: 0.000807 | Perplexity: 691.451080
2025-09-14 21:35:53,200 Stage: Train 0.5 | Epoch: 156 | Iter: 476400 | Total Loss: 0.002497 | Recon Loss: 0.002091 | Commit Loss: 0.000812 | Perplexity: 692.282307
2025-09-14 21:36:01,425 Stage: Train 0.5 | Epoch: 156 | Iter: 476600 | Total Loss: 0.002527 | Recon Loss: 0.002122 | Commit Loss: 0.000811 | Perplexity: 692.735692
2025-09-14 21:36:09,655 Stage: Train 0.5 | Epoch: 156 | Iter: 476800 | Total Loss: 0.002527 | Recon Loss: 0.002122 | Commit Loss: 0.000810 | Perplexity: 692.936127
Trainning Epoch:  95%|█████████▌| 157/165 [5:13:27<16:37, 124.67s/it]2025-09-14 21:36:17,854 Stage: Train 0.5 | Epoch: 157 | Iter: 477000 | Total Loss: 0.002546 | Recon Loss: 0.002140 | Commit Loss: 0.000811 | Perplexity: 691.313687
2025-09-14 21:36:26,123 Stage: Train 0.5 | Epoch: 157 | Iter: 477200 | Total Loss: 0.002505 | Recon Loss: 0.002101 | Commit Loss: 0.000807 | Perplexity: 691.424567
2025-09-14 21:36:34,357 Stage: Train 0.5 | Epoch: 157 | Iter: 477400 | Total Loss: 0.002488 | Recon Loss: 0.002084 | Commit Loss: 0.000808 | Perplexity: 690.922755
2025-09-14 21:36:42,589 Stage: Train 0.5 | Epoch: 157 | Iter: 477600 | Total Loss: 0.002493 | Recon Loss: 0.002092 | Commit Loss: 0.000802 | Perplexity: 690.812845
2025-09-14 21:36:50,854 Stage: Train 0.5 | Epoch: 157 | Iter: 477800 | Total Loss: 0.002518 | Recon Loss: 0.002119 | Commit Loss: 0.000797 | Perplexity: 690.222471
2025-09-14 21:36:59,101 Stage: Train 0.5 | Epoch: 157 | Iter: 478000 | Total Loss: 0.002528 | Recon Loss: 0.002125 | Commit Loss: 0.000805 | Perplexity: 691.533107
2025-09-14 21:37:07,362 Stage: Train 0.5 | Epoch: 157 | Iter: 478200 | Total Loss: 0.002495 | Recon Loss: 0.002095 | Commit Loss: 0.000800 | Perplexity: 686.093175
2025-09-14 21:37:15,502 Stage: Train 0.5 | Epoch: 157 | Iter: 478400 | Total Loss: 0.002517 | Recon Loss: 0.002114 | Commit Loss: 0.000805 | Perplexity: 693.134269
2025-09-14 21:37:23,718 Stage: Train 0.5 | Epoch: 157 | Iter: 478600 | Total Loss: 0.002561 | Recon Loss: 0.002154 | Commit Loss: 0.000814 | Perplexity: 693.356376
2025-09-14 21:37:31,975 Stage: Train 0.5 | Epoch: 157 | Iter: 478800 | Total Loss: 0.002539 | Recon Loss: 0.002134 | Commit Loss: 0.000810 | Perplexity: 691.900403
2025-09-14 21:37:40,217 Stage: Train 0.5 | Epoch: 157 | Iter: 479000 | Total Loss: 0.002497 | Recon Loss: 0.002091 | Commit Loss: 0.000811 | Perplexity: 690.982672
2025-09-14 21:37:48,471 Stage: Train 0.5 | Epoch: 157 | Iter: 479200 | Total Loss: 0.002519 | Recon Loss: 0.002119 | Commit Loss: 0.000800 | Perplexity: 692.402869
2025-09-14 21:37:56,744 Stage: Train 0.5 | Epoch: 157 | Iter: 479400 | Total Loss: 0.002508 | Recon Loss: 0.002102 | Commit Loss: 0.000811 | Perplexity: 692.044409
2025-09-14 21:38:04,922 Stage: Train 0.5 | Epoch: 157 | Iter: 479600 | Total Loss: 0.002481 | Recon Loss: 0.002074 | Commit Loss: 0.000813 | Perplexity: 693.655596
2025-09-14 21:38:13,077 Stage: Train 0.5 | Epoch: 157 | Iter: 479800 | Total Loss: 0.002536 | Recon Loss: 0.002135 | Commit Loss: 0.000803 | Perplexity: 691.919333
2025-09-14 21:38:21,296 Stage: Train 0.5 | Epoch: 157 | Iter: 480000 | Total Loss: 0.002499 | Recon Loss: 0.002094 | Commit Loss: 0.000810 | Perplexity: 694.615710
2025-09-14 21:38:21,297 Saving model at iteration 480000
2025-09-14 21:38:21,660 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_158_step_480000
2025-09-14 21:38:21,789 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_158_step_480000/pytorch_model.bin
2025-09-14 21:38:22,029 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_158_step_480000/optimizer.bin
2025-09-14 21:38:22,029 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_158_step_480000/scheduler.bin
2025-09-14 21:38:22,030 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_158_step_480000/random_states_0.pkl
Trainning Epoch:  96%|█████████▌| 158/165 [5:15:32<14:34, 124.99s/it]2025-09-14 21:38:30,272 Stage: Train 0.5 | Epoch: 158 | Iter: 480200 | Total Loss: 0.002516 | Recon Loss: 0.002115 | Commit Loss: 0.000803 | Perplexity: 692.008922
2025-09-14 21:38:38,483 Stage: Train 0.5 | Epoch: 158 | Iter: 480400 | Total Loss: 0.002548 | Recon Loss: 0.002143 | Commit Loss: 0.000810 | Perplexity: 693.227074
2025-09-14 21:38:46,687 Stage: Train 0.5 | Epoch: 158 | Iter: 480600 | Total Loss: 0.002502 | Recon Loss: 0.002099 | Commit Loss: 0.000805 | Perplexity: 693.851399
2025-09-14 21:38:54,912 Stage: Train 0.5 | Epoch: 158 | Iter: 480800 | Total Loss: 0.002517 | Recon Loss: 0.002115 | Commit Loss: 0.000803 | Perplexity: 692.096469
2025-09-14 21:39:03,128 Stage: Train 0.5 | Epoch: 158 | Iter: 481000 | Total Loss: 0.002519 | Recon Loss: 0.002117 | Commit Loss: 0.000803 | Perplexity: 692.720256
2025-09-14 21:39:11,365 Stage: Train 0.5 | Epoch: 158 | Iter: 481200 | Total Loss: 0.002509 | Recon Loss: 0.002109 | Commit Loss: 0.000800 | Perplexity: 689.856793
2025-09-14 21:39:19,618 Stage: Train 0.5 | Epoch: 158 | Iter: 481400 | Total Loss: 0.002522 | Recon Loss: 0.002125 | Commit Loss: 0.000795 | Perplexity: 690.451089
2025-09-14 21:39:27,844 Stage: Train 0.5 | Epoch: 158 | Iter: 481600 | Total Loss: 0.002488 | Recon Loss: 0.002087 | Commit Loss: 0.000803 | Perplexity: 691.226947
2025-09-14 21:39:36,040 Stage: Train 0.5 | Epoch: 158 | Iter: 481800 | Total Loss: 0.002453 | Recon Loss: 0.002060 | Commit Loss: 0.000787 | Perplexity: 690.948748
2025-09-14 21:39:44,260 Stage: Train 0.5 | Epoch: 158 | Iter: 482000 | Total Loss: 0.002524 | Recon Loss: 0.002121 | Commit Loss: 0.000807 | Perplexity: 692.621936
2025-09-14 21:39:52,492 Stage: Train 0.5 | Epoch: 158 | Iter: 482200 | Total Loss: 0.002514 | Recon Loss: 0.002114 | Commit Loss: 0.000799 | Perplexity: 687.508186
2025-09-14 21:40:00,750 Stage: Train 0.5 | Epoch: 158 | Iter: 482400 | Total Loss: 0.002504 | Recon Loss: 0.002100 | Commit Loss: 0.000807 | Perplexity: 693.896669
2025-09-14 21:40:08,930 Stage: Train 0.5 | Epoch: 158 | Iter: 482600 | Total Loss: 0.002477 | Recon Loss: 0.002076 | Commit Loss: 0.000802 | Perplexity: 689.811579
2025-09-14 21:40:17,134 Stage: Train 0.5 | Epoch: 158 | Iter: 482800 | Total Loss: 0.002545 | Recon Loss: 0.002143 | Commit Loss: 0.000805 | Perplexity: 692.353655
2025-09-14 21:40:25,307 Stage: Train 0.5 | Epoch: 158 | Iter: 483000 | Total Loss: 0.002498 | Recon Loss: 0.002095 | Commit Loss: 0.000806 | Perplexity: 688.888414
Trainning Epoch:  96%|█████████▋| 159/165 [5:17:37<12:29, 124.95s/it]2025-09-14 21:40:33,538 Stage: Train 0.5 | Epoch: 159 | Iter: 483200 | Total Loss: 0.002519 | Recon Loss: 0.002120 | Commit Loss: 0.000799 | Perplexity: 691.901271
2025-09-14 21:40:41,760 Stage: Train 0.5 | Epoch: 159 | Iter: 483400 | Total Loss: 0.002493 | Recon Loss: 0.002090 | Commit Loss: 0.000807 | Perplexity: 689.837934
2025-09-14 21:40:49,973 Stage: Train 0.5 | Epoch: 159 | Iter: 483600 | Total Loss: 0.002525 | Recon Loss: 0.002127 | Commit Loss: 0.000797 | Perplexity: 691.910722
2025-09-14 21:40:58,158 Stage: Train 0.5 | Epoch: 159 | Iter: 483800 | Total Loss: 0.002474 | Recon Loss: 0.002073 | Commit Loss: 0.000802 | Perplexity: 695.329972
2025-09-14 21:41:06,402 Stage: Train 0.5 | Epoch: 159 | Iter: 484000 | Total Loss: 0.002519 | Recon Loss: 0.002119 | Commit Loss: 0.000800 | Perplexity: 692.789971
2025-09-14 21:41:14,615 Stage: Train 0.5 | Epoch: 159 | Iter: 484200 | Total Loss: 0.002525 | Recon Loss: 0.002127 | Commit Loss: 0.000794 | Perplexity: 690.705347
2025-09-14 21:41:22,789 Stage: Train 0.5 | Epoch: 159 | Iter: 484400 | Total Loss: 0.002483 | Recon Loss: 0.002078 | Commit Loss: 0.000809 | Perplexity: 692.305973
2025-09-14 21:41:30,956 Stage: Train 0.5 | Epoch: 159 | Iter: 484600 | Total Loss: 0.002521 | Recon Loss: 0.002122 | Commit Loss: 0.000799 | Perplexity: 690.224336
2025-09-14 21:41:39,175 Stage: Train 0.5 | Epoch: 159 | Iter: 484800 | Total Loss: 0.002528 | Recon Loss: 0.002126 | Commit Loss: 0.000804 | Perplexity: 692.804842
2025-09-14 21:41:47,352 Stage: Train 0.5 | Epoch: 159 | Iter: 485000 | Total Loss: 0.002470 | Recon Loss: 0.002066 | Commit Loss: 0.000807 | Perplexity: 693.319428
2025-09-14 21:41:55,557 Stage: Train 0.5 | Epoch: 159 | Iter: 485200 | Total Loss: 0.002490 | Recon Loss: 0.002087 | Commit Loss: 0.000806 | Perplexity: 693.336613
2025-09-14 21:42:03,738 Stage: Train 0.5 | Epoch: 159 | Iter: 485400 | Total Loss: 0.002546 | Recon Loss: 0.002146 | Commit Loss: 0.000801 | Perplexity: 691.438378
2025-09-14 21:42:11,911 Stage: Train 0.5 | Epoch: 159 | Iter: 485600 | Total Loss: 0.002507 | Recon Loss: 0.002107 | Commit Loss: 0.000800 | Perplexity: 690.120828
2025-09-14 21:42:20,274 Stage: Train 0.5 | Epoch: 159 | Iter: 485800 | Total Loss: 0.002504 | Recon Loss: 0.002105 | Commit Loss: 0.000797 | Perplexity: 687.023082
2025-09-14 21:42:28,532 Stage: Train 0.5 | Epoch: 159 | Iter: 486000 | Total Loss: 0.002508 | Recon Loss: 0.002104 | Commit Loss: 0.000808 | Perplexity: 692.944317
Trainning Epoch:  97%|█████████▋| 160/165 [5:19:42<10:24, 124.90s/it]2025-09-14 21:42:36,782 Stage: Train 0.5 | Epoch: 160 | Iter: 486200 | Total Loss: 0.002506 | Recon Loss: 0.002107 | Commit Loss: 0.000798 | Perplexity: 690.931726
2025-09-14 21:42:45,048 Stage: Train 0.5 | Epoch: 160 | Iter: 486400 | Total Loss: 0.002515 | Recon Loss: 0.002115 | Commit Loss: 0.000800 | Perplexity: 691.229048
2025-09-14 21:42:53,328 Stage: Train 0.5 | Epoch: 160 | Iter: 486600 | Total Loss: 0.002513 | Recon Loss: 0.002113 | Commit Loss: 0.000800 | Perplexity: 692.692087
2025-09-14 21:43:01,645 Stage: Train 0.5 | Epoch: 160 | Iter: 486800 | Total Loss: 0.002493 | Recon Loss: 0.002094 | Commit Loss: 0.000800 | Perplexity: 692.128635
2025-09-14 21:43:09,952 Stage: Train 0.5 | Epoch: 160 | Iter: 487000 | Total Loss: 0.002541 | Recon Loss: 0.002144 | Commit Loss: 0.000792 | Perplexity: 690.241382
2025-09-14 21:43:18,158 Stage: Train 0.5 | Epoch: 160 | Iter: 487200 | Total Loss: 0.002491 | Recon Loss: 0.002091 | Commit Loss: 0.000799 | Perplexity: 692.353833
2025-09-14 21:43:26,368 Stage: Train 0.5 | Epoch: 160 | Iter: 487400 | Total Loss: 0.002502 | Recon Loss: 0.002104 | Commit Loss: 0.000796 | Perplexity: 691.860086
2025-09-14 21:43:34,639 Stage: Train 0.5 | Epoch: 160 | Iter: 487600 | Total Loss: 0.002508 | Recon Loss: 0.002111 | Commit Loss: 0.000796 | Perplexity: 691.767838
2025-09-14 21:43:42,889 Stage: Train 0.5 | Epoch: 160 | Iter: 487800 | Total Loss: 0.002512 | Recon Loss: 0.002114 | Commit Loss: 0.000796 | Perplexity: 688.744409
2025-09-14 21:43:51,092 Stage: Train 0.5 | Epoch: 160 | Iter: 488000 | Total Loss: 0.002482 | Recon Loss: 0.002081 | Commit Loss: 0.000803 | Perplexity: 694.468249
2025-09-14 21:43:59,324 Stage: Train 0.5 | Epoch: 160 | Iter: 488200 | Total Loss: 0.002488 | Recon Loss: 0.002092 | Commit Loss: 0.000793 | Perplexity: 688.886475
2025-09-14 21:44:07,527 Stage: Train 0.5 | Epoch: 160 | Iter: 488400 | Total Loss: 0.002547 | Recon Loss: 0.002146 | Commit Loss: 0.000803 | Perplexity: 691.325290
2025-09-14 21:44:15,876 Stage: Train 0.5 | Epoch: 160 | Iter: 488600 | Total Loss: 0.002500 | Recon Loss: 0.002098 | Commit Loss: 0.000804 | Perplexity: 691.350667
2025-09-14 21:44:24,091 Stage: Train 0.5 | Epoch: 160 | Iter: 488800 | Total Loss: 0.002455 | Recon Loss: 0.002056 | Commit Loss: 0.000798 | Perplexity: 691.927070
2025-09-14 21:44:32,289 Stage: Train 0.5 | Epoch: 160 | Iter: 489000 | Total Loss: 0.002532 | Recon Loss: 0.002129 | Commit Loss: 0.000807 | Perplexity: 689.411262
Trainning Epoch:  98%|█████████▊| 161/165 [5:21:47<08:20, 125.04s/it]2025-09-14 21:44:40,554 Stage: Train 0.5 | Epoch: 161 | Iter: 489200 | Total Loss: 0.002507 | Recon Loss: 0.002110 | Commit Loss: 0.000795 | Perplexity: 688.979927
2025-09-14 21:44:48,747 Stage: Train 0.5 | Epoch: 161 | Iter: 489400 | Total Loss: 0.002490 | Recon Loss: 0.002097 | Commit Loss: 0.000786 | Perplexity: 689.352831
2025-09-14 21:44:56,980 Stage: Train 0.5 | Epoch: 161 | Iter: 489600 | Total Loss: 0.002515 | Recon Loss: 0.002116 | Commit Loss: 0.000797 | Perplexity: 693.046053
2025-09-14 21:45:05,330 Stage: Train 0.5 | Epoch: 161 | Iter: 489800 | Total Loss: 0.002465 | Recon Loss: 0.002070 | Commit Loss: 0.000790 | Perplexity: 690.200267
2025-09-14 21:45:13,573 Stage: Train 0.5 | Epoch: 161 | Iter: 490000 | Total Loss: 0.002498 | Recon Loss: 0.002102 | Commit Loss: 0.000791 | Perplexity: 689.648430
2025-09-14 21:45:21,770 Stage: Train 0.5 | Epoch: 161 | Iter: 490200 | Total Loss: 0.002465 | Recon Loss: 0.002067 | Commit Loss: 0.000796 | Perplexity: 690.988720
2025-09-14 21:45:29,981 Stage: Train 0.5 | Epoch: 161 | Iter: 490400 | Total Loss: 0.002513 | Recon Loss: 0.002117 | Commit Loss: 0.000792 | Perplexity: 690.858343
2025-09-14 21:45:38,173 Stage: Train 0.5 | Epoch: 161 | Iter: 490600 | Total Loss: 0.002528 | Recon Loss: 0.002129 | Commit Loss: 0.000799 | Perplexity: 693.618482
2025-09-14 21:45:46,390 Stage: Train 0.5 | Epoch: 161 | Iter: 490800 | Total Loss: 0.002493 | Recon Loss: 0.002092 | Commit Loss: 0.000803 | Perplexity: 693.750853
2025-09-14 21:45:54,553 Stage: Train 0.5 | Epoch: 161 | Iter: 491000 | Total Loss: 0.002489 | Recon Loss: 0.002091 | Commit Loss: 0.000797 | Perplexity: 690.336264
2025-09-14 21:46:02,736 Stage: Train 0.5 | Epoch: 161 | Iter: 491200 | Total Loss: 0.002497 | Recon Loss: 0.002097 | Commit Loss: 0.000800 | Perplexity: 694.622433
2025-09-14 21:46:10,935 Stage: Train 0.5 | Epoch: 161 | Iter: 491400 | Total Loss: 0.002489 | Recon Loss: 0.002089 | Commit Loss: 0.000800 | Perplexity: 692.955516
2025-09-14 21:46:19,162 Stage: Train 0.5 | Epoch: 161 | Iter: 491600 | Total Loss: 0.002487 | Recon Loss: 0.002088 | Commit Loss: 0.000797 | Perplexity: 693.816627
2025-09-14 21:46:27,388 Stage: Train 0.5 | Epoch: 161 | Iter: 491800 | Total Loss: 0.002518 | Recon Loss: 0.002119 | Commit Loss: 0.000799 | Perplexity: 689.643297
2025-09-14 21:46:35,605 Stage: Train 0.5 | Epoch: 161 | Iter: 492000 | Total Loss: 0.002466 | Recon Loss: 0.002068 | Commit Loss: 0.000796 | Perplexity: 690.289396
Trainning Epoch:  98%|█████████▊| 162/165 [5:23:52<06:14, 124.98s/it]2025-09-14 21:46:43,853 Stage: Train 0.5 | Epoch: 162 | Iter: 492200 | Total Loss: 0.002483 | Recon Loss: 0.002085 | Commit Loss: 0.000797 | Perplexity: 688.720811
2025-09-14 21:46:52,121 Stage: Train 0.5 | Epoch: 162 | Iter: 492400 | Total Loss: 0.002508 | Recon Loss: 0.002110 | Commit Loss: 0.000796 | Perplexity: 690.462871
2025-09-14 21:47:00,387 Stage: Train 0.5 | Epoch: 162 | Iter: 492600 | Total Loss: 0.002472 | Recon Loss: 0.002072 | Commit Loss: 0.000799 | Perplexity: 691.305424
2025-09-14 21:47:08,654 Stage: Train 0.5 | Epoch: 162 | Iter: 492800 | Total Loss: 0.002522 | Recon Loss: 0.002125 | Commit Loss: 0.000793 | Perplexity: 692.246228
2025-09-14 21:47:16,922 Stage: Train 0.5 | Epoch: 162 | Iter: 493000 | Total Loss: 0.002449 | Recon Loss: 0.002049 | Commit Loss: 0.000799 | Perplexity: 694.532059
2025-09-14 21:47:25,128 Stage: Train 0.5 | Epoch: 162 | Iter: 493200 | Total Loss: 0.002540 | Recon Loss: 0.002145 | Commit Loss: 0.000791 | Perplexity: 691.473518
2025-09-14 21:47:33,318 Stage: Train 0.5 | Epoch: 162 | Iter: 493400 | Total Loss: 0.002441 | Recon Loss: 0.002044 | Commit Loss: 0.000793 | Perplexity: 692.240605
2025-09-14 21:47:41,556 Stage: Train 0.5 | Epoch: 162 | Iter: 493600 | Total Loss: 0.002449 | Recon Loss: 0.002053 | Commit Loss: 0.000791 | Perplexity: 691.226410
2025-09-14 21:47:49,750 Stage: Train 0.5 | Epoch: 162 | Iter: 493800 | Total Loss: 0.002496 | Recon Loss: 0.002098 | Commit Loss: 0.000795 | Perplexity: 690.455680
2025-09-14 21:47:57,982 Stage: Train 0.5 | Epoch: 162 | Iter: 494000 | Total Loss: 0.002512 | Recon Loss: 0.002112 | Commit Loss: 0.000801 | Perplexity: 693.539580
2025-09-14 21:48:06,195 Stage: Train 0.5 | Epoch: 162 | Iter: 494200 | Total Loss: 0.002494 | Recon Loss: 0.002092 | Commit Loss: 0.000803 | Perplexity: 691.468759
2025-09-14 21:48:14,394 Stage: Train 0.5 | Epoch: 162 | Iter: 494400 | Total Loss: 0.002484 | Recon Loss: 0.002086 | Commit Loss: 0.000795 | Perplexity: 688.655297
2025-09-14 21:48:22,632 Stage: Train 0.5 | Epoch: 162 | Iter: 494600 | Total Loss: 0.002526 | Recon Loss: 0.002126 | Commit Loss: 0.000798 | Perplexity: 691.658812
2025-09-14 21:48:30,851 Stage: Train 0.5 | Epoch: 162 | Iter: 494800 | Total Loss: 0.002496 | Recon Loss: 0.002096 | Commit Loss: 0.000798 | Perplexity: 693.023646
2025-09-14 21:48:39,033 Stage: Train 0.5 | Epoch: 162 | Iter: 495000 | Total Loss: 0.002465 | Recon Loss: 0.002068 | Commit Loss: 0.000793 | Perplexity: 690.325392
Trainning Epoch:  99%|█████████▉| 163/165 [5:25:57<04:09, 124.96s/it]2025-09-14 21:48:47,192 Stage: Train 0.5 | Epoch: 163 | Iter: 495200 | Total Loss: 0.002471 | Recon Loss: 0.002076 | Commit Loss: 0.000791 | Perplexity: 686.656514
2025-09-14 21:48:55,384 Stage: Train 0.5 | Epoch: 163 | Iter: 495400 | Total Loss: 0.002456 | Recon Loss: 0.002061 | Commit Loss: 0.000790 | Perplexity: 687.651896
2025-09-14 21:49:03,624 Stage: Train 0.5 | Epoch: 163 | Iter: 495600 | Total Loss: 0.002498 | Recon Loss: 0.002098 | Commit Loss: 0.000799 | Perplexity: 692.575941
2025-09-14 21:49:11,764 Stage: Train 0.5 | Epoch: 163 | Iter: 495800 | Total Loss: 0.002444 | Recon Loss: 0.002047 | Commit Loss: 0.000795 | Perplexity: 692.553378
2025-09-14 21:49:19,955 Stage: Train 0.5 | Epoch: 163 | Iter: 496000 | Total Loss: 0.002447 | Recon Loss: 0.002053 | Commit Loss: 0.000790 | Perplexity: 692.008293
2025-09-14 21:49:28,181 Stage: Train 0.5 | Epoch: 163 | Iter: 496200 | Total Loss: 0.002527 | Recon Loss: 0.002129 | Commit Loss: 0.000796 | Perplexity: 692.497583
2025-09-14 21:49:36,392 Stage: Train 0.5 | Epoch: 163 | Iter: 496400 | Total Loss: 0.002494 | Recon Loss: 0.002100 | Commit Loss: 0.000788 | Perplexity: 687.792251
2025-09-14 21:49:44,596 Stage: Train 0.5 | Epoch: 163 | Iter: 496600 | Total Loss: 0.002482 | Recon Loss: 0.002085 | Commit Loss: 0.000794 | Perplexity: 691.656818
2025-09-14 21:49:52,850 Stage: Train 0.5 | Epoch: 163 | Iter: 496800 | Total Loss: 0.002500 | Recon Loss: 0.002102 | Commit Loss: 0.000796 | Perplexity: 691.189650
2025-09-14 21:50:01,167 Stage: Train 0.5 | Epoch: 163 | Iter: 497000 | Total Loss: 0.002476 | Recon Loss: 0.002082 | Commit Loss: 0.000788 | Perplexity: 691.091659
2025-09-14 21:50:09,399 Stage: Train 0.5 | Epoch: 163 | Iter: 497200 | Total Loss: 0.002490 | Recon Loss: 0.002094 | Commit Loss: 0.000792 | Perplexity: 690.543626
2025-09-14 21:50:17,626 Stage: Train 0.5 | Epoch: 163 | Iter: 497400 | Total Loss: 0.002477 | Recon Loss: 0.002082 | Commit Loss: 0.000792 | Perplexity: 691.267014
2025-09-14 21:50:25,830 Stage: Train 0.5 | Epoch: 163 | Iter: 497600 | Total Loss: 0.002525 | Recon Loss: 0.002125 | Commit Loss: 0.000800 | Perplexity: 691.037939
2025-09-14 21:50:34,054 Stage: Train 0.5 | Epoch: 163 | Iter: 497800 | Total Loss: 0.002478 | Recon Loss: 0.002082 | Commit Loss: 0.000792 | Perplexity: 691.138155
2025-09-14 21:50:42,277 Stage: Train 0.5 | Epoch: 163 | Iter: 498000 | Total Loss: 0.002454 | Recon Loss: 0.002057 | Commit Loss: 0.000795 | Perplexity: 693.239126
2025-09-14 21:50:50,515 Stage: Train 0.5 | Epoch: 163 | Iter: 498200 | Total Loss: 0.002510 | Recon Loss: 0.002110 | Commit Loss: 0.000799 | Perplexity: 693.449802
Trainning Epoch:  99%|█████████▉| 164/165 [5:28:02<02:04, 124.94s/it]2025-09-14 21:50:58,758 Stage: Train 0.5 | Epoch: 164 | Iter: 498400 | Total Loss: 0.002481 | Recon Loss: 0.002088 | Commit Loss: 0.000788 | Perplexity: 688.159388
2025-09-14 21:51:06,920 Stage: Train 0.5 | Epoch: 164 | Iter: 498600 | Total Loss: 0.002452 | Recon Loss: 0.002057 | Commit Loss: 0.000790 | Perplexity: 689.425238
2025-09-14 21:51:15,133 Stage: Train 0.5 | Epoch: 164 | Iter: 498800 | Total Loss: 0.002489 | Recon Loss: 0.002092 | Commit Loss: 0.000794 | Perplexity: 691.560950
2025-09-14 21:51:23,338 Stage: Train 0.5 | Epoch: 164 | Iter: 499000 | Total Loss: 0.002505 | Recon Loss: 0.002108 | Commit Loss: 0.000793 | Perplexity: 690.375515
2025-09-14 21:51:31,521 Stage: Train 0.5 | Epoch: 164 | Iter: 499200 | Total Loss: 0.002475 | Recon Loss: 0.002080 | Commit Loss: 0.000791 | Perplexity: 692.719390
2025-09-14 21:51:39,741 Stage: Train 0.5 | Epoch: 164 | Iter: 499400 | Total Loss: 0.002469 | Recon Loss: 0.002072 | Commit Loss: 0.000793 | Perplexity: 691.713814
2025-09-14 21:51:47,925 Stage: Train 0.5 | Epoch: 164 | Iter: 499600 | Total Loss: 0.002487 | Recon Loss: 0.002090 | Commit Loss: 0.000792 | Perplexity: 693.041295
2025-09-14 21:51:56,125 Stage: Train 0.5 | Epoch: 164 | Iter: 499800 | Total Loss: 0.002447 | Recon Loss: 0.002050 | Commit Loss: 0.000794 | Perplexity: 690.116072
2025-09-14 21:52:04,330 Stage: Train 0.5 | Epoch: 164 | Iter: 500000 | Total Loss: 0.002478 | Recon Loss: 0.002081 | Commit Loss: 0.000795 | Perplexity: 693.663729
2025-09-14 21:52:04,330 Saving model at iteration 500000
2025-09-14 21:52:04,486 Saving current state to vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_165_step_500000
2025-09-14 21:52:04,624 Model weights saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_165_step_500000/pytorch_model.bin
2025-09-14 21:52:04,879 Optimizer state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_165_step_500000/optimizer.bin
2025-09-14 21:52:04,880 Scheduler state saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_165_step_500000/scheduler.bin
2025-09-14 21:52:04,880 Random states saved in vqvae_experiment/h36m_j3d_f16s1_cb2048x1024/models/checkpoint_epoch_165_step_500000/random_states_0.pkl
Trainning Epoch:  99%|█████████▉| 164/165 [5:29:15<02:00, 120.46s/it]
2025-09-14 21:52:04,882 Training finished
